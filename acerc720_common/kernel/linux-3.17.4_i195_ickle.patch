diff -urN a/arch/x86/vdso/vdso-image-32-int80.c b/arch/x86/vdso/vdso-image-32-int80.c
--- a/arch/x86/vdso/vdso-image-32-int80.c	1969-12-31 17:00:00.000000000 -0700
+++ b/arch/x86/vdso/vdso-image-32-int80.c	2014-11-22 18:41:11.681004288 -0700
@@ -0,0 +1,420 @@
+/* AUTOMATICALLY GENERATED -- DO NOT EDIT */
+
+#include <linux/linkage.h>
+#include <asm/page_types.h>
+#include <asm/vdso.h>
+
+static unsigned char raw_data[4096] __page_aligned_data = {
+	0x7F, 0x45, 0x4C, 0x46, 0x01, 0x01, 0x01, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x00, 0x03, 0x00, 
+	0x01, 0x00, 0x00, 0x00, 0xE0, 0x0B, 0x00, 0x00, 0x34, 0x00, 
+	0x00, 0x00, 0xC8, 0x0C, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x34, 0x00, 0x20, 0x00, 0x04, 0x00, 0x28, 0x00, 0x10, 0x00, 
+	0x0F, 0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x0C, 
+	0x00, 0x00, 0x01, 0x0C, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00, 
+	0x00, 0x10, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x78, 0x02, 
+	0x00, 0x00, 0x78, 0x02, 0x00, 0x00, 0x78, 0x02, 0x00, 0x00, 
+	0x80, 0x00, 0x00, 0x00, 0x80, 0x00, 0x00, 0x00, 0x04, 0x00, 
+	0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 
+	0x20, 0x05, 0x00, 0x00, 0x20, 0x05, 0x00, 0x00, 0x20, 0x05, 
+	0x00, 0x00, 0x60, 0x00, 0x00, 0x00, 0x60, 0x00, 0x00, 0x00, 
+	0x04, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x50, 0xE5, 
+	0x74, 0x64, 0x80, 0x05, 0x00, 0x00, 0x80, 0x05, 0x00, 0x00, 
+	0x80, 0x05, 0x00, 0x00, 0x24, 0x00, 0x00, 0x00, 0x24, 0x00, 
+	0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 
+	0x03, 0x00, 0x00, 0x00, 0x09, 0x00, 0x00, 0x00, 0x08, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 
+	0x04, 0x00, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00, 0x03, 0x00, 
+	0x00, 0x00, 0x07, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x81, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x11, 0x00, 0xF1, 0xFF, 0x16, 0x00, 
+	0x00, 0x00, 0xE0, 0x0B, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 
+	0x12, 0x00, 0x0B, 0x00, 0x28, 0x00, 0x00, 0x00, 0xE0, 0x09, 
+	0x00, 0x00, 0x9A, 0x01, 0x00, 0x00, 0x12, 0x00, 0x0B, 0x00, 
+	0x8B, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x11, 0x00, 0xF1, 0xFF, 0x5B, 0x00, 0x00, 0x00, 
+	0xD0, 0x0B, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 0x12, 0x00, 
+	0x0B, 0x00, 0x01, 0x00, 0x00, 0x00, 0xE0, 0x06, 0x00, 0x00, 
+	0xFC, 0x02, 0x00, 0x00, 0x12, 0x00, 0x0B, 0x00, 0x48, 0x00, 
+	0x00, 0x00, 0xC0, 0x0B, 0x00, 0x00, 0x09, 0x00, 0x00, 0x00, 
+	0x12, 0x00, 0x0B, 0x00, 0x3C, 0x00, 0x00, 0x00, 0x80, 0x0B, 
+	0x00, 0x00, 0x2B, 0x00, 0x00, 0x00, 0x12, 0x00, 0x0B, 0x00, 
+	0x00, 0x5F, 0x5F, 0x76, 0x64, 0x73, 0x6F, 0x5F, 0x63, 0x6C, 
+	0x6F, 0x63, 0x6B, 0x5F, 0x67, 0x65, 0x74, 0x74, 0x69, 0x6D, 
+	0x65, 0x00, 0x5F, 0x5F, 0x6B, 0x65, 0x72, 0x6E, 0x65, 0x6C, 
+	0x5F, 0x76, 0x73, 0x79, 0x73, 0x63, 0x61, 0x6C, 0x6C, 0x00, 
+	0x5F, 0x5F, 0x76, 0x64, 0x73, 0x6F, 0x5F, 0x67, 0x65, 0x74, 
+	0x74, 0x69, 0x6D, 0x65, 0x6F, 0x66, 0x64, 0x61, 0x79, 0x00, 
+	0x5F, 0x5F, 0x76, 0x64, 0x73, 0x6F, 0x5F, 0x74, 0x69, 0x6D, 
+	0x65, 0x00, 0x5F, 0x5F, 0x6B, 0x65, 0x72, 0x6E, 0x65, 0x6C, 
+	0x5F, 0x73, 0x69, 0x67, 0x72, 0x65, 0x74, 0x75, 0x72, 0x6E, 
+	0x00, 0x5F, 0x5F, 0x6B, 0x65, 0x72, 0x6E, 0x65, 0x6C, 0x5F, 
+	0x72, 0x74, 0x5F, 0x73, 0x69, 0x67, 0x72, 0x65, 0x74, 0x75, 
+	0x72, 0x6E, 0x00, 0x6C, 0x69, 0x6E, 0x75, 0x78, 0x2D, 0x67, 
+	0x61, 0x74, 0x65, 0x2E, 0x73, 0x6F, 0x2E, 0x31, 0x00, 0x4C, 
+	0x49, 0x4E, 0x55, 0x58, 0x5F, 0x32, 0x2E, 0x36, 0x00, 0x4C, 
+	0x49, 0x4E, 0x55, 0x58, 0x5F, 0x32, 0x2E, 0x35, 0x00, 0x00, 
+	0x00, 0x00, 0x02, 0x00, 0x03, 0x00, 0x02, 0x00, 0x03, 0x00, 
+	0x03, 0x00, 0x02, 0x00, 0x03, 0x00, 0x02, 0x00, 0x01, 0x00, 
+	0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x91, 0xF0, 0xCE, 0x0F, 
+	0x14, 0x00, 0x00, 0x00, 0x1C, 0x00, 0x00, 0x00, 0x71, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x01, 0x00, 0xF6, 0x75, 0xAE, 0x03, 0x14, 0x00, 
+	0x00, 0x00, 0x1C, 0x00, 0x00, 0x00, 0x81, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x03, 0x00, 
+	0x01, 0x00, 0xF5, 0x75, 0xAE, 0x03, 0x14, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x8B, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x0E, 0x00, 0x00, 0x00, 0x71, 0x00, 0x00, 0x00, 
+	0x10, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 
+	0x00, 0x00, 0xB4, 0x00, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00, 
+	0x7C, 0x01, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 0xEC, 0x00, 
+	0x00, 0x00, 0x0A, 0x00, 0x00, 0x00, 0x95, 0x00, 0x00, 0x00, 
+	0x0B, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0xFC, 0xFF, 
+	0xFF, 0x6F, 0x24, 0x02, 0x00, 0x00, 0xFD, 0xFF, 0xFF, 0x6F, 
+	0x03, 0x00, 0x00, 0x00, 0xF0, 0xFF, 0xFF, 0x6F, 0x12, 0x02, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x78, 0x02, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00, 
+	0x03, 0x00, 0x00, 0x00, 0x47, 0x4E, 0x55, 0x00, 0xC8, 0xED, 
+	0x87, 0x8F, 0x12, 0x6F, 0x2B, 0x44, 0xB2, 0x43, 0x62, 0x13, 
+	0x00, 0x01, 0xAB, 0xA1, 0x4E, 0x5A, 0x97, 0x7E, 0x06, 0x00, 
+	0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x4C, 0x69, 0x6E, 0x75, 0x78, 0x00, 0x00, 0x00, 0x04, 0x11, 
+	0x03, 0x00, 0x04, 0x00, 0x00, 0x00, 0x12, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x00, 0x00, 0x47, 0x4E, 0x55, 0x00, 0x01, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x6E, 0x6F, 0x73, 
+	0x65, 0x67, 0x6E, 0x65, 0x67, 0x00, 0x00, 0x00, 0x01, 0x1B, 
+	0x03, 0x3B, 0x20, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 
+	0x3F, 0x06, 0x00, 0x00, 0x38, 0x00, 0x00, 0x00, 0x4F, 0x06, 
+	0x00, 0x00, 0xA4, 0x00, 0x00, 0x00, 0x60, 0x06, 0x00, 0x00, 
+	0x04, 0x01, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x01, 0x7A, 0x52, 0x53, 0x00, 0x01, 0x7C, 0x08, 
+	0x01, 0x1B, 0x00, 0x00, 0x68, 0x00, 0x00, 0x00, 0x18, 0x00, 
+	0x00, 0x00, 0xFF, 0x05, 0x00, 0x00, 0x09, 0x00, 0x00, 0x00, 
+	0x00, 0x0F, 0x03, 0x74, 0x20, 0x06, 0x10, 0x00, 0x02, 0x74, 
+	0x30, 0x10, 0x01, 0x02, 0x74, 0x2C, 0x10, 0x02, 0x02, 0x74, 
+	0x28, 0x10, 0x03, 0x02, 0x74, 0x24, 0x10, 0x05, 0x02, 0x74, 
+	0x1C, 0x10, 0x06, 0x02, 0x74, 0x18, 0x10, 0x07, 0x02, 0x74, 
+	0x14, 0x10, 0x08, 0x02, 0x74, 0x3C, 0x42, 0x0F, 0x03, 0x74, 
+	0x1C, 0x06, 0x10, 0x00, 0x02, 0x74, 0x2C, 0x10, 0x01, 0x02, 
+	0x74, 0x28, 0x10, 0x02, 0x02, 0x74, 0x24, 0x10, 0x03, 0x02, 
+	0x74, 0x20, 0x10, 0x05, 0x02, 0x74, 0x18, 0x10, 0x06, 0x02, 
+	0x74, 0x14, 0x10, 0x07, 0x02, 0x74, 0x10, 0x10, 0x08, 0x02, 
+	0x74, 0x38, 0x44, 0x00, 0x00, 0x00, 0x84, 0x00, 0x00, 0x00, 
+	0xA3, 0x05, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 0x00, 0x0F, 
+	0x04, 0x74, 0xBC, 0x01, 0x06, 0x10, 0x00, 0x03, 0x74, 0xCC, 
+	0x01, 0x10, 0x01, 0x03, 0x74, 0xC8, 0x01, 0x10, 0x02, 0x03, 
+	0x74, 0xC4, 0x01, 0x10, 0x03, 0x03, 0x74, 0xC0, 0x01, 0x10, 
+	0x05, 0x03, 0x74, 0xB8, 0x01, 0x10, 0x06, 0x03, 0x74, 0xB4, 
+	0x01, 0x10, 0x07, 0x03, 0x74, 0xB0, 0x01, 0x10, 0x08, 0x03, 
+	0x74, 0xD8, 0x01, 0x00, 0x14, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x01, 0x7A, 0x52, 0x00, 0x01, 0x7C, 0x08, 0x01, 
+	0x1B, 0x0C, 0x04, 0x04, 0x88, 0x01, 0x00, 0x00, 0x10, 0x00, 
+	0x00, 0x00, 0x1C, 0x00, 0x00, 0x00, 0x54, 0x05, 0x00, 0x00, 
+	0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x55, 0x89, 0xE5, 0x56, 
+	0x53, 0xE8, 0x05, 0x05, 0x00, 0x00, 0x81, 0xC3, 0x62, 0xFC, 
+	0xFF, 0xFF, 0x8D, 0x76, 0x00, 0x8D, 0x76, 0x00, 0x0F, 0x31, 
+	0x8B, 0x8B, 0x80, 0xDD, 0xFF, 0xFF, 0x8B, 0xB3, 0x7C, 0xDD, 
+	0xFF, 0xFF, 0x39, 0xCA, 0x77, 0x08, 0x73, 0x0E, 0x89, 0xF0, 
+	0x89, 0xCA, 0x66, 0x90, 0x5B, 0x5E, 0x5D, 0xC3, 0x8D, 0x74, 
+	0x26, 0x00, 0x39, 0xF0, 0x72, 0xEE, 0x5B, 0x5E, 0x5D, 0xC3, 
+	0x55, 0x89, 0xE5, 0x57, 0x56, 0x53, 0x8D, 0x35, 0x74, 0xDD, 
+	0xFF, 0xFF, 0xE8, 0xBE, 0x04, 0x00, 0x00, 0x81, 0xC3, 0x1B, 
+	0xFC, 0xFF, 0xFF, 0x83, 0xEC, 0x14, 0x83, 0x7D, 0x08, 0x01, 
+	0x8B, 0x7D, 0x0C, 0x74, 0x75, 0x0F, 0x8E, 0x47, 0x01, 0x00, 
+	0x00, 0x83, 0x7D, 0x08, 0x05, 0x74, 0x41, 0x83, 0x7D, 0x08, 
+	0x06, 0x0F, 0x85, 0x17, 0x01, 0x00, 0x00, 0x8B, 0x04, 0x1E, 
+	0xA8, 0x01, 0x0F, 0x85, 0x7C, 0x02, 0x00, 0x00, 0x8B, 0x54, 
+	0x1E, 0x50, 0x89, 0x17, 0x8B, 0x54, 0x1E, 0x58, 0x89, 0x57, 
+	0x04, 0x39, 0x83, 0x74, 0xDD, 0xFF, 0xFF, 0x75, 0xE0, 0x83, 
+	0xC4, 0x14, 0x31, 0xC0, 0x5B, 0x5E, 0x5F, 0x5D, 0xC3, 0x90, 
+	0x8D, 0x74, 0x26, 0x00, 0xF3, 0x90, 0x8D, 0xB6, 0x00, 0x00, 
+	0x00, 0x00, 0x8B, 0x04, 0x1E, 0xA8, 0x01, 0x75, 0xF1, 0x8B, 
+	0x54, 0x1E, 0x40, 0x89, 0x17, 0x8B, 0x54, 0x1E, 0x48, 0x89, 
+	0x57, 0x04, 0x39, 0x83, 0x74, 0xDD, 0xFF, 0xFF, 0x74, 0xCD, 
+	0xEB, 0xE2, 0x66, 0x90, 0xF3, 0x90, 0x8D, 0xB6, 0x00, 0x00, 
+	0x00, 0x00, 0x8B, 0x3C, 0x1E, 0xF7, 0xC7, 0x01, 0x00, 0x00, 
+	0x00, 0x75, 0xED, 0x8B, 0x44, 0x1E, 0x04, 0x8B, 0x55, 0x0C, 
+	0x89, 0x45, 0xEC, 0x8B, 0x44, 0x1E, 0x30, 0x89, 0x02, 0x8B, 
+	0x4C, 0x1E, 0x04, 0x8B, 0x44, 0x1E, 0x38, 0x8B, 0x54, 0x1E, 
+	0x3C, 0x83, 0xF9, 0x01, 0x89, 0x45, 0xE0, 0x89, 0x55, 0xE4, 
+	0x0F, 0x84, 0xA2, 0x01, 0x00, 0x00, 0x83, 0xF9, 0x02, 0x0F, 
+	0x84, 0xC9, 0x01, 0x00, 0x00, 0x31, 0xC0, 0x31, 0xD2, 0x83, 
+	0xF9, 0x03, 0x0F, 0x84, 0x0C, 0x02, 0x00, 0x00, 0x8B, 0x4C, 
+	0x1E, 0x1C, 0x3B, 0xBB, 0x74, 0xDD, 0xFF, 0xFF, 0x75, 0xA8, 
+	0x03, 0x45, 0xE0, 0x8B, 0x7D, 0x0C, 0x13, 0x55, 0xE4, 0x31, 
+	0xF6, 0x0F, 0xAD, 0xD0, 0xD3, 0xEA, 0xF6, 0xC1, 0x20, 0x0F, 
+	0x45, 0xC2, 0x0F, 0x45, 0xD6, 0x83, 0xFA, 0x00, 0x77, 0x09, 
+	0x31, 0xC9, 0x3D, 0xFF, 0xC9, 0x9A, 0x3B, 0x76, 0x20, 0x31, 
+	0xC9, 0x8D, 0xB4, 0x26, 0x00, 0x00, 0x00, 0x00, 0x05, 0x00, 
+	0x36, 0x65, 0xC4, 0x83, 0xD2, 0xFF, 0x83, 0xC1, 0x01, 0x83, 
+	0xFA, 0x00, 0x77, 0xF0, 0x3D, 0xFF, 0xC9, 0x9A, 0x3B, 0x77, 
+	0xE9, 0x89, 0x47, 0x04, 0x8B, 0x45, 0xEC, 0x01, 0x0F, 0x85, 
+	0xC0, 0x0F, 0x85, 0x12, 0xFF, 0xFF, 0xFF, 0x89, 0xF6, 0x8D, 
+	0xBC, 0x27, 0x00, 0x00, 0x00, 0x00, 0x89, 0xF9, 0xB8, 0x09, 
+	0x01, 0x00, 0x00, 0x89, 0xDA, 0x8B, 0x5D, 0x08, 0xE8, 0x9F, 
+	0x03, 0x00, 0x00, 0x89, 0xD3, 0x83, 0xC4, 0x14, 0x5B, 0x5E, 
+	0x5F, 0x5D, 0xC3, 0x90, 0x8D, 0x74, 0x26, 0x00, 0x8B, 0x45, 
+	0x08, 0x85, 0xC0, 0x75, 0xD9, 0x89, 0x7D, 0x0C, 0x8B, 0x3C, 
+	0x1E, 0xF7, 0xC7, 0x01, 0x00, 0x00, 0x00, 0x0F, 0x85, 0x47, 
+	0x01, 0x00, 0x00, 0x8B, 0x44, 0x1E, 0x04, 0x8B, 0x55, 0x0C, 
+	0x89, 0x45, 0xEC, 0x8B, 0x44, 0x1E, 0x28, 0x89, 0x02, 0x8B, 
+	0x4C, 0x1E, 0x04, 0x8B, 0x44, 0x1E, 0x20, 0x8B, 0x54, 0x1E, 
+	0x24, 0x83, 0xF9, 0x01, 0x89, 0x45, 0xE0, 0x89, 0x55, 0xE4, 
+	0x0F, 0x84, 0x84, 0x00, 0x00, 0x00, 0x83, 0xF9, 0x02, 0x0F, 
+	0x84, 0xF3, 0x00, 0x00, 0x00, 0x31, 0xC0, 0x31, 0xD2, 0x83, 
+	0xF9, 0x03, 0x0F, 0x84, 0x16, 0x01, 0x00, 0x00, 0x8B, 0x4C, 
+	0x1E, 0x1C, 0x3B, 0xBB, 0x74, 0xDD, 0xFF, 0xFF, 0x75, 0xA4, 
+	0x03, 0x45, 0xE0, 0x8B, 0x7D, 0x0C, 0x13, 0x55, 0xE4, 0x31, 
+	0xF6, 0x0F, 0xAD, 0xD0, 0xD3, 0xEA, 0xF6, 0xC1, 0x20, 0x0F, 
+	0x45, 0xC2, 0x0F, 0x45, 0xD6, 0x83, 0xFA, 0x00, 0x77, 0x0D, 
+	0x31, 0xC9, 0x3D, 0xFF, 0xC9, 0x9A, 0x3B, 0x0F, 0x86, 0x36, 
+	0xFF, 0xFF, 0xFF, 0x31, 0xC9, 0x90, 0x8D, 0x74, 0x26, 0x00, 
+	0x05, 0x00, 0x36, 0x65, 0xC4, 0x83, 0xD2, 0xFF, 0x83, 0xC1, 
+	0x01, 0x83, 0xFA, 0x00, 0x77, 0xF0, 0x3D, 0xFF, 0xC9, 0x9A, 
+	0x3B, 0x77, 0xE9, 0x89, 0x47, 0x04, 0x8B, 0x45, 0xEC, 0x01, 
+	0x0F, 0x85, 0xC0, 0x0F, 0x84, 0x21, 0xFF, 0xFF, 0xFF, 0xE9, 
+	0x25, 0xFE, 0xFF, 0xFF, 0x8D, 0x74, 0x26, 0x00, 0xE8, 0x83, 
+	0xFD, 0xFF, 0xFF, 0x2B, 0x44, 0x1E, 0x08, 0x8B, 0x4C, 0x1E, 
+	0x10, 0x1B, 0x54, 0x1E, 0x0C, 0x21, 0xC1, 0x8B, 0x44, 0x1E, 
+	0x14, 0x89, 0x4D, 0xE8, 0x21, 0xD0, 0x89, 0xC1, 0x8B, 0x44, 
+	0x1E, 0x18, 0x0F, 0xAF, 0xC8, 0xF7, 0x65, 0xE8, 0x01, 0xCA, 
+	0xE9, 0x63, 0xFF, 0xFF, 0xFF, 0x89, 0xF6, 0x8D, 0xBC, 0x27, 
+	0x00, 0x00, 0x00, 0x00, 0xE8, 0x4B, 0xFD, 0xFF, 0xFF, 0x2B, 
+	0x44, 0x1E, 0x08, 0x8B, 0x4C, 0x1E, 0x10, 0x1B, 0x54, 0x1E, 
+	0x0C, 0x21, 0xC1, 0x8B, 0x44, 0x1E, 0x14, 0x89, 0x4D, 0xE8, 
+	0x21, 0xD0, 0x89, 0xC1, 0x8B, 0x44, 0x1E, 0x18, 0x0F, 0xAF, 
+	0xC8, 0xF7, 0x65, 0xE8, 0x01, 0xCA, 0xE9, 0x45, 0xFE, 0xFF, 
+	0xFF, 0x90, 0x8B, 0x83, 0xE4, 0xED, 0xFF, 0xFF, 0x31, 0xD2, 
+	0xEB, 0xCB, 0x8D, 0xB6, 0x00, 0x00, 0x00, 0x00, 0x8B, 0x83, 
+	0xE4, 0xED, 0xFF, 0xFF, 0x31, 0xD2, 0xEB, 0x83, 0x8D, 0xB6, 
+	0x00, 0x00, 0x00, 0x00, 0xF3, 0x90, 0xE9, 0x72, 0xFD, 0xFF, 
+	0xFF, 0x89, 0xF6, 0x8D, 0xBC, 0x27, 0x00, 0x00, 0x00, 0x00, 
+	0xF3, 0x90, 0xE9, 0xA3, 0xFE, 0xFF, 0xFF, 0x89, 0xF6, 0x8D, 
+	0xBC, 0x27, 0x00, 0x00, 0x00, 0x00, 0xC7, 0x45, 0xEC, 0x00, 
+	0x00, 0x00, 0x00, 0xE9, 0x51, 0xFF, 0xFF, 0xFF, 0x8D, 0x74, 
+	0x26, 0x00, 0xC7, 0x45, 0xEC, 0x00, 0x00, 0x00, 0x00, 0xE9, 
+	0x79, 0xFF, 0xFF, 0xFF, 0x8D, 0x74, 0x26, 0x00, 0x55, 0x89, 
+	0xE5, 0x57, 0x56, 0x53, 0x8D, 0x35, 0x74, 0xDD, 0xFF, 0xFF, 
+	0xE8, 0xBE, 0x01, 0x00, 0x00, 0x81, 0xC3, 0x1B, 0xF9, 0xFF, 
+	0xFF, 0x83, 0xEC, 0x14, 0x8B, 0x7D, 0x08, 0x85, 0xFF, 0x0F, 
+	0x84, 0xBE, 0x00, 0x00, 0x00, 0x8B, 0x3C, 0x1E, 0xF7, 0xC7, 
+	0x01, 0x00, 0x00, 0x00, 0x0F, 0x85, 0x1C, 0x01, 0x00, 0x00, 
+	0x8B, 0x44, 0x1E, 0x04, 0x8B, 0x55, 0x08, 0x89, 0x45, 0xEC, 
+	0x8B, 0x44, 0x1E, 0x28, 0x89, 0x02, 0x8B, 0x4C, 0x1E, 0x04, 
+	0x8B, 0x44, 0x1E, 0x20, 0x8B, 0x54, 0x1E, 0x24, 0x83, 0xF9, 
+	0x01, 0x89, 0x45, 0xE0, 0x89, 0x55, 0xE4, 0x0F, 0x84, 0xA9, 
+	0x00, 0x00, 0x00, 0x83, 0xF9, 0x02, 0x0F, 0x84, 0xD8, 0x00, 
+	0x00, 0x00, 0x31, 0xC0, 0x31, 0xD2, 0x83, 0xF9, 0x03, 0x0F, 
+	0x84, 0xEB, 0x00, 0x00, 0x00, 0x8B, 0x4C, 0x1E, 0x1C, 0x3B, 
+	0xBB, 0x74, 0xDD, 0xFF, 0xFF, 0x75, 0xA4, 0x03, 0x45, 0xE0, 
+	0x8B, 0x7D, 0x08, 0x13, 0x55, 0xE4, 0x31, 0xF6, 0x0F, 0xAD, 
+	0xD0, 0xD3, 0xEA, 0xF6, 0xC1, 0x20, 0x0F, 0x45, 0xC2, 0x0F, 
+	0x45, 0xD6, 0x83, 0xFA, 0x00, 0x76, 0x59, 0x31, 0xC9, 0x8D, 
+	0xB4, 0x26, 0x00, 0x00, 0x00, 0x00, 0x05, 0x00, 0x36, 0x65, 
+	0xC4, 0x83, 0xD2, 0xFF, 0x83, 0xC1, 0x01, 0x83, 0xFA, 0x00, 
+	0x77, 0xF0, 0x3D, 0xFF, 0xC9, 0x9A, 0x3B, 0x77, 0xE9, 0x8B, 
+	0x75, 0xEC, 0x01, 0x0F, 0x89, 0xC1, 0x89, 0x47, 0x04, 0x85, 
+	0xF6, 0x0F, 0x84, 0xB1, 0x00, 0x00, 0x00, 0xBA, 0xD3, 0x4D, 
+	0x62, 0x10, 0xC1, 0xF9, 0x1F, 0xF7, 0xEA, 0xC1, 0xFA, 0x06, 
+	0x29, 0xCA, 0x89, 0x57, 0x04, 0x8B, 0x55, 0x0C, 0x31, 0xC0, 
+	0x85, 0xD2, 0x75, 0x7D, 0x83, 0xC4, 0x14, 0x5B, 0x5E, 0x5F, 
+	0x5D, 0xC3, 0x8D, 0x74, 0x26, 0x00, 0x31, 0xC9, 0x3D, 0xFF, 
+	0xC9, 0x9A, 0x3B, 0x76, 0xBE, 0xEB, 0x9C, 0x90, 0x8D, 0x74, 
+	0x26, 0x00, 0xE8, 0xB3, 0xFB, 0xFF, 0xFF, 0x2B, 0x44, 0x1E, 
+	0x08, 0x8B, 0x4C, 0x1E, 0x10, 0x1B, 0x54, 0x1E, 0x0C, 0x21, 
+	0xC1, 0x8B, 0x44, 0x1E, 0x14, 0x89, 0x4D, 0xE8, 0x21, 0xD0, 
+	0x89, 0xC1, 0x8B, 0x44, 0x1E, 0x18, 0x0F, 0xAF, 0xC8, 0xF7, 
+	0x65, 0xE8, 0x01, 0xCA, 0xE9, 0x3E, 0xFF, 0xFF, 0xFF, 0x89, 
+	0xF6, 0x8D, 0xBC, 0x27, 0x00, 0x00, 0x00, 0x00, 0x8B, 0x83, 
+	0xE4, 0xED, 0xFF, 0xFF, 0x31, 0xD2, 0xEB, 0xC3, 0x8D, 0xB6, 
+	0x00, 0x00, 0x00, 0x00, 0xF3, 0x90, 0xE9, 0xCE, 0xFE, 0xFF, 
+	0xFF, 0x89, 0xF6, 0x8D, 0xBC, 0x27, 0x00, 0x00, 0x00, 0x00, 
+	0xC7, 0x45, 0xEC, 0x00, 0x00, 0x00, 0x00, 0xEB, 0xA4, 0x8B, 
+	0x93, 0xD4, 0xDD, 0xFF, 0xFF, 0x8B, 0x75, 0x0C, 0x89, 0x16, 
+	0x8B, 0x93, 0xD8, 0xDD, 0xFF, 0xFF, 0x89, 0x56, 0x04, 0xE9, 
+	0x6A, 0xFF, 0xFF, 0xFF, 0xB8, 0x4E, 0x00, 0x00, 0x00, 0x8B, 
+	0x4D, 0x0C, 0x89, 0xDA, 0x89, 0xFB, 0xE8, 0x6D, 0x00, 0x00, 
+	0x00, 0x89, 0xD3, 0xE9, 0x52, 0xFF, 0xFF, 0xFF, 0x8D, 0xB6, 
+	0x00, 0x00, 0x00, 0x00, 0x55, 0xE8, 0x25, 0x00, 0x00, 0x00, 
+	0x81, 0xC1, 0x86, 0xF7, 0xFF, 0xFF, 0x89, 0xE5, 0x57, 0x56, 
+	0x8B, 0x55, 0x08, 0x8B, 0xB1, 0x9C, 0xDD, 0xFF, 0xFF, 0x8B, 
+	0xB9, 0xA0, 0xDD, 0xFF, 0xFF, 0x85, 0xD2, 0x89, 0xF0, 0x74, 
+	0x02, 0x89, 0x32, 0x5E, 0x5F, 0x5D, 0xC3, 0x8B, 0x0C, 0x24, 
+	0xC3, 0x8B, 0x1C, 0x24, 0xC3, 0x90, 0x90, 0x90, 0x90, 0x90, 
+	0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x58, 0xB8, 
+	0x77, 0x00, 0x00, 0x00, 0xCD, 0x80, 0x90, 0x8D, 0xB4, 0x26, 
+	0x00, 0x00, 0x00, 0x00, 0xB8, 0xAD, 0x00, 0x00, 0x00, 0xCD, 
+	0x80, 0x90, 0x90, 0x8D, 0xB4, 0x26, 0x00, 0x00, 0x00, 0x00, 
+	0xCD, 0x80, 0xC3, 0xCD, 0xFA, 0xFF, 0xFF, 0x14, 0x00, 0x00, 
+	0x00, 0x71, 0x00, 0x03, 0x03, 0xC4, 0xFA, 0xFF, 0xFF, 0x0B, 
+	0x00, 0x00, 0x00, 0x72, 0x00, 0x03, 0x03, 0x0F, 0xAE, 0xF0, 
+	0x0F, 0xAE, 0xE8, 0x47, 0x43, 0x43, 0x3A, 0x20, 0x28, 0x55, 
+	0x62, 0x75, 0x6E, 0x74, 0x75, 0x20, 0x34, 0x2E, 0x39, 0x2E, 
+	0x31, 0x2D, 0x31, 0x36, 0x75, 0x62, 0x75, 0x6E, 0x74, 0x75, 
+	0x36, 0x29, 0x20, 0x34, 0x2E, 0x39, 0x2E, 0x31, 0x00, 0x00, 
+	0x2E, 0x73, 0x68, 0x73, 0x74, 0x72, 0x74, 0x61, 0x62, 0x00, 
+	0x2E, 0x68, 0x61, 0x73, 0x68, 0x00, 0x2E, 0x64, 0x79, 0x6E, 
+	0x73, 0x79, 0x6D, 0x00, 0x2E, 0x64, 0x79, 0x6E, 0x73, 0x74, 
+	0x72, 0x00, 0x2E, 0x67, 0x6E, 0x75, 0x2E, 0x76, 0x65, 0x72, 
+	0x73, 0x69, 0x6F, 0x6E, 0x00, 0x2E, 0x67, 0x6E, 0x75, 0x2E, 
+	0x76, 0x65, 0x72, 0x73, 0x69, 0x6F, 0x6E, 0x5F, 0x64, 0x00, 
+	0x2E, 0x64, 0x79, 0x6E, 0x61, 0x6D, 0x69, 0x63, 0x00, 0x2E, 
+	0x72, 0x6F, 0x64, 0x61, 0x74, 0x61, 0x00, 0x2E, 0x6E, 0x6F, 
+	0x74, 0x65, 0x00, 0x2E, 0x65, 0x68, 0x5F, 0x66, 0x72, 0x61, 
+	0x6D, 0x65, 0x5F, 0x68, 0x64, 0x72, 0x00, 0x2E, 0x65, 0x68, 
+	0x5F, 0x66, 0x72, 0x61, 0x6D, 0x65, 0x00, 0x2E, 0x74, 0x65, 
+	0x78, 0x74, 0x00, 0x2E, 0x61, 0x6C, 0x74, 0x69, 0x6E, 0x73, 
+	0x74, 0x72, 0x75, 0x63, 0x74, 0x69, 0x6F, 0x6E, 0x73, 0x00, 
+	0x2E, 0x61, 0x6C, 0x74, 0x69, 0x6E, 0x73, 0x74, 0x72, 0x5F, 
+	0x72, 0x65, 0x70, 0x6C, 0x61, 0x63, 0x65, 0x6D, 0x65, 0x6E, 
+	0x74, 0x00, 0x2E, 0x63, 0x6F, 0x6D, 0x6D, 0x65, 0x6E, 0x74, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x0B, 0x00, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x00, 0x00, 0xB4, 0x00, 0x00, 0x00, 0xB4, 0x00, 
+	0x00, 0x00, 0x38, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x04, 0x00, 
+	0x00, 0x00, 0x11, 0x00, 0x00, 0x00, 0x0B, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x00, 0x00, 0xEC, 0x00, 0x00, 0x00, 0xEC, 0x00, 
+	0x00, 0x00, 0x90, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 
+	0x01, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x10, 0x00, 
+	0x00, 0x00, 0x19, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x00, 0x00, 0x7C, 0x01, 0x00, 0x00, 0x7C, 0x01, 
+	0x00, 0x00, 0x95, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x21, 0x00, 0x00, 0x00, 0xFF, 0xFF, 0xFF, 0x6F, 
+	0x02, 0x00, 0x00, 0x00, 0x12, 0x02, 0x00, 0x00, 0x12, 0x02, 
+	0x00, 0x00, 0x12, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x02, 0x00, 
+	0x00, 0x00, 0x2E, 0x00, 0x00, 0x00, 0xFD, 0xFF, 0xFF, 0x6F, 
+	0x02, 0x00, 0x00, 0x00, 0x24, 0x02, 0x00, 0x00, 0x24, 0x02, 
+	0x00, 0x00, 0x54, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 
+	0x03, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x3D, 0x00, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 
+	0x03, 0x00, 0x00, 0x00, 0x78, 0x02, 0x00, 0x00, 0x78, 0x02, 
+	0x00, 0x00, 0x80, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x08, 0x00, 
+	0x00, 0x00, 0x46, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x03, 0x00, 0x00, 0x00, 0xF8, 0x02, 0x00, 0x00, 0xF8, 0x02, 
+	0x00, 0x00, 0x28, 0x02, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x04, 0x00, 
+	0x00, 0x00, 0x4E, 0x00, 0x00, 0x00, 0x07, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x00, 0x00, 0x20, 0x05, 0x00, 0x00, 0x20, 0x05, 
+	0x00, 0x00, 0x60, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x54, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x00, 0x00, 0x80, 0x05, 0x00, 0x00, 0x80, 0x05, 
+	0x00, 0x00, 0x24, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x62, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x00, 0x00, 0xA4, 0x05, 0x00, 0x00, 0xA4, 0x05, 
+	0x00, 0x00, 0xF4, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x6C, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x06, 0x00, 0x00, 0x00, 0xA0, 0x06, 0x00, 0x00, 0xA0, 0x06, 
+	0x00, 0x00, 0x43, 0x05, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x72, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x00, 0x00, 0xE3, 0x0B, 0x00, 0x00, 0xE3, 0x0B, 
+	0x00, 0x00, 0x18, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x83, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x06, 0x00, 0x00, 0x00, 0xFB, 0x0B, 0x00, 0x00, 0xFB, 0x0B, 
+	0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x99, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x30, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x0C, 
+	0x00, 0x00, 0x24, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01, 0x00, 
+	0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x25, 0x0C, 
+	0x00, 0x00, 0xA2, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 
+};
+
+static struct page *pages[1];
+
+const struct vdso_image vdso_image_32_int80 = {
+	.data = raw_data,
+	.size = 4096,
+	.text_mapping = {
+		.name = "[vdso]",
+		.pages = pages,
+	},
+	.alt = 3043,
+	.alt_len = 24,
+	.sym_vvar_start = -8192,
+	.sym_vvar_page = -8192,
+	.sym_hpet_page = -4096,
+	.sym_VDSO32_NOTE_MASK = 1392,
+	.sym___kernel_vsyscall = 3040,
+	.sym___kernel_sigreturn = 3008,
+	.sym___kernel_rt_sigreturn = 3024,
+};
diff -urN a/arch/x86/vdso/vdso-image-32-syscall.c b/arch/x86/vdso/vdso-image-32-syscall.c
--- a/arch/x86/vdso/vdso-image-32-syscall.c	1969-12-31 17:00:00.000000000 -0700
+++ b/arch/x86/vdso/vdso-image-32-syscall.c	2014-11-22 18:41:11.689004287 -0700
@@ -0,0 +1,421 @@
+/* AUTOMATICALLY GENERATED -- DO NOT EDIT */
+
+#include <linux/linkage.h>
+#include <asm/page_types.h>
+#include <asm/vdso.h>
+
+static unsigned char raw_data[4096] __page_aligned_data = {
+	0x7F, 0x45, 0x4C, 0x46, 0x01, 0x01, 0x01, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x00, 0x03, 0x00, 
+	0x01, 0x00, 0x00, 0x00, 0xE0, 0x0B, 0x00, 0x00, 0x34, 0x00, 
+	0x00, 0x00, 0xD4, 0x0C, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x34, 0x00, 0x20, 0x00, 0x04, 0x00, 0x28, 0x00, 0x10, 0x00, 
+	0x0F, 0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0E, 0x0C, 
+	0x00, 0x00, 0x0E, 0x0C, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00, 
+	0x00, 0x10, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x78, 0x02, 
+	0x00, 0x00, 0x78, 0x02, 0x00, 0x00, 0x78, 0x02, 0x00, 0x00, 
+	0x80, 0x00, 0x00, 0x00, 0x80, 0x00, 0x00, 0x00, 0x04, 0x00, 
+	0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 
+	0x18, 0x05, 0x00, 0x00, 0x18, 0x05, 0x00, 0x00, 0x18, 0x05, 
+	0x00, 0x00, 0x60, 0x00, 0x00, 0x00, 0x60, 0x00, 0x00, 0x00, 
+	0x04, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x50, 0xE5, 
+	0x74, 0x64, 0x78, 0x05, 0x00, 0x00, 0x78, 0x05, 0x00, 0x00, 
+	0x78, 0x05, 0x00, 0x00, 0x24, 0x00, 0x00, 0x00, 0x24, 0x00, 
+	0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 
+	0x03, 0x00, 0x00, 0x00, 0x09, 0x00, 0x00, 0x00, 0x08, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 
+	0x04, 0x00, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00, 0x03, 0x00, 
+	0x00, 0x00, 0x07, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x81, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x11, 0x00, 0xF1, 0xFF, 0x16, 0x00, 
+	0x00, 0x00, 0xE0, 0x0B, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 
+	0x12, 0x00, 0x0B, 0x00, 0x28, 0x00, 0x00, 0x00, 0xE0, 0x09, 
+	0x00, 0x00, 0x9A, 0x01, 0x00, 0x00, 0x12, 0x00, 0x0B, 0x00, 
+	0x8B, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x11, 0x00, 0xF1, 0xFF, 0x5B, 0x00, 0x00, 0x00, 
+	0xD0, 0x0B, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 0x12, 0x00, 
+	0x0B, 0x00, 0x01, 0x00, 0x00, 0x00, 0xE0, 0x06, 0x00, 0x00, 
+	0xFC, 0x02, 0x00, 0x00, 0x12, 0x00, 0x0B, 0x00, 0x48, 0x00, 
+	0x00, 0x00, 0xC0, 0x0B, 0x00, 0x00, 0x09, 0x00, 0x00, 0x00, 
+	0x12, 0x00, 0x0B, 0x00, 0x3C, 0x00, 0x00, 0x00, 0x80, 0x0B, 
+	0x00, 0x00, 0x2B, 0x00, 0x00, 0x00, 0x12, 0x00, 0x0B, 0x00, 
+	0x00, 0x5F, 0x5F, 0x76, 0x64, 0x73, 0x6F, 0x5F, 0x63, 0x6C, 
+	0x6F, 0x63, 0x6B, 0x5F, 0x67, 0x65, 0x74, 0x74, 0x69, 0x6D, 
+	0x65, 0x00, 0x5F, 0x5F, 0x6B, 0x65, 0x72, 0x6E, 0x65, 0x6C, 
+	0x5F, 0x76, 0x73, 0x79, 0x73, 0x63, 0x61, 0x6C, 0x6C, 0x00, 
+	0x5F, 0x5F, 0x76, 0x64, 0x73, 0x6F, 0x5F, 0x67, 0x65, 0x74, 
+	0x74, 0x69, 0x6D, 0x65, 0x6F, 0x66, 0x64, 0x61, 0x79, 0x00, 
+	0x5F, 0x5F, 0x76, 0x64, 0x73, 0x6F, 0x5F, 0x74, 0x69, 0x6D, 
+	0x65, 0x00, 0x5F, 0x5F, 0x6B, 0x65, 0x72, 0x6E, 0x65, 0x6C, 
+	0x5F, 0x73, 0x69, 0x67, 0x72, 0x65, 0x74, 0x75, 0x72, 0x6E, 
+	0x00, 0x5F, 0x5F, 0x6B, 0x65, 0x72, 0x6E, 0x65, 0x6C, 0x5F, 
+	0x72, 0x74, 0x5F, 0x73, 0x69, 0x67, 0x72, 0x65, 0x74, 0x75, 
+	0x72, 0x6E, 0x00, 0x6C, 0x69, 0x6E, 0x75, 0x78, 0x2D, 0x67, 
+	0x61, 0x74, 0x65, 0x2E, 0x73, 0x6F, 0x2E, 0x31, 0x00, 0x4C, 
+	0x49, 0x4E, 0x55, 0x58, 0x5F, 0x32, 0x2E, 0x36, 0x00, 0x4C, 
+	0x49, 0x4E, 0x55, 0x58, 0x5F, 0x32, 0x2E, 0x35, 0x00, 0x00, 
+	0x00, 0x00, 0x02, 0x00, 0x03, 0x00, 0x02, 0x00, 0x03, 0x00, 
+	0x03, 0x00, 0x02, 0x00, 0x03, 0x00, 0x02, 0x00, 0x01, 0x00, 
+	0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x91, 0xF0, 0xCE, 0x0F, 
+	0x14, 0x00, 0x00, 0x00, 0x1C, 0x00, 0x00, 0x00, 0x71, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x01, 0x00, 0xF6, 0x75, 0xAE, 0x03, 0x14, 0x00, 
+	0x00, 0x00, 0x1C, 0x00, 0x00, 0x00, 0x81, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x03, 0x00, 
+	0x01, 0x00, 0xF5, 0x75, 0xAE, 0x03, 0x14, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x8B, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x0E, 0x00, 0x00, 0x00, 0x71, 0x00, 0x00, 0x00, 
+	0x10, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 
+	0x00, 0x00, 0xB4, 0x00, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00, 
+	0x7C, 0x01, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 0xEC, 0x00, 
+	0x00, 0x00, 0x0A, 0x00, 0x00, 0x00, 0x95, 0x00, 0x00, 0x00, 
+	0x0B, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0xFC, 0xFF, 
+	0xFF, 0x6F, 0x24, 0x02, 0x00, 0x00, 0xFD, 0xFF, 0xFF, 0x6F, 
+	0x03, 0x00, 0x00, 0x00, 0xF0, 0xFF, 0xFF, 0x6F, 0x12, 0x02, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x78, 0x02, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x14, 0x00, 
+	0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x47, 0x4E, 0x55, 0x00, 
+	0x3E, 0xE1, 0x6E, 0xFB, 0xF3, 0x10, 0x57, 0x87, 0xA8, 0xB7, 
+	0x89, 0x71, 0x6A, 0x53, 0x02, 0xB8, 0x4D, 0xBB, 0xA0, 0x93, 
+	0x06, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x4C, 0x69, 0x6E, 0x75, 0x78, 0x00, 0x00, 0x00, 
+	0x04, 0x11, 0x03, 0x00, 0x04, 0x00, 0x00, 0x00, 0x12, 0x00, 
+	0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x47, 0x4E, 0x55, 0x00, 
+	0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x6E, 
+	0x6F, 0x73, 0x65, 0x67, 0x6E, 0x65, 0x67, 0x00, 0x00, 0x00, 
+	0x01, 0x1B, 0x03, 0x3B, 0x20, 0x00, 0x00, 0x00, 0x03, 0x00, 
+	0x00, 0x00, 0x47, 0x06, 0x00, 0x00, 0x38, 0x00, 0x00, 0x00, 
+	0x57, 0x06, 0x00, 0x00, 0xA4, 0x00, 0x00, 0x00, 0x68, 0x06, 
+	0x00, 0x00, 0x04, 0x01, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x01, 0x7A, 0x52, 0x53, 0x00, 0x01, 
+	0x7C, 0x08, 0x01, 0x1B, 0x00, 0x00, 0x68, 0x00, 0x00, 0x00, 
+	0x18, 0x00, 0x00, 0x00, 0x07, 0x06, 0x00, 0x00, 0x09, 0x00, 
+	0x00, 0x00, 0x00, 0x0F, 0x03, 0x74, 0x20, 0x06, 0x10, 0x00, 
+	0x02, 0x74, 0x30, 0x10, 0x01, 0x02, 0x74, 0x2C, 0x10, 0x02, 
+	0x02, 0x74, 0x28, 0x10, 0x03, 0x02, 0x74, 0x24, 0x10, 0x05, 
+	0x02, 0x74, 0x1C, 0x10, 0x06, 0x02, 0x74, 0x18, 0x10, 0x07, 
+	0x02, 0x74, 0x14, 0x10, 0x08, 0x02, 0x74, 0x3C, 0x42, 0x0F, 
+	0x03, 0x74, 0x1C, 0x06, 0x10, 0x00, 0x02, 0x74, 0x2C, 0x10, 
+	0x01, 0x02, 0x74, 0x28, 0x10, 0x02, 0x02, 0x74, 0x24, 0x10, 
+	0x03, 0x02, 0x74, 0x20, 0x10, 0x05, 0x02, 0x74, 0x18, 0x10, 
+	0x06, 0x02, 0x74, 0x14, 0x10, 0x07, 0x02, 0x74, 0x10, 0x10, 
+	0x08, 0x02, 0x74, 0x38, 0x44, 0x00, 0x00, 0x00, 0x84, 0x00, 
+	0x00, 0x00, 0xAB, 0x05, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 
+	0x00, 0x0F, 0x04, 0x74, 0xBC, 0x01, 0x06, 0x10, 0x00, 0x03, 
+	0x74, 0xCC, 0x01, 0x10, 0x01, 0x03, 0x74, 0xC8, 0x01, 0x10, 
+	0x02, 0x03, 0x74, 0xC4, 0x01, 0x10, 0x03, 0x03, 0x74, 0xC0, 
+	0x01, 0x10, 0x05, 0x03, 0x74, 0xB8, 0x01, 0x10, 0x06, 0x03, 
+	0x74, 0xB4, 0x01, 0x10, 0x07, 0x03, 0x74, 0xB0, 0x01, 0x10, 
+	0x08, 0x03, 0x74, 0xD8, 0x01, 0x00, 0x14, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x01, 0x7A, 0x52, 0x00, 0x01, 0x7C, 
+	0x08, 0x01, 0x1B, 0x0C, 0x04, 0x04, 0x88, 0x01, 0x00, 0x00, 
+	0x18, 0x00, 0x00, 0x00, 0x1C, 0x00, 0x00, 0x00, 0x5C, 0x05, 
+	0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x00, 0x41, 0x0E, 0x08, 
+	0x85, 0x02, 0x4E, 0xC5, 0x0E, 0x04, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x55, 0x89, 0xE5, 0x56, 
+	0x53, 0xE8, 0x05, 0x05, 0x00, 0x00, 0x81, 0xC3, 0x5A, 0xFC, 
+	0xFF, 0xFF, 0x8D, 0x76, 0x00, 0x8D, 0x76, 0x00, 0x0F, 0x31, 
+	0x8B, 0x8B, 0x88, 0xDD, 0xFF, 0xFF, 0x8B, 0xB3, 0x84, 0xDD, 
+	0xFF, 0xFF, 0x39, 0xCA, 0x77, 0x08, 0x73, 0x0E, 0x89, 0xF0, 
+	0x89, 0xCA, 0x66, 0x90, 0x5B, 0x5E, 0x5D, 0xC3, 0x8D, 0x74, 
+	0x26, 0x00, 0x39, 0xF0, 0x72, 0xEE, 0x5B, 0x5E, 0x5D, 0xC3, 
+	0x55, 0x89, 0xE5, 0x57, 0x56, 0x53, 0x8D, 0x35, 0x7C, 0xDD, 
+	0xFF, 0xFF, 0xE8, 0xBE, 0x04, 0x00, 0x00, 0x81, 0xC3, 0x13, 
+	0xFC, 0xFF, 0xFF, 0x83, 0xEC, 0x14, 0x83, 0x7D, 0x08, 0x01, 
+	0x8B, 0x7D, 0x0C, 0x74, 0x75, 0x0F, 0x8E, 0x47, 0x01, 0x00, 
+	0x00, 0x83, 0x7D, 0x08, 0x05, 0x74, 0x41, 0x83, 0x7D, 0x08, 
+	0x06, 0x0F, 0x85, 0x17, 0x01, 0x00, 0x00, 0x8B, 0x04, 0x1E, 
+	0xA8, 0x01, 0x0F, 0x85, 0x7C, 0x02, 0x00, 0x00, 0x8B, 0x54, 
+	0x1E, 0x50, 0x89, 0x17, 0x8B, 0x54, 0x1E, 0x58, 0x89, 0x57, 
+	0x04, 0x39, 0x83, 0x7C, 0xDD, 0xFF, 0xFF, 0x75, 0xE0, 0x83, 
+	0xC4, 0x14, 0x31, 0xC0, 0x5B, 0x5E, 0x5F, 0x5D, 0xC3, 0x90, 
+	0x8D, 0x74, 0x26, 0x00, 0xF3, 0x90, 0x8D, 0xB6, 0x00, 0x00, 
+	0x00, 0x00, 0x8B, 0x04, 0x1E, 0xA8, 0x01, 0x75, 0xF1, 0x8B, 
+	0x54, 0x1E, 0x40, 0x89, 0x17, 0x8B, 0x54, 0x1E, 0x48, 0x89, 
+	0x57, 0x04, 0x39, 0x83, 0x7C, 0xDD, 0xFF, 0xFF, 0x74, 0xCD, 
+	0xEB, 0xE2, 0x66, 0x90, 0xF3, 0x90, 0x8D, 0xB6, 0x00, 0x00, 
+	0x00, 0x00, 0x8B, 0x3C, 0x1E, 0xF7, 0xC7, 0x01, 0x00, 0x00, 
+	0x00, 0x75, 0xED, 0x8B, 0x44, 0x1E, 0x04, 0x8B, 0x55, 0x0C, 
+	0x89, 0x45, 0xEC, 0x8B, 0x44, 0x1E, 0x30, 0x89, 0x02, 0x8B, 
+	0x4C, 0x1E, 0x04, 0x8B, 0x44, 0x1E, 0x38, 0x8B, 0x54, 0x1E, 
+	0x3C, 0x83, 0xF9, 0x01, 0x89, 0x45, 0xE0, 0x89, 0x55, 0xE4, 
+	0x0F, 0x84, 0xA2, 0x01, 0x00, 0x00, 0x83, 0xF9, 0x02, 0x0F, 
+	0x84, 0xC9, 0x01, 0x00, 0x00, 0x31, 0xC0, 0x31, 0xD2, 0x83, 
+	0xF9, 0x03, 0x0F, 0x84, 0x0C, 0x02, 0x00, 0x00, 0x8B, 0x4C, 
+	0x1E, 0x1C, 0x3B, 0xBB, 0x7C, 0xDD, 0xFF, 0xFF, 0x75, 0xA8, 
+	0x03, 0x45, 0xE0, 0x8B, 0x7D, 0x0C, 0x13, 0x55, 0xE4, 0x31, 
+	0xF6, 0x0F, 0xAD, 0xD0, 0xD3, 0xEA, 0xF6, 0xC1, 0x20, 0x0F, 
+	0x45, 0xC2, 0x0F, 0x45, 0xD6, 0x83, 0xFA, 0x00, 0x77, 0x09, 
+	0x31, 0xC9, 0x3D, 0xFF, 0xC9, 0x9A, 0x3B, 0x76, 0x20, 0x31, 
+	0xC9, 0x8D, 0xB4, 0x26, 0x00, 0x00, 0x00, 0x00, 0x05, 0x00, 
+	0x36, 0x65, 0xC4, 0x83, 0xD2, 0xFF, 0x83, 0xC1, 0x01, 0x83, 
+	0xFA, 0x00, 0x77, 0xF0, 0x3D, 0xFF, 0xC9, 0x9A, 0x3B, 0x77, 
+	0xE9, 0x89, 0x47, 0x04, 0x8B, 0x45, 0xEC, 0x01, 0x0F, 0x85, 
+	0xC0, 0x0F, 0x85, 0x12, 0xFF, 0xFF, 0xFF, 0x89, 0xF6, 0x8D, 
+	0xBC, 0x27, 0x00, 0x00, 0x00, 0x00, 0x89, 0xF9, 0xB8, 0x09, 
+	0x01, 0x00, 0x00, 0x89, 0xDA, 0x8B, 0x5D, 0x08, 0xE8, 0x9F, 
+	0x03, 0x00, 0x00, 0x89, 0xD3, 0x83, 0xC4, 0x14, 0x5B, 0x5E, 
+	0x5F, 0x5D, 0xC3, 0x90, 0x8D, 0x74, 0x26, 0x00, 0x8B, 0x45, 
+	0x08, 0x85, 0xC0, 0x75, 0xD9, 0x89, 0x7D, 0x0C, 0x8B, 0x3C, 
+	0x1E, 0xF7, 0xC7, 0x01, 0x00, 0x00, 0x00, 0x0F, 0x85, 0x47, 
+	0x01, 0x00, 0x00, 0x8B, 0x44, 0x1E, 0x04, 0x8B, 0x55, 0x0C, 
+	0x89, 0x45, 0xEC, 0x8B, 0x44, 0x1E, 0x28, 0x89, 0x02, 0x8B, 
+	0x4C, 0x1E, 0x04, 0x8B, 0x44, 0x1E, 0x20, 0x8B, 0x54, 0x1E, 
+	0x24, 0x83, 0xF9, 0x01, 0x89, 0x45, 0xE0, 0x89, 0x55, 0xE4, 
+	0x0F, 0x84, 0x84, 0x00, 0x00, 0x00, 0x83, 0xF9, 0x02, 0x0F, 
+	0x84, 0xF3, 0x00, 0x00, 0x00, 0x31, 0xC0, 0x31, 0xD2, 0x83, 
+	0xF9, 0x03, 0x0F, 0x84, 0x16, 0x01, 0x00, 0x00, 0x8B, 0x4C, 
+	0x1E, 0x1C, 0x3B, 0xBB, 0x7C, 0xDD, 0xFF, 0xFF, 0x75, 0xA4, 
+	0x03, 0x45, 0xE0, 0x8B, 0x7D, 0x0C, 0x13, 0x55, 0xE4, 0x31, 
+	0xF6, 0x0F, 0xAD, 0xD0, 0xD3, 0xEA, 0xF6, 0xC1, 0x20, 0x0F, 
+	0x45, 0xC2, 0x0F, 0x45, 0xD6, 0x83, 0xFA, 0x00, 0x77, 0x0D, 
+	0x31, 0xC9, 0x3D, 0xFF, 0xC9, 0x9A, 0x3B, 0x0F, 0x86, 0x36, 
+	0xFF, 0xFF, 0xFF, 0x31, 0xC9, 0x90, 0x8D, 0x74, 0x26, 0x00, 
+	0x05, 0x00, 0x36, 0x65, 0xC4, 0x83, 0xD2, 0xFF, 0x83, 0xC1, 
+	0x01, 0x83, 0xFA, 0x00, 0x77, 0xF0, 0x3D, 0xFF, 0xC9, 0x9A, 
+	0x3B, 0x77, 0xE9, 0x89, 0x47, 0x04, 0x8B, 0x45, 0xEC, 0x01, 
+	0x0F, 0x85, 0xC0, 0x0F, 0x84, 0x21, 0xFF, 0xFF, 0xFF, 0xE9, 
+	0x25, 0xFE, 0xFF, 0xFF, 0x8D, 0x74, 0x26, 0x00, 0xE8, 0x83, 
+	0xFD, 0xFF, 0xFF, 0x2B, 0x44, 0x1E, 0x08, 0x8B, 0x4C, 0x1E, 
+	0x10, 0x1B, 0x54, 0x1E, 0x0C, 0x21, 0xC1, 0x8B, 0x44, 0x1E, 
+	0x14, 0x89, 0x4D, 0xE8, 0x21, 0xD0, 0x89, 0xC1, 0x8B, 0x44, 
+	0x1E, 0x18, 0x0F, 0xAF, 0xC8, 0xF7, 0x65, 0xE8, 0x01, 0xCA, 
+	0xE9, 0x63, 0xFF, 0xFF, 0xFF, 0x89, 0xF6, 0x8D, 0xBC, 0x27, 
+	0x00, 0x00, 0x00, 0x00, 0xE8, 0x4B, 0xFD, 0xFF, 0xFF, 0x2B, 
+	0x44, 0x1E, 0x08, 0x8B, 0x4C, 0x1E, 0x10, 0x1B, 0x54, 0x1E, 
+	0x0C, 0x21, 0xC1, 0x8B, 0x44, 0x1E, 0x14, 0x89, 0x4D, 0xE8, 
+	0x21, 0xD0, 0x89, 0xC1, 0x8B, 0x44, 0x1E, 0x18, 0x0F, 0xAF, 
+	0xC8, 0xF7, 0x65, 0xE8, 0x01, 0xCA, 0xE9, 0x45, 0xFE, 0xFF, 
+	0xFF, 0x90, 0x8B, 0x83, 0xEC, 0xED, 0xFF, 0xFF, 0x31, 0xD2, 
+	0xEB, 0xCB, 0x8D, 0xB6, 0x00, 0x00, 0x00, 0x00, 0x8B, 0x83, 
+	0xEC, 0xED, 0xFF, 0xFF, 0x31, 0xD2, 0xEB, 0x83, 0x8D, 0xB6, 
+	0x00, 0x00, 0x00, 0x00, 0xF3, 0x90, 0xE9, 0x72, 0xFD, 0xFF, 
+	0xFF, 0x89, 0xF6, 0x8D, 0xBC, 0x27, 0x00, 0x00, 0x00, 0x00, 
+	0xF3, 0x90, 0xE9, 0xA3, 0xFE, 0xFF, 0xFF, 0x89, 0xF6, 0x8D, 
+	0xBC, 0x27, 0x00, 0x00, 0x00, 0x00, 0xC7, 0x45, 0xEC, 0x00, 
+	0x00, 0x00, 0x00, 0xE9, 0x51, 0xFF, 0xFF, 0xFF, 0x8D, 0x74, 
+	0x26, 0x00, 0xC7, 0x45, 0xEC, 0x00, 0x00, 0x00, 0x00, 0xE9, 
+	0x79, 0xFF, 0xFF, 0xFF, 0x8D, 0x74, 0x26, 0x00, 0x55, 0x89, 
+	0xE5, 0x57, 0x56, 0x53, 0x8D, 0x35, 0x7C, 0xDD, 0xFF, 0xFF, 
+	0xE8, 0xBE, 0x01, 0x00, 0x00, 0x81, 0xC3, 0x13, 0xF9, 0xFF, 
+	0xFF, 0x83, 0xEC, 0x14, 0x8B, 0x7D, 0x08, 0x85, 0xFF, 0x0F, 
+	0x84, 0xBE, 0x00, 0x00, 0x00, 0x8B, 0x3C, 0x1E, 0xF7, 0xC7, 
+	0x01, 0x00, 0x00, 0x00, 0x0F, 0x85, 0x1C, 0x01, 0x00, 0x00, 
+	0x8B, 0x44, 0x1E, 0x04, 0x8B, 0x55, 0x08, 0x89, 0x45, 0xEC, 
+	0x8B, 0x44, 0x1E, 0x28, 0x89, 0x02, 0x8B, 0x4C, 0x1E, 0x04, 
+	0x8B, 0x44, 0x1E, 0x20, 0x8B, 0x54, 0x1E, 0x24, 0x83, 0xF9, 
+	0x01, 0x89, 0x45, 0xE0, 0x89, 0x55, 0xE4, 0x0F, 0x84, 0xA9, 
+	0x00, 0x00, 0x00, 0x83, 0xF9, 0x02, 0x0F, 0x84, 0xD8, 0x00, 
+	0x00, 0x00, 0x31, 0xC0, 0x31, 0xD2, 0x83, 0xF9, 0x03, 0x0F, 
+	0x84, 0xEB, 0x00, 0x00, 0x00, 0x8B, 0x4C, 0x1E, 0x1C, 0x3B, 
+	0xBB, 0x7C, 0xDD, 0xFF, 0xFF, 0x75, 0xA4, 0x03, 0x45, 0xE0, 
+	0x8B, 0x7D, 0x08, 0x13, 0x55, 0xE4, 0x31, 0xF6, 0x0F, 0xAD, 
+	0xD0, 0xD3, 0xEA, 0xF6, 0xC1, 0x20, 0x0F, 0x45, 0xC2, 0x0F, 
+	0x45, 0xD6, 0x83, 0xFA, 0x00, 0x76, 0x59, 0x31, 0xC9, 0x8D, 
+	0xB4, 0x26, 0x00, 0x00, 0x00, 0x00, 0x05, 0x00, 0x36, 0x65, 
+	0xC4, 0x83, 0xD2, 0xFF, 0x83, 0xC1, 0x01, 0x83, 0xFA, 0x00, 
+	0x77, 0xF0, 0x3D, 0xFF, 0xC9, 0x9A, 0x3B, 0x77, 0xE9, 0x8B, 
+	0x75, 0xEC, 0x01, 0x0F, 0x89, 0xC1, 0x89, 0x47, 0x04, 0x85, 
+	0xF6, 0x0F, 0x84, 0xB1, 0x00, 0x00, 0x00, 0xBA, 0xD3, 0x4D, 
+	0x62, 0x10, 0xC1, 0xF9, 0x1F, 0xF7, 0xEA, 0xC1, 0xFA, 0x06, 
+	0x29, 0xCA, 0x89, 0x57, 0x04, 0x8B, 0x55, 0x0C, 0x31, 0xC0, 
+	0x85, 0xD2, 0x75, 0x7D, 0x83, 0xC4, 0x14, 0x5B, 0x5E, 0x5F, 
+	0x5D, 0xC3, 0x8D, 0x74, 0x26, 0x00, 0x31, 0xC9, 0x3D, 0xFF, 
+	0xC9, 0x9A, 0x3B, 0x76, 0xBE, 0xEB, 0x9C, 0x90, 0x8D, 0x74, 
+	0x26, 0x00, 0xE8, 0xB3, 0xFB, 0xFF, 0xFF, 0x2B, 0x44, 0x1E, 
+	0x08, 0x8B, 0x4C, 0x1E, 0x10, 0x1B, 0x54, 0x1E, 0x0C, 0x21, 
+	0xC1, 0x8B, 0x44, 0x1E, 0x14, 0x89, 0x4D, 0xE8, 0x21, 0xD0, 
+	0x89, 0xC1, 0x8B, 0x44, 0x1E, 0x18, 0x0F, 0xAF, 0xC8, 0xF7, 
+	0x65, 0xE8, 0x01, 0xCA, 0xE9, 0x3E, 0xFF, 0xFF, 0xFF, 0x89, 
+	0xF6, 0x8D, 0xBC, 0x27, 0x00, 0x00, 0x00, 0x00, 0x8B, 0x83, 
+	0xEC, 0xED, 0xFF, 0xFF, 0x31, 0xD2, 0xEB, 0xC3, 0x8D, 0xB6, 
+	0x00, 0x00, 0x00, 0x00, 0xF3, 0x90, 0xE9, 0xCE, 0xFE, 0xFF, 
+	0xFF, 0x89, 0xF6, 0x8D, 0xBC, 0x27, 0x00, 0x00, 0x00, 0x00, 
+	0xC7, 0x45, 0xEC, 0x00, 0x00, 0x00, 0x00, 0xEB, 0xA4, 0x8B, 
+	0x93, 0xDC, 0xDD, 0xFF, 0xFF, 0x8B, 0x75, 0x0C, 0x89, 0x16, 
+	0x8B, 0x93, 0xE0, 0xDD, 0xFF, 0xFF, 0x89, 0x56, 0x04, 0xE9, 
+	0x6A, 0xFF, 0xFF, 0xFF, 0xB8, 0x4E, 0x00, 0x00, 0x00, 0x8B, 
+	0x4D, 0x0C, 0x89, 0xDA, 0x89, 0xFB, 0xE8, 0x6D, 0x00, 0x00, 
+	0x00, 0x89, 0xD3, 0xE9, 0x52, 0xFF, 0xFF, 0xFF, 0x8D, 0xB6, 
+	0x00, 0x00, 0x00, 0x00, 0x55, 0xE8, 0x25, 0x00, 0x00, 0x00, 
+	0x81, 0xC1, 0x7E, 0xF7, 0xFF, 0xFF, 0x89, 0xE5, 0x57, 0x56, 
+	0x8B, 0x55, 0x08, 0x8B, 0xB1, 0xA4, 0xDD, 0xFF, 0xFF, 0x8B, 
+	0xB9, 0xA8, 0xDD, 0xFF, 0xFF, 0x85, 0xD2, 0x89, 0xF0, 0x74, 
+	0x02, 0x89, 0x32, 0x5E, 0x5F, 0x5D, 0xC3, 0x8B, 0x0C, 0x24, 
+	0xC3, 0x8B, 0x1C, 0x24, 0xC3, 0x90, 0x90, 0x90, 0x90, 0x90, 
+	0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x58, 0xB8, 
+	0x77, 0x00, 0x00, 0x00, 0x0F, 0x05, 0x90, 0x8D, 0xB4, 0x26, 
+	0x00, 0x00, 0x00, 0x00, 0xB8, 0xAD, 0x00, 0x00, 0x00, 0x0F, 
+	0x05, 0x90, 0x90, 0x8D, 0xB4, 0x26, 0x00, 0x00, 0x00, 0x00, 
+	0x55, 0x89, 0xCD, 0x0F, 0x05, 0xB9, 0x2B, 0x00, 0x00, 0x00, 
+	0x8E, 0xD1, 0x89, 0xE9, 0x5D, 0xC3, 0xC0, 0xFA, 0xFF, 0xFF, 
+	0x14, 0x00, 0x00, 0x00, 0x71, 0x00, 0x03, 0x03, 0xB7, 0xFA, 
+	0xFF, 0xFF, 0x0B, 0x00, 0x00, 0x00, 0x72, 0x00, 0x03, 0x03, 
+	0x0F, 0xAE, 0xF0, 0x0F, 0xAE, 0xE8, 0x47, 0x43, 0x43, 0x3A, 
+	0x20, 0x28, 0x55, 0x62, 0x75, 0x6E, 0x74, 0x75, 0x20, 0x34, 
+	0x2E, 0x39, 0x2E, 0x31, 0x2D, 0x31, 0x36, 0x75, 0x62, 0x75, 
+	0x6E, 0x74, 0x75, 0x36, 0x29, 0x20, 0x34, 0x2E, 0x39, 0x2E, 
+	0x31, 0x00, 0x00, 0x2E, 0x73, 0x68, 0x73, 0x74, 0x72, 0x74, 
+	0x61, 0x62, 0x00, 0x2E, 0x68, 0x61, 0x73, 0x68, 0x00, 0x2E, 
+	0x64, 0x79, 0x6E, 0x73, 0x79, 0x6D, 0x00, 0x2E, 0x64, 0x79, 
+	0x6E, 0x73, 0x74, 0x72, 0x00, 0x2E, 0x67, 0x6E, 0x75, 0x2E, 
+	0x76, 0x65, 0x72, 0x73, 0x69, 0x6F, 0x6E, 0x00, 0x2E, 0x67, 
+	0x6E, 0x75, 0x2E, 0x76, 0x65, 0x72, 0x73, 0x69, 0x6F, 0x6E, 
+	0x5F, 0x64, 0x00, 0x2E, 0x64, 0x79, 0x6E, 0x61, 0x6D, 0x69, 
+	0x63, 0x00, 0x2E, 0x72, 0x6F, 0x64, 0x61, 0x74, 0x61, 0x00, 
+	0x2E, 0x6E, 0x6F, 0x74, 0x65, 0x00, 0x2E, 0x65, 0x68, 0x5F, 
+	0x66, 0x72, 0x61, 0x6D, 0x65, 0x5F, 0x68, 0x64, 0x72, 0x00, 
+	0x2E, 0x65, 0x68, 0x5F, 0x66, 0x72, 0x61, 0x6D, 0x65, 0x00, 
+	0x2E, 0x74, 0x65, 0x78, 0x74, 0x00, 0x2E, 0x61, 0x6C, 0x74, 
+	0x69, 0x6E, 0x73, 0x74, 0x72, 0x75, 0x63, 0x74, 0x69, 0x6F, 
+	0x6E, 0x73, 0x00, 0x2E, 0x61, 0x6C, 0x74, 0x69, 0x6E, 0x73, 
+	0x74, 0x72, 0x5F, 0x72, 0x65, 0x70, 0x6C, 0x61, 0x63, 0x65, 
+	0x6D, 0x65, 0x6E, 0x74, 0x00, 0x2E, 0x63, 0x6F, 0x6D, 0x6D, 
+	0x65, 0x6E, 0x74, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x0B, 0x00, 0x00, 0x00, 0x05, 0x00, 
+	0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0xB4, 0x00, 0x00, 0x00, 
+	0xB4, 0x00, 0x00, 0x00, 0x38, 0x00, 0x00, 0x00, 0x02, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 
+	0x04, 0x00, 0x00, 0x00, 0x11, 0x00, 0x00, 0x00, 0x0B, 0x00, 
+	0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0xEC, 0x00, 0x00, 0x00, 
+	0xEC, 0x00, 0x00, 0x00, 0x90, 0x00, 0x00, 0x00, 0x03, 0x00, 
+	0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 
+	0x10, 0x00, 0x00, 0x00, 0x19, 0x00, 0x00, 0x00, 0x03, 0x00, 
+	0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x7C, 0x01, 0x00, 0x00, 
+	0x7C, 0x01, 0x00, 0x00, 0x95, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x21, 0x00, 0x00, 0x00, 0xFF, 0xFF, 
+	0xFF, 0x6F, 0x02, 0x00, 0x00, 0x00, 0x12, 0x02, 0x00, 0x00, 
+	0x12, 0x02, 0x00, 0x00, 0x12, 0x00, 0x00, 0x00, 0x02, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x00, 0x00, 0x2E, 0x00, 0x00, 0x00, 0xFD, 0xFF, 
+	0xFF, 0x6F, 0x02, 0x00, 0x00, 0x00, 0x24, 0x02, 0x00, 0x00, 
+	0x24, 0x02, 0x00, 0x00, 0x54, 0x00, 0x00, 0x00, 0x03, 0x00, 
+	0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x3D, 0x00, 0x00, 0x00, 0x06, 0x00, 
+	0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x78, 0x02, 0x00, 0x00, 
+	0x78, 0x02, 0x00, 0x00, 0x80, 0x00, 0x00, 0x00, 0x03, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 
+	0x08, 0x00, 0x00, 0x00, 0x46, 0x00, 0x00, 0x00, 0x01, 0x00, 
+	0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0xF8, 0x02, 0x00, 0x00, 
+	0xF8, 0x02, 0x00, 0x00, 0x20, 0x02, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 
+	0x04, 0x00, 0x00, 0x00, 0x4E, 0x00, 0x00, 0x00, 0x07, 0x00, 
+	0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x18, 0x05, 0x00, 0x00, 
+	0x18, 0x05, 0x00, 0x00, 0x60, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x54, 0x00, 0x00, 0x00, 0x01, 0x00, 
+	0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x78, 0x05, 0x00, 0x00, 
+	0x78, 0x05, 0x00, 0x00, 0x24, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x62, 0x00, 0x00, 0x00, 0x01, 0x00, 
+	0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x9C, 0x05, 0x00, 0x00, 
+	0x9C, 0x05, 0x00, 0x00, 0xFC, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x6C, 0x00, 0x00, 0x00, 0x01, 0x00, 
+	0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 0xA0, 0x06, 0x00, 0x00, 
+	0xA0, 0x06, 0x00, 0x00, 0x50, 0x05, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x72, 0x00, 0x00, 0x00, 0x01, 0x00, 
+	0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0xF0, 0x0B, 0x00, 0x00, 
+	0xF0, 0x0B, 0x00, 0x00, 0x18, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x83, 0x00, 0x00, 0x00, 0x01, 0x00, 
+	0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 0x08, 0x0C, 0x00, 0x00, 
+	0x08, 0x0C, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x99, 0x00, 0x00, 0x00, 0x01, 0x00, 
+	0x00, 0x00, 0x30, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x0E, 0x0C, 0x00, 0x00, 0x24, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x01, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x03, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x32, 0x0C, 0x00, 0x00, 0xA2, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 
+};
+
+static struct page *pages[1];
+
+const struct vdso_image vdso_image_32_syscall = {
+	.data = raw_data,
+	.size = 4096,
+	.text_mapping = {
+		.name = "[vdso]",
+		.pages = pages,
+	},
+	.alt = 3056,
+	.alt_len = 24,
+	.sym_vvar_start = -8192,
+	.sym_vvar_page = -8192,
+	.sym_hpet_page = -4096,
+	.sym_VDSO32_NOTE_MASK = 1384,
+	.sym___kernel_vsyscall = 3040,
+	.sym___kernel_sigreturn = 3008,
+	.sym___kernel_rt_sigreturn = 3024,
+};
diff -urN a/arch/x86/vdso/vdso-image-32-sysenter.c b/arch/x86/vdso/vdso-image-32-sysenter.c
--- a/arch/x86/vdso/vdso-image-32-sysenter.c	1969-12-31 17:00:00.000000000 -0700
+++ b/arch/x86/vdso/vdso-image-32-sysenter.c	2014-11-22 18:41:11.701004287 -0700
@@ -0,0 +1,422 @@
+/* AUTOMATICALLY GENERATED -- DO NOT EDIT */
+
+#include <linux/linkage.h>
+#include <asm/page_types.h>
+#include <asm/vdso.h>
+
+static unsigned char raw_data[4096] __page_aligned_data = {
+	0x7F, 0x45, 0x4C, 0x46, 0x01, 0x01, 0x01, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x00, 0x03, 0x00, 
+	0x01, 0x00, 0x00, 0x00, 0xE0, 0x0B, 0x00, 0x00, 0x34, 0x00, 
+	0x00, 0x00, 0xD8, 0x0C, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x34, 0x00, 0x20, 0x00, 0x04, 0x00, 0x28, 0x00, 0x10, 0x00, 
+	0x0F, 0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x12, 0x0C, 
+	0x00, 0x00, 0x12, 0x0C, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00, 
+	0x00, 0x10, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x78, 0x02, 
+	0x00, 0x00, 0x78, 0x02, 0x00, 0x00, 0x78, 0x02, 0x00, 0x00, 
+	0x80, 0x00, 0x00, 0x00, 0x80, 0x00, 0x00, 0x00, 0x04, 0x00, 
+	0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 
+	0x0C, 0x05, 0x00, 0x00, 0x0C, 0x05, 0x00, 0x00, 0x0C, 0x05, 
+	0x00, 0x00, 0x60, 0x00, 0x00, 0x00, 0x60, 0x00, 0x00, 0x00, 
+	0x04, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x50, 0xE5, 
+	0x74, 0x64, 0x6C, 0x05, 0x00, 0x00, 0x6C, 0x05, 0x00, 0x00, 
+	0x6C, 0x05, 0x00, 0x00, 0x24, 0x00, 0x00, 0x00, 0x24, 0x00, 
+	0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 
+	0x03, 0x00, 0x00, 0x00, 0x09, 0x00, 0x00, 0x00, 0x08, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 
+	0x04, 0x00, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00, 0x03, 0x00, 
+	0x00, 0x00, 0x07, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x81, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x11, 0x00, 0xF1, 0xFF, 0x16, 0x00, 
+	0x00, 0x00, 0xE0, 0x0B, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00, 
+	0x12, 0x00, 0x0B, 0x00, 0x28, 0x00, 0x00, 0x00, 0xE0, 0x09, 
+	0x00, 0x00, 0x9A, 0x01, 0x00, 0x00, 0x12, 0x00, 0x0B, 0x00, 
+	0x8B, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x11, 0x00, 0xF1, 0xFF, 0x5B, 0x00, 0x00, 0x00, 
+	0xD0, 0x0B, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 0x12, 0x00, 
+	0x0B, 0x00, 0x01, 0x00, 0x00, 0x00, 0xE0, 0x06, 0x00, 0x00, 
+	0xFC, 0x02, 0x00, 0x00, 0x12, 0x00, 0x0B, 0x00, 0x48, 0x00, 
+	0x00, 0x00, 0xC0, 0x0B, 0x00, 0x00, 0x09, 0x00, 0x00, 0x00, 
+	0x12, 0x00, 0x0B, 0x00, 0x3C, 0x00, 0x00, 0x00, 0x80, 0x0B, 
+	0x00, 0x00, 0x2B, 0x00, 0x00, 0x00, 0x12, 0x00, 0x0B, 0x00, 
+	0x00, 0x5F, 0x5F, 0x76, 0x64, 0x73, 0x6F, 0x5F, 0x63, 0x6C, 
+	0x6F, 0x63, 0x6B, 0x5F, 0x67, 0x65, 0x74, 0x74, 0x69, 0x6D, 
+	0x65, 0x00, 0x5F, 0x5F, 0x6B, 0x65, 0x72, 0x6E, 0x65, 0x6C, 
+	0x5F, 0x76, 0x73, 0x79, 0x73, 0x63, 0x61, 0x6C, 0x6C, 0x00, 
+	0x5F, 0x5F, 0x76, 0x64, 0x73, 0x6F, 0x5F, 0x67, 0x65, 0x74, 
+	0x74, 0x69, 0x6D, 0x65, 0x6F, 0x66, 0x64, 0x61, 0x79, 0x00, 
+	0x5F, 0x5F, 0x76, 0x64, 0x73, 0x6F, 0x5F, 0x74, 0x69, 0x6D, 
+	0x65, 0x00, 0x5F, 0x5F, 0x6B, 0x65, 0x72, 0x6E, 0x65, 0x6C, 
+	0x5F, 0x73, 0x69, 0x67, 0x72, 0x65, 0x74, 0x75, 0x72, 0x6E, 
+	0x00, 0x5F, 0x5F, 0x6B, 0x65, 0x72, 0x6E, 0x65, 0x6C, 0x5F, 
+	0x72, 0x74, 0x5F, 0x73, 0x69, 0x67, 0x72, 0x65, 0x74, 0x75, 
+	0x72, 0x6E, 0x00, 0x6C, 0x69, 0x6E, 0x75, 0x78, 0x2D, 0x67, 
+	0x61, 0x74, 0x65, 0x2E, 0x73, 0x6F, 0x2E, 0x31, 0x00, 0x4C, 
+	0x49, 0x4E, 0x55, 0x58, 0x5F, 0x32, 0x2E, 0x36, 0x00, 0x4C, 
+	0x49, 0x4E, 0x55, 0x58, 0x5F, 0x32, 0x2E, 0x35, 0x00, 0x00, 
+	0x00, 0x00, 0x02, 0x00, 0x03, 0x00, 0x02, 0x00, 0x03, 0x00, 
+	0x03, 0x00, 0x02, 0x00, 0x03, 0x00, 0x02, 0x00, 0x01, 0x00, 
+	0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x91, 0xF0, 0xCE, 0x0F, 
+	0x14, 0x00, 0x00, 0x00, 0x1C, 0x00, 0x00, 0x00, 0x71, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x01, 0x00, 0xF6, 0x75, 0xAE, 0x03, 0x14, 0x00, 
+	0x00, 0x00, 0x1C, 0x00, 0x00, 0x00, 0x81, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x03, 0x00, 
+	0x01, 0x00, 0xF5, 0x75, 0xAE, 0x03, 0x14, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x8B, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x0E, 0x00, 0x00, 0x00, 0x71, 0x00, 0x00, 0x00, 
+	0x10, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 
+	0x00, 0x00, 0xB4, 0x00, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00, 
+	0x7C, 0x01, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 0xEC, 0x00, 
+	0x00, 0x00, 0x0A, 0x00, 0x00, 0x00, 0x95, 0x00, 0x00, 0x00, 
+	0x0B, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0xFC, 0xFF, 
+	0xFF, 0x6F, 0x24, 0x02, 0x00, 0x00, 0xFD, 0xFF, 0xFF, 0x6F, 
+	0x03, 0x00, 0x00, 0x00, 0xF0, 0xFF, 0xFF, 0x6F, 0x12, 0x02, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x78, 0x02, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00, 
+	0x03, 0x00, 0x00, 0x00, 0x47, 0x4E, 0x55, 0x00, 0x31, 0x02, 
+	0xCB, 0x68, 0x14, 0x71, 0x50, 0xF6, 0x73, 0xEB, 0x77, 0x08, 
+	0xD0, 0x62, 0x9B, 0x53, 0x56, 0xCC, 0x17, 0x4C, 0x06, 0x00, 
+	0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x4C, 0x69, 0x6E, 0x75, 0x78, 0x00, 0x00, 0x00, 0x04, 0x11, 
+	0x03, 0x00, 0x04, 0x00, 0x00, 0x00, 0x12, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x00, 0x00, 0x47, 0x4E, 0x55, 0x00, 0x01, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x6E, 0x6F, 0x73, 
+	0x65, 0x67, 0x6E, 0x65, 0x67, 0x00, 0x00, 0x00, 0x01, 0x1B, 
+	0x03, 0x3B, 0x20, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 
+	0x53, 0x06, 0x00, 0x00, 0x38, 0x00, 0x00, 0x00, 0x63, 0x06, 
+	0x00, 0x00, 0xA4, 0x00, 0x00, 0x00, 0x74, 0x06, 0x00, 0x00, 
+	0x04, 0x01, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x01, 0x7A, 0x52, 0x53, 0x00, 0x01, 0x7C, 0x08, 
+	0x01, 0x1B, 0x00, 0x00, 0x68, 0x00, 0x00, 0x00, 0x18, 0x00, 
+	0x00, 0x00, 0x13, 0x06, 0x00, 0x00, 0x09, 0x00, 0x00, 0x00, 
+	0x00, 0x0F, 0x03, 0x74, 0x20, 0x06, 0x10, 0x00, 0x02, 0x74, 
+	0x30, 0x10, 0x01, 0x02, 0x74, 0x2C, 0x10, 0x02, 0x02, 0x74, 
+	0x28, 0x10, 0x03, 0x02, 0x74, 0x24, 0x10, 0x05, 0x02, 0x74, 
+	0x1C, 0x10, 0x06, 0x02, 0x74, 0x18, 0x10, 0x07, 0x02, 0x74, 
+	0x14, 0x10, 0x08, 0x02, 0x74, 0x3C, 0x42, 0x0F, 0x03, 0x74, 
+	0x1C, 0x06, 0x10, 0x00, 0x02, 0x74, 0x2C, 0x10, 0x01, 0x02, 
+	0x74, 0x28, 0x10, 0x02, 0x02, 0x74, 0x24, 0x10, 0x03, 0x02, 
+	0x74, 0x20, 0x10, 0x05, 0x02, 0x74, 0x18, 0x10, 0x06, 0x02, 
+	0x74, 0x14, 0x10, 0x07, 0x02, 0x74, 0x10, 0x10, 0x08, 0x02, 
+	0x74, 0x38, 0x44, 0x00, 0x00, 0x00, 0x84, 0x00, 0x00, 0x00, 
+	0xB7, 0x05, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 0x00, 0x0F, 
+	0x04, 0x74, 0xBC, 0x01, 0x06, 0x10, 0x00, 0x03, 0x74, 0xCC, 
+	0x01, 0x10, 0x01, 0x03, 0x74, 0xC8, 0x01, 0x10, 0x02, 0x03, 
+	0x74, 0xC4, 0x01, 0x10, 0x03, 0x03, 0x74, 0xC0, 0x01, 0x10, 
+	0x05, 0x03, 0x74, 0xB8, 0x01, 0x10, 0x06, 0x03, 0x74, 0xB4, 
+	0x01, 0x10, 0x07, 0x03, 0x74, 0xB0, 0x01, 0x10, 0x08, 0x03, 
+	0x74, 0xD8, 0x01, 0x00, 0x14, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x01, 0x7A, 0x52, 0x00, 0x01, 0x7C, 0x08, 0x01, 
+	0x1B, 0x0C, 0x04, 0x04, 0x88, 0x01, 0x00, 0x00, 0x24, 0x00, 
+	0x00, 0x00, 0x1C, 0x00, 0x00, 0x00, 0x68, 0x05, 0x00, 0x00, 
+	0x14, 0x00, 0x00, 0x00, 0x00, 0x41, 0x0E, 0x08, 0x41, 0x0E, 
+	0x0C, 0x41, 0x0E, 0x10, 0x85, 0x04, 0x4E, 0x0E, 0x0C, 0xC5, 
+	0x41, 0x0E, 0x08, 0x41, 0x0E, 0x04, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x55, 0x89, 0xE5, 0x56, 
+	0x53, 0xE8, 0x05, 0x05, 0x00, 0x00, 0x81, 0xC3, 0x4E, 0xFC, 
+	0xFF, 0xFF, 0x8D, 0x76, 0x00, 0x8D, 0x76, 0x00, 0x0F, 0x31, 
+	0x8B, 0x8B, 0x94, 0xDD, 0xFF, 0xFF, 0x8B, 0xB3, 0x90, 0xDD, 
+	0xFF, 0xFF, 0x39, 0xCA, 0x77, 0x08, 0x73, 0x0E, 0x89, 0xF0, 
+	0x89, 0xCA, 0x66, 0x90, 0x5B, 0x5E, 0x5D, 0xC3, 0x8D, 0x74, 
+	0x26, 0x00, 0x39, 0xF0, 0x72, 0xEE, 0x5B, 0x5E, 0x5D, 0xC3, 
+	0x55, 0x89, 0xE5, 0x57, 0x56, 0x53, 0x8D, 0x35, 0x88, 0xDD, 
+	0xFF, 0xFF, 0xE8, 0xBE, 0x04, 0x00, 0x00, 0x81, 0xC3, 0x07, 
+	0xFC, 0xFF, 0xFF, 0x83, 0xEC, 0x14, 0x83, 0x7D, 0x08, 0x01, 
+	0x8B, 0x7D, 0x0C, 0x74, 0x75, 0x0F, 0x8E, 0x47, 0x01, 0x00, 
+	0x00, 0x83, 0x7D, 0x08, 0x05, 0x74, 0x41, 0x83, 0x7D, 0x08, 
+	0x06, 0x0F, 0x85, 0x17, 0x01, 0x00, 0x00, 0x8B, 0x04, 0x1E, 
+	0xA8, 0x01, 0x0F, 0x85, 0x7C, 0x02, 0x00, 0x00, 0x8B, 0x54, 
+	0x1E, 0x50, 0x89, 0x17, 0x8B, 0x54, 0x1E, 0x58, 0x89, 0x57, 
+	0x04, 0x39, 0x83, 0x88, 0xDD, 0xFF, 0xFF, 0x75, 0xE0, 0x83, 
+	0xC4, 0x14, 0x31, 0xC0, 0x5B, 0x5E, 0x5F, 0x5D, 0xC3, 0x90, 
+	0x8D, 0x74, 0x26, 0x00, 0xF3, 0x90, 0x8D, 0xB6, 0x00, 0x00, 
+	0x00, 0x00, 0x8B, 0x04, 0x1E, 0xA8, 0x01, 0x75, 0xF1, 0x8B, 
+	0x54, 0x1E, 0x40, 0x89, 0x17, 0x8B, 0x54, 0x1E, 0x48, 0x89, 
+	0x57, 0x04, 0x39, 0x83, 0x88, 0xDD, 0xFF, 0xFF, 0x74, 0xCD, 
+	0xEB, 0xE2, 0x66, 0x90, 0xF3, 0x90, 0x8D, 0xB6, 0x00, 0x00, 
+	0x00, 0x00, 0x8B, 0x3C, 0x1E, 0xF7, 0xC7, 0x01, 0x00, 0x00, 
+	0x00, 0x75, 0xED, 0x8B, 0x44, 0x1E, 0x04, 0x8B, 0x55, 0x0C, 
+	0x89, 0x45, 0xEC, 0x8B, 0x44, 0x1E, 0x30, 0x89, 0x02, 0x8B, 
+	0x4C, 0x1E, 0x04, 0x8B, 0x44, 0x1E, 0x38, 0x8B, 0x54, 0x1E, 
+	0x3C, 0x83, 0xF9, 0x01, 0x89, 0x45, 0xE0, 0x89, 0x55, 0xE4, 
+	0x0F, 0x84, 0xA2, 0x01, 0x00, 0x00, 0x83, 0xF9, 0x02, 0x0F, 
+	0x84, 0xC9, 0x01, 0x00, 0x00, 0x31, 0xC0, 0x31, 0xD2, 0x83, 
+	0xF9, 0x03, 0x0F, 0x84, 0x0C, 0x02, 0x00, 0x00, 0x8B, 0x4C, 
+	0x1E, 0x1C, 0x3B, 0xBB, 0x88, 0xDD, 0xFF, 0xFF, 0x75, 0xA8, 
+	0x03, 0x45, 0xE0, 0x8B, 0x7D, 0x0C, 0x13, 0x55, 0xE4, 0x31, 
+	0xF6, 0x0F, 0xAD, 0xD0, 0xD3, 0xEA, 0xF6, 0xC1, 0x20, 0x0F, 
+	0x45, 0xC2, 0x0F, 0x45, 0xD6, 0x83, 0xFA, 0x00, 0x77, 0x09, 
+	0x31, 0xC9, 0x3D, 0xFF, 0xC9, 0x9A, 0x3B, 0x76, 0x20, 0x31, 
+	0xC9, 0x8D, 0xB4, 0x26, 0x00, 0x00, 0x00, 0x00, 0x05, 0x00, 
+	0x36, 0x65, 0xC4, 0x83, 0xD2, 0xFF, 0x83, 0xC1, 0x01, 0x83, 
+	0xFA, 0x00, 0x77, 0xF0, 0x3D, 0xFF, 0xC9, 0x9A, 0x3B, 0x77, 
+	0xE9, 0x89, 0x47, 0x04, 0x8B, 0x45, 0xEC, 0x01, 0x0F, 0x85, 
+	0xC0, 0x0F, 0x85, 0x12, 0xFF, 0xFF, 0xFF, 0x89, 0xF6, 0x8D, 
+	0xBC, 0x27, 0x00, 0x00, 0x00, 0x00, 0x89, 0xF9, 0xB8, 0x09, 
+	0x01, 0x00, 0x00, 0x89, 0xDA, 0x8B, 0x5D, 0x08, 0xE8, 0x9F, 
+	0x03, 0x00, 0x00, 0x89, 0xD3, 0x83, 0xC4, 0x14, 0x5B, 0x5E, 
+	0x5F, 0x5D, 0xC3, 0x90, 0x8D, 0x74, 0x26, 0x00, 0x8B, 0x45, 
+	0x08, 0x85, 0xC0, 0x75, 0xD9, 0x89, 0x7D, 0x0C, 0x8B, 0x3C, 
+	0x1E, 0xF7, 0xC7, 0x01, 0x00, 0x00, 0x00, 0x0F, 0x85, 0x47, 
+	0x01, 0x00, 0x00, 0x8B, 0x44, 0x1E, 0x04, 0x8B, 0x55, 0x0C, 
+	0x89, 0x45, 0xEC, 0x8B, 0x44, 0x1E, 0x28, 0x89, 0x02, 0x8B, 
+	0x4C, 0x1E, 0x04, 0x8B, 0x44, 0x1E, 0x20, 0x8B, 0x54, 0x1E, 
+	0x24, 0x83, 0xF9, 0x01, 0x89, 0x45, 0xE0, 0x89, 0x55, 0xE4, 
+	0x0F, 0x84, 0x84, 0x00, 0x00, 0x00, 0x83, 0xF9, 0x02, 0x0F, 
+	0x84, 0xF3, 0x00, 0x00, 0x00, 0x31, 0xC0, 0x31, 0xD2, 0x83, 
+	0xF9, 0x03, 0x0F, 0x84, 0x16, 0x01, 0x00, 0x00, 0x8B, 0x4C, 
+	0x1E, 0x1C, 0x3B, 0xBB, 0x88, 0xDD, 0xFF, 0xFF, 0x75, 0xA4, 
+	0x03, 0x45, 0xE0, 0x8B, 0x7D, 0x0C, 0x13, 0x55, 0xE4, 0x31, 
+	0xF6, 0x0F, 0xAD, 0xD0, 0xD3, 0xEA, 0xF6, 0xC1, 0x20, 0x0F, 
+	0x45, 0xC2, 0x0F, 0x45, 0xD6, 0x83, 0xFA, 0x00, 0x77, 0x0D, 
+	0x31, 0xC9, 0x3D, 0xFF, 0xC9, 0x9A, 0x3B, 0x0F, 0x86, 0x36, 
+	0xFF, 0xFF, 0xFF, 0x31, 0xC9, 0x90, 0x8D, 0x74, 0x26, 0x00, 
+	0x05, 0x00, 0x36, 0x65, 0xC4, 0x83, 0xD2, 0xFF, 0x83, 0xC1, 
+	0x01, 0x83, 0xFA, 0x00, 0x77, 0xF0, 0x3D, 0xFF, 0xC9, 0x9A, 
+	0x3B, 0x77, 0xE9, 0x89, 0x47, 0x04, 0x8B, 0x45, 0xEC, 0x01, 
+	0x0F, 0x85, 0xC0, 0x0F, 0x84, 0x21, 0xFF, 0xFF, 0xFF, 0xE9, 
+	0x25, 0xFE, 0xFF, 0xFF, 0x8D, 0x74, 0x26, 0x00, 0xE8, 0x83, 
+	0xFD, 0xFF, 0xFF, 0x2B, 0x44, 0x1E, 0x08, 0x8B, 0x4C, 0x1E, 
+	0x10, 0x1B, 0x54, 0x1E, 0x0C, 0x21, 0xC1, 0x8B, 0x44, 0x1E, 
+	0x14, 0x89, 0x4D, 0xE8, 0x21, 0xD0, 0x89, 0xC1, 0x8B, 0x44, 
+	0x1E, 0x18, 0x0F, 0xAF, 0xC8, 0xF7, 0x65, 0xE8, 0x01, 0xCA, 
+	0xE9, 0x63, 0xFF, 0xFF, 0xFF, 0x89, 0xF6, 0x8D, 0xBC, 0x27, 
+	0x00, 0x00, 0x00, 0x00, 0xE8, 0x4B, 0xFD, 0xFF, 0xFF, 0x2B, 
+	0x44, 0x1E, 0x08, 0x8B, 0x4C, 0x1E, 0x10, 0x1B, 0x54, 0x1E, 
+	0x0C, 0x21, 0xC1, 0x8B, 0x44, 0x1E, 0x14, 0x89, 0x4D, 0xE8, 
+	0x21, 0xD0, 0x89, 0xC1, 0x8B, 0x44, 0x1E, 0x18, 0x0F, 0xAF, 
+	0xC8, 0xF7, 0x65, 0xE8, 0x01, 0xCA, 0xE9, 0x45, 0xFE, 0xFF, 
+	0xFF, 0x90, 0x8B, 0x83, 0xF8, 0xED, 0xFF, 0xFF, 0x31, 0xD2, 
+	0xEB, 0xCB, 0x8D, 0xB6, 0x00, 0x00, 0x00, 0x00, 0x8B, 0x83, 
+	0xF8, 0xED, 0xFF, 0xFF, 0x31, 0xD2, 0xEB, 0x83, 0x8D, 0xB6, 
+	0x00, 0x00, 0x00, 0x00, 0xF3, 0x90, 0xE9, 0x72, 0xFD, 0xFF, 
+	0xFF, 0x89, 0xF6, 0x8D, 0xBC, 0x27, 0x00, 0x00, 0x00, 0x00, 
+	0xF3, 0x90, 0xE9, 0xA3, 0xFE, 0xFF, 0xFF, 0x89, 0xF6, 0x8D, 
+	0xBC, 0x27, 0x00, 0x00, 0x00, 0x00, 0xC7, 0x45, 0xEC, 0x00, 
+	0x00, 0x00, 0x00, 0xE9, 0x51, 0xFF, 0xFF, 0xFF, 0x8D, 0x74, 
+	0x26, 0x00, 0xC7, 0x45, 0xEC, 0x00, 0x00, 0x00, 0x00, 0xE9, 
+	0x79, 0xFF, 0xFF, 0xFF, 0x8D, 0x74, 0x26, 0x00, 0x55, 0x89, 
+	0xE5, 0x57, 0x56, 0x53, 0x8D, 0x35, 0x88, 0xDD, 0xFF, 0xFF, 
+	0xE8, 0xBE, 0x01, 0x00, 0x00, 0x81, 0xC3, 0x07, 0xF9, 0xFF, 
+	0xFF, 0x83, 0xEC, 0x14, 0x8B, 0x7D, 0x08, 0x85, 0xFF, 0x0F, 
+	0x84, 0xBE, 0x00, 0x00, 0x00, 0x8B, 0x3C, 0x1E, 0xF7, 0xC7, 
+	0x01, 0x00, 0x00, 0x00, 0x0F, 0x85, 0x1C, 0x01, 0x00, 0x00, 
+	0x8B, 0x44, 0x1E, 0x04, 0x8B, 0x55, 0x08, 0x89, 0x45, 0xEC, 
+	0x8B, 0x44, 0x1E, 0x28, 0x89, 0x02, 0x8B, 0x4C, 0x1E, 0x04, 
+	0x8B, 0x44, 0x1E, 0x20, 0x8B, 0x54, 0x1E, 0x24, 0x83, 0xF9, 
+	0x01, 0x89, 0x45, 0xE0, 0x89, 0x55, 0xE4, 0x0F, 0x84, 0xA9, 
+	0x00, 0x00, 0x00, 0x83, 0xF9, 0x02, 0x0F, 0x84, 0xD8, 0x00, 
+	0x00, 0x00, 0x31, 0xC0, 0x31, 0xD2, 0x83, 0xF9, 0x03, 0x0F, 
+	0x84, 0xEB, 0x00, 0x00, 0x00, 0x8B, 0x4C, 0x1E, 0x1C, 0x3B, 
+	0xBB, 0x88, 0xDD, 0xFF, 0xFF, 0x75, 0xA4, 0x03, 0x45, 0xE0, 
+	0x8B, 0x7D, 0x08, 0x13, 0x55, 0xE4, 0x31, 0xF6, 0x0F, 0xAD, 
+	0xD0, 0xD3, 0xEA, 0xF6, 0xC1, 0x20, 0x0F, 0x45, 0xC2, 0x0F, 
+	0x45, 0xD6, 0x83, 0xFA, 0x00, 0x76, 0x59, 0x31, 0xC9, 0x8D, 
+	0xB4, 0x26, 0x00, 0x00, 0x00, 0x00, 0x05, 0x00, 0x36, 0x65, 
+	0xC4, 0x83, 0xD2, 0xFF, 0x83, 0xC1, 0x01, 0x83, 0xFA, 0x00, 
+	0x77, 0xF0, 0x3D, 0xFF, 0xC9, 0x9A, 0x3B, 0x77, 0xE9, 0x8B, 
+	0x75, 0xEC, 0x01, 0x0F, 0x89, 0xC1, 0x89, 0x47, 0x04, 0x85, 
+	0xF6, 0x0F, 0x84, 0xB1, 0x00, 0x00, 0x00, 0xBA, 0xD3, 0x4D, 
+	0x62, 0x10, 0xC1, 0xF9, 0x1F, 0xF7, 0xEA, 0xC1, 0xFA, 0x06, 
+	0x29, 0xCA, 0x89, 0x57, 0x04, 0x8B, 0x55, 0x0C, 0x31, 0xC0, 
+	0x85, 0xD2, 0x75, 0x7D, 0x83, 0xC4, 0x14, 0x5B, 0x5E, 0x5F, 
+	0x5D, 0xC3, 0x8D, 0x74, 0x26, 0x00, 0x31, 0xC9, 0x3D, 0xFF, 
+	0xC9, 0x9A, 0x3B, 0x76, 0xBE, 0xEB, 0x9C, 0x90, 0x8D, 0x74, 
+	0x26, 0x00, 0xE8, 0xB3, 0xFB, 0xFF, 0xFF, 0x2B, 0x44, 0x1E, 
+	0x08, 0x8B, 0x4C, 0x1E, 0x10, 0x1B, 0x54, 0x1E, 0x0C, 0x21, 
+	0xC1, 0x8B, 0x44, 0x1E, 0x14, 0x89, 0x4D, 0xE8, 0x21, 0xD0, 
+	0x89, 0xC1, 0x8B, 0x44, 0x1E, 0x18, 0x0F, 0xAF, 0xC8, 0xF7, 
+	0x65, 0xE8, 0x01, 0xCA, 0xE9, 0x3E, 0xFF, 0xFF, 0xFF, 0x89, 
+	0xF6, 0x8D, 0xBC, 0x27, 0x00, 0x00, 0x00, 0x00, 0x8B, 0x83, 
+	0xF8, 0xED, 0xFF, 0xFF, 0x31, 0xD2, 0xEB, 0xC3, 0x8D, 0xB6, 
+	0x00, 0x00, 0x00, 0x00, 0xF3, 0x90, 0xE9, 0xCE, 0xFE, 0xFF, 
+	0xFF, 0x89, 0xF6, 0x8D, 0xBC, 0x27, 0x00, 0x00, 0x00, 0x00, 
+	0xC7, 0x45, 0xEC, 0x00, 0x00, 0x00, 0x00, 0xEB, 0xA4, 0x8B, 
+	0x93, 0xE8, 0xDD, 0xFF, 0xFF, 0x8B, 0x75, 0x0C, 0x89, 0x16, 
+	0x8B, 0x93, 0xEC, 0xDD, 0xFF, 0xFF, 0x89, 0x56, 0x04, 0xE9, 
+	0x6A, 0xFF, 0xFF, 0xFF, 0xB8, 0x4E, 0x00, 0x00, 0x00, 0x8B, 
+	0x4D, 0x0C, 0x89, 0xDA, 0x89, 0xFB, 0xE8, 0x6D, 0x00, 0x00, 
+	0x00, 0x89, 0xD3, 0xE9, 0x52, 0xFF, 0xFF, 0xFF, 0x8D, 0xB6, 
+	0x00, 0x00, 0x00, 0x00, 0x55, 0xE8, 0x25, 0x00, 0x00, 0x00, 
+	0x81, 0xC1, 0x72, 0xF7, 0xFF, 0xFF, 0x89, 0xE5, 0x57, 0x56, 
+	0x8B, 0x55, 0x08, 0x8B, 0xB1, 0xB0, 0xDD, 0xFF, 0xFF, 0x8B, 
+	0xB9, 0xB4, 0xDD, 0xFF, 0xFF, 0x85, 0xD2, 0x89, 0xF0, 0x74, 
+	0x02, 0x89, 0x32, 0x5E, 0x5F, 0x5D, 0xC3, 0x8B, 0x0C, 0x24, 
+	0xC3, 0x8B, 0x1C, 0x24, 0xC3, 0x90, 0x90, 0x90, 0x90, 0x90, 
+	0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x58, 0xB8, 
+	0x77, 0x00, 0x00, 0x00, 0xCD, 0x80, 0x90, 0x8D, 0xB4, 0x26, 
+	0x00, 0x00, 0x00, 0x00, 0xB8, 0xAD, 0x00, 0x00, 0x00, 0xCD, 
+	0x80, 0x90, 0x90, 0x8D, 0xB4, 0x26, 0x00, 0x00, 0x00, 0x00, 
+	0x51, 0x52, 0x55, 0x89, 0xE5, 0x0F, 0x34, 0x90, 0x90, 0x90, 
+	0x90, 0x90, 0x90, 0x90, 0xCD, 0x80, 0x5D, 0x5A, 0x59, 0xC3, 
+	0xBC, 0xFA, 0xFF, 0xFF, 0x14, 0x00, 0x00, 0x00, 0x71, 0x00, 
+	0x03, 0x03, 0xB3, 0xFA, 0xFF, 0xFF, 0x0B, 0x00, 0x00, 0x00, 
+	0x72, 0x00, 0x03, 0x03, 0x0F, 0xAE, 0xF0, 0x0F, 0xAE, 0xE8, 
+	0x47, 0x43, 0x43, 0x3A, 0x20, 0x28, 0x55, 0x62, 0x75, 0x6E, 
+	0x74, 0x75, 0x20, 0x34, 0x2E, 0x39, 0x2E, 0x31, 0x2D, 0x31, 
+	0x36, 0x75, 0x62, 0x75, 0x6E, 0x74, 0x75, 0x36, 0x29, 0x20, 
+	0x34, 0x2E, 0x39, 0x2E, 0x31, 0x00, 0x00, 0x2E, 0x73, 0x68, 
+	0x73, 0x74, 0x72, 0x74, 0x61, 0x62, 0x00, 0x2E, 0x68, 0x61, 
+	0x73, 0x68, 0x00, 0x2E, 0x64, 0x79, 0x6E, 0x73, 0x79, 0x6D, 
+	0x00, 0x2E, 0x64, 0x79, 0x6E, 0x73, 0x74, 0x72, 0x00, 0x2E, 
+	0x67, 0x6E, 0x75, 0x2E, 0x76, 0x65, 0x72, 0x73, 0x69, 0x6F, 
+	0x6E, 0x00, 0x2E, 0x67, 0x6E, 0x75, 0x2E, 0x76, 0x65, 0x72, 
+	0x73, 0x69, 0x6F, 0x6E, 0x5F, 0x64, 0x00, 0x2E, 0x64, 0x79, 
+	0x6E, 0x61, 0x6D, 0x69, 0x63, 0x00, 0x2E, 0x72, 0x6F, 0x64, 
+	0x61, 0x74, 0x61, 0x00, 0x2E, 0x6E, 0x6F, 0x74, 0x65, 0x00, 
+	0x2E, 0x65, 0x68, 0x5F, 0x66, 0x72, 0x61, 0x6D, 0x65, 0x5F, 
+	0x68, 0x64, 0x72, 0x00, 0x2E, 0x65, 0x68, 0x5F, 0x66, 0x72, 
+	0x61, 0x6D, 0x65, 0x00, 0x2E, 0x74, 0x65, 0x78, 0x74, 0x00, 
+	0x2E, 0x61, 0x6C, 0x74, 0x69, 0x6E, 0x73, 0x74, 0x72, 0x75, 
+	0x63, 0x74, 0x69, 0x6F, 0x6E, 0x73, 0x00, 0x2E, 0x61, 0x6C, 
+	0x74, 0x69, 0x6E, 0x73, 0x74, 0x72, 0x5F, 0x72, 0x65, 0x70, 
+	0x6C, 0x61, 0x63, 0x65, 0x6D, 0x65, 0x6E, 0x74, 0x00, 0x2E, 
+	0x63, 0x6F, 0x6D, 0x6D, 0x65, 0x6E, 0x74, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0B, 0x00, 
+	0x00, 0x00, 0x05, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 
+	0xB4, 0x00, 0x00, 0x00, 0xB4, 0x00, 0x00, 0x00, 0x38, 0x00, 
+	0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x04, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x11, 0x00, 
+	0x00, 0x00, 0x0B, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 
+	0xEC, 0x00, 0x00, 0x00, 0xEC, 0x00, 0x00, 0x00, 0x90, 0x00, 
+	0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x04, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x19, 0x00, 
+	0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 
+	0x7C, 0x01, 0x00, 0x00, 0x7C, 0x01, 0x00, 0x00, 0x95, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x21, 0x00, 
+	0x00, 0x00, 0xFF, 0xFF, 0xFF, 0x6F, 0x02, 0x00, 0x00, 0x00, 
+	0x12, 0x02, 0x00, 0x00, 0x12, 0x02, 0x00, 0x00, 0x12, 0x00, 
+	0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x2E, 0x00, 
+	0x00, 0x00, 0xFD, 0xFF, 0xFF, 0x6F, 0x02, 0x00, 0x00, 0x00, 
+	0x24, 0x02, 0x00, 0x00, 0x24, 0x02, 0x00, 0x00, 0x54, 0x00, 
+	0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 
+	0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x3D, 0x00, 
+	0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 
+	0x78, 0x02, 0x00, 0x00, 0x78, 0x02, 0x00, 0x00, 0x80, 0x00, 
+	0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x04, 0x00, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 0x46, 0x00, 
+	0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 
+	0xF8, 0x02, 0x00, 0x00, 0xF8, 0x02, 0x00, 0x00, 0x14, 0x02, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x04, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x4E, 0x00, 
+	0x00, 0x00, 0x07, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 
+	0x0C, 0x05, 0x00, 0x00, 0x0C, 0x05, 0x00, 0x00, 0x60, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x54, 0x00, 
+	0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 
+	0x6C, 0x05, 0x00, 0x00, 0x6C, 0x05, 0x00, 0x00, 0x24, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x62, 0x00, 
+	0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 
+	0x90, 0x05, 0x00, 0x00, 0x90, 0x05, 0x00, 0x00, 0x08, 0x01, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x6C, 0x00, 
+	0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 
+	0xA0, 0x06, 0x00, 0x00, 0xA0, 0x06, 0x00, 0x00, 0x54, 0x05, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x10, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x72, 0x00, 
+	0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 
+	0xF4, 0x0B, 0x00, 0x00, 0xF4, 0x0B, 0x00, 0x00, 0x18, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x83, 0x00, 
+	0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 
+	0x0C, 0x0C, 0x00, 0x00, 0x0C, 0x0C, 0x00, 0x00, 0x06, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x99, 0x00, 
+	0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x30, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x12, 0x0C, 0x00, 0x00, 0x24, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x01, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01, 0x00, 
+	0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x36, 0x0C, 0x00, 0x00, 0xA2, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+};
+
+static struct page *pages[1];
+
+const struct vdso_image vdso_image_32_sysenter = {
+	.data = raw_data,
+	.size = 4096,
+	.text_mapping = {
+		.name = "[vdso]",
+		.pages = pages,
+	},
+	.alt = 3060,
+	.alt_len = 24,
+	.sym_vvar_start = -8192,
+	.sym_vvar_page = -8192,
+	.sym_hpet_page = -4096,
+	.sym_VDSO32_NOTE_MASK = 1372,
+	.sym_VDSO32_SYSENTER_RETURN = 3056,
+	.sym___kernel_vsyscall = 3040,
+	.sym___kernel_sigreturn = 3008,
+	.sym___kernel_rt_sigreturn = 3024,
+};
diff -urN a/arch/x86/vdso/vdso-image-64.c b/arch/x86/vdso/vdso-image-64.c
--- a/arch/x86/vdso/vdso-image-64.c	1969-12-31 17:00:00.000000000 -0700
+++ b/arch/x86/vdso/vdso-image-64.c	2014-11-22 18:41:11.649004289 -0700
@@ -0,0 +1,622 @@
+/* AUTOMATICALLY GENERATED -- DO NOT EDIT */
+
+#include <linux/linkage.h>
+#include <asm/page_types.h>
+#include <asm/vdso.h>
+
+static unsigned char raw_data[8192] __page_aligned_data = {
+	0x7F, 0x45, 0x4C, 0x46, 0x02, 0x01, 0x01, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x00, 0x3E, 0x00, 
+	0x01, 0x00, 0x00, 0x00, 0xD0, 0x08, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x40, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x58, 0x13, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x40, 0x00, 0x38, 0x00, 0x04, 0x00, 0x40, 0x00, 
+	0x10, 0x00, 0x0F, 0x00, 0x01, 0x00, 0x00, 0x00, 0x05, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x8B, 0x12, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x8B, 0x12, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x18, 0x03, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x18, 0x03, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x18, 0x03, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x08, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 
+	0x04, 0x00, 0x00, 0x00, 0x78, 0x07, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x78, 0x07, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x78, 0x07, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x3C, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x3C, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x50, 0xE5, 0x74, 0x64, 0x04, 0x00, 0x00, 0x00, 
+	0xB4, 0x07, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xB4, 0x07, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xB4, 0x07, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x2C, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x2C, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x00, 
+	0x00, 0x00, 0x0B, 0x00, 0x00, 0x00, 0x0A, 0x00, 0x00, 0x00, 
+	0x06, 0x00, 0x00, 0x00, 0x09, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x04, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 
+	0x05, 0x00, 0x00, 0x00, 0x07, 0x00, 0x00, 0x00, 0x08, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x03, 0x00, 0x07, 0x00, 0x18, 0x04, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x08, 0x00, 0x00, 0x00, 0x22, 0x00, 0x0B, 0x00, 0xD0, 0x08, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x83, 0x05, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x54, 0x00, 0x00, 0x00, 0x11, 0x00, 
+	0xF1, 0xFF, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x16, 0x00, 
+	0x00, 0x00, 0x12, 0x00, 0x0B, 0x00, 0x60, 0x0E, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0xBF, 0x02, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x36, 0x00, 0x00, 0x00, 0x12, 0x00, 0x0B, 0x00, 
+	0x40, 0x11, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x3D, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x1D, 0x00, 0x00, 0x00, 
+	0x22, 0x00, 0x0B, 0x00, 0x60, 0x0E, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0xBF, 0x02, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x31, 0x00, 0x00, 0x00, 0x22, 0x00, 0x0B, 0x00, 0x20, 0x11, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x15, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x3D, 0x00, 0x00, 0x00, 0x22, 0x00, 
+	0x0B, 0x00, 0x40, 0x11, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x3D, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 
+	0x00, 0x00, 0x12, 0x00, 0x0B, 0x00, 0xD0, 0x08, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x83, 0x05, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x2A, 0x00, 0x00, 0x00, 0x12, 0x00, 0x0B, 0x00, 
+	0x20, 0x11, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x15, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x5F, 0x5F, 0x76, 
+	0x64, 0x73, 0x6F, 0x5F, 0x63, 0x6C, 0x6F, 0x63, 0x6B, 0x5F, 
+	0x67, 0x65, 0x74, 0x74, 0x69, 0x6D, 0x65, 0x00, 0x5F, 0x5F, 
+	0x76, 0x64, 0x73, 0x6F, 0x5F, 0x67, 0x65, 0x74, 0x74, 0x69, 
+	0x6D, 0x65, 0x6F, 0x66, 0x64, 0x61, 0x79, 0x00, 0x5F, 0x5F, 
+	0x76, 0x64, 0x73, 0x6F, 0x5F, 0x74, 0x69, 0x6D, 0x65, 0x00, 
+	0x5F, 0x5F, 0x76, 0x64, 0x73, 0x6F, 0x5F, 0x67, 0x65, 0x74, 
+	0x63, 0x70, 0x75, 0x00, 0x6C, 0x69, 0x6E, 0x75, 0x78, 0x2D, 
+	0x76, 0x64, 0x73, 0x6F, 0x2E, 0x73, 0x6F, 0x2E, 0x31, 0x00, 
+	0x4C, 0x49, 0x4E, 0x55, 0x58, 0x5F, 0x32, 0x2E, 0x36, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x02, 0x00, 0x02, 0x00, 0x02, 0x00, 
+	0x02, 0x00, 0x02, 0x00, 0x02, 0x00, 0x02, 0x00, 0x02, 0x00, 
+	0x02, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x01, 0x00, 
+	0x01, 0x00, 0x01, 0x00, 0xA1, 0xBF, 0xEE, 0x0D, 0x14, 0x00, 
+	0x00, 0x00, 0x1C, 0x00, 0x00, 0x00, 0x44, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x02, 0x00, 
+	0x01, 0x00, 0xF6, 0x75, 0xAE, 0x03, 0x14, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x54, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x0E, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x44, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x10, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x20, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x05, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x68, 0x02, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x60, 0x01, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x0A, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x5E, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0B, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x18, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0xFC, 0xFF, 0xFF, 0x6F, 0x00, 0x00, 
+	0x00, 0x00, 0xE0, 0x02, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0xFD, 0xFF, 0xFF, 0x6F, 0x00, 0x00, 0x00, 0x00, 0x02, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xF0, 0xFF, 0xFF, 0x6F, 
+	0x00, 0x00, 0x00, 0x00, 0xC6, 0x02, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x61, 0x72, 
+	0x63, 0x68, 0x2F, 0x78, 0x38, 0x36, 0x2F, 0x76, 0x64, 0x73, 
+	0x6F, 0x2F, 0x76, 0x63, 0x6C, 0x6F, 0x63, 0x6B, 0x5F, 0x67, 
+	0x65, 0x74, 0x74, 0x69, 0x6D, 0x65, 0x2E, 0x63, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x4C, 0x69, 0x6E, 0x75, 0x78, 0x00, 
+	0x00, 0x00, 0x04, 0x11, 0x03, 0x00, 0x04, 0x00, 0x00, 0x00, 
+	0x14, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x47, 0x4E, 
+	0x55, 0x00, 0x16, 0x13, 0xE2, 0x54, 0x2F, 0xDA, 0x4E, 0x5D, 
+	0x0B, 0x51, 0xA2, 0xD5, 0x2E, 0x5D, 0x9A, 0xB5, 0xB1, 0x88, 
+	0xF8, 0x30, 0x01, 0x1B, 0x03, 0x3B, 0x28, 0x00, 0x00, 0x00, 
+	0x04, 0x00, 0x00, 0x00, 0x1C, 0x01, 0x00, 0x00, 0x44, 0x00, 
+	0x00, 0x00, 0xAC, 0x06, 0x00, 0x00, 0x94, 0x00, 0x00, 0x00, 
+	0x6C, 0x09, 0x00, 0x00, 0xCC, 0x00, 0x00, 0x00, 0x8C, 0x09, 
+	0x00, 0x00, 0xEC, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x01, 0x7A, 0x52, 0x00, 0x01, 0x78, 
+	0x10, 0x01, 0x1B, 0x0C, 0x07, 0x08, 0x90, 0x01, 0x00, 0x00, 
+	0x4C, 0x00, 0x00, 0x00, 0x1C, 0x00, 0x00, 0x00, 0xD0, 0x00, 
+	0x00, 0x00, 0x83, 0x05, 0x00, 0x00, 0x00, 0x41, 0x0E, 0x10, 
+	0x86, 0x02, 0x43, 0x0D, 0x06, 0x4D, 0x8F, 0x03, 0x8E, 0x04, 
+	0x8D, 0x05, 0x8C, 0x06, 0x83, 0x07, 0x02, 0x4F, 0x0A, 0xC3, 
+	0x42, 0xCC, 0x42, 0xCD, 0x42, 0xCE, 0x42, 0xCF, 0x41, 0xC6, 
+	0x0C, 0x07, 0x08, 0x47, 0x0B, 0x02, 0x5F, 0x0A, 0xC3, 0x42, 
+	0xCC, 0x42, 0xCD, 0x42, 0xCE, 0x42, 0xCF, 0x41, 0xC6, 0x0C, 
+	0x07, 0x08, 0x48, 0x0B, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x34, 0x00, 0x00, 0x00, 0x6C, 0x00, 0x00, 0x00, 0x10, 0x06, 
+	0x00, 0x00, 0xBF, 0x02, 0x00, 0x00, 0x00, 0x41, 0x0E, 0x10, 
+	0x86, 0x02, 0x43, 0x0D, 0x06, 0x4D, 0x8F, 0x03, 0x8E, 0x04, 
+	0x8D, 0x05, 0x8C, 0x06, 0x83, 0x07, 0x02, 0xEB, 0x0A, 0xC3, 
+	0x42, 0xCC, 0x42, 0xCD, 0x42, 0xCE, 0x42, 0xCF, 0x41, 0xC6, 
+	0x0C, 0x07, 0x08, 0x4B, 0x0B, 0x00, 0x1C, 0x00, 0x00, 0x00, 
+	0xA4, 0x00, 0x00, 0x00, 0x98, 0x08, 0x00, 0x00, 0x15, 0x00, 
+	0x00, 0x00, 0x00, 0x41, 0x0E, 0x10, 0x86, 0x02, 0x4D, 0x0D, 
+	0x06, 0x46, 0xC6, 0x0C, 0x07, 0x08, 0x00, 0x00, 0x24, 0x00, 
+	0x00, 0x00, 0xC4, 0x00, 0x00, 0x00, 0x98, 0x08, 0x00, 0x00, 
+	0x3D, 0x00, 0x00, 0x00, 0x00, 0x48, 0x0E, 0x10, 0x86, 0x02, 
+	0x43, 0x0D, 0x06, 0x65, 0x0A, 0xC6, 0x0C, 0x07, 0x08, 0x48, 
+	0x0B, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x55, 0x48, 0x89, 0xE5, 
+	0x41, 0x57, 0x41, 0x56, 0x41, 0x55, 0x41, 0x54, 0x53, 0x48, 
+	0x83, 0xEC, 0x20, 0x83, 0xFF, 0x01, 0x0F, 0x84, 0x86, 0x01, 
+	0x00, 0x00, 0x0F, 0x8E, 0xC0, 0x00, 0x00, 0x00, 0x83, 0xFF, 
+	0x05, 0x74, 0x53, 0x83, 0xFF, 0x06, 0x0F, 0x85, 0x92, 0x00, 
+	0x00, 0x00, 0x8B, 0x05, 0x7C, 0xD7, 0xFF, 0xFF, 0xA8, 0x01, 
+	0x0F, 0x85, 0xF4, 0x04, 0x00, 0x00, 0x48, 0x8B, 0x15, 0xBD, 
+	0xD7, 0xFF, 0xFF, 0x48, 0x89, 0x16, 0x48, 0x8B, 0x15, 0xBB, 
+	0xD7, 0xFF, 0xFF, 0x48, 0x89, 0x56, 0x08, 0x39, 0x05, 0x59, 
+	0xD7, 0xFF, 0xFF, 0x75, 0xD5, 0x48, 0x83, 0xC4, 0x20, 0x31, 
+	0xC0, 0x5B, 0x41, 0x5C, 0x41, 0x5D, 0x41, 0x5E, 0x41, 0x5F, 
+	0x5D, 0xC3, 0x66, 0x0F, 0x1F, 0x44, 0x00, 0x00, 0xF3, 0x90, 
+	0x66, 0x0F, 0x1F, 0x44, 0x00, 0x00, 0x8B, 0x05, 0x32, 0xD7, 
+	0xFF, 0xFF, 0xA8, 0x01, 0x75, 0xEE, 0x48, 0x8B, 0x15, 0x67, 
+	0xD7, 0xFF, 0xFF, 0x48, 0x89, 0x16, 0x48, 0x8B, 0x15, 0x65, 
+	0xD7, 0xFF, 0xFF, 0x48, 0x89, 0x56, 0x08, 0x39, 0x05, 0x13, 
+	0xD7, 0xFF, 0xFF, 0x74, 0xBA, 0xEB, 0xD7, 0x0F, 0x1F, 0x80, 
+	0x00, 0x00, 0x00, 0x00, 0x31, 0xD2, 0x48, 0x01, 0x16, 0x45, 
+	0x85, 0xFF, 0x48, 0x89, 0x46, 0x08, 0x75, 0xA3, 0x66, 0x2E, 
+	0x0F, 0x1F, 0x84, 0x00, 0x00, 0x00, 0x00, 0x00, 0x48, 0x63, 
+	0xFF, 0xB8, 0xE4, 0x00, 0x00, 0x00, 0x0F, 0x05, 0x48, 0x83, 
+	0xC4, 0x20, 0x5B, 0x41, 0x5C, 0x41, 0x5D, 0x41, 0x5E, 0x41, 
+	0x5F, 0x5D, 0xC3, 0x0F, 0x1F, 0x80, 0x00, 0x00, 0x00, 0x00, 
+	0x85, 0xFF, 0x75, 0xDC, 0xB8, 0x7B, 0x00, 0x00, 0x00, 0x49, 
+	0xC7, 0xC4, 0x00, 0xF0, 0x7F, 0xFF, 0x45, 0x31, 0xF6, 0x44, 
+	0x0F, 0x03, 0xE8, 0x45, 0x89, 0xEB, 0x0F, 0x03, 0xD8, 0x44, 
+	0x8B, 0x15, 0xAC, 0xD6, 0xFF, 0xFF, 0x41, 0xF6, 0xC2, 0x01, 
+	0x0F, 0x85, 0x12, 0x04, 0x00, 0x00, 0x48, 0x8B, 0x05, 0xC3, 
+	0xD6, 0xFF, 0xFF, 0x44, 0x8B, 0x3D, 0x98, 0xD6, 0xFF, 0xFF, 
+	0x48, 0x89, 0x06, 0x8B, 0x15, 0x8F, 0xD6, 0xFF, 0xFF, 0x4C, 
+	0x8B, 0x0D, 0xA4, 0xD6, 0xFF, 0xFF, 0x83, 0xFA, 0x01, 0x0F, 
+	0x84, 0x6B, 0x01, 0x00, 0x00, 0x83, 0xFA, 0x02, 0x0F, 0x84, 
+	0x3A, 0x01, 0x00, 0x00, 0x31, 0xC0, 0x83, 0xFA, 0x03, 0x0F, 
+	0x84, 0xBF, 0x02, 0x00, 0x00, 0x8B, 0x0D, 0x7D, 0xD6, 0xFF, 
+	0xFF, 0x44, 0x3B, 0x15, 0x5A, 0xD6, 0xFF, 0xFF, 0x75, 0xA5, 
+	0x4C, 0x01, 0xC8, 0x48, 0xD3, 0xE8, 0x48, 0x3D, 0xFF, 0xC9, 
+	0x9A, 0x3B, 0x0F, 0x86, 0x3E, 0xFF, 0xFF, 0xFF, 0x31, 0xD2, 
+	0x0F, 0x1F, 0x40, 0x00, 0x48, 0x2D, 0x00, 0xCA, 0x9A, 0x3B, 
+	0x83, 0xC2, 0x01, 0x48, 0x3D, 0xFF, 0xC9, 0x9A, 0x3B, 0x77, 
+	0xEF, 0x48, 0x01, 0x16, 0x45, 0x85, 0xFF, 0x48, 0x89, 0x46, 
+	0x08, 0x0F, 0x84, 0x2F, 0xFF, 0xFF, 0xFF, 0xE9, 0xC3, 0xFE, 
+	0xFF, 0xFF, 0x66, 0x2E, 0x0F, 0x1F, 0x84, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0xB8, 0x7B, 0x00, 0x00, 0x00, 0x49, 0xC7, 0xC4, 
+	0x00, 0xF0, 0x7F, 0xFF, 0x45, 0x31, 0xF6, 0x44, 0x0F, 0x03, 
+	0xE8, 0x45, 0x89, 0xEB, 0x0F, 0x03, 0xD8, 0x44, 0x8B, 0x15, 
+	0xF0, 0xD5, 0xFF, 0xFF, 0x41, 0xF6, 0xC2, 0x01, 0x0F, 0x85, 
+	0x46, 0x03, 0x00, 0x00, 0x48, 0x8B, 0x05, 0x0F, 0xD6, 0xFF, 
+	0xFF, 0x44, 0x8B, 0x3D, 0xDC, 0xD5, 0xFF, 0xFF, 0x48, 0x89, 
+	0x06, 0x8B, 0x15, 0xD3, 0xD5, 0xFF, 0xFF, 0x4C, 0x8B, 0x0D, 
+	0x00, 0xD6, 0xFF, 0xFF, 0x83, 0xFA, 0x01, 0x0F, 0x84, 0xDF, 
+	0x00, 0x00, 0x00, 0x83, 0xFA, 0x02, 0x74, 0x5A, 0x31, 0xC0, 
+	0x83, 0xFA, 0x03, 0x0F, 0x84, 0xFF, 0x00, 0x00, 0x00, 0x8B, 
+	0x0D, 0xC5, 0xD5, 0xFF, 0xFF, 0x44, 0x3B, 0x15, 0xA2, 0xD5, 
+	0xFF, 0xFF, 0x75, 0xA9, 0x4C, 0x01, 0xC8, 0x48, 0xD3, 0xE8, 
+	0x48, 0x3D, 0xFF, 0xC9, 0x9A, 0x3B, 0x0F, 0x86, 0x86, 0xFE, 
+	0xFF, 0xFF, 0x31, 0xD2, 0x0F, 0x1F, 0x40, 0x00, 0x48, 0x2D, 
+	0x00, 0xCA, 0x9A, 0x3B, 0x83, 0xC2, 0x01, 0x48, 0x3D, 0xFF, 
+	0xC9, 0x9A, 0x3B, 0x77, 0xEF, 0x48, 0x01, 0x16, 0x45, 0x85, 
+	0xFF, 0x48, 0x89, 0x46, 0x08, 0x0F, 0x84, 0x77, 0xFE, 0xFF, 
+	0xFF, 0xE9, 0x0B, 0xFE, 0xFF, 0xFF, 0x66, 0x90, 0x44, 0x8B, 
+	0x2D, 0xC9, 0xE5, 0xFF, 0xFF, 0x4C, 0x2B, 0x2D, 0x5A, 0xD5, 
+	0xFF, 0xFF, 0x4C, 0x23, 0x2D, 0x5B, 0xD5, 0xFF, 0xFF, 0x8B, 
+	0x05, 0x5D, 0xD5, 0xFF, 0xFF, 0x49, 0x0F, 0xAF, 0xC5, 0xEB, 
+	0x90, 0x0F, 0x1F, 0x80, 0x00, 0x00, 0x00, 0x00, 0x44, 0x8B, 
+	0x2D, 0xA1, 0xE5, 0xFF, 0xFF, 0x4C, 0x2B, 0x2D, 0x32, 0xD5, 
+	0xFF, 0xFF, 0x4C, 0x23, 0x2D, 0x33, 0xD5, 0xFF, 0xFF, 0x8B, 
+	0x05, 0x35, 0xD5, 0xFF, 0xFF, 0x49, 0x0F, 0xAF, 0xC5, 0xE9, 
+	0xAD, 0xFE, 0xFF, 0xFF, 0x0F, 0x1F, 0x40, 0x00, 0x66, 0x66, 
+	0x90, 0x66, 0x66, 0x90, 0x0F, 0x31, 0x49, 0x89, 0xD5, 0x89, 
+	0xC0, 0x49, 0xC1, 0xE5, 0x20, 0x49, 0x09, 0xC5, 0x48, 0x8B, 
+	0x05, 0xFD, 0xD4, 0xFF, 0xFF, 0x49, 0x39, 0xC5, 0x0F, 0x82, 
+	0xB7, 0x02, 0x00, 0x00, 0x49, 0x29, 0xC5, 0xEB, 0xBD, 0x0F, 
+	0x1F, 0x80, 0x00, 0x00, 0x00, 0x00, 0x66, 0x66, 0x90, 0x66, 
+	0x66, 0x90, 0x0F, 0x31, 0x49, 0x89, 0xD5, 0x89, 0xC0, 0x49, 
+	0xC1, 0xE5, 0x20, 0x49, 0x09, 0xC5, 0x48, 0x8B, 0x05, 0xCD, 
+	0xD4, 0xFF, 0xFF, 0x49, 0x39, 0xC5, 0x0F, 0x82, 0x7F, 0x02, 
+	0x00, 0x00, 0x49, 0x29, 0xC5, 0xE9, 0x62, 0xFF, 0xFF, 0xFF, 
+	0x0F, 0x1F, 0x40, 0x00, 0x44, 0x8B, 0x2D, 0x39, 0xD4, 0xFF, 
+	0xFF, 0x48, 0x89, 0x75, 0xB8, 0x41, 0x83, 0xFD, 0x01, 0x89, 
+	0xD9, 0x0F, 0x84, 0x28, 0x02, 0x00, 0x00, 0x89, 0xCE, 0x83, 
+	0xE1, 0x3F, 0x81, 0xE6, 0xFF, 0x0F, 0x00, 0x00, 0x89, 0xF0, 
+	0x48, 0xC1, 0xE8, 0x06, 0x05, 0x00, 0x02, 0x00, 0x00, 0x3D, 
+	0x03, 0x02, 0x00, 0x00, 0x0F, 0x8F, 0x1F, 0x02, 0x00, 0x00, 
+	0xC1, 0xE0, 0x0C, 0x4D, 0x89, 0xE0, 0x48, 0x63, 0xC9, 0x48, 
+	0x98, 0x48, 0xC1, 0xE1, 0x06, 0x49, 0x29, 0xC0, 0x49, 0x01, 
+	0xC8, 0x41, 0x8B, 0x00, 0x89, 0x45, 0xD4, 0x66, 0x66, 0x90, 
+	0x66, 0x66, 0x90, 0x0F, 0x31, 0x48, 0xC1, 0xE2, 0x20, 0x89, 
+	0xC0, 0x41, 0x0F, 0xBE, 0x48, 0x1C, 0x48, 0x09, 0xD0, 0x49, 
+	0x2B, 0x40, 0x08, 0x41, 0x8B, 0x50, 0x18, 0x49, 0x89, 0xC5, 
+	0x49, 0xD3, 0xE5, 0x85, 0xC9, 0x0F, 0x88, 0xDA, 0x01, 0x00, 
+	0x00, 0x4C, 0x89, 0xE8, 0x48, 0xF7, 0xE2, 0x48, 0x0F, 0xAC, 
+	0xD0, 0x20, 0x48, 0x89, 0x45, 0xC0, 0x49, 0x8B, 0x40, 0x10, 
+	0x48, 0x89, 0x45, 0xC8, 0x41, 0x0F, 0xB6, 0x40, 0x1D, 0x88, 
+	0x45, 0xD3, 0x66, 0x66, 0x90, 0x66, 0x66, 0x90, 0x44, 0x8B, 
+	0x2D, 0x95, 0xD3, 0xFF, 0xFF, 0x44, 0x89, 0xD9, 0x41, 0x83, 
+	0xFD, 0x01, 0x0F, 0x84, 0x8F, 0x01, 0x00, 0x00, 0x81, 0xE1, 
+	0xFF, 0x0F, 0x00, 0x00, 0x39, 0xCE, 0x0F, 0x85, 0x45, 0xFF, 
+	0xFF, 0xFF, 0x41, 0x8B, 0x00, 0xA8, 0x01, 0x0F, 0x85, 0x3A, 
+	0xFF, 0xFF, 0xFF, 0x39, 0x45, 0xD4, 0x0F, 0x85, 0x31, 0xFF, 
+	0xFF, 0xFF, 0x4C, 0x8B, 0x6D, 0xC8, 0x4C, 0x03, 0x6D, 0xC0, 
+	0xF6, 0x45, 0xD3, 0x01, 0x48, 0x8B, 0x05, 0xCB, 0xD3, 0xFF, 
+	0xFF, 0x48, 0x8B, 0x75, 0xB8, 0x45, 0x0F, 0x44, 0xFE, 0x49, 
+	0x39, 0xC5, 0x4C, 0x0F, 0x42, 0xE8, 0xE9, 0xF3, 0xFE, 0xFF, 
+	0xFF, 0x0F, 0x1F, 0x80, 0x00, 0x00, 0x00, 0x00, 0x44, 0x8B, 
+	0x2D, 0x31, 0xD3, 0xFF, 0xFF, 0x48, 0x89, 0x75, 0xB8, 0x41, 
+	0x83, 0xFD, 0x01, 0x89, 0xD9, 0x0F, 0x84, 0x18, 0x01, 0x00, 
+	0x00, 0x89, 0xCE, 0x83, 0xE1, 0x3F, 0x81, 0xE6, 0xFF, 0x0F, 
+	0x00, 0x00, 0x89, 0xF0, 0x48, 0xC1, 0xE8, 0x06, 0x05, 0x00, 
+	0x02, 0x00, 0x00, 0x3D, 0x03, 0x02, 0x00, 0x00, 0x0F, 0x8F, 
+	0x17, 0x01, 0x00, 0x00, 0xC1, 0xE0, 0x0C, 0x4D, 0x89, 0xE0, 
+	0x48, 0x63, 0xC9, 0x48, 0x98, 0x48, 0xC1, 0xE1, 0x06, 0x49, 
+	0x29, 0xC0, 0x49, 0x01, 0xC8, 0x41, 0x8B, 0x00, 0x89, 0x45, 
+	0xD4, 0x66, 0x66, 0x90, 0x66, 0x66, 0x90, 0x0F, 0x31, 0x48, 
+	0xC1, 0xE2, 0x20, 0x89, 0xC0, 0x41, 0x0F, 0xBE, 0x48, 0x1C, 
+	0x48, 0x09, 0xD0, 0x49, 0x2B, 0x40, 0x08, 0x41, 0x8B, 0x50, 
+	0x18, 0x49, 0x89, 0xC5, 0x49, 0xD3, 0xE5, 0x85, 0xC9, 0x0F, 
+	0x88, 0xDF, 0x00, 0x00, 0x00, 0x4C, 0x89, 0xE8, 0x48, 0xF7, 
+	0xE2, 0x48, 0x0F, 0xAC, 0xD0, 0x20, 0x48, 0x89, 0x45, 0xC0, 
+	0x49, 0x8B, 0x40, 0x10, 0x48, 0x89, 0x45, 0xC8, 0x41, 0x0F, 
+	0xB6, 0x40, 0x1D, 0x88, 0x45, 0xD3, 0x66, 0x66, 0x90, 0x66, 
+	0x66, 0x90, 0x44, 0x8B, 0x2D, 0x8D, 0xD2, 0xFF, 0xFF, 0x44, 
+	0x89, 0xD9, 0x41, 0x83, 0xFD, 0x01, 0x0F, 0x84, 0x8F, 0x00, 
+	0x00, 0x00, 0x81, 0xE1, 0xFF, 0x0F, 0x00, 0x00, 0x39, 0xCE, 
+	0x0F, 0x85, 0x45, 0xFF, 0xFF, 0xFF, 0x41, 0x8B, 0x00, 0xA8, 
+	0x01, 0x0F, 0x85, 0x3A, 0xFF, 0xFF, 0xFF, 0x39, 0x45, 0xD4, 
+	0x0F, 0x85, 0x31, 0xFF, 0xFF, 0xFF, 0x4C, 0x8B, 0x6D, 0xC8, 
+	0x4C, 0x03, 0x6D, 0xC0, 0xF6, 0x45, 0xD3, 0x01, 0x48, 0x8B, 
+	0x05, 0xC3, 0xD2, 0xFF, 0xFF, 0x48, 0x8B, 0x75, 0xB8, 0x45, 
+	0x0F, 0x44, 0xFE, 0x49, 0x39, 0xC5, 0x4C, 0x0F, 0x42, 0xE8, 
+	0xE9, 0xBB, 0xFD, 0xFF, 0xFF, 0x0F, 0x1F, 0x80, 0x00, 0x00, 
+	0x00, 0x00, 0xF3, 0x90, 0xE9, 0xA2, 0xFC, 0xFF, 0xFF, 0x66, 
+	0x0F, 0x1F, 0x84, 0x00, 0x00, 0x00, 0x00, 0x00, 0xF3, 0x90, 
+	0xE9, 0xD6, 0xFB, 0xFF, 0xFF, 0x66, 0x0F, 0x1F, 0x84, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0xF3, 0x90, 0xE9, 0xF7, 0xFA, 0xFF, 
+	0xFF, 0x0F, 0x01, 0xF9, 0xE9, 0xE0, 0xFE, 0xFF, 0xFF, 0x0F, 
+	0x01, 0xF9, 0xE9, 0xD0, 0xFD, 0xFF, 0xFF, 0x0F, 0x01, 0xF9, 
+	0xE9, 0x69, 0xFE, 0xFF, 0xFF, 0x0F, 0x01, 0xF9, 0xE9, 0x69, 
+	0xFF, 0xFF, 0xFF, 0x0F, 0x0B, 0xF7, 0xD9, 0x48, 0xD3, 0xE8, 
+	0x49, 0x89, 0xC5, 0xE9, 0x19, 0xFE, 0xFF, 0xFF, 0xF7, 0xD9, 
+	0x48, 0xD3, 0xE8, 0x49, 0x89, 0xC5, 0xE9, 0x14, 0xFF, 0xFF, 
+	0xFF, 0x45, 0x31, 0xED, 0xE9, 0xE3, 0xFC, 0xFF, 0xFF, 0x45, 
+	0x31, 0xED, 0xE9, 0x03, 0xFD, 0xFF, 0xFF, 0x66, 0x66, 0x66, 
+	0x66, 0x2E, 0x0F, 0x1F, 0x84, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x55, 0x48, 0x89, 0xE5, 0x41, 0x57, 0x41, 0x56, 0x41, 0x55, 
+	0x41, 0x54, 0x53, 0x48, 0x83, 0xEC, 0x20, 0x48, 0x85, 0xFF, 
+	0x0F, 0x84, 0xD2, 0x00, 0x00, 0x00, 0xB8, 0x7B, 0x00, 0x00, 
+	0x00, 0x49, 0xC7, 0xC5, 0x00, 0xF0, 0x7F, 0xFF, 0x45, 0x31, 
+	0xFF, 0x44, 0x0F, 0x03, 0xF0, 0x45, 0x89, 0xF3, 0x44, 0x0F, 
+	0x03, 0xE0, 0x44, 0x8B, 0x15, 0xE5, 0xD1, 0xFF, 0xFF, 0x41, 
+	0xF6, 0xC2, 0x01, 0x0F, 0x85, 0x2B, 0x02, 0x00, 0x00, 0x48, 
+	0x8B, 0x05, 0xFC, 0xD1, 0xFF, 0xFF, 0x8B, 0x1D, 0xD2, 0xD1, 
+	0xFF, 0xFF, 0x48, 0x89, 0x07, 0x8B, 0x15, 0xC9, 0xD1, 0xFF, 
+	0xFF, 0x4C, 0x8B, 0x0D, 0xDE, 0xD1, 0xFF, 0xFF, 0x83, 0xFA, 
+	0x01, 0x0F, 0x84, 0xD5, 0x01, 0x00, 0x00, 0x83, 0xFA, 0x02, 
+	0x0F, 0x84, 0x9C, 0x00, 0x00, 0x00, 0x31, 0xC0, 0x83, 0xFA, 
+	0x03, 0x0F, 0x84, 0xB9, 0x00, 0x00, 0x00, 0x8B, 0x0D, 0xB7, 
+	0xD1, 0xFF, 0xFF, 0x44, 0x3B, 0x15, 0x94, 0xD1, 0xFF, 0xFF, 
+	0x75, 0xA6, 0x4C, 0x01, 0xC8, 0x48, 0xD3, 0xE8, 0x48, 0x3D, 
+	0xFF, 0xC9, 0x9A, 0x3B, 0x48, 0x89, 0xC1, 0x0F, 0x86, 0xF6, 
+	0x01, 0x00, 0x00, 0x31, 0xD2, 0x0F, 0x1F, 0x00, 0x48, 0x81, 
+	0xE9, 0x00, 0xCA, 0x9A, 0x3B, 0x83, 0xC2, 0x01, 0x48, 0x81, 
+	0xF9, 0xFF, 0xC9, 0x9A, 0x3B, 0x77, 0xED, 0x48, 0x01, 0x17, 
+	0x85, 0xDB, 0x48, 0x89, 0x4F, 0x08, 0x0F, 0x84, 0xC3, 0x01, 
+	0x00, 0x00, 0x48, 0x89, 0xC8, 0x48, 0xBA, 0xCF, 0xF7, 0x53, 
+	0xE3, 0xA5, 0x9B, 0xC4, 0x20, 0x48, 0xF7, 0xEA, 0x48, 0x89, 
+	0xC8, 0x48, 0xC1, 0xF8, 0x3F, 0x48, 0xC1, 0xFA, 0x07, 0x48, 
+	0x29, 0xC2, 0x48, 0x89, 0x57, 0x08, 0x31, 0xC0, 0x48, 0x85, 
+	0xF6, 0x0F, 0x85, 0x80, 0x01, 0x00, 0x00, 0x48, 0x83, 0xC4, 
+	0x20, 0x5B, 0x41, 0x5C, 0x41, 0x5D, 0x41, 0x5E, 0x41, 0x5F, 
+	0x5D, 0xC3, 0x66, 0x2E, 0x0F, 0x1F, 0x84, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x44, 0x8B, 0x35, 0x79, 0xE1, 0xFF, 0xFF, 0x4C, 
+	0x2B, 0x35, 0x0A, 0xD1, 0xFF, 0xFF, 0x4C, 0x23, 0x35, 0x0B, 
+	0xD1, 0xFF, 0xFF, 0x8B, 0x05, 0x0D, 0xD1, 0xFF, 0xFF, 0x49, 
+	0x0F, 0xAF, 0xC6, 0xE9, 0x4B, 0xFF, 0xFF, 0xFF, 0x0F, 0x1F, 
+	0x40, 0x00, 0x44, 0x8B, 0x35, 0x71, 0xD0, 0xFF, 0xFF, 0x48, 
+	0x89, 0x75, 0xB8, 0x41, 0x83, 0xFE, 0x01, 0x44, 0x89, 0xE1, 
+	0x0F, 0x84, 0x50, 0x01, 0x00, 0x00, 0x89, 0xCE, 0x83, 0xE1, 
+	0x3F, 0x81, 0xE6, 0xFF, 0x0F, 0x00, 0x00, 0x89, 0xF0, 0x48, 
+	0xC1, 0xE8, 0x06, 0x05, 0x00, 0x02, 0x00, 0x00, 0x3D, 0x03, 
+	0x02, 0x00, 0x00, 0x0F, 0x8F, 0x4C, 0x01, 0x00, 0x00, 0xC1, 
+	0xE0, 0x0C, 0x4D, 0x89, 0xE8, 0x48, 0x63, 0xC9, 0x48, 0x98, 
+	0x48, 0xC1, 0xE1, 0x06, 0x49, 0x29, 0xC0, 0x49, 0x01, 0xC8, 
+	0x41, 0x8B, 0x00, 0x89, 0x45, 0xD4, 0x66, 0x66, 0x90, 0x66, 
+	0x66, 0x90, 0x0F, 0x31, 0x48, 0xC1, 0xE2, 0x20, 0x89, 0xC0, 
+	0x41, 0x0F, 0xBE, 0x48, 0x1C, 0x48, 0x09, 0xD0, 0x49, 0x2B, 
+	0x40, 0x08, 0x41, 0x8B, 0x50, 0x18, 0x49, 0x89, 0xC6, 0x49, 
+	0xD3, 0xE6, 0x85, 0xC9, 0x0F, 0x88, 0xF8, 0x00, 0x00, 0x00, 
+	0x4C, 0x89, 0xF0, 0x48, 0xF7, 0xE2, 0x48, 0x0F, 0xAC, 0xD0, 
+	0x20, 0x48, 0x89, 0x45, 0xC0, 0x49, 0x8B, 0x40, 0x10, 0x48, 
+	0x89, 0x45, 0xC8, 0x41, 0x0F, 0xB6, 0x40, 0x1D, 0x88, 0x45, 
+	0xD3, 0x66, 0x66, 0x90, 0x66, 0x66, 0x90, 0x44, 0x8B, 0x35, 
+	0xCC, 0xCF, 0xFF, 0xFF, 0x44, 0x89, 0xD9, 0x41, 0x83, 0xFE, 
+	0x01, 0x0F, 0x84, 0xB7, 0x00, 0x00, 0x00, 0x81, 0xE1, 0xFF, 
+	0x0F, 0x00, 0x00, 0x39, 0xCE, 0x0F, 0x85, 0x44, 0xFF, 0xFF, 
+	0xFF, 0x41, 0x8B, 0x00, 0xA8, 0x01, 0x0F, 0x85, 0x39, 0xFF, 
+	0xFF, 0xFF, 0x39, 0x45, 0xD4, 0x0F, 0x85, 0x30, 0xFF, 0xFF, 
+	0xFF, 0x4C, 0x8B, 0x75, 0xC8, 0x4C, 0x03, 0x75, 0xC0, 0xF6, 
+	0x45, 0xD3, 0x01, 0x48, 0x8B, 0x05, 0x02, 0xD0, 0xFF, 0xFF, 
+	0x48, 0x8B, 0x75, 0xB8, 0x41, 0x0F, 0x44, 0xDF, 0x49, 0x39, 
+	0xC6, 0x4C, 0x0F, 0x42, 0xF0, 0x49, 0x29, 0xC6, 0xE9, 0xE1, 
+	0xFE, 0xFF, 0xFF, 0x0F, 0x1F, 0x00, 0x66, 0x66, 0x90, 0x66, 
+	0x66, 0x90, 0x0F, 0x31, 0x49, 0x89, 0xD6, 0x89, 0xC0, 0x49, 
+	0xC1, 0xE6, 0x20, 0x49, 0x09, 0xC6, 0x48, 0x8B, 0x05, 0xCD, 
+	0xCF, 0xFF, 0xFF, 0x49, 0x39, 0xC6, 0x73, 0xD5, 0x45, 0x31, 
+	0xF6, 0xE9, 0xB6, 0xFE, 0xFF, 0xFF, 0x0F, 0x1F, 0x84, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0xF3, 0x90, 0xE9, 0xBD, 0xFD, 0xFF, 
+	0xFF, 0x8B, 0x15, 0x03, 0xD0, 0xFF, 0xFF, 0x89, 0x16, 0x8B, 
+	0x15, 0xFF, 0xCF, 0xFF, 0xFF, 0x89, 0x56, 0x04, 0xE9, 0x6A, 
+	0xFE, 0xFF, 0xFF, 0xB8, 0x60, 0x00, 0x00, 0x00, 0x0F, 0x05, 
+	0xE9, 0x5E, 0xFE, 0xFF, 0xFF, 0x31, 0xD2, 0xE9, 0x1B, 0xFE, 
+	0xFF, 0xFF, 0x0F, 0x01, 0xF9, 0xE9, 0xA8, 0xFE, 0xFF, 0xFF, 
+	0x0F, 0x01, 0xF9, 0xE9, 0x41, 0xFF, 0xFF, 0xFF, 0xF7, 0xD9, 
+	0x48, 0xD3, 0xE8, 0x49, 0x89, 0xC6, 0xE9, 0xFB, 0xFE, 0xFF, 
+	0xFF, 0x0F, 0x0B, 0x90, 0x55, 0x48, 0x85, 0xFF, 0x48, 0x8B, 
+	0x05, 0x7D, 0xCF, 0xFF, 0xFF, 0x48, 0x89, 0xE5, 0x74, 0x03, 
+	0x48, 0x89, 0x07, 0x5D, 0xC3, 0x90, 0x90, 0x90, 0x90, 0x90, 
+	0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x83, 0x3D, 0xC9, 0xCE, 
+	0xFF, 0xFF, 0x01, 0x55, 0x48, 0x89, 0xE5, 0x74, 0x2B, 0xB9, 
+	0x7B, 0x00, 0x00, 0x00, 0x0F, 0x03, 0xC9, 0x48, 0x85, 0xFF, 
+	0x74, 0x09, 0x89, 0xC8, 0x25, 0xFF, 0x0F, 0x00, 0x00, 0x89, 
+	0x07, 0x48, 0x85, 0xF6, 0x74, 0x05, 0xC1, 0xE9, 0x0C, 0x89, 
+	0x0E, 0x31, 0xC0, 0x5D, 0xC3, 0x0F, 0x1F, 0x80, 0x00, 0x00, 
+	0x00, 0x00, 0x0F, 0x01, 0xF9, 0xEB, 0xD8, 0xF3, 0xF9, 0xFF, 
+	0xFF, 0xD4, 0x00, 0x00, 0x00, 0x71, 0x00, 0x03, 0x03, 0xEA, 
+	0xF9, 0xFF, 0xFF, 0xCB, 0x00, 0x00, 0x00, 0x72, 0x00, 0x03, 
+	0x03, 0x0B, 0xFA, 0xFF, 0xFF, 0xC2, 0x00, 0x00, 0x00, 0x71, 
+	0x00, 0x03, 0x03, 0x02, 0xFA, 0xFF, 0xFF, 0xB9, 0x00, 0x00, 
+	0x00, 0x72, 0x00, 0x03, 0x03, 0x76, 0xFA, 0xFF, 0xFF, 0xB0, 
+	0x00, 0x00, 0x00, 0x71, 0x00, 0x03, 0x03, 0x6D, 0xFA, 0xFF, 
+	0xFF, 0xA7, 0x00, 0x00, 0x00, 0x72, 0x00, 0x03, 0x03, 0xA9, 
+	0xFA, 0xFF, 0xFF, 0x9E, 0x00, 0x00, 0x00, 0x71, 0x00, 0x03, 
+	0x03, 0xA0, 0xFA, 0xFF, 0xFF, 0x95, 0x00, 0x00, 0x00, 0x72, 
+	0x00, 0x03, 0x03, 0x4E, 0xFB, 0xFF, 0xFF, 0x8C, 0x00, 0x00, 
+	0x00, 0x71, 0x00, 0x03, 0x03, 0x45, 0xFB, 0xFF, 0xFF, 0x83, 
+	0x00, 0x00, 0x00, 0x72, 0x00, 0x03, 0x03, 0x81, 0xFB, 0xFF, 
+	0xFF, 0x7A, 0x00, 0x00, 0x00, 0x71, 0x00, 0x03, 0x03, 0x78, 
+	0xFB, 0xFF, 0xFF, 0x71, 0x00, 0x00, 0x00, 0x72, 0x00, 0x03, 
+	0x03, 0xDF, 0xFD, 0xFF, 0xFF, 0x68, 0x00, 0x00, 0x00, 0x71, 
+	0x00, 0x03, 0x03, 0xD6, 0xFD, 0xFF, 0xFF, 0x5F, 0x00, 0x00, 
+	0x00, 0x72, 0x00, 0x03, 0x03, 0x12, 0xFE, 0xFF, 0xFF, 0x56, 
+	0x00, 0x00, 0x00, 0x71, 0x00, 0x03, 0x03, 0x09, 0xFE, 0xFF, 
+	0xFF, 0x4D, 0x00, 0x00, 0x00, 0x72, 0x00, 0x03, 0x03, 0x63, 
+	0xFE, 0xFF, 0xFF, 0x44, 0x00, 0x00, 0x00, 0x71, 0x00, 0x03, 
+	0x03, 0x5A, 0xFE, 0xFF, 0xFF, 0x3B, 0x00, 0x00, 0x00, 0x72, 
+	0x00, 0x03, 0x03, 0x0F, 0xAE, 0xF0, 0x0F, 0xAE, 0xE8, 0x0F, 
+	0xAE, 0xF0, 0x0F, 0xAE, 0xE8, 0x0F, 0xAE, 0xF0, 0x0F, 0xAE, 
+	0xE8, 0x0F, 0xAE, 0xF0, 0x0F, 0xAE, 0xE8, 0x0F, 0xAE, 0xF0, 
+	0x0F, 0xAE, 0xE8, 0x0F, 0xAE, 0xF0, 0x0F, 0xAE, 0xE8, 0x0F, 
+	0xAE, 0xF0, 0x0F, 0xAE, 0xE8, 0x0F, 0xAE, 0xF0, 0x0F, 0xAE, 
+	0xE8, 0x0F, 0xAE, 0xF0, 0x0F, 0xAE, 0xE8, 0x47, 0x43, 0x43, 
+	0x3A, 0x20, 0x28, 0x55, 0x62, 0x75, 0x6E, 0x74, 0x75, 0x20, 
+	0x34, 0x2E, 0x39, 0x2E, 0x31, 0x2D, 0x31, 0x36, 0x75, 0x62, 
+	0x75, 0x6E, 0x74, 0x75, 0x36, 0x29, 0x20, 0x34, 0x2E, 0x39, 
+	0x2E, 0x31, 0x00, 0x00, 0x2E, 0x73, 0x68, 0x73, 0x74, 0x72, 
+	0x74, 0x61, 0x62, 0x00, 0x2E, 0x68, 0x61, 0x73, 0x68, 0x00, 
+	0x2E, 0x64, 0x79, 0x6E, 0x73, 0x79, 0x6D, 0x00, 0x2E, 0x64, 
+	0x79, 0x6E, 0x73, 0x74, 0x72, 0x00, 0x2E, 0x67, 0x6E, 0x75, 
+	0x2E, 0x76, 0x65, 0x72, 0x73, 0x69, 0x6F, 0x6E, 0x00, 0x2E, 
+	0x67, 0x6E, 0x75, 0x2E, 0x76, 0x65, 0x72, 0x73, 0x69, 0x6F, 
+	0x6E, 0x5F, 0x64, 0x00, 0x2E, 0x64, 0x79, 0x6E, 0x61, 0x6D, 
+	0x69, 0x63, 0x00, 0x2E, 0x72, 0x6F, 0x64, 0x61, 0x74, 0x61, 
+	0x00, 0x2E, 0x6E, 0x6F, 0x74, 0x65, 0x00, 0x2E, 0x65, 0x68, 
+	0x5F, 0x66, 0x72, 0x61, 0x6D, 0x65, 0x5F, 0x68, 0x64, 0x72, 
+	0x00, 0x2E, 0x65, 0x68, 0x5F, 0x66, 0x72, 0x61, 0x6D, 0x65, 
+	0x00, 0x2E, 0x74, 0x65, 0x78, 0x74, 0x00, 0x2E, 0x61, 0x6C, 
+	0x74, 0x69, 0x6E, 0x73, 0x74, 0x72, 0x75, 0x63, 0x74, 0x69, 
+	0x6F, 0x6E, 0x73, 0x00, 0x2E, 0x61, 0x6C, 0x74, 0x69, 0x6E, 
+	0x73, 0x74, 0x72, 0x5F, 0x72, 0x65, 0x70, 0x6C, 0x61, 0x63, 
+	0x65, 0x6D, 0x65, 0x6E, 0x74, 0x00, 0x2E, 0x63, 0x6F, 0x6D, 
+	0x6D, 0x65, 0x6E, 0x74, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0B, 0x00, 0x00, 0x00, 
+	0x05, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x20, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x20, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x40, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x11, 0x00, 0x00, 0x00, 0x0B, 0x00, 0x00, 0x00, 0x02, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x60, 0x01, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x60, 0x01, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x08, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x03, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x08, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x18, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x19, 0x00, 0x00, 0x00, 0x03, 0x00, 
+	0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x68, 0x02, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x68, 0x02, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x5E, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x21, 0x00, 
+	0x00, 0x00, 0xFF, 0xFF, 0xFF, 0x6F, 0x02, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0xC6, 0x02, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0xC6, 0x02, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x16, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x2E, 0x00, 0x00, 0x00, 0xFD, 0xFF, 0xFF, 0x6F, 
+	0x02, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xE0, 0x02, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xE0, 0x02, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x38, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 
+	0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x3D, 0x00, 0x00, 0x00, 
+	0x06, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x18, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x18, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x46, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x03, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x18, 0x04, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x18, 0x04, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x60, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x08, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x4E, 0x00, 0x00, 0x00, 0x07, 0x00, 
+	0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x78, 0x07, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x78, 0x07, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x3C, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x54, 0x00, 
+	0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0xB4, 0x07, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0xB4, 0x07, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x2C, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x62, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xE0, 0x07, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xE0, 0x07, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0xE8, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x6C, 0x00, 0x00, 0x00, 
+	0x01, 0x00, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0xD0, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0xD0, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xAD, 0x08, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x72, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x02, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x7D, 0x11, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x7D, 0x11, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0xD8, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x83, 0x00, 0x00, 0x00, 0x01, 0x00, 
+	0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x55, 0x12, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x55, 0x12, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x36, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x99, 0x00, 
+	0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x30, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x8B, 0x12, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x24, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xAF, 0x12, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0xA2, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+};
+
+static struct page *pages[2];
+
+const struct vdso_image vdso_image_64 = {
+	.data = raw_data,
+	.size = 8192,
+	.text_mapping = {
+		.name = "[vdso]",
+		.pages = pages,
+	},
+	.alt = 4477,
+	.alt_len = 216,
+	.sym_vvar_start = -8192,
+	.sym_vvar_page = -8192,
+	.sym_hpet_page = -4096,
+};
diff -urN a/arch/x86/vdso/vdso-image-x32.c b/arch/x86/vdso/vdso-image-x32.c
--- a/arch/x86/vdso/vdso-image-x32.c	1969-12-31 17:00:00.000000000 -0700
+++ b/arch/x86/vdso/vdso-image-x32.c	2014-11-22 18:41:11.657004289 -0700
@@ -0,0 +1,510 @@
+/* AUTOMATICALLY GENERATED -- DO NOT EDIT */
+
+#include <linux/linkage.h>
+#include <asm/page_types.h>
+#include <asm/vdso.h>
+
+static unsigned char raw_data[8192] __page_aligned_data = {
+	0x7F, 0x45, 0x4C, 0x46, 0x01, 0x01, 0x01, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x00, 0x3E, 0x00, 
+	0x01, 0x00, 0x00, 0x00, 0xF0, 0x05, 0x00, 0x00, 0x34, 0x00, 
+	0x00, 0x00, 0x74, 0x10, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x34, 0x00, 0x20, 0x00, 0x04, 0x00, 0x28, 0x00, 0x10, 0x00, 
+	0x0F, 0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xAB, 0x0F, 
+	0x00, 0x00, 0xAB, 0x0F, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00, 
+	0x00, 0x10, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0xF8, 0x01, 
+	0x00, 0x00, 0xF8, 0x01, 0x00, 0x00, 0xF8, 0x01, 0x00, 0x00, 
+	0x80, 0x00, 0x00, 0x00, 0x80, 0x00, 0x00, 0x00, 0x04, 0x00, 
+	0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 
+	0xA0, 0x04, 0x00, 0x00, 0xA0, 0x04, 0x00, 0x00, 0xA0, 0x04, 
+	0x00, 0x00, 0x3C, 0x00, 0x00, 0x00, 0x3C, 0x00, 0x00, 0x00, 
+	0x04, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x50, 0xE5, 
+	0x74, 0x64, 0xDC, 0x04, 0x00, 0x00, 0xDC, 0x04, 0x00, 0x00, 
+	0xDC, 0x04, 0x00, 0x00, 0x2C, 0x00, 0x00, 0x00, 0x2C, 0x00, 
+	0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 
+	0x03, 0x00, 0x00, 0x00, 0x07, 0x00, 0x00, 0x00, 0x06, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x78, 0x02, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x00, 0x07, 0x00, 
+	0x54, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x11, 0x00, 0xF1, 0xFF, 0x16, 0x00, 0x00, 0x00, 
+	0x80, 0x0B, 0x00, 0x00, 0xBF, 0x02, 0x00, 0x00, 0x12, 0x00, 
+	0x0B, 0x00, 0x36, 0x00, 0x00, 0x00, 0x60, 0x0E, 0x00, 0x00, 
+	0x3D, 0x00, 0x00, 0x00, 0x12, 0x00, 0x0B, 0x00, 0x01, 0x00, 
+	0x00, 0x00, 0xF0, 0x05, 0x00, 0x00, 0x83, 0x05, 0x00, 0x00, 
+	0x12, 0x00, 0x0B, 0x00, 0x2A, 0x00, 0x00, 0x00, 0x40, 0x0E, 
+	0x00, 0x00, 0x15, 0x00, 0x00, 0x00, 0x12, 0x00, 0x0B, 0x00, 
+	0x00, 0x5F, 0x5F, 0x76, 0x64, 0x73, 0x6F, 0x5F, 0x63, 0x6C, 
+	0x6F, 0x63, 0x6B, 0x5F, 0x67, 0x65, 0x74, 0x74, 0x69, 0x6D, 
+	0x65, 0x00, 0x5F, 0x5F, 0x76, 0x64, 0x73, 0x6F, 0x5F, 0x67, 
+	0x65, 0x74, 0x74, 0x69, 0x6D, 0x65, 0x6F, 0x66, 0x64, 0x61, 
+	0x79, 0x00, 0x5F, 0x5F, 0x76, 0x64, 0x73, 0x6F, 0x5F, 0x74, 
+	0x69, 0x6D, 0x65, 0x00, 0x5F, 0x5F, 0x76, 0x64, 0x73, 0x6F, 
+	0x5F, 0x67, 0x65, 0x74, 0x63, 0x70, 0x75, 0x00, 0x6C, 0x69, 
+	0x6E, 0x75, 0x78, 0x2D, 0x76, 0x64, 0x73, 0x6F, 0x2E, 0x73, 
+	0x6F, 0x2E, 0x31, 0x00, 0x4C, 0x49, 0x4E, 0x55, 0x58, 0x5F, 
+	0x32, 0x2E, 0x36, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0x00, 
+	0x02, 0x00, 0x02, 0x00, 0x02, 0x00, 0x02, 0x00, 0x01, 0x00, 
+	0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0xA1, 0xBF, 0xEE, 0x0D, 
+	0x14, 0x00, 0x00, 0x00, 0x1C, 0x00, 0x00, 0x00, 0x44, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x01, 0x00, 0xF6, 0x75, 0xAE, 0x03, 0x14, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x54, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x0E, 0x00, 0x00, 0x00, 0x44, 0x00, 
+	0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x04, 0x00, 0x00, 0x00, 0xB4, 0x00, 0x00, 0x00, 0x05, 0x00, 
+	0x00, 0x00, 0x54, 0x01, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 
+	0xE4, 0x00, 0x00, 0x00, 0x0A, 0x00, 0x00, 0x00, 0x5E, 0x00, 
+	0x00, 0x00, 0x0B, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 
+	0xFC, 0xFF, 0xFF, 0x6F, 0xC0, 0x01, 0x00, 0x00, 0xFD, 0xFF, 
+	0xFF, 0x6F, 0x02, 0x00, 0x00, 0x00, 0xF0, 0xFF, 0xFF, 0x6F, 
+	0xB2, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x61, 0x72, 0x63, 0x68, 0x2F, 0x78, 0x38, 0x36, 
+	0x2F, 0x76, 0x64, 0x73, 0x6F, 0x2F, 0x76, 0x63, 0x6C, 0x6F, 
+	0x63, 0x6B, 0x5F, 0x67, 0x65, 0x74, 0x74, 0x69, 0x6D, 0x65, 
+	0x2E, 0x63, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 0x04, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x4C, 0x69, 0x6E, 0x75, 
+	0x78, 0x00, 0x00, 0x00, 0x04, 0x11, 0x03, 0x00, 0x04, 0x00, 
+	0x00, 0x00, 0x14, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 
+	0x47, 0x4E, 0x55, 0x00, 0x4F, 0x7F, 0x6C, 0x8B, 0xF1, 0x0E, 
+	0x4D, 0x63, 0x54, 0xE5, 0x6F, 0x43, 0x3D, 0xEC, 0x35, 0x6A, 
+	0x0F, 0x2A, 0xB1, 0xF6, 0x01, 0x1B, 0x03, 0x3B, 0x28, 0x00, 
+	0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x14, 0x01, 0x00, 0x00, 
+	0x44, 0x00, 0x00, 0x00, 0xA4, 0x06, 0x00, 0x00, 0x90, 0x00, 
+	0x00, 0x00, 0x64, 0x09, 0x00, 0x00, 0xC8, 0x00, 0x00, 0x00, 
+	0x84, 0x09, 0x00, 0x00, 0xEC, 0x00, 0x00, 0x00, 0x14, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x7A, 0x52, 0x00, 
+	0x01, 0x78, 0x10, 0x01, 0x1B, 0x0C, 0x07, 0x08, 0x90, 0x01, 
+	0x00, 0x00, 0x48, 0x00, 0x00, 0x00, 0x1C, 0x00, 0x00, 0x00, 
+	0xC8, 0x00, 0x00, 0x00, 0x83, 0x05, 0x00, 0x00, 0x00, 0x41, 
+	0x0E, 0x10, 0x86, 0x02, 0x43, 0x0D, 0x06, 0x4D, 0x8F, 0x03, 
+	0x8E, 0x04, 0x8D, 0x05, 0x8C, 0x06, 0x83, 0x07, 0x02, 0x4F, 
+	0x0A, 0xC3, 0x42, 0xCC, 0x42, 0xCD, 0x42, 0xCE, 0x42, 0xCF, 
+	0x41, 0xC6, 0x0C, 0x07, 0x08, 0x47, 0x0B, 0x02, 0x5F, 0x0A, 
+	0xC3, 0x42, 0xCC, 0x42, 0xCD, 0x42, 0xCE, 0x42, 0xCF, 0x41, 
+	0xC6, 0x0C, 0x07, 0x08, 0x48, 0x0B, 0x00, 0x00, 0x34, 0x00, 
+	0x00, 0x00, 0x68, 0x00, 0x00, 0x00, 0x0C, 0x06, 0x00, 0x00, 
+	0xBF, 0x02, 0x00, 0x00, 0x00, 0x41, 0x0E, 0x10, 0x86, 0x02, 
+	0x43, 0x0D, 0x06, 0x4D, 0x8F, 0x03, 0x8E, 0x04, 0x8D, 0x05, 
+	0x8C, 0x06, 0x83, 0x07, 0x02, 0xEB, 0x0A, 0xC3, 0x42, 0xCC, 
+	0x42, 0xCD, 0x42, 0xCE, 0x42, 0xCF, 0x41, 0xC6, 0x0C, 0x07, 
+	0x08, 0x4B, 0x0B, 0x00, 0x1C, 0x00, 0x00, 0x00, 0xA0, 0x00, 
+	0x00, 0x00, 0x94, 0x08, 0x00, 0x00, 0x15, 0x00, 0x00, 0x00, 
+	0x00, 0x41, 0x0E, 0x10, 0x86, 0x02, 0x4D, 0x0D, 0x06, 0x46, 
+	0xC6, 0x0C, 0x07, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x20, 0x00, 0x00, 0x00, 0xC4, 0x00, 0x00, 0x00, 0x90, 0x08, 
+	0x00, 0x00, 0x3D, 0x00, 0x00, 0x00, 0x00, 0x48, 0x0E, 0x10, 
+	0x86, 0x02, 0x43, 0x0D, 0x06, 0x65, 0x0A, 0xC6, 0x0C, 0x07, 
+	0x08, 0x48, 0x0B, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x55, 0x48, 0x89, 0xE5, 0x41, 0x57, 0x41, 0x56, 0x41, 0x55, 
+	0x41, 0x54, 0x53, 0x48, 0x83, 0xEC, 0x20, 0x83, 0xFF, 0x01, 
+	0x0F, 0x84, 0x86, 0x01, 0x00, 0x00, 0x0F, 0x8E, 0xC0, 0x00, 
+	0x00, 0x00, 0x83, 0xFF, 0x05, 0x74, 0x53, 0x83, 0xFF, 0x06, 
+	0x0F, 0x85, 0x92, 0x00, 0x00, 0x00, 0x8B, 0x05, 0x5C, 0xDA, 
+	0xFF, 0xFF, 0xA8, 0x01, 0x0F, 0x85, 0xF4, 0x04, 0x00, 0x00, 
+	0x48, 0x8B, 0x15, 0x9D, 0xDA, 0xFF, 0xFF, 0x48, 0x89, 0x16, 
+	0x48, 0x8B, 0x15, 0x9B, 0xDA, 0xFF, 0xFF, 0x48, 0x89, 0x56, 
+	0x08, 0x39, 0x05, 0x39, 0xDA, 0xFF, 0xFF, 0x75, 0xD5, 0x48, 
+	0x83, 0xC4, 0x20, 0x31, 0xC0, 0x5B, 0x41, 0x5C, 0x41, 0x5D, 
+	0x41, 0x5E, 0x41, 0x5F, 0x5D, 0xC3, 0x66, 0x0F, 0x1F, 0x44, 
+	0x00, 0x00, 0xF3, 0x90, 0x66, 0x0F, 0x1F, 0x44, 0x00, 0x00, 
+	0x8B, 0x05, 0x12, 0xDA, 0xFF, 0xFF, 0xA8, 0x01, 0x75, 0xEE, 
+	0x48, 0x8B, 0x15, 0x47, 0xDA, 0xFF, 0xFF, 0x48, 0x89, 0x16, 
+	0x48, 0x8B, 0x15, 0x45, 0xDA, 0xFF, 0xFF, 0x48, 0x89, 0x56, 
+	0x08, 0x39, 0x05, 0xF3, 0xD9, 0xFF, 0xFF, 0x74, 0xBA, 0xEB, 
+	0xD7, 0x0F, 0x1F, 0x80, 0x00, 0x00, 0x00, 0x00, 0x31, 0xD2, 
+	0x48, 0x01, 0x16, 0x45, 0x85, 0xFF, 0x48, 0x89, 0x46, 0x08, 
+	0x75, 0xA3, 0x66, 0x2E, 0x0F, 0x1F, 0x84, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x48, 0x63, 0xFF, 0xB8, 0xE4, 0x00, 0x00, 0x00, 
+	0x0F, 0x05, 0x48, 0x83, 0xC4, 0x20, 0x5B, 0x41, 0x5C, 0x41, 
+	0x5D, 0x41, 0x5E, 0x41, 0x5F, 0x5D, 0xC3, 0x0F, 0x1F, 0x80, 
+	0x00, 0x00, 0x00, 0x00, 0x85, 0xFF, 0x75, 0xDC, 0xB8, 0x7B, 
+	0x00, 0x00, 0x00, 0x49, 0xC7, 0xC4, 0x00, 0xF0, 0x7F, 0xFF, 
+	0x45, 0x31, 0xF6, 0x44, 0x0F, 0x03, 0xE8, 0x45, 0x89, 0xEB, 
+	0x0F, 0x03, 0xD8, 0x44, 0x8B, 0x15, 0x8C, 0xD9, 0xFF, 0xFF, 
+	0x41, 0xF6, 0xC2, 0x01, 0x0F, 0x85, 0x12, 0x04, 0x00, 0x00, 
+	0x48, 0x8B, 0x05, 0xA3, 0xD9, 0xFF, 0xFF, 0x44, 0x8B, 0x3D, 
+	0x78, 0xD9, 0xFF, 0xFF, 0x48, 0x89, 0x06, 0x8B, 0x15, 0x6F, 
+	0xD9, 0xFF, 0xFF, 0x4C, 0x8B, 0x0D, 0x84, 0xD9, 0xFF, 0xFF, 
+	0x83, 0xFA, 0x01, 0x0F, 0x84, 0x6B, 0x01, 0x00, 0x00, 0x83, 
+	0xFA, 0x02, 0x0F, 0x84, 0x3A, 0x01, 0x00, 0x00, 0x31, 0xC0, 
+	0x83, 0xFA, 0x03, 0x0F, 0x84, 0xBF, 0x02, 0x00, 0x00, 0x8B, 
+	0x0D, 0x5D, 0xD9, 0xFF, 0xFF, 0x44, 0x3B, 0x15, 0x3A, 0xD9, 
+	0xFF, 0xFF, 0x75, 0xA5, 0x4C, 0x01, 0xC8, 0x48, 0xD3, 0xE8, 
+	0x48, 0x3D, 0xFF, 0xC9, 0x9A, 0x3B, 0x0F, 0x86, 0x3E, 0xFF, 
+	0xFF, 0xFF, 0x31, 0xD2, 0x0F, 0x1F, 0x40, 0x00, 0x48, 0x2D, 
+	0x00, 0xCA, 0x9A, 0x3B, 0x83, 0xC2, 0x01, 0x48, 0x3D, 0xFF, 
+	0xC9, 0x9A, 0x3B, 0x77, 0xEF, 0x48, 0x01, 0x16, 0x45, 0x85, 
+	0xFF, 0x48, 0x89, 0x46, 0x08, 0x0F, 0x84, 0x2F, 0xFF, 0xFF, 
+	0xFF, 0xE9, 0xC3, 0xFE, 0xFF, 0xFF, 0x66, 0x2E, 0x0F, 0x1F, 
+	0x84, 0x00, 0x00, 0x00, 0x00, 0x00, 0xB8, 0x7B, 0x00, 0x00, 
+	0x00, 0x49, 0xC7, 0xC4, 0x00, 0xF0, 0x7F, 0xFF, 0x45, 0x31, 
+	0xF6, 0x44, 0x0F, 0x03, 0xE8, 0x45, 0x89, 0xEB, 0x0F, 0x03, 
+	0xD8, 0x44, 0x8B, 0x15, 0xD0, 0xD8, 0xFF, 0xFF, 0x41, 0xF6, 
+	0xC2, 0x01, 0x0F, 0x85, 0x46, 0x03, 0x00, 0x00, 0x48, 0x8B, 
+	0x05, 0xEF, 0xD8, 0xFF, 0xFF, 0x44, 0x8B, 0x3D, 0xBC, 0xD8, 
+	0xFF, 0xFF, 0x48, 0x89, 0x06, 0x8B, 0x15, 0xB3, 0xD8, 0xFF, 
+	0xFF, 0x4C, 0x8B, 0x0D, 0xE0, 0xD8, 0xFF, 0xFF, 0x83, 0xFA, 
+	0x01, 0x0F, 0x84, 0xDF, 0x00, 0x00, 0x00, 0x83, 0xFA, 0x02, 
+	0x74, 0x5A, 0x31, 0xC0, 0x83, 0xFA, 0x03, 0x0F, 0x84, 0xFF, 
+	0x00, 0x00, 0x00, 0x8B, 0x0D, 0xA5, 0xD8, 0xFF, 0xFF, 0x44, 
+	0x3B, 0x15, 0x82, 0xD8, 0xFF, 0xFF, 0x75, 0xA9, 0x4C, 0x01, 
+	0xC8, 0x48, 0xD3, 0xE8, 0x48, 0x3D, 0xFF, 0xC9, 0x9A, 0x3B, 
+	0x0F, 0x86, 0x86, 0xFE, 0xFF, 0xFF, 0x31, 0xD2, 0x0F, 0x1F, 
+	0x40, 0x00, 0x48, 0x2D, 0x00, 0xCA, 0x9A, 0x3B, 0x83, 0xC2, 
+	0x01, 0x48, 0x3D, 0xFF, 0xC9, 0x9A, 0x3B, 0x77, 0xEF, 0x48, 
+	0x01, 0x16, 0x45, 0x85, 0xFF, 0x48, 0x89, 0x46, 0x08, 0x0F, 
+	0x84, 0x77, 0xFE, 0xFF, 0xFF, 0xE9, 0x0B, 0xFE, 0xFF, 0xFF, 
+	0x66, 0x90, 0x44, 0x8B, 0x2D, 0xA9, 0xE8, 0xFF, 0xFF, 0x4C, 
+	0x2B, 0x2D, 0x3A, 0xD8, 0xFF, 0xFF, 0x4C, 0x23, 0x2D, 0x3B, 
+	0xD8, 0xFF, 0xFF, 0x8B, 0x05, 0x3D, 0xD8, 0xFF, 0xFF, 0x49, 
+	0x0F, 0xAF, 0xC5, 0xEB, 0x90, 0x0F, 0x1F, 0x80, 0x00, 0x00, 
+	0x00, 0x00, 0x44, 0x8B, 0x2D, 0x81, 0xE8, 0xFF, 0xFF, 0x4C, 
+	0x2B, 0x2D, 0x12, 0xD8, 0xFF, 0xFF, 0x4C, 0x23, 0x2D, 0x13, 
+	0xD8, 0xFF, 0xFF, 0x8B, 0x05, 0x15, 0xD8, 0xFF, 0xFF, 0x49, 
+	0x0F, 0xAF, 0xC5, 0xE9, 0xAD, 0xFE, 0xFF, 0xFF, 0x0F, 0x1F, 
+	0x40, 0x00, 0x66, 0x66, 0x90, 0x66, 0x66, 0x90, 0x0F, 0x31, 
+	0x49, 0x89, 0xD5, 0x89, 0xC0, 0x49, 0xC1, 0xE5, 0x20, 0x49, 
+	0x09, 0xC5, 0x48, 0x8B, 0x05, 0xDD, 0xD7, 0xFF, 0xFF, 0x49, 
+	0x39, 0xC5, 0x0F, 0x82, 0xB7, 0x02, 0x00, 0x00, 0x49, 0x29, 
+	0xC5, 0xEB, 0xBD, 0x0F, 0x1F, 0x80, 0x00, 0x00, 0x00, 0x00, 
+	0x66, 0x66, 0x90, 0x66, 0x66, 0x90, 0x0F, 0x31, 0x49, 0x89, 
+	0xD5, 0x89, 0xC0, 0x49, 0xC1, 0xE5, 0x20, 0x49, 0x09, 0xC5, 
+	0x48, 0x8B, 0x05, 0xAD, 0xD7, 0xFF, 0xFF, 0x49, 0x39, 0xC5, 
+	0x0F, 0x82, 0x7F, 0x02, 0x00, 0x00, 0x49, 0x29, 0xC5, 0xE9, 
+	0x62, 0xFF, 0xFF, 0xFF, 0x0F, 0x1F, 0x40, 0x00, 0x44, 0x8B, 
+	0x2D, 0x19, 0xD7, 0xFF, 0xFF, 0x48, 0x89, 0x75, 0xB8, 0x41, 
+	0x83, 0xFD, 0x01, 0x89, 0xD9, 0x0F, 0x84, 0x28, 0x02, 0x00, 
+	0x00, 0x89, 0xCE, 0x83, 0xE1, 0x3F, 0x81, 0xE6, 0xFF, 0x0F, 
+	0x00, 0x00, 0x89, 0xF0, 0x48, 0xC1, 0xE8, 0x06, 0x05, 0x00, 
+	0x02, 0x00, 0x00, 0x3D, 0x03, 0x02, 0x00, 0x00, 0x0F, 0x8F, 
+	0x1F, 0x02, 0x00, 0x00, 0xC1, 0xE0, 0x0C, 0x4D, 0x89, 0xE0, 
+	0x48, 0x63, 0xC9, 0x48, 0x98, 0x48, 0xC1, 0xE1, 0x06, 0x49, 
+	0x29, 0xC0, 0x49, 0x01, 0xC8, 0x41, 0x8B, 0x00, 0x89, 0x45, 
+	0xD4, 0x66, 0x66, 0x90, 0x66, 0x66, 0x90, 0x0F, 0x31, 0x48, 
+	0xC1, 0xE2, 0x20, 0x89, 0xC0, 0x41, 0x0F, 0xBE, 0x48, 0x1C, 
+	0x48, 0x09, 0xD0, 0x49, 0x2B, 0x40, 0x08, 0x41, 0x8B, 0x50, 
+	0x18, 0x49, 0x89, 0xC5, 0x49, 0xD3, 0xE5, 0x85, 0xC9, 0x0F, 
+	0x88, 0xDA, 0x01, 0x00, 0x00, 0x4C, 0x89, 0xE8, 0x48, 0xF7, 
+	0xE2, 0x48, 0x0F, 0xAC, 0xD0, 0x20, 0x48, 0x89, 0x45, 0xC0, 
+	0x49, 0x8B, 0x40, 0x10, 0x48, 0x89, 0x45, 0xC8, 0x41, 0x0F, 
+	0xB6, 0x40, 0x1D, 0x88, 0x45, 0xD3, 0x66, 0x66, 0x90, 0x66, 
+	0x66, 0x90, 0x44, 0x8B, 0x2D, 0x75, 0xD6, 0xFF, 0xFF, 0x44, 
+	0x89, 0xD9, 0x41, 0x83, 0xFD, 0x01, 0x0F, 0x84, 0x8F, 0x01, 
+	0x00, 0x00, 0x81, 0xE1, 0xFF, 0x0F, 0x00, 0x00, 0x39, 0xCE, 
+	0x0F, 0x85, 0x45, 0xFF, 0xFF, 0xFF, 0x41, 0x8B, 0x00, 0xA8, 
+	0x01, 0x0F, 0x85, 0x3A, 0xFF, 0xFF, 0xFF, 0x39, 0x45, 0xD4, 
+	0x0F, 0x85, 0x31, 0xFF, 0xFF, 0xFF, 0x4C, 0x8B, 0x6D, 0xC8, 
+	0x4C, 0x03, 0x6D, 0xC0, 0xF6, 0x45, 0xD3, 0x01, 0x48, 0x8B, 
+	0x05, 0xAB, 0xD6, 0xFF, 0xFF, 0x48, 0x8B, 0x75, 0xB8, 0x45, 
+	0x0F, 0x44, 0xFE, 0x49, 0x39, 0xC5, 0x4C, 0x0F, 0x42, 0xE8, 
+	0xE9, 0xF3, 0xFE, 0xFF, 0xFF, 0x0F, 0x1F, 0x80, 0x00, 0x00, 
+	0x00, 0x00, 0x44, 0x8B, 0x2D, 0x11, 0xD6, 0xFF, 0xFF, 0x48, 
+	0x89, 0x75, 0xB8, 0x41, 0x83, 0xFD, 0x01, 0x89, 0xD9, 0x0F, 
+	0x84, 0x18, 0x01, 0x00, 0x00, 0x89, 0xCE, 0x83, 0xE1, 0x3F, 
+	0x81, 0xE6, 0xFF, 0x0F, 0x00, 0x00, 0x89, 0xF0, 0x48, 0xC1, 
+	0xE8, 0x06, 0x05, 0x00, 0x02, 0x00, 0x00, 0x3D, 0x03, 0x02, 
+	0x00, 0x00, 0x0F, 0x8F, 0x17, 0x01, 0x00, 0x00, 0xC1, 0xE0, 
+	0x0C, 0x4D, 0x89, 0xE0, 0x48, 0x63, 0xC9, 0x48, 0x98, 0x48, 
+	0xC1, 0xE1, 0x06, 0x49, 0x29, 0xC0, 0x49, 0x01, 0xC8, 0x41, 
+	0x8B, 0x00, 0x89, 0x45, 0xD4, 0x66, 0x66, 0x90, 0x66, 0x66, 
+	0x90, 0x0F, 0x31, 0x48, 0xC1, 0xE2, 0x20, 0x89, 0xC0, 0x41, 
+	0x0F, 0xBE, 0x48, 0x1C, 0x48, 0x09, 0xD0, 0x49, 0x2B, 0x40, 
+	0x08, 0x41, 0x8B, 0x50, 0x18, 0x49, 0x89, 0xC5, 0x49, 0xD3, 
+	0xE5, 0x85, 0xC9, 0x0F, 0x88, 0xDF, 0x00, 0x00, 0x00, 0x4C, 
+	0x89, 0xE8, 0x48, 0xF7, 0xE2, 0x48, 0x0F, 0xAC, 0xD0, 0x20, 
+	0x48, 0x89, 0x45, 0xC0, 0x49, 0x8B, 0x40, 0x10, 0x48, 0x89, 
+	0x45, 0xC8, 0x41, 0x0F, 0xB6, 0x40, 0x1D, 0x88, 0x45, 0xD3, 
+	0x66, 0x66, 0x90, 0x66, 0x66, 0x90, 0x44, 0x8B, 0x2D, 0x6D, 
+	0xD5, 0xFF, 0xFF, 0x44, 0x89, 0xD9, 0x41, 0x83, 0xFD, 0x01, 
+	0x0F, 0x84, 0x8F, 0x00, 0x00, 0x00, 0x81, 0xE1, 0xFF, 0x0F, 
+	0x00, 0x00, 0x39, 0xCE, 0x0F, 0x85, 0x45, 0xFF, 0xFF, 0xFF, 
+	0x41, 0x8B, 0x00, 0xA8, 0x01, 0x0F, 0x85, 0x3A, 0xFF, 0xFF, 
+	0xFF, 0x39, 0x45, 0xD4, 0x0F, 0x85, 0x31, 0xFF, 0xFF, 0xFF, 
+	0x4C, 0x8B, 0x6D, 0xC8, 0x4C, 0x03, 0x6D, 0xC0, 0xF6, 0x45, 
+	0xD3, 0x01, 0x48, 0x8B, 0x05, 0xA3, 0xD5, 0xFF, 0xFF, 0x48, 
+	0x8B, 0x75, 0xB8, 0x45, 0x0F, 0x44, 0xFE, 0x49, 0x39, 0xC5, 
+	0x4C, 0x0F, 0x42, 0xE8, 0xE9, 0xBB, 0xFD, 0xFF, 0xFF, 0x0F, 
+	0x1F, 0x80, 0x00, 0x00, 0x00, 0x00, 0xF3, 0x90, 0xE9, 0xA2, 
+	0xFC, 0xFF, 0xFF, 0x66, 0x0F, 0x1F, 0x84, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0xF3, 0x90, 0xE9, 0xD6, 0xFB, 0xFF, 0xFF, 0x66, 
+	0x0F, 0x1F, 0x84, 0x00, 0x00, 0x00, 0x00, 0x00, 0xF3, 0x90, 
+	0xE9, 0xF7, 0xFA, 0xFF, 0xFF, 0x0F, 0x01, 0xF9, 0xE9, 0xE0, 
+	0xFE, 0xFF, 0xFF, 0x0F, 0x01, 0xF9, 0xE9, 0xD0, 0xFD, 0xFF, 
+	0xFF, 0x0F, 0x01, 0xF9, 0xE9, 0x69, 0xFE, 0xFF, 0xFF, 0x0F, 
+	0x01, 0xF9, 0xE9, 0x69, 0xFF, 0xFF, 0xFF, 0x0F, 0x0B, 0xF7, 
+	0xD9, 0x48, 0xD3, 0xE8, 0x49, 0x89, 0xC5, 0xE9, 0x19, 0xFE, 
+	0xFF, 0xFF, 0xF7, 0xD9, 0x48, 0xD3, 0xE8, 0x49, 0x89, 0xC5, 
+	0xE9, 0x14, 0xFF, 0xFF, 0xFF, 0x45, 0x31, 0xED, 0xE9, 0xE3, 
+	0xFC, 0xFF, 0xFF, 0x45, 0x31, 0xED, 0xE9, 0x03, 0xFD, 0xFF, 
+	0xFF, 0x66, 0x66, 0x66, 0x66, 0x2E, 0x0F, 0x1F, 0x84, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x55, 0x48, 0x89, 0xE5, 0x41, 0x57, 
+	0x41, 0x56, 0x41, 0x55, 0x41, 0x54, 0x53, 0x48, 0x83, 0xEC, 
+	0x20, 0x48, 0x85, 0xFF, 0x0F, 0x84, 0xD2, 0x00, 0x00, 0x00, 
+	0xB8, 0x7B, 0x00, 0x00, 0x00, 0x49, 0xC7, 0xC5, 0x00, 0xF0, 
+	0x7F, 0xFF, 0x45, 0x31, 0xFF, 0x44, 0x0F, 0x03, 0xF0, 0x45, 
+	0x89, 0xF3, 0x44, 0x0F, 0x03, 0xE0, 0x44, 0x8B, 0x15, 0xC5, 
+	0xD4, 0xFF, 0xFF, 0x41, 0xF6, 0xC2, 0x01, 0x0F, 0x85, 0x2B, 
+	0x02, 0x00, 0x00, 0x48, 0x8B, 0x05, 0xDC, 0xD4, 0xFF, 0xFF, 
+	0x8B, 0x1D, 0xB2, 0xD4, 0xFF, 0xFF, 0x48, 0x89, 0x07, 0x8B, 
+	0x15, 0xA9, 0xD4, 0xFF, 0xFF, 0x4C, 0x8B, 0x0D, 0xBE, 0xD4, 
+	0xFF, 0xFF, 0x83, 0xFA, 0x01, 0x0F, 0x84, 0xD5, 0x01, 0x00, 
+	0x00, 0x83, 0xFA, 0x02, 0x0F, 0x84, 0x9C, 0x00, 0x00, 0x00, 
+	0x31, 0xC0, 0x83, 0xFA, 0x03, 0x0F, 0x84, 0xB9, 0x00, 0x00, 
+	0x00, 0x8B, 0x0D, 0x97, 0xD4, 0xFF, 0xFF, 0x44, 0x3B, 0x15, 
+	0x74, 0xD4, 0xFF, 0xFF, 0x75, 0xA6, 0x4C, 0x01, 0xC8, 0x48, 
+	0xD3, 0xE8, 0x48, 0x3D, 0xFF, 0xC9, 0x9A, 0x3B, 0x48, 0x89, 
+	0xC1, 0x0F, 0x86, 0xF6, 0x01, 0x00, 0x00, 0x31, 0xD2, 0x0F, 
+	0x1F, 0x00, 0x48, 0x81, 0xE9, 0x00, 0xCA, 0x9A, 0x3B, 0x83, 
+	0xC2, 0x01, 0x48, 0x81, 0xF9, 0xFF, 0xC9, 0x9A, 0x3B, 0x77, 
+	0xED, 0x48, 0x01, 0x17, 0x85, 0xDB, 0x48, 0x89, 0x4F, 0x08, 
+	0x0F, 0x84, 0xC3, 0x01, 0x00, 0x00, 0x48, 0x89, 0xC8, 0x48, 
+	0xBA, 0xCF, 0xF7, 0x53, 0xE3, 0xA5, 0x9B, 0xC4, 0x20, 0x48, 
+	0xF7, 0xEA, 0x48, 0x89, 0xC8, 0x48, 0xC1, 0xF8, 0x3F, 0x48, 
+	0xC1, 0xFA, 0x07, 0x48, 0x29, 0xC2, 0x48, 0x89, 0x57, 0x08, 
+	0x31, 0xC0, 0x48, 0x85, 0xF6, 0x0F, 0x85, 0x80, 0x01, 0x00, 
+	0x00, 0x48, 0x83, 0xC4, 0x20, 0x5B, 0x41, 0x5C, 0x41, 0x5D, 
+	0x41, 0x5E, 0x41, 0x5F, 0x5D, 0xC3, 0x66, 0x2E, 0x0F, 0x1F, 
+	0x84, 0x00, 0x00, 0x00, 0x00, 0x00, 0x44, 0x8B, 0x35, 0x59, 
+	0xE4, 0xFF, 0xFF, 0x4C, 0x2B, 0x35, 0xEA, 0xD3, 0xFF, 0xFF, 
+	0x4C, 0x23, 0x35, 0xEB, 0xD3, 0xFF, 0xFF, 0x8B, 0x05, 0xED, 
+	0xD3, 0xFF, 0xFF, 0x49, 0x0F, 0xAF, 0xC6, 0xE9, 0x4B, 0xFF, 
+	0xFF, 0xFF, 0x0F, 0x1F, 0x40, 0x00, 0x44, 0x8B, 0x35, 0x51, 
+	0xD3, 0xFF, 0xFF, 0x48, 0x89, 0x75, 0xB8, 0x41, 0x83, 0xFE, 
+	0x01, 0x44, 0x89, 0xE1, 0x0F, 0x84, 0x50, 0x01, 0x00, 0x00, 
+	0x89, 0xCE, 0x83, 0xE1, 0x3F, 0x81, 0xE6, 0xFF, 0x0F, 0x00, 
+	0x00, 0x89, 0xF0, 0x48, 0xC1, 0xE8, 0x06, 0x05, 0x00, 0x02, 
+	0x00, 0x00, 0x3D, 0x03, 0x02, 0x00, 0x00, 0x0F, 0x8F, 0x4C, 
+	0x01, 0x00, 0x00, 0xC1, 0xE0, 0x0C, 0x4D, 0x89, 0xE8, 0x48, 
+	0x63, 0xC9, 0x48, 0x98, 0x48, 0xC1, 0xE1, 0x06, 0x49, 0x29, 
+	0xC0, 0x49, 0x01, 0xC8, 0x41, 0x8B, 0x00, 0x89, 0x45, 0xD4, 
+	0x66, 0x66, 0x90, 0x66, 0x66, 0x90, 0x0F, 0x31, 0x48, 0xC1, 
+	0xE2, 0x20, 0x89, 0xC0, 0x41, 0x0F, 0xBE, 0x48, 0x1C, 0x48, 
+	0x09, 0xD0, 0x49, 0x2B, 0x40, 0x08, 0x41, 0x8B, 0x50, 0x18, 
+	0x49, 0x89, 0xC6, 0x49, 0xD3, 0xE6, 0x85, 0xC9, 0x0F, 0x88, 
+	0xF8, 0x00, 0x00, 0x00, 0x4C, 0x89, 0xF0, 0x48, 0xF7, 0xE2, 
+	0x48, 0x0F, 0xAC, 0xD0, 0x20, 0x48, 0x89, 0x45, 0xC0, 0x49, 
+	0x8B, 0x40, 0x10, 0x48, 0x89, 0x45, 0xC8, 0x41, 0x0F, 0xB6, 
+	0x40, 0x1D, 0x88, 0x45, 0xD3, 0x66, 0x66, 0x90, 0x66, 0x66, 
+	0x90, 0x44, 0x8B, 0x35, 0xAC, 0xD2, 0xFF, 0xFF, 0x44, 0x89, 
+	0xD9, 0x41, 0x83, 0xFE, 0x01, 0x0F, 0x84, 0xB7, 0x00, 0x00, 
+	0x00, 0x81, 0xE1, 0xFF, 0x0F, 0x00, 0x00, 0x39, 0xCE, 0x0F, 
+	0x85, 0x44, 0xFF, 0xFF, 0xFF, 0x41, 0x8B, 0x00, 0xA8, 0x01, 
+	0x0F, 0x85, 0x39, 0xFF, 0xFF, 0xFF, 0x39, 0x45, 0xD4, 0x0F, 
+	0x85, 0x30, 0xFF, 0xFF, 0xFF, 0x4C, 0x8B, 0x75, 0xC8, 0x4C, 
+	0x03, 0x75, 0xC0, 0xF6, 0x45, 0xD3, 0x01, 0x48, 0x8B, 0x05, 
+	0xE2, 0xD2, 0xFF, 0xFF, 0x48, 0x8B, 0x75, 0xB8, 0x41, 0x0F, 
+	0x44, 0xDF, 0x49, 0x39, 0xC6, 0x4C, 0x0F, 0x42, 0xF0, 0x49, 
+	0x29, 0xC6, 0xE9, 0xE1, 0xFE, 0xFF, 0xFF, 0x0F, 0x1F, 0x00, 
+	0x66, 0x66, 0x90, 0x66, 0x66, 0x90, 0x0F, 0x31, 0x49, 0x89, 
+	0xD6, 0x89, 0xC0, 0x49, 0xC1, 0xE6, 0x20, 0x49, 0x09, 0xC6, 
+	0x48, 0x8B, 0x05, 0xAD, 0xD2, 0xFF, 0xFF, 0x49, 0x39, 0xC6, 
+	0x73, 0xD5, 0x45, 0x31, 0xF6, 0xE9, 0xB6, 0xFE, 0xFF, 0xFF, 
+	0x0F, 0x1F, 0x84, 0x00, 0x00, 0x00, 0x00, 0x00, 0xF3, 0x90, 
+	0xE9, 0xBD, 0xFD, 0xFF, 0xFF, 0x8B, 0x15, 0xE3, 0xD2, 0xFF, 
+	0xFF, 0x89, 0x16, 0x8B, 0x15, 0xDF, 0xD2, 0xFF, 0xFF, 0x89, 
+	0x56, 0x04, 0xE9, 0x6A, 0xFE, 0xFF, 0xFF, 0xB8, 0x60, 0x00, 
+	0x00, 0x00, 0x0F, 0x05, 0xE9, 0x5E, 0xFE, 0xFF, 0xFF, 0x31, 
+	0xD2, 0xE9, 0x1B, 0xFE, 0xFF, 0xFF, 0x0F, 0x01, 0xF9, 0xE9, 
+	0xA8, 0xFE, 0xFF, 0xFF, 0x0F, 0x01, 0xF9, 0xE9, 0x41, 0xFF, 
+	0xFF, 0xFF, 0xF7, 0xD9, 0x48, 0xD3, 0xE8, 0x49, 0x89, 0xC6, 
+	0xE9, 0xFB, 0xFE, 0xFF, 0xFF, 0x0F, 0x0B, 0x90, 0x55, 0x48, 
+	0x85, 0xFF, 0x48, 0x8B, 0x05, 0x5D, 0xD2, 0xFF, 0xFF, 0x48, 
+	0x89, 0xE5, 0x74, 0x03, 0x48, 0x89, 0x07, 0x5D, 0xC3, 0x90, 
+	0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 
+	0x83, 0x3D, 0xA9, 0xD1, 0xFF, 0xFF, 0x01, 0x55, 0x48, 0x89, 
+	0xE5, 0x74, 0x2B, 0xB9, 0x7B, 0x00, 0x00, 0x00, 0x0F, 0x03, 
+	0xC9, 0x48, 0x85, 0xFF, 0x74, 0x09, 0x89, 0xC8, 0x25, 0xFF, 
+	0x0F, 0x00, 0x00, 0x89, 0x07, 0x48, 0x85, 0xF6, 0x74, 0x05, 
+	0xC1, 0xE9, 0x0C, 0x89, 0x0E, 0x31, 0xC0, 0x5D, 0xC3, 0x0F, 
+	0x1F, 0x80, 0x00, 0x00, 0x00, 0x00, 0x0F, 0x01, 0xF9, 0xEB, 
+	0xD8, 0xF3, 0xF9, 0xFF, 0xFF, 0xD4, 0x00, 0x00, 0x00, 0x71, 
+	0x00, 0x03, 0x03, 0xEA, 0xF9, 0xFF, 0xFF, 0xCB, 0x00, 0x00, 
+	0x00, 0x72, 0x00, 0x03, 0x03, 0x0B, 0xFA, 0xFF, 0xFF, 0xC2, 
+	0x00, 0x00, 0x00, 0x71, 0x00, 0x03, 0x03, 0x02, 0xFA, 0xFF, 
+	0xFF, 0xB9, 0x00, 0x00, 0x00, 0x72, 0x00, 0x03, 0x03, 0x76, 
+	0xFA, 0xFF, 0xFF, 0xB0, 0x00, 0x00, 0x00, 0x71, 0x00, 0x03, 
+	0x03, 0x6D, 0xFA, 0xFF, 0xFF, 0xA7, 0x00, 0x00, 0x00, 0x72, 
+	0x00, 0x03, 0x03, 0xA9, 0xFA, 0xFF, 0xFF, 0x9E, 0x00, 0x00, 
+	0x00, 0x71, 0x00, 0x03, 0x03, 0xA0, 0xFA, 0xFF, 0xFF, 0x95, 
+	0x00, 0x00, 0x00, 0x72, 0x00, 0x03, 0x03, 0x4E, 0xFB, 0xFF, 
+	0xFF, 0x8C, 0x00, 0x00, 0x00, 0x71, 0x00, 0x03, 0x03, 0x45, 
+	0xFB, 0xFF, 0xFF, 0x83, 0x00, 0x00, 0x00, 0x72, 0x00, 0x03, 
+	0x03, 0x81, 0xFB, 0xFF, 0xFF, 0x7A, 0x00, 0x00, 0x00, 0x71, 
+	0x00, 0x03, 0x03, 0x78, 0xFB, 0xFF, 0xFF, 0x71, 0x00, 0x00, 
+	0x00, 0x72, 0x00, 0x03, 0x03, 0xDF, 0xFD, 0xFF, 0xFF, 0x68, 
+	0x00, 0x00, 0x00, 0x71, 0x00, 0x03, 0x03, 0xD6, 0xFD, 0xFF, 
+	0xFF, 0x5F, 0x00, 0x00, 0x00, 0x72, 0x00, 0x03, 0x03, 0x12, 
+	0xFE, 0xFF, 0xFF, 0x56, 0x00, 0x00, 0x00, 0x71, 0x00, 0x03, 
+	0x03, 0x09, 0xFE, 0xFF, 0xFF, 0x4D, 0x00, 0x00, 0x00, 0x72, 
+	0x00, 0x03, 0x03, 0x63, 0xFE, 0xFF, 0xFF, 0x44, 0x00, 0x00, 
+	0x00, 0x71, 0x00, 0x03, 0x03, 0x5A, 0xFE, 0xFF, 0xFF, 0x3B, 
+	0x00, 0x00, 0x00, 0x72, 0x00, 0x03, 0x03, 0x0F, 0xAE, 0xF0, 
+	0x0F, 0xAE, 0xE8, 0x0F, 0xAE, 0xF0, 0x0F, 0xAE, 0xE8, 0x0F, 
+	0xAE, 0xF0, 0x0F, 0xAE, 0xE8, 0x0F, 0xAE, 0xF0, 0x0F, 0xAE, 
+	0xE8, 0x0F, 0xAE, 0xF0, 0x0F, 0xAE, 0xE8, 0x0F, 0xAE, 0xF0, 
+	0x0F, 0xAE, 0xE8, 0x0F, 0xAE, 0xF0, 0x0F, 0xAE, 0xE8, 0x0F, 
+	0xAE, 0xF0, 0x0F, 0xAE, 0xE8, 0x0F, 0xAE, 0xF0, 0x0F, 0xAE, 
+	0xE8, 0x47, 0x43, 0x43, 0x3A, 0x20, 0x28, 0x55, 0x62, 0x75, 
+	0x6E, 0x74, 0x75, 0x20, 0x34, 0x2E, 0x39, 0x2E, 0x31, 0x2D, 
+	0x31, 0x36, 0x75, 0x62, 0x75, 0x6E, 0x74, 0x75, 0x36, 0x29, 
+	0x20, 0x34, 0x2E, 0x39, 0x2E, 0x31, 0x00, 0x00, 0x2E, 0x73, 
+	0x68, 0x73, 0x74, 0x72, 0x74, 0x61, 0x62, 0x00, 0x2E, 0x68, 
+	0x61, 0x73, 0x68, 0x00, 0x2E, 0x64, 0x79, 0x6E, 0x73, 0x79, 
+	0x6D, 0x00, 0x2E, 0x64, 0x79, 0x6E, 0x73, 0x74, 0x72, 0x00, 
+	0x2E, 0x67, 0x6E, 0x75, 0x2E, 0x76, 0x65, 0x72, 0x73, 0x69, 
+	0x6F, 0x6E, 0x00, 0x2E, 0x67, 0x6E, 0x75, 0x2E, 0x76, 0x65, 
+	0x72, 0x73, 0x69, 0x6F, 0x6E, 0x5F, 0x64, 0x00, 0x2E, 0x64, 
+	0x79, 0x6E, 0x61, 0x6D, 0x69, 0x63, 0x00, 0x2E, 0x72, 0x6F, 
+	0x64, 0x61, 0x74, 0x61, 0x00, 0x2E, 0x6E, 0x6F, 0x74, 0x65, 
+	0x00, 0x2E, 0x65, 0x68, 0x5F, 0x66, 0x72, 0x61, 0x6D, 0x65, 
+	0x5F, 0x68, 0x64, 0x72, 0x00, 0x2E, 0x65, 0x68, 0x5F, 0x66, 
+	0x72, 0x61, 0x6D, 0x65, 0x00, 0x2E, 0x74, 0x65, 0x78, 0x74, 
+	0x00, 0x2E, 0x61, 0x6C, 0x74, 0x69, 0x6E, 0x73, 0x74, 0x72, 
+	0x75, 0x63, 0x74, 0x69, 0x6F, 0x6E, 0x73, 0x00, 0x2E, 0x61, 
+	0x6C, 0x74, 0x69, 0x6E, 0x73, 0x74, 0x72, 0x5F, 0x72, 0x65, 
+	0x70, 0x6C, 0x61, 0x63, 0x65, 0x6D, 0x65, 0x6E, 0x74, 0x00, 
+	0x2E, 0x63, 0x6F, 0x6D, 0x6D, 0x65, 0x6E, 0x74, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x0B, 0x00, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x00, 0x00, 0xB4, 0x00, 0x00, 0x00, 0xB4, 0x00, 
+	0x00, 0x00, 0x30, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x04, 0x00, 
+	0x00, 0x00, 0x11, 0x00, 0x00, 0x00, 0x0B, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x00, 0x00, 0xE4, 0x00, 0x00, 0x00, 0xE4, 0x00, 
+	0x00, 0x00, 0x70, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x10, 0x00, 
+	0x00, 0x00, 0x19, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x00, 0x00, 0x54, 0x01, 0x00, 0x00, 0x54, 0x01, 
+	0x00, 0x00, 0x5E, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x21, 0x00, 0x00, 0x00, 0xFF, 0xFF, 0xFF, 0x6F, 
+	0x02, 0x00, 0x00, 0x00, 0xB2, 0x01, 0x00, 0x00, 0xB2, 0x01, 
+	0x00, 0x00, 0x0E, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x02, 0x00, 
+	0x00, 0x00, 0x2E, 0x00, 0x00, 0x00, 0xFD, 0xFF, 0xFF, 0x6F, 
+	0x02, 0x00, 0x00, 0x00, 0xC0, 0x01, 0x00, 0x00, 0xC0, 0x01, 
+	0x00, 0x00, 0x38, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x3D, 0x00, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00, 
+	0x03, 0x00, 0x00, 0x00, 0xF8, 0x01, 0x00, 0x00, 0xF8, 0x01, 
+	0x00, 0x00, 0x80, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x08, 0x00, 
+	0x00, 0x00, 0x46, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x03, 0x00, 0x00, 0x00, 0x78, 0x02, 0x00, 0x00, 0x78, 0x02, 
+	0x00, 0x00, 0x28, 0x02, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 0x08, 0x00, 
+	0x00, 0x00, 0x4E, 0x00, 0x00, 0x00, 0x07, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x00, 0x00, 0xA0, 0x04, 0x00, 0x00, 0xA0, 0x04, 
+	0x00, 0x00, 0x3C, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x54, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x00, 0x00, 0xDC, 0x04, 0x00, 0x00, 0xDC, 0x04, 
+	0x00, 0x00, 0x2C, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x62, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x00, 0x00, 0x08, 0x05, 0x00, 0x00, 0x08, 0x05, 
+	0x00, 0x00, 0xE4, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x6C, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x06, 0x00, 0x00, 0x00, 0xF0, 0x05, 0x00, 0x00, 0xF0, 0x05, 
+	0x00, 0x00, 0xAD, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x72, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x02, 0x00, 0x00, 0x00, 0x9D, 0x0E, 0x00, 0x00, 0x9D, 0x0E, 
+	0x00, 0x00, 0xD8, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x83, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x06, 0x00, 0x00, 0x00, 0x75, 0x0F, 0x00, 0x00, 0x75, 0x0F, 
+	0x00, 0x00, 0x36, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x99, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 
+	0x30, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xAB, 0x0F, 
+	0x00, 0x00, 0x24, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01, 0x00, 
+	0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xCF, 0x0F, 
+	0x00, 0x00, 0xA2, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 
+	0x00, 0x00, 
+};
+
+static struct page *pages[2];
+
+const struct vdso_image vdso_image_x32 = {
+	.data = raw_data,
+	.size = 8192,
+	.text_mapping = {
+		.name = "[vdso]",
+		.pages = pages,
+	},
+	.alt = 3741,
+	.alt_len = 216,
+	.sym_vvar_start = -8192,
+	.sym_vvar_page = -8192,
+	.sym_hpet_page = -4096,
+};
diff -urN a/drivers/gpu/drm/drm_crtc.c b/drivers/gpu/drm/drm_crtc.c
--- a/drivers/gpu/drm/drm_crtc.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/drm_crtc.c	2014-11-22 09:55:56.084790475 -0700
@@ -3900,7 +3900,7 @@
 }
 
 static struct drm_property_blob *drm_property_create_blob(struct drm_device *dev, int length,
-							  void *data)
+							  const void *data)
 {
 	struct drm_property_blob *blob;
 	int ret;
@@ -3981,7 +3981,7 @@
 }
 
 int drm_mode_connector_set_path_property(struct drm_connector *connector,
-					 char *path)
+					 const char *path)
 {
 	struct drm_device *dev = connector->dev;
 	int ret, size;
@@ -4011,7 +4011,7 @@
  * Zero on success, errno on failure.
  */
 int drm_mode_connector_update_edid_property(struct drm_connector *connector,
-					    struct edid *edid)
+					    const struct edid *edid)
 {
 	struct drm_device *dev = connector->dev;
 	int ret, size;
diff -urN a/drivers/gpu/drm/drm_irq.c b/drivers/gpu/drm/drm_irq.c
--- a/drivers/gpu/drm/drm_irq.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/drm_irq.c	2014-11-22 09:55:56.088792512 -0700
@@ -914,16 +914,20 @@
  */
 int drm_vblank_get(struct drm_device *dev, int crtc)
 {
+	struct drm_vblank_crtc *vblank = &dev->vblank[crtc];
 	unsigned long irqflags;
 	int ret = 0;
 
+	if (WARN_ON(crtc >= dev->num_crtcs))
+		return -EINVAL;
+
 	spin_lock_irqsave(&dev->vbl_lock, irqflags);
 	/* Going from 0->1 means we have to enable interrupts again */
-	if (atomic_add_return(1, &dev->vblank[crtc].refcount) == 1) {
+	if (atomic_add_return(1, &vblank->refcount) == 1) {
 		ret = drm_vblank_enable(dev, crtc);
 	} else {
-		if (!dev->vblank[crtc].enabled) {
-			atomic_dec(&dev->vblank[crtc].refcount);
+		if (!vblank->enabled) {
+			atomic_dec(&vblank->refcount);
 			ret = -EINVAL;
 		}
 	}
@@ -989,6 +993,36 @@
 EXPORT_SYMBOL(drm_crtc_vblank_put);
 
 /**
+ * drm_wait_one_vblank - wait for one vblank
+ * @dev: DRM device
+ * @crtc: crtc index
+ *
+ * This waits for one vblank to pass on @crtc, using the irq driver interfaces.
+ * It is a failure to call this when the vblank irq for @crtc is disabled, e.g.
+ * due to lack of driver support or because the crtc is off.
+ */
+void drm_wait_one_vblank(struct drm_device *dev, int crtc)
+{
+	int ret;
+	u32 last;
+
+	ret = drm_vblank_get(dev, crtc);
+	if (WARN(ret, "vblank not available on crtc %i, ret=%i\n", crtc, ret))
+		return;
+
+	last = drm_vblank_count(dev, crtc);
+
+	ret = wait_event_timeout(dev->vblank[crtc].queue,
+				 last != drm_vblank_count(dev, crtc),
+				 msecs_to_jiffies(100));
+
+	WARN(ret == 0, "vblank wait timed out on crtc %i\n", crtc);
+
+	drm_vblank_put(dev, crtc);
+}
+EXPORT_SYMBOL(drm_wait_one_vblank);
+
+/**
  * drm_vblank_off - disable vblank events on a CRTC
  * @dev: DRM device
  * @crtc: CRTC in question
@@ -1004,19 +1038,34 @@
  */
 void drm_vblank_off(struct drm_device *dev, int crtc)
 {
+	struct drm_vblank_crtc *vblank = &dev->vblank[crtc];
 	struct drm_pending_vblank_event *e, *t;
 	struct timeval now;
 	unsigned long irqflags;
 	unsigned int seq;
 
-	spin_lock_irqsave(&dev->vbl_lock, irqflags);
+	if (WARN_ON(crtc >= dev->num_crtcs))
+		return;
+
+	spin_lock_irqsave(&dev->event_lock, irqflags);
+
+	spin_lock(&dev->vbl_lock);
 	vblank_disable_and_save(dev, crtc);
-	wake_up(&dev->vblank[crtc].queue);
+	wake_up(&vblank->queue);
+
+	/*
+	 * Prevent subsequent drm_vblank_get() from re-enabling
+	 * the vblank interrupt by bumping the refcount.
+	 */
+	if (!vblank->inmodeset) {
+		atomic_inc(&vblank->refcount);
+		vblank->inmodeset = 1;
+	}
+	spin_unlock(&dev->vbl_lock);
 
 	/* Send any queued vblank events, lest the natives grow disquiet */
 	seq = drm_vblank_count_and_time(dev, crtc, &now);
 
-	spin_lock(&dev->event_lock);
 	list_for_each_entry_safe(e, t, &dev->vblank_event_list, base.link) {
 		if (e->pipe != crtc)
 			continue;
@@ -1027,9 +1076,7 @@
 		drm_vblank_put(dev, e->pipe);
 		send_vblank_event(dev, e, seq, &now);
 	}
-	spin_unlock(&dev->event_lock);
-
-	spin_unlock_irqrestore(&dev->vbl_lock, irqflags);
+	spin_unlock_irqrestore(&dev->event_lock, irqflags);
 }
 EXPORT_SYMBOL(drm_vblank_off);
 
@@ -1066,11 +1113,35 @@
  */
 void drm_vblank_on(struct drm_device *dev, int crtc)
 {
+	struct drm_vblank_crtc *vblank = &dev->vblank[crtc];
 	unsigned long irqflags;
 
+	if (WARN_ON(crtc >= dev->num_crtcs))
+		return;
+
 	spin_lock_irqsave(&dev->vbl_lock, irqflags);
-	/* re-enable interrupts if there's are users left */
-	if (atomic_read(&dev->vblank[crtc].refcount) != 0)
+	/* Drop our private "prevent drm_vblank_get" refcount */
+	if (vblank->inmodeset) {
+		atomic_dec(&vblank->refcount);
+		vblank->inmodeset = 0;
+	}
+
+	/*
+	 * sample the current counter to avoid random jumps
+	 * when drm_vblank_enable() applies the diff
+	 *
+	 * -1 to make sure user will never see the same
+	 * vblank counter value before and after a modeset
+	 */
+	vblank->last =
+		(dev->driver->get_vblank_counter(dev, crtc) - 1) &
+		dev->max_vblank_count;
+	/*
+	 * re-enable interrupts if there are users left, or the
+	 * user wishes vblank interrupts to be enabled all the time.
+	 */
+	if (atomic_read(&vblank->refcount) != 0 ||
+	    (!dev->vblank_disable_immediate && drm_vblank_offdelay == 0))
 		WARN_ON(drm_vblank_enable(dev, crtc));
 	spin_unlock_irqrestore(&dev->vbl_lock, irqflags);
 }
diff -urN a/drivers/gpu/drm/drm_pci.c b/drivers/gpu/drm/drm_pci.c
--- a/drivers/gpu/drm/drm_pci.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/drm_pci.c	2014-11-22 09:55:56.088792512 -0700
@@ -127,7 +127,7 @@
 	return pci_domain_nr(dev->pdev->bus);
 }
 
-static int drm_pci_set_busid(struct drm_device *dev, struct drm_master *master)
+int drm_pci_set_busid(struct drm_device *dev, struct drm_master *master)
 {
 	int len, ret;
 	master->unique_len = 40;
@@ -155,6 +155,7 @@
 err:
 	return ret;
 }
+EXPORT_SYMBOL(drm_pci_set_busid);
 
 int drm_pci_set_unique(struct drm_device *dev,
 		       struct drm_master *master,
diff -urN a/drivers/gpu/drm/i915/dvo_ns2501.c b/drivers/gpu/drm/i915/dvo_ns2501.c
--- a/drivers/gpu/drm/i915/dvo_ns2501.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/dvo_ns2501.c	2014-11-22 14:37:49.330700418 -0700
@@ -60,16 +60,297 @@
 
 #define NS2501_REGC 0x0c
 
+enum {
+	MODE_640x480,
+	MODE_800x600,
+	MODE_1024x768,
+};
+
+struct ns2501_reg {
+	 uint8_t offset;
+	 uint8_t value;
+};
+
+/*
+ * Magic values based on what the BIOS on
+ * Fujitsu-Siemens Lifebook S6010 programs (1024x768 panel).
+ */
+static const struct ns2501_reg regs_1024x768[][86] = {
+	[MODE_640x480] = {
+		[0] = { .offset = 0x0a, .value = 0x81, },
+		[1] = { .offset = 0x18, .value = 0x07, },
+		[2] = { .offset = 0x19, .value = 0x00, },
+		[3] = { .offset = 0x1a, .value = 0x00, },
+		[4] = { .offset = 0x1b, .value = 0x11, },
+		[5] = { .offset = 0x1c, .value = 0x54, },
+		[6] = { .offset = 0x1d, .value = 0x03, },
+		[7] = { .offset = 0x1e, .value = 0x02, },
+		[8] = { .offset = 0xf3, .value = 0x90, },
+		[9] = { .offset = 0xf9, .value = 0x00, },
+		[10] = { .offset = 0xc1, .value = 0x90, },
+		[11] = { .offset = 0xc2, .value = 0x00, },
+		[12] = { .offset = 0xc3, .value = 0x0f, },
+		[13] = { .offset = 0xc4, .value = 0x03, },
+		[14] = { .offset = 0xc5, .value = 0x16, },
+		[15] = { .offset = 0xc6, .value = 0x00, },
+		[16] = { .offset = 0xc7, .value = 0x02, },
+		[17] = { .offset = 0xc8, .value = 0x02, },
+		[18] = { .offset = 0xf4, .value = 0x00, },
+		[19] = { .offset = 0x80, .value = 0xff, },
+		[20] = { .offset = 0x81, .value = 0x07, },
+		[21] = { .offset = 0x82, .value = 0x3d, },
+		[22] = { .offset = 0x83, .value = 0x05, },
+		[23] = { .offset = 0x94, .value = 0x00, },
+		[24] = { .offset = 0x95, .value = 0x00, },
+		[25] = { .offset = 0x96, .value = 0x05, },
+		[26] = { .offset = 0x97, .value = 0x00, },
+		[27] = { .offset = 0x9a, .value = 0x88, },
+		[28] = { .offset = 0x9b, .value = 0x00, },
+		[29] = { .offset = 0x98, .value = 0x00, },
+		[30] = { .offset = 0x99, .value = 0x00, },
+		[31] = { .offset = 0xf7, .value = 0x88, },
+		[32] = { .offset = 0xf8, .value = 0x0a, },
+		[33] = { .offset = 0x9c, .value = 0x24, },
+		[34] = { .offset = 0x9d, .value = 0x00, },
+		[35] = { .offset = 0x9e, .value = 0x25, },
+		[36] = { .offset = 0x9f, .value = 0x03, },
+		[37] = { .offset = 0xa0, .value = 0x28, },
+		[38] = { .offset = 0xa1, .value = 0x01, },
+		[39] = { .offset = 0xa2, .value = 0x28, },
+		[40] = { .offset = 0xa3, .value = 0x05, },
+		[41] = { .offset = 0xb6, .value = 0x09, },
+		[42] = { .offset = 0xb8, .value = 0x00, },
+		[43] = { .offset = 0xb9, .value = 0xa0, },
+		[44] = { .offset = 0xba, .value = 0x00, },
+		[45] = { .offset = 0xbb, .value = 0x20, },
+		[46] = { .offset = 0x10, .value = 0x00, },
+		[47] = { .offset = 0x11, .value = 0xa0, },
+		[48] = { .offset = 0x12, .value = 0x02, },
+		[49] = { .offset = 0x20, .value = 0x00, },
+		[50] = { .offset = 0x22, .value = 0x00, },
+		[51] = { .offset = 0x23, .value = 0x00, },
+		[52] = { .offset = 0x24, .value = 0x00, },
+		[53] = { .offset = 0x25, .value = 0x00, },
+		[54] = { .offset = 0x8c, .value = 0x10, },
+		[55] = { .offset = 0x8d, .value = 0x02, },
+		[56] = { .offset = 0x8e, .value = 0x10, },
+		[57] = { .offset = 0x8f, .value = 0x00, },
+		[58] = { .offset = 0x90, .value = 0xff, },
+		[59] = { .offset = 0x91, .value = 0x07, },
+		[60] = { .offset = 0x92, .value = 0xa0, },
+		[61] = { .offset = 0x93, .value = 0x02, },
+		[62] = { .offset = 0xa5, .value = 0x00, },
+		[63] = { .offset = 0xa6, .value = 0x00, },
+		[64] = { .offset = 0xa7, .value = 0x00, },
+		[65] = { .offset = 0xa8, .value = 0x00, },
+		[66] = { .offset = 0xa9, .value = 0x04, },
+		[67] = { .offset = 0xaa, .value = 0x70, },
+		[68] = { .offset = 0xab, .value = 0x4f, },
+		[69] = { .offset = 0xac, .value = 0x00, },
+		[70] = { .offset = 0xa4, .value = 0x84, },
+		[71] = { .offset = 0x7e, .value = 0x18, },
+		[72] = { .offset = 0x84, .value = 0x00, },
+		[73] = { .offset = 0x85, .value = 0x00, },
+		[74] = { .offset = 0x86, .value = 0x00, },
+		[75] = { .offset = 0x87, .value = 0x00, },
+		[76] = { .offset = 0x88, .value = 0x00, },
+		[77] = { .offset = 0x89, .value = 0x00, },
+		[78] = { .offset = 0x8a, .value = 0x00, },
+		[79] = { .offset = 0x8b, .value = 0x00, },
+		[80] = { .offset = 0x26, .value = 0x00, },
+		[81] = { .offset = 0x27, .value = 0x00, },
+		[82] = { .offset = 0xad, .value = 0x00, },
+		[83] = { .offset = 0x08, .value = 0x30, }, /* 0x31 */
+		[84] = { .offset = 0x41, .value = 0x00, },
+		[85] = { .offset = 0xc0, .value = 0x05, },
+	},
+	[MODE_800x600] = {
+		[0] = { .offset = 0x0a, .value = 0x81, },
+		[1] = { .offset = 0x18, .value = 0x07, },
+		[2] = { .offset = 0x19, .value = 0x00, },
+		[3] = { .offset = 0x1a, .value = 0x00, },
+		[4] = { .offset = 0x1b, .value = 0x19, },
+		[5] = { .offset = 0x1c, .value = 0x64, },
+		[6] = { .offset = 0x1d, .value = 0x02, },
+		[7] = { .offset = 0x1e, .value = 0x02, },
+		[8] = { .offset = 0xf3, .value = 0x90, },
+		[9] = { .offset = 0xf9, .value = 0x00, },
+		[10] = { .offset = 0xc1, .value = 0xd7, },
+		[11] = { .offset = 0xc2, .value = 0x00, },
+		[12] = { .offset = 0xc3, .value = 0xf8, },
+		[13] = { .offset = 0xc4, .value = 0x03, },
+		[14] = { .offset = 0xc5, .value = 0x1a, },
+		[15] = { .offset = 0xc6, .value = 0x00, },
+		[16] = { .offset = 0xc7, .value = 0x73, },
+		[17] = { .offset = 0xc8, .value = 0x02, },
+		[18] = { .offset = 0xf4, .value = 0x00, },
+		[19] = { .offset = 0x80, .value = 0x27, },
+		[20] = { .offset = 0x81, .value = 0x03, },
+		[21] = { .offset = 0x82, .value = 0x41, },
+		[22] = { .offset = 0x83, .value = 0x05, },
+		[23] = { .offset = 0x94, .value = 0x00, },
+		[24] = { .offset = 0x95, .value = 0x00, },
+		[25] = { .offset = 0x96, .value = 0x05, },
+		[26] = { .offset = 0x97, .value = 0x00, },
+		[27] = { .offset = 0x9a, .value = 0x88, },
+		[28] = { .offset = 0x9b, .value = 0x00, },
+		[29] = { .offset = 0x98, .value = 0x00, },
+		[30] = { .offset = 0x99, .value = 0x00, },
+		[31] = { .offset = 0xf7, .value = 0x88, },
+		[32] = { .offset = 0xf8, .value = 0x06, },
+		[33] = { .offset = 0x9c, .value = 0x23, },
+		[34] = { .offset = 0x9d, .value = 0x00, },
+		[35] = { .offset = 0x9e, .value = 0x25, },
+		[36] = { .offset = 0x9f, .value = 0x03, },
+		[37] = { .offset = 0xa0, .value = 0x28, },
+		[38] = { .offset = 0xa1, .value = 0x01, },
+		[39] = { .offset = 0xa2, .value = 0x28, },
+		[40] = { .offset = 0xa3, .value = 0x05, },
+		[41] = { .offset = 0xb6, .value = 0x09, },
+		[42] = { .offset = 0xb8, .value = 0x30, },
+		[43] = { .offset = 0xb9, .value = 0xc8, },
+		[44] = { .offset = 0xba, .value = 0x00, },
+		[45] = { .offset = 0xbb, .value = 0x20, },
+		[46] = { .offset = 0x10, .value = 0x20, },
+		[47] = { .offset = 0x11, .value = 0xc8, },
+		[48] = { .offset = 0x12, .value = 0x02, },
+		[49] = { .offset = 0x20, .value = 0x00, },
+		[50] = { .offset = 0x22, .value = 0x00, },
+		[51] = { .offset = 0x23, .value = 0x00, },
+		[52] = { .offset = 0x24, .value = 0x00, },
+		[53] = { .offset = 0x25, .value = 0x00, },
+		[54] = { .offset = 0x8c, .value = 0x10, },
+		[55] = { .offset = 0x8d, .value = 0x02, },
+		[56] = { .offset = 0x8e, .value = 0x04, },
+		[57] = { .offset = 0x8f, .value = 0x00, },
+		[58] = { .offset = 0x90, .value = 0xff, },
+		[59] = { .offset = 0x91, .value = 0x07, },
+		[60] = { .offset = 0x92, .value = 0xa0, },
+		[61] = { .offset = 0x93, .value = 0x02, },
+		[62] = { .offset = 0xa5, .value = 0x00, },
+		[63] = { .offset = 0xa6, .value = 0x00, },
+		[64] = { .offset = 0xa7, .value = 0x00, },
+		[65] = { .offset = 0xa8, .value = 0x00, },
+		[66] = { .offset = 0xa9, .value = 0x83, },
+		[67] = { .offset = 0xaa, .value = 0x40, },
+		[68] = { .offset = 0xab, .value = 0x32, },
+		[69] = { .offset = 0xac, .value = 0x00, },
+		[70] = { .offset = 0xa4, .value = 0x80, },
+		[71] = { .offset = 0x7e, .value = 0x18, },
+		[72] = { .offset = 0x84, .value = 0x00, },
+		[73] = { .offset = 0x85, .value = 0x00, },
+		[74] = { .offset = 0x86, .value = 0x00, },
+		[75] = { .offset = 0x87, .value = 0x00, },
+		[76] = { .offset = 0x88, .value = 0x00, },
+		[77] = { .offset = 0x89, .value = 0x00, },
+		[78] = { .offset = 0x8a, .value = 0x00, },
+		[79] = { .offset = 0x8b, .value = 0x00, },
+		[80] = { .offset = 0x26, .value = 0x00, },
+		[81] = { .offset = 0x27, .value = 0x00, },
+		[82] = { .offset = 0xad, .value = 0x00, },
+		[83] = { .offset = 0x08, .value = 0x30, }, /* 0x31 */
+		[84] = { .offset = 0x41, .value = 0x00, },
+		[85] = { .offset = 0xc0, .value = 0x07, },
+	},
+	[MODE_1024x768] = {
+		[0] = { .offset = 0x0a, .value = 0x81, },
+		[1] = { .offset = 0x18, .value = 0x07, },
+		[2] = { .offset = 0x19, .value = 0x00, },
+		[3] = { .offset = 0x1a, .value = 0x00, },
+		[4] = { .offset = 0x1b, .value = 0x11, },
+		[5] = { .offset = 0x1c, .value = 0x54, },
+		[6] = { .offset = 0x1d, .value = 0x03, },
+		[7] = { .offset = 0x1e, .value = 0x02, },
+		[8] = { .offset = 0xf3, .value = 0x90, },
+		[9] = { .offset = 0xf9, .value = 0x00, },
+		[10] = { .offset = 0xc1, .value = 0x90, },
+		[11] = { .offset = 0xc2, .value = 0x00, },
+		[12] = { .offset = 0xc3, .value = 0x0f, },
+		[13] = { .offset = 0xc4, .value = 0x03, },
+		[14] = { .offset = 0xc5, .value = 0x16, },
+		[15] = { .offset = 0xc6, .value = 0x00, },
+		[16] = { .offset = 0xc7, .value = 0x02, },
+		[17] = { .offset = 0xc8, .value = 0x02, },
+		[18] = { .offset = 0xf4, .value = 0x00, },
+		[19] = { .offset = 0x80, .value = 0xff, },
+		[20] = { .offset = 0x81, .value = 0x07, },
+		[21] = { .offset = 0x82, .value = 0x3d, },
+		[22] = { .offset = 0x83, .value = 0x05, },
+		[23] = { .offset = 0x94, .value = 0x00, },
+		[24] = { .offset = 0x95, .value = 0x00, },
+		[25] = { .offset = 0x96, .value = 0x05, },
+		[26] = { .offset = 0x97, .value = 0x00, },
+		[27] = { .offset = 0x9a, .value = 0x88, },
+		[28] = { .offset = 0x9b, .value = 0x00, },
+		[29] = { .offset = 0x98, .value = 0x00, },
+		[30] = { .offset = 0x99, .value = 0x00, },
+		[31] = { .offset = 0xf7, .value = 0x88, },
+		[32] = { .offset = 0xf8, .value = 0x0a, },
+		[33] = { .offset = 0x9c, .value = 0x24, },
+		[34] = { .offset = 0x9d, .value = 0x00, },
+		[35] = { .offset = 0x9e, .value = 0x25, },
+		[36] = { .offset = 0x9f, .value = 0x03, },
+		[37] = { .offset = 0xa0, .value = 0x28, },
+		[38] = { .offset = 0xa1, .value = 0x01, },
+		[39] = { .offset = 0xa2, .value = 0x28, },
+		[40] = { .offset = 0xa3, .value = 0x05, },
+		[41] = { .offset = 0xb6, .value = 0x09, },
+		[42] = { .offset = 0xb8, .value = 0x00, },
+		[43] = { .offset = 0xb9, .value = 0xa0, },
+		[44] = { .offset = 0xba, .value = 0x00, },
+		[45] = { .offset = 0xbb, .value = 0x20, },
+		[46] = { .offset = 0x10, .value = 0x00, },
+		[47] = { .offset = 0x11, .value = 0xa0, },
+		[48] = { .offset = 0x12, .value = 0x02, },
+		[49] = { .offset = 0x20, .value = 0x00, },
+		[50] = { .offset = 0x22, .value = 0x00, },
+		[51] = { .offset = 0x23, .value = 0x00, },
+		[52] = { .offset = 0x24, .value = 0x00, },
+		[53] = { .offset = 0x25, .value = 0x00, },
+		[54] = { .offset = 0x8c, .value = 0x10, },
+		[55] = { .offset = 0x8d, .value = 0x02, },
+		[56] = { .offset = 0x8e, .value = 0x10, },
+		[57] = { .offset = 0x8f, .value = 0x00, },
+		[58] = { .offset = 0x90, .value = 0xff, },
+		[59] = { .offset = 0x91, .value = 0x07, },
+		[60] = { .offset = 0x92, .value = 0xa0, },
+		[61] = { .offset = 0x93, .value = 0x02, },
+		[62] = { .offset = 0xa5, .value = 0x00, },
+		[63] = { .offset = 0xa6, .value = 0x00, },
+		[64] = { .offset = 0xa7, .value = 0x00, },
+		[65] = { .offset = 0xa8, .value = 0x00, },
+		[66] = { .offset = 0xa9, .value = 0x04, },
+		[67] = { .offset = 0xaa, .value = 0x70, },
+		[68] = { .offset = 0xab, .value = 0x4f, },
+		[69] = { .offset = 0xac, .value = 0x00, },
+		[70] = { .offset = 0xa4, .value = 0x84, },
+		[71] = { .offset = 0x7e, .value = 0x18, },
+		[72] = { .offset = 0x84, .value = 0x00, },
+		[73] = { .offset = 0x85, .value = 0x00, },
+		[74] = { .offset = 0x86, .value = 0x00, },
+		[75] = { .offset = 0x87, .value = 0x00, },
+		[76] = { .offset = 0x88, .value = 0x00, },
+		[77] = { .offset = 0x89, .value = 0x00, },
+		[78] = { .offset = 0x8a, .value = 0x00, },
+		[79] = { .offset = 0x8b, .value = 0x00, },
+		[80] = { .offset = 0x26, .value = 0x00, },
+		[81] = { .offset = 0x27, .value = 0x00, },
+		[82] = { .offset = 0xad, .value = 0x00, },
+		[83] = { .offset = 0x08, .value = 0x34, }, /* 0x35 */
+		[84] = { .offset = 0x41, .value = 0x00, },
+		[85] = { .offset = 0xc0, .value = 0x01, },
+	},
+};
+
+static const struct ns2501_reg regs_init[] = {
+	[0] = { .offset = 0x35, .value = 0xff, },
+	[1] = { .offset = 0x34, .value = 0x00, },
+	[2] = { .offset = 0x08, .value = 0x30, },
+};
+
 struct ns2501_priv {
-	//I2CDevRec d;
 	bool quiet;
-	int reg_8_shadow;
-	int reg_8_set;
-	// Shadow registers for i915
-	int dvoc;
-	int pll_a;
-	int srcdim;
-	int fw_blc;
+	const struct ns2501_reg *regs;
 };
 
 #define NSPTR(d) ((NS2501Ptr)(d->DriverPrivate.ptr))
@@ -205,11 +486,9 @@
 		goto out;
 	}
 	ns->quiet = false;
-	ns->reg_8_set = 0;
-	ns->reg_8_shadow =
-	    NS2501_8_PD | NS2501_8_BPAS | NS2501_8_VEN | NS2501_8_HEN;
 
 	DRM_DEBUG_KMS("init ns2501 dvo controller successfully!\n");
+
 	return true;
 
 out:
@@ -242,9 +521,9 @@
 	 * of the panel in here so we could always accept it
 	 * by disabling the scaler.
 	 */
-	if ((mode->hdisplay == 800 && mode->vdisplay == 600) ||
-	    (mode->hdisplay == 640 && mode->vdisplay == 480) ||
-	    (mode->hdisplay == 1024 && mode->vdisplay == 768)) {
+	if ((mode->hdisplay == 640 && mode->vdisplay == 480 && mode->clock == 25175) ||
+	    (mode->hdisplay == 800 && mode->vdisplay == 600 && mode->clock == 40000) ||
+	    (mode->hdisplay == 1024 && mode->vdisplay == 768 && mode->clock == 65000)) {
 		return MODE_OK;
 	} else {
 		return MODE_ONE_SIZE;	/* Is this a reasonable error? */
@@ -255,180 +534,30 @@
 			    struct drm_display_mode *mode,
 			    struct drm_display_mode *adjusted_mode)
 {
-	bool ok;
-	int retries = 10;
 	struct ns2501_priv *ns = (struct ns2501_priv *)(dvo->dev_priv);
+	int mode_idx, i;
 
 	DRM_DEBUG_KMS
 	    ("set mode (hdisplay=%d,htotal=%d,vdisplay=%d,vtotal=%d).\n",
 	     mode->hdisplay, mode->htotal, mode->vdisplay, mode->vtotal);
 
-	/*
-	 * Where do I find the native resolution for which scaling is not required???
-	 *
-	 * First trigger the DVO on as otherwise the chip does not appear on the i2c
-	 * bus.
-	 */
-	do {
-		ok = true;
+	if (mode->hdisplay == 640 && mode->vdisplay == 480)
+		mode_idx = MODE_640x480;
+	else if (mode->hdisplay == 800 && mode->vdisplay == 600)
+		mode_idx = MODE_800x600;
+	else if (mode->hdisplay == 1024 && mode->vdisplay == 768)
+		mode_idx = MODE_1024x768;
+	else
+		return;
+
+	/* Hopefully doing it every time won't hurt... */
+	for (i = 0; i < ARRAY_SIZE(regs_init); i++)
+		ns2501_writeb(dvo, regs_init[i].offset, regs_init[i].value);
 
-		if (mode->hdisplay == 800 && mode->vdisplay == 600) {
-			/* mode 277 */
-			ns->reg_8_shadow &= ~NS2501_8_BPAS;
-			DRM_DEBUG_KMS("switching to 800x600\n");
-
-			/*
-			 * No, I do not know where this data comes from.
-			 * It is just what the video bios left in the DVO, so
-			 * I'm just copying it here over.
-			 * This also means that I cannot support any other modes
-			 * except the ones supported by the bios.
-			 */
-			ok &= ns2501_writeb(dvo, 0x11, 0xc8);	// 0xc7 also works.
-			ok &= ns2501_writeb(dvo, 0x1b, 0x19);
-			ok &= ns2501_writeb(dvo, 0x1c, 0x62);	// VBIOS left 0x64 here, but 0x62 works nicer
-			ok &= ns2501_writeb(dvo, 0x1d, 0x02);
-
-			ok &= ns2501_writeb(dvo, 0x34, 0x03);
-			ok &= ns2501_writeb(dvo, 0x35, 0xff);
-
-			ok &= ns2501_writeb(dvo, 0x80, 0x27);
-			ok &= ns2501_writeb(dvo, 0x81, 0x03);
-			ok &= ns2501_writeb(dvo, 0x82, 0x41);
-			ok &= ns2501_writeb(dvo, 0x83, 0x05);
-
-			ok &= ns2501_writeb(dvo, 0x8d, 0x02);
-			ok &= ns2501_writeb(dvo, 0x8e, 0x04);
-			ok &= ns2501_writeb(dvo, 0x8f, 0x00);
-
-			ok &= ns2501_writeb(dvo, 0x90, 0xfe);	/* vertical. VBIOS left 0xff here, but 0xfe works better */
-			ok &= ns2501_writeb(dvo, 0x91, 0x07);
-			ok &= ns2501_writeb(dvo, 0x94, 0x00);
-			ok &= ns2501_writeb(dvo, 0x95, 0x00);
-
-			ok &= ns2501_writeb(dvo, 0x96, 0x00);
-
-			ok &= ns2501_writeb(dvo, 0x99, 0x00);
-			ok &= ns2501_writeb(dvo, 0x9a, 0x88);
-
-			ok &= ns2501_writeb(dvo, 0x9c, 0x23);	/* Looks like first and last line of the image. */
-			ok &= ns2501_writeb(dvo, 0x9d, 0x00);
-			ok &= ns2501_writeb(dvo, 0x9e, 0x25);
-			ok &= ns2501_writeb(dvo, 0x9f, 0x03);
-
-			ok &= ns2501_writeb(dvo, 0xa4, 0x80);
-
-			ok &= ns2501_writeb(dvo, 0xb6, 0x00);
-
-			ok &= ns2501_writeb(dvo, 0xb9, 0xc8);	/* horizontal? */
-			ok &= ns2501_writeb(dvo, 0xba, 0x00);	/* horizontal? */
-
-			ok &= ns2501_writeb(dvo, 0xc0, 0x05);	/* horizontal? */
-			ok &= ns2501_writeb(dvo, 0xc1, 0xd7);
-
-			ok &= ns2501_writeb(dvo, 0xc2, 0x00);
-			ok &= ns2501_writeb(dvo, 0xc3, 0xf8);
-
-			ok &= ns2501_writeb(dvo, 0xc4, 0x03);
-			ok &= ns2501_writeb(dvo, 0xc5, 0x1a);
-
-			ok &= ns2501_writeb(dvo, 0xc6, 0x00);
-			ok &= ns2501_writeb(dvo, 0xc7, 0x73);
-			ok &= ns2501_writeb(dvo, 0xc8, 0x02);
-
-		} else if (mode->hdisplay == 640 && mode->vdisplay == 480) {
-			/* mode 274 */
-			DRM_DEBUG_KMS("switching to 640x480\n");
-			/*
-			 * No, I do not know where this data comes from.
-			 * It is just what the video bios left in the DVO, so
-			 * I'm just copying it here over.
-			 * This also means that I cannot support any other modes
-			 * except the ones supported by the bios.
-			 */
-			ns->reg_8_shadow &= ~NS2501_8_BPAS;
-
-			ok &= ns2501_writeb(dvo, 0x11, 0xa0);
-			ok &= ns2501_writeb(dvo, 0x1b, 0x11);
-			ok &= ns2501_writeb(dvo, 0x1c, 0x54);
-			ok &= ns2501_writeb(dvo, 0x1d, 0x03);
-
-			ok &= ns2501_writeb(dvo, 0x34, 0x03);
-			ok &= ns2501_writeb(dvo, 0x35, 0xff);
-
-			ok &= ns2501_writeb(dvo, 0x80, 0xff);
-			ok &= ns2501_writeb(dvo, 0x81, 0x07);
-			ok &= ns2501_writeb(dvo, 0x82, 0x3d);
-			ok &= ns2501_writeb(dvo, 0x83, 0x05);
-
-			ok &= ns2501_writeb(dvo, 0x8d, 0x02);
-			ok &= ns2501_writeb(dvo, 0x8e, 0x10);
-			ok &= ns2501_writeb(dvo, 0x8f, 0x00);
-
-			ok &= ns2501_writeb(dvo, 0x90, 0xff);	/* vertical */
-			ok &= ns2501_writeb(dvo, 0x91, 0x07);
-			ok &= ns2501_writeb(dvo, 0x94, 0x00);
-			ok &= ns2501_writeb(dvo, 0x95, 0x00);
-
-			ok &= ns2501_writeb(dvo, 0x96, 0x05);
-
-			ok &= ns2501_writeb(dvo, 0x99, 0x00);
-			ok &= ns2501_writeb(dvo, 0x9a, 0x88);
-
-			ok &= ns2501_writeb(dvo, 0x9c, 0x24);
-			ok &= ns2501_writeb(dvo, 0x9d, 0x00);
-			ok &= ns2501_writeb(dvo, 0x9e, 0x25);
-			ok &= ns2501_writeb(dvo, 0x9f, 0x03);
-
-			ok &= ns2501_writeb(dvo, 0xa4, 0x84);
-
-			ok &= ns2501_writeb(dvo, 0xb6, 0x09);
-
-			ok &= ns2501_writeb(dvo, 0xb9, 0xa0);	/* horizontal? */
-			ok &= ns2501_writeb(dvo, 0xba, 0x00);	/* horizontal? */
-
-			ok &= ns2501_writeb(dvo, 0xc0, 0x05);	/* horizontal? */
-			ok &= ns2501_writeb(dvo, 0xc1, 0x90);
-
-			ok &= ns2501_writeb(dvo, 0xc2, 0x00);
-			ok &= ns2501_writeb(dvo, 0xc3, 0x0f);
-
-			ok &= ns2501_writeb(dvo, 0xc4, 0x03);
-			ok &= ns2501_writeb(dvo, 0xc5, 0x16);
-
-			ok &= ns2501_writeb(dvo, 0xc6, 0x00);
-			ok &= ns2501_writeb(dvo, 0xc7, 0x02);
-			ok &= ns2501_writeb(dvo, 0xc8, 0x02);
-
-		} else if (mode->hdisplay == 1024 && mode->vdisplay == 768) {
-			/* mode 280 */
-			DRM_DEBUG_KMS("switching to 1024x768\n");
-			/*
-			 * This might or might not work, actually. I'm silently
-			 * assuming here that the native panel resolution is
-			 * 1024x768. If not, then this leaves the scaler disabled
-			 * generating a picture that is likely not the expected.
-			 *
-			 * Problem is that I do not know where to take the panel
-			 * dimensions from.
-			 *
-			 * Enable the bypass, scaling not required.
-			 *
-			 * The scaler registers are irrelevant here....
-			 *
-			 */
-			ns->reg_8_shadow |= NS2501_8_BPAS;
-			ok &= ns2501_writeb(dvo, 0x37, 0x44);
-		} else {
-			/*
-			 * Data not known. Bummer!
-			 * Hopefully, the code should not go here
-			 * as mode_OK delivered no other modes.
-			 */
-			ns->reg_8_shadow |= NS2501_8_BPAS;
-		}
-		ok &= ns2501_writeb(dvo, NS2501_REG8, ns->reg_8_shadow);
-	} while (!ok && retries--);
+	ns->regs = regs_1024x768[mode_idx];
+
+	for (i = 0; i < 84; i++)
+		ns2501_writeb(dvo, ns->regs[i].offset, ns->regs[i].value);
 }
 
 /* set the NS2501 power state */
@@ -439,60 +568,46 @@
 	if (!ns2501_readb(dvo, NS2501_REG8, &ch))
 		return false;
 
-	if (ch & NS2501_8_PD)
-		return true;
-	else
-		return false;
+	return ch & NS2501_8_PD;
 }
 
 /* set the NS2501 power state */
 static void ns2501_dpms(struct intel_dvo_device *dvo, bool enable)
 {
-	bool ok;
-	int retries = 10;
 	struct ns2501_priv *ns = (struct ns2501_priv *)(dvo->dev_priv);
-	unsigned char ch;
 
 	DRM_DEBUG_KMS("Trying set the dpms of the DVO to %i\n", enable);
 
-	ch = ns->reg_8_shadow;
+	if (enable) {
+		if (WARN_ON(ns->regs[83].offset != 0x08 ||
+			    ns->regs[84].offset != 0x41 ||
+			    ns->regs[85].offset != 0xc0))
+			return;
 
-	if (enable)
-		ch |= NS2501_8_PD;
-	else
-		ch &= ~NS2501_8_PD;
+		ns2501_writeb(dvo, 0xc0, ns->regs[85].value | 0x08);
 
-	if (ns->reg_8_set == 0 || ns->reg_8_shadow != ch) {
-		ns->reg_8_set = 1;
-		ns->reg_8_shadow = ch;
-
-		do {
-			ok = true;
-			ok &= ns2501_writeb(dvo, NS2501_REG8, ch);
-			ok &=
-			    ns2501_writeb(dvo, 0x34,
-					  enable ? 0x03 : 0x00);
-			ok &=
-			    ns2501_writeb(dvo, 0x35,
-					  enable ? 0xff : 0x00);
-		} while (!ok && retries--);
-	}
-}
+		ns2501_writeb(dvo, 0x41, ns->regs[84].value);
 
-static void ns2501_dump_regs(struct intel_dvo_device *dvo)
-{
-	uint8_t val;
+		ns2501_writeb(dvo, 0x34, 0x01);
+		msleep(15);
+
+		ns2501_writeb(dvo, 0x08, 0x35);
+		if (!(ns->regs[83].value & NS2501_8_BPAS))
+			ns2501_writeb(dvo, 0x08, 0x31);
+		msleep(200);
 
-	ns2501_readb(dvo, NS2501_FREQ_LO, &val);
-	DRM_DEBUG_KMS("NS2501_FREQ_LO: 0x%02x\n", val);
-	ns2501_readb(dvo, NS2501_FREQ_HI, &val);
-	DRM_DEBUG_KMS("NS2501_FREQ_HI: 0x%02x\n", val);
-	ns2501_readb(dvo, NS2501_REG8, &val);
-	DRM_DEBUG_KMS("NS2501_REG8: 0x%02x\n", val);
-	ns2501_readb(dvo, NS2501_REG9, &val);
-	DRM_DEBUG_KMS("NS2501_REG9: 0x%02x\n", val);
-	ns2501_readb(dvo, NS2501_REGC, &val);
-	DRM_DEBUG_KMS("NS2501_REGC: 0x%02x\n", val);
+		ns2501_writeb(dvo, 0x34, 0x03);
+
+		ns2501_writeb(dvo, 0xc0, ns->regs[85].value);
+	} else {
+		ns2501_writeb(dvo, 0x34, 0x01);
+		msleep(200);
+
+		ns2501_writeb(dvo, 0x08, 0x34);
+		msleep(15);
+
+		ns2501_writeb(dvo, 0x34, 0x00);
+	}
 }
 
 static void ns2501_destroy(struct intel_dvo_device *dvo)
@@ -512,6 +627,5 @@
 	.mode_set = ns2501_mode_set,
 	.dpms = ns2501_dpms,
 	.get_hw_state = ns2501_get_hw_state,
-	.dump_regs = ns2501_dump_regs,
 	.destroy = ns2501_destroy,
 };
diff -urN a/drivers/gpu/drm/i915/i915_cmd_parser.c b/drivers/gpu/drm/i915/i915_cmd_parser.c
--- a/drivers/gpu/drm/i915/i915_cmd_parser.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_cmd_parser.c	2014-11-22 14:37:49.330700418 -0700
@@ -73,7 +73,7 @@
  * those commands required by the parser. This generally works because command
  * opcode ranges have standard command length encodings. So for commands that
  * the parser does not need to check, it can easily skip them. This is
- * implementated via a per-ring length decoding vfunc.
+ * implemented via a per-ring length decoding vfunc.
  *
  * Unfortunately, there are a number of commands that do not follow the standard
  * length encoding for their opcode range, primarily amongst the MI_* commands.
@@ -138,6 +138,11 @@
 			.mask = MI_GLOBAL_GTT,
 			.expected = 0,
 	      }},						       ),
+	/*
+	 * MI_BATCH_BUFFER_START requires some special handling. It's not
+	 * really a 'skip' action but it doesn't seem like it's worth adding
+	 * a new action. See i915_parse_cmds().
+	 */
 	CMD(  MI_BATCH_BUFFER_START,            SMI,   !F,  0xFF,   S  ),
 };
 
@@ -408,6 +413,8 @@
 	REG64(PS_INVOCATION_COUNT),
 	REG64(PS_DEPTH_COUNT),
 	OACONTROL, /* Only allowed for LRI and SRM. See below. */
+	REG64(MI_PREDICATE_SRC0),
+	REG64(MI_PREDICATE_SRC1),
 	GEN7_3DPRIM_END_OFFSET,
 	GEN7_3DPRIM_START_VERTEX,
 	GEN7_3DPRIM_VERTEX_COUNT,
@@ -501,7 +508,7 @@
 	return 0;
 }
 
-static bool validate_cmds_sorted(struct intel_engine_cs *ring,
+static bool validate_cmds_sorted(struct intel_engine_cs *engine,
 				 const struct drm_i915_cmd_table *cmd_tables,
 				 int cmd_table_count)
 {
@@ -523,7 +530,7 @@
 
 			if (curr < previous) {
 				DRM_ERROR("CMD: table not sorted ring=%d table=%d entry=%d cmd=0x%08X prev=0x%08X\n",
-					  ring->id, i, j, curr, previous);
+					  engine->id, i, j, curr, previous);
 				ret = false;
 			}
 
@@ -555,11 +562,11 @@
 	return ret;
 }
 
-static bool validate_regs_sorted(struct intel_engine_cs *ring)
+static bool validate_regs_sorted(struct intel_engine_cs *engine)
 {
-	return check_sorted(ring->id, ring->reg_table, ring->reg_count) &&
-		check_sorted(ring->id, ring->master_reg_table,
-			     ring->master_reg_count);
+	return check_sorted(engine->id, engine->reg_table, engine->reg_count) &&
+		check_sorted(engine->id, engine->master_reg_table,
+			     engine->master_reg_count);
 }
 
 struct cmd_node {
@@ -583,13 +590,13 @@
  */
 #define CMD_HASH_MASK STD_MI_OPCODE_MASK
 
-static int init_hash_table(struct intel_engine_cs *ring,
+static int init_hash_table(struct intel_engine_cs *engine,
 			   const struct drm_i915_cmd_table *cmd_tables,
 			   int cmd_table_count)
 {
 	int i, j;
 
-	hash_init(ring->cmd_hash);
+	hash_init(engine->cmd_hash);
 
 	for (i = 0; i < cmd_table_count; i++) {
 		const struct drm_i915_cmd_table *table = &cmd_tables[i];
@@ -604,7 +611,7 @@
 				return -ENOMEM;
 
 			desc_node->desc = desc;
-			hash_add(ring->cmd_hash, &desc_node->node,
+			hash_add(engine->cmd_hash, &desc_node->node,
 				 desc->cmd.value & CMD_HASH_MASK);
 		}
 	}
@@ -612,21 +619,21 @@
 	return 0;
 }
 
-static void fini_hash_table(struct intel_engine_cs *ring)
+static void fini_hash_table(struct intel_engine_cs *engine)
 {
 	struct hlist_node *tmp;
 	struct cmd_node *desc_node;
 	int i;
 
-	hash_for_each_safe(ring->cmd_hash, i, tmp, desc_node, node) {
+	hash_for_each_safe(engine->cmd_hash, i, tmp, desc_node, node) {
 		hash_del(&desc_node->node);
 		kfree(desc_node);
 	}
 }
 
 /**
- * i915_cmd_parser_init_ring() - set cmd parser related fields for a ringbuffer
- * @ring: the ringbuffer to initialize
+ * i915_cmd_parser_init_engine() - set cmd parser related fields for a ringbuffer
+ * @engine: the ringbuffer to initialize
  *
  * Optionally initializes fields related to batch buffer command parsing in the
  * struct intel_engine_cs based on whether the platform requires software
@@ -634,18 +641,18 @@
  *
  * Return: non-zero if initialization fails
  */
-int i915_cmd_parser_init_ring(struct intel_engine_cs *ring)
+int i915_cmd_parser_init_engine(struct intel_engine_cs *engine)
 {
 	const struct drm_i915_cmd_table *cmd_tables;
 	int cmd_table_count;
 	int ret;
 
-	if (!IS_GEN7(ring->dev))
+	if (!IS_GEN7(engine->i915))
 		return 0;
 
-	switch (ring->id) {
+	switch (engine->id) {
 	case RCS:
-		if (IS_HASWELL(ring->dev)) {
+		if (IS_HASWELL(engine->i915)) {
 			cmd_tables = hsw_render_ring_cmds;
 			cmd_table_count =
 				ARRAY_SIZE(hsw_render_ring_cmds);
@@ -654,26 +661,26 @@
 			cmd_table_count = ARRAY_SIZE(gen7_render_cmds);
 		}
 
-		ring->reg_table = gen7_render_regs;
-		ring->reg_count = ARRAY_SIZE(gen7_render_regs);
+		engine->reg_table = gen7_render_regs;
+		engine->reg_count = ARRAY_SIZE(gen7_render_regs);
 
-		if (IS_HASWELL(ring->dev)) {
-			ring->master_reg_table = hsw_master_regs;
-			ring->master_reg_count = ARRAY_SIZE(hsw_master_regs);
+		if (IS_HASWELL(engine->i915)) {
+			engine->master_reg_table = hsw_master_regs;
+			engine->master_reg_count = ARRAY_SIZE(hsw_master_regs);
 		} else {
-			ring->master_reg_table = ivb_master_regs;
-			ring->master_reg_count = ARRAY_SIZE(ivb_master_regs);
+			engine->master_reg_table = ivb_master_regs;
+			engine->master_reg_count = ARRAY_SIZE(ivb_master_regs);
 		}
 
-		ring->get_cmd_length_mask = gen7_render_get_cmd_length_mask;
+		engine->get_cmd_length_mask = gen7_render_get_cmd_length_mask;
 		break;
 	case VCS:
 		cmd_tables = gen7_video_cmds;
 		cmd_table_count = ARRAY_SIZE(gen7_video_cmds);
-		ring->get_cmd_length_mask = gen7_bsd_get_cmd_length_mask;
+		engine->get_cmd_length_mask = gen7_bsd_get_cmd_length_mask;
 		break;
 	case BCS:
-		if (IS_HASWELL(ring->dev)) {
+		if (IS_HASWELL(engine->i915)) {
 			cmd_tables = hsw_blt_ring_cmds;
 			cmd_table_count = ARRAY_SIZE(hsw_blt_ring_cmds);
 		} else {
@@ -681,70 +688,68 @@
 			cmd_table_count = ARRAY_SIZE(gen7_blt_cmds);
 		}
 
-		ring->reg_table = gen7_blt_regs;
-		ring->reg_count = ARRAY_SIZE(gen7_blt_regs);
+		engine->reg_table = gen7_blt_regs;
+		engine->reg_count = ARRAY_SIZE(gen7_blt_regs);
 
-		if (IS_HASWELL(ring->dev)) {
-			ring->master_reg_table = hsw_master_regs;
-			ring->master_reg_count = ARRAY_SIZE(hsw_master_regs);
+		if (IS_HASWELL(engine->i915)) {
+			engine->master_reg_table = hsw_master_regs;
+			engine->master_reg_count = ARRAY_SIZE(hsw_master_regs);
 		} else {
-			ring->master_reg_table = ivb_master_regs;
-			ring->master_reg_count = ARRAY_SIZE(ivb_master_regs);
+			engine->master_reg_table = ivb_master_regs;
+			engine->master_reg_count = ARRAY_SIZE(ivb_master_regs);
 		}
 
-		ring->get_cmd_length_mask = gen7_blt_get_cmd_length_mask;
+		engine->get_cmd_length_mask = gen7_blt_get_cmd_length_mask;
 		break;
 	case VECS:
 		cmd_tables = hsw_vebox_cmds;
 		cmd_table_count = ARRAY_SIZE(hsw_vebox_cmds);
 		/* VECS can use the same length_mask function as VCS */
-		ring->get_cmd_length_mask = gen7_bsd_get_cmd_length_mask;
+		engine->get_cmd_length_mask = gen7_bsd_get_cmd_length_mask;
 		break;
 	default:
-		DRM_ERROR("CMD: cmd_parser_init with unknown ring: %d\n",
-			  ring->id);
+		DRM_ERROR("CMD: cmd_parser_init with unknown engine: %d\n",
+			  engine->id);
 		BUG();
 	}
 
-	BUG_ON(!validate_cmds_sorted(ring, cmd_tables, cmd_table_count));
-	BUG_ON(!validate_regs_sorted(ring));
+	BUG_ON(!validate_cmds_sorted(engine, cmd_tables, cmd_table_count));
+	BUG_ON(!validate_regs_sorted(engine));
 
-	if (hash_empty(ring->cmd_hash)) {
-		ret = init_hash_table(ring, cmd_tables, cmd_table_count);
-		if (ret) {
-			DRM_ERROR("CMD: cmd_parser_init failed!\n");
-			fini_hash_table(ring);
-			return ret;
-		}
+	ret = init_hash_table(engine, cmd_tables, cmd_table_count);
+	if (ret) {
+		DRM_ERROR("CMD: cmd_parser_init failed!\n");
+		fini_hash_table(engine);
+		return ret;
 	}
 
-	ring->needs_cmd_parser = true;
+	engine->needs_cmd_parser = true;
 
 	return 0;
 }
 
 /**
- * i915_cmd_parser_fini_ring() - clean up cmd parser related fields
- * @ring: the ringbuffer to clean up
+ * i915_cmd_parser_fini_engine() - clean up cmd parser related fields
+ * @engine: the ringbuffer to clean up
  *
  * Releases any resources related to command parsing that may have been
- * initialized for the specified ring.
+ * initialized for the specified engine.
  */
-void i915_cmd_parser_fini_ring(struct intel_engine_cs *ring)
+void i915_cmd_parser_fini_engine(struct intel_engine_cs *engine)
 {
-	if (!ring->needs_cmd_parser)
+	if (!engine->needs_cmd_parser)
 		return;
 
-	fini_hash_table(ring);
+	fini_hash_table(engine);
 }
 
 static const struct drm_i915_cmd_descriptor*
-find_cmd_in_table(struct intel_engine_cs *ring,
+find_cmd_in_table(struct intel_engine_cs *engine,
 		  u32 cmd_header)
 {
 	struct cmd_node *desc_node;
 
-	hash_for_each_possible(ring->cmd_hash, desc_node, node,
+	hash_for_each_possible(engine->cmd_hash, desc_node, node,
 			       cmd_header & CMD_HASH_MASK) {
 		const struct drm_i915_cmd_descriptor *desc = desc_node->desc;
 		u32 masked_cmd = desc->cmd.mask & cmd_header;
@@ -761,23 +766,23 @@
  * Returns a pointer to a descriptor for the command specified by cmd_header.
  *
  * The caller must supply space for a default descriptor via the default_desc
- * parameter. If no descriptor for the specified command exists in the ring's
+ * parameter. If no descriptor for the specified command exists in the engine's
  * command parser tables, this function fills in default_desc based on the
- * ring's default length encoding and returns default_desc.
+ * engine's default length encoding and returns default_desc.
  */
 static const struct drm_i915_cmd_descriptor*
-find_cmd(struct intel_engine_cs *ring,
+find_cmd(struct intel_engine_cs *engine,
 	 u32 cmd_header,
 	 struct drm_i915_cmd_descriptor *default_desc)
 {
 	const struct drm_i915_cmd_descriptor *desc;
 	u32 mask;
 
-	desc = find_cmd_in_table(ring, cmd_header);
+	desc = find_cmd_in_table(engine, cmd_header);
 	if (desc)
 		return desc;
 
-	mask = ring->get_cmd_length_mask(cmd_header);
+	mask = engine->get_cmd_length_mask(cmd_header);
 	if (!mask)
 		return NULL;
 
@@ -834,33 +839,26 @@
 }
 
 /**
- * i915_needs_cmd_parser() - should a given ring use software command parsing?
- * @ring: the ring in question
+ * i915_needs_cmd_parser() - should a given engine use software command parsing?
+ * @engine: the engine in question
  *
  * Only certain platforms require software batch buffer command parsing, and
- * only when enabled via module paramter.
+ * only when enabled via module parameter.
  *
- * Return: true if the ring requires software command parsing
+ * Return: true if the engine requires software command parsing
  */
-bool i915_needs_cmd_parser(struct intel_engine_cs *ring)
+bool i915_needs_cmd_parser(struct intel_engine_cs *engine)
 {
-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
-
-	if (!ring->needs_cmd_parser)
+	if (!engine->needs_cmd_parser)
 		return false;
 
-	/*
-	 * XXX: VLV is Gen7 and therefore has cmd_tables, but has PPGTT
-	 * disabled. That will cause all of the parser's PPGTT checks to
-	 * fail. For now, disable parsing when PPGTT is off.
-	 */
-	if (!dev_priv->mm.aliasing_ppgtt)
+	if (USES_PPGTT(engine->dev))
 		return false;
 
-	return (i915.enable_cmd_parser == 1);
+	return (i915_module.enable_cmd_parser == 1);
 }
 
-static bool check_cmd(const struct intel_engine_cs *ring,
+static bool check_cmd(const struct intel_engine_cs *engine,
 		      const struct drm_i915_cmd_descriptor *desc,
 		      const u32 *cmd,
 		      const bool is_master,
@@ -890,23 +888,25 @@
 		 * OACONTROL writes to only MI_LOAD_REGISTER_IMM commands.
 		 */
 		if (reg_addr == OACONTROL) {
-			if (desc->cmd.value == MI_LOAD_REGISTER_MEM)
+			if (desc->cmd.value == MI_LOAD_REGISTER_MEM) {
+				DRM_DEBUG_DRIVER("CMD: Rejected LRM to OACONTROL\n");
 				return false;
+			}
 
 			if (desc->cmd.value == MI_LOAD_REGISTER_IMM(1))
 				*oacontrol_set = (cmd[2] != 0);
 		}
 
-		if (!valid_reg(ring->reg_table,
-			       ring->reg_count, reg_addr)) {
+		if (!valid_reg(engine->reg_table,
+			       engine->reg_count, reg_addr)) {
 			if (!is_master ||
-			    !valid_reg(ring->master_reg_table,
-				       ring->master_reg_count,
+			    !valid_reg(engine->master_reg_table,
+				       engine->master_reg_count,
 				       reg_addr)) {
-				DRM_DEBUG_DRIVER("CMD: Rejected register 0x%08X in command: 0x%08X (ring=%d)\n",
+				DRM_DEBUG_DRIVER("CMD: Rejected register 0x%08X in command: 0x%08X (engine=%d)\n",
 						 reg_addr,
 						 *cmd,
-						 ring->id);
+						 engine->id);
 				return false;
 			}
 		}
@@ -935,11 +935,11 @@
 				desc->bits[i].mask;
 
 			if (dword != desc->bits[i].expected) {
-				DRM_DEBUG_DRIVER("CMD: Rejected command 0x%08X for bitmask 0x%08X (exp=0x%08X act=0x%08X) (ring=%d)\n",
+				DRM_DEBUG_DRIVER("CMD: Rejected command 0x%08X for bitmask 0x%08X (exp=0x%08X act=0x%08X) (engine=%d)\n",
 						 *cmd,
 						 desc->bits[i].mask,
 						 desc->bits[i].expected,
-						 dword, ring->id);
+						 dword, engine->id);
 				return false;
 			}
 		}
@@ -952,7 +952,7 @@
 
 /**
  * i915_parse_cmds() - parse a submitted batch buffer for privilege violations
- * @ring: the ring on which the batch is to execute
+ * @engine: the engine on which the batch is to execute
  * @batch_obj: the batch buffer in question
  * @batch_start_offset: byte offset in the batch at which execution starts
  * @is_master: is the submitting process the drm master?
@@ -960,9 +960,10 @@
  * Parses the specified batch buffer looking for privilege violations as
  * described in the overview.
  *
- * Return: non-zero if the parser finds violations or otherwise fails
+ * Return: non-zero if the parser finds violations or otherwise fails; -EACCES
+ * if the batch appears legal but should use hardware parsing
  */
-int i915_parse_cmds(struct intel_engine_cs *ring,
+int i915_parse_cmds(struct intel_engine_cs *engine,
 		    struct drm_i915_gem_object *batch_obj,
 		    u32 batch_start_offset,
 		    bool is_master)
@@ -999,7 +1000,7 @@
 		if (*cmd == MI_BATCH_BUFFER_END)
 			break;
 
-		desc = find_cmd(ring, *cmd, &default_desc);
+		desc = find_cmd(engine, *cmd, &default_desc);
 		if (!desc) {
 			DRM_DEBUG_DRIVER("CMD: Unrecognized command: 0x%08X\n",
 					 *cmd);
@@ -1007,6 +1008,16 @@
 			break;
 		}
 
+		/*
+		 * If the batch buffer contains a chained batch, return an
+		 * error that tells the caller to abort and dispatch the
+		 * workload as a non-secure batch.
+		 */
+		if (desc->cmd.value == MI_BATCH_BUFFER_START) {
+			ret = -EACCES;
+			break;
+		}
+
 		if (desc->flags & CMD_DESC_FIXED)
 			length = desc->length.fixed;
 		else
@@ -1021,7 +1032,7 @@
 			break;
 		}
 
-		if (!check_cmd(ring, desc, cmd, is_master, &oacontrol_set)) {
+		if (!check_cmd(engine, desc, cmd, is_master, &oacontrol_set)) {
 			ret = -EINVAL;
 			break;
 		}
@@ -1061,6 +1072,8 @@
 	 *
 	 * 1. Initial version. Checks batches and reports violations, but leaves
 	 *    hardware parsing enabled (so does not allow new use cases).
+	 * 2. Allow access to the MI_PREDICATE_SRC0 and
+	 *    MI_PREDICATE_SRC1 registers.
 	 */
-	return 1;
+	return 2;
 }
diff -urN a/drivers/gpu/drm/i915/i915_debugfs.c b/drivers/gpu/drm/i915/i915_debugfs.c
--- a/drivers/gpu/drm/i915/i915_debugfs.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_debugfs.c	2014-11-22 14:37:49.330700418 -0700
@@ -116,32 +116,35 @@
 
 static inline const char *get_global_flag(struct drm_i915_gem_object *obj)
 {
-	return obj->has_global_gtt_mapping ? "g" : " ";
+	return i915_gem_obj_to_ggtt(obj) ? "g" : " ";
 }
 
 static void
 describe_obj(struct seq_file *m, struct drm_i915_gem_object *obj)
 {
 	struct i915_vma *vma;
-	int pin_count = 0;
+	int pin_count = 0, n;
 
-	seq_printf(m, "%pK: %s%s%s %8zdKiB %02x %02x %u %u %u%s%s%s",
+	seq_printf(m, "%pK: %s%s%s %8zdKiB %02x %02x [",
 		   &obj->base,
 		   get_pin_flag(obj),
 		   get_tiling_flag(obj),
 		   get_global_flag(obj),
 		   obj->base.size / 1024,
 		   obj->base.read_domains,
-		   obj->base.write_domain,
-		   obj->last_read_seqno,
-		   obj->last_write_seqno,
-		   obj->last_fenced_seqno,
-		   i915_cache_level_str(obj->cache_level),
+		   obj->base.write_domain);
+	for (n = 0; n < ARRAY_SIZE(obj->last_read); n++)
+		seq_printf(m, " %x",
+			   i915_request_seqno(obj->last_read[n].request));
+	seq_printf(m, " ] %x %x%s%s%s",
+		   i915_request_seqno(obj->last_write.request),
+		   i915_request_seqno(obj->last_fence.request),
+		   i915_cache_level_str(to_i915(obj->base.dev), obj->cache_level),
 		   obj->dirty ? " dirty" : "",
 		   obj->madv == I915_MADV_DONTNEED ? " purgeable" : "");
 	if (obj->base.name)
 		seq_printf(m, " (name: %d)", obj->base.name);
-	list_for_each_entry(vma, &obj->vma_list, vma_link)
+	list_for_each_entry(vma, &obj->vma_list, obj_link)
 		if (vma->pin_count > 0)
 			pin_count++;
 		seq_printf(m, " (pinned x %d)", pin_count);
@@ -149,7 +152,7 @@
 		seq_printf(m, " (display)");
 	if (obj->fence_reg != I915_FENCE_REG_NONE)
 		seq_printf(m, " (fence: %d)", obj->fence_reg);
-	list_for_each_entry(vma, &obj->vma_list, vma_link) {
+	list_for_each_entry(vma, &obj->vma_list, obj_link) {
 		if (!i915_is_ggtt(vma->vm))
 			seq_puts(m, " (pp");
 		else
@@ -168,15 +171,15 @@
 		*t = '\0';
 		seq_printf(m, " (%s mappable)", s);
 	}
-	if (obj->ring != NULL)
-		seq_printf(m, " (%s)", obj->ring->name);
+	if (obj->last_write.request)
+		seq_printf(m, " (%s)", obj->last_write.request->engine->name);
 	if (obj->frontbuffer_bits)
 		seq_printf(m, " (frontbuffer: 0x%03x)", obj->frontbuffer_bits);
 }
 
 static void describe_ctx(struct seq_file *m, struct intel_context *ctx)
 {
-	seq_putc(m, ctx->legacy_hw_ctx.initialized ? 'I' : 'i');
+	seq_putc(m, ctx->ring[RCS].initialized ? 'I' : 'i');
 	seq_putc(m, ctx->remap_slice ? 'R' : 'r');
 	seq_putc(m, ' ');
 }
@@ -301,7 +304,6 @@
 } while (0)
 
 struct file_stats {
-	struct drm_i915_file_private *file_priv;
 	int count;
 	size_t total, unbound;
 	size_t global, shared;
@@ -312,7 +314,6 @@
 {
 	struct drm_i915_gem_object *obj = ptr;
 	struct file_stats *stats = data;
-	struct i915_vma *vma;
 
 	stats->count++;
 	stats->total += obj->base.size;
@@ -320,33 +321,10 @@
 	if (obj->base.name || obj->base.dma_buf)
 		stats->shared += obj->base.size;
 
-	if (USES_FULL_PPGTT(obj->base.dev)) {
-		list_for_each_entry(vma, &obj->vma_list, vma_link) {
-			struct i915_hw_ppgtt *ppgtt;
-
-			if (!drm_mm_node_allocated(&vma->node))
-				continue;
-
-			if (i915_is_ggtt(vma->vm)) {
-				stats->global += obj->base.size;
-				continue;
-			}
-
-			ppgtt = container_of(vma->vm, struct i915_hw_ppgtt, base);
-			if (ppgtt->ctx && ppgtt->ctx->file_priv != stats->file_priv)
-				continue;
-
-			if (obj->ring) /* XXX per-vma statistic */
-				stats->active += obj->base.size;
-			else
-				stats->inactive += obj->base.size;
-
-			return 0;
-		}
-	} else {
+	if (!USES_FULL_PPGTT(obj->base.dev)) {
 		if (i915_gem_obj_ggtt_bound(obj)) {
 			stats->global += obj->base.size;
-			if (obj->ring)
+			if (obj->active)
 				stats->active += obj->base.size;
 			else
 				stats->inactive += obj->base.size;
@@ -360,6 +338,28 @@
 	return 0;
 }
 
+static int per_ctx_stats(int id, void *ptr, void *data)
+{
+	struct intel_context *ctx = ptr;
+	struct file_stats *stats = data;
+	struct i915_vma *vma;
+
+	if (ctx->ppgtt == NULL)
+		return 0;
+
+	list_for_each_entry(vma, &ctx->ppgtt->base.vma_list, vm_link) {
+		if (!drm_mm_node_allocated(&vma->node))
+			continue;
+
+		if (vma->active)
+			stats->active += vma->obj->base.size;
+		else
+			stats->inactive += vma->obj->base.size;
+	}
+
+	return 0;
+}
+
 #define count_vmas(list, member) do { \
 	list_for_each_entry(vma, list, member) { \
 		size += i915_gem_obj_ggtt_size(vma->obj); \
@@ -443,13 +443,15 @@
 
 	seq_putc(m, '\n');
 	list_for_each_entry_reverse(file, &dev->filelist, lhead) {
+		struct drm_i915_file_private *file_priv = file->driver_priv;
 		struct file_stats stats;
 		struct task_struct *task;
 
 		memset(&stats, 0, sizeof(stats));
-		stats.file_priv = file->driver_priv;
 		spin_lock(&file->table_lock);
 		idr_for_each(&file->object_idr, per_file_stats, &stats);
+		if (USES_FULL_PPGTT(obj->base.dev))
+			idr_for_each(&file_priv->context_idr, per_ctx_stats, &stats);
 		spin_unlock(&file->table_lock);
 		/*
 		 * Although we have a valid reference on file->pid, that does
@@ -476,6 +478,89 @@
 	return 0;
 }
 
+static int i915_gem_gtt_contents(struct seq_file *m, struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	gen6_gtt_pte_t __iomem *gtt_entries;
+	gen6_gtt_pte_t scratch_pte;
+	gen6_gtt_pte_t zero[8] = {};
+	int i, j, last_zero = 0;
+	int ret;
+
+	if (INTEL_INFO(dev)->gen < 6)
+		return 0;
+
+	ret = mutex_lock_interruptible(&dev->struct_mutex);
+	if (ret)
+		return ret;
+
+	gtt_entries = (gen6_gtt_pte_t __iomem *)dev_priv->gtt.gsm;
+	scratch_pte = dev_priv->gtt.base.pte_encode(dev_priv->gtt.base.scratch.addr, I915_CACHE_LLC, true, 0);
+	for (i = 0; i < gtt_total_entries(dev_priv->gtt); i += 8) {
+		gen6_gtt_pte_t pte[8];
+		int this_zero;
+
+		for (j = 0; j < 8; j++) {
+			pte[j] = ioread32(&gtt_entries[i+j]);
+			if (pte[j] == scratch_pte)
+				pte[j] = 0;
+			if ((pte[j] & 1) == 0)
+				pte[j] = 0;
+		}
+
+		this_zero = memcmp(pte, zero, sizeof(pte)) == 0;
+		if (last_zero && this_zero) {
+			if (last_zero++ == 1)
+				seq_puts(m, "...\n");
+			continue;
+		}
+
+		seq_printf(m, "[%08x] %08x %08x %08x %08x %08x %08x %08x %08x\n",
+			   i, pte[0], pte[1], pte[2], pte[3], pte[4], pte[5], pte[6], pte[7]);
+		last_zero = this_zero;
+	}
+
+	mutex_unlock(&dev->struct_mutex);
+
+	return 0;
+}
+
+static int obj_rank_by_ggtt(void *priv, struct list_head *A, struct list_head *B)
+{
+	struct drm_i915_gem_object *a = list_entry(A, typeof(*a), obj_exec_link);
+	struct drm_i915_gem_object *b = list_entry(B, typeof(*b), obj_exec_link);
+
+	return i915_gem_obj_ggtt_offset(a) - i915_gem_obj_ggtt_offset(b);
+}
+
+static int i915_gem_aperture_info(struct seq_file *m, void *data)
+{
+	struct drm_info_node *node = m->private;
+	struct drm_i915_gem_get_aperture arg;
+	int ret;
+
+	ret = i915_gem_get_aperture_ioctl(node->minor->dev, &arg, NULL);
+	if (ret)
+		return ret;
+
+	seq_printf(m, "Total size of the GTT: %llu bytes\n",
+		   arg.aper_size);
+	seq_printf(m, "Available space in the GTT: %llu bytes\n",
+		   arg.aper_available_size);
+	seq_printf(m, "Total size of the mappable aperture: %llu bytes\n",
+		   arg.map_total_size);
+	seq_printf(m, "Available space in the mappable aperture: %llu bytes\n",
+		   arg.map_available_size);
+	seq_printf(m, "Single largest space in the mappable aperture: %llu bytes\n",
+		   arg.map_largest_size);
+	seq_printf(m, "Available space for fences: %llu bytes\n",
+		   arg.fence_available_size);
+	seq_printf(m, "Single largest fence available: %llu bytes\n",
+		   arg.fence_largest_size);
+
+	return 0;
+}
+
 static int i915_gem_gtt_info(struct seq_file *m, void *data)
 {
 	struct drm_info_node *node = m->private;
@@ -483,6 +568,7 @@
 	uintptr_t list = (uintptr_t) node->info_ent->data;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_i915_gem_object *obj;
+	struct list_head sorted;
 	size_t total_obj_size, total_gtt_size;
 	int count, ret;
 
@@ -490,11 +576,27 @@
 	if (ret)
 		return ret;
 
+	INIT_LIST_HEAD(&sorted);
+
 	total_obj_size = total_gtt_size = count = 0;
 	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list) {
-		if (list == PINNED_LIST && !i915_gem_obj_is_pinned(obj))
+		struct i915_vma *vma = i915_gem_obj_to_ggtt(obj);
+
+		if (vma == NULL)
 			continue;
 
+		if (list == PINNED_LIST && vma->pin_count == 0)
+			continue;
+
+		list_add(&obj->obj_exec_link, &sorted);
+	}
+
+	list_sort(NULL, &sorted, obj_rank_by_ggtt);
+
+	while (!list_empty(&sorted)) {
+		obj = list_first_entry(&sorted, typeof(*obj), obj_exec_link);
+		list_del_init(&obj->obj_exec_link);
+
 		seq_puts(m, "   ");
 		describe_obj(m, obj);
 		seq_putc(m, '\n');
@@ -508,14 +610,17 @@
 	seq_printf(m, "Total %d objects, %zu bytes, %zu GTT size\n",
 		   count, total_obj_size, total_gtt_size);
 
-	return 0;
+	if (list == PINNED_LIST)
+		return 0;
+
+	return i915_gem_gtt_contents(m, dev);
 }
 
 static int i915_gem_pageflip_info(struct seq_file *m, void *data)
 {
 	struct drm_info_node *node = m->private;
 	struct drm_device *dev = node->minor->dev;
-	unsigned long flags;
+	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_crtc *crtc;
 	int ret;
 
@@ -528,12 +633,14 @@
 		const char plane = plane_name(crtc->plane);
 		struct intel_unpin_work *work;
 
-		spin_lock_irqsave(&dev->event_lock, flags);
+		spin_lock_irq(&dev->event_lock);
 		work = crtc->unpin_work;
 		if (work == NULL) {
 			seq_printf(m, "No flip due on pipe %c (plane %c)\n",
 				   pipe, plane);
 		} else {
+			u32 addr;
+
 			if (atomic_read(&work->pending) < INTEL_FLIP_COMPLETE) {
 				seq_printf(m, "Flip queued on pipe %c (plane %c)\n",
 					   pipe, plane);
@@ -541,26 +648,38 @@
 				seq_printf(m, "Flip pending (waiting for vsync) on pipe %c (plane %c)\n",
 					   pipe, plane);
 			}
+			if (work->flip_queued_request) {
+				struct i915_gem_request *rq =
+					work->flip_queued_request;
+				seq_printf(m, "Flip queued on %s at seqno %u, next seqno %u [current breadcrumb %u], completed? %d\n",
+					   rq->engine->name,
+					   rq->seqno, rq->i915->next_seqno,
+					   intel_engine_get_seqno(rq->engine),
+					   __i915_request_complete__wa(rq));
+			} else
+				seq_printf(m, "Flip not associated with any ring\n");
+			seq_printf(m, "Flip queued on frame %d, (was ready on frame %d), now %d\n",
+				   work->flip_queued_vblank,
+				   work->flip_ready_vblank,
+				   drm_vblank_count(dev, crtc->pipe));
 			if (work->enable_stall_check)
 				seq_puts(m, "Stall check enabled, ");
 			else
 				seq_puts(m, "Stall check waiting for page flip ioctl, ");
 			seq_printf(m, "%d prepares\n", atomic_read(&work->pending));
 
-			if (work->old_fb_obj) {
-				struct drm_i915_gem_object *obj = work->old_fb_obj;
-				if (obj)
-					seq_printf(m, "Old framebuffer gtt_offset 0x%08lx\n",
-						   i915_gem_obj_ggtt_offset(obj));
-			}
+			if (INTEL_INFO(dev)->gen >= 4)
+				addr = I915_HI_DISPBASE(I915_READ(DSPSURF(crtc->plane)));
+			else
+				addr = I915_READ(DSPADDR(crtc->plane));
+			seq_printf(m, "Current scanout address 0x%08x\n", addr);
+
 			if (work->pending_flip_obj) {
-				struct drm_i915_gem_object *obj = work->pending_flip_obj;
-				if (obj)
-					seq_printf(m, "New framebuffer gtt_offset 0x%08lx\n",
-						   i915_gem_obj_ggtt_offset(obj));
+				seq_printf(m, "New framebuffer address 0x%08lx\n", (long)work->gtt_offset);
+				seq_printf(m, "MMIO update completed? %d\n",  addr == work->gtt_offset);
 			}
 		}
-		spin_unlock_irqrestore(&dev->event_lock, flags);
+		spin_unlock_irq(&dev->event_lock);
 	}
 
 	mutex_unlock(&dev->struct_mutex);
@@ -573,8 +692,8 @@
 	struct drm_info_node *node = m->private;
 	struct drm_device *dev = node->minor->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
-	struct drm_i915_gem_request *gem_request;
+	struct intel_engine_cs *engine;
+	struct i915_gem_request *rq;
 	int ret, count, i;
 
 	ret = mutex_lock_interruptible(&dev->struct_mutex);
@@ -582,17 +701,15 @@
 		return ret;
 
 	count = 0;
-	for_each_ring(ring, dev_priv, i) {
-		if (list_empty(&ring->request_list))
+	for_each_engine(engine, dev_priv, i) {
+		if (list_empty(&engine->requests))
 			continue;
 
-		seq_printf(m, "%s requests:\n", ring->name);
-		list_for_each_entry(gem_request,
-				    &ring->request_list,
-				    list) {
+		seq_printf(m, "%s requests:\n", engine->name);
+		list_for_each_entry(rq, &engine->requests, engine_link) {
 			seq_printf(m, "    %d @ %d\n",
-				   gem_request->seqno,
-				   (int) (jiffies - gem_request->emitted_jiffies));
+				   rq->seqno,
+				   rq->emitted_jiffies ? (int)(jiffies - rq->emitted_jiffies) : 0);
 		}
 		count++;
 	}
@@ -604,13 +721,17 @@
 	return 0;
 }
 
-static void i915_ring_seqno_info(struct seq_file *m,
-				 struct intel_engine_cs *ring)
+static void i915_engine_seqno_info(struct seq_file *m,
+				   struct intel_engine_cs *engine)
 {
-	if (ring->get_seqno) {
-		seq_printf(m, "Current sequence (%s): %u\n",
-			   ring->name, ring->get_seqno(ring, false));
-	}
+	seq_printf(m, "Current sequence (%s): seqno=%u, tag=%u [last breadcrumb %u, last request %u], next seqno=%u, next tag=%u\n",
+		   engine->name,
+		   intel_engine_get_seqno(engine),
+		   engine->tag,
+		   engine->breadcrumb[engine->id],
+		   engine->last_request ? engine->last_request->seqno : 0,
+		   engine->i915->next_seqno,
+		   engine->next_tag);
 }
 
 static int i915_gem_seqno_info(struct seq_file *m, void *data)
@@ -618,7 +739,7 @@
 	struct drm_info_node *node = m->private;
 	struct drm_device *dev = node->minor->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
+	struct intel_engine_cs *engine;
 	int ret, i;
 
 	ret = mutex_lock_interruptible(&dev->struct_mutex);
@@ -626,8 +747,8 @@
 		return ret;
 	intel_runtime_pm_get(dev_priv);
 
-	for_each_ring(ring, dev_priv, i)
-		i915_ring_seqno_info(m, ring);
+	for_each_engine(engine, dev_priv, i)
+		i915_engine_seqno_info(m, engine);
 
 	intel_runtime_pm_put(dev_priv);
 	mutex_unlock(&dev->struct_mutex);
@@ -641,7 +762,7 @@
 	struct drm_info_node *node = m->private;
 	struct drm_device *dev = node->minor->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
+	struct intel_engine_cs *engine;
 	int ret, i, pipe;
 
 	ret = mutex_lock_interruptible(&dev->struct_mutex);
@@ -650,7 +771,6 @@
 	intel_runtime_pm_get(dev_priv);
 
 	if (IS_CHERRYVIEW(dev)) {
-		int i;
 		seq_printf(m, "Master Interrupt Control:\t%08x\n",
 			   I915_READ(GEN8_MASTER_IRQ));
 
@@ -662,7 +782,7 @@
 			   I915_READ(VLV_IIR_RW));
 		seq_printf(m, "Display IMR:\t%08x\n",
 			   I915_READ(VLV_IMR));
-		for_each_pipe(pipe)
+		for_each_pipe(dev_priv, pipe)
 			seq_printf(m, "Pipe %c stat:\t%08x\n",
 				   pipe_name(pipe),
 				   I915_READ(PIPESTAT(pipe)));
@@ -702,7 +822,13 @@
 				   i, I915_READ(GEN8_GT_IER(i)));
 		}
 
-		for_each_pipe(pipe) {
+		for_each_pipe(dev_priv, pipe) {
+			if (!intel_display_power_is_enabled(dev_priv,
+						POWER_DOMAIN_PIPE(pipe))) {
+				seq_printf(m, "Pipe %c power disabled\n",
+					   pipe_name(pipe));
+				continue;
+			}
 			seq_printf(m, "Pipe %c IMR:\t%08x\n",
 				   pipe_name(pipe),
 				   I915_READ(GEN8_DE_PIPE_IMR(pipe)));
@@ -743,7 +869,7 @@
 			   I915_READ(VLV_IIR_RW));
 		seq_printf(m, "Display IMR:\t%08x\n",
 			   I915_READ(VLV_IMR));
-		for_each_pipe(pipe)
+		for_each_pipe(dev_priv, pipe)
 			seq_printf(m, "Pipe %c stat:\t%08x\n",
 				   pipe_name(pipe),
 				   I915_READ(PIPESTAT(pipe)));
@@ -779,7 +905,7 @@
 			   I915_READ(IIR));
 		seq_printf(m, "Interrupt mask:      %08x\n",
 			   I915_READ(IMR));
-		for_each_pipe(pipe)
+		for_each_pipe(dev_priv, pipe)
 			seq_printf(m, "Pipe %c stat:         %08x\n",
 				   pipe_name(pipe),
 				   I915_READ(PIPESTAT(pipe)));
@@ -803,13 +929,13 @@
 		seq_printf(m, "Graphics Interrupt mask:		%08x\n",
 			   I915_READ(GTIMR));
 	}
-	for_each_ring(ring, dev_priv, i) {
+	for_each_engine(engine, dev_priv, i) {
 		if (INTEL_INFO(dev)->gen >= 6) {
 			seq_printf(m,
 				   "Graphics Interrupt mask (%s):	%08x\n",
-				   ring->name, I915_READ_IMR(ring));
+				   engine->name, I915_READ_IMR(engine));
 		}
-		i915_ring_seqno_info(m, ring);
+		i915_engine_seqno_info(m, engine);
 	}
 	intel_runtime_pm_put(dev_priv);
 	mutex_unlock(&dev->struct_mutex);
@@ -851,12 +977,12 @@
 	struct drm_info_node *node = m->private;
 	struct drm_device *dev = node->minor->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
+	struct intel_engine_cs *engine;
 	const u32 *hws;
 	int i;
 
-	ring = &dev_priv->ring[(uintptr_t)node->info_ent->data];
-	hws = ring->status_page.page_addr;
+	engine = &dev_priv->engine[(uintptr_t)node->info_ent->data];
+	hws = engine->status_page.page_addr;
 	if (hws == NULL)
 		return 0;
 
@@ -927,7 +1053,7 @@
 	ssize_t ret_count = 0;
 	int ret;
 
-	ret = i915_error_state_buf_init(&error_str, count, *pos);
+	ret = i915_error_state_buf_init(&error_str, to_i915(error_priv->dev), count, *pos);
 	if (ret)
 		return ret;
 
@@ -980,7 +1106,7 @@
 	struct drm_device *dev = data;
 	int ret;
 
-	ret = mutex_lock_interruptible(&dev->struct_mutex);
+	ret = i915_mutex_lock_interruptible(dev);
 	if (ret)
 		return ret;
 
@@ -1024,6 +1150,7 @@
 		u32 rpstat, cagf, reqf;
 		u32 rpupei, rpcurup, rpprevup;
 		u32 rpdownei, rpcurdown, rpprevdown;
+		u32 pm_ier, pm_imr, pm_isr, pm_iir, pm_mask;
 		int max_freq;
 
 		/* RPSTAT1 is in the GT power well */
@@ -1061,12 +1188,21 @@
 		gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
 		mutex_unlock(&dev->struct_mutex);
 
+		if (IS_GEN6(dev) || IS_GEN7(dev)) {
+			pm_ier = I915_READ(GEN6_PMIER);
+			pm_imr = I915_READ(GEN6_PMIMR);
+			pm_isr = I915_READ(GEN6_PMISR);
+			pm_iir = I915_READ(GEN6_PMIIR);
+			pm_mask = I915_READ(GEN6_PMINTRMSK);
+		} else {
+			pm_ier = I915_READ(GEN8_GT_IER(2));
+			pm_imr = I915_READ(GEN8_GT_IMR(2));
+			pm_isr = I915_READ(GEN8_GT_ISR(2));
+			pm_iir = I915_READ(GEN8_GT_IIR(2));
+			pm_mask = I915_READ(GEN6_PMINTRMSK);
+		}
 		seq_printf(m, "PM IER=0x%08x IMR=0x%08x ISR=0x%08x IIR=0x%08x, MASK=0x%08x\n",
-			   I915_READ(GEN6_PMIER),
-			   I915_READ(GEN6_PMIMR),
-			   I915_READ(GEN6_PMISR),
-			   I915_READ(GEN6_PMIIR),
-			   I915_READ(GEN6_PMINTRMSK));
+			   pm_ier, pm_imr, pm_isr, pm_iir, pm_mask);
 		seq_printf(m, "GT_PERF_STATUS: 0x%08x\n", gt_perf_status);
 		seq_printf(m, "Render p-state ratio: %d\n",
 			   (gt_perf_status & 0xff00) >> 8);
@@ -1205,14 +1341,37 @@
 	return 0;
 }
 
-static int vlv_drpc_info(struct seq_file *m)
+static int i915_gen6_forcewake_count_info(struct seq_file *m, void *data)
 {
+	struct drm_info_node *node = m->private;
+	struct drm_device *dev = node->minor->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	const char *domain_names[] = {
+		"render",
+		"media",
+	};
+	int i;
+
+	spin_lock_irq(&dev_priv->uncore.lock);
+	for (i = 0; i < FW_DOMAIN_COUNT; i++) {
+		if ((dev_priv->uncore.fw_domains & (1 << i)) == 0)
+			continue;
+
+		seq_printf(m, "%s.wake_count = %u\n",
+			   domain_names[i],
+			   dev_priv->uncore.fw_domain[i].wake_count);
+	}
+	spin_unlock_irq(&dev_priv->uncore.lock);
 
+	return 0;
+}
+
+static int vlv_drpc_info(struct seq_file *m)
+{
 	struct drm_info_node *node = m->private;
 	struct drm_device *dev = node->minor->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	u32 rpmodectl1, rcctl1;
-	unsigned fw_rendercount = 0, fw_mediacount = 0;
 
 	intel_runtime_pm_get(dev_priv);
 
@@ -1245,22 +1404,11 @@
 	seq_printf(m, "Media RC6 residency since boot: %u\n",
 		   I915_READ(VLV_GT_MEDIA_RC6));
 
-	spin_lock_irq(&dev_priv->uncore.lock);
-	fw_rendercount = dev_priv->uncore.fw_rendercount;
-	fw_mediacount = dev_priv->uncore.fw_mediacount;
-	spin_unlock_irq(&dev_priv->uncore.lock);
-
-	seq_printf(m, "Forcewake Render Count = %u\n", fw_rendercount);
-	seq_printf(m, "Forcewake Media Count = %u\n", fw_mediacount);
-
-
-	return 0;
+	return i915_gen6_forcewake_count_info(m, NULL);
 }
 
-
 static int gen6_drpc_info(struct seq_file *m)
 {
-
 	struct drm_info_node *node = m->private;
 	struct drm_device *dev = node->minor->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -1274,7 +1422,7 @@
 	intel_runtime_pm_get(dev_priv);
 
 	spin_lock_irq(&dev_priv->uncore.lock);
-	forcewake_count = dev_priv->uncore.forcewake_count;
+	forcewake_count = dev_priv->uncore.fw_domain[FW_DOMAIN_RENDER].wake_count;
 	spin_unlock_irq(&dev_priv->uncore.lock);
 
 	if (forcewake_count) {
@@ -1365,7 +1513,7 @@
 
 	if (IS_VALLEYVIEW(dev))
 		return vlv_drpc_info(m);
-	else if (IS_GEN6(dev) || IS_GEN7(dev))
+	else if (INTEL_INFO(dev)->gen >= 6)
 		return gen6_drpc_info(m);
 	else
 		return ironlake_drpc_info(m);
@@ -1433,6 +1581,47 @@
 	return 0;
 }
 
+static int i915_fbc_fc_get(void *data, u64 *val)
+{
+	struct drm_device *dev = data;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	if (INTEL_INFO(dev)->gen < 7 || !HAS_FBC(dev))
+		return -ENODEV;
+
+	drm_modeset_lock_all(dev);
+	*val = dev_priv->fbc.false_color;
+	drm_modeset_unlock_all(dev);
+
+	return 0;
+}
+
+static int i915_fbc_fc_set(void *data, u64 val)
+{
+	struct drm_device *dev = data;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	u32 reg;
+
+	if (INTEL_INFO(dev)->gen < 7 || !HAS_FBC(dev))
+		return -ENODEV;
+
+	drm_modeset_lock_all(dev);
+
+	reg = I915_READ(ILK_DPFC_CONTROL);
+	dev_priv->fbc.false_color = val;
+
+	I915_WRITE(ILK_DPFC_CONTROL, val ?
+		   (reg | FBC_CTL_FALSE_COLOR) :
+		   (reg & ~FBC_CTL_FALSE_COLOR));
+
+	drm_modeset_unlock_all(dev);
+	return 0;
+}
+
+DEFINE_SIMPLE_ATTRIBUTE(i915_fbc_fc_fops,
+			i915_fbc_fc_get, i915_fbc_fc_set,
+			"%llu\n");
+
 static int i915_ips_status(struct seq_file *m, void *unused)
 {
 	struct drm_info_node *node = m->private;
@@ -1447,7 +1636,7 @@
 	intel_runtime_pm_get(dev_priv);
 
 	seq_printf(m, "Enabled by kernel parameter: %s\n",
-		   yesno(i915.enable_ips));
+		   yesno(i915_module.enable_ips));
 
 	if (INTEL_INFO(dev)->gen >= 8) {
 		seq_puts(m, "Currently: unknown\n");
@@ -1630,12 +1819,18 @@
 	return 0;
 }
 
+static void describe_ring(struct seq_file *m, struct intel_ringbuffer *ring)
+{
+	seq_printf(m, " (ringbuffer, space: %d, head: %u, tail: %u, last head: %d)",
+		   ring->space, ring->head, ring->tail, ring->retired_head);
+}
+
 static int i915_context_status(struct seq_file *m, void *unused)
 {
 	struct drm_info_node *node = m->private;
 	struct drm_device *dev = node->minor->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
+	struct intel_engine_cs *engine;
 	struct intel_context *ctx;
 	int ret, i;
 
@@ -1649,23 +1844,28 @@
 		seq_putc(m, '\n');
 	}
 
-	if (dev_priv->ips.renderctx) {
-		seq_puts(m, "render context ");
-		describe_obj(m, dev_priv->ips.renderctx);
-		seq_putc(m, '\n');
-	}
-
 	list_for_each_entry(ctx, &dev_priv->context_list, link) {
-		if (ctx->legacy_hw_ctx.rcs_state == NULL)
-			continue;
-
 		seq_puts(m, "HW context ");
 		describe_ctx(m, ctx);
-		for_each_ring(ring, dev_priv, i)
-			if (ring->default_context == ctx)
-				seq_printf(m, "(default context %s) ", ring->name);
+		for_each_engine(engine, dev_priv, i) {
+			if (engine->default_context == ctx)
+				seq_printf(m, "(default context %s) ",
+					   engine->name);
+		}
+
+		seq_putc(m, '\n');
+		for_each_engine(engine, dev_priv, i) {
+			struct drm_i915_gem_object *obj = ctx->ring[i].state;
+			struct intel_ringbuffer *ring = ctx->ring[i].ring;
+
+			seq_printf(m, "%s: ", engine->name);
+			if (obj)
+				describe_obj(m, obj);
+			if (ring)
+				describe_ring(m, ring);
+			seq_putc(m, '\n');
+		}
 
-		describe_obj(m, ctx->legacy_hw_ctx.rcs_state);
 		seq_putc(m, '\n');
 	}
 
@@ -1674,26 +1874,150 @@
 	return 0;
 }
 
-static int i915_gen6_forcewake_count_info(struct seq_file *m, void *data)
+static int i915_dump_lrc(struct seq_file *m, void *unused)
 {
-	struct drm_info_node *node = m->private;
+	struct drm_info_node *node = (struct drm_info_node *) m->private;
+	struct drm_device *dev = node->minor->dev;
+	struct intel_engine_cs *engine;
+	int ret, i;
+
+	ret = mutex_lock_interruptible(&dev->struct_mutex);
+	if (ret)
+		return ret;
+
+	for_each_engine(engine, to_i915(dev), i) {
+		struct intel_ringbuffer *ring;
+
+		list_for_each_entry(ring, &engine->rings, engine_link) {
+			struct intel_context *ctx = ring->ctx;
+			struct task_struct *task;
+
+			if (ctx == NULL)
+				continue;
+
+			seq_printf(m, "CONTEXT: %s", engine->name);
+
+			rcu_read_lock();
+			task = ctx->file_priv ? pid_task(ctx->file_priv->file->pid, PIDTYPE_PID) : NULL;
+			seq_printf(m, " %d:%d\n", task ? task->pid : 0, ctx->file_priv ? ctx->user_handle : 0);
+			rcu_read_unlock();
+
+			if (ctx->ring[engine->id].state) {
+				struct drm_i915_gem_object *obj;
+				struct page *page;
+				uint32_t *reg_state;
+				int j;
+
+				obj = ctx->ring[engine->id].state;
+
+				if (i915_gem_object_get_pages(obj))
+					continue;
+
+				page = i915_gem_object_get_page(obj, 1);
+				reg_state = kmap_atomic(page);
+
+				seq_printf(m, "\tLRCA:\n");
+				for (j = 0; j < 0x600 / sizeof(u32) / 4; j += 4) {
+					seq_printf(m, "\t[0x%08x] 0x%08x 0x%08x 0x%08x 0x%08x\n",
+						   4096 + (j * 4),
+						   reg_state[j], reg_state[j + 1],
+						   reg_state[j + 2], reg_state[j + 3]);
+				}
+				kunmap_atomic(reg_state);
+
+				seq_putc(m, '\n');
+			}
+		}
+	}
+
+	mutex_unlock(&dev->struct_mutex);
+
+	return 0;
+}
+
+static int i915_execlists(struct seq_file *m, void *data)
+{
+	struct drm_info_node *node = (struct drm_info_node *)m->private;
 	struct drm_device *dev = node->minor->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	unsigned forcewake_count = 0, fw_rendercount = 0, fw_mediacount = 0;
+	struct intel_engine_cs *engine;
+	u32 status_pointer;
+	u8 read_pointer;
+	u8 write_pointer;
+	u32 status;
+	u32 ctx_id;
+	struct list_head *cursor;
+	int ring_id, i;
+	int ret;
 
-	spin_lock_irq(&dev_priv->uncore.lock);
-	if (IS_VALLEYVIEW(dev)) {
-		fw_rendercount = dev_priv->uncore.fw_rendercount;
-		fw_mediacount = dev_priv->uncore.fw_mediacount;
-	} else
-		forcewake_count = dev_priv->uncore.forcewake_count;
-	spin_unlock_irq(&dev_priv->uncore.lock);
+	ret = mutex_lock_interruptible(&dev->struct_mutex);
+	if (ret)
+		return ret;
 
-	if (IS_VALLEYVIEW(dev)) {
-		seq_printf(m, "fw_rendercount = %u\n", fw_rendercount);
-		seq_printf(m, "fw_mediacount = %u\n", fw_mediacount);
-	} else
-		seq_printf(m, "forcewake count = %u\n", forcewake_count);
+	intel_runtime_pm_get(dev_priv);
+
+	for_each_engine(engine, dev_priv, ring_id) {
+		struct i915_gem_request *rq = NULL;
+		int count = 0;
+		unsigned long flags;
+
+		seq_printf(m, "%s\n", engine->name);
+
+		if (!engine->execlists_enabled) {
+			seq_puts(m, "\tExeclists are disabled\n");
+			continue;
+		}
+
+		status = I915_READ(RING_EXECLIST_STATUS(engine));
+		ctx_id = I915_READ(RING_EXECLIST_STATUS(engine) + 4);
+		seq_printf(m, "\tExeclist status: 0x%08X, context: %u\n",
+			   status, ctx_id);
+
+		status_pointer = I915_READ(RING_CONTEXT_STATUS_PTR(engine));
+		seq_printf(m, "\tStatus pointer: 0x%08X\n", status_pointer);
+
+		read_pointer = engine->next_context_status_buffer;
+		write_pointer = status_pointer & 0x07;
+		if (read_pointer > write_pointer)
+			write_pointer += 6;
+		seq_printf(m, "\tRead pointer: 0x%08X, write pointer 0x%08X\n",
+			   read_pointer, write_pointer);
+
+		for (i = 0; i < 6; i++) {
+			status = I915_READ(RING_CONTEXT_STATUS_BUF(engine) + 8*i);
+			ctx_id = I915_READ(RING_CONTEXT_STATUS_BUF(engine) + 8*i + 4);
+
+			seq_printf(m, "\tStatus buffer %d: 0x%08X, context: %u\n",
+				   i, status, ctx_id);
+		}
+
+		spin_lock_irqsave(&engine->irqlock, flags);
+		list_for_each(cursor, &engine->pending)
+			count++;
+		rq = list_first_entry_or_null(&engine->pending, typeof(*rq), engine_link);
+		spin_unlock_irqrestore(&engine->irqlock, flags);
+
+		seq_printf(m, "\t%d requests in queue\n", count);
+		if (rq) {
+			struct intel_context *ctx = rq->ctx;
+			struct task_struct *task;
+
+			seq_printf(m, "\tHead request ctx:");
+
+			rcu_read_lock();
+			task = ctx->file_priv ? pid_task(ctx->file_priv->file->pid, PIDTYPE_PID) : NULL;
+			seq_printf(m, " %d:%d\n", task ? task->pid : 0, ctx->file_priv ? ctx->user_handle : 0);
+			rcu_read_unlock();
+
+			seq_printf(m, "\tHead request tail: %u\n", rq->tail);
+			seq_printf(m, "\tHead request seqno: %d\n", rq->seqno);
+		}
+
+		seq_putc(m, '\n');
+	}
+
+	intel_runtime_pm_put(dev_priv);
+	mutex_unlock(&dev->struct_mutex);
 
 	return 0;
 }
@@ -1755,7 +2079,7 @@
 			   I915_READ(MAD_DIMM_C2));
 		seq_printf(m, "TILECTL = 0x%08x\n",
 			   I915_READ(TILECTL));
-		if (IS_GEN8(dev))
+		if (INTEL_INFO(dev)->gen >= 8)
 			seq_printf(m, "GAMTARBMODE = 0x%08x\n",
 				   I915_READ(GAMTARBMODE));
 		else
@@ -1774,7 +2098,13 @@
 {
 	struct intel_context *ctx = ptr;
 	struct seq_file *m = data;
-	struct i915_hw_ppgtt *ppgtt = ctx_to_ppgtt(ctx);
+	struct i915_hw_ppgtt *ppgtt = ctx->ppgtt;
+
+	if (!ppgtt) {
+		seq_printf(m, "  no ppgtt for context %d\n",
+			   ctx->user_handle);
+		return 0;
+	}
 
 	if (i915_gem_context_is_default(ctx))
 		seq_puts(m, "  default context:\n");
@@ -1788,7 +2118,7 @@
 static void gen8_ppgtt_info(struct seq_file *m, struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
+	struct intel_engine_cs *engine;
 	struct i915_hw_ppgtt *ppgtt = dev_priv->mm.aliasing_ppgtt;
 	int unused, i;
 
@@ -1797,13 +2127,13 @@
 
 	seq_printf(m, "Page directories: %d\n", ppgtt->num_pd_pages);
 	seq_printf(m, "Page tables: %d\n", ppgtt->num_pd_entries);
-	for_each_ring(ring, dev_priv, unused) {
-		seq_printf(m, "%s\n", ring->name);
+	for_each_engine(engine, dev_priv, unused) {
+		seq_printf(m, "%s\n", engine->name);
 		for (i = 0; i < 4; i++) {
 			u32 offset = 0x270 + i * 8;
-			u64 pdp = I915_READ(ring->mmio_base + offset + 4);
+			u64 pdp = I915_READ(engine->mmio_base + offset + 4);
 			pdp <<= 32;
-			pdp |= I915_READ(ring->mmio_base + offset);
+			pdp |= I915_READ(engine->mmio_base + offset);
 			seq_printf(m, "\tPDP%d 0x%016llx\n", i, pdp);
 		}
 	}
@@ -1812,30 +2142,30 @@
 static void gen6_ppgtt_info(struct seq_file *m, struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
+	struct intel_engine_cs *engine;
 	struct drm_file *file;
 	int i;
 
 	if (INTEL_INFO(dev)->gen == 6)
 		seq_printf(m, "GFX_MODE: 0x%08x\n", I915_READ(GFX_MODE));
 
-	for_each_ring(ring, dev_priv, i) {
-		seq_printf(m, "%s\n", ring->name);
+	for_each_engine(engine, dev_priv, i) {
+		seq_printf(m, "%s\n", engine->name);
 		if (INTEL_INFO(dev)->gen == 7)
-			seq_printf(m, "GFX_MODE: 0x%08x\n", I915_READ(RING_MODE_GEN7(ring)));
-		seq_printf(m, "PP_DIR_BASE: 0x%08x\n", I915_READ(RING_PP_DIR_BASE(ring)));
-		seq_printf(m, "PP_DIR_BASE_READ: 0x%08x\n", I915_READ(RING_PP_DIR_BASE_READ(ring)));
-		seq_printf(m, "PP_DIR_DCLV: 0x%08x\n", I915_READ(RING_PP_DIR_DCLV(ring)));
+			seq_printf(m, "GFX_MODE: 0x%08x\n", I915_READ(RING_MODE_GEN7(engine)));
+		seq_printf(m, "PP_DIR_BASE: 0x%08x\n", I915_READ(RING_PP_DIR_BASE(engine)));
+		seq_printf(m, "PP_DIR_BASE_READ: 0x%08x\n", I915_READ(RING_PP_DIR_BASE_READ(engine)));
+		seq_printf(m, "PP_DIR_DCLV: 0x%08x\n", I915_READ(RING_PP_DIR_DCLV(engine)));
 	}
 	if (dev_priv->mm.aliasing_ppgtt) {
 		struct i915_hw_ppgtt *ppgtt = dev_priv->mm.aliasing_ppgtt;
 
 		seq_puts(m, "aliasing PPGTT:\n");
-		seq_printf(m, "pd gtt offset: 0x%08x\n", ppgtt->pd_offset);
+		seq_printf(m, "pd gtt offset: 0x%08llx\n",
+			   (long long)i915_gem_obj_to_ggtt(ppgtt->state)->node.start);
 
 		ppgtt->debug_dump(ppgtt, m);
-	} else
-		return;
+	}
 
 	list_for_each_entry_reverse(file, &dev->filelist, lhead) {
 		struct drm_i915_file_private *file_priv = file->driver_priv;
@@ -2313,68 +2643,63 @@
 	struct drm_info_node *node = (struct drm_info_node *) m->private;
 	struct drm_device *dev = node->minor->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
+	struct intel_engine_cs *engine;
 	int num_rings = hweight32(INTEL_INFO(dev)->ring_mask);
 	int i, j, ret;
 
-	if (!i915_semaphore_is_enabled(dev)) {
-		seq_puts(m, "Semaphores are disabled\n");
-		return 0;
-	}
-
 	ret = mutex_lock_interruptible(&dev->struct_mutex);
 	if (ret)
 		return ret;
 	intel_runtime_pm_get(dev_priv);
 
-	if (IS_BROADWELL(dev)) {
-		struct page *page;
-		uint64_t *seqno;
-
-		page = i915_gem_object_get_page(dev_priv->semaphore_obj, 0);
-
-		seqno = (uint64_t *)kmap_atomic(page);
-		for_each_ring(ring, dev_priv, i) {
-			uint64_t offset;
-
-			seq_printf(m, "%s\n", ring->name);
+	seq_puts(m, "  Last breadcrumb:");
+	for_each_engine(engine, dev_priv, i)
+		for (j = 0; j < num_rings; j++)
+			seq_printf(m, "0x%08x\n",
+				   engine->breadcrumb[j]);
+	seq_putc(m, '\n');
 
-			seq_puts(m, "  Last signal:");
-			for (j = 0; j < num_rings; j++) {
-				offset = i * I915_NUM_RINGS + j;
-				seq_printf(m, "0x%08llx (0x%02llx) ",
-					   seqno[offset], offset * 8);
-			}
-			seq_putc(m, '\n');
+	if (engine->semaphore.wait) {
+		if (IS_BROADWELL(dev)) {
+			struct page *page;
+			uint64_t *seqno;
+
+			page = i915_gem_object_get_page(dev_priv->semaphore_obj, 0);
+
+			seqno = (uint64_t *)kmap_atomic(page);
+			for_each_engine(engine, dev_priv, i) {
+				uint64_t offset;
+
+				seq_printf(m, "%s\n", engine->name);
+
+				seq_puts(m, "  Last signal:");
+				for (j = 0; j < num_rings; j++) {
+					offset = i * I915_NUM_ENGINES + j;
+					seq_printf(m, "0x%08llx (0x%02llx) ",
+						   seqno[offset], offset * 8);
+				}
+				seq_putc(m, '\n');
+
+				seq_puts(m, "  Last wait:  ");
+				for (j = 0; j < num_rings; j++) {
+					offset = i + (j * I915_NUM_ENGINES);
+					seq_printf(m, "0x%08llx (0x%02llx) ",
+						   seqno[offset], offset * 8);
+				}
+				seq_putc(m, '\n');
 
-			seq_puts(m, "  Last wait:  ");
-			for (j = 0; j < num_rings; j++) {
-				offset = i + (j * I915_NUM_RINGS);
-				seq_printf(m, "0x%08llx (0x%02llx) ",
-					   seqno[offset], offset * 8);
 			}
+			kunmap_atomic(seqno);
+		} else {
+			seq_puts(m, "  Last signal:");
+			for_each_engine(engine, dev_priv, i)
+				for (j = 0; j < num_rings; j++)
+					seq_printf(m, "0x%08x\n",
+						   I915_READ(engine->semaphore.mbox.signal[j]));
 			seq_putc(m, '\n');
-
 		}
-		kunmap_atomic(seqno);
-	} else {
-		seq_puts(m, "  Last signal:");
-		for_each_ring(ring, dev_priv, i)
-			for (j = 0; j < num_rings; j++)
-				seq_printf(m, "0x%08x\n",
-					   I915_READ(ring->semaphore.mbox.signal[j]));
-		seq_putc(m, '\n');
 	}
 
-	seq_puts(m, "\nSync seqno:\n");
-	for_each_ring(ring, dev_priv, i) {
-		for (j = 0; j < num_rings; j++) {
-			seq_printf(m, "  0x%08x ", ring->semaphore.sync_seqno[j]);
-		}
-		seq_putc(m, '\n');
-	}
-	seq_putc(m, '\n');
-
 	intel_runtime_pm_put(dev_priv);
 	mutex_unlock(&dev->struct_mutex);
 	return 0;
@@ -2392,20 +2717,91 @@
 		struct intel_shared_dpll *pll = &dev_priv->shared_dplls[i];
 
 		seq_printf(m, "DPLL%i: %s, id: %i\n", i, pll->name, pll->id);
-		seq_printf(m, " refcount: %i, active: %i, on: %s\n", pll->refcount,
-			   pll->active, yesno(pll->on));
+		seq_printf(m, " crtc_mask: 0x%08x, active: %d, on: %s\n",
+			   pll->config.crtc_mask, pll->active, yesno(pll->on));
 		seq_printf(m, " tracked hardware state:\n");
-		seq_printf(m, " dpll:    0x%08x\n", pll->hw_state.dpll);
-		seq_printf(m, " dpll_md: 0x%08x\n", pll->hw_state.dpll_md);
-		seq_printf(m, " fp0:     0x%08x\n", pll->hw_state.fp0);
-		seq_printf(m, " fp1:     0x%08x\n", pll->hw_state.fp1);
-		seq_printf(m, " wrpll:   0x%08x\n", pll->hw_state.wrpll);
+		seq_printf(m, " dpll:    0x%08x\n", pll->config.hw_state.dpll);
+		seq_printf(m, " dpll_md: 0x%08x\n",
+			   pll->config.hw_state.dpll_md);
+		seq_printf(m, " fp0:     0x%08x\n", pll->config.hw_state.fp0);
+		seq_printf(m, " fp1:     0x%08x\n", pll->config.hw_state.fp1);
+		seq_printf(m, " wrpll:   0x%08x\n", pll->config.hw_state.wrpll);
 	}
 	drm_modeset_unlock_all(dev);
 
 	return 0;
 }
 
+static int i915_wa_registers(struct seq_file *m, void *unused)
+{
+	int i;
+	int ret;
+	struct drm_info_node *node = (struct drm_info_node *) m->private;
+	struct drm_device *dev = node->minor->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	ret = mutex_lock_interruptible(&dev->struct_mutex);
+	if (ret)
+		return ret;
+
+	intel_runtime_pm_get(dev_priv);
+
+	seq_printf(m, "Workarounds applied: %d\n", dev_priv->workarounds.count);
+	for (i = 0; i < dev_priv->workarounds.count; ++i) {
+		u32 addr, mask, value, read;
+		bool ok;
+
+		addr = dev_priv->workarounds.reg[i].addr;
+		mask = dev_priv->workarounds.reg[i].mask;
+		value = dev_priv->workarounds.reg[i].value;
+		read = I915_READ(addr);
+		ok = (value & mask) == (read & mask);
+		seq_printf(m, "0x%X: 0x%08X, mask: 0x%08X, read: 0x%08x, status: %s\n",
+			   addr, value, mask, read, ok ? "OK" : "FAIL");
+	}
+
+	intel_runtime_pm_put(dev_priv);
+	mutex_unlock(&dev->struct_mutex);
+
+	return 0;
+}
+
+static int i915_ddb_info(struct seq_file *m, void *unused)
+{
+	struct drm_info_node *node = m->private;
+	struct drm_device *dev = node->minor->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct skl_ddb_allocation *ddb;
+	struct skl_ddb_entry *entry;
+	enum pipe pipe;
+	int plane;
+
+	drm_modeset_lock_all(dev);
+
+	ddb = &dev_priv->wm.skl_hw.ddb;
+
+	seq_printf(m, "%-15s%8s%8s%8s\n", "", "Start", "End", "Size");
+
+	for_each_pipe(dev_priv, pipe) {
+		seq_printf(m, "Pipe %c\n", pipe_name(pipe));
+
+		for_each_plane(pipe, plane) {
+			entry = &ddb->plane[pipe][plane];
+			seq_printf(m, "  Plane%-8d%8u%8u%8u\n", plane + 1,
+				   entry->start, entry->end,
+				   skl_ddb_entry_size(entry));
+		}
+
+		entry = &ddb->cursor[pipe];
+		seq_printf(m, "  %-13s%8u%8u%8u\n", "Cursor", entry->start,
+			   entry->end, skl_ddb_entry_size(entry));
+	}
+
+	drm_modeset_unlock_all(dev);
+
+	return 0;
+}
+
 struct pipe_crc_info {
 	const char *name;
 	struct drm_device *dev;
@@ -2667,8 +3063,7 @@
 	*source = INTEL_PIPE_CRC_SOURCE_PIPE;
 
 	drm_modeset_lock_all(dev);
-	list_for_each_entry(encoder, &dev->mode_config.encoder_list,
-			    base.head) {
+	for_each_intel_encoder(dev, encoder) {
 		if (!encoder->base.crtc)
 			continue;
 
@@ -2700,6 +3095,8 @@
 				break;
 			}
 			break;
+		default:
+			break;
 		}
 	}
 	drm_modeset_unlock_all(dev);
@@ -2987,6 +3384,8 @@
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_pipe_crc *pipe_crc = &dev_priv->pipe_crc[pipe];
+	struct intel_crtc *crtc = to_intel_crtc(intel_get_crtc_for_pipe(dev,
+									pipe));
 	u32 val = 0; /* shut up gcc */
 	int ret;
 
@@ -3022,6 +3421,14 @@
 		if (!pipe_crc->entries)
 			return -ENOMEM;
 
+		/*
+		 * When IPS gets enabled, the pipe CRC changes. Since IPS gets
+		 * enabled and disabled dynamically based on package C states,
+		 * user space can't make reliable use of the CRCs, so let's just
+		 * completely disable it.
+		 */
+		hsw_disable_ips(crtc);
+
 		spin_lock_irq(&pipe_crc->lock);
 		pipe_crc->head = 0;
 		pipe_crc->tail = 0;
@@ -3060,6 +3467,8 @@
 			vlv_undo_pipe_scramble_reset(dev, pipe);
 		else if (IS_HASWELL(dev) && pipe == PIPE_A)
 			hsw_undo_trans_edp_pipe_A_crc_wa(dev);
+
+		hsw_enable_ips(crtc);
 	}
 
 	return 0;
@@ -3237,7 +3646,7 @@
 	.write = display_crc_ctl_write
 };
 
-static void wm_latency_show(struct seq_file *m, const uint16_t wm[5])
+static void wm_latency_show(struct seq_file *m, const uint16_t wm[8])
 {
 	struct drm_device *dev = m->private;
 	int num_levels = ilk_wm_max_level(dev) + 1;
@@ -3248,13 +3657,17 @@
 	for (level = 0; level < num_levels; level++) {
 		unsigned int latency = wm[level];
 
-		/* WM1+ latency values in 0.5us units */
-		if (level > 0)
+		/*
+		 * - WM1+ latency values in 0.5us units
+		 * - latencies are in us on gen9
+		 */
+		if (INTEL_INFO(dev)->gen >= 9)
+			latency *= 10;
+		else if (level > 0)
 			latency *= 5;
 
 		seq_printf(m, "WM%d %u (%u.%u usec)\n",
-			   level, wm[level],
-			   latency / 10, latency % 10);
+			   level, wm[level], latency / 10, latency % 10);
 	}
 
 	drm_modeset_unlock_all(dev);
@@ -3263,8 +3676,15 @@
 static int pri_wm_latency_show(struct seq_file *m, void *data)
 {
 	struct drm_device *dev = m->private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	const uint16_t *latencies;
+
+	if (INTEL_INFO(dev)->gen >= 9)
+		latencies = dev_priv->wm.skl_latency;
+	else
+		latencies = to_i915(dev)->wm.pri_latency;
 
-	wm_latency_show(m, to_i915(dev)->wm.pri_latency);
+	wm_latency_show(m, latencies);
 
 	return 0;
 }
@@ -3272,8 +3692,15 @@
 static int spr_wm_latency_show(struct seq_file *m, void *data)
 {
 	struct drm_device *dev = m->private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	const uint16_t *latencies;
+
+	if (INTEL_INFO(dev)->gen >= 9)
+		latencies = dev_priv->wm.skl_latency;
+	else
+		latencies = to_i915(dev)->wm.spr_latency;
 
-	wm_latency_show(m, to_i915(dev)->wm.spr_latency);
+	wm_latency_show(m, latencies);
 
 	return 0;
 }
@@ -3281,8 +3708,15 @@
 static int cur_wm_latency_show(struct seq_file *m, void *data)
 {
 	struct drm_device *dev = m->private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	const uint16_t *latencies;
 
-	wm_latency_show(m, to_i915(dev)->wm.cur_latency);
+	if (INTEL_INFO(dev)->gen >= 9)
+		latencies = dev_priv->wm.skl_latency;
+	else
+		latencies = to_i915(dev)->wm.cur_latency;
+
+	wm_latency_show(m, latencies);
 
 	return 0;
 }
@@ -3318,11 +3752,11 @@
 }
 
 static ssize_t wm_latency_write(struct file *file, const char __user *ubuf,
-				size_t len, loff_t *offp, uint16_t wm[5])
+				size_t len, loff_t *offp, uint16_t wm[8])
 {
 	struct seq_file *m = file->private_data;
 	struct drm_device *dev = m->private;
-	uint16_t new[5] = { 0 };
+	uint16_t new[8] = { 0 };
 	int num_levels = ilk_wm_max_level(dev) + 1;
 	int level;
 	int ret;
@@ -3336,7 +3770,9 @@
 
 	tmp[len] = '\0';
 
-	ret = sscanf(tmp, "%hu %hu %hu %hu %hu", &new[0], &new[1], &new[2], &new[3], &new[4]);
+	ret = sscanf(tmp, "%hu %hu %hu %hu %hu %hu %hu %hu",
+		     &new[0], &new[1], &new[2], &new[3],
+		     &new[4], &new[5], &new[6], &new[7]);
 	if (ret != num_levels)
 		return -EINVAL;
 
@@ -3356,8 +3792,15 @@
 {
 	struct seq_file *m = file->private_data;
 	struct drm_device *dev = m->private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	uint16_t *latencies;
 
-	return wm_latency_write(file, ubuf, len, offp, to_i915(dev)->wm.pri_latency);
+	if (INTEL_INFO(dev)->gen >= 9)
+		latencies = dev_priv->wm.skl_latency;
+	else
+		latencies = to_i915(dev)->wm.pri_latency;
+
+	return wm_latency_write(file, ubuf, len, offp, latencies);
 }
 
 static ssize_t spr_wm_latency_write(struct file *file, const char __user *ubuf,
@@ -3365,8 +3808,15 @@
 {
 	struct seq_file *m = file->private_data;
 	struct drm_device *dev = m->private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	uint16_t *latencies;
 
-	return wm_latency_write(file, ubuf, len, offp, to_i915(dev)->wm.spr_latency);
+	if (INTEL_INFO(dev)->gen >= 9)
+		latencies = dev_priv->wm.skl_latency;
+	else
+		latencies = to_i915(dev)->wm.spr_latency;
+
+	return wm_latency_write(file, ubuf, len, offp, latencies);
 }
 
 static ssize_t cur_wm_latency_write(struct file *file, const char __user *ubuf,
@@ -3374,8 +3824,15 @@
 {
 	struct seq_file *m = file->private_data;
 	struct drm_device *dev = m->private;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	uint16_t *latencies;
+
+	if (INTEL_INFO(dev)->gen >= 9)
+		latencies = dev_priv->wm.skl_latency;
+	else
+		latencies = to_i915(dev)->wm.cur_latency;
 
-	return wm_latency_write(file, ubuf, len, offp, to_i915(dev)->wm.cur_latency);
+	return wm_latency_write(file, ubuf, len, offp, latencies);
 }
 
 static const struct file_operations i915_pri_wm_latency_fops = {
@@ -3557,9 +4014,6 @@
 {
 	struct drm_device *dev = data;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_i915_gem_object *obj, *next;
-	struct i915_address_space *vm;
-	struct i915_vma *vma, *x;
 	int ret;
 
 	DRM_DEBUG("Dropping caches: 0x%08llx\n", val);
@@ -3579,29 +4033,11 @@
 	if (val & (DROP_RETIRE | DROP_ACTIVE))
 		i915_gem_retire_requests(dev);
 
-	if (val & DROP_BOUND) {
-		list_for_each_entry(vm, &dev_priv->vm_list, global_link) {
-			list_for_each_entry_safe(vma, x, &vm->inactive_list,
-						 mm_list) {
-				if (vma->pin_count)
-					continue;
+	if (val & DROP_BOUND)
+		i915_gem_shrink(dev_priv, LONG_MAX, I915_SHRINK_BOUND);
 
-				ret = i915_vma_unbind(vma);
-				if (ret)
-					goto unlock;
-			}
-		}
-	}
-
-	if (val & DROP_UNBOUND) {
-		list_for_each_entry_safe(obj, next, &dev_priv->mm.unbound_list,
-					 global_list)
-			if (obj->pages_pin_count == 0) {
-				ret = i915_gem_object_put_pages(obj);
-				if (ret)
-					goto unlock;
-			}
-	}
+	if (val & DROP_UNBOUND)
+		i915_gem_shrink(dev_priv, LONG_MAX, I915_SHRINK_UNBOUND);
 
 unlock:
 	mutex_unlock(&dev->struct_mutex);
@@ -3839,6 +4275,7 @@
 	if (INTEL_INFO(dev)->gen < 6)
 		return 0;
 
+	intel_runtime_pm_get(dev_priv);
 	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
 
 	return 0;
@@ -3853,6 +4290,7 @@
 		return 0;
 
 	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
+	intel_runtime_pm_put(dev_priv);
 
 	return 0;
 }
@@ -3899,6 +4337,7 @@
 static const struct drm_info_list i915_debugfs_list[] = {
 	{"i915_capabilities", i915_capabilities, 0},
 	{"i915_gem_objects", i915_gem_object_info, 0},
+	{"i915_gem_aperture", i915_gem_aperture_info, 0},
 	{"i915_gem_gtt", i915_gem_gtt_info, 0},
 	{"i915_gem_pinned", i915_gem_gtt_info, 0, (void *) PINNED_LIST},
 	{"i915_gem_active", i915_gem_object_list_info, 0, (void *) ACTIVE_LIST},
@@ -3923,6 +4362,8 @@
 	{"i915_opregion", i915_opregion, 0},
 	{"i915_gem_framebuffer", i915_gem_framebuffer_info, 0},
 	{"i915_context_status", i915_context_status, 0},
+	{"i915_dump_lrc", i915_dump_lrc, 0},
+	{"i915_execlists", i915_execlists, 0},
 	{"i915_gen6_forcewake_count", i915_gen6_forcewake_count_info, 0},
 	{"i915_swizzle_info", i915_swizzle_info, 0},
 	{"i915_ppgtt_info", i915_ppgtt_info, 0},
@@ -3936,6 +4377,8 @@
 	{"i915_semaphore_status", i915_semaphore_status, 0},
 	{"i915_shared_dplls_info", i915_shared_dplls_info, 0},
 	{"i915_dp_mst_info", i915_dp_mst_info, 0},
+	{"i915_wa_registers", i915_wa_registers, 0},
+	{"i915_ddb_info", i915_ddb_info, 0},
 };
 #define I915_DEBUGFS_ENTRIES ARRAY_SIZE(i915_debugfs_list)
 
@@ -3957,6 +4400,7 @@
 	{"i915_pri_wm_latency", &i915_pri_wm_latency_fops},
 	{"i915_spr_wm_latency", &i915_spr_wm_latency_fops},
 	{"i915_cur_wm_latency", &i915_cur_wm_latency_fops},
+	{"i915_fbc_false_color", &i915_fbc_fc_fops},
 };
 
 void intel_display_crc_init(struct drm_device *dev)
@@ -3964,7 +4408,7 @@
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	enum pipe pipe;
 
-	for_each_pipe(pipe) {
+	for_each_pipe(dev_priv, pipe) {
 		struct intel_pipe_crc *pipe_crc = &dev_priv->pipe_crc[pipe];
 
 		pipe_crc->opened = false;
diff -urN a/drivers/gpu/drm/i915/i915_dma.c b/drivers/gpu/drm/i915/i915_dma.c
--- a/drivers/gpu/drm/i915/i915_dma.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_dma.c	2014-11-22 14:37:49.330700418 -0700
@@ -28,9 +28,11 @@
 
 #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
 
+#include <linux/async.h>
 #include <drm/drmP.h>
 #include <drm/drm_crtc_helper.h>
 #include <drm/drm_fb_helper.h>
+#include <drm/drm_legacy.h>
 #include "intel_drv.h"
 #include <drm/i915_drm.h>
 #include "i915_drv.h"
@@ -48,884 +50,6 @@
 #include <linux/pm_runtime.h>
 #include <linux/oom.h>
 
-#define LP_RING(d) (&((struct drm_i915_private *)(d))->ring[RCS])
-
-#define BEGIN_LP_RING(n) \
-	intel_ring_begin(LP_RING(dev_priv), (n))
-
-#define OUT_RING(x) \
-	intel_ring_emit(LP_RING(dev_priv), x)
-
-#define ADVANCE_LP_RING() \
-	__intel_ring_advance(LP_RING(dev_priv))
-
-/**
- * Lock test for when it's just for synchronization of ring access.
- *
- * In that case, we don't need to do it when GEM is initialized as nobody else
- * has access to the ring.
- */
-#define RING_LOCK_TEST_WITH_RETURN(dev, file) do {			\
-	if (LP_RING(dev->dev_private)->buffer->obj == NULL)			\
-		LOCK_TEST_WITH_RETURN(dev, file);			\
-} while (0)
-
-static inline u32
-intel_read_legacy_status_page(struct drm_i915_private *dev_priv, int reg)
-{
-	if (I915_NEED_GFX_HWS(dev_priv->dev))
-		return ioread32(dev_priv->dri1.gfx_hws_cpu_addr + reg);
-	else
-		return intel_read_status_page(LP_RING(dev_priv), reg);
-}
-
-#define READ_HWSP(dev_priv, reg) intel_read_legacy_status_page(dev_priv, reg)
-#define READ_BREADCRUMB(dev_priv) READ_HWSP(dev_priv, I915_BREADCRUMB_INDEX)
-#define I915_BREADCRUMB_INDEX		0x21
-
-void i915_update_dri1_breadcrumb(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_i915_master_private *master_priv;
-
-	/*
-	 * The dri breadcrumb update races against the drm master disappearing.
-	 * Instead of trying to fix this (this is by far not the only ums issue)
-	 * just don't do the update in kms mode.
-	 */
-	if (drm_core_check_feature(dev, DRIVER_MODESET))
-		return;
-
-	if (dev->primary->master) {
-		master_priv = dev->primary->master->driver_priv;
-		if (master_priv->sarea_priv)
-			master_priv->sarea_priv->last_dispatch =
-				READ_BREADCRUMB(dev_priv);
-	}
-}
-
-static void i915_write_hws_pga(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	u32 addr;
-
-	addr = dev_priv->status_page_dmah->busaddr;
-	if (INTEL_INFO(dev)->gen >= 4)
-		addr |= (dev_priv->status_page_dmah->busaddr >> 28) & 0xf0;
-	I915_WRITE(HWS_PGA, addr);
-}
-
-/**
- * Frees the hardware status page, whether it's a physical address or a virtual
- * address set up by the X Server.
- */
-static void i915_free_hws(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring = LP_RING(dev_priv);
-
-	if (dev_priv->status_page_dmah) {
-		drm_pci_free(dev, dev_priv->status_page_dmah);
-		dev_priv->status_page_dmah = NULL;
-	}
-
-	if (ring->status_page.gfx_addr) {
-		ring->status_page.gfx_addr = 0;
-		iounmap(dev_priv->dri1.gfx_hws_cpu_addr);
-	}
-
-	/* Need to rewrite hardware status page */
-	I915_WRITE(HWS_PGA, 0x1ffff000);
-}
-
-void i915_kernel_lost_context(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_i915_master_private *master_priv;
-	struct intel_engine_cs *ring = LP_RING(dev_priv);
-	struct intel_ringbuffer *ringbuf = ring->buffer;
-
-	/*
-	 * We should never lose context on the ring with modesetting
-	 * as we don't expose it to userspace
-	 */
-	if (drm_core_check_feature(dev, DRIVER_MODESET))
-		return;
-
-	ringbuf->head = I915_READ_HEAD(ring) & HEAD_ADDR;
-	ringbuf->tail = I915_READ_TAIL(ring) & TAIL_ADDR;
-	ringbuf->space = ringbuf->head - (ringbuf->tail + I915_RING_FREE_SPACE);
-	if (ringbuf->space < 0)
-		ringbuf->space += ringbuf->size;
-
-	if (!dev->primary->master)
-		return;
-
-	master_priv = dev->primary->master->driver_priv;
-	if (ringbuf->head == ringbuf->tail && master_priv->sarea_priv)
-		master_priv->sarea_priv->perf_boxes |= I915_BOX_RING_EMPTY;
-}
-
-static int i915_dma_cleanup(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	int i;
-
-	/* Make sure interrupts are disabled here because the uninstall ioctl
-	 * may not have been called from userspace and after dev_private
-	 * is freed, it's too late.
-	 */
-	if (dev->irq_enabled)
-		drm_irq_uninstall(dev);
-
-	mutex_lock(&dev->struct_mutex);
-	for (i = 0; i < I915_NUM_RINGS; i++)
-		intel_cleanup_ring_buffer(&dev_priv->ring[i]);
-	mutex_unlock(&dev->struct_mutex);
-
-	/* Clear the HWS virtual address at teardown */
-	if (I915_NEED_GFX_HWS(dev))
-		i915_free_hws(dev);
-
-	return 0;
-}
-
-static int i915_initialize(struct drm_device *dev, drm_i915_init_t *init)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_i915_master_private *master_priv = dev->primary->master->driver_priv;
-	int ret;
-
-	master_priv->sarea = drm_getsarea(dev);
-	if (master_priv->sarea) {
-		master_priv->sarea_priv = (drm_i915_sarea_t *)
-			((u8 *)master_priv->sarea->handle + init->sarea_priv_offset);
-	} else {
-		DRM_DEBUG_DRIVER("sarea not found assuming DRI2 userspace\n");
-	}
-
-	if (init->ring_size != 0) {
-		if (LP_RING(dev_priv)->buffer->obj != NULL) {
-			i915_dma_cleanup(dev);
-			DRM_ERROR("Client tried to initialize ringbuffer in "
-				  "GEM mode\n");
-			return -EINVAL;
-		}
-
-		ret = intel_render_ring_init_dri(dev,
-						 init->ring_start,
-						 init->ring_size);
-		if (ret) {
-			i915_dma_cleanup(dev);
-			return ret;
-		}
-	}
-
-	dev_priv->dri1.cpp = init->cpp;
-	dev_priv->dri1.back_offset = init->back_offset;
-	dev_priv->dri1.front_offset = init->front_offset;
-	dev_priv->dri1.current_page = 0;
-	if (master_priv->sarea_priv)
-		master_priv->sarea_priv->pf_current_page = 0;
-
-	/* Allow hardware batchbuffers unless told otherwise.
-	 */
-	dev_priv->dri1.allow_batchbuffer = 1;
-
-	return 0;
-}
-
-static int i915_dma_resume(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring = LP_RING(dev_priv);
-
-	DRM_DEBUG_DRIVER("%s\n", __func__);
-
-	if (ring->buffer->virtual_start == NULL) {
-		DRM_ERROR("can not ioremap virtual address for"
-			  " ring buffer\n");
-		return -ENOMEM;
-	}
-
-	/* Program Hardware Status Page */
-	if (!ring->status_page.page_addr) {
-		DRM_ERROR("Can not find hardware status page\n");
-		return -EINVAL;
-	}
-	DRM_DEBUG_DRIVER("hw status page @ %p\n",
-				ring->status_page.page_addr);
-	if (ring->status_page.gfx_addr != 0)
-		intel_ring_setup_status_page(ring);
-	else
-		i915_write_hws_pga(dev);
-
-	DRM_DEBUG_DRIVER("Enabled hardware status page\n");
-
-	return 0;
-}
-
-static int i915_dma_init(struct drm_device *dev, void *data,
-			 struct drm_file *file_priv)
-{
-	drm_i915_init_t *init = data;
-	int retcode = 0;
-
-	if (drm_core_check_feature(dev, DRIVER_MODESET))
-		return -ENODEV;
-
-	switch (init->func) {
-	case I915_INIT_DMA:
-		retcode = i915_initialize(dev, init);
-		break;
-	case I915_CLEANUP_DMA:
-		retcode = i915_dma_cleanup(dev);
-		break;
-	case I915_RESUME_DMA:
-		retcode = i915_dma_resume(dev);
-		break;
-	default:
-		retcode = -EINVAL;
-		break;
-	}
-
-	return retcode;
-}
-
-/* Implement basically the same security restrictions as hardware does
- * for MI_BATCH_NON_SECURE.  These can be made stricter at any time.
- *
- * Most of the calculations below involve calculating the size of a
- * particular instruction.  It's important to get the size right as
- * that tells us where the next instruction to check is.  Any illegal
- * instruction detected will be given a size of zero, which is a
- * signal to abort the rest of the buffer.
- */
-static int validate_cmd(int cmd)
-{
-	switch (((cmd >> 29) & 0x7)) {
-	case 0x0:
-		switch ((cmd >> 23) & 0x3f) {
-		case 0x0:
-			return 1;	/* MI_NOOP */
-		case 0x4:
-			return 1;	/* MI_FLUSH */
-		default:
-			return 0;	/* disallow everything else */
-		}
-		break;
-	case 0x1:
-		return 0;	/* reserved */
-	case 0x2:
-		return (cmd & 0xff) + 2;	/* 2d commands */
-	case 0x3:
-		if (((cmd >> 24) & 0x1f) <= 0x18)
-			return 1;
-
-		switch ((cmd >> 24) & 0x1f) {
-		case 0x1c:
-			return 1;
-		case 0x1d:
-			switch ((cmd >> 16) & 0xff) {
-			case 0x3:
-				return (cmd & 0x1f) + 2;
-			case 0x4:
-				return (cmd & 0xf) + 2;
-			default:
-				return (cmd & 0xffff) + 2;
-			}
-		case 0x1e:
-			if (cmd & (1 << 23))
-				return (cmd & 0xffff) + 1;
-			else
-				return 1;
-		case 0x1f:
-			if ((cmd & (1 << 23)) == 0)	/* inline vertices */
-				return (cmd & 0x1ffff) + 2;
-			else if (cmd & (1 << 17))	/* indirect random */
-				if ((cmd & 0xffff) == 0)
-					return 0;	/* unknown length, too hard */
-				else
-					return (((cmd & 0xffff) + 1) / 2) + 1;
-			else
-				return 2;	/* indirect sequential */
-		default:
-			return 0;
-		}
-	default:
-		return 0;
-	}
-
-	return 0;
-}
-
-static int i915_emit_cmds(struct drm_device *dev, int *buffer, int dwords)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	int i, ret;
-
-	if ((dwords+1) * sizeof(int) >= LP_RING(dev_priv)->buffer->size - 8)
-		return -EINVAL;
-
-	for (i = 0; i < dwords;) {
-		int sz = validate_cmd(buffer[i]);
-
-		if (sz == 0 || i + sz > dwords)
-			return -EINVAL;
-		i += sz;
-	}
-
-	ret = BEGIN_LP_RING((dwords+1)&~1);
-	if (ret)
-		return ret;
-
-	for (i = 0; i < dwords; i++)
-		OUT_RING(buffer[i]);
-	if (dwords & 1)
-		OUT_RING(0);
-
-	ADVANCE_LP_RING();
-
-	return 0;
-}
-
-int
-i915_emit_box(struct drm_device *dev,
-	      struct drm_clip_rect *box,
-	      int DR1, int DR4)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	int ret;
-
-	if (box->y2 <= box->y1 || box->x2 <= box->x1 ||
-	    box->y2 <= 0 || box->x2 <= 0) {
-		DRM_ERROR("Bad box %d,%d..%d,%d\n",
-			  box->x1, box->y1, box->x2, box->y2);
-		return -EINVAL;
-	}
-
-	if (INTEL_INFO(dev)->gen >= 4) {
-		ret = BEGIN_LP_RING(4);
-		if (ret)
-			return ret;
-
-		OUT_RING(GFX_OP_DRAWRECT_INFO_I965);
-		OUT_RING((box->x1 & 0xffff) | (box->y1 << 16));
-		OUT_RING(((box->x2 - 1) & 0xffff) | ((box->y2 - 1) << 16));
-		OUT_RING(DR4);
-	} else {
-		ret = BEGIN_LP_RING(6);
-		if (ret)
-			return ret;
-
-		OUT_RING(GFX_OP_DRAWRECT_INFO);
-		OUT_RING(DR1);
-		OUT_RING((box->x1 & 0xffff) | (box->y1 << 16));
-		OUT_RING(((box->x2 - 1) & 0xffff) | ((box->y2 - 1) << 16));
-		OUT_RING(DR4);
-		OUT_RING(0);
-	}
-	ADVANCE_LP_RING();
-
-	return 0;
-}
-
-/* XXX: Emitting the counter should really be moved to part of the IRQ
- * emit. For now, do it in both places:
- */
-
-static void i915_emit_breadcrumb(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_i915_master_private *master_priv = dev->primary->master->driver_priv;
-
-	dev_priv->dri1.counter++;
-	if (dev_priv->dri1.counter > 0x7FFFFFFFUL)
-		dev_priv->dri1.counter = 0;
-	if (master_priv->sarea_priv)
-		master_priv->sarea_priv->last_enqueue = dev_priv->dri1.counter;
-
-	if (BEGIN_LP_RING(4) == 0) {
-		OUT_RING(MI_STORE_DWORD_INDEX);
-		OUT_RING(I915_BREADCRUMB_INDEX << MI_STORE_DWORD_INDEX_SHIFT);
-		OUT_RING(dev_priv->dri1.counter);
-		OUT_RING(0);
-		ADVANCE_LP_RING();
-	}
-}
-
-static int i915_dispatch_cmdbuffer(struct drm_device *dev,
-				   drm_i915_cmdbuffer_t *cmd,
-				   struct drm_clip_rect *cliprects,
-				   void *cmdbuf)
-{
-	int nbox = cmd->num_cliprects;
-	int i = 0, count, ret;
-
-	if (cmd->sz & 0x3) {
-		DRM_ERROR("alignment");
-		return -EINVAL;
-	}
-
-	i915_kernel_lost_context(dev);
-
-	count = nbox ? nbox : 1;
-
-	for (i = 0; i < count; i++) {
-		if (i < nbox) {
-			ret = i915_emit_box(dev, &cliprects[i],
-					    cmd->DR1, cmd->DR4);
-			if (ret)
-				return ret;
-		}
-
-		ret = i915_emit_cmds(dev, cmdbuf, cmd->sz / 4);
-		if (ret)
-			return ret;
-	}
-
-	i915_emit_breadcrumb(dev);
-	return 0;
-}
-
-static int i915_dispatch_batchbuffer(struct drm_device *dev,
-				     drm_i915_batchbuffer_t *batch,
-				     struct drm_clip_rect *cliprects)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	int nbox = batch->num_cliprects;
-	int i, count, ret;
-
-	if ((batch->start | batch->used) & 0x7) {
-		DRM_ERROR("alignment");
-		return -EINVAL;
-	}
-
-	i915_kernel_lost_context(dev);
-
-	count = nbox ? nbox : 1;
-	for (i = 0; i < count; i++) {
-		if (i < nbox) {
-			ret = i915_emit_box(dev, &cliprects[i],
-					    batch->DR1, batch->DR4);
-			if (ret)
-				return ret;
-		}
-
-		if (!IS_I830(dev) && !IS_845G(dev)) {
-			ret = BEGIN_LP_RING(2);
-			if (ret)
-				return ret;
-
-			if (INTEL_INFO(dev)->gen >= 4) {
-				OUT_RING(MI_BATCH_BUFFER_START | (2 << 6) | MI_BATCH_NON_SECURE_I965);
-				OUT_RING(batch->start);
-			} else {
-				OUT_RING(MI_BATCH_BUFFER_START | (2 << 6));
-				OUT_RING(batch->start | MI_BATCH_NON_SECURE);
-			}
-		} else {
-			ret = BEGIN_LP_RING(4);
-			if (ret)
-				return ret;
-
-			OUT_RING(MI_BATCH_BUFFER);
-			OUT_RING(batch->start | MI_BATCH_NON_SECURE);
-			OUT_RING(batch->start + batch->used - 4);
-			OUT_RING(0);
-		}
-		ADVANCE_LP_RING();
-	}
-
-
-	if (IS_G4X(dev) || IS_GEN5(dev)) {
-		if (BEGIN_LP_RING(2) == 0) {
-			OUT_RING(MI_FLUSH | MI_NO_WRITE_FLUSH | MI_INVALIDATE_ISP);
-			OUT_RING(MI_NOOP);
-			ADVANCE_LP_RING();
-		}
-	}
-
-	i915_emit_breadcrumb(dev);
-	return 0;
-}
-
-static int i915_dispatch_flip(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_i915_master_private *master_priv =
-		dev->primary->master->driver_priv;
-	int ret;
-
-	if (!master_priv->sarea_priv)
-		return -EINVAL;
-
-	DRM_DEBUG_DRIVER("%s: page=%d pfCurrentPage=%d\n",
-			  __func__,
-			 dev_priv->dri1.current_page,
-			 master_priv->sarea_priv->pf_current_page);
-
-	i915_kernel_lost_context(dev);
-
-	ret = BEGIN_LP_RING(10);
-	if (ret)
-		return ret;
-
-	OUT_RING(MI_FLUSH | MI_READ_FLUSH);
-	OUT_RING(0);
-
-	OUT_RING(CMD_OP_DISPLAYBUFFER_INFO | ASYNC_FLIP);
-	OUT_RING(0);
-	if (dev_priv->dri1.current_page == 0) {
-		OUT_RING(dev_priv->dri1.back_offset);
-		dev_priv->dri1.current_page = 1;
-	} else {
-		OUT_RING(dev_priv->dri1.front_offset);
-		dev_priv->dri1.current_page = 0;
-	}
-	OUT_RING(0);
-
-	OUT_RING(MI_WAIT_FOR_EVENT | MI_WAIT_FOR_PLANE_A_FLIP);
-	OUT_RING(0);
-
-	ADVANCE_LP_RING();
-
-	master_priv->sarea_priv->last_enqueue = dev_priv->dri1.counter++;
-
-	if (BEGIN_LP_RING(4) == 0) {
-		OUT_RING(MI_STORE_DWORD_INDEX);
-		OUT_RING(I915_BREADCRUMB_INDEX << MI_STORE_DWORD_INDEX_SHIFT);
-		OUT_RING(dev_priv->dri1.counter);
-		OUT_RING(0);
-		ADVANCE_LP_RING();
-	}
-
-	master_priv->sarea_priv->pf_current_page = dev_priv->dri1.current_page;
-	return 0;
-}
-
-static int i915_quiescent(struct drm_device *dev)
-{
-	i915_kernel_lost_context(dev);
-	return intel_ring_idle(LP_RING(dev->dev_private));
-}
-
-static int i915_flush_ioctl(struct drm_device *dev, void *data,
-			    struct drm_file *file_priv)
-{
-	int ret;
-
-	if (drm_core_check_feature(dev, DRIVER_MODESET))
-		return -ENODEV;
-
-	RING_LOCK_TEST_WITH_RETURN(dev, file_priv);
-
-	mutex_lock(&dev->struct_mutex);
-	ret = i915_quiescent(dev);
-	mutex_unlock(&dev->struct_mutex);
-
-	return ret;
-}
-
-static int i915_batchbuffer(struct drm_device *dev, void *data,
-			    struct drm_file *file_priv)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_i915_master_private *master_priv;
-	drm_i915_sarea_t *sarea_priv;
-	drm_i915_batchbuffer_t *batch = data;
-	int ret;
-	struct drm_clip_rect *cliprects = NULL;
-
-	if (drm_core_check_feature(dev, DRIVER_MODESET))
-		return -ENODEV;
-
-	master_priv = dev->primary->master->driver_priv;
-	sarea_priv = (drm_i915_sarea_t *) master_priv->sarea_priv;
-
-	if (!dev_priv->dri1.allow_batchbuffer) {
-		DRM_ERROR("Batchbuffer ioctl disabled\n");
-		return -EINVAL;
-	}
-
-	DRM_DEBUG_DRIVER("i915 batchbuffer, start %x used %d cliprects %d\n",
-			batch->start, batch->used, batch->num_cliprects);
-
-	RING_LOCK_TEST_WITH_RETURN(dev, file_priv);
-
-	if (batch->num_cliprects < 0)
-		return -EINVAL;
-
-	if (batch->num_cliprects) {
-		cliprects = kcalloc(batch->num_cliprects,
-				    sizeof(*cliprects),
-				    GFP_KERNEL);
-		if (cliprects == NULL)
-			return -ENOMEM;
-
-		ret = copy_from_user(cliprects, batch->cliprects,
-				     batch->num_cliprects *
-				     sizeof(struct drm_clip_rect));
-		if (ret != 0) {
-			ret = -EFAULT;
-			goto fail_free;
-		}
-	}
-
-	mutex_lock(&dev->struct_mutex);
-	ret = i915_dispatch_batchbuffer(dev, batch, cliprects);
-	mutex_unlock(&dev->struct_mutex);
-
-	if (sarea_priv)
-		sarea_priv->last_dispatch = READ_BREADCRUMB(dev_priv);
-
-fail_free:
-	kfree(cliprects);
-
-	return ret;
-}
-
-static int i915_cmdbuffer(struct drm_device *dev, void *data,
-			  struct drm_file *file_priv)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_i915_master_private *master_priv;
-	drm_i915_sarea_t *sarea_priv;
-	drm_i915_cmdbuffer_t *cmdbuf = data;
-	struct drm_clip_rect *cliprects = NULL;
-	void *batch_data;
-	int ret;
-
-	DRM_DEBUG_DRIVER("i915 cmdbuffer, buf %p sz %d cliprects %d\n",
-			cmdbuf->buf, cmdbuf->sz, cmdbuf->num_cliprects);
-
-	if (drm_core_check_feature(dev, DRIVER_MODESET))
-		return -ENODEV;
-
-	master_priv = dev->primary->master->driver_priv;
-	sarea_priv = (drm_i915_sarea_t *) master_priv->sarea_priv;
-
-	RING_LOCK_TEST_WITH_RETURN(dev, file_priv);
-
-	if (cmdbuf->num_cliprects < 0)
-		return -EINVAL;
-
-	batch_data = kmalloc(cmdbuf->sz, GFP_KERNEL);
-	if (batch_data == NULL)
-		return -ENOMEM;
-
-	ret = copy_from_user(batch_data, cmdbuf->buf, cmdbuf->sz);
-	if (ret != 0) {
-		ret = -EFAULT;
-		goto fail_batch_free;
-	}
-
-	if (cmdbuf->num_cliprects) {
-		cliprects = kcalloc(cmdbuf->num_cliprects,
-				    sizeof(*cliprects), GFP_KERNEL);
-		if (cliprects == NULL) {
-			ret = -ENOMEM;
-			goto fail_batch_free;
-		}
-
-		ret = copy_from_user(cliprects, cmdbuf->cliprects,
-				     cmdbuf->num_cliprects *
-				     sizeof(struct drm_clip_rect));
-		if (ret != 0) {
-			ret = -EFAULT;
-			goto fail_clip_free;
-		}
-	}
-
-	mutex_lock(&dev->struct_mutex);
-	ret = i915_dispatch_cmdbuffer(dev, cmdbuf, cliprects, batch_data);
-	mutex_unlock(&dev->struct_mutex);
-	if (ret) {
-		DRM_ERROR("i915_dispatch_cmdbuffer failed\n");
-		goto fail_clip_free;
-	}
-
-	if (sarea_priv)
-		sarea_priv->last_dispatch = READ_BREADCRUMB(dev_priv);
-
-fail_clip_free:
-	kfree(cliprects);
-fail_batch_free:
-	kfree(batch_data);
-
-	return ret;
-}
-
-static int i915_emit_irq(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_i915_master_private *master_priv = dev->primary->master->driver_priv;
-
-	i915_kernel_lost_context(dev);
-
-	DRM_DEBUG_DRIVER("\n");
-
-	dev_priv->dri1.counter++;
-	if (dev_priv->dri1.counter > 0x7FFFFFFFUL)
-		dev_priv->dri1.counter = 1;
-	if (master_priv->sarea_priv)
-		master_priv->sarea_priv->last_enqueue = dev_priv->dri1.counter;
-
-	if (BEGIN_LP_RING(4) == 0) {
-		OUT_RING(MI_STORE_DWORD_INDEX);
-		OUT_RING(I915_BREADCRUMB_INDEX << MI_STORE_DWORD_INDEX_SHIFT);
-		OUT_RING(dev_priv->dri1.counter);
-		OUT_RING(MI_USER_INTERRUPT);
-		ADVANCE_LP_RING();
-	}
-
-	return dev_priv->dri1.counter;
-}
-
-static int i915_wait_irq(struct drm_device *dev, int irq_nr)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_i915_master_private *master_priv = dev->primary->master->driver_priv;
-	int ret = 0;
-	struct intel_engine_cs *ring = LP_RING(dev_priv);
-
-	DRM_DEBUG_DRIVER("irq_nr=%d breadcrumb=%d\n", irq_nr,
-		  READ_BREADCRUMB(dev_priv));
-
-	if (READ_BREADCRUMB(dev_priv) >= irq_nr) {
-		if (master_priv->sarea_priv)
-			master_priv->sarea_priv->last_dispatch = READ_BREADCRUMB(dev_priv);
-		return 0;
-	}
-
-	if (master_priv->sarea_priv)
-		master_priv->sarea_priv->perf_boxes |= I915_BOX_WAIT;
-
-	if (ring->irq_get(ring)) {
-		DRM_WAIT_ON(ret, ring->irq_queue, 3 * HZ,
-			    READ_BREADCRUMB(dev_priv) >= irq_nr);
-		ring->irq_put(ring);
-	} else if (wait_for(READ_BREADCRUMB(dev_priv) >= irq_nr, 3000))
-		ret = -EBUSY;
-
-	if (ret == -EBUSY) {
-		DRM_ERROR("EBUSY -- rec: %d emitted: %d\n",
-			  READ_BREADCRUMB(dev_priv), (int)dev_priv->dri1.counter);
-	}
-
-	return ret;
-}
-
-/* Needs the lock as it touches the ring.
- */
-static int i915_irq_emit(struct drm_device *dev, void *data,
-			 struct drm_file *file_priv)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	drm_i915_irq_emit_t *emit = data;
-	int result;
-
-	if (drm_core_check_feature(dev, DRIVER_MODESET))
-		return -ENODEV;
-
-	if (!dev_priv || !LP_RING(dev_priv)->buffer->virtual_start) {
-		DRM_ERROR("called with no initialization\n");
-		return -EINVAL;
-	}
-
-	RING_LOCK_TEST_WITH_RETURN(dev, file_priv);
-
-	mutex_lock(&dev->struct_mutex);
-	result = i915_emit_irq(dev);
-	mutex_unlock(&dev->struct_mutex);
-
-	if (copy_to_user(emit->irq_seq, &result, sizeof(int))) {
-		DRM_ERROR("copy_to_user\n");
-		return -EFAULT;
-	}
-
-	return 0;
-}
-
-/* Doesn't need the hardware lock.
- */
-static int i915_irq_wait(struct drm_device *dev, void *data,
-			 struct drm_file *file_priv)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	drm_i915_irq_wait_t *irqwait = data;
-
-	if (drm_core_check_feature(dev, DRIVER_MODESET))
-		return -ENODEV;
-
-	if (!dev_priv) {
-		DRM_ERROR("called with no initialization\n");
-		return -EINVAL;
-	}
-
-	return i915_wait_irq(dev, irqwait->irq_seq);
-}
-
-static int i915_vblank_pipe_get(struct drm_device *dev, void *data,
-			 struct drm_file *file_priv)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	drm_i915_vblank_pipe_t *pipe = data;
-
-	if (drm_core_check_feature(dev, DRIVER_MODESET))
-		return -ENODEV;
-
-	if (!dev_priv) {
-		DRM_ERROR("called with no initialization\n");
-		return -EINVAL;
-	}
-
-	pipe->pipe = DRM_I915_VBLANK_PIPE_A | DRM_I915_VBLANK_PIPE_B;
-
-	return 0;
-}
-
-/**
- * Schedule buffer swap at given vertical blank.
- */
-static int i915_vblank_swap(struct drm_device *dev, void *data,
-		     struct drm_file *file_priv)
-{
-	/* The delayed swap mechanism was fundamentally racy, and has been
-	 * removed.  The model was that the client requested a delayed flip/swap
-	 * from the kernel, then waited for vblank before continuing to perform
-	 * rendering.  The problem was that the kernel might wake the client
-	 * up before it dispatched the vblank swap (since the lock has to be
-	 * held while touching the ringbuffer), in which case the client would
-	 * clear and start the next frame before the swap occurred, and
-	 * flicker would occur in addition to likely missing the vblank.
-	 *
-	 * In the absence of this ioctl, userland falls back to a correct path
-	 * of waiting for a vblank, then dispatching the swap on its own.
-	 * Context switching to userland and back is plenty fast enough for
-	 * meeting the requirements of vblank swapping.
-	 */
-	return -EINVAL;
-}
-
-static int i915_flip_bufs(struct drm_device *dev, void *data,
-			  struct drm_file *file_priv)
-{
-	int ret;
-
-	if (drm_core_check_feature(dev, DRIVER_MODESET))
-		return -ENODEV;
-
-	DRM_DEBUG_DRIVER("%s\n", __func__);
-
-	RING_LOCK_TEST_WITH_RETURN(dev, file_priv);
-
-	mutex_lock(&dev->struct_mutex);
-	ret = i915_dispatch_flip(dev);
-	mutex_unlock(&dev->struct_mutex);
-
-	return ret;
-}
 
 static int i915_getparam(struct drm_device *dev, void *data,
 			 struct drm_file *file_priv)
@@ -934,21 +58,12 @@
 	drm_i915_getparam_t *param = data;
 	int value;
 
-	if (!dev_priv) {
-		DRM_ERROR("called with no initialization\n");
-		return -EINVAL;
-	}
-
 	switch (param->param) {
 	case I915_PARAM_IRQ_ACTIVE:
-		value = dev->pdev->irq ? 1 : 0;
-		break;
 	case I915_PARAM_ALLOW_BATCHBUFFER:
-		value = dev_priv->dri1.allow_batchbuffer ? 1 : 0;
-		break;
 	case I915_PARAM_LAST_DISPATCH:
-		value = READ_BREADCRUMB(dev_priv);
-		break;
+		/* Reject all old ums/dri params. */
+		return -ENODEV;
 	case I915_PARAM_CHIPSET_ID:
 		value = dev->pdev->device;
 		break;
@@ -969,13 +84,13 @@
 		value = 1;
 		break;
 	case I915_PARAM_HAS_BSD:
-		value = intel_ring_initialized(&dev_priv->ring[VCS]);
+		value = intel_engine_initialized(&dev_priv->engine[VCS]);
 		break;
 	case I915_PARAM_HAS_BLT:
-		value = intel_ring_initialized(&dev_priv->ring[BCS]);
+		value = intel_engine_initialized(&dev_priv->engine[BCS]);
 		break;
 	case I915_PARAM_HAS_VEBOX:
-		value = intel_ring_initialized(&dev_priv->ring[VECS]);
+		value = intel_engine_initialized(&dev_priv->engine[VECS]);
 		break;
 	case I915_PARAM_HAS_RELAXED_FENCING:
 		value = 1;
@@ -999,13 +114,13 @@
 		value = HAS_WT(dev);
 		break;
 	case I915_PARAM_HAS_ALIASING_PPGTT:
-		value = dev_priv->mm.aliasing_ppgtt || USES_FULL_PPGTT(dev);
+		value = USES_PPGTT(dev);
 		break;
 	case I915_PARAM_HAS_WAIT_TIMEOUT:
 		value = 1;
 		break;
 	case I915_PARAM_HAS_SEMAPHORES:
-		value = i915_semaphore_is_enabled(dev);
+		value = RCS_ENGINE(dev_priv)->semaphore.wait != NULL;
 		break;
 	case I915_PARAM_HAS_PRIME_VMAP_FLUSH:
 		value = 1;
@@ -1025,6 +140,12 @@
 	case I915_PARAM_CMD_PARSER_VERSION:
 		value = i915_cmd_parser_get_version();
 		break;
+	case I915_PARAM_HAS_COHERENT_PHYS_GTT:
+		value = 1;
+		break;
+	case I915_PARAM_MMAP_VERSION:
+		value = 1;
+		break;
 	default:
 		DRM_DEBUG("Unknown parameter %d\n", param->param);
 		return -EINVAL;
@@ -1044,19 +165,13 @@
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	drm_i915_setparam_t *param = data;
 
-	if (!dev_priv) {
-		DRM_ERROR("called with no initialization\n");
-		return -EINVAL;
-	}
-
 	switch (param->param) {
 	case I915_SETPARAM_USE_MI_BATCHBUFFER_START:
-		break;
 	case I915_SETPARAM_TEX_LRU_LOG_GRANULARITY:
-		break;
 	case I915_SETPARAM_ALLOW_BATCHBUFFER:
-		dev_priv->dri1.allow_batchbuffer = param->value ? 1 : 0;
-		break;
+		/* Reject all old ums/dri params. */
+		return -ENODEV;
+
 	case I915_SETPARAM_NUM_USED_FENCES:
 		if (param->value > dev_priv->num_fence_regs ||
 		    param->value < 0)
@@ -1073,54 +188,6 @@
 	return 0;
 }
 
-static int i915_set_status_page(struct drm_device *dev, void *data,
-				struct drm_file *file_priv)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	drm_i915_hws_addr_t *hws = data;
-	struct intel_engine_cs *ring;
-
-	if (drm_core_check_feature(dev, DRIVER_MODESET))
-		return -ENODEV;
-
-	if (!I915_NEED_GFX_HWS(dev))
-		return -EINVAL;
-
-	if (!dev_priv) {
-		DRM_ERROR("called with no initialization\n");
-		return -EINVAL;
-	}
-
-	if (drm_core_check_feature(dev, DRIVER_MODESET)) {
-		WARN(1, "tried to set status page when mode setting active\n");
-		return 0;
-	}
-
-	DRM_DEBUG_DRIVER("set status page addr 0x%08x\n", (u32)hws->addr);
-
-	ring = LP_RING(dev_priv);
-	ring->status_page.gfx_addr = hws->addr & (0x1ffff<<12);
-
-	dev_priv->dri1.gfx_hws_cpu_addr =
-		ioremap_wc(dev_priv->gtt.mappable_base + hws->addr, 4096);
-	if (dev_priv->dri1.gfx_hws_cpu_addr == NULL) {
-		i915_dma_cleanup(dev);
-		ring->status_page.gfx_addr = 0;
-		DRM_ERROR("can not ioremap virtual address for"
-				" G33 hw status page\n");
-		return -ENOMEM;
-	}
-
-	memset_io(dev_priv->dri1.gfx_hws_cpu_addr, 0, PAGE_SIZE);
-	I915_WRITE(HWS_PGA, ring->status_page.gfx_addr);
-
-	DRM_DEBUG_DRIVER("load hws HWS_PGA with gfx mem 0x%x\n",
-			 ring->status_page.gfx_addr);
-	DRM_DEBUG_DRIVER("load hws at %p\n",
-			 ring->status_page.page_addr);
-	return 0;
-}
-
 static int i915_get_bridge_dev(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -1273,12 +340,12 @@
 		dev->switch_power_state = DRM_SWITCH_POWER_CHANGING;
 		/* i915 resume handler doesn't set to D0 */
 		pci_set_power_state(dev->pdev, PCI_D0);
-		i915_resume(dev);
+		i915_resume_legacy(dev);
 		dev->switch_power_state = DRM_SWITCH_POWER_ON;
 	} else {
 		pr_err("switched off\n");
 		dev->switch_power_state = DRM_SWITCH_POWER_CHANGING;
-		i915_suspend(dev, pmm);
+		i915_suspend_legacy(dev, pmm);
 		dev->switch_power_state = DRM_SWITCH_POWER_OFF;
 	}
 }
@@ -1318,44 +385,45 @@
 	 * vga_client_register() fails with -ENODEV.
 	 */
 	ret = vga_client_register(dev->pdev, dev, NULL, i915_vga_set_decode);
-	if (ret && ret != -ENODEV)
+	if (ret && ret != -ENODEV) {
+		DRM_ERROR("unable to register VGA client\n");
 		goto out;
+	}
 
 	intel_register_dsm_handler();
 
 	ret = vga_switcheroo_register_client(dev->pdev, &i915_switcheroo_ops, false);
-	if (ret)
+	if (ret) {
+		DRM_ERROR("unable to register VGA switcheroo\n");
 		goto cleanup_vga_client;
+	}
 
 	/* Initialise stolen first so that we may reserve preallocated
 	 * objects for the BIOS to KMS transition.
 	 */
 	ret = i915_gem_init_stolen(dev);
-	if (ret)
+	if (ret) {
+		DRM_ERROR("unable to initialise stolen memory\n");
 		goto cleanup_vga_switcheroo;
+	}
 
 	intel_power_domains_init_hw(dev_priv);
 
-	/*
-	 * We enable some interrupt sources in our postinstall hooks, so mark
-	 * interrupts as enabled _before_ actually enabling them to avoid
-	 * special cases in our ordering checks.
-	 */
-	dev_priv->pm._irqs_disabled = false;
-
-	ret = drm_irq_install(dev, dev->pdev->irq);
-	if (ret)
+	ret = intel_irq_install(dev_priv);
+	if (ret) {
+		DRM_ERROR("unable to install IRQ\n");
 		goto cleanup_gem_stolen;
+	}
 
 	/* Important: The output setup functions called by modeset_init need
 	 * working irqs for e.g. gmbus and dp aux transfers. */
 	intel_modeset_init(dev);
 
 	ret = i915_gem_init(dev);
-	if (ret)
+	if (ret) {
+		DRM_ERROR("unable to initialise GEM\n");
 		goto cleanup_irq;
-
-	INIT_WORK(&dev_priv->console_resume_work, intel_console_resume);
+	}
 
 	intel_modeset_gem_init(dev);
 
@@ -1366,11 +434,13 @@
 		return 0;
 
 	ret = intel_fbdev_init(dev);
-	if (ret)
+	if (ret) {
+		DRM_ERROR("unable to initialise fbdev\n");
 		goto cleanup_gem;
+	}
 
 	/* Only enable hotplug handling once the fbdev is fully set up. */
-	intel_hpd_init(dev);
+	intel_hpd_init(dev_priv);
 
 	/*
 	 * Some ports require correctly set-up hpd registers for detection to
@@ -1382,7 +452,7 @@
 	 * scanning against hotplug events. Hence do this first and ignore the
 	 * tiny window where we will loose hotplug notifactions.
 	 */
-	intel_fbdev_initial_config(dev);
+	async_schedule(intel_fbdev_initial_config, dev_priv);
 
 	drm_kms_helper_poll_init(dev);
 
@@ -1390,11 +460,10 @@
 
 cleanup_gem:
 	mutex_lock(&dev->struct_mutex);
-	i915_gem_cleanup_ringbuffer(dev);
-	i915_gem_context_fini(dev);
+	i915_gem_fini(dev);
 	mutex_unlock(&dev->struct_mutex);
-	WARN_ON(dev_priv->mm.aliasing_ppgtt);
 cleanup_irq:
+	intel_modeset_cleanup(dev);
 	drm_irq_uninstall(dev);
 cleanup_gem_stolen:
 	i915_gem_cleanup_stolen(dev);
@@ -1406,30 +475,6 @@
 	return ret;
 }
 
-int i915_master_create(struct drm_device *dev, struct drm_master *master)
-{
-	struct drm_i915_master_private *master_priv;
-
-	master_priv = kzalloc(sizeof(*master_priv), GFP_KERNEL);
-	if (!master_priv)
-		return -ENOMEM;
-
-	master->driver_priv = master_priv;
-	return 0;
-}
-
-void i915_master_destroy(struct drm_device *dev, struct drm_master *master)
-{
-	struct drm_i915_master_private *master_priv = master->driver_priv;
-
-	if (!master_priv)
-		return;
-
-	kfree(master_priv);
-
-	master->driver_priv = NULL;
-}
-
 #if IS_ENABLED(CONFIG_FB)
 static int i915_kick_out_firmware_fb(struct drm_i915_private *dev_priv)
 {
@@ -1535,14 +580,14 @@
 
 	info = (struct intel_device_info *)&dev_priv->info;
 
-	if (IS_VALLEYVIEW(dev))
-		for_each_pipe(pipe)
+	if (IS_VALLEYVIEW(dev) || INTEL_INFO(dev)->gen == 9)
+		for_each_pipe(dev_priv, pipe)
 			info->num_sprites[pipe] = 2;
 	else
-		for_each_pipe(pipe)
+		for_each_pipe(dev_priv, pipe)
 			info->num_sprites[pipe] = 1;
 
-	if (i915.disable_display) {
+	if (i915_module.disable_display) {
 		DRM_INFO("Display disabled (module parameter)\n");
 		info->num_pipes = 0;
 	} else if (info->num_pipes > 0 &&
@@ -1601,6 +646,8 @@
 	if (!drm_core_check_feature(dev, DRIVER_MODESET) && !dev->agp)
 		return -EINVAL;
 
+	BUILD_BUG_ON(I915_NUM_ENGINES >= (1 << I915_NUM_ENGINE_BITS));
+
 	dev_priv = kzalloc(sizeof(*dev_priv), GFP_KERNEL);
 	if (dev_priv == NULL)
 		return -ENOMEM;
@@ -1608,13 +655,14 @@
 	dev->dev_private = dev_priv;
 	dev_priv->dev = dev;
 
-	/* copy initial configuration to dev_priv->info */
+	/* Setup the write-once "constant" device info */
 	device_info = (struct intel_device_info *)&dev_priv->info;
-	*device_info = *info;
+	memcpy(device_info, info, sizeof(dev_priv->info));
+	device_info->device_id = dev->pdev->device;
 
 	spin_lock_init(&dev_priv->irq_lock);
 	spin_lock_init(&dev_priv->gpu_error.lock);
-	spin_lock_init(&dev_priv->backlight_lock);
+	mutex_init(&dev_priv->backlight_lock);
 	spin_lock_init(&dev_priv->uncore.lock);
 	spin_lock_init(&dev_priv->mm.object_stat_lock);
 	spin_lock_init(&dev_priv->mmio_flip_lock);
@@ -1670,15 +718,17 @@
 		goto out_regs;
 
 	if (drm_core_check_feature(dev, DRIVER_MODESET)) {
-		ret = i915_kick_out_vgacon(dev_priv);
+		/* WARNING: Apparently we must kick fbdev drivers before vgacon,
+		 * otherwise the vga fbdev driver falls over. */
+		ret = i915_kick_out_firmware_fb(dev_priv);
 		if (ret) {
-			DRM_ERROR("failed to remove conflicting VGA console\n");
+			DRM_ERROR("failed to remove conflicting framebuffer drivers\n");
 			goto out_gtt;
 		}
 
-		ret = i915_kick_out_firmware_fb(dev_priv);
+		ret = i915_kick_out_vgacon(dev_priv);
 		if (ret) {
-			DRM_ERROR("failed to remove conflicting framebuffer drivers\n");
+			DRM_ERROR("failed to remove conflicting VGA console\n");
 			goto out_gtt;
 		}
 	}
@@ -1740,7 +790,7 @@
 		goto out_freewq;
 	}
 
-	intel_irq_init(dev);
+	intel_irq_init(dev_priv);
 	intel_uncore_sanitize(dev);
 
 	/* Try to make sure MCHBAR is enabled before poking at it */
@@ -1782,9 +832,6 @@
 			DRM_ERROR("failed to init modeset\n");
 			goto out_power_well;
 		}
-	} else {
-		/* Start out suspended in ums mode. */
-		dev_priv->ums.mm_suspended = 1;
 	}
 
 	i915_setup_sysfs(dev);
@@ -1798,12 +845,13 @@
 	if (IS_GEN5(dev))
 		intel_gpu_ips_init(dev_priv);
 
-	intel_init_runtime_pm(dev_priv);
+	intel_runtime_pm_enable(dev_priv);
+	i915_perf_register(dev);
 
 	return 0;
 
 out_power_well:
-	intel_power_domains_remove(dev_priv);
+	intel_power_domains_fini(dev_priv);
 	drm_vblank_cleanup(dev);
 out_gem_unload:
 	WARN_ON(unregister_oom_notifier(&dev_priv->mm.oom_notifier));
@@ -1822,7 +870,9 @@
 	arch_phys_wc_del(dev_priv->gtt.mtrr);
 	io_mapping_free(dev_priv->gtt.mappable);
 out_gtt:
-	dev_priv->gtt.base.cleanup(&dev_priv->gtt.base);
+	mutex_lock(&dev->struct_mutex);
+	i915_global_gtt_cleanup(dev);
+	mutex_unlock(&dev->struct_mutex);
 out_regs:
 	intel_uncore_fini(dev);
 	pci_iounmap(dev->pdev, dev_priv->regs);
@@ -1846,16 +896,11 @@
 		return ret;
 	}
 
-	intel_fini_runtime_pm(dev_priv);
+	i915_perf_unregister(dev);
+	intel_power_domains_fini(dev_priv);
 
 	intel_gpu_ips_teardown();
 
-	/* The i915.ko module is still not prepared to be loaded when
-	 * the power well is not enabled, so just enable it in case
-	 * we're going to unload/reload. */
-	intel_display_set_init_power(dev_priv, true);
-	intel_power_domains_remove(dev_priv);
-
 	i915_teardown_sysfs(dev);
 
 	WARN_ON(unregister_oom_notifier(&dev_priv->mm.oom_notifier));
@@ -1866,10 +911,13 @@
 
 	acpi_video_unregister();
 
-	if (drm_core_check_feature(dev, DRIVER_MODESET)) {
+	if (drm_core_check_feature(dev, DRIVER_MODESET))
 		intel_fbdev_fini(dev);
+
+	drm_vblank_cleanup(dev);
+
+	if (drm_core_check_feature(dev, DRIVER_MODESET)) {
 		intel_modeset_cleanup(dev);
-		cancel_work_sync(&dev_priv->console_resume_work);
 
 		/*
 		 * free the memory space allocated for the child device
@@ -1886,7 +934,7 @@
 	}
 
 	/* Free error state after interrupts are fully disabled. */
-	del_timer_sync(&dev_priv->gpu_error.hangcheck_timer);
+	cancel_delayed_work_sync(&dev_priv->gpu_error.hangcheck_work);
 	cancel_work_sync(&dev_priv->gpu_error.work);
 	i915_destroy_error_state(dev);
 
@@ -1900,20 +948,10 @@
 		flush_workqueue(dev_priv->wq);
 
 		mutex_lock(&dev->struct_mutex);
-		i915_gem_cleanup_ringbuffer(dev);
-		i915_gem_context_fini(dev);
-		WARN_ON(dev_priv->mm.aliasing_ppgtt);
+		i915_gem_fini(dev);
 		mutex_unlock(&dev->struct_mutex);
-		i915_gem_cleanup_stolen(dev);
-
-		if (!I915_NEED_GFX_HWS(dev))
-			i915_free_hws(dev);
 	}
 
-	WARN_ON(!list_empty(&dev_priv->vm_list));
-
-	drm_vblank_cleanup(dev);
-
 	intel_teardown_gmbus(dev);
 	intel_teardown_mchbar(dev);
 
@@ -1921,7 +959,10 @@
 	destroy_workqueue(dev_priv->wq);
 	pm_qos_remove_request(&dev_priv->pm_qos);
 
-	dev_priv->gtt.base.cleanup(&dev_priv->gtt.base);
+	mutex_lock(&dev->struct_mutex);
+	i915_global_gtt_cleanup(dev);
+	i915_gem_cleanup_stolen(dev);
+	mutex_unlock(&dev->struct_mutex);
 
 	intel_uncore_fini(dev);
 	if (dev_priv->regs != NULL)
@@ -1961,61 +1002,56 @@
  */
 void i915_driver_lastclose(struct drm_device *dev)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-
-	/* On gen6+ we refuse to init without kms enabled, but then the drm core
-	 * goes right around and calls lastclose. Check for this and don't clean
-	 * up anything. */
-	if (!dev_priv)
-		return;
-
-	if (drm_core_check_feature(dev, DRIVER_MODESET)) {
-		intel_fbdev_restore_mode(dev);
-		vga_switcheroo_process_delayed_switch();
-		return;
-	}
-
-	i915_gem_lastclose(dev);
-
-	i915_dma_cleanup(dev);
+	intel_fbdev_restore_mode(dev);
+	vga_switcheroo_process_delayed_switch();
 }
 
 void i915_driver_preclose(struct drm_device *dev, struct drm_file *file)
 {
+	struct drm_i915_private *dev_priv = to_i915(dev);
+	bool was_interruptible;
+
 	mutex_lock(&dev->struct_mutex);
+	was_interruptible = dev_priv->mm.interruptible;
+	WARN_ON(!was_interruptible);
+	dev_priv->mm.interruptible = false;
+
 	i915_gem_context_close(dev, file);
 	i915_gem_release(dev, file);
+
+	dev_priv->mm.interruptible = was_interruptible;
 	mutex_unlock(&dev->struct_mutex);
+
+	if (drm_core_check_feature(dev, DRIVER_MODESET))
+		intel_modeset_preclose(dev, file);
 }
 
 void i915_driver_postclose(struct drm_device *dev, struct drm_file *file)
 {
 	struct drm_i915_file_private *file_priv = file->driver_priv;
 
-	if (file_priv && file_priv->bsd_ring)
-		file_priv->bsd_ring = NULL;
 	kfree(file_priv);
 }
 
 const struct drm_ioctl_desc i915_ioctls[] = {
-	DRM_IOCTL_DEF_DRV(I915_INIT, i915_dma_init, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
-	DRM_IOCTL_DEF_DRV(I915_FLUSH, i915_flush_ioctl, DRM_AUTH),
-	DRM_IOCTL_DEF_DRV(I915_FLIP, i915_flip_bufs, DRM_AUTH),
-	DRM_IOCTL_DEF_DRV(I915_BATCHBUFFER, i915_batchbuffer, DRM_AUTH),
-	DRM_IOCTL_DEF_DRV(I915_IRQ_EMIT, i915_irq_emit, DRM_AUTH),
-	DRM_IOCTL_DEF_DRV(I915_IRQ_WAIT, i915_irq_wait, DRM_AUTH),
+	DRM_IOCTL_DEF_DRV(I915_INIT, drm_noop, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF_DRV(I915_FLUSH, drm_noop, DRM_AUTH),
+	DRM_IOCTL_DEF_DRV(I915_FLIP, drm_noop, DRM_AUTH),
+	DRM_IOCTL_DEF_DRV(I915_BATCHBUFFER, drm_noop, DRM_AUTH),
+	DRM_IOCTL_DEF_DRV(I915_IRQ_EMIT, drm_noop, DRM_AUTH),
+	DRM_IOCTL_DEF_DRV(I915_IRQ_WAIT, drm_noop, DRM_AUTH),
 	DRM_IOCTL_DEF_DRV(I915_GETPARAM, i915_getparam, DRM_AUTH|DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(I915_SETPARAM, i915_setparam, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
 	DRM_IOCTL_DEF_DRV(I915_ALLOC, drm_noop, DRM_AUTH),
 	DRM_IOCTL_DEF_DRV(I915_FREE, drm_noop, DRM_AUTH),
 	DRM_IOCTL_DEF_DRV(I915_INIT_HEAP, drm_noop, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
-	DRM_IOCTL_DEF_DRV(I915_CMDBUFFER, i915_cmdbuffer, DRM_AUTH),
+	DRM_IOCTL_DEF_DRV(I915_CMDBUFFER, drm_noop, DRM_AUTH),
 	DRM_IOCTL_DEF_DRV(I915_DESTROY_HEAP,  drm_noop, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
 	DRM_IOCTL_DEF_DRV(I915_SET_VBLANK_PIPE,  drm_noop, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
-	DRM_IOCTL_DEF_DRV(I915_GET_VBLANK_PIPE,  i915_vblank_pipe_get, DRM_AUTH),
-	DRM_IOCTL_DEF_DRV(I915_VBLANK_SWAP, i915_vblank_swap, DRM_AUTH),
-	DRM_IOCTL_DEF_DRV(I915_HWS_ADDR, i915_set_status_page, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
-	DRM_IOCTL_DEF_DRV(I915_GEM_INIT, i915_gem_init_ioctl, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY|DRM_UNLOCKED),
+	DRM_IOCTL_DEF_DRV(I915_GET_VBLANK_PIPE,  drm_noop, DRM_AUTH),
+	DRM_IOCTL_DEF_DRV(I915_VBLANK_SWAP, drm_noop, DRM_AUTH),
+	DRM_IOCTL_DEF_DRV(I915_HWS_ADDR, drm_noop, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF_DRV(I915_GEM_INIT, drm_noop, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY|DRM_UNLOCKED),
 	DRM_IOCTL_DEF_DRV(I915_GEM_EXECBUFFER, i915_gem_execbuffer, DRM_AUTH|DRM_UNLOCKED),
 	DRM_IOCTL_DEF_DRV(I915_GEM_EXECBUFFER2, i915_gem_execbuffer2, DRM_AUTH|DRM_UNLOCKED|DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(I915_GEM_PIN, i915_gem_pin_ioctl, DRM_AUTH|DRM_ROOT_ONLY|DRM_UNLOCKED),
@@ -2024,8 +1060,8 @@
 	DRM_IOCTL_DEF_DRV(I915_GEM_SET_CACHING, i915_gem_set_caching_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(I915_GEM_GET_CACHING, i915_gem_get_caching_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(I915_GEM_THROTTLE, i915_gem_throttle_ioctl, DRM_AUTH|DRM_UNLOCKED|DRM_RENDER_ALLOW),
-	DRM_IOCTL_DEF_DRV(I915_GEM_ENTERVT, i915_gem_entervt_ioctl, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY|DRM_UNLOCKED),
-	DRM_IOCTL_DEF_DRV(I915_GEM_LEAVEVT, i915_gem_leavevt_ioctl, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY|DRM_UNLOCKED),
+	DRM_IOCTL_DEF_DRV(I915_GEM_ENTERVT, drm_noop, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY|DRM_UNLOCKED),
+	DRM_IOCTL_DEF_DRV(I915_GEM_LEAVEVT, drm_noop, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY|DRM_UNLOCKED),
 	DRM_IOCTL_DEF_DRV(I915_GEM_CREATE, i915_gem_create_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(I915_GEM_PREAD, i915_gem_pread_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(I915_GEM_PWRITE, i915_gem_pwrite_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
@@ -2048,6 +1084,9 @@
 	DRM_IOCTL_DEF_DRV(I915_REG_READ, i915_reg_read_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(I915_GET_RESET_STATS, i915_get_reset_stats_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(I915_GEM_USERPTR, i915_gem_userptr_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(I915_GEM_CONTEXT_GETPARAM, i915_gem_context_getparam_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(I915_GEM_CONTEXT_SETPARAM, i915_gem_context_setparam_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(I915_GEM_CONTEXT_DUMP, i915_gem_context_dump_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
 };
 
 int i915_max_ioctl = ARRAY_SIZE(i915_ioctls);
diff -urN a/drivers/gpu/drm/i915/i915_drv.c b/drivers/gpu/drm/i915/i915_drv.c
--- a/drivers/gpu/drm/i915/i915_drv.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_drv.c	2014-11-22 14:37:49.330700418 -0700
@@ -356,6 +356,19 @@
 	CURSOR_OFFSETS,
 };
 
+static const struct intel_device_info intel_skylake_info = {
+	.is_preliminary = 1,
+	.is_skylake = 1,
+	.gen = 9, .num_pipes = 3,
+	.need_gfx_hws = 1, .has_hotplug = 1,
+	.ring_mask = RENDER_RING | BSD_RING | BLT_RING | VEBOX_RING,
+	.has_llc = 1,
+	.has_ddi = 1,
+	.has_fbc = 1,
+	GEN_DEFAULT_PIPEOFFSETS,
+	IVB_CURSOR_OFFSETS,
+};
+
 /*
  * Make sure any device matches here are from most specific to most
  * general.  For example, since the Quanta match is based on the subsystem
@@ -392,7 +405,8 @@
 	INTEL_BDW_GT12D_IDS(&intel_broadwell_d_info),	\
 	INTEL_BDW_GT3M_IDS(&intel_broadwell_gt3m_info),	\
 	INTEL_BDW_GT3D_IDS(&intel_broadwell_gt3d_info), \
-	INTEL_CHV_IDS(&intel_cherryview_info)
+	INTEL_CHV_IDS(&intel_cherryview_info),	\
+	INTEL_SKL_IDS(&intel_skylake_info)
 
 static const struct pci_device_id pciidlist[] = {		/* aka */
 	INTEL_PCI_IDS,
@@ -449,7 +463,7 @@
 				dev_priv->pch_type = PCH_LPT;
 				DRM_DEBUG_KMS("Found LynxPoint PCH\n");
 				WARN_ON(!IS_HASWELL(dev));
-				WARN_ON(IS_ULT(dev));
+				WARN_ON(IS_HSW_ULT(dev));
 			} else if (IS_BROADWELL(dev)) {
 				dev_priv->pch_type = PCH_LPT;
 				dev_priv->pch_id =
@@ -460,7 +474,15 @@
 				dev_priv->pch_type = PCH_LPT;
 				DRM_DEBUG_KMS("Found LynxPoint LP PCH\n");
 				WARN_ON(!IS_HASWELL(dev));
-				WARN_ON(!IS_ULT(dev));
+				WARN_ON(!IS_HSW_ULT(dev));
+			} else if (id == INTEL_PCH_SPT_DEVICE_ID_TYPE) {
+				dev_priv->pch_type = PCH_SPT;
+				DRM_DEBUG_KMS("Found SunrisePoint PCH\n");
+				WARN_ON(!IS_SKYLAKE(dev));
+			} else if (id == INTEL_PCH_SPT_LP_DEVICE_ID_TYPE) {
+				dev_priv->pch_type = PCH_SPT;
+				DRM_DEBUG_KMS("Found SunrisePoint LP PCH\n");
+				WARN_ON(!IS_SKYLAKE(dev));
 			} else
 				continue;
 
@@ -473,26 +495,6 @@
 	pci_dev_put(pch);
 }
 
-bool i915_semaphore_is_enabled(struct drm_device *dev)
-{
-	if (INTEL_INFO(dev)->gen < 6)
-		return false;
-
-	if (i915.semaphores >= 0)
-		return i915.semaphores;
-
-	/* Until we get further testing... */
-	if (IS_GEN8(dev))
-		return false;
-
-#ifdef CONFIG_INTEL_IOMMU
-	/* Enable semaphores on SNB when IO remapping is off */
-	if (INTEL_INFO(dev)->gen == 6 && intel_iommu_gfx_mapped)
-		return false;
-#endif
-
-	return true;
-}
 
 void intel_hpd_cancel_work(struct drm_i915_private *dev_priv)
 {
@@ -505,7 +507,7 @@
 	spin_unlock_irq(&dev_priv->irq_lock);
 
 	cancel_work_sync(&dev_priv->dig_port_work);
-	cancel_work_sync(&dev_priv->hotplug_work);
+	cancel_delayed_work_sync(&dev_priv->hotplug_work);
 	cancel_delayed_work_sync(&dev_priv->hotplug_reenable_work);
 }
 
@@ -524,7 +526,11 @@
 	drm_modeset_unlock_all(dev);
 }
 
-static int i915_drm_freeze(struct drm_device *dev)
+static int intel_suspend_complete(struct drm_i915_private *dev_priv);
+static int vlv_resume_prepare(struct drm_i915_private *dev_priv,
+			      bool rpm_resume);
+
+static int i915_drm_suspend(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_crtc *crtc;
@@ -554,6 +560,8 @@
 			return error;
 		}
 
+		intel_suspend_gt_powersave(dev);
+
 		/*
 		 * Disable CRTCs directly since we want to preserve sw state
 		 * for _thaw. Also, power gate the CRTC power wells.
@@ -565,16 +573,12 @@
 
 		intel_dp_mst_suspend(dev);
 
-		flush_delayed_work(&dev_priv->rps.delayed_resume_work);
-
-		intel_runtime_pm_disable_interrupts(dev);
+		intel_runtime_pm_disable_interrupts(dev_priv);
 		intel_hpd_cancel_work(dev_priv);
 
 		intel_suspend_encoders(dev_priv);
 
-		intel_suspend_gt_powersave(dev);
-
-		intel_modeset_suspend_hw(dev);
+		intel_suspend_hw(dev);
 	}
 
 	i915_gem_suspend_gtt_mappings(dev);
@@ -591,9 +595,7 @@
 	intel_uncore_forcewake_reset(dev, false);
 	intel_opregion_fini(dev);
 
-	console_lock();
-	intel_fbdev_set_suspend(dev, FBINFO_STATE_SUSPENDED);
-	console_unlock();
+	intel_fbdev_set_suspend(dev, FBINFO_STATE_SUSPENDED, true);
 
 	dev_priv->suspend_count++;
 
@@ -602,7 +604,26 @@
 	return 0;
 }
 
-int i915_suspend(struct drm_device *dev, pm_message_t state)
+static int i915_drm_suspend_late(struct drm_device *drm_dev)
+{
+	struct drm_i915_private *dev_priv = drm_dev->dev_private;
+	int ret;
+
+	ret = intel_suspend_complete(dev_priv);
+
+	if (ret) {
+		DRM_ERROR("Suspend complete failed: %d\n", ret);
+
+		return ret;
+	}
+
+	pci_disable_device(drm_dev->pdev);
+	pci_set_power_state(drm_dev->pdev, PCI_D3hot);
+
+	return 0;
+}
+
+int i915_suspend_legacy(struct drm_device *dev, pm_message_t state)
 {
 	int error;
 
@@ -612,58 +633,25 @@
 		return -ENODEV;
 	}
 
-	if (state.event == PM_EVENT_PRETHAW)
-		return 0;
-
+	if (WARN_ON_ONCE(state.event != PM_EVENT_SUSPEND &&
+			 state.event != PM_EVENT_FREEZE))
+		return -EINVAL;
 
 	if (dev->switch_power_state == DRM_SWITCH_POWER_OFF)
 		return 0;
 
-	error = i915_drm_freeze(dev);
+	error = i915_drm_suspend(dev);
 	if (error)
 		return error;
 
-	if (state.event == PM_EVENT_SUSPEND) {
-		/* Shut down the device */
-		pci_disable_device(dev->pdev);
-		pci_set_power_state(dev->pdev, PCI_D3hot);
-	}
-
-	return 0;
-}
-
-void intel_console_resume(struct work_struct *work)
-{
-	struct drm_i915_private *dev_priv =
-		container_of(work, struct drm_i915_private,
-			     console_resume_work);
-	struct drm_device *dev = dev_priv->dev;
-
-	console_lock();
-	intel_fbdev_set_suspend(dev, FBINFO_STATE_RUNNING);
-	console_unlock();
-}
-
-static int i915_drm_thaw_early(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-
-	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
-		hsw_disable_pc8(dev_priv);
-
-	intel_uncore_early_sanitize(dev, true);
-	intel_uncore_sanitize(dev);
-	intel_power_domains_init_hw(dev_priv);
-
-	return 0;
+	return i915_drm_suspend_late(dev);
 }
 
-static int __i915_drm_thaw(struct drm_device *dev, bool restore_gtt_mappings)
+static int i915_drm_resume(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	if (drm_core_check_feature(dev, DRIVER_MODESET) &&
-	    restore_gtt_mappings) {
+	if (drm_core_check_feature(dev, DRIVER_MODESET)) {
 		mutex_lock(&dev->struct_mutex);
 		i915_gem_restore_gtt_mappings(dev);
 		mutex_unlock(&dev->struct_mutex);
@@ -684,17 +672,15 @@
 		}
 		mutex_unlock(&dev->struct_mutex);
 
-		intel_runtime_pm_restore_interrupts(dev);
+		/* We need working interrupts for modeset enabling ... */
+		intel_runtime_pm_enable_interrupts(dev_priv);
 
 		intel_modeset_init_hw(dev);
 
-		{
-			unsigned long irqflags;
-			spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
-			if (dev_priv->display.hpd_irq_setup)
-				dev_priv->display.hpd_irq_setup(dev);
-			spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
-		}
+		spin_lock_irq(&dev_priv->irq_lock);
+		if (dev_priv->display.hpd_irq_setup)
+			dev_priv->display.hpd_irq_setup(dev);
+		spin_unlock_irq(&dev_priv->irq_lock);
 
 		intel_dp_mst_resume(dev);
 		drm_modeset_lock_all(dev);
@@ -707,24 +693,14 @@
 		 * bother with the tiny race here where we might loose hotplug
 		 * notifications.
 		 * */
-		intel_hpd_init(dev);
+		intel_hpd_init(dev_priv);
 		/* Config may have changed between suspend and resume */
 		drm_helper_hpd_irq_event(dev);
 	}
 
 	intel_opregion_init(dev);
 
-	/*
-	 * The console lock can be pretty contented on resume due
-	 * to all the printk activity.  Try to keep it out of the hot
-	 * path of resume if possible.
-	 */
-	if (console_trylock()) {
-		intel_fbdev_set_suspend(dev, FBINFO_STATE_RUNNING);
-		console_unlock();
-	} else {
-		schedule_work(&dev_priv->console_resume_work);
-	}
+	intel_fbdev_set_suspend(dev, FBINFO_STATE_RUNNING, false);
 
 	mutex_lock(&dev_priv->modeset_restore_lock);
 	dev_priv->modeset_restore = MODESET_DONE;
@@ -732,21 +708,15 @@
 
 	intel_opregion_notify_adapter(dev, PCI_D0);
 
-	return 0;
-}
-
-static int i915_drm_thaw(struct drm_device *dev)
-{
-	if (drm_core_check_feature(dev, DRIVER_MODESET))
-		i915_check_and_clear_faults(dev);
+	drm_kms_helper_poll_enable(dev);
 
-	return __i915_drm_thaw(dev, true);
+	return 0;
 }
 
-static int i915_resume_early(struct drm_device *dev)
+static int i915_drm_resume_early(struct drm_device *dev)
 {
-	if (dev->switch_power_state == DRM_SWITCH_POWER_OFF)
-		return 0;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	int ret = 0;
 
 	/*
 	 * We have a resume ordering issue with the snd-hda driver also
@@ -762,33 +732,34 @@
 
 	pci_set_master(dev->pdev);
 
-	return i915_drm_thaw_early(dev);
+	if (IS_VALLEYVIEW(dev_priv))
+		ret = vlv_resume_prepare(dev_priv, false);
+	if (ret)
+		DRM_ERROR("Resume prepare failed: %d,Continuing resume\n", ret);
+
+	intel_uncore_early_sanitize(dev, true);
+
+	if (IS_HASWELL(dev_priv) || IS_BROADWELL(dev_priv))
+		hsw_disable_pc8(dev_priv);
+
+	intel_uncore_sanitize(dev);
+	intel_power_domains_init_hw(dev_priv);
+
+	return ret;
 }
 
-int i915_resume(struct drm_device *dev)
+int i915_resume_legacy(struct drm_device *dev)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
 	int ret;
 
-	/*
-	 * Platforms with opregion should have sane BIOS, older ones (gen3 and
-	 * earlier) need to restore the GTT mappings since the BIOS might clear
-	 * all our scratch PTEs.
-	 */
-	ret = __i915_drm_thaw(dev, !dev_priv->opregion.header);
+	if (dev->switch_power_state == DRM_SWITCH_POWER_OFF)
+		return 0;
+
+	ret = i915_drm_resume_early(dev);
 	if (ret)
 		return ret;
 
-	drm_kms_helper_poll_enable(dev);
-	return 0;
-}
-
-static int i915_resume_legacy(struct drm_device *dev)
-{
-	i915_resume_early(dev);
-	i915_resume(dev);
-
-	return 0;
+	return i915_drm_resume(dev);
 }
 
 /**
@@ -809,22 +780,23 @@
 int i915_reset(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	bool simulated;
 	int ret;
 
-	if (!i915.reset)
+	if (!i915_module.reset)
 		return 0;
 
 	mutex_lock(&dev->struct_mutex);
 
-	i915_gem_reset(dev);
-
-	simulated = dev_priv->gpu_error.stop_rings != 0;
-
 	ret = intel_gpu_reset(dev);
 
+	/* Clear the reset counter. Before anyone else
+	 * can grab the mutex, we will declare whether or
+	 * not the GPU is wedged.
+	 */
+	atomic_inc(&dev_priv->gpu_error.reset_counter);
+
 	/* Also reset the gpu hangman. */
-	if (simulated) {
+	if (dev_priv->gpu_error.stop_rings) {
 		DRM_INFO("Simulated gpu hang, resetting stop_rings\n");
 		dev_priv->gpu_error.stop_rings = 0;
 		if (ret == -ENODEV) {
@@ -834,6 +806,11 @@
 		}
 	}
 
+	if (i915_stop_ring_allow_warn(dev_priv))
+		pr_notice("drm/i915: Resetting chip after gpu hang\n");
+
+	i915_gem_reset(dev);
+
 	if (ret) {
 		DRM_ERROR("Failed to reset chip: %i\n", ret);
 		mutex_unlock(&dev->struct_mutex);
@@ -854,10 +831,7 @@
 	 * was running at the time of the reset (i.e. we weren't VT
 	 * switched away).
 	 */
-	if (drm_core_check_feature(dev, DRIVER_MODESET) ||
-			!dev_priv->ums.mm_suspended) {
-		dev_priv->ums.mm_suspended = 0;
-
+	if (drm_core_check_feature(dev, DRIVER_MODESET)) {
 		ret = i915_gem_init_hw(dev);
 		mutex_unlock(&dev->struct_mutex);
 		if (ret) {
@@ -879,8 +853,6 @@
 		 */
 		if (INTEL_INFO(dev)->gen > 5)
 			intel_reset_gt_powersave(dev);
-
-		intel_hpd_init(dev);
 	} else {
 		mutex_unlock(&dev->struct_mutex);
 	}
@@ -893,7 +865,8 @@
 	struct intel_device_info *intel_info =
 		(struct intel_device_info *) ent->driver_data;
 
-	if (IS_PRELIMINARY_HW(intel_info) && !i915.preliminary_hw_support) {
+	if (IS_PRELIMINARY_HW(intel_info) &&
+	    !i915_module.preliminary_hw_support) {
 		DRM_INFO("This hardware requires preliminary hardware support.\n"
 			 "See CONFIG_DRM_I915_PRELIMINARY_HW_SUPPORT, and/or modparam preliminary_hw_support\n");
 		return -ENODEV;
@@ -933,14 +906,13 @@
 	if (drm_dev->switch_power_state == DRM_SWITCH_POWER_OFF)
 		return 0;
 
-	return i915_drm_freeze(drm_dev);
+	return i915_drm_suspend(drm_dev);
 }
 
 static int i915_pm_suspend_late(struct device *dev)
 {
 	struct pci_dev *pdev = to_pci_dev(dev);
 	struct drm_device *drm_dev = pci_get_drvdata(pdev);
-	struct drm_i915_private *dev_priv = drm_dev->dev_private;
 
 	/*
 	 * We have a suspedn ordering issue with the snd-hda driver also
@@ -954,13 +926,7 @@
 	if (drm_dev->switch_power_state == DRM_SWITCH_POWER_OFF)
 		return 0;
 
-	if (IS_HASWELL(drm_dev) || IS_BROADWELL(drm_dev))
-		hsw_enable_pc8(dev_priv);
-
-	pci_disable_device(pdev);
-	pci_set_power_state(pdev, PCI_D3hot);
-
-	return 0;
+	return i915_drm_suspend_late(drm_dev);
 }
 
 static int i915_pm_resume_early(struct device *dev)
@@ -968,77 +934,30 @@
 	struct pci_dev *pdev = to_pci_dev(dev);
 	struct drm_device *drm_dev = pci_get_drvdata(pdev);
 
-	return i915_resume_early(drm_dev);
-}
-
-static int i915_pm_resume(struct device *dev)
-{
-	struct pci_dev *pdev = to_pci_dev(dev);
-	struct drm_device *drm_dev = pci_get_drvdata(pdev);
-
-	return i915_resume(drm_dev);
-}
-
-static int i915_pm_freeze(struct device *dev)
-{
-	struct pci_dev *pdev = to_pci_dev(dev);
-	struct drm_device *drm_dev = pci_get_drvdata(pdev);
-
-	if (!drm_dev || !drm_dev->dev_private) {
-		dev_err(dev, "DRM not initialized, aborting suspend.\n");
-		return -ENODEV;
-	}
-
-	return i915_drm_freeze(drm_dev);
-}
-
-static int i915_pm_thaw_early(struct device *dev)
-{
-	struct pci_dev *pdev = to_pci_dev(dev);
-	struct drm_device *drm_dev = pci_get_drvdata(pdev);
+	if (drm_dev->switch_power_state == DRM_SWITCH_POWER_OFF)
+		return 0;
 
-	return i915_drm_thaw_early(drm_dev);
+	return i915_drm_resume_early(drm_dev);
 }
 
-static int i915_pm_thaw(struct device *dev)
+static int i915_pm_resume(struct device *dev)
 {
 	struct pci_dev *pdev = to_pci_dev(dev);
 	struct drm_device *drm_dev = pci_get_drvdata(pdev);
 
-	return i915_drm_thaw(drm_dev);
-}
-
-static int i915_pm_poweroff(struct device *dev)
-{
-	struct pci_dev *pdev = to_pci_dev(dev);
-	struct drm_device *drm_dev = pci_get_drvdata(pdev);
+	if (drm_dev->switch_power_state == DRM_SWITCH_POWER_OFF)
+		return 0;
 
-	return i915_drm_freeze(drm_dev);
+	return i915_drm_resume(drm_dev);
 }
 
-static int hsw_runtime_suspend(struct drm_i915_private *dev_priv)
+static int hsw_suspend_complete(struct drm_i915_private *dev_priv)
 {
 	hsw_enable_pc8(dev_priv);
 
 	return 0;
 }
 
-static int snb_runtime_resume(struct drm_i915_private *dev_priv)
-{
-	struct drm_device *dev = dev_priv->dev;
-
-	intel_init_pch_refclk(dev);
-
-	return 0;
-}
-
-static int hsw_runtime_resume(struct drm_i915_private *dev_priv)
-{
-	hsw_disable_pc8(dev_priv);
-
-	return 0;
-}
-
 /*
  * Save all Gunit registers that may be lost after a D3 and a subsequent
  * S0i[R123] transition. The list of registers needing a save/restore is
@@ -1328,7 +1247,7 @@
 	I915_WRITE(VLV_GTLC_PW_STATUS, VLV_GTLC_ALLOWWAKEERR);
 }
 
-static int vlv_runtime_suspend(struct drm_i915_private *dev_priv)
+static int vlv_suspend_complete(struct drm_i915_private *dev_priv)
 {
 	u32 mask;
 	int err;
@@ -1368,7 +1287,8 @@
 	return err;
 }
 
-static int vlv_runtime_resume(struct drm_i915_private *dev_priv)
+static int vlv_resume_prepare(struct drm_i915_private *dev_priv,
+				bool rpm_resume)
 {
 	struct drm_device *dev = dev_priv->dev;
 	int err;
@@ -1393,8 +1313,10 @@
 
 	vlv_check_no_gt_access(dev_priv);
 
-	intel_init_clock_gating(dev);
-	i915_gem_restore_fences(dev);
+	if (rpm_resume) {
+		intel_init_clock_gating(dev);
+		i915_gem_restore_fences(dev);
+	}
 
 	return ret;
 }
@@ -1409,7 +1331,9 @@
 	if (WARN_ON_ONCE(!(dev_priv->rps.enabled && intel_enable_rc6(dev))))
 		return -ENODEV;
 
-	WARN_ON(!HAS_RUNTIME_PM(dev));
+	if (WARN_ON_ONCE(!HAS_RUNTIME_PM(dev)))
+		return -ENODEV;
+
 	assert_force_wake_inactive(dev_priv);
 
 	DRM_DEBUG_KMS("Suspending device\n");
@@ -1438,43 +1362,45 @@
 	i915_gem_release_all_mmaps(dev_priv);
 	mutex_unlock(&dev->struct_mutex);
 
-	/*
-	 * rps.work can't be rearmed here, since we get here only after making
-	 * sure the GPU is idle and the RPS freq is set to the minimum. See
-	 * intel_mark_idle().
-	 */
-	cancel_work_sync(&dev_priv->rps.work);
-	intel_runtime_pm_disable_interrupts(dev);
-
-	if (IS_GEN6(dev)) {
-		ret = 0;
-	} else if (IS_HASWELL(dev) || IS_BROADWELL(dev)) {
-		ret = hsw_runtime_suspend(dev_priv);
-	} else if (IS_VALLEYVIEW(dev)) {
-		ret = vlv_runtime_suspend(dev_priv);
-	} else {
-		ret = -ENODEV;
-		WARN_ON(1);
-	}
+	intel_suspend_gt_powersave(dev);
+	intel_runtime_pm_disable_interrupts(dev_priv);
 
+	ret = intel_suspend_complete(dev_priv);
 	if (ret) {
 		DRM_ERROR("Runtime suspend failed, disabling it (%d)\n", ret);
-		intel_runtime_pm_restore_interrupts(dev);
+		intel_runtime_pm_enable_interrupts(dev_priv);
 
 		return ret;
 	}
 
-	del_timer_sync(&dev_priv->gpu_error.hangcheck_timer);
+	cancel_delayed_work_sync(&dev_priv->gpu_error.hangcheck_work);
+	intel_uncore_forcewake_reset(dev, false);
 	dev_priv->pm.suspended = true;
 
 	/*
-	 * current versions of firmware which depend on this opregion
-	 * notification have repurposed the D1 definition to mean
-	 * "runtime suspended" vs. what you would normally expect (D3)
-	 * to distinguish it from notifications that might be sent
-	 * via the suspend path.
+	 * FIXME: We really should find a document that references the arguments
+	 * used below!
 	 */
-	intel_opregion_notify_adapter(dev, PCI_D1);
+	if (IS_HASWELL(dev)) {
+		/*
+		 * current versions of firmware which depend on this opregion
+		 * notification have repurposed the D1 definition to mean
+		 * "runtime suspended" vs. what you would normally expect (D3)
+		 * to distinguish it from notifications that might be sent via
+		 * the suspend path.
+		 */
+		intel_opregion_notify_adapter(dev, PCI_D1);
+	} else {
+		/*
+		 * On Broadwell, if we use PCI_D1 the PCH DDI ports will stop
+		 * being detected, and the call we do at intel_runtime_resume()
+		 * won't be able to restore them. Since PCI_D3hot matches the
+		 * actual specification and appears to be working, use it. Let's
+		 * assume the other non-Haswell platforms will stay the same as
+		 * Broadwell.
+		 */
+		intel_opregion_notify_adapter(dev, PCI_D3hot);
+	}
 
 	DRM_DEBUG_KMS("Device suspended\n");
 	return 0;
@@ -1485,25 +1411,22 @@
 	struct pci_dev *pdev = to_pci_dev(device);
 	struct drm_device *dev = pci_get_drvdata(pdev);
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	int ret;
+	int ret = 0;
 
-	WARN_ON(!HAS_RUNTIME_PM(dev));
+	if (WARN_ON_ONCE(!HAS_RUNTIME_PM(dev)))
+		return -ENODEV;
 
 	DRM_DEBUG_KMS("Resuming device\n");
 
 	intel_opregion_notify_adapter(dev, PCI_D0);
 	dev_priv->pm.suspended = false;
 
-	if (IS_GEN6(dev)) {
-		ret = snb_runtime_resume(dev_priv);
-	} else if (IS_HASWELL(dev) || IS_BROADWELL(dev)) {
-		ret = hsw_runtime_resume(dev_priv);
-	} else if (IS_VALLEYVIEW(dev)) {
-		ret = vlv_runtime_resume(dev_priv);
-	} else {
-		WARN_ON(1);
-		ret = -ENODEV;
-	}
+	if (IS_GEN6(dev_priv))
+		intel_init_pch_refclk(dev);
+	else if (IS_HASWELL(dev_priv) || IS_BROADWELL(dev_priv))
+		hsw_disable_pc8(dev_priv);
+	else if (IS_VALLEYVIEW(dev_priv))
+		ret = vlv_resume_prepare(dev_priv, true);
 
 	/*
 	 * No point of rolling back things in case of an error, as the best
@@ -1512,8 +1435,8 @@
 	i915_gem_init_swizzling(dev);
 	gen6_update_ring_freq(dev);
 
-	intel_runtime_pm_restore_interrupts(dev);
-	intel_reset_gt_powersave(dev);
+	intel_runtime_pm_enable_interrupts(dev_priv);
+	intel_enable_gt_powersave(dev);
 
 	if (ret)
 		DRM_ERROR("Runtime resume failed, disabling it (%d)\n", ret);
@@ -1523,17 +1446,60 @@
 	return ret;
 }
 
+/*
+ * This function implements common functionality of runtime and system
+ * suspend sequence.
+ */
+static int intel_suspend_complete(struct drm_i915_private *dev_priv)
+{
+	struct drm_device *dev = dev_priv->dev;
+	int ret;
+
+	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
+		ret = hsw_suspend_complete(dev_priv);
+	else if (IS_VALLEYVIEW(dev))
+		ret = vlv_suspend_complete(dev_priv);
+	else
+		ret = 0;
+
+	return ret;
+}
+
 static const struct dev_pm_ops i915_pm_ops = {
+	/*
+	 * S0ix (via system suspend) and S3 event handlers [PMSG_SUSPEND,
+	 * PMSG_RESUME]
+	 */
 	.suspend = i915_pm_suspend,
 	.suspend_late = i915_pm_suspend_late,
 	.resume_early = i915_pm_resume_early,
 	.resume = i915_pm_resume,
-	.freeze = i915_pm_freeze,
-	.thaw_early = i915_pm_thaw_early,
-	.thaw = i915_pm_thaw,
-	.poweroff = i915_pm_poweroff,
+
+	/*
+	 * S4 event handlers
+	 * @freeze, @freeze_late    : called (1) before creating the
+	 *                            hibernation image [PMSG_FREEZE] and
+	 *                            (2) after rebooting, before restoring
+	 *                            the image [PMSG_QUIESCE]
+	 * @thaw, @thaw_early       : called (1) after creating the hibernation
+	 *                            image, before writing it [PMSG_THAW]
+	 *                            and (2) after failing to create or
+	 *                            restore the image [PMSG_RECOVER]
+	 * @poweroff, @poweroff_late: called after writing the hibernation
+	 *                            image, before rebooting [PMSG_HIBERNATE]
+	 * @restore, @restore_early : called after rebooting and restoring the
+	 *                            hibernation image [PMSG_RESTORE]
+	 */
+	.freeze = i915_pm_suspend,
+	.freeze_late = i915_pm_suspend_late,
+	.thaw_early = i915_pm_resume_early,
+	.thaw = i915_pm_resume,
+	.poweroff = i915_pm_suspend,
+	.poweroff_late = i915_pm_suspend_late,
 	.restore_early = i915_pm_resume_early,
 	.restore = i915_pm_resume,
+
+	/* S0ix (via runtime suspend) event handlers */
 	.runtime_suspend = intel_runtime_suspend,
 	.runtime_resume = intel_runtime_resume,
 };
@@ -1572,14 +1538,13 @@
 	.lastclose = i915_driver_lastclose,
 	.preclose = i915_driver_preclose,
 	.postclose = i915_driver_postclose,
+	.set_busid = drm_pci_set_busid,
 
 	/* Used in place of i915_pm_ops for non-DRIVER_MODESET */
-	.suspend = i915_suspend,
+	.suspend = i915_suspend_legacy,
 	.resume = i915_resume_legacy,
 
 	.device_is_agp = i915_driver_device_is_agp,
-	.master_create = i915_master_create,
-	.master_destroy = i915_master_destroy,
 #if defined(CONFIG_DEBUG_FS)
 	.debugfs_init = i915_debugfs_init,
 	.debugfs_cleanup = i915_debugfs_cleanup,
@@ -1627,14 +1592,14 @@
 	 * the default behavior.
 	 */
 #if defined(CONFIG_DRM_I915_KMS)
-	if (i915.modeset != 0)
+	if (i915_module.modeset != 0)
 		driver.driver_features |= DRIVER_MODESET;
 #endif
-	if (i915.modeset == 1)
+	if (i915_module.modeset == 1)
 		driver.driver_features |= DRIVER_MODESET;
 
 #ifdef CONFIG_VGA_CONSOLE
-	if (vgacon_text_force() && i915.modeset == -1)
+	if (vgacon_text_force() && i915_module.modeset == -1)
 		driver.driver_features &= ~DRIVER_MODESET;
 #endif
 
@@ -1663,6 +1628,8 @@
 module_init(i915_init);
 module_exit(i915_exit);
 
-MODULE_AUTHOR(DRIVER_AUTHOR);
+MODULE_AUTHOR("Tungsten Graphics, Inc.");
+MODULE_AUTHOR("Intel Corporation");
+
 MODULE_DESCRIPTION(DRIVER_DESC);
 MODULE_LICENSE("GPL and additional rights");
diff -urN a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
--- a/drivers/gpu/drm/i915/i915_drv.h	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_drv.h	2014-11-22 18:36:45.365015415 -0700
@@ -35,25 +35,29 @@
 #include "i915_reg.h"
 #include "intel_bios.h"
 #include "intel_ringbuffer.h"
+#include "intel_lrc.h"
 #include "i915_gem_gtt.h"
 #include <linux/io-mapping.h>
 #include <linux/i2c.h>
 #include <linux/i2c-algo-bit.h>
 #include <drm/intel-gtt.h>
+#include <drm/drm_legacy.h> /* for struct drm_dma_handle */
 #include <linux/backlight.h>
 #include <linux/hashtable.h>
 #include <linux/intel-iommu.h>
 #include <linux/kref.h>
+#include <linux/perf_event.h>
 #include <linux/pm_qos.h>
 
 /* General customization:
  */
 
-#define DRIVER_AUTHOR		"Tungsten Graphics, Inc."
-
 #define DRIVER_NAME		"i915"
 #define DRIVER_DESC		"Intel Graphics"
-#define DRIVER_DATE		"20140725"
+#define DRIVER_DATE		"20141107"
+
+#undef WARN_ON
+#define WARN_ON(x)		WARN(x, "WARN_ON(" #x ")")
 
 enum pipe {
 	INVALID_PIPE = -1,
@@ -74,6 +78,14 @@
 };
 #define transcoder_name(t) ((t) + 'A')
 
+/*
+ * This is the maximum (across all platforms) number of planes (primary +
+ * sprites) that can be active at the same time on one pipe.
+ *
+ * This value doesn't count the cursor plane.
+ */
+#define I915_MAX_PLANES	3
+
 enum plane {
 	PLANE_A = 0,
 	PLANE_B,
@@ -162,7 +174,10 @@
 	 I915_GEM_DOMAIN_INSTRUCTION | \
 	 I915_GEM_DOMAIN_VERTEX)
 
-#define for_each_pipe(p) for ((p) = 0; (p) < INTEL_INFO(dev)->num_pipes; (p)++)
+#define for_each_pipe(__dev_priv, __p) \
+	for ((__p) = 0; (__p) < INTEL_INFO(__dev_priv)->num_pipes; (__p)++)
+#define for_each_plane(pipe, p) \
+	for ((p) = 0; (p) < INTEL_INFO(dev)->num_sprites[(pipe)] + 1; (p)++)
 #define for_each_sprite(p, s) for ((s) = 0; (s) < INTEL_INFO(dev)->num_sprites[(p)]; (s)++)
 
 #define for_each_crtc(dev, crtc) \
@@ -171,6 +186,11 @@
 #define for_each_intel_crtc(dev, intel_crtc) \
 	list_for_each_entry(intel_crtc, &dev->mode_config.crtc_list, base.head)
 
+#define for_each_intel_encoder(dev, intel_encoder)		\
+	list_for_each_entry(intel_encoder,			\
+			    &(dev)->mode_config.encoder_list,	\
+			    base.head)
+
 #define for_each_encoder_on_crtc(dev, __crtc, intel_encoder) \
 	list_for_each_entry((intel_encoder), &(dev)->mode_config.encoder_list, base.head) \
 		if ((intel_encoder)->base.crtc == (__crtc))
@@ -186,33 +206,59 @@
 struct drm_i915_private;
 struct i915_mm_struct;
 struct i915_mmu_object;
+struct i915_gem_request;
 
 enum intel_dpll_id {
 	DPLL_ID_PRIVATE = -1, /* non-shared dpll in use */
 	/* real shared dpll ids must be >= 0 */
 	DPLL_ID_PCH_PLL_A = 0,
 	DPLL_ID_PCH_PLL_B = 1,
+	/* hsw/bdw */
 	DPLL_ID_WRPLL1 = 0,
 	DPLL_ID_WRPLL2 = 1,
+	/* skl */
+	DPLL_ID_SKL_DPLL1 = 0,
+	DPLL_ID_SKL_DPLL2 = 1,
+	DPLL_ID_SKL_DPLL3 = 2,
 };
-#define I915_NUM_PLLS 2
+#define I915_NUM_PLLS 3
 
 struct intel_dpll_hw_state {
+	/* i9xx, pch plls */
 	uint32_t dpll;
 	uint32_t dpll_md;
 	uint32_t fp0;
 	uint32_t fp1;
+
+	/* hsw, bdw */
 	uint32_t wrpll;
+
+	/* skl */
+	/*
+	 * DPLL_CTRL1 has 6 bits for each each this DPLL. We store those in
+	 * lower part of crtl1 and they get shifted into position when writing
+	 * the register.  This allows us to easily compare the state to share
+	 * the DPLL.
+	 */
+	uint32_t ctrl1;
+	/* HDMI only, 0 when used for DP */
+	uint32_t cfgcr1, cfgcr2;
+};
+
+struct intel_shared_dpll_config {
+	unsigned crtc_mask; /* mask of CRTCs sharing this PLL */
+	struct intel_dpll_hw_state hw_state;
 };
 
 struct intel_shared_dpll {
-	int refcount; /* count of number of CRTCs sharing this PLL */
+	struct intel_shared_dpll_config config;
+	struct intel_shared_dpll_config *new_config;
+
 	int active; /* count of number of active CRTCs (i.e. DPMS on) */
 	bool on; /* is the PLL actually active? Disabled during modeset */
 	const char *name;
 	/* should match the index in the dev_priv->shared_dplls array */
 	enum intel_dpll_id id;
-	struct intel_dpll_hw_state hw_state;
 	/* The mode_set hook is optional and should be used together with the
 	 * intel_prepare_shared_dpll function. */
 	void (*mode_set)(struct drm_i915_private *dev_priv,
@@ -226,6 +272,11 @@
 			     struct intel_dpll_hw_state *hw_state);
 };
 
+#define SKL_DPLL0 0
+#define SKL_DPLL1 1
+#define SKL_DPLL2 2
+#define SKL_DPLL3 3
+
 /* Used by dp and fdi links */
 struct intel_link_m_n {
 	uint32_t	tu;
@@ -254,7 +305,6 @@
 #define DRIVER_PATCHLEVEL	0
 
 #define WATCH_LISTS	0
-#define WATCH_GTT	0
 
 struct opregion_header;
 struct opregion_acpi;
@@ -277,10 +327,6 @@
 struct intel_overlay;
 struct intel_overlay_error_state;
 
-struct drm_i915_master_private {
-	drm_local_map_t *sarea;
-	struct _drm_i915_sarea *sarea_priv;
-};
 #define I915_FENCE_REG_NONE -1
 #define I915_MAX_NUM_FENCES 32
 /* 32 fences + sign bit for FENCE_REG_NONE */
@@ -316,6 +362,7 @@
 	u32 pgtbl_er;
 	u32 ier;
 	u32 gtier[4];
+	u32 gtimr[4];
 	u32 ccid;
 	u32 derrmr;
 	u32 forcewake;
@@ -333,23 +380,26 @@
 	struct drm_i915_error_object *semaphore_obj;
 
 	struct drm_i915_error_ring {
+		int id;
 		bool valid;
 		/* Software tracked state */
 		bool waiting;
 		int hangcheck_score;
-		enum intel_ring_hangcheck_action hangcheck_action;
-		int num_requests;
+		enum intel_engine_hangcheck_action hangcheck_action;
+		int num_batches;
 
 		/* our own tracking of ring head and tail */
 		u32 cpu_ring_head;
 		u32 cpu_ring_tail;
-
-		u32 semaphore_seqno[I915_NUM_RINGS - 1];
+		u32 interrupts;
+		u32 irq_count;
 
 		/* Register state */
 		u32 tail;
 		u32 head;
+		u32 start;
 		u32 ctl;
+		u32 mode;
 		u32 hws;
 		u32 ipeir;
 		u32 ipehr;
@@ -357,25 +407,34 @@
 		u32 bbstate;
 		u32 instpm;
 		u32 instps;
-		u32 seqno;
+		u32 seqno, request, tag, hangcheck;
+		u32 breadcrumb[I915_NUM_ENGINES];
 		u64 bbaddr;
 		u64 acthd;
 		u32 fault_reg;
 		u64 faddr;
 		u32 rc_psmi; /* sleep state */
-		u32 semaphore_mboxes[I915_NUM_RINGS - 1];
+		u32 semaphore_mboxes[I915_NUM_ENGINES];
+		u32 semaphore_sync[I915_NUM_ENGINES];
 
 		struct drm_i915_error_object {
 			int page_count;
+			int unused;
 			u32 gtt_offset;
 			u32 *pages[0];
 		} *ringbuffer, *batchbuffer, *wa_batchbuffer, *ctx, *hws_page;
 
 		struct drm_i915_error_request {
 			long jiffies;
-			u32 seqno;
+			long pid;
+			u32 batch;
+			u32 head;
 			u32 tail;
-		} *requests;
+			u32 seqno;
+			u32 breadcrumb[I915_NUM_ENGINES];
+			u32 complete;
+			u32 tag;
+		} *batches;
 
 		struct {
 			u32 gfx_mode;
@@ -387,11 +446,12 @@
 
 		pid_t pid;
 		char comm[TASK_COMM_LEN];
-	} ring[I915_NUM_RINGS];
+	} ring[I915_NUM_ENGINES];
+
 	struct drm_i915_error_buffer {
 		u32 size;
 		u32 name;
-		u32 rseqno, wseqno;
+		u32 rseqno[I915_NUM_ENGINES], wseqno, fseqno;
 		u32 gtt_offset;
 		u32 read_domains;
 		u32 write_domain;
@@ -406,9 +466,11 @@
 	} **active_bo, **pinned_bo;
 
 	u32 *active_bo_count, *pinned_bo_count;
+	u32 vm_count;
 };
 
 struct intel_connector;
+struct intel_encoder;
 struct intel_crtc_config;
 struct intel_plane_config;
 struct intel_crtc;
@@ -435,7 +497,7 @@
 	 * Returns true on success, false on failure.
 	 */
 	bool (*find_dpll)(const struct intel_limit *limit,
-			  struct drm_crtc *crtc,
+			  struct intel_crtc *crtc,
 			  int target, int refclk,
 			  struct dpll *match_clock,
 			  struct dpll *best_clock);
@@ -451,21 +513,20 @@
 				struct intel_crtc_config *);
 	void (*get_plane_config)(struct intel_crtc *,
 				 struct intel_plane_config *);
-	int (*crtc_mode_set)(struct drm_crtc *crtc,
-			     int x, int y,
-			     struct drm_framebuffer *old_fb);
+	int (*crtc_compute_clock)(struct intel_crtc *crtc);
 	void (*crtc_enable)(struct drm_crtc *crtc);
 	void (*crtc_disable)(struct drm_crtc *crtc);
 	void (*off)(struct drm_crtc *crtc);
-	void (*write_eld)(struct drm_connector *connector,
-			  struct drm_crtc *crtc,
-			  struct drm_display_mode *mode);
+	void (*audio_codec_enable)(struct drm_connector *connector,
+				   struct intel_encoder *encoder,
+				   struct drm_display_mode *mode);
+	void (*audio_codec_disable)(struct intel_encoder *encoder);
 	void (*fdi_link_train)(struct drm_crtc *crtc);
 	void (*init_clock_gating)(struct drm_device *dev);
-	int (*queue_flip)(struct drm_device *dev, struct drm_crtc *crtc,
+	int (*queue_flip)(struct i915_gem_request *rq,
+			  struct intel_crtc *crtc,
 			  struct drm_framebuffer *fb,
 			  struct drm_i915_gem_object *obj,
-			  struct intel_engine_cs *ring,
 			  uint32_t flags);
 	void (*update_primary_plane)(struct drm_crtc *crtc,
 				     struct drm_framebuffer *fb,
@@ -477,7 +538,7 @@
 	/* display clock increase/decrease */
 	/* pll clock increase/decrease */
 
-	int (*setup_backlight)(struct intel_connector *connector);
+	int (*setup_backlight)(struct intel_connector *connector, enum pipe pipe);
 	uint32_t (*get_backlight)(struct intel_connector *connector);
 	void (*set_backlight)(struct intel_connector *connector,
 			      uint32_t level);
@@ -506,18 +567,32 @@
 				uint64_t val, bool trace);
 };
 
+enum {
+	FW_DOMAIN_RENDER = 0,
+	FW_DOMAIN_MEDIA,
+	FW_DOMAIN_BLITTER,
+
+	FW_DOMAIN_COUNT
+};
+
 struct intel_uncore {
 	spinlock_t lock; /** lock is also taken in irq contexts. */
 
 	struct intel_uncore_funcs funcs;
 
 	unsigned fifo_count;
-	unsigned forcewake_count;
+	unsigned fw_domains;
 
-	unsigned fw_rendercount;
-	unsigned fw_mediacount;
-
-	struct timer_list force_wake_timer;
+	struct intel_uncore_forcewake_domain {
+		struct drm_i915_private *i915;
+		int id;
+		unsigned wake_count;
+		struct timer_list timer;
+	} fw_domain[FW_DOMAIN_COUNT];
+#define FORCEWAKE_RENDER	(1 << FW_DOMAIN_RENDER)
+#define FORCEWAKE_MEDIA		(1 << FW_DOMAIN_MEDIA)
+#define FORCEWAKE_BLITTER	(1 << FW_DOMAIN_BLITTER)
+#define FORCEWAKE_ALL		(FORCEWAKE_RENDER | FORCEWAKE_MEDIA | FORCEWAKE_BLITTER)
 };
 
 #define DEV_INFO_FOR_EACH_FLAG(func, sep) \
@@ -534,6 +609,7 @@
 	func(is_ivybridge) sep \
 	func(is_valleyview) sep \
 	func(is_haswell) sep \
+	func(is_skylake) sep \
 	func(is_preliminary) sep \
 	func(has_fbc) sep \
 	func(has_pipe_cxsr) sep \
@@ -551,6 +627,7 @@
 
 struct intel_device_info {
 	u32 display_mmio_offset;
+	u16 device_id;
 	u8 num_pipes:3;
 	u8 num_sprites[I915_MAX_PIPES];
 	u8 gen;
@@ -586,6 +663,11 @@
 	/* Time when this context was last blamed for a GPU reset */
 	unsigned long guilty_ts;
 
+	/* If the contexts causes a second GPU hang within this time,
+	 * it is permanently banned from submitting any more work.
+	 */
+	unsigned long ban_period_seconds;
+
 	/* This context is banned to submit more work */
 	bool banned;
 };
@@ -611,16 +693,18 @@
  */
 struct intel_context {
 	struct kref ref;
+	struct drm_i915_private *i915;
 	int user_handle;
 	uint8_t remap_slice;
 	struct drm_i915_file_private *file_priv;
 	struct i915_ctx_hang_stats hang_stats;
-	struct i915_address_space *vm;
+	struct i915_hw_ppgtt *ppgtt;
 
-	struct {
-		struct drm_i915_gem_object *rcs_state;
+	struct intel_engine_context {
+		struct intel_ringbuffer *ring;
+		struct drm_i915_gem_object *state;
 		bool initialized;
-	} legacy_hw_ctx;
+	} ring[I915_NUM_ENGINES];
 
 	struct list_head link;
 };
@@ -635,6 +719,20 @@
 	struct drm_mm_node compressed_fb;
 	struct drm_mm_node *compressed_llb;
 
+	bool false_color;
+
+	/* Tracks whether the HW is actually enabled, not whether the feature is
+	 * possible. */
+	bool enabled;
+
+	/* On gen8 some rings cannont perform fbc clean operation so for now
+	 * we are doing this on SW with mmio.
+	 * This variable works in the opposite information direction
+	 * of ring->fbc_dirty telling software on frontbuffer tracking
+	 * to perform the cache clean on sw side.
+	 */
+	bool need_sw_cache_clean;
+
 	struct intel_fbc_work {
 		struct delayed_work work;
 		struct drm_crtc *crtc;
@@ -676,6 +774,7 @@
 	PCH_IBX,	/* Ibexpeak PCH */
 	PCH_CPT,	/* Cougarpoint PCH */
 	PCH_LPT,	/* Lynxpoint PCH */
+	PCH_SPT,        /* Sunrisepoint PCH */
 	PCH_NOP,
 };
 
@@ -688,6 +787,7 @@
 #define QUIRK_LVDS_SSC_DISABLE (1<<1)
 #define QUIRK_INVERT_BRIGHTNESS (1<<2)
 #define QUIRK_BACKLIGHT_PRESENT (1<<3)
+#define QUIRK_PIPEB_FORCE (1<<4)
 
 struct intel_fbdev;
 struct intel_fbc_work;
@@ -739,7 +839,6 @@
 	u32 saveBLC_HIST_CTL;
 	u32 saveBLC_PWM_CTL;
 	u32 saveBLC_PWM_CTL2;
-	u32 saveBLC_HIST_CTL_B;
 	u32 saveBLC_CPU_PWM_CTL;
 	u32 saveBLC_CPU_PWM_CTL2;
 	u32 saveFPB0;
@@ -918,9 +1017,14 @@
 };
 
 struct intel_gen6_power_mgmt {
-	/* work and pm_iir are protected by dev_priv->irq_lock */
+	/*
+	 * work, interrupts_enabled and pm_iir are protected by
+	 * dev_priv->irq_lock
+	 */
 	struct work_struct work;
+	bool interrupts_enabled;
 	u32 pm_iir;
+	u32 pm_events;
 
 	/* Frequencies are stored in potentially platform dependent multiples.
 	 * In other words, *_freq needs to be multiplied by X to be interesting.
@@ -942,13 +1046,15 @@
 	u8 rp0_freq;		/* Non-overclocked max frequency. */
 	u32 cz_freq;
 
-	u32 ei_interrupt_count;
+	u8 up_threshold; /* Current %busy required to uplock */
+	u8 down_threshold; /* Current %busy required to downclock */
 
 	int last_adj;
 	enum { LOW_POWER, BETWEEN, HIGH_POWER } power;
 
 	bool enabled;
 	struct delayed_work delayed_resume_work;
+	struct list_head clients;
 
 	/* manual wa residency calculations */
 	struct intel_rps_ei up_ei, down_ei;
@@ -982,7 +1088,6 @@
 	int r_t;
 
 	struct drm_i915_gem_object *pwrctx;
-	struct drm_i915_gem_object *renderctx;
 };
 
 struct drm_i915_private;
@@ -1042,31 +1147,6 @@
 	struct i915_power_well *power_wells;
 };
 
-struct i915_dri1_state {
-	unsigned allow_batchbuffer : 1;
-	u32 __iomem *gfx_hws_cpu_addr;
-
-	unsigned int cpp;
-	int back_offset;
-	int front_offset;
-	int current_page;
-	int page_flipping;
-
-	uint32_t counter;
-};
-
-struct i915_ums_state {
-	/**
-	 * Flag if the X Server, and thus DRM, is not currently in
-	 * control of the device.
-	 *
-	 * This is set between LeaveVT and EnterVT.  It needs to be
-	 * replaced with a semaphore.  It also needs to be
-	 * transitioned away from for kernel modesetting.
-	 */
-	int mm_suspended;
-};
-
 #define MAX_L3_SLICES 2
 struct intel_l3_parity {
 	u32 *remap_info[MAX_L3_SLICES];
@@ -1147,6 +1227,7 @@
 };
 
 struct drm_i915_error_state_buf {
+	struct drm_i915_private *i915;
 	unsigned bytes;
 	unsigned size;
 	int err;
@@ -1167,7 +1248,7 @@
 	/* Hang gpu twice in this window and your context gets banned */
 #define DRM_I915_CTX_BAN_PERIOD DIV_ROUND_UP(8*DRM_I915_HANGCHECK_PERIOD, 1000)
 
-	struct timer_list hangcheck_timer;
+	struct delayed_work hangcheck_work;
 
 	/* For reset and error_state handling. */
 	spinlock_t lock;
@@ -1213,7 +1294,7 @@
 	/* Userspace knobs for gpu hang simulation;
 	 * combines both a ring mask, and extra flags
 	 */
-	u32 stop_rings;
+	unsigned long stop_rings;
 #define I915_STOP_RING_ALLOW_BAN       (1 << 31)
 #define I915_STOP_RING_ALLOW_WARN      (1 << 30)
 
@@ -1228,6 +1309,12 @@
 };
 
 struct ddi_vbt_port_info {
+	/*
+	 * This is an index in the HDMI/DVI DDI buffer translation table.
+	 * The special value HDMI_LEVEL_SHIFT_UNKNOWN means the VBT didn't
+	 * populate this field.
+	 */
+#define HDMI_LEVEL_SHIFT_UNKNOWN	0xff
 	uint8_t hdmi_level_shift;
 
 	uint8_t supports_dvi:1;
@@ -1318,6 +1405,49 @@
 	enum intel_ddb_partitioning partitioning;
 };
 
+struct skl_ddb_entry {
+	uint16_t start, end;	/* in number of blocks, 'end' is exclusive */
+};
+
+static inline uint16_t skl_ddb_entry_size(const struct skl_ddb_entry *entry)
+{
+	return entry->end - entry->start;
+}
+
+static inline bool skl_ddb_entry_equal(const struct skl_ddb_entry *e1,
+				       const struct skl_ddb_entry *e2)
+{
+	if (e1->start == e2->start && e1->end == e2->end)
+		return true;
+
+	return false;
+}
+
+struct skl_ddb_allocation {
+	struct skl_ddb_entry pipe[I915_MAX_PIPES];
+	struct skl_ddb_entry plane[I915_MAX_PIPES][I915_MAX_PLANES];
+	struct skl_ddb_entry cursor[I915_MAX_PIPES];
+};
+
+struct skl_wm_values {
+	bool dirty[I915_MAX_PIPES];
+	struct skl_ddb_allocation ddb;
+	uint32_t wm_linetime[I915_MAX_PIPES];
+	uint32_t plane[I915_MAX_PIPES][I915_MAX_PLANES][8];
+	uint32_t cursor[I915_MAX_PIPES][8];
+	uint32_t plane_trans[I915_MAX_PIPES][I915_MAX_PLANES];
+	uint32_t cursor_trans[I915_MAX_PIPES];
+};
+
+struct skl_wm_level {
+	bool plane_en[I915_MAX_PLANES];
+	bool cursor_en;
+	uint16_t plane_res_b[I915_MAX_PLANES];
+	uint8_t plane_res_l[I915_MAX_PLANES];
+	uint16_t cursor_res_b;
+	uint8_t cursor_res_l;
+};
+
 /*
  * This struct helps tracking the state needed for runtime PM, which puts the
  * device in PCI D3 state. Notice that when this happens, nothing on the
@@ -1330,7 +1460,7 @@
  *
  * Our driver uses the autosuspend delay feature, which means we'll only really
  * suspend if we stay with zero refcount for a certain amount of time. The
- * default value is currently very conservative (see intel_init_runtime_pm), but
+ * default value is currently very conservative (see intel_runtime_pm_enable), but
  * it can be changed with the standard runtime PM files from sysfs.
  *
  * The irqs_disabled variable becomes true exactly after we disable the IRQs and
@@ -1343,7 +1473,7 @@
  */
 struct i915_runtime_pm {
 	bool suspended;
-	bool _irqs_disabled;
+	bool irqs_enabled;
 };
 
 enum intel_pipe_crc_source {
@@ -1387,6 +1517,28 @@
 	unsigned flip_bits;
 };
 
+struct i915_wa_reg {
+	u32 addr;
+	u32 value;
+	/* bitmask representing WA bits */
+	u32 mask;
+};
+
+#define I915_MAX_WA_REGS 16
+
+struct i915_workarounds {
+	struct i915_wa_reg reg[I915_MAX_WA_REGS];
+	u32 count;
+};
+
+enum {
+	__I915_SAMPLE_FREQ_ACT = 0,
+	__I915_SAMPLE_FREQ_REQ,
+	__I915_SAMPLE_INSTDONE_0,
+	__I915_SAMPLE_INSTDONE_63 = __I915_SAMPLE_INSTDONE_0 + 63,
+	__I915_NUM_PMU_SAMPLERS
+};
+
 struct drm_i915_private {
 	struct drm_device *dev;
 	struct kmem_cache *slab;
@@ -1417,11 +1569,11 @@
 	wait_queue_head_t gmbus_wait_queue;
 
 	struct pci_dev *bridge_dev;
-	struct intel_engine_cs ring[I915_NUM_RINGS];
+	struct intel_engine_cs engine[I915_NUM_ENGINES];
 	struct drm_i915_gem_object *semaphore_obj;
-	uint32_t last_seqno, next_seqno;
+	uint32_t next_seqno;
 
-	drm_dma_handle_t *status_page_dmah;
+	struct drm_dma_handle *status_page_dmah;
 	struct resource mch_res;
 
 	/* protects the irq masks */
@@ -1438,17 +1590,17 @@
 	/* DPIO indirect register protection */
 	struct mutex dpio_lock;
 
-	/** Cached value of IMR to avoid reads in updating the bitfield */
+	/** Cached value of IMR/IER to avoid reads in updating the bitfield */
 	union {
 		u32 irq_mask;
 		u32 de_irq_mask[I915_MAX_PIPES];
 	};
+	u32 irq_enable;
 	u32 gt_irq_mask;
 	u32 pm_irq_mask;
-	u32 pm_rps_events;
 	u32 pipestat_irq_mask[I915_MAX_PIPES];
 
-	struct work_struct hotplug_work;
+	struct delayed_work hotplug_work;
 	struct {
 		unsigned long hpd_last_jiffies;
 		int hpd_cnt;
@@ -1466,21 +1618,27 @@
 	struct intel_opregion opregion;
 	struct intel_vbt_data vbt;
 
+	bool preserve_bios_swizzle;
+
 	/* overlay */
 	struct intel_overlay *overlay;
 
 	/* backlight registers and fields in struct intel_panel */
-	spinlock_t backlight_lock;
+	struct mutex backlight_lock;
 
 	/* LVDS info */
 	bool no_aux_handshake;
 
+	/* protects panel power sequencer state */
+	struct mutex pps_mutex;
+
 	struct drm_i915_fence_reg fence_regs[I915_MAX_NUM_FENCES]; /* assume 965 */
 	int fence_reg_start; /* 4 if userland hasn't ioctl'd us yet */
 	int num_fence_regs; /* 8 on pre-965, 16 otherwise */
 
 	unsigned int fsb_freq, mem_freq, is_ddr3;
 	unsigned int vlv_cdclk_freq;
+	unsigned int hpll_freq;
 
 	/**
 	 * wq - Driver workqueue for GEM.
@@ -1526,6 +1684,8 @@
 	struct intel_shared_dpll shared_dplls[I915_NUM_PLLS];
 	int dpio_phy_iosf_port[I915_NUM_PHYS_VLV];
 
+	struct i915_workarounds workarounds;
+
 	/* Reclocking support */
 	bool render_reclock_avail;
 	bool lvds_downclock_avail;
@@ -1561,14 +1721,9 @@
 #ifdef CONFIG_DRM_I915_FBDEV
 	/* list of fbdev register on this device */
 	struct intel_fbdev *fbdev;
+	struct work_struct fbdev_suspend_work;
 #endif
 
-	/*
-	 * The console may be contended at resume, but we don't
-	 * want it to block on it.
-	 */
-	struct work_struct console_resume_work;
-
 	struct drm_property *broadcast_rgb_property;
 	struct drm_property *force_audio_property;
 
@@ -1593,9 +1748,25 @@
 		uint16_t spr_latency[5];
 		/* cursor */
 		uint16_t cur_latency[5];
+		/*
+		 * Raw watermark memory latency values
+		 * for SKL for all 8 levels
+		 * in 1us units.
+		 */
+		uint16_t skl_latency[8];
+
+		/*
+		 * The skl_wm_values structure is a bit too big for stack
+		 * allocation, so we keep the staging struct where we store
+		 * intermediate results here instead.
+		 */
+		struct skl_wm_values skl_results;
 
 		/* current hardware state */
-		struct ilk_wm_values hw;
+		union {
+			struct ilk_wm_values hw;
+			struct skl_wm_values skl_hw;
+		};
 	} wm;
 
 	struct i915_runtime_pm pm;
@@ -1614,11 +1785,15 @@
 	 */
 	struct workqueue_struct *dp_wq;
 
-	/* Old dri1 support infrastructure, beware the dragons ya fools entering
-	 * here! */
-	struct i915_dri1_state dri1;
-	/* Old ums support infrastructure, same warning applies. */
-	struct i915_ums_state ums;
+	uint32_t bios_vgacntr;
+
+	struct {
+		struct pmu base;
+		struct hrtimer timer;
+		u64 enable;
+		u64 instdone;
+		u64 sample[__I915_NUM_PMU_SAMPLERS];
+	} pmu;
 
 	/*
 	 * NOTE: This is the dri1/ums dungeon, don't add stuff here. Your patch
@@ -1632,9 +1807,11 @@
 }
 
 /* Iterate over initialised rings */
-#define for_each_ring(ring__, dev_priv__, i__) \
-	for ((i__) = 0; (i__) < I915_NUM_RINGS; (i__)++) \
-		if (((ring__) = &(dev_priv__)->ring[(i__)]), intel_ring_initialized((ring__)))
+#define for_each_engine(engine__, dev_priv__, i__) \
+	for ((i__) = 0; (i__) < I915_NUM_ENGINES; (i__)++) \
+		if (((engine__) = &(dev_priv__)->engine[(i__)]), intel_engine_initialized((engine__)))
+
+#define RCS_ENGINE(x) (&__I915__(x)->engine[RCS])
 
 enum hdmi_force_audio {
 	HDMI_AUDIO_OFF_DVI = -2,	/* no aux data for HDMI-DVI converter */
@@ -1699,16 +1876,15 @@
 	struct drm_mm_node *stolen;
 	struct list_head global_list;
 
-	struct list_head ring_list;
 	/** Used in execbuf to temporarily hold a ref */
 	struct list_head obj_exec_link;
 
 	/**
 	 * This is set if the object is on the active lists (has pending
-	 * rendering and so a non-zero seqno), and is not set if it i s on
-	 * inactive (ready to be unbound) list.
+	 * rendering and so a submitted request), and is not set if it is on
+	 * inactive (ready to be unbound) list. We track activity per engine.
 	 */
-	unsigned int active:1;
+	unsigned int active:I915_NUM_ENGINE_BITS;
 
 	/**
 	 * This is set if the object has been written to since last bound
@@ -1761,19 +1937,16 @@
 	 * Only honoured if hardware has relevant pte bit
 	 */
 	unsigned long gt_ro:1;
-
-	/*
-	 * Is the GPU currently using a fence to access this buffer,
-	 */
-	unsigned int pending_fenced_gpu_access:1;
-	unsigned int fenced_gpu_access:1;
-
 	unsigned int cache_level:3;
 
-	unsigned int has_aliasing_ppgtt_mapping:1;
-	unsigned int has_global_gtt_mapping:1;
 	unsigned int has_dma_mapping:1;
 
+	/**
+	 * This is set if the object is a special page directory used
+	 * for ppGTT.
+	 */
+	unsigned int pde:1;
+
 	unsigned int frontbuffer_bits:INTEL_FRONTBUFFER_BITS;
 
 	struct sg_table *pages;
@@ -1783,13 +1956,11 @@
 	void *dma_buf_vmapping;
 	int vmapping_count;
 
-	struct intel_engine_cs *ring;
-
-	/** Breadcrumb of last rendering to the buffer. */
-	uint32_t last_read_seqno;
-	uint32_t last_write_seqno;
-	/** Breadcrumb of last fenced GPU access to the buffer. */
-	uint32_t last_fenced_seqno;
+	/** Breadcrumbs of last rendering to the buffer. */
+	struct {
+		struct i915_gem_request *request;
+		struct list_head engine_link;
+	} last_write, last_read[I915_NUM_ENGINES], last_fence;
 
 	/** Current tiling stride for the object, if it's tiled. */
 	uint32_t stride;
@@ -1804,10 +1975,10 @@
 	unsigned long user_pin_count;
 	struct drm_file *pin_filp;
 
-	/** for phy allocated objects */
-	drm_dma_handle_t *phys_handle;
-
 	union {
+		/** for phy allocated objects */
+		struct drm_dma_handle *phys_handle;
+
 		struct i915_gem_userptr {
 			uintptr_t ptr;
 			unsigned read_only :1;
@@ -1827,44 +1998,13 @@
 		       unsigned frontbuffer_bits);
 
 /**
- * Request queue structure.
- *
- * The request queue allows us to note sequence numbers that have been emitted
- * and may be associated with active buffers to be retired.
- *
- * By keeping this list, we can avoid having to do questionable
- * sequence-number comparisons on buffer last_rendering_seqnos, and associate
- * an emission time with seqnos for tracking how far ahead of the GPU we are.
+ * Returns true if seq1 is later than seq2.
  */
-struct drm_i915_gem_request {
-	/** On Which ring this request was generated */
-	struct intel_engine_cs *ring;
-
-	/** GEM sequence number associated with this request. */
-	uint32_t seqno;
-
-	/** Position in the ringbuffer of the start of the request */
-	u32 head;
-
-	/** Position in the ringbuffer of the end of the request */
-	u32 tail;
-
-	/** Context related to this request */
-	struct intel_context *ctx;
-
-	/** Batch buffer related to this request if any */
-	struct drm_i915_gem_object *batch_obj;
-
-	/** Time at which this request was emitted, in jiffies. */
-	unsigned long emitted_jiffies;
-
-	/** global list entry for this request */
-	struct list_head list;
-
-	struct drm_i915_file_private *file_priv;
-	/** file_priv list entry for this request */
-	struct list_head client_list;
-};
+static inline bool
+__i915_seqno_passed(uint32_t seq1, uint32_t seq2)
+{
+	return (int32_t)(seq1 - seq2) >= 0;
+}
 
 struct drm_i915_file_private {
 	struct drm_i915_private *dev_priv;
@@ -1873,12 +2013,13 @@
 	struct {
 		spinlock_t lock;
 		struct list_head request_list;
-		struct delayed_work idle_work;
 	} mm;
 	struct idr context_idr;
 
-	atomic_t rps_wait_boost;
-	struct  intel_engine_cs *bsd_ring;
+	struct list_head rps_boost;
+	struct intel_engine_cs *bsd_engine;
+
+	unsigned rps_boosts;
 };
 
 /*
@@ -1971,51 +2112,65 @@
 	int count;
 };
 
-#define INTEL_INFO(dev)	(&to_i915(dev)->info)
+/* Note that the (struct drm_i915_private *) cast is just to shut up gcc. */
+#define __I915__(p) ({ \
+	struct drm_i915_private *__p; \
+	if (__builtin_types_compatible_p(typeof(*p), struct drm_i915_private)) \
+		__p = (struct drm_i915_private *)p; \
+	else if (__builtin_types_compatible_p(typeof(*p), struct drm_device)) \
+		__p = to_i915((struct drm_device *)p); \
+	else \
+		BUILD_BUG(); \
+	__p; \
+})
+#define INTEL_INFO(p) 	(&__I915__(p)->info)
+#define INTEL_DEVID(p)	(INTEL_INFO(p)->device_id)
 
-#define IS_I830(dev)		((dev)->pdev->device == 0x3577)
-#define IS_845G(dev)		((dev)->pdev->device == 0x2562)
+#define IS_I830(dev)		(INTEL_DEVID(dev) == 0x3577)
+#define IS_845G(dev)		(INTEL_DEVID(dev) == 0x2562)
 #define IS_I85X(dev)		(INTEL_INFO(dev)->is_i85x)
-#define IS_I865G(dev)		((dev)->pdev->device == 0x2572)
+#define IS_I865G(dev)		(INTEL_DEVID(dev) == 0x2572)
 #define IS_I915G(dev)		(INTEL_INFO(dev)->is_i915g)
-#define IS_I915GM(dev)		((dev)->pdev->device == 0x2592)
-#define IS_I945G(dev)		((dev)->pdev->device == 0x2772)
+#define IS_I915GM(dev)		(INTEL_DEVID(dev) == 0x2592)
+#define IS_I945G(dev)		(INTEL_DEVID(dev) == 0x2772)
 #define IS_I945GM(dev)		(INTEL_INFO(dev)->is_i945gm)
 #define IS_BROADWATER(dev)	(INTEL_INFO(dev)->is_broadwater)
 #define IS_CRESTLINE(dev)	(INTEL_INFO(dev)->is_crestline)
-#define IS_GM45(dev)		((dev)->pdev->device == 0x2A42)
+#define IS_GM45(dev)		(INTEL_DEVID(dev) == 0x2A42)
 #define IS_G4X(dev)		(INTEL_INFO(dev)->is_g4x)
-#define IS_PINEVIEW_G(dev)	((dev)->pdev->device == 0xa001)
-#define IS_PINEVIEW_M(dev)	((dev)->pdev->device == 0xa011)
+#define IS_PINEVIEW_G(dev)	(INTEL_DEVID(dev) == 0xa001)
+#define IS_PINEVIEW_M(dev)	(INTEL_DEVID(dev) == 0xa011)
 #define IS_PINEVIEW(dev)	(INTEL_INFO(dev)->is_pineview)
 #define IS_G33(dev)		(INTEL_INFO(dev)->is_g33)
-#define IS_IRONLAKE_M(dev)	((dev)->pdev->device == 0x0046)
+#define IS_IRONLAKE_M(dev)	(INTEL_DEVID(dev) == 0x0046)
 #define IS_IVYBRIDGE(dev)	(INTEL_INFO(dev)->is_ivybridge)
-#define IS_IVB_GT1(dev)		((dev)->pdev->device == 0x0156 || \
-				 (dev)->pdev->device == 0x0152 || \
-				 (dev)->pdev->device == 0x015a)
-#define IS_SNB_GT1(dev)		((dev)->pdev->device == 0x0102 || \
-				 (dev)->pdev->device == 0x0106 || \
-				 (dev)->pdev->device == 0x010A)
+#define IS_IVB_GT1(dev)		(INTEL_DEVID(dev) == 0x0156 || \
+				 INTEL_DEVID(dev) == 0x0152 || \
+				 INTEL_DEVID(dev) == 0x015a)
+#define IS_SNB_GT1(dev)		(INTEL_DEVID(dev) == 0x0102 || \
+				 INTEL_DEVID(dev) == 0x0106 || \
+				 INTEL_DEVID(dev) == 0x010A)
 #define IS_VALLEYVIEW(dev)	(INTEL_INFO(dev)->is_valleyview)
 #define IS_CHERRYVIEW(dev)	(INTEL_INFO(dev)->is_valleyview && IS_GEN8(dev))
 #define IS_HASWELL(dev)	(INTEL_INFO(dev)->is_haswell)
 #define IS_BROADWELL(dev)	(!INTEL_INFO(dev)->is_valleyview && IS_GEN8(dev))
+#define IS_SKYLAKE(dev)	(INTEL_INFO(dev)->is_skylake)
 #define IS_MOBILE(dev)		(INTEL_INFO(dev)->is_mobile)
 #define IS_HSW_EARLY_SDV(dev)	(IS_HASWELL(dev) && \
-				 ((dev)->pdev->device & 0xFF00) == 0x0C00)
+				 (INTEL_DEVID(dev) & 0xFF00) == 0x0C00)
 #define IS_BDW_ULT(dev)		(IS_BROADWELL(dev) && \
-				 (((dev)->pdev->device & 0xf) == 0x2  || \
-				 ((dev)->pdev->device & 0xf) == 0x6 || \
-				 ((dev)->pdev->device & 0xf) == 0xe))
+				 ((INTEL_DEVID(dev) & 0xf) == 0x2  || \
+				 (INTEL_DEVID(dev) & 0xf) == 0x6 || \
+				 (INTEL_DEVID(dev) & 0xf) == 0xe))
+#define IS_BDW_GT3(dev)		(IS_BROADWELL(dev) && \
+				 (INTEL_DEVID(dev) & 0x00F0) == 0x0020)
 #define IS_HSW_ULT(dev)		(IS_HASWELL(dev) && \
-				 ((dev)->pdev->device & 0xFF00) == 0x0A00)
-#define IS_ULT(dev)		(IS_HSW_ULT(dev) || IS_BDW_ULT(dev))
+				 (INTEL_DEVID(dev) & 0xFF00) == 0x0A00)
 #define IS_HSW_GT3(dev)		(IS_HASWELL(dev) && \
-				 ((dev)->pdev->device & 0x00F0) == 0x0020)
+				 (INTEL_DEVID(dev) & 0x00F0) == 0x0020)
 /* ULX machines are also considered ULT. */
-#define IS_HSW_ULX(dev)		((dev)->pdev->device == 0x0A0E || \
-				 (dev)->pdev->device == 0x0A1E)
+#define IS_HSW_ULX(dev)		(INTEL_DEVID(dev) == 0x0A0E || \
+				 INTEL_DEVID(dev) == 0x0A1E)
 #define IS_PRELIMINARY_HW(intel_info) ((intel_info)->is_preliminary)
 
 /*
@@ -2031,6 +2186,7 @@
 #define IS_GEN6(dev)	(INTEL_INFO(dev)->gen == 6)
 #define IS_GEN7(dev)	(INTEL_INFO(dev)->gen == 7)
 #define IS_GEN8(dev)	(INTEL_INFO(dev)->gen == 8)
+#define IS_GEN9(dev)	(INTEL_INFO(dev)->gen == 9)
 
 #define RENDER_RING		(1<<RCS)
 #define BSD_RING		(1<<VCS)
@@ -2043,14 +2199,13 @@
 #define HAS_VEBOX(dev)		(INTEL_INFO(dev)->ring_mask & VEBOX_RING)
 #define HAS_LLC(dev)		(INTEL_INFO(dev)->has_llc)
 #define HAS_WT(dev)		((IS_HASWELL(dev) || IS_BROADWELL(dev)) && \
-				 to_i915(dev)->ellc_size)
+				 __I915__(dev)->ellc_size)
 #define I915_NEED_GFX_HWS(dev)	(INTEL_INFO(dev)->need_gfx_hws)
 
-#define HAS_HW_CONTEXTS(dev)	(INTEL_INFO(dev)->gen >= 6)
-#define HAS_ALIASING_PPGTT(dev)	(INTEL_INFO(dev)->gen >= 6)
-#define HAS_PPGTT(dev)		(INTEL_INFO(dev)->gen >= 7 && !IS_GEN8(dev))
-#define USES_PPGTT(dev)		intel_enable_ppgtt(dev, false)
-#define USES_FULL_PPGTT(dev)	intel_enable_ppgtt(dev, true)
+#define HAS_HW_CONTEXTS(dev)	(INTEL_INFO(dev)->gen >= 5)
+#define HAS_LOGICAL_RING_CONTEXTS(dev)	(INTEL_INFO(dev)->gen >= 8)
+#define USES_PPGTT(dev)		(i915_module.enable_ppgtt)
+#define USES_FULL_PPGTT(dev)	(i915_module.enable_ppgtt == 2)
 
 #define HAS_OVERLAY(dev)		(INTEL_INFO(dev)->has_overlay)
 #define OVERLAY_NEEDS_PHYSICAL(dev)	(INTEL_INFO(dev)->overlay_needs_physical)
@@ -2081,13 +2236,15 @@
 #define HAS_PIPE_CXSR(dev) (INTEL_INFO(dev)->has_pipe_cxsr)
 #define HAS_FBC(dev) (INTEL_INFO(dev)->has_fbc)
 
-#define HAS_IPS(dev)		(IS_ULT(dev) || IS_BROADWELL(dev))
+#define HAS_IPS(dev)		(IS_HSW_ULT(dev) || IS_BROADWELL(dev))
 
 #define HAS_DDI(dev)		(INTEL_INFO(dev)->has_ddi)
 #define HAS_FPGA_DBG_UNCLAIMED(dev)	(INTEL_INFO(dev)->has_fpga_dbg)
 #define HAS_PSR(dev)		(IS_HASWELL(dev) || IS_BROADWELL(dev))
 #define HAS_RUNTIME_PM(dev)	(IS_GEN6(dev) || IS_HASWELL(dev) || \
 				 IS_BROADWELL(dev) || IS_VALLEYVIEW(dev))
+#define HAS_RC6(dev)		(INTEL_INFO(dev)->gen >= 6)
+#define HAS_RC6p(dev)		(INTEL_INFO(dev)->gen == 6 || IS_IVYBRIDGE(dev))
 
 #define INTEL_PCH_DEVICE_ID_MASK		0xff00
 #define INTEL_PCH_IBX_DEVICE_ID_TYPE		0x3b00
@@ -2095,8 +2252,11 @@
 #define INTEL_PCH_PPT_DEVICE_ID_TYPE		0x1e00
 #define INTEL_PCH_LPT_DEVICE_ID_TYPE		0x8c00
 #define INTEL_PCH_LPT_LP_DEVICE_ID_TYPE		0x9c00
+#define INTEL_PCH_SPT_DEVICE_ID_TYPE		0xA100
+#define INTEL_PCH_SPT_LP_DEVICE_ID_TYPE		0x9D00
 
-#define INTEL_PCH_TYPE(dev) (to_i915(dev)->pch_type)
+#define INTEL_PCH_TYPE(dev) (__I915__(dev)->pch_type)
+#define HAS_PCH_SPT(dev) (INTEL_PCH_TYPE(dev) == PCH_SPT)
 #define HAS_PCH_LPT(dev) (INTEL_PCH_TYPE(dev) == PCH_LPT)
 #define HAS_PCH_CPT(dev) (INTEL_PCH_TYPE(dev) == PCH_CPT)
 #define HAS_PCH_IBX(dev) (INTEL_PCH_TYPE(dev) == PCH_IBX)
@@ -2116,13 +2276,13 @@
 extern const struct drm_ioctl_desc i915_ioctls[];
 extern int i915_max_ioctl;
 
-extern int i915_suspend(struct drm_device *dev, pm_message_t state);
-extern int i915_resume(struct drm_device *dev);
+extern int i915_suspend_legacy(struct drm_device *dev, pm_message_t state);
+extern int i915_resume_legacy(struct drm_device *dev);
 extern int i915_master_create(struct drm_device *dev, struct drm_master *master);
 extern void i915_master_destroy(struct drm_device *dev, struct drm_master *master);
 
 /* i915_params.c */
-struct i915_params {
+extern struct i915_module_parameters {
 	int modeset;
 	int panel_ignore_lid;
 	unsigned int powersave;
@@ -2134,6 +2294,7 @@
 	int enable_rc6;
 	int enable_fbc;
 	int enable_ppgtt;
+	int enable_execlists;
 	int enable_psr;
 	unsigned int preliminary_hw_support;
 	int disable_power_well;
@@ -2149,12 +2310,9 @@
 	bool disable_vtd_wa;
 	int use_mmio_flip;
 	bool mmio_debug;
-};
-extern struct i915_params i915 __read_mostly;
+} i915_module __read_mostly;
 
-				/* i915_dma.c */
-void i915_update_dri1_breadcrumb(struct drm_device *dev);
-extern void i915_kernel_lost_context(struct drm_device * dev);
+/* i915_dma.c */
 extern int i915_driver_load(struct drm_device *, unsigned long flags);
 extern int i915_driver_unload(struct drm_device *);
 extern int i915_driver_open(struct drm_device *dev, struct drm_file *file);
@@ -2168,9 +2326,6 @@
 extern long i915_compat_ioctl(struct file *filp, unsigned int cmd,
 			      unsigned long arg);
 #endif
-extern int i915_emit_box(struct drm_device *dev,
-			 struct drm_clip_rect *box,
-			 int DR1, int DR4);
 extern int intel_gpu_reset(struct drm_device *dev);
 extern int i915_reset(struct drm_device *dev);
 extern unsigned long i915_chipset_val(struct drm_i915_private *dev_priv);
@@ -2180,18 +2335,19 @@
 int vlv_force_gfx_clock(struct drm_i915_private *dev_priv, bool on);
 void intel_hpd_cancel_work(struct drm_i915_private *dev_priv);
 
-extern void intel_console_resume(struct work_struct *work);
-
 /* i915_irq.c */
 void i915_queue_hangcheck(struct drm_device *dev);
 __printf(3, 4)
-void i915_handle_error(struct drm_device *dev, bool wedged,
+void i915_handle_error(struct drm_device *dev,
+		       unsigned flags,
 		       const char *fmt, ...);
+#define I915_HANG_RESET 0x1
+#define I915_HANG_SIMULATED 0x2
 
-void gen6_set_pm_mask(struct drm_i915_private *dev_priv, u32 pm_iir,
-							int new_delay);
-extern void intel_irq_init(struct drm_device *dev);
-extern void intel_hpd_init(struct drm_device *dev);
+extern void intel_irq_init(struct drm_i915_private *dev_priv);
+extern void intel_hpd_init(struct drm_i915_private *dev_priv);
+int intel_irq_install(struct drm_i915_private *dev_priv);
+void intel_irq_uninstall(struct drm_i915_private *dev_priv);
 
 extern void intel_uncore_sanitize(struct drm_device *dev);
 extern void intel_uncore_early_sanitize(struct drm_device *dev,
@@ -2211,10 +2367,19 @@
 
 void valleyview_enable_display_irqs(struct drm_i915_private *dev_priv);
 void valleyview_disable_display_irqs(struct drm_i915_private *dev_priv);
+void
+ironlake_enable_display_irq(struct drm_i915_private *dev_priv, u32 mask);
+void
+ironlake_disable_display_irq(struct drm_i915_private *dev_priv, u32 mask);
+void ibx_display_interrupt_update(struct drm_i915_private *dev_priv,
+				  uint32_t interrupt_mask,
+				  uint32_t enabled_irq_mask);
+#define ibx_enable_display_interrupt(dev_priv, bits) \
+	ibx_display_interrupt_update((dev_priv), (bits), (bits))
+#define ibx_disable_display_interrupt(dev_priv, bits) \
+	ibx_display_interrupt_update((dev_priv), (bits), 0)
 
 /* i915_gem.c */
-int i915_gem_init_ioctl(struct drm_device *dev, void *data,
-			struct drm_file *file_priv);
 int i915_gem_create_ioctl(struct drm_device *dev, void *data,
 			  struct drm_file *file_priv);
 int i915_gem_pread_ioctl(struct drm_device *dev, void *data,
@@ -2247,15 +2412,11 @@
 			    struct drm_file *file_priv);
 int i915_gem_madvise_ioctl(struct drm_device *dev, void *data,
 			   struct drm_file *file_priv);
-int i915_gem_entervt_ioctl(struct drm_device *dev, void *data,
-			   struct drm_file *file_priv);
-int i915_gem_leavevt_ioctl(struct drm_device *dev, void *data,
-			   struct drm_file *file_priv);
 int i915_gem_set_tiling(struct drm_device *dev, void *data,
 			struct drm_file *file_priv);
 int i915_gem_get_tiling(struct drm_device *dev, void *data,
 			struct drm_file *file_priv);
-int i915_gem_init_userptr(struct drm_device *dev);
+void i915_gem_init_userptr(struct drm_device *dev);
 int i915_gem_userptr_ioctl(struct drm_device *dev, void *data,
 			   struct drm_file *file);
 int i915_gem_get_aperture_ioctl(struct drm_device *dev, void *data,
@@ -2263,6 +2424,12 @@
 int i915_gem_wait_ioctl(struct drm_device *dev, void *data,
 			struct drm_file *file_priv);
 void i915_gem_load(struct drm_device *dev);
+unsigned long i915_gem_shrink(struct drm_i915_private *dev_priv,
+			      long target,
+			      unsigned flags);
+#define I915_SHRINK_PURGEABLE 0x1
+#define I915_SHRINK_UNBOUND 0x2
+#define I915_SHRINK_BOUND 0x4
 void *i915_gem_object_alloc(struct drm_device *dev);
 void i915_gem_object_free(struct drm_i915_gem_object *obj);
 void i915_gem_object_init(struct drm_i915_gem_object *obj,
@@ -2272,22 +2439,22 @@
 void i915_init_vm(struct drm_i915_private *dev_priv,
 		  struct i915_address_space *vm);
 void i915_gem_free_object(struct drm_gem_object *obj);
-void i915_gem_vma_destroy(struct i915_vma *vma);
+void i915_vma_unreserve(struct i915_vma *vma);
 
-#define PIN_MAPPABLE 0x1
-#define PIN_NONBLOCK 0x2
-#define PIN_GLOBAL 0x4
-#define PIN_OFFSET_BIAS 0x8
+#define PIN_OFFSET_FIXED 0x1
+#define PIN_OFFSET_BIAS 0x2
+#define PIN_LOCAL 0x4
+#define PIN_GLOBAL 0x8
+#define PIN_MAPPABLE 0x10
+#define PIN_NONBLOCK 0x20
 #define PIN_OFFSET_MASK (~4095)
-int __must_check i915_gem_object_pin(struct drm_i915_gem_object *obj,
-				     struct i915_address_space *vm,
-				     uint32_t alignment,
-				     uint64_t flags);
+int __must_check i915_vma_pin(struct i915_vma *vma,
+			      uint32_t alignment,
+			      uint64_t flags);
 int __must_check i915_vma_unbind(struct i915_vma *vma);
 int i915_gem_object_put_pages(struct drm_i915_gem_object *obj);
 void i915_gem_release_all_mmaps(struct drm_i915_private *dev_priv);
 void i915_gem_release_mmap(struct drm_i915_gem_object *obj);
-void i915_gem_lastclose(struct drm_device *dev);
 
 int i915_gem_obj_prepare_shmem_read(struct drm_i915_gem_object *obj,
 				    int *needs_clflush);
@@ -2315,22 +2482,12 @@
 
 int __must_check i915_mutex_lock_interruptible(struct drm_device *dev);
 int i915_gem_object_sync(struct drm_i915_gem_object *obj,
-			 struct intel_engine_cs *to);
-void i915_vma_move_to_active(struct i915_vma *vma,
-			     struct intel_engine_cs *ring);
+			 struct i915_gem_request *rq);
 int i915_gem_dumb_create(struct drm_file *file_priv,
 			 struct drm_device *dev,
 			 struct drm_mode_create_dumb *args);
 int i915_gem_mmap_gtt(struct drm_file *file_priv, struct drm_device *dev,
 		      uint32_t handle, uint64_t *offset);
-/**
- * Returns true if seq1 is later than seq2.
- */
-static inline bool
-i915_seqno_passed(uint32_t seq1, uint32_t seq2)
-{
-	return (int32_t)(seq1 - seq2) >= 0;
-}
 
 int __must_check i915_gem_get_seqno(struct drm_device *dev, u32 *seqno);
 int __must_check i915_gem_set_seqno(struct drm_device *dev, u32 seqno);
@@ -2340,24 +2497,33 @@
 bool i915_gem_object_pin_fence(struct drm_i915_gem_object *obj);
 void i915_gem_object_unpin_fence(struct drm_i915_gem_object *obj);
 
-struct drm_i915_gem_request *
-i915_gem_find_active_request(struct intel_engine_cs *ring);
-
 bool i915_gem_retire_requests(struct drm_device *dev);
-void i915_gem_retire_requests_ring(struct intel_engine_cs *ring);
-int __must_check i915_gem_check_wedge(struct i915_gpu_error *error,
-				      bool interruptible);
-int __must_check i915_gem_check_olr(struct intel_engine_cs *ring, u32 seqno);
+void i915_gem_retire_requests__engine(struct intel_engine_cs *engine);
+
+static inline bool __i915_reset_in_progress(unsigned x)
+{
+	return unlikely(x & I915_RESET_IN_PROGRESS_FLAG);
+}
 
 static inline bool i915_reset_in_progress(struct i915_gpu_error *error)
 {
-	return unlikely(atomic_read(&error->reset_counter)
-			& (I915_RESET_IN_PROGRESS_FLAG | I915_WEDGED));
+	return __i915_reset_in_progress(atomic_read(&error->reset_counter));
+}
+
+static inline bool __i915_terminally_wedged(unsigned x)
+{
+	return unlikely(x & I915_WEDGED);
 }
 
 static inline bool i915_terminally_wedged(struct i915_gpu_error *error)
 {
-	return atomic_read(&error->reset_counter) & I915_WEDGED;
+	return __i915_terminally_wedged(atomic_read(&error->reset_counter));
+}
+
+static inline bool i915_recovery_pending(struct i915_gpu_error *error)
+{
+	unsigned x = atomic_read(&error->reset_counter);
+	return __i915_reset_in_progress(x) && !__i915_terminally_wedged(x);
 }
 
 static inline u32 i915_reset_count(struct i915_gpu_error *error)
@@ -2382,19 +2548,10 @@
 int __must_check i915_gem_object_finish_gpu(struct drm_i915_gem_object *obj);
 int __must_check i915_gem_init(struct drm_device *dev);
 int __must_check i915_gem_init_hw(struct drm_device *dev);
-int i915_gem_l3_remap(struct intel_engine_cs *ring, int slice);
+void i915_gem_fini(struct drm_device *dev);
 void i915_gem_init_swizzling(struct drm_device *dev);
-void i915_gem_cleanup_ringbuffer(struct drm_device *dev);
 int __must_check i915_gpu_idle(struct drm_device *dev);
 int __must_check i915_gem_suspend(struct drm_device *dev);
-int __i915_add_request(struct intel_engine_cs *ring,
-		       struct drm_file *file,
-		       struct drm_i915_gem_object *batch_obj,
-		       u32 *seqno);
-#define i915_add_request(ring, seqno) \
-	__i915_add_request(ring, NULL, NULL, seqno)
-int __must_check i915_wait_seqno(struct intel_engine_cs *ring,
-				 uint32_t seqno);
 int i915_gem_fault(struct vm_area_struct *vma, struct vm_fault *vmf);
 int __must_check
 i915_gem_object_set_to_gtt_domain(struct drm_i915_gem_object *obj,
@@ -2404,7 +2561,7 @@
 int __must_check
 i915_gem_object_pin_to_display_plane(struct drm_i915_gem_object *obj,
 				     u32 alignment,
-				     struct intel_engine_cs *pipelined);
+				     struct i915_gem_request *pipelined);
 void i915_gem_object_unpin_from_display_plane(struct drm_i915_gem_object *obj);
 int i915_gem_object_attach_phys(struct drm_i915_gem_object *obj,
 				int align);
@@ -2438,74 +2595,77 @@
 struct i915_vma *i915_gem_obj_to_vma(struct drm_i915_gem_object *obj,
 				     struct i915_address_space *vm);
 struct i915_vma *
-i915_gem_obj_lookup_or_create_vma(struct drm_i915_gem_object *obj,
-				  struct i915_address_space *vm);
+i915_gem_obj_get_vma(struct drm_i915_gem_object *obj,
+		     struct i915_address_space *vm);
 
-struct i915_vma *i915_gem_obj_to_ggtt(struct drm_i915_gem_object *obj);
 static inline bool i915_gem_obj_is_pinned(struct drm_i915_gem_object *obj) {
 	struct i915_vma *vma;
-	list_for_each_entry(vma, &obj->vma_list, vma_link)
+	list_for_each_entry(vma, &obj->vma_list, obj_link)
 		if (vma->pin_count > 0)
 			return true;
 	return false;
 }
 
 /* Some GGTT VM helpers */
-#define obj_to_ggtt(obj) \
-	(&((struct drm_i915_private *)(obj)->base.dev->dev_private)->gtt.base)
+#define i915_obj_to_ggtt(obj) (&to_i915((obj)->base.dev)->gtt.base)
 static inline bool i915_is_ggtt(struct i915_address_space *vm)
 {
-	struct i915_address_space *ggtt =
-		&((struct drm_i915_private *)(vm)->dev->dev_private)->gtt.base;
-	return vm == ggtt;
+	return vm == &to_i915(vm->dev)->gtt.base;
 }
 
-static inline bool i915_gem_obj_ggtt_bound(struct drm_i915_gem_object *obj)
+struct i915_vma *i915_gem_obj_to_ggtt(struct drm_i915_gem_object *obj);
+static inline struct i915_vma *
+i915_gem_obj_get_ggtt(struct drm_i915_gem_object *obj)
 {
-	return i915_gem_obj_bound(obj, obj_to_ggtt(obj));
+	return i915_gem_obj_get_vma(obj, i915_obj_to_ggtt(obj));
 }
 
-static inline unsigned long
-i915_gem_obj_ggtt_offset(struct drm_i915_gem_object *obj)
+
+static inline struct i915_hw_ppgtt *
+i915_vm_to_ppgtt(struct i915_address_space *vm)
 {
-	return i915_gem_obj_offset(obj, obj_to_ggtt(obj));
+	WARN_ON(i915_is_ggtt(vm));
+
+	return container_of(vm, struct i915_hw_ppgtt, base);
 }
 
-static inline unsigned long
-i915_gem_obj_ggtt_size(struct drm_i915_gem_object *obj)
+
+static inline bool i915_gem_obj_ggtt_bound(struct drm_i915_gem_object *obj)
 {
-	return i915_gem_obj_size(obj, obj_to_ggtt(obj));
+	return i915_gem_obj_bound(obj, i915_obj_to_ggtt(obj));
 }
 
-static inline int __must_check
-i915_gem_obj_ggtt_pin(struct drm_i915_gem_object *obj,
-		      uint32_t alignment,
-		      unsigned flags)
+static inline unsigned long
+i915_gem_obj_ggtt_offset(struct drm_i915_gem_object *obj)
 {
-	return i915_gem_object_pin(obj, obj_to_ggtt(obj), alignment, flags | PIN_GLOBAL);
+	return i915_gem_obj_offset(obj, i915_obj_to_ggtt(obj));
 }
 
-static inline int
-i915_gem_object_ggtt_unbind(struct drm_i915_gem_object *obj)
+static inline unsigned long
+i915_gem_obj_ggtt_size(struct drm_i915_gem_object *obj)
 {
-	return i915_vma_unbind(i915_gem_obj_to_ggtt(obj));
+	return i915_gem_obj_size(obj, i915_obj_to_ggtt(obj));
 }
 
+int __must_check i915_gem_object_ggtt_pin(struct drm_i915_gem_object *obj,
+					  uint32_t alignment,
+					  unsigned flags);
 void i915_gem_object_ggtt_unpin(struct drm_i915_gem_object *obj);
+void i915_gem_object_finish_gtt(struct drm_i915_gem_object *obj);
 
 /* i915_gem_context.c */
-#define ctx_to_ppgtt(ctx) container_of((ctx)->vm, struct i915_hw_ppgtt, base)
 int __must_check i915_gem_context_init(struct drm_device *dev);
 void i915_gem_context_fini(struct drm_device *dev);
-void i915_gem_context_reset(struct drm_device *dev);
 int i915_gem_context_open(struct drm_device *dev, struct drm_file *file);
 int i915_gem_context_enable(struct drm_i915_private *dev_priv);
 void i915_gem_context_close(struct drm_device *dev, struct drm_file *file);
-int i915_switch_context(struct intel_engine_cs *ring,
-			struct intel_context *to);
+int i915_request_switch_context(struct i915_gem_request *rq);
+void i915_request_switch_context__commit(struct i915_gem_request *rq);
 struct intel_context *
 i915_gem_context_get(struct drm_i915_file_private *file_priv, u32 id);
-void i915_gem_context_free(struct kref *ctx_ref);
+void __i915_gem_context_free(struct kref *ctx_ref);
+struct drm_i915_gem_object *
+i915_gem_alloc_context_obj(struct drm_device *dev, size_t size);
 static inline void i915_gem_context_reference(struct intel_context *ctx)
 {
 	kref_get(&ctx->ref);
@@ -2513,7 +2673,7 @@
 
 static inline void i915_gem_context_unreference(struct intel_context *ctx)
 {
-	kref_put(&ctx->ref, i915_gem_context_free);
+	kref_put(&ctx->ref, __i915_gem_context_free);
 }
 
 static inline bool i915_gem_context_is_default(const struct intel_context *c)
@@ -2525,9 +2685,15 @@
 				  struct drm_file *file);
 int i915_gem_context_destroy_ioctl(struct drm_device *dev, void *data,
 				   struct drm_file *file);
+int i915_gem_context_getparam_ioctl(struct drm_device *dev, void *data,
+				    struct drm_file *file_priv);
+int i915_gem_context_setparam_ioctl(struct drm_device *dev, void *data,
+				    struct drm_file *file_priv);
+int i915_gem_context_dump_ioctl(struct drm_device *dev, void *data,
+				struct drm_file *file);
 
 /* i915_gem_render_state.c */
-int i915_gem_render_state_init(struct intel_engine_cs *ring);
+int i915_gem_render_state_init(struct i915_gem_request *rq);
 /* i915_gem_evict.c */
 int __must_check i915_gem_evict_something(struct drm_device *dev,
 					  struct i915_address_space *vm,
@@ -2537,7 +2703,10 @@
 					  unsigned long start,
 					  unsigned long end,
 					  unsigned flags);
-int i915_gem_evict_vm(struct i915_address_space *vm, bool do_idle);
+int __must_check
+i915_gem_evict_range(struct drm_device *dev, struct i915_address_space *vm,
+		     unsigned long start, unsigned long end);
+int i915_gem_evict_vm(struct i915_address_space *vm);
 int i915_gem_evict_everything(struct drm_device *dev);
 
 /* belongs in i915_gem_gtt.h */
@@ -2547,6 +2716,164 @@
 		intel_gtt_chipset_flush();
 }
 
+/* i915_gem_request.c */
+
+/**
+ * Request queue structure.
+ *
+ * The request queue allows us to note sequence numbers that have been emitted
+ * and may be associated with active buffers to be retired.
+ *
+ * By keeping this list, we can avoid having to do questionable
+ * sequence-number comparisons on buffer last_rendering_seqnos, and associate
+ * an emission time with seqnos for tracking how far ahead of the GPU we are.
+ */
+struct i915_gem_request {
+	struct kref kref;
+
+	/** On which ring/engine/ctx this request was generated */
+	struct drm_i915_private *i915;
+	struct intel_context *ctx;
+	struct intel_engine_cs *engine;
+	struct intel_ringbuffer *ring;
+
+	/** How many GPU resets ago was this request first constructed? */
+	unsigned reset_counter;
+
+	/** GEM sequence number/breadcrumb associated with this request. */
+	u32 seqno;
+	u32 breadcrumb[I915_NUM_ENGINES];
+	u32 semaphore[I915_NUM_ENGINES];
+
+	/** Position in the ringbuffer of the request */
+	u32 head, tail;
+
+	/** Batch buffer and objects related to this request if any */
+	struct i915_vma *batch;
+	struct list_head vmas;
+
+	/** Time at which this request was emitted, in jiffies. */
+	unsigned long emitted_jiffies;
+
+	/** global list entry for this request */
+	struct list_head engine_link;
+	struct list_head breadcrumb_link;
+
+	struct drm_i915_file_private *file_priv;
+	/** file_priv list entry for this request */
+	struct list_head client_list;
+
+	u16 tag;
+	unsigned remap_l3:8;
+	unsigned pending_flush:4;
+	bool outstanding:1;
+	bool has_ctx_switch:1;
+
+	bool completed; /* kept separate for atomicity */
+};
+
+struct i915_gem_request * __must_check __attribute__((nonnull))
+i915_request_create(struct intel_context *ctx,
+		    struct intel_engine_cs *engine);
+
+static inline struct intel_engine_cs *i915_request_engine(struct i915_gem_request *rq)
+{
+	return rq ? rq->engine : NULL;
+}
+
+static inline int i915_request_engine_id(struct i915_gem_request *rq)
+{
+	return rq ? rq->engine->id : -1;
+}
+
+static inline u32 i915_request_seqno(struct i915_gem_request *rq)
+{
+	return rq ? rq->seqno : 0;
+}
+
+bool __i915_request_complete__wa(struct i915_gem_request *rq);
+
+static inline bool
+i915_request_complete(struct i915_gem_request *rq)
+{
+	if (!rq->completed && rq->engine->is_complete(rq)) {
+		trace_i915_gem_request_complete(rq);
+		rq->completed = true;
+	}
+	return rq->completed;
+}
+
+static inline struct i915_gem_request *
+i915_request_get(struct i915_gem_request *rq)
+{
+	if (rq)
+		kref_get(&rq->kref);
+	return rq;
+}
+
+void __i915_request_free(struct kref *kref);
+
+static inline void
+i915_request_put(struct i915_gem_request *rq)
+{
+	if (rq == NULL)
+		return;
+
+	lockdep_assert_held(&rq->i915->dev->struct_mutex);
+	kref_put(&rq->kref, __i915_request_free);
+}
+
+static inline void
+i915_request_put__unlocked(struct i915_gem_request *rq)
+{
+	if (!atomic_add_unless(&rq->kref.refcount, -1, 1)) {
+		struct drm_device *dev = rq->i915->dev;
+
+		mutex_lock(&dev->struct_mutex);
+		if (likely(atomic_dec_and_test(&rq->kref.refcount)))
+			__i915_request_free(&rq->kref);
+		mutex_unlock(&dev->struct_mutex);
+	}
+}
+
+void
+i915_request_add_vma(struct i915_gem_request *rq,
+		     struct i915_vma *vma,
+		     unsigned fenced);
+#define VMA_IS_FENCED 0x1
+#define VMA_HAS_FENCE 0x2
+int __must_check
+i915_request_emit_flush(struct i915_gem_request *rq,
+			unsigned flags);
+int __must_check
+__i915_request_emit_breadcrumb(struct i915_gem_request *rq, int id);
+static inline int __must_check
+i915_request_emit_breadcrumb(struct i915_gem_request *rq)
+{
+	return __i915_request_emit_breadcrumb(rq, rq->engine->id);
+}
+static inline int __must_check
+i915_request_emit_semaphore(struct i915_gem_request *rq, int id)
+{
+	return __i915_request_emit_breadcrumb(rq, id);
+}
+int __must_check
+i915_request_emit_batchbuffer(struct i915_gem_request *rq,
+			      struct i915_vma *batch,
+			      uint64_t start, uint32_t len,
+			      unsigned flags);
+int __must_check
+i915_request_commit(struct i915_gem_request *rq);
+struct i915_gem_request *
+i915_request_get_breadcrumb(struct i915_gem_request *rq);
+int __must_check
+i915_request_wait(struct i915_gem_request *rq);
+int __i915_request_wait(struct i915_gem_request *rq,
+			bool interruptible,
+			s64 *timeout,
+			struct drm_i915_file_private *file);
+void i915_request_retire(struct i915_gem_request *rq);
+
 /* i915_gem_stolen.c */
 int i915_gem_init_stolen(struct drm_device *dev);
 int i915_gem_stolen_setup_compression(struct drm_device *dev, int size, int fb_cpp);
@@ -2573,13 +2900,6 @@
 void i915_gem_object_do_bit_17_swizzle(struct drm_i915_gem_object *obj);
 void i915_gem_object_save_bit_17_swizzle(struct drm_i915_gem_object *obj);
 
-/* i915_gem_debug.c */
-#if WATCH_LISTS
-int i915_verify_lists(struct drm_device *dev);
-#else
-#define i915_verify_lists(dev) 0
-#endif
-
 /* i915_debugfs.c */
 int i915_debugfs_init(struct drm_minor *minor);
 void i915_debugfs_cleanup(struct drm_minor *minor);
@@ -2595,6 +2915,7 @@
 int i915_error_state_to_str(struct drm_i915_error_state_buf *estr,
 			    const struct i915_error_state_file_priv *error);
 int i915_error_state_buf_init(struct drm_i915_error_state_buf *eb,
+			      struct drm_i915_private *i915,
 			      size_t count, loff_t pos);
 static inline void i915_error_state_buf_release(
 	struct drm_i915_error_state_buf *eb)
@@ -2609,18 +2930,27 @@
 void i915_destroy_error_state(struct drm_device *dev);
 
 void i915_get_extra_instdone(struct drm_device *dev, uint32_t *instdone);
-const char *i915_cache_level_str(int type);
+const char *i915_cache_level_str(struct drm_i915_private *i915, int type);
 
 /* i915_cmd_parser.c */
 int i915_cmd_parser_get_version(void);
-int i915_cmd_parser_init_ring(struct intel_engine_cs *ring);
-void i915_cmd_parser_fini_ring(struct intel_engine_cs *ring);
-bool i915_needs_cmd_parser(struct intel_engine_cs *ring);
-int i915_parse_cmds(struct intel_engine_cs *ring,
+int i915_cmd_parser_init_engine(struct intel_engine_cs *engine);
+void i915_cmd_parser_fini_engine(struct intel_engine_cs *engine);
+bool i915_needs_cmd_parser(struct intel_engine_cs *engine);
+int i915_parse_cmds(struct intel_engine_cs *engine,
 		    struct drm_i915_gem_object *batch_obj,
 		    u32 batch_start_offset,
 		    bool is_master);
 
+/* i915_perf.c */
+#ifdef CONFIG_PERF_EVENTS
+extern void i915_perf_register(struct drm_device *dev);
+extern void i915_perf_unregister(struct drm_device *dev);
+#else
+static inline void i915_perf_register(struct drm_device *dev) {}
+static inline void i915_perf_unregister(struct drm_device *dev) {}
+#endif
+
 /* i915_suspend.c */
 extern int i915_save_state(struct drm_device *dev);
 extern int i915_restore_state(struct drm_device *dev);
@@ -2652,7 +2982,6 @@
 extern void intel_i2c_reset(struct drm_device *dev);
 
 /* intel_opregion.c */
-struct intel_encoder;
 #ifdef CONFIG_ACPI
 extern int intel_opregion_setup(struct drm_device *dev);
 extern void intel_opregion_init(struct drm_device *dev);
@@ -2690,7 +3019,6 @@
 
 /* modesetting */
 extern void intel_modeset_init_hw(struct drm_device *dev);
-extern void intel_modeset_suspend_hw(struct drm_device *dev);
 extern void intel_modeset_init(struct drm_device *dev);
 extern void intel_modeset_gem_init(struct drm_device *dev);
 extern void intel_modeset_cleanup(struct drm_device *dev);
@@ -2701,6 +3029,7 @@
 extern void i915_redisable_vga(struct drm_device *dev);
 extern void i915_redisable_vga_power_on(struct drm_device *dev);
 extern bool intel_fbc_enabled(struct drm_device *dev);
+extern void bdw_fbc_sw_flush(struct drm_device *dev, u32 value);
 extern void intel_disable_fbc(struct drm_device *dev);
 extern bool ironlake_set_drps(struct drm_device *dev, u8 val);
 extern void intel_init_pch_refclk(struct drm_device *dev);
@@ -2712,14 +3041,11 @@
 extern int intel_trans_dp_port_sel(struct drm_crtc *crtc);
 extern int intel_enable_rc6(const struct drm_device *dev);
 
-extern bool i915_semaphore_is_enabled(struct drm_device *dev);
 int i915_reg_read_ioctl(struct drm_device *dev, void *data,
 			struct drm_file *file);
 int i915_get_reset_stats_ioctl(struct drm_device *dev, void *data,
 			       struct drm_file *file);
 
-void intel_notify_mmio_flip(struct intel_engine_cs *ring);
-
 /* overlay */
 extern struct intel_overlay_error_state *intel_overlay_capture_error_state(struct drm_device *dev);
 extern void intel_overlay_print_error_state(struct drm_i915_error_state_buf *e,
@@ -2734,12 +3060,14 @@
  * must be set to prevent GT core from power down and stale values being
  * returned.
  */
-void gen6_gt_force_wake_get(struct drm_i915_private *dev_priv, int fw_engine);
-void gen6_gt_force_wake_put(struct drm_i915_private *dev_priv, int fw_engine);
+void gen6_gt_force_wake_get(struct drm_i915_private *dev_priv,
+			    unsigned fw_domains);
+void gen6_gt_force_wake_put(struct drm_i915_private *dev_priv,
+			    unsigned fw_domains);
 void assert_force_wake_inactive(struct drm_i915_private *dev_priv);
 
-int sandybridge_pcode_read(struct drm_i915_private *dev_priv, u8 mbox, u32 *val);
-int sandybridge_pcode_write(struct drm_i915_private *dev_priv, u8 mbox, u32 val);
+int sandybridge_pcode_read(struct drm_i915_private *dev_priv, u32 mbox, u32 *val);
+int sandybridge_pcode_write(struct drm_i915_private *dev_priv, u32 mbox, u32 val);
 
 /* intel_sideband.c */
 u32 vlv_punit_read(struct drm_i915_private *dev_priv, u8 addr);
@@ -2767,11 +3095,6 @@
 int vlv_gpu_freq(struct drm_i915_private *dev_priv, int val);
 int vlv_freq_opcode(struct drm_i915_private *dev_priv, int val);
 
-#define FORCEWAKE_RENDER	(1 << 0)
-#define FORCEWAKE_MEDIA		(1 << 1)
-#define FORCEWAKE_ALL		(FORCEWAKE_RENDER | FORCEWAKE_MEDIA)
-
-
 #define I915_READ8(reg)		dev_priv->uncore.funcs.mmio_readb(dev_priv, (reg), true)
 #define I915_WRITE8(reg, val)	dev_priv->uncore.funcs.mmio_writeb(dev_priv, (reg), (val), true)
 
diff -urN a/drivers/gpu/drm/i915/i915_gem.c b/drivers/gpu/drm/i915/i915_gem.c
--- a/drivers/gpu/drm/i915/i915_gem.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_gem.c	2014-11-22 14:37:49.330700418 -0700
@@ -31,6 +31,7 @@
 #include "i915_drv.h"
 #include "i915_trace.h"
 #include "intel_drv.h"
+#include <linux/list_sort.h>
 #include <linux/oom.h>
 #include <linux/shmem_fs.h>
 #include <linux/slab.h>
@@ -38,14 +39,18 @@
 #include <linux/pci.h>
 #include <linux/dma-buf.h>
 
+#define RQ_BUG_ON(x) BUG_ON(x)
+
 static void i915_gem_object_flush_gtt_write_domain(struct drm_i915_gem_object *obj);
 static void i915_gem_object_flush_cpu_write_domain(struct drm_i915_gem_object *obj,
 						   bool force);
 static __must_check int
 i915_gem_object_wait_rendering(struct drm_i915_gem_object *obj,
 			       bool readonly);
-static void
-i915_gem_object_retire(struct drm_i915_gem_object *obj);
+static __must_check int
+i915_gem_object_wait_rendering__nonblocking(struct drm_i915_gem_object *obj,
+					    struct drm_i915_file_private *file_priv,
+					    bool readonly);
 
 static void i915_gem_write_fence(struct drm_device *dev, int reg,
 				 struct drm_i915_gem_object *obj);
@@ -60,7 +65,6 @@
 static int i915_gem_shrinker_oom(struct notifier_block *nb,
 				 unsigned long event,
 				 void *ptr);
-static unsigned long i915_gem_purge(struct drm_i915_private *dev_priv, long target);
 static unsigned long i915_gem_shrink_all(struct drm_i915_private *dev_priv);
 
 static bool cpu_cache_is_coherent(struct drm_device *dev,
@@ -108,23 +112,127 @@
 	spin_unlock(&dev_priv->mm.object_stat_lock);
 }
 
+static void
+i915_gem_object_retire__write(struct drm_i915_gem_object *obj)
+{
+	RQ_BUG_ON(obj->active == 0);
+	RQ_BUG_ON(obj->last_write.request == NULL);
+	RQ_BUG_ON(list_empty(&obj->last_write.engine_link));
+
+	intel_fb_obj_flush(obj, true);
+	list_del_init(&obj->last_write.engine_link);
+	i915_request_put(obj->last_write.request);
+	obj->last_write.request = NULL;
+}
+
+static void
+i915_gem_object_retire__fence(struct drm_i915_gem_object *obj)
+{
+	RQ_BUG_ON(obj->active == 0);
+	RQ_BUG_ON(obj->last_fence.request == NULL);
+	RQ_BUG_ON(list_empty(&obj->last_fence.engine_link));
+
+	list_del_init(&obj->last_fence.engine_link);
+	i915_request_put(obj->last_fence.request);
+	obj->last_fence.request = NULL;
+}
+
+static void
+i915_gem_object_retire__read(struct drm_i915_gem_object *obj,
+			     struct intel_engine_cs *engine)
+{
+	RQ_BUG_ON(obj->active == 0);
+	RQ_BUG_ON(obj->last_read[engine->id].request == NULL);
+	RQ_BUG_ON(list_empty(&obj->last_read[engine->id].engine_link));
+
+	list_del_init(&obj->last_read[engine->id].engine_link);
+	i915_request_put(obj->last_read[engine->id].request);
+	obj->last_read[engine->id].request = NULL;
+
+	if (obj->last_write.request &&
+	    obj->last_write.request->engine == engine)
+		i915_gem_object_retire__write(obj);
+
+	if (obj->last_fence.request &&
+	    obj->last_fence.request->engine == engine)
+		i915_gem_object_retire__fence(obj);
+
+	if (--obj->active)
+		return;
+
+	RQ_BUG_ON(obj->last_write.request);
+	RQ_BUG_ON(obj->last_fence.request);
+
+	drm_gem_object_unreference(&obj->base);
+}
+
+static void
+i915_vma_retire__read(struct i915_vma *vma, int engine)
+{
+	RQ_BUG_ON(vma->active == 0);
+	RQ_BUG_ON(vma->last_read[engine].request == NULL);
+	RQ_BUG_ON(list_empty(&vma->last_read[engine].engine_link));
+
+	list_del_init(&vma->last_read[engine].engine_link);
+	i915_request_put(vma->last_read[engine].request);
+	vma->last_read[engine].request = NULL;
+
+	if (--vma->active)
+		return;
+
+	if (drm_mm_node_allocated(&vma->node)) {
+		RQ_BUG_ON(list_empty(&vma->mm_list));
+		list_move_tail(&vma->mm_list, &vma->vm->inactive_list);
+		if (vma->vm->closed)
+			WARN_ON(i915_vma_unbind(vma));
+	}
+
+	drm_gem_object_unreference(&vma->obj->base);
+	i915_vma_put(vma);
+}
+
+static void
+i915_gem_object_retire(struct drm_i915_gem_object *obj)
+{
+	struct i915_gem_request *rq;
+	int i;
+
+	/* We should only be called from code paths where we know we
+	 * hold both the active reference *and* a user reference.
+	 * Therefore we can safely access the object after retiring as
+	 * we will hold a second reference and not free the object.
+	 */
+
+	rq = obj->last_write.request;
+	if (rq && i915_request_complete(rq))
+		i915_gem_object_retire__write(obj);
+
+	rq = obj->last_fence.request;
+	if (rq && i915_request_complete(rq))
+		i915_gem_object_retire__fence(obj);
+
+	for (i = 0; i < I915_NUM_ENGINES; i++) {
+		rq = obj->last_read[i].request;
+		if (rq && i915_request_complete(rq))
+			i915_gem_object_retire__read(obj, rq->engine);
+	}
+
+	if (!obj->active)
+		i915_gem_retire_requests(obj->base.dev);
+}
+
 static int
 i915_gem_wait_for_error(struct i915_gpu_error *error)
 {
 	int ret;
 
-#define EXIT_COND (!i915_reset_in_progress(error) || \
-		   i915_terminally_wedged(error))
-	if (EXIT_COND)
-		return 0;
-
 	/*
 	 * Only wait 10 seconds for the gpu reset to complete to avoid hanging
 	 * userspace. If it takes that long something really bad is going on and
 	 * we should simply try to bail out and fail as gracefully as possible.
 	 */
 	ret = wait_event_interruptible_timeout(error->reset_queue,
-					       EXIT_COND,
+					       !i915_recovery_pending(error),
 					       10*HZ);
 	if (ret == 0) {
 		DRM_ERROR("Timed out waiting for the gpu reset to complete\n");
@@ -132,7 +240,6 @@
 	} else if (ret < 0) {
 		return ret;
 	}
-#undef EXIT_COND
 
 	return 0;
 }
@@ -146,45 +253,56 @@
 	if (ret)
 		return ret;
 
-	ret = mutex_lock_interruptible(&dev->struct_mutex);
+	ret = mutex_lock_wrapper(&dev->struct_mutex,
+				 TASK_INTERRUPTIBLE,
+				 _RET_IP_);
 	if (ret)
 		return ret;
 
-	WARN_ON(i915_verify_lists(dev));
 	return 0;
 }
 
-static inline bool
-i915_gem_object_is_inactive(struct drm_i915_gem_object *obj)
+static int obj_rank_by_ggtt(void *priv,
+			    struct list_head *A,
+			    struct list_head *B)
 {
-	return i915_gem_obj_bound_any(obj) && !obj->active;
+	struct drm_i915_gem_object *a = list_entry(A,typeof(*a), obj_exec_link);
+	struct drm_i915_gem_object *b = list_entry(B,typeof(*b), obj_exec_link);
+
+	return i915_gem_obj_ggtt_offset(a) - i915_gem_obj_ggtt_offset(b);
 }
 
-int
-i915_gem_init_ioctl(struct drm_device *dev, void *data,
-		    struct drm_file *file)
+static u32 __fence_size(struct drm_i915_private *dev_priv, u32 start, u32 end)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_i915_gem_init *args = data;
+	u32 size = end - start;
+	u32 fence_size;
 
-	if (drm_core_check_feature(dev, DRIVER_MODESET))
-		return -ENODEV;
+	if (INTEL_INFO(dev_priv)->gen < 4) {
+		u32 fence_max;
+		u32 fence_next;
 
-	if (args->gtt_start >= args->gtt_end ||
-	    (args->gtt_end | args->gtt_start) & (PAGE_SIZE - 1))
-		return -EINVAL;
+		if (IS_GEN3(dev_priv)) {
+			fence_max = I830_FENCE_MAX_SIZE_VAL << 20;
+			fence_next = 1024*1024;
+		} else {
+			fence_max = I830_FENCE_MAX_SIZE_VAL << 19;
+			fence_next = 512*1024;
+		}
 
-	/* GEM with user mode setting was never supported on ilk and later. */
-	if (INTEL_INFO(dev)->gen >= 5)
-		return -ENODEV;
+		fence_max = min(fence_max, size);
+		fence_size = 0;
+		while (fence_next <= fence_max) {
+			u32 base = ALIGN(start, fence_next);
+			if (base + fence_next > end)
+				break;
 
-	mutex_lock(&dev->struct_mutex);
-	i915_gem_setup_global_gtt(dev, args->gtt_start, args->gtt_end,
-				  args->gtt_end);
-	dev_priv->gtt.mappable_end = args->gtt_end;
-	mutex_unlock(&dev->struct_mutex);
+			fence_size = fence_next;
+			fence_next <<= 1;
+		}
+	} else
+		fence_size = size;
 
-	return 0;
+	return fence_size;
 }
 
 int
@@ -194,55 +312,208 @@
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_i915_gem_get_aperture *args = data;
 	struct drm_i915_gem_object *obj;
-	size_t pinned;
+	struct list_head map_list;
+	const u32 map_limit = dev_priv->gtt.mappable_end;
+	size_t pinned, map_space, map_largest, fence_space, fence_largest;
+	u32 last, size;
+
+	INIT_LIST_HEAD(&map_list);
 
 	pinned = 0;
+	map_space = map_largest = 0;
+	fence_space = fence_largest = 0;
+
 	mutex_lock(&dev->struct_mutex);
-	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list)
-		if (i915_gem_obj_is_pinned(obj))
-			pinned += i915_gem_obj_ggtt_size(obj);
+	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list) {
+		struct i915_vma *vma = i915_gem_obj_to_ggtt(obj);
+
+		if (vma == NULL || !vma->pin_count)
+			continue;
+
+		pinned += vma->node.size;
+
+		if (vma->node.start < map_limit)
+			list_add(&obj->obj_exec_link, &map_list);
+	}
+
+	last = 0;
+	list_sort(NULL, &map_list, obj_rank_by_ggtt);
+	while (!list_empty(&map_list)) {
+		struct i915_vma *vma;
+
+		obj = list_first_entry(&map_list, typeof(*obj), obj_exec_link);
+		list_del_init(&obj->obj_exec_link);
+
+		vma = i915_gem_obj_to_ggtt(obj);
+		if (last == 0)
+			goto skip_first;
+
+		size = vma->node.start - last;
+		if (size > map_largest)
+			map_largest = size;
+		map_space += size;
+
+		size = __fence_size(dev_priv, last, vma->node.start);
+		if (size > fence_largest)
+			fence_largest = size;
+		fence_space += size;
+
+skip_first:
+		last = vma->node.start + vma->node.size;
+	}
+	if (last < map_limit) {
+		size = map_limit - last;
+		if (size > map_largest)
+			map_largest = size;
+		map_space += size;
+
+		size = __fence_size(dev_priv, last, map_limit);
+		if (size > fence_largest)
+			fence_largest = size;
+		fence_space += size;
+	}
 	mutex_unlock(&dev->struct_mutex);
 
 	args->aper_size = dev_priv->gtt.base.total;
 	args->aper_available_size = args->aper_size - pinned;
+	args->map_available_size = map_space;
+	args->map_largest_size = map_largest;
+	args->map_total_size = dev_priv->gtt.mappable_end;
+	args->fence_available_size = fence_space;
+	args->fence_largest_size = fence_largest;
 
 	return 0;
 }
 
-static void i915_gem_object_detach_phys(struct drm_i915_gem_object *obj)
+static int
+i915_gem_object_get_pages_phys(struct drm_i915_gem_object *obj)
 {
-	drm_dma_handle_t *phys = obj->phys_handle;
+	struct address_space *mapping = file_inode(obj->base.filp)->i_mapping;
+	char *vaddr = obj->phys_handle->vaddr;
+	struct sg_table *st;
+	struct scatterlist *sg;
+	int i;
 
-	if (!phys)
-		return;
+	if (WARN_ON(i915_gem_object_needs_bit17_swizzle(obj)))
+		return -EINVAL;
+
+	for (i = 0; i < obj->base.size / PAGE_SIZE; i++) {
+		struct page *page;
+		char *src;
+
+		page = shmem_read_mapping_page(mapping, i);
+		if (IS_ERR(page))
+			return PTR_ERR(page);
+
+		src = kmap_atomic(page);
+		memcpy(vaddr, src, PAGE_SIZE);
+		drm_clflush_virt_range(vaddr, PAGE_SIZE);
+		kunmap_atomic(src);
+
+		page_cache_release(page);
+		vaddr += PAGE_SIZE;
+	}
+
+	i915_gem_chipset_flush(obj->base.dev);
+
+	st = kmalloc(sizeof(*st), GFP_KERNEL);
+	if (st == NULL)
+		return -ENOMEM;
+
+	if (sg_alloc_table(st, 1, GFP_KERNEL)) {
+		kfree(st);
+		return -ENOMEM;
+	}
+
+	sg = st->sgl;
+	sg->offset = 0;
+	sg->length = obj->base.size;
+
+	sg_dma_address(sg) = obj->phys_handle->busaddr;
+	sg_dma_len(sg) = obj->base.size;
+
+	obj->pages = st;
+	obj->has_dma_mapping = true;
+	return 0;
+}
+
+static void
+i915_gem_object_put_pages_phys(struct drm_i915_gem_object *obj)
+{
+	int ret;
+
+	ret = i915_gem_object_set_to_cpu_domain(obj, true);
+	if (ret) {
+		/* In the event of a disaster, abandon all caches and
+		 * hope for the best.
+		 */
+		WARN_ON(ret != -EIO);
+		obj->base.read_domains = obj->base.write_domain = I915_GEM_DOMAIN_CPU;
+	}
+
+	if (obj->madv == I915_MADV_DONTNEED)
+		obj->dirty = 0;
 
-	if (obj->madv == I915_MADV_WILLNEED) {
+	if (obj->dirty) {
 		struct address_space *mapping = file_inode(obj->base.filp)->i_mapping;
-		char *vaddr = phys->vaddr;
+		char *vaddr = obj->phys_handle->vaddr;
 		int i;
 
 		for (i = 0; i < obj->base.size / PAGE_SIZE; i++) {
-			struct page *page = shmem_read_mapping_page(mapping, i);
-			if (!IS_ERR(page)) {
-				char *dst = kmap_atomic(page);
-				memcpy(dst, vaddr, PAGE_SIZE);
-				drm_clflush_virt_range(dst, PAGE_SIZE);
-				kunmap_atomic(dst);
+			struct page *page;
+			char *dst;
 
-				set_page_dirty(page);
+			page = shmem_read_mapping_page(mapping, i);
+			if (IS_ERR(page))
+				continue;
+
+			dst = kmap_atomic(page);
+			drm_clflush_virt_range(vaddr, PAGE_SIZE);
+			memcpy(dst, vaddr, PAGE_SIZE);
+			kunmap_atomic(dst);
+
+			set_page_dirty(page);
+			if (obj->madv == I915_MADV_WILLNEED)
 				mark_page_accessed(page);
-				page_cache_release(page);
-			}
+			page_cache_release(page);
 			vaddr += PAGE_SIZE;
 		}
-		i915_gem_chipset_flush(obj->base.dev);
+		obj->dirty = 0;
 	}
 
-#ifdef CONFIG_X86
-	set_memory_wb((unsigned long)phys->vaddr, phys->size / PAGE_SIZE);
-#endif
-	drm_pci_free(obj->base.dev, phys);
-	obj->phys_handle = NULL;
+	sg_free_table(obj->pages);
+	kfree(obj->pages);
+
+	obj->has_dma_mapping = false;
+}
+
+static void
+i915_gem_object_release_phys(struct drm_i915_gem_object *obj)
+{
+	drm_pci_free(obj->base.dev, obj->phys_handle);
+}
+
+static const struct drm_i915_gem_object_ops i915_gem_phys_ops = {
+	.get_pages = i915_gem_object_get_pages_phys,
+	.put_pages = i915_gem_object_put_pages_phys,
+	.release = i915_gem_object_release_phys,
+};
+
+static int
+drop_pages(struct drm_i915_gem_object *obj)
+{
+	struct i915_vma *vma, *next;
+	int ret;
+
+	drm_gem_object_reference(&obj->base);
+	list_for_each_entry_safe(vma, next, &obj->vma_list, obj_link)
+		if (i915_vma_unbind(vma))
+			break;
+
+	ret = i915_gem_object_put_pages(obj);
+	drm_gem_object_unreference(&obj->base);
+
+	return ret;
 }
 
 int
@@ -250,9 +521,7 @@
 			    int align)
 {
 	drm_dma_handle_t *phys;
-	struct address_space *mapping;
-	char *vaddr;
-	int i;
+	int ret;
 
 	if (obj->phys_handle) {
 		if ((unsigned long)obj->phys_handle->vaddr & (align -1))
@@ -267,41 +536,19 @@
 	if (obj->base.filp == NULL)
 		return -EINVAL;
 
+	ret = drop_pages(obj);
+	if (ret)
+		return ret;
+
 	/* create a new object */
 	phys = drm_pci_alloc(obj->base.dev, obj->base.size, align);
 	if (!phys)
 		return -ENOMEM;
 
-	vaddr = phys->vaddr;
-#ifdef CONFIG_X86
-	set_memory_wc((unsigned long)vaddr, phys->size / PAGE_SIZE);
-#endif
-	mapping = file_inode(obj->base.filp)->i_mapping;
-	for (i = 0; i < obj->base.size / PAGE_SIZE; i++) {
-		struct page *page;
-		char *src;
-
-		page = shmem_read_mapping_page(mapping, i);
-		if (IS_ERR(page)) {
-#ifdef CONFIG_X86
-			set_memory_wb((unsigned long)phys->vaddr, phys->size / PAGE_SIZE);
-#endif
-			drm_pci_free(obj->base.dev, phys);
-			return PTR_ERR(page);
-		}
-
-		src = kmap_atomic(page);
-		memcpy(vaddr, src, PAGE_SIZE);
-		kunmap_atomic(src);
-
-		mark_page_accessed(page);
-		page_cache_release(page);
-
-		vaddr += PAGE_SIZE;
-	}
-
 	obj->phys_handle = phys;
-	return 0;
+	obj->ops = &i915_gem_phys_ops;
+
+	return i915_gem_object_get_pages(obj);
 }
 
 static int
@@ -312,6 +559,14 @@
 	struct drm_device *dev = obj->base.dev;
 	void *vaddr = obj->phys_handle->vaddr + args->offset;
 	char __user *user_data = to_user_ptr(args->data_ptr);
+	int ret;
+
+	/* We manually control the domain here and pretend that it
+	 * remains coherent i.e. in the GTT domain, like shmem_pwrite.
+	 */
+	ret = i915_gem_object_wait_rendering(obj, false);
+	if (ret)
+		return ret;
 
 	if (__copy_from_user_inatomic_nocache(vaddr, user_data, args->size)) {
 		unsigned long unwritten;
@@ -327,6 +582,7 @@
 			return -EFAULT;
 	}
 
+	drm_clflush_virt_range(vaddr, args->size);
 	i915_gem_chipset_flush(dev);
 	return 0;
 }
@@ -474,8 +730,6 @@
 		ret = i915_gem_object_wait_rendering(obj, true);
 		if (ret)
 			return ret;
-
-		i915_gem_object_retire(obj);
 	}
 
 	ret = i915_gem_object_get_pages(obj);
@@ -618,7 +872,7 @@
 
 		mutex_unlock(&dev->struct_mutex);
 
-		if (likely(!i915.prefault_disable) && !prefaulted) {
+		if (likely(!i915_module.prefault_disable) && !prefaulted) {
 			ret = fault_in_multipages_writeable(user_data, remain);
 			/* Userspace is tricking us, but we've already clobbered
 			 * its pages with the prefault and promised to write the
@@ -695,6 +949,12 @@
 		goto out;
 	}
 
+	ret = i915_gem_object_wait_rendering__nonblocking(obj,
+							  file->driver_priv,
+							  false);
+	if (ret)
+		goto out;
+
 	trace_i915_gem_object_pread(obj, args->offset, args->size);
 
 	ret = i915_gem_shmem_pread(dev, obj, args, file);
@@ -745,7 +1005,7 @@
 	char __user *user_data;
 	int page_offset, page_length, ret;
 
-	ret = i915_gem_obj_ggtt_pin(obj, 0, PIN_MAPPABLE | PIN_NONBLOCK);
+	ret = i915_gem_object_ggtt_pin(obj, 0, PIN_MAPPABLE | PIN_NONBLOCK);
 	if (ret)
 		goto out;
 
@@ -877,6 +1137,14 @@
 	int needs_clflush_before = 0;
 	struct sg_page_iter sg_iter;
 
+	/* prime objects have no backing filp to GEM pread/pwrite
+	 * pages from.
+	 */
+	if (!obj->base.filp) {
+		ret = -EINVAL;
+		goto out;
+	}
+
 	user_data = to_user_ptr(args->data_ptr);
 	remain = args->size;
 
@@ -891,8 +1159,6 @@
 		ret = i915_gem_object_wait_rendering(obj, false);
 		if (ret)
 			return ret;
-
-		i915_gem_object_retire(obj);
 	}
 	/* Same trick applies to invalidate partially written cachelines read
 	 * before writing. */
@@ -1006,7 +1272,7 @@
 		       args->size))
 		return -EFAULT;
 
-	if (likely(!i915.prefault_disable)) {
+	if (likely(!i915_module.prefault_disable)) {
 		ret = fault_in_multipages_readable(to_user_ptr(args->data_ptr),
 						   args->size);
 		if (ret)
@@ -1030,13 +1296,11 @@
 		goto out;
 	}
 
-	/* prime objects have no backing filp to GEM pread/pwrite
-	 * pages from.
-	 */
-	if (!obj->base.filp) {
-		ret = -EINVAL;
+	ret = i915_gem_object_wait_rendering__nonblocking(obj,
+							  file->driver_priv,
+							  true);
+	if (ret)
 		goto out;
-	}
 
 	trace_i915_gem_object_pwrite(obj, args->offset, args->size);
 
@@ -1047,22 +1311,22 @@
 	 * pread/pwrite currently are reading and writing from the CPU
 	 * perspective, requiring manual detiling by the client.
 	 */
-	if (obj->phys_handle) {
-		ret = i915_gem_phys_pwrite(obj, args, file);
-		goto out;
-	}
-
 	if (obj->tiling_mode == I915_TILING_NONE &&
-	    obj->base.write_domain != I915_GEM_DOMAIN_CPU &&
-	    cpu_write_needs_clflush(obj)) {
+	    (obj->base.filp == NULL ||
+	     (obj->base.write_domain != I915_GEM_DOMAIN_CPU &&
+	      cpu_write_needs_clflush(obj)))) {
 		ret = i915_gem_gtt_pwrite_fast(dev, obj, args, file);
 		/* Note that the gtt paths might fail with non-page-backed user
 		 * pointers (e.g. gtt mappings when moving data between
 		 * textures). Fallback to the shmem path in that case. */
 	}
 
-	if (ret == -EFAULT || ret == -ENOSPC)
-		ret = i915_gem_shmem_pwrite(dev, obj, args, file);
+	if (ret == -EFAULT || ret == -ENOSPC) {
+		if (obj->phys_handle)
+			ret = i915_gem_phys_pwrite(obj, args, file);
+		else
+			ret = i915_gem_shmem_pwrite(dev, obj, args, file);
+	}
 
 out:
 	drm_gem_object_unreference(&obj->base);
@@ -1071,293 +1335,98 @@
 	return ret;
 }
 
-int
-i915_gem_check_wedge(struct i915_gpu_error *error,
-		     bool interruptible)
+/**
+ * Ensures that all rendering to the object has completed and the object is
+ * safe to unbind from the GTT or access from the CPU.
+ */
+static __must_check int
+i915_gem_object_wait_rendering(struct drm_i915_gem_object *obj,
+			       bool readonly)
 {
-	if (i915_reset_in_progress(error)) {
-		/* Non-interruptible callers can't handle -EAGAIN, hence return
-		 * -EIO unconditionally for these. */
-		if (!interruptible)
-			return -EIO;
+	int i, ret;
 
-		/* Recovery complete, but the reset failed ... */
-		if (i915_terminally_wedged(error))
-			return -EIO;
+	if (!obj->active)
+		return 0;
+
+	if (readonly) {
+		if (obj->last_write.request) {
+			ret = i915_request_wait(obj->last_write.request);
+			if (ret)
+				return ret;
+		}
+	} else {
+		for (i = 0; i < I915_NUM_ENGINES; i++) {
+			if (obj->last_read[i].request == NULL)
+				continue;
 
-		return -EAGAIN;
+			ret = i915_request_wait(obj->last_read[i].request);
+			if (ret)
+				return ret;
+		}
 	}
 
+	i915_gem_object_retire(obj);
 	return 0;
 }
 
-/*
- * Compare seqno against outstanding lazy request. Emit a request if they are
- * equal.
+/* A nonblocking variant of the above wait. This is a highly dangerous routine
+ * as the object state may change during this call.
  */
-int
-i915_gem_check_olr(struct intel_engine_cs *ring, u32 seqno)
+static __must_check int
+i915_gem_object_wait_rendering__nonblocking(struct drm_i915_gem_object *obj,
+					    struct drm_i915_file_private *file_priv,
+					    bool readonly)
 {
-	int ret;
-
-	BUG_ON(!mutex_is_locked(&ring->dev->struct_mutex));
-
-	ret = 0;
-	if (seqno == ring->outstanding_lazy_seqno)
-		ret = i915_add_request(ring, NULL);
-
-	return ret;
-}
-
-static void fake_irq(unsigned long data)
-{
-	wake_up_process((struct task_struct *)data);
-}
-
-static bool missed_irq(struct drm_i915_private *dev_priv,
-		       struct intel_engine_cs *ring)
-{
-	return test_bit(ring->id, &dev_priv->gpu_error.missed_irq_rings);
-}
-
-static bool can_wait_boost(struct drm_i915_file_private *file_priv)
-{
-	if (file_priv == NULL)
-		return true;
-
-	return !atomic_xchg(&file_priv->rps_wait_boost, true);
-}
-
-/**
- * __wait_seqno - wait until execution of seqno has finished
- * @ring: the ring expected to report seqno
- * @seqno: duh!
- * @reset_counter: reset sequence associated with the given seqno
- * @interruptible: do an interruptible wait (normally yes)
- * @timeout: in - how long to wait (NULL forever); out - how much time remaining
- *
- * Note: It is of utmost importance that the passed in seqno and reset_counter
- * values have been read by the caller in an smp safe manner. Where read-side
- * locks are involved, it is sufficient to read the reset_counter before
- * unlocking the lock that protects the seqno. For lockless tricks, the
- * reset_counter _must_ be read before, and an appropriate smp_rmb must be
- * inserted.
- *
- * Returns 0 if the seqno was found within the alloted time. Else returns the
- * errno with remaining time filled in timeout argument.
- */
-static int __wait_seqno(struct intel_engine_cs *ring, u32 seqno,
-			unsigned reset_counter,
-			bool interruptible,
-			s64 *timeout,
-			struct drm_i915_file_private *file_priv)
-{
-	struct drm_device *dev = ring->dev;
+	struct drm_device *dev = obj->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	const bool irq_test_in_progress =
-		ACCESS_ONCE(dev_priv->gpu_error.test_irq_rings) & intel_ring_flag(ring);
-	DEFINE_WAIT(wait);
-	unsigned long timeout_expire;
-	s64 before, now;
-	int ret;
-
-	WARN(!intel_irqs_enabled(dev_priv), "IRQs disabled");
-
-	if (i915_seqno_passed(ring->get_seqno(ring, true), seqno))
-		return 0;
+	struct i915_gem_request *rq[I915_NUM_ENGINES] = {};
+	int i, n, ret;
 
-	timeout_expire = timeout ? jiffies + nsecs_to_jiffies((u64)*timeout) : 0;
-
-	if (INTEL_INFO(dev)->gen >= 6 && ring->id == RCS && can_wait_boost(file_priv)) {
-		gen6_rps_boost(dev_priv);
-		if (file_priv)
-			mod_delayed_work(dev_priv->wq,
-					 &file_priv->mm.idle_work,
-					 msecs_to_jiffies(100));
-	}
-
-	if (!irq_test_in_progress && WARN_ON(!ring->irq_get(ring)))
-		return -ENODEV;
+	BUG_ON(!mutex_is_locked(&dev->struct_mutex));
+	BUG_ON(!dev_priv->mm.interruptible);
 
-	/* Record current time in case interrupted by signal, or wedged */
-	trace_i915_gem_request_wait_begin(ring, seqno);
-	before = ktime_get_raw_ns();
-	for (;;) {
-		struct timer_list timer;
-
-		prepare_to_wait(&ring->irq_queue, &wait,
-				interruptible ? TASK_INTERRUPTIBLE : TASK_UNINTERRUPTIBLE);
-
-		/* We need to check whether any gpu reset happened in between
-		 * the caller grabbing the seqno and now ... */
-		if (reset_counter != atomic_read(&dev_priv->gpu_error.reset_counter)) {
-			/* ... but upgrade the -EAGAIN to an -EIO if the gpu
-			 * is truely gone. */
-			ret = i915_gem_check_wedge(&dev_priv->gpu_error, interruptible);
-			if (ret == 0)
-				ret = -EAGAIN;
-			break;
+	n = 0;
+	if (readonly) {
+		if (obj->last_write.request) {
+			rq[n] = i915_request_get_breadcrumb(obj->last_write.request);
+			if (IS_ERR(rq[n]))
+				return PTR_ERR(rq[n]);
+			n++;
 		}
-
-		if (i915_seqno_passed(ring->get_seqno(ring, false), seqno)) {
-			ret = 0;
-			break;
+	} else {
+		for (i = 0; i < I915_NUM_ENGINES; i++) {
+			if (obj->last_read[i].request == NULL)
+				continue;
+
+			rq[n] = i915_request_get_breadcrumb(obj->last_read[i].request);
+			if (IS_ERR(rq[n])) {
+				ret = PTR_ERR(rq[n]);
+				goto out;
+			}
+			n++;
 		}
+	}
+	if (n == 0)
+		return 0;
 
-		if (interruptible && signal_pending(current)) {
-			ret = -ERESTARTSYS;
-			break;
-		}
+	mutex_unlock(&dev->struct_mutex);
 
-		if (timeout && time_after_eq(jiffies, timeout_expire)) {
-			ret = -ETIME;
+	for (i = 0; i < n; i++) {
+		ret = __i915_request_wait(rq[i], true, NULL, file_priv);
+		if (ret)
 			break;
-		}
-
-		timer.function = NULL;
-		if (timeout || missed_irq(dev_priv, ring)) {
-			unsigned long expire;
-
-			setup_timer_on_stack(&timer, fake_irq, (unsigned long)current);
-			expire = missed_irq(dev_priv, ring) ? jiffies + 1 : timeout_expire;
-			mod_timer(&timer, expire);
-		}
-
-		io_schedule();
-
-		if (timer.function) {
-			del_singleshot_timer_sync(&timer);
-			destroy_timer_on_stack(&timer);
-		}
 	}
-	now = ktime_get_raw_ns();
-	trace_i915_gem_request_wait_end(ring, seqno);
-
-	if (!irq_test_in_progress)
-		ring->irq_put(ring);
 
-	finish_wait(&ring->irq_queue, &wait);
-
-	if (timeout) {
-		s64 tres = *timeout - (now - before);
+	mutex_lock(&dev->struct_mutex);
 
-		*timeout = tres < 0 ? 0 : tres;
-	}
+out:
+	for (i = 0; i < n; i++)
+		i915_request_put(rq[i]);
 
 	return ret;
 }
 
 /**
- * Waits for a sequence number to be signaled, and cleans up the
- * request and object lists appropriately for that event.
- */
-int
-i915_wait_seqno(struct intel_engine_cs *ring, uint32_t seqno)
-{
-	struct drm_device *dev = ring->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	bool interruptible = dev_priv->mm.interruptible;
-	int ret;
-
-	BUG_ON(!mutex_is_locked(&dev->struct_mutex));
-	BUG_ON(seqno == 0);
-
-	ret = i915_gem_check_wedge(&dev_priv->gpu_error, interruptible);
-	if (ret)
-		return ret;
-
-	ret = i915_gem_check_olr(ring, seqno);
-	if (ret)
-		return ret;
-
-	return __wait_seqno(ring, seqno,
-			    atomic_read(&dev_priv->gpu_error.reset_counter),
-			    interruptible, NULL, NULL);
-}
-
-static int
-i915_gem_object_wait_rendering__tail(struct drm_i915_gem_object *obj,
-				     struct intel_engine_cs *ring)
-{
-	if (!obj->active)
-		return 0;
-
-	/* Manually manage the write flush as we may have not yet
-	 * retired the buffer.
-	 *
-	 * Note that the last_write_seqno is always the earlier of
-	 * the two (read/write) seqno, so if we haved successfully waited,
-	 * we know we have passed the last write.
-	 */
-	obj->last_write_seqno = 0;
-
-	return 0;
-}
-
-/**
- * Ensures that all rendering to the object has completed and the object is
- * safe to unbind from the GTT or access from the CPU.
- */
-static __must_check int
-i915_gem_object_wait_rendering(struct drm_i915_gem_object *obj,
-			       bool readonly)
-{
-	struct intel_engine_cs *ring = obj->ring;
-	u32 seqno;
-	int ret;
-
-	seqno = readonly ? obj->last_write_seqno : obj->last_read_seqno;
-	if (seqno == 0)
-		return 0;
-
-	ret = i915_wait_seqno(ring, seqno);
-	if (ret)
-		return ret;
-
-	return i915_gem_object_wait_rendering__tail(obj, ring);
-}
-
-/* A nonblocking variant of the above wait. This is a highly dangerous routine
- * as the object state may change during this call.
- */
-static __must_check int
-i915_gem_object_wait_rendering__nonblocking(struct drm_i915_gem_object *obj,
-					    struct drm_i915_file_private *file_priv,
-					    bool readonly)
-{
-	struct drm_device *dev = obj->base.dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring = obj->ring;
-	unsigned reset_counter;
-	u32 seqno;
-	int ret;
-
-	BUG_ON(!mutex_is_locked(&dev->struct_mutex));
-	BUG_ON(!dev_priv->mm.interruptible);
-
-	seqno = readonly ? obj->last_write_seqno : obj->last_read_seqno;
-	if (seqno == 0)
-		return 0;
-
-	ret = i915_gem_check_wedge(&dev_priv->gpu_error, true);
-	if (ret)
-		return ret;
-
-	ret = i915_gem_check_olr(ring, seqno);
-	if (ret)
-		return ret;
-
-	reset_counter = atomic_read(&dev_priv->gpu_error.reset_counter);
-	mutex_unlock(&dev->struct_mutex);
-	ret = __wait_seqno(ring, seqno, reset_counter, true, NULL, file_priv);
-	mutex_lock(&dev->struct_mutex);
-	if (ret)
-		return ret;
-
-	return i915_gem_object_wait_rendering__tail(obj, ring);
-}
-
-/**
  * Called when user space prepares to use an object with the CPU, either
  * through the mmap ioctl's mapping or a GTT mapping.
  */
@@ -1404,18 +1473,10 @@
 	if (ret)
 		goto unref;
 
-	if (read_domains & I915_GEM_DOMAIN_GTT) {
+	if (read_domains & I915_GEM_DOMAIN_GTT)
 		ret = i915_gem_object_set_to_gtt_domain(obj, write_domain != 0);
-
-		/* Silently promote "you're not bound, there was nothing to do"
-		 * to success, since the client was just asking us to
-		 * make sure everything was done.
-		 */
-		if (ret == -EINVAL)
-			ret = 0;
-	} else {
+	else
 		ret = i915_gem_object_set_to_cpu_domain(obj, write_domain != 0);
-	}
 
 unref:
 	drm_gem_object_unreference(&obj->base);
@@ -1461,6 +1522,16 @@
  *
  * While the mapping holds a reference on the contents of the object, it doesn't
  * imply a ref on the object itself.
+ *
+ * IMPORTANT:
+ *
+ * DRM driver writers who look a this function as an example for how to do GEM
+ * mmap support, please don't implement mmap support like here. The modern way
+ * to implement DRM mmap support is with an mmap offset ioctl (like
+ * i915_gem_mmap_gtt) and then using the mmap syscall on the DRM fd directly.
+ * That way debug tooling like valgrind will understand what's going on, hiding
+ * the mmap call in a driver private ioctl will break that. The i915 driver only
+ * does cpu mmaps this way because we didn't know better.
  */
 int
 i915_gem_mmap_ioctl(struct drm_device *dev, void *data,
@@ -1470,6 +1541,12 @@
 	struct drm_gem_object *obj;
 	unsigned long addr;
 
+	if (args->flags & ~(I915_MMAP_WC))
+		return -EINVAL;
+
+	if (args->flags & I915_MMAP_WC && !cpu_has_pat)
+		return -ENODEV;
+
 	obj = drm_gem_object_lookup(dev, file, args->handle);
 	if (obj == NULL)
 		return -ENOENT;
@@ -1485,6 +1562,19 @@
 	addr = vm_mmap(obj->filp, 0, args->size,
 		       PROT_READ | PROT_WRITE, MAP_SHARED,
 		       args->offset);
+	if (args->flags & I915_MMAP_WC) {
+		struct mm_struct *mm = current->mm;
+		struct vm_area_struct *vma;
+
+		down_write(&mm->mmap_sem);
+		vma = find_vma(mm, addr);
+		if (vma)
+			vma->vm_page_prot =
+				pgprot_writecombine(vm_get_page_prot(vma->vm_flags));
+		else
+			addr = -ENOMEM;
+		up_write(&mm->mmap_sem);
+	}
 	drm_gem_object_unreference_unlocked(obj);
 	if (IS_ERR((void *)addr))
 		return addr;
@@ -1548,7 +1638,7 @@
 	}
 
 	/* Now bind it into the GTT if needed */
-	ret = i915_gem_obj_ggtt_pin(obj, 0, PIN_MAPPABLE);
+	ret = i915_gem_object_ggtt_pin(obj, 0, PIN_MAPPABLE);
 	if (ret)
 		goto unlock;
 
@@ -1564,51 +1654,35 @@
 	pfn = dev_priv->gtt.mappable_base + i915_gem_obj_ggtt_offset(obj);
 	pfn >>= PAGE_SHIFT;
 
-	if (!obj->fault_mappable) {
-		unsigned long size = min_t(unsigned long,
-					   vma->vm_end - vma->vm_start,
-					   obj->base.size);
-		int i;
+	ret = remap_io_mapping(vma,
+			       vma->vm_start, pfn, vma->vm_end - vma->vm_start,
+			       dev_priv->gtt.mappable);
+	if (ret)
+		goto unpin;
 
-		for (i = 0; i < size >> PAGE_SHIFT; i++) {
-			ret = vm_insert_pfn(vma,
-					    (unsigned long)vma->vm_start + i * PAGE_SIZE,
-					    pfn + i);
-			if (ret)
-				break;
-		}
+	obj->fault_mappable = true;
 
-		obj->fault_mappable = true;
-	} else
-		ret = vm_insert_pfn(vma,
-				    (unsigned long)vmf->virtual_address,
-				    pfn + page_offset);
 unpin:
 	i915_gem_object_ggtt_unpin(obj);
 unlock:
 	mutex_unlock(&dev->struct_mutex);
 out:
 	switch (ret) {
-	case -EIO:
+	case 0:
 		/*
-		 * We eat errors when the gpu is terminally wedged to avoid
-		 * userspace unduly crashing (gl has no provisions for mmaps to
-		 * fail). But any other -EIO isn't ours (e.g. swap in failure)
-		 * and so needs to be reported.
+		 * Success. You may proceed.
 		 */
-		if (!i915_terminally_wedged(&dev_priv->gpu_error)) {
-			ret = VM_FAULT_SIGBUS;
-			break;
-		}
 	case -EAGAIN:
 		/*
 		 * EAGAIN means the gpu is hung and we'll wait for the error
 		 * handler to reset everything when re-faulting in
 		 * i915_mutex_lock_interruptible.
 		 */
-	case 0:
 	case -ERESTARTSYS:
 	case -EINTR:
+		/* Signal interruptus. Return to userspace and await
+		 * further instructions.
+		 */
 	case -EBUSY:
 		/*
 		 * EBUSY is ok: this just means that another thread
@@ -1619,6 +1693,20 @@
 	case -ENOMEM:
 		ret = VM_FAULT_OOM;
 		break;
+	case -EIO:
+		/* The driver should never report an EIO here due to GPU hangs,
+		 * all requests shold have been reset and the object should
+		 * be idle. There should be no impediment for us to mmap the
+		 * object in those cases. It would be nice if we could throw
+		 * a warning in such cases, but...
+		 *
+		 * EIO can also arise from a failure to swap in pages,
+		 * for example. Here, we must report SIGBUS as there is
+		 * no recovery for memory corruption.
+		 *
+		 * And since we can have a wedged GPU and swap failure,
+		 * having a WARN here could be misleading.
+		 */
 	case -ENOSPC:
 	case -EFAULT:
 		ret = VM_FAULT_SIGBUS;
@@ -1735,7 +1823,11 @@
 	 * offsets on purgeable objects by truncating it and marking it purged,
 	 * which prevents userspace from ever using that object again.
 	 */
-	i915_gem_purge(dev_priv, obj->base.size >> PAGE_SHIFT);
+	i915_gem_shrink(dev_priv,
+			obj->base.size >> PAGE_SHIFT,
+			I915_SHRINK_BOUND |
+			I915_SHRINK_UNBOUND |
+			I915_SHRINK_PURGEABLE);
 	ret = drm_gem_create_mmap_offset(&obj->base);
 	if (ret != -ENOSPC)
 		goto out;
@@ -1871,8 +1963,6 @@
 	struct sg_page_iter sg_iter;
 	int ret;
 
-	BUG_ON(obj->madv == __I915_MADV_PURGED);
-
 	ret = i915_gem_object_set_to_cpu_domain(obj, true);
 	if (ret) {
 		/* In the event of a disaster, abandon all caches and
@@ -1918,6 +2008,7 @@
 		return -EBUSY;
 
 	BUG_ON(i915_gem_obj_bound_any(obj));
+	BUG_ON(obj->madv == __I915_MADV_PURGED);
 
 	/* ->put_pages might need to allocate memory for the bit17 swizzle
 	 * array, hence protect them from being reaped by removing them from gtt
@@ -1932,12 +2023,18 @@
 	return 0;
 }
 
-static unsigned long
-__i915_gem_shrink(struct drm_i915_private *dev_priv, long target,
-		  bool purgeable_only)
-{
-	struct list_head still_in_list;
-	struct drm_i915_gem_object *obj;
+unsigned long
+i915_gem_shrink(struct drm_i915_private *dev_priv,
+		long target, unsigned flags)
+{
+	const struct {
+		struct list_head *list;
+		unsigned int bit;
+	} phases[] = {
+		{ &dev_priv->mm.unbound_list, I915_SHRINK_UNBOUND },
+		{ &dev_priv->mm.bound_list, I915_SHRINK_BOUND },
+		{ NULL, 0 },
+	}, *phase;
 	unsigned long count = 0;
 
 	/*
@@ -1959,62 +2056,56 @@
 	 * dev->struct_mutex and so we won't ever be able to observe an
 	 * object on the bound_list with a reference count equals 0.
 	 */
-	INIT_LIST_HEAD(&still_in_list);
-	while (count < target && !list_empty(&dev_priv->mm.unbound_list)) {
-		obj = list_first_entry(&dev_priv->mm.unbound_list,
-				       typeof(*obj), global_list);
-		list_move_tail(&obj->global_list, &still_in_list);
-
-		if (!i915_gem_object_is_purgeable(obj) && purgeable_only)
-			continue;
-
-		drm_gem_object_reference(&obj->base);
+	for (phase = phases; phase->list; phase++) {
+		struct list_head still_in_list;
 
-		if (i915_gem_object_put_pages(obj) == 0)
-			count += obj->base.size >> PAGE_SHIFT;
-
-		drm_gem_object_unreference(&obj->base);
-	}
-	list_splice(&still_in_list, &dev_priv->mm.unbound_list);
-
-	INIT_LIST_HEAD(&still_in_list);
-	while (count < target && !list_empty(&dev_priv->mm.bound_list)) {
-		struct i915_vma *vma, *v;
-
-		obj = list_first_entry(&dev_priv->mm.bound_list,
-				       typeof(*obj), global_list);
-		list_move_tail(&obj->global_list, &still_in_list);
-
-		if (!i915_gem_object_is_purgeable(obj) && purgeable_only)
+		if ((flags & phase->bit) == 0)
 			continue;
 
-		drm_gem_object_reference(&obj->base);
-
-		list_for_each_entry_safe(vma, v, &obj->vma_list, vma_link)
-			if (i915_vma_unbind(vma))
-				break;
+		INIT_LIST_HEAD(&still_in_list);
+		while (count < target && !list_empty(phase->list)) {
+			struct list_head vma_list;
+			struct drm_i915_gem_object *obj;
+
+			obj = list_first_entry(phase->list,
+					       typeof(*obj), global_list);
+			list_move_tail(&obj->global_list, &still_in_list);
+
+			if (flags & I915_SHRINK_PURGEABLE &&
+			    !i915_gem_object_is_purgeable(obj))
+				continue;
+
+			drm_gem_object_reference(&obj->base);
+
+			/* For the unbound phase, this should be a no-op! */
+			INIT_LIST_HEAD(&vma_list);
+			while (!list_empty(&obj->vma_list)) {
+				struct i915_vma *vma;
+
+				vma = list_first_entry(&obj->vma_list,
+						       typeof(*vma), obj_link);
+				list_move_tail(&vma->obj_link, &vma_list);
+				if (i915_vma_unbind(vma))
+					break;
+			}
+			list_splice(&vma_list, &obj->vma_list);
 
-		if (i915_gem_object_put_pages(obj) == 0)
-			count += obj->base.size >> PAGE_SHIFT;
+			if (i915_gem_object_put_pages(obj) == 0)
+				count += obj->base.size >> PAGE_SHIFT;
 
-		drm_gem_object_unreference(&obj->base);
+			drm_gem_object_unreference(&obj->base);
+		}
+		list_splice(&still_in_list, phase->list);
 	}
-	list_splice(&still_in_list, &dev_priv->mm.bound_list);
 
 	return count;
 }
 
 static unsigned long
-i915_gem_purge(struct drm_i915_private *dev_priv, long target)
-{
-	return __i915_gem_shrink(dev_priv, target, true);
-}
-
-static unsigned long
 i915_gem_shrink_all(struct drm_i915_private *dev_priv)
 {
-	i915_gem_evict_everything(dev_priv->dev);
-	return __i915_gem_shrink(dev_priv, LONG_MAX, false);
+	return i915_gem_shrink(dev_priv, LONG_MAX,
+			       I915_SHRINK_BOUND | I915_SHRINK_UNBOUND);
 }
 
 static int
@@ -2061,15 +2152,18 @@
 	for (i = 0; i < page_count; i++) {
 		page = shmem_read_mapping_page_gfp(mapping, i, gfp);
 		if (IS_ERR(page)) {
-			i915_gem_purge(dev_priv, page_count);
+			i915_gem_shrink(dev_priv,
+					page_count,
+					I915_SHRINK_BOUND |
+					I915_SHRINK_UNBOUND |
+					I915_SHRINK_PURGEABLE);
 			page = shmem_read_mapping_page_gfp(mapping, i, gfp);
 		}
 		if (IS_ERR(page)) {
 			/* We've tried hard to allocate the memory by reaping
-			 * our own buffer, now let the real VM do its job and
+			 * our own buffers, now let the real VM do its job and
 			 * go down in flames if truly OOM.
 			 */
-			i915_gem_shrink_all(dev_priv);
 			page = shmem_read_mapping_page(mapping, i);
 			if (IS_ERR(page))
 				goto err_pages;
@@ -2159,529 +2253,243 @@
 	return 0;
 }
 
-static void
-i915_gem_object_move_to_active(struct drm_i915_gem_object *obj,
-			       struct intel_engine_cs *ring)
+int i915_gem_set_seqno(struct drm_device *dev, u32 seqno)
 {
-	struct drm_device *dev = obj->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	u32 seqno = intel_ring_get_seqno(ring);
-
-	BUG_ON(ring == NULL);
-	if (obj->ring != ring && obj->last_write_seqno) {
-		/* Keep the seqno relative to the current ring */
-		obj->last_write_seqno = seqno;
-	}
-	obj->ring = ring;
+	struct intel_engine_cs *signaller, *waiter;
+	int ret, i, j;
 
-	/* Add a reference if we're newly entering the active list. */
-	if (!obj->active) {
-		drm_gem_object_reference(&obj->base);
-		obj->active = 1;
-	}
+	if (seqno == 0)
+		return -EINVAL;
 
-	list_move_tail(&obj->ring_list, &ring->active_list);
+	if (seqno == dev_priv->next_seqno)
+		return 0;
 
-	obj->last_read_seqno = seqno;
+	do {
+		/* Flush the breadcrumbs */
+		ret = i915_gpu_idle(dev);
+		if (ret)
+			return ret;
 
-	if (obj->fenced_gpu_access) {
-		obj->last_fenced_seqno = seqno;
+		if (!i915_gem_retire_requests(dev))
+			return -EIO;
 
-		/* Bump MRU to take account of the delayed flush */
-		if (obj->fence_reg != I915_FENCE_REG_NONE) {
-			struct drm_i915_fence_reg *reg;
+		/* Update all semaphores to the current value */
+		for_each_engine(signaller, to_i915(dev), i) {
+			struct i915_gem_request *rq;
+
+			if (!signaller->semaphore.signal)
+				continue;
+
+			rq = i915_request_create(signaller->default_context,
+						 signaller);
+			if (IS_ERR(rq))
+				return PTR_ERR(rq);
+
+			for_each_engine(waiter, to_i915(dev), j) {
+				if (signaller == waiter)
+					continue;
+
+				if (!waiter->semaphore.wait)
+					continue;
+
+				ret = i915_request_emit_semaphore(rq, waiter->id);
+				if (ret)
+					break;
+			}
 
-			reg = &dev_priv->fence_regs[obj->fence_reg];
-			list_move_tail(&reg->lru_list,
-				       &dev_priv->mm.fence_list);
+			if (ret == 0)
+				ret = i915_request_commit(rq);
+			i915_request_put(rq);
+			if (ret)
+				return ret;
 		}
-	}
-}
 
-void i915_vma_move_to_active(struct i915_vma *vma,
-			     struct intel_engine_cs *ring)
-{
-	list_move_tail(&vma->mm_list, &vma->vm->active_list);
-	return i915_gem_object_move_to_active(vma->obj, ring);
+		/* We can only roll seqno forwards across a wraparound.
+		 * This ship is not for turning!
+		 */
+		if (!__i915_seqno_passed(dev_priv->next_seqno, seqno))
+			break;
+
+		dev_priv->next_seqno += 0x40000000;
+	}while (1);
+
+	dev_priv->next_seqno = seqno;
+	return 0;
 }
 
-static void
-i915_gem_object_move_to_inactive(struct drm_i915_gem_object *obj)
+void i915_gem_restore_fences(struct drm_device *dev)
 {
-	struct drm_i915_private *dev_priv = obj->base.dev->dev_private;
-	struct i915_address_space *vm;
-	struct i915_vma *vma;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	int i;
 
-	BUG_ON(obj->base.write_domain & ~I915_GEM_GPU_DOMAINS);
-	BUG_ON(!obj->active);
+	for (i = 0; i < dev_priv->num_fence_regs; i++) {
+		struct drm_i915_fence_reg *reg = &dev_priv->fence_regs[i];
 
-	list_for_each_entry(vm, &dev_priv->vm_list, global_link) {
-		vma = i915_gem_obj_to_vma(obj, vm);
-		if (vma && !list_empty(&vma->mm_list))
-			list_move_tail(&vma->mm_list, &vm->inactive_list);
+		/*
+		 * Commit delayed tiling changes if we have an object still
+		 * attached to the fence, otherwise just clear the fence.
+		 */
+		if (reg->obj) {
+			i915_gem_object_update_fence(reg->obj, reg,
+						     reg->obj->tiling_mode);
+		} else {
+			i915_gem_write_fence(dev, i, NULL);
+		}
 	}
+}
 
-	intel_fb_obj_flush(obj, true);
+void i915_gem_reset(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_engine_cs *engine;
+	int i;
 
-	list_del_init(&obj->ring_list);
-	obj->ring = NULL;
+	for_each_engine(engine, dev_priv, i) {
+		/* Clearing the read list will also clear the write
+		 * and fence lists, 3 birds with one stone.
+		 */
+		while (!list_empty(&engine->read_list)) {
+			struct drm_i915_gem_object *obj;
 
-	obj->last_read_seqno = 0;
-	obj->last_write_seqno = 0;
-	obj->base.write_domain = 0;
+			obj = list_first_entry(&engine->read_list,
+					       struct drm_i915_gem_object,
+					       last_read[i].engine_link);
 
-	obj->last_fenced_seqno = 0;
-	obj->fenced_gpu_access = false;
+			i915_gem_object_retire__read(obj, engine);
+		}
 
-	obj->active = 0;
-	drm_gem_object_unreference(&obj->base);
+		while (!list_empty(&engine->vma_list)) {
+			struct i915_vma *vma;
 
-	WARN_ON(i915_verify_lists(dev));
-}
+			vma = list_first_entry(&engine->vma_list,
+					       struct i915_vma,
+					       last_read[i].engine_link);
 
-static void
-i915_gem_object_retire(struct drm_i915_gem_object *obj)
-{
-	struct intel_engine_cs *ring = obj->ring;
+			i915_vma_retire__read(vma, i);
+		}
 
-	if (ring == NULL)
-		return;
+		intel_engine_reset(engine);
+	}
 
-	if (i915_seqno_passed(ring->get_seqno(ring, true),
-			      obj->last_read_seqno))
-		i915_gem_object_move_to_inactive(obj);
+	i915_gem_restore_fences(dev);
 }
 
-static int
-i915_gem_init_seqno(struct drm_device *dev, u32 seqno)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
-	int ret, i, j;
-
-	/* Carefully retire all requests without writing to the rings */
-	for_each_ring(ring, dev_priv, i) {
-		ret = intel_ring_idle(ring);
-		if (ret)
-			return ret;
-	}
-	i915_gem_retire_requests(dev);
-
-	/* Finally reset hw state */
-	for_each_ring(ring, dev_priv, i) {
-		intel_ring_init_seqno(ring, seqno);
-
-		for (j = 0; j < ARRAY_SIZE(ring->semaphore.sync_seqno); j++)
-			ring->semaphore.sync_seqno[j] = 0;
-	}
-
-	return 0;
-}
-
-int i915_gem_set_seqno(struct drm_device *dev, u32 seqno)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	int ret;
-
-	if (seqno == 0)
-		return -EINVAL;
-
-	/* HWS page needs to be set less than what we
-	 * will inject to ring
-	 */
-	ret = i915_gem_init_seqno(dev, seqno - 1);
-	if (ret)
-		return ret;
-
-	/* Carefully set the last_seqno value so that wrap
-	 * detection still works
-	 */
-	dev_priv->next_seqno = seqno;
-	dev_priv->last_seqno = seqno - 1;
-	if (dev_priv->last_seqno == 0)
-		dev_priv->last_seqno--;
-
-	return 0;
-}
-
-int
-i915_gem_get_seqno(struct drm_device *dev, u32 *seqno)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-
-	/* reserve 0 for non-seqno */
-	if (dev_priv->next_seqno == 0) {
-		int ret = i915_gem_init_seqno(dev, 0);
-		if (ret)
-			return ret;
-
-		dev_priv->next_seqno = 1;
-	}
-
-	*seqno = dev_priv->last_seqno = dev_priv->next_seqno++;
-	return 0;
-}
-
-int __i915_add_request(struct intel_engine_cs *ring,
-		       struct drm_file *file,
-		       struct drm_i915_gem_object *obj,
-		       u32 *out_seqno)
-{
-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
-	struct drm_i915_gem_request *request;
-	u32 request_ring_position, request_start;
-	int ret;
-
-	request_start = intel_ring_get_tail(ring->buffer);
-	/*
-	 * Emit any outstanding flushes - execbuf can fail to emit the flush
-	 * after having emitted the batchbuffer command. Hence we need to fix
-	 * things up similar to emitting the lazy request. The difference here
-	 * is that the flush _must_ happen before the next request, no matter
-	 * what.
-	 */
-	ret = intel_ring_flush_all_caches(ring);
-	if (ret)
-		return ret;
-
-	request = ring->preallocated_lazy_request;
-	if (WARN_ON(request == NULL))
-		return -ENOMEM;
-
-	/* Record the position of the start of the request so that
-	 * should we detect the updated seqno part-way through the
-	 * GPU processing the request, we never over-estimate the
-	 * position of the head.
-	 */
-	request_ring_position = intel_ring_get_tail(ring->buffer);
-
-	ret = ring->add_request(ring);
-	if (ret)
-		return ret;
-
-	request->seqno = intel_ring_get_seqno(ring);
-	request->ring = ring;
-	request->head = request_start;
-	request->tail = request_ring_position;
-
-	/* Whilst this request exists, batch_obj will be on the
-	 * active_list, and so will hold the active reference. Only when this
-	 * request is retired will the the batch_obj be moved onto the
-	 * inactive_list and lose its active reference. Hence we do not need
-	 * to explicitly hold another reference here.
-	 */
-	request->batch_obj = obj;
-
-	/* Hold a reference to the current context so that we can inspect
-	 * it later in case a hangcheck error event fires.
-	 */
-	request->ctx = ring->last_context;
-	if (request->ctx)
-		i915_gem_context_reference(request->ctx);
-
-	request->emitted_jiffies = jiffies;
-	list_add_tail(&request->list, &ring->request_list);
-	request->file_priv = NULL;
-
-	if (file) {
-		struct drm_i915_file_private *file_priv = file->driver_priv;
-
-		spin_lock(&file_priv->mm.lock);
-		request->file_priv = file_priv;
-		list_add_tail(&request->client_list,
-			      &file_priv->mm.request_list);
-		spin_unlock(&file_priv->mm.lock);
-	}
-
-	trace_i915_gem_request_add(ring, request->seqno);
-	ring->outstanding_lazy_seqno = 0;
-	ring->preallocated_lazy_request = NULL;
-
-	if (!dev_priv->ums.mm_suspended) {
-		i915_queue_hangcheck(ring->dev);
-
-		cancel_delayed_work_sync(&dev_priv->mm.idle_work);
-		queue_delayed_work(dev_priv->wq,
-				   &dev_priv->mm.retire_work,
-				   round_jiffies_up_relative(HZ));
-		intel_mark_busy(dev_priv->dev);
-	}
-
-	if (out_seqno)
-		*out_seqno = request->seqno;
-	return 0;
-}
-
-static inline void
-i915_gem_request_remove_from_client(struct drm_i915_gem_request *request)
+/**
+ * This function clears the request list as sequence numbers are passed.
+ */
+void
+i915_gem_retire_requests__engine(struct intel_engine_cs *engine)
 {
-	struct drm_i915_file_private *file_priv = request->file_priv;
-
-	if (!file_priv)
+	if (engine->last_request == NULL)
 		return;
 
-	spin_lock(&file_priv->mm.lock);
-	list_del(&request->client_list);
-	request->file_priv = NULL;
-	spin_unlock(&file_priv->mm.lock);
-}
-
-static bool i915_context_is_banned(struct drm_i915_private *dev_priv,
-				   const struct intel_context *ctx)
-{
-	unsigned long elapsed;
-
-	elapsed = get_seconds() - ctx->hang_stats.guilty_ts;
-
-	if (ctx->hang_stats.banned)
-		return true;
-
-	if (elapsed <= DRM_I915_CTX_BAN_PERIOD) {
-		if (!i915_gem_context_is_default(ctx)) {
-			DRM_DEBUG("context hanging too fast, banning!\n");
-			return true;
-		} else if (i915_stop_ring_allow_ban(dev_priv)) {
-			if (i915_stop_ring_allow_warn(dev_priv))
-				DRM_ERROR("gpu hanging too fast, banning!\n");
-			return true;
-		}
-	}
-
-	return false;
-}
-
-static void i915_set_reset_status(struct drm_i915_private *dev_priv,
-				  struct intel_context *ctx,
-				  const bool guilty)
-{
-	struct i915_ctx_hang_stats *hs;
-
-	if (WARN_ON(!ctx))
+	if (!intel_engine_retire(engine, intel_engine_get_seqno(engine)))
 		return;
 
-	hs = &ctx->hang_stats;
-
-	if (guilty) {
-		hs->banned = i915_context_is_banned(dev_priv, ctx);
-		hs->batch_active++;
-		hs->guilty_ts = get_seconds();
-	} else {
-		hs->batch_pending++;
-	}
-}
-
-static void i915_gem_free_request(struct drm_i915_gem_request *request)
-{
-	list_del(&request->list);
-	i915_gem_request_remove_from_client(request);
-
-	if (request->ctx)
-		i915_gem_context_unreference(request->ctx);
-
-	kfree(request);
-}
-
-struct drm_i915_gem_request *
-i915_gem_find_active_request(struct intel_engine_cs *ring)
-{
-	struct drm_i915_gem_request *request;
-	u32 completed_seqno;
+	while (!list_empty(&engine->read_list)) {
+		struct drm_i915_gem_object *obj;
 
-	completed_seqno = ring->get_seqno(ring, false);
+		obj = list_first_entry(&engine->read_list,
+				       struct drm_i915_gem_object,
+				       last_read[engine->id].engine_link);
 
-	list_for_each_entry(request, &ring->request_list, list) {
-		if (i915_seqno_passed(completed_seqno, request->seqno))
-			continue;
+		if (!obj->last_read[engine->id].request->completed)
+			break;
 
-		return request;
+		i915_gem_object_retire__read(obj, engine);
 	}
 
-	return NULL;
-}
-
-static void i915_gem_reset_ring_status(struct drm_i915_private *dev_priv,
-				       struct intel_engine_cs *ring)
-{
-	struct drm_i915_gem_request *request;
-	bool ring_hung;
-
-	request = i915_gem_find_active_request(ring);
-
-	if (request == NULL)
-		return;
-
-	ring_hung = ring->hangcheck.score >= HANGCHECK_SCORE_RING_HUNG;
-
-	i915_set_reset_status(dev_priv, request->ctx, ring_hung);
-
-	list_for_each_entry_continue(request, &ring->request_list, list)
-		i915_set_reset_status(dev_priv, request->ctx, false);
-}
-
-static void i915_gem_reset_ring_cleanup(struct drm_i915_private *dev_priv,
-					struct intel_engine_cs *ring)
-{
-	while (!list_empty(&ring->active_list)) {
+	while (!list_empty(&engine->fence_list)) {
 		struct drm_i915_gem_object *obj;
 
-		obj = list_first_entry(&ring->active_list,
+		obj = list_first_entry(&engine->fence_list,
 				       struct drm_i915_gem_object,
-				       ring_list);
-
-		i915_gem_object_move_to_inactive(obj);
-	}
-
-	/*
-	 * We must free the requests after all the corresponding objects have
-	 * been moved off active lists. Which is the same order as the normal
-	 * retire_requests function does. This is important if object hold
-	 * implicit references on things like e.g. ppgtt address spaces through
-	 * the request.
-	 */
-	while (!list_empty(&ring->request_list)) {
-		struct drm_i915_gem_request *request;
-
-		request = list_first_entry(&ring->request_list,
-					   struct drm_i915_gem_request,
-					   list);
-
-		i915_gem_free_request(request);
-	}
-
-	/* These may not have been flush before the reset, do so now */
-	kfree(ring->preallocated_lazy_request);
-	ring->preallocated_lazy_request = NULL;
-	ring->outstanding_lazy_seqno = 0;
-}
+				       last_fence.engine_link);
 
-void i915_gem_restore_fences(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	int i;
-
-	for (i = 0; i < dev_priv->num_fence_regs; i++) {
-		struct drm_i915_fence_reg *reg = &dev_priv->fence_regs[i];
+		if (!obj->last_fence.request->completed)
+			break;
 
-		/*
-		 * Commit delayed tiling changes if we have an object still
-		 * attached to the fence, otherwise just clear the fence.
-		 */
-		if (reg->obj) {
-			i915_gem_object_update_fence(reg->obj, reg,
-						     reg->obj->tiling_mode);
-		} else {
-			i915_gem_write_fence(dev, i, NULL);
-		}
+		i915_gem_object_retire__fence(obj);
 	}
-}
-
-void i915_gem_reset(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
-	int i;
-
-	/*
-	 * Before we free the objects from the requests, we need to inspect
-	 * them for finding the guilty party. As the requests only borrow
-	 * their reference to the objects, the inspection must be done first.
-	 */
-	for_each_ring(ring, dev_priv, i)
-		i915_gem_reset_ring_status(dev_priv, ring);
-
-	for_each_ring(ring, dev_priv, i)
-		i915_gem_reset_ring_cleanup(dev_priv, ring);
-
-	i915_gem_context_reset(dev);
-
-	i915_gem_restore_fences(dev);
-}
-
-/**
- * This function clears the request list as sequence numbers are passed.
- */
-void
-i915_gem_retire_requests_ring(struct intel_engine_cs *ring)
-{
-	uint32_t seqno;
 
-	if (list_empty(&ring->request_list))
-		return;
-
-	WARN_ON(i915_verify_lists(ring->dev));
-
-	seqno = ring->get_seqno(ring, true);
-
-	/* Move any buffers on the active list that are no longer referenced
-	 * by the ringbuffer to the flushing/inactive lists as appropriate,
-	 * before we free the context associated with the requests.
-	 */
-	while (!list_empty(&ring->active_list)) {
+	while (!list_empty(&engine->write_list)) {
 		struct drm_i915_gem_object *obj;
 
-		obj = list_first_entry(&ring->active_list,
-				      struct drm_i915_gem_object,
-				      ring_list);
+		obj = list_first_entry(&engine->write_list,
+				       struct drm_i915_gem_object,
+				       last_write.engine_link);
 
-		if (!i915_seqno_passed(seqno, obj->last_read_seqno))
+		if (!obj->last_write.request->completed)
 			break;
 
-		i915_gem_object_move_to_inactive(obj);
+		i915_gem_object_retire__write(obj);
 	}
 
+	while (!list_empty(&engine->vma_list)) {
+		struct i915_vma *vma;
 
-	while (!list_empty(&ring->request_list)) {
-		struct drm_i915_gem_request *request;
-
-		request = list_first_entry(&ring->request_list,
-					   struct drm_i915_gem_request,
-					   list);
+		vma = list_first_entry(&engine->vma_list,
+				       struct i915_vma,
+				       last_read[engine->id].engine_link);
 
-		if (!i915_seqno_passed(seqno, request->seqno))
+		if (!vma->last_read[engine->id].request->completed)
 			break;
 
-		trace_i915_gem_request_retire(ring, request->seqno);
-		/* We know the GPU must have read the request to have
-		 * sent us the seqno + interrupt, so use the position
-		 * of tail of the request to update the last known position
-		 * of the GPU head.
-		 */
-		ring->buffer->last_retired_head = request->tail;
-
-		i915_gem_free_request(request);
+		i915_vma_retire__read(vma, engine->id);
 	}
-
-	if (unlikely(ring->trace_irq_seqno &&
-		     i915_seqno_passed(seqno, ring->trace_irq_seqno))) {
-		ring->irq_put(ring);
-		ring->trace_irq_seqno = 0;
-	}
-
-	WARN_ON(i915_verify_lists(ring->dev));
 }
 
 bool
 i915_gem_retire_requests(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
+	struct intel_engine_cs *engine;
 	bool idle = true;
 	int i;
 
-	for_each_ring(ring, dev_priv, i) {
-		i915_gem_retire_requests_ring(ring);
-		idle &= list_empty(&ring->request_list);
+	for_each_engine(engine, dev_priv, i) {
+		i915_gem_retire_requests__engine(engine);
+		idle &= engine->last_request == NULL;
 	}
 
 	if (idle)
 		mod_delayed_work(dev_priv->wq,
-				   &dev_priv->mm.idle_work,
-				   msecs_to_jiffies(100));
+				 &dev_priv->mm.idle_work,
+				 msecs_to_jiffies(100));
 
 	return idle;
 }
 
 static void
+i915_gem_flush_requests(struct drm_i915_private *i915)
+{
+	struct intel_engine_cs *engine;
+	int i;
+
+	for_each_engine(engine, i915, i) {
+		struct i915_gem_request *rq;
+		int ret;
+
+		if (engine->last_request == NULL)
+			continue;
+
+		if (engine->last_request->breadcrumb[i])
+			continue;
+
+		rq = i915_request_create(engine->last_request->ctx,
+					 engine);
+		if (IS_ERR(rq))
+			continue;
+
+		if (i915_request_emit_breadcrumb(rq) == 0)
+			ret = i915_request_commit(rq);
+		i915_request_put(rq);
+		(void)ret;
+	}
+}
+
+static void
 i915_gem_retire_work_handler(struct work_struct *work)
 {
 	struct drm_i915_private *dev_priv =
@@ -2692,11 +2500,13 @@
 	/* Come back later if the device is busy... */
 	idle = false;
 	if (mutex_trylock(&dev->struct_mutex)) {
+		i915_gem_flush_requests(dev_priv);
 		idle = i915_gem_retire_requests(dev);
 		mutex_unlock(&dev->struct_mutex);
 	}
 	if (!idle)
-		queue_delayed_work(dev_priv->wq, &dev_priv->mm.retire_work,
+		queue_delayed_work(dev_priv->wq,
+				   &dev_priv->mm.retire_work,
 				   round_jiffies_up_relative(HZ));
 }
 
@@ -2717,16 +2527,21 @@
 static int
 i915_gem_object_flush_active(struct drm_i915_gem_object *obj)
 {
-	int ret;
+	int ret, n;
 
-	if (obj->active) {
-		ret = i915_gem_check_olr(obj->ring, obj->last_read_seqno);
+	if (!obj->active)
+		return 0;
+
+	for (n = 0; n < I915_NUM_ENGINES; n++) {
+		if (obj->last_read[n].request == NULL)
+			continue;
+
+		ret = i915_request_emit_breadcrumb(obj->last_read[n].request);
 		if (ret)
 			return ret;
-
-		i915_gem_retire_requests_ring(obj->ring);
 	}
 
+	i915_gem_object_retire(obj);
 	return 0;
 }
 
@@ -2755,13 +2570,13 @@
 int
 i915_gem_wait_ioctl(struct drm_device *dev, void *data, struct drm_file *file)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_i915_gem_wait *args = data;
 	struct drm_i915_gem_object *obj;
-	struct intel_engine_cs *ring = NULL;
-	unsigned reset_counter;
-	u32 seqno = 0;
-	int ret = 0;
+	struct i915_gem_request *rq[I915_NUM_ENGINES] = {};
+	int i, n = 0, ret;
+
+	if (args->flags != 0)
+		return -EINVAL;
 
 	ret = i915_mutex_lock_interruptible(dev);
 	if (ret)
@@ -2773,40 +2588,87 @@
 		return -ENOENT;
 	}
 
-	/* Need to make sure the object gets inactive eventually. */
-	ret = i915_gem_object_flush_active(obj);
-	if (ret)
-		goto out;
-
-	if (obj->active) {
-		seqno = obj->last_read_seqno;
-		ring = obj->ring;
-	}
-
-	if (seqno == 0)
-		 goto out;
-
-	/* Do this after OLR check to make sure we make forward progress polling
-	 * on this IOCTL with a timeout <=0 (like busy ioctl)
+	/* Make sure we make forward progress polling with a timeout <=0
+	 * (like busy ioctl)
 	 */
 	if (args->timeout_ns <= 0) {
-		ret = -ETIME;
+		ret = i915_gem_object_flush_active(obj);
+		if (ret == 0)
+			ret = -ETIME;
 		goto out;
 	}
 
-	drm_gem_object_unreference(&obj->base);
-	reset_counter = atomic_read(&dev_priv->gpu_error.reset_counter);
-	mutex_unlock(&dev->struct_mutex);
+	for (i = 0; i < I915_NUM_ENGINES; i++) {
+		if (obj->last_read[i].request == NULL)
+			continue;
 
-	return __wait_seqno(ring, seqno, reset_counter, true, &args->timeout_ns,
-			    file->driver_priv);
+		rq[n] = i915_request_get_breadcrumb(obj->last_read[i].request);
+		if (IS_ERR(rq[n])) {
+			ret = PTR_ERR(rq[n]);
+			break;
+		}
+		n++;
+	}
 
 out:
 	drm_gem_object_unreference(&obj->base);
 	mutex_unlock(&dev->struct_mutex);
+
+	for (i = 0; i < n; i++) {
+		if (ret == 0)
+			ret = __i915_request_wait(rq[i], true,
+						  &args->timeout_ns,
+						  file->driver_priv);
+
+		i915_request_put__unlocked(rq[i]);
+	}
+
 	return ret;
 }
 
+static int
+__i915_request_sync(struct i915_gem_request *waiter,
+		    struct i915_gem_request *signaller,
+		    bool *retire)
+{
+	int ret;
+
+	if (signaller == NULL || i915_request_complete(signaller))
+		return 0;
+
+	if (waiter == NULL)
+		goto wait;
+
+	/* XXX still true with execlists? */
+	if (waiter->engine == signaller->engine)
+		return 0;
+
+	if (!waiter->engine->semaphore.wait)
+		goto wait;
+
+	/* Try to emit only one wait per request per ring */
+	if (waiter->semaphore[signaller->engine->id] &&
+	    __i915_seqno_passed(waiter->semaphore[signaller->engine->id],
+				signaller->seqno))
+		return 0;
+
+	ret = i915_request_emit_semaphore(signaller, waiter->engine->id);
+	if (ret)
+		goto wait;
+
+	trace_i915_gem_ring_wait(signaller, waiter);
+	if (waiter->engine->semaphore.wait(waiter, signaller))
+		goto wait;
+
+	waiter->pending_flush &= ~I915_COMMAND_BARRIER;
+	waiter->semaphore[signaller->engine->id] = signaller->breadcrumb[waiter->engine->id];
+	return 0;
+
+wait:
+	*retire = true;
+	return i915_request_wait(signaller);
+}
+
 /**
  * i915_gem_object_sync - sync an object to a ring.
  *
@@ -2821,43 +2683,28 @@
  */
 int
 i915_gem_object_sync(struct drm_i915_gem_object *obj,
-		     struct intel_engine_cs *to)
+		     struct i915_gem_request *rq)
 {
-	struct intel_engine_cs *from = obj->ring;
-	u32 seqno;
-	int ret, idx;
-
-	if (from == NULL || to == from)
-		return 0;
-
-	if (to == NULL || !i915_semaphore_is_enabled(obj->base.dev))
-		return i915_gem_object_wait_rendering(obj, false);
+	int ret = 0, i;
+	bool retire = false;
 
-	idx = intel_ring_sync_index(from, to);
-
-	seqno = obj->last_read_seqno;
-	/* Optimization: Avoid semaphore sync when we are sure we already
-	 * waited for an object with higher seqno */
-	if (seqno <= from->semaphore.sync_seqno[idx])
-		return 0;
-
-	ret = i915_gem_check_olr(obj->ring, seqno);
-	if (ret)
-		return ret;
+	if (obj->base.pending_write_domain == 0) {
+		ret = __i915_request_sync(rq, obj->last_write.request, &retire);
+	} else {
+		for (i = 0; i < I915_NUM_ENGINES; i++) {
+			ret = __i915_request_sync(rq, obj->last_read[i].request, &retire);
+			if (ret)
+				break;
+		}
+	}
 
-	trace_i915_gem_ring_sync_to(from, to, seqno);
-	ret = to->semaphore.sync_to(to, from, seqno);
-	if (!ret)
-		/* We use last_read_seqno because sync_to()
-		 * might have just caused seqno wrap under
-		 * the radar.
-		 */
-		from->semaphore.sync_seqno[idx] = obj->last_read_seqno;
+	if (retire && obj->active)
+		i915_gem_object_retire(obj);
 
 	return ret;
 }
 
-static void i915_gem_object_finish_gtt(struct drm_i915_gem_object *obj)
+void i915_gem_object_finish_gtt(struct drm_i915_gem_object *obj)
 {
 	u32 old_write_domain, old_read_domains;
 
@@ -2881,17 +2728,44 @@
 					    old_write_domain);
 }
 
+static __must_check int
+i915_vma_wait_rendering(struct i915_vma *vma)
+{
+	int i, ret;
+
+	if (!vma->active)
+		return 0;
+
+	for (i = 0; i < I915_NUM_ENGINES; i++) {
+		struct i915_gem_request *rq;
+
+		rq = i915_request_get(vma->last_read[i].request);
+		if (rq == NULL)
+			continue;
+
+		/* Waiting on a request, may require emitting a new
+		 * request for this vma - e.g. a context object, or
+		 * it may invoke the shrinker and also complete the
+		 * request.
+		 */
+		ret = i915_request_wait(rq);
+		if (ret == 0 && rq == vma->last_read[i].request)
+			i915_vma_retire__read(vma, i);
+		i915_request_put(rq);
+		if (ret)
+			return ret;
+	}
+
+	return 0;
+}
+
 int i915_vma_unbind(struct i915_vma *vma)
 {
 	struct drm_i915_gem_object *obj = vma->obj;
-	struct drm_i915_private *dev_priv = obj->base.dev->dev_private;
 	int ret;
 
-	if (list_empty(&vma->vma_link))
-		return 0;
-
 	if (!drm_mm_node_allocated(&vma->node)) {
-		i915_gem_vma_destroy(vma);
+		RQ_BUG_ON(!list_empty(&vma->mm_list));
 		return 0;
 	}
 
@@ -2899,41 +2773,45 @@
 		return -EBUSY;
 
 	BUG_ON(obj->pages == NULL);
+	BUG_ON(obj->madv == __I915_MADV_PURGED);
 
-	ret = i915_gem_object_finish_gpu(obj);
-	if (ret)
-		return ret;
-	/* Continue on if we fail due to EIO, the GPU is hung so we
-	 * should be safe and we need to cleanup or else we might
-	 * cause memory corruption through use-after-free.
-	 */
+	i915_vma_get(vma);
 
-	if (i915_is_ggtt(vma->vm)) {
-		i915_gem_object_finish_gtt(obj);
+	ret = i915_vma_wait_rendering(vma);
+	if (ret)
+		goto out;
 
-		/* release the fence reg _after_ flushing */
-		ret = i915_gem_object_put_fence(obj);
-		if (ret)
-			return ret;
+	/* Double check that waiting on the rendering did not inadvertently
+	 * pin our vma (for example due to a context switch).
+	 */
+	if (vma->pin_count) {
+		ret = -EBUSY;
+		goto out;
 	}
 
-	trace_i915_vma_unbind(vma);
+	/* Whilst waiting we may have invoked the shrinker who gazzumped us
+	 * and freed the vma allocation already.
+	 */
+	if (!drm_mm_node_allocated(&vma->node))
+		goto out;
 
-	vma->unbind_vma(vma);
+	trace_i915_vma_unbind(vma);
+	ret = vma->unbind_vma(vma);
+	if (ret)
+		goto out;
+	RQ_BUG_ON(vma->bound);
 
 	list_del_init(&vma->mm_list);
-	/* Avoid an unnecessary call to unbind on rebind. */
-	if (i915_is_ggtt(vma->vm))
-		obj->map_and_fenceable = true;
-
 	drm_mm_remove_node(&vma->node);
-	i915_gem_vma_destroy(vma);
+	RQ_BUG_ON(drm_mm_node_allocated(&vma->node));
+
+	i915_vma_put(vma);
 
-	/* Since the unbound list is global, only move to that list if
-	 * no more VMAs exist. */
-	if (list_empty(&obj->vma_list)) {
+	/* The unbound list is global, check if we were the last bound */
+	if (!i915_gem_obj_bound_any(obj)) {
 		i915_gem_gtt_finish_object(obj);
-		list_move_tail(&obj->global_list, &dev_priv->mm.unbound_list);
+		list_move_tail(&obj->global_list,
+			       &to_i915(obj->base.dev)->mm.unbound_list);
 	}
 
 	/* And finally now the object is completely decoupled from this vma,
@@ -2942,22 +2820,34 @@
 	 */
 	i915_gem_object_unpin_pages(obj);
 
-	return 0;
+out:
+	i915_vma_put(vma);
+	return ret;
 }
 
 int i915_gpu_idle(struct drm_device *dev)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
-	int ret, i;
+	struct intel_engine_cs *engine;
+	int i;
 
-	/* Flush everything onto the inactive list. */
-	for_each_ring(ring, dev_priv, i) {
-		ret = i915_switch_context(ring, ring->default_context);
-		if (ret)
-			return ret;
+	/* Flush everything including contexts onto the inactive list. */
+	for_each_engine(engine, to_i915(dev), i) {
+		struct i915_gem_request *rq;
+		int ret;
+
+		/* Only emit a wait if busy or we need to force a ctx switch */
+		rq = engine->last_request;
+		if (rq == NULL || rq->ctx != engine->default_context) {
+			rq = i915_request_create(engine->default_context,
+						 engine);
+			if (IS_ERR(rq))
+				return PTR_ERR(rq);
+		} else
+			rq = i915_request_get(rq);
+
+		ret = i915_request_wait(rq);
+		i915_request_put(rq);
 
-		ret = intel_ring_idle(ring);
 		if (ret)
 			return ret;
 	}
@@ -3114,6 +3004,7 @@
 	     obj->stride, obj->tiling_mode);
 
 	switch (INTEL_INFO(dev)->gen) {
+	case 9:
 	case 8:
 	case 7:
 	case 6:
@@ -3161,15 +3052,16 @@
 static int
 i915_gem_object_wait_fence(struct drm_i915_gem_object *obj)
 {
-	if (obj->last_fenced_seqno) {
-		int ret = i915_wait_seqno(obj->ring, obj->last_fenced_seqno);
-		if (ret)
-			return ret;
+	int ret;
 
-		obj->last_fenced_seqno = 0;
-	}
+	if (obj->last_fence.request == NULL)
+		return 0;
 
-	obj->fenced_gpu_access = false;
+	ret = i915_request_wait(obj->last_fence.request);
+	if (ret)
+		return ret;
+
+	i915_gem_object_retire__fence(obj);
 	return 0;
 }
 
@@ -3276,6 +3168,9 @@
 			return 0;
 		}
 	} else if (enable) {
+		if (WARN_ON(!obj->map_and_fenceable))
+			return -EINVAL;
+
 		reg = i915_find_fence_reg(dev);
 		if (IS_ERR(reg))
 			return PTR_ERR(reg);
@@ -3297,17 +3192,20 @@
 	return 0;
 }
 
-static bool i915_gem_valid_gtt_space(struct drm_device *dev,
-				     struct drm_mm_node *gtt_space,
+static bool i915_gem_valid_gtt_space(struct i915_vma *vma,
 				     unsigned long cache_level)
 {
+	struct drm_mm_node *gtt_space = &vma->node;
 	struct drm_mm_node *other;
 
-	/* On non-LLC machines we have to be careful when putting differing
-	 * types of snoopable memory together to avoid the prefetcher
-	 * crossing memory domains and dying.
+	/*
+	 * On some machines we have to be careful when putting differing types
+	 * of snoopable memory together to avoid the prefetcher crossing memory
+	 * domains and dying. During vm initialisation, we decide whether or not
+	 * these constraints apply and set the drm_mm.color_adjust
+	 * appropriately.
 	 */
-	if (HAS_LLC(dev))
+	if (vma->vm->mm.color_adjust == NULL)
 		return true;
 
 	if (!drm_mm_node_allocated(gtt_space))
@@ -3327,63 +3225,20 @@
 	return true;
 }
 
-static void i915_gem_verify_gtt(struct drm_device *dev)
-{
-#if WATCH_GTT
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_i915_gem_object *obj;
-	int err = 0;
-
-	list_for_each_entry(obj, &dev_priv->mm.gtt_list, global_list) {
-		if (obj->gtt_space == NULL) {
-			printk(KERN_ERR "object found on GTT list with no space reserved\n");
-			err++;
-			continue;
-		}
-
-		if (obj->cache_level != obj->gtt_space->color) {
-			printk(KERN_ERR "object reserved space [%08lx, %08lx] with wrong color, cache_level=%x, color=%lx\n",
-			       i915_gem_obj_ggtt_offset(obj),
-			       i915_gem_obj_ggtt_offset(obj) + i915_gem_obj_ggtt_size(obj),
-			       obj->cache_level,
-			       obj->gtt_space->color);
-			err++;
-			continue;
-		}
-
-		if (!i915_gem_valid_gtt_space(dev,
-					      obj->gtt_space,
-					      obj->cache_level)) {
-			printk(KERN_ERR "invalid GTT space found at [%08lx, %08lx] - color=%x\n",
-			       i915_gem_obj_ggtt_offset(obj),
-			       i915_gem_obj_ggtt_offset(obj) + i915_gem_obj_ggtt_size(obj),
-			       obj->cache_level);
-			err++;
-			continue;
-		}
-	}
-
-	WARN_ON(err);
-#endif
-}
-
 /**
  * Finds free space in the GTT aperture and binds the object there.
  */
-static struct i915_vma *
-i915_gem_object_bind_to_vm(struct drm_i915_gem_object *obj,
-			   struct i915_address_space *vm,
-			   unsigned alignment,
-			   uint64_t flags)
+static int
+i915_vma_reserve(struct i915_vma *vma, unsigned alignment, uint64_t flags)
 {
+	struct drm_i915_gem_object *obj = vma->obj;
 	struct drm_device *dev = obj->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	u32 size, fence_size, fence_alignment, unfenced_alignment;
 	unsigned long start =
 		flags & PIN_OFFSET_BIAS ? flags & PIN_OFFSET_MASK : 0;
 	unsigned long end =
-		flags & PIN_MAPPABLE ? dev_priv->gtt.mappable_end : vm->total;
-	struct i915_vma *vma;
+		flags & PIN_MAPPABLE ? dev_priv->gtt.mappable_end : vma->vm->total;
 	int ret;
 
 	fence_size = i915_gem_get_gtt_size(dev,
@@ -3402,7 +3257,7 @@
 						unfenced_alignment;
 	if (flags & PIN_MAPPABLE && alignment & (fence_alignment - 1)) {
 		DRM_DEBUG("Invalid object alignment requested %u\n", alignment);
-		return ERR_PTR(-EINVAL);
+		return -EINVAL;
 	}
 
 	size = flags & PIN_MAPPABLE ? fence_size : obj->base.size;
@@ -3415,38 +3270,64 @@
 			  obj->base.size,
 			  flags & PIN_MAPPABLE ? "mappable" : "total",
 			  end);
-		return ERR_PTR(-E2BIG);
+		return -E2BIG;
 	}
 
 	ret = i915_gem_object_get_pages(obj);
 	if (ret)
-		return ERR_PTR(ret);
+		return ret;
 
 	i915_gem_object_pin_pages(obj);
 
-	vma = i915_gem_obj_lookup_or_create_vma(obj, vm);
-	if (IS_ERR(vma))
-		goto err_unpin;
-
+	if (flags & PIN_OFFSET_FIXED) {
+		uint64_t offset = flags & PIN_OFFSET_MASK;
+		if (alignment && offset & (alignment - 1)) {
+			ret = -EINVAL;
+			goto err_unpin;
+		}
+		vma->node.start = offset;
+		vma->node.size = size;
+		vma->node.color = obj->cache_level;
+		ret = drm_mm_reserve_node(&vma->vm->mm, &vma->node);
+		if (ret) {
+			ret = i915_gem_evict_range(dev, vma->vm, start, end);
+			if (ret == 0)
+				ret = drm_mm_reserve_node(&vma->vm->mm, &vma->node);
+		}
+		if (ret)
+			goto err_unpin;
+	} else {
+		unsigned search = DRM_MM_SEARCH_DEFAULT;
+		unsigned create = DRM_MM_CREATE_DEFAULT;
+		if (i915_is_ggtt(vma->vm) &&
+		    (flags & PIN_MAPPABLE) == 0 &&
+		    obj->cache_level != I915_CACHE_NONE) {
+			search = DRM_MM_SEARCH_BELOW;
+			create = DRM_MM_CREATE_TOP;
+		}
 search_free:
-	ret = drm_mm_insert_node_in_range_generic(&vm->mm, &vma->node,
-						  size, alignment,
-						  obj->cache_level,
-						  start, end,
-						  DRM_MM_SEARCH_DEFAULT,
-						  DRM_MM_CREATE_DEFAULT);
-	if (ret) {
-		ret = i915_gem_evict_something(dev, vm, size, alignment,
-					       obj->cache_level,
-					       start, end,
-					       flags);
-		if (ret == 0)
-			goto search_free;
+		ret = drm_mm_insert_node_in_range_generic(&vma->vm->mm,
+							  &vma->node,
+							  size, alignment,
+							  obj->cache_level,
+							  start, end,
+							  search, create);
+		if (ret) {
+			ret = i915_gem_evict_something(dev, vma->vm,
+						       size, alignment,
+						       obj->cache_level,
+						       start, end,
+						       flags);
+			if (ret == 0)
+				goto search_free;
 
-		goto err_free_vma;
+			goto err_unpin;
+		}
 	}
-	if (WARN_ON(!i915_gem_valid_gtt_space(dev, &vma->node,
-					      obj->cache_level))) {
+	i915_vma_get(vma);
+	vma->pin_count++;
+
+	if (WARN_ON(!i915_gem_valid_gtt_space(vma, obj->cache_level))) {
 		ret = -EINVAL;
 		goto err_remove_node;
 	}
@@ -3455,38 +3336,20 @@
 	if (ret)
 		goto err_remove_node;
 
+	RQ_BUG_ON(!list_empty(&vma->mm_list));
 	list_move_tail(&obj->global_list, &dev_priv->mm.bound_list);
-	list_add_tail(&vma->mm_list, &vm->inactive_list);
-
-	if (i915_is_ggtt(vm)) {
-		bool mappable, fenceable;
-
-		fenceable = (vma->node.size == fence_size &&
-			     (vma->node.start & (fence_alignment - 1)) == 0);
-
-		mappable = (vma->node.start + obj->base.size <=
-			    dev_priv->gtt.mappable_end);
-
-		obj->map_and_fenceable = mappable && fenceable;
-	}
-
-	WARN_ON(flags & PIN_MAPPABLE && !obj->map_and_fenceable);
-
-	trace_i915_vma_bind(vma, flags);
-	vma->bind_vma(vma, obj->cache_level,
-		      flags & (PIN_MAPPABLE | PIN_GLOBAL) ? GLOBAL_BIND : 0);
+	list_add_tail(&vma->mm_list, &vma->vm->inactive_list);
 
-	i915_gem_verify_gtt(dev);
-	return vma;
+	return 0;
 
 err_remove_node:
+	vma->pin_count--;
+	RQ_BUG_ON(vma->pin_count != 0);
+	i915_vma_put(vma);
 	drm_mm_remove_node(&vma->node);
-err_free_vma:
-	i915_gem_vma_destroy(vma);
-	vma = ERR_PTR(ret);
 err_unpin:
 	i915_gem_object_unpin_pages(obj);
-	return vma;
+	return ret;
 }
 
 bool
@@ -3504,7 +3367,7 @@
 	 * Stolen memory is always coherent with the GPU as it is explicitly
 	 * marked as wc by the system, or the system is cache-coherent.
 	 */
-	if (obj->stolen)
+	if (obj->stolen || obj->phys_handle)
 		return false;
 
 	/* If the GPU is snooping the contents of the CPU cache,
@@ -3585,14 +3448,10 @@
 int
 i915_gem_object_set_to_gtt_domain(struct drm_i915_gem_object *obj, bool write)
 {
-	struct drm_i915_private *dev_priv = obj->base.dev->dev_private;
 	uint32_t old_write_domain, old_read_domains;
+	struct i915_vma *vma;
 	int ret;
 
-	/* Not valid to be called on unbound objects. */
-	if (!i915_gem_obj_bound_any(obj))
-		return -EINVAL;
-
 	if (obj->base.write_domain == I915_GEM_DOMAIN_GTT)
 		return 0;
 
@@ -3600,7 +3459,18 @@
 	if (ret)
 		return ret;
 
-	i915_gem_object_retire(obj);
+	/* Flush and acquire obj->pages so that we are coherent through
+	 * direct access in memory with previous cached writes through
+	 * shmemfs and that our cache domain tracking remains valid.
+	 * For example, if the obj->filp was moved to swap without us
+	 * being notified and releasing the pages, we would mistakenly
+	 * continue to assume that the obj remained out of the CPU cached
+	 * domain.
+	 */
+	ret = i915_gem_object_get_pages(obj);
+	if (ret)
+		return ret;
+
 	i915_gem_object_flush_cpu_write_domain(obj, false);
 
 	/* Serialise direct access to this object with the barriers for
@@ -3619,25 +3489,22 @@
 	BUG_ON((obj->base.write_domain & ~I915_GEM_DOMAIN_GTT) != 0);
 	obj->base.read_domains |= I915_GEM_DOMAIN_GTT;
 	if (write) {
+		intel_fb_obj_invalidate(obj, NULL);
 		obj->base.read_domains = I915_GEM_DOMAIN_GTT;
 		obj->base.write_domain = I915_GEM_DOMAIN_GTT;
 		obj->dirty = 1;
 	}
 
-	if (write)
-		intel_fb_obj_invalidate(obj, NULL);
-
 	trace_i915_gem_object_change_domain(obj,
 					    old_read_domains,
 					    old_write_domain);
 
 	/* And bump the LRU for this access */
-	if (i915_gem_object_is_inactive(obj)) {
-		struct i915_vma *vma = i915_gem_obj_to_ggtt(obj);
-		if (vma)
-			list_move_tail(&vma->mm_list,
-				       &dev_priv->gtt.base.inactive_list);
-
+	vma = i915_gem_obj_to_ggtt(obj);
+	if (vma && vma->bound & GLOBAL_BIND && !vma->active) {
+		RQ_BUG_ON(list_empty(&vma->mm_list));
+		list_move_tail(&vma->mm_list,
+			       &to_i915(obj->base.dev)->gtt.base.inactive_list);
 	}
 
 	return 0;
@@ -3658,8 +3525,8 @@
 		return -EBUSY;
 	}
 
-	list_for_each_entry_safe(vma, next, &obj->vma_list, vma_link) {
-		if (!i915_gem_valid_gtt_space(dev, &vma->node, cache_level)) {
+	list_for_each_entry_safe(vma, next, &obj->vma_list, obj_link) {
+		if (!i915_gem_valid_gtt_space(vma, cache_level)) {
 			ret = i915_vma_unbind(vma);
 			if (ret)
 				return ret;
@@ -3683,13 +3550,12 @@
 				return ret;
 		}
 
-		list_for_each_entry(vma, &obj->vma_list, vma_link)
+		list_for_each_entry(vma, &obj->vma_list, obj_link)
 			if (drm_mm_node_allocated(&vma->node))
-				vma->bind_vma(vma, cache_level,
-					      obj->has_global_gtt_mapping ? GLOBAL_BIND : 0);
+				vma->bind_vma(vma, cache_level, REBIND);
 	}
 
-	list_for_each_entry(vma, &obj->vma_list, vma_link)
+	list_for_each_entry(vma, &obj->vma_list, obj_link)
 		vma->node.color = cache_level;
 	obj->cache_level = cache_level;
 
@@ -3702,7 +3568,6 @@
 		 * in obj->write_domain and have been skipping the clflushes.
 		 * Just set it to the CPU cache for now.
 		 */
-		i915_gem_object_retire(obj);
 		WARN_ON(obj->base.write_domain & ~I915_GEM_DOMAIN_CPU);
 
 		old_read_domains = obj->base.read_domains;
@@ -3716,7 +3581,6 @@
 						    old_write_domain);
 	}
 
-	i915_gem_verify_gtt(dev);
 	return 0;
 }
 
@@ -3802,9 +3666,6 @@
 {
 	struct i915_vma *vma;
 
-	if (list_empty(&obj->vma_list))
-		return false;
-
 	vma = i915_gem_obj_to_ggtt(obj);
 	if (!vma)
 		return false;
@@ -3831,17 +3692,15 @@
 int
 i915_gem_object_pin_to_display_plane(struct drm_i915_gem_object *obj,
 				     u32 alignment,
-				     struct intel_engine_cs *pipelined)
+				     struct i915_gem_request *pipelined)
 {
 	u32 old_read_domains, old_write_domain;
 	bool was_pin_display;
 	int ret;
 
-	if (pipelined != obj->ring) {
-		ret = i915_gem_object_sync(obj, pipelined);
-		if (ret)
-			return ret;
-	}
+	ret = i915_gem_object_sync(obj, pipelined);
+	if (ret)
+		return ret;
 
 	/* Mark the pin_display early so that we account for the
 	 * display coherency whilst setting up the cache domains.
@@ -3867,7 +3726,7 @@
 	 * (e.g. libkms for the bootup splash), we have to ensure that we
 	 * always use map_and_fenceable for all scanout buffers.
 	 */
-	ret = i915_gem_obj_ggtt_pin(obj, alignment, PIN_MAPPABLE);
+	ret = i915_gem_object_ggtt_pin(obj, alignment, PIN_MAPPABLE);
 	if (ret)
 		goto err_unpin_display;
 
@@ -3937,7 +3796,6 @@
 	if (ret)
 		return ret;
 
-	i915_gem_object_retire(obj);
 	i915_gem_object_flush_gtt_write_domain(obj);
 
 	old_write_domain = obj->base.write_domain;
@@ -3959,13 +3817,11 @@
 	 * need to be invalidated at next use.
 	 */
 	if (write) {
+		intel_fb_obj_invalidate(obj, NULL);
 		obj->base.read_domains = I915_GEM_DOMAIN_CPU;
 		obj->base.write_domain = I915_GEM_DOMAIN_CPU;
 	}
 
-	if (write)
-		intel_fb_obj_invalidate(obj, NULL);
-
 	trace_i915_gem_object_change_domain(obj,
 					    old_read_domains,
 					    old_write_domain);
@@ -3973,64 +3829,15 @@
 	return 0;
 }
 
-/* Throttle our rendering by waiting until the ring has completed our requests
- * emitted over 20 msec ago.
- *
- * Note that if we were to use the current jiffies each time around the loop,
- * we wouldn't escape the function with any frames outstanding if the time to
- * render a frame was over 20ms.
- *
- * This should get us reasonable parallelism between CPU and GPU but also
- * relatively low latency when blocking on a particular request to finish.
- */
-static int
-i915_gem_ring_throttle(struct drm_device *dev, struct drm_file *file)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_i915_file_private *file_priv = file->driver_priv;
-	unsigned long recent_enough = jiffies - msecs_to_jiffies(20);
-	struct drm_i915_gem_request *request;
-	struct intel_engine_cs *ring = NULL;
-	unsigned reset_counter;
-	u32 seqno = 0;
-	int ret;
-
-	ret = i915_gem_wait_for_error(&dev_priv->gpu_error);
-	if (ret)
-		return ret;
-
-	ret = i915_gem_check_wedge(&dev_priv->gpu_error, false);
-	if (ret)
-		return ret;
-
-	spin_lock(&file_priv->mm.lock);
-	list_for_each_entry(request, &file_priv->mm.request_list, client_list) {
-		if (time_after_eq(request->emitted_jiffies, recent_enough))
-			break;
-
-		ring = request->ring;
-		seqno = request->seqno;
-	}
-	reset_counter = atomic_read(&dev_priv->gpu_error.reset_counter);
-	spin_unlock(&file_priv->mm.lock);
-
-	if (seqno == 0)
-		return 0;
-
-	ret = __wait_seqno(ring, seqno, reset_counter, true, NULL, NULL);
-	if (ret == 0)
-		queue_delayed_work(dev_priv->wq, &dev_priv->mm.retire_work, 0);
-
-	return ret;
-}
-
 static bool
 i915_vma_misplaced(struct i915_vma *vma, uint32_t alignment, uint64_t flags)
 {
 	struct drm_i915_gem_object *obj = vma->obj;
 
-	if (alignment &&
-	    vma->node.start & (alignment - 1))
+	if (!drm_mm_node_allocated(&vma->node))
+		return false;
+
+	if (alignment && vma->node.start & (alignment - 1))
 		return true;
 
 	if (flags & PIN_MAPPABLE && !obj->map_and_fenceable)
@@ -4040,70 +3847,96 @@
 	    vma->node.start < (flags & PIN_OFFSET_MASK))
 		return true;
 
+	if (flags & PIN_OFFSET_FIXED &&
+	    vma->node.start != (flags & PIN_OFFSET_MASK))
+		return true;
+
 	return false;
 }
 
 int
-i915_gem_object_pin(struct drm_i915_gem_object *obj,
-		    struct i915_address_space *vm,
-		    uint32_t alignment,
-		    uint64_t flags)
+i915_vma_pin(struct i915_vma *vma, uint32_t alignment, uint64_t flags)
 {
-	struct drm_i915_private *dev_priv = obj->base.dev->dev_private;
-	struct i915_vma *vma;
+	unsigned bind;
 	int ret;
 
-	if (WARN_ON(vm == &dev_priv->mm.aliasing_ppgtt->base))
-		return -ENODEV;
+	if (WARN_ON(flags & (PIN_GLOBAL | PIN_MAPPABLE) &&
+		    !i915_is_ggtt(vma->vm)))
+		return -EINVAL;
 
-	if (WARN_ON(flags & (PIN_GLOBAL | PIN_MAPPABLE) && !i915_is_ggtt(vm)))
+	if (WARN_ON((flags & (PIN_MAPPABLE | PIN_GLOBAL)) == PIN_MAPPABLE))
 		return -EINVAL;
 
-	vma = i915_gem_obj_to_vma(obj, vm);
-	if (vma) {
-		if (WARN_ON(vma->pin_count == DRM_I915_GEM_OBJECT_MAX_PIN_COUNT))
-			return -EBUSY;
+	if (WARN_ON(vma->obj->madv == __I915_MADV_PURGED))
+		return -EFAULT;
 
-		if (i915_vma_misplaced(vma, alignment, flags)) {
-			WARN(vma->pin_count,
-			     "bo is already pinned with incorrect alignment:"
-			     " offset=%lx, req.alignment=%x, req.map_and_fenceable=%d,"
-			     " obj->map_and_fenceable=%d\n",
-			     i915_gem_obj_offset(obj, vm), alignment,
-			     !!(flags & PIN_MAPPABLE),
-			     obj->map_and_fenceable);
-			ret = i915_vma_unbind(vma);
-			if (ret)
-				return ret;
+	RQ_BUG_ON(vma->vm->closed);
 
-			vma = NULL;
-		}
+	if (i915_vma_misplaced(vma, alignment, flags)) {
+		WARN(vma->pin_count,
+		     "bo is already pinned with incorrect alignment:"
+		     " offset=%lx, req.alignment=%x, req.map_and_fenceable=%d,"
+		     " obj->map_and_fenceable=%d\n",
+		     vma->node.start, alignment,
+		     !!(flags & PIN_MAPPABLE),
+		     vma->obj->map_and_fenceable);
+		ret = i915_vma_unbind(vma);
+		if (ret)
+			return ret;
 	}
 
-	if (vma == NULL || !drm_mm_node_allocated(&vma->node)) {
-		vma = i915_gem_object_bind_to_vm(obj, vm, alignment, flags);
-		if (IS_ERR(vma))
-			return PTR_ERR(vma);
-	}
+	if (!drm_mm_node_allocated(&vma->node)) {
+		ret = i915_vma_reserve(vma, alignment, flags);
+		if (ret)
+			return ret;
+	} else
+		vma->pin_count++;
 
-	if (flags & PIN_GLOBAL && !obj->has_global_gtt_mapping)
-		vma->bind_vma(vma, obj->cache_level, GLOBAL_BIND);
+	bind = 0;
+	if (flags & PIN_GLOBAL)
+		bind |= GLOBAL_BIND;
+	if (flags & PIN_LOCAL)
+		bind |= LOCAL_BIND;
+	ret = vma->bind_vma(vma, vma->obj->cache_level, bind);
+	if (ret) {
+		--vma->pin_count;
+		return ret;
+	}
 
-	vma->pin_count++;
-	if (flags & PIN_MAPPABLE)
-		obj->pin_mappable |= true;
+	if (flags & PIN_MAPPABLE) {
+		WARN_ON(!vma->obj->map_and_fenceable);
+		vma->obj->pin_mappable = true;
+	}
 
 	return 0;
 }
 
+int
+i915_gem_object_ggtt_pin(struct drm_i915_gem_object *obj,
+			 uint32_t alignment,
+			 unsigned flags)
+{
+	struct i915_vma *vma;
+	int ret;
+
+	vma = i915_gem_obj_get_ggtt(obj);
+	if (IS_ERR(vma))
+		return PTR_ERR(vma);
+
+	ret = i915_vma_pin(vma, alignment, flags | PIN_GLOBAL);
+	i915_vma_put(vma);
+
+	return ret;
+}
+
 void
 i915_gem_object_ggtt_unpin(struct drm_i915_gem_object *obj)
 {
 	struct i915_vma *vma = i915_gem_obj_to_ggtt(obj);
 
-	BUG_ON(!vma);
+	BUG_ON(vma == NULL);
 	BUG_ON(vma->pin_count == 0);
-	BUG_ON(!i915_gem_obj_ggtt_bound(obj));
+	BUG_ON(!drm_mm_node_allocated(&vma->node));
 
 	if (--vma->pin_count == 0)
 		obj->pin_mappable = false;
@@ -4175,7 +4008,9 @@
 	}
 
 	if (obj->user_pin_count == 0) {
-		ret = i915_gem_obj_ggtt_pin(obj, args->alignment, PIN_MAPPABLE);
+		ret = i915_gem_object_ggtt_pin(obj,
+					       args->alignment,
+					       PIN_MAPPABLE);
 		if (ret)
 			goto out;
 	}
@@ -4234,7 +4069,7 @@
 {
 	struct drm_i915_gem_busy *args = data;
 	struct drm_i915_gem_object *obj;
-	int ret;
+	int ret, i;
 
 	ret = i915_mutex_lock_interruptible(dev);
 	if (ret)
@@ -4246,30 +4081,83 @@
 		goto unlock;
 	}
 
-	/* Count all active objects as busy, even if they are currently not used
-	 * by the gpu. Users of this interface expect objects to eventually
-	 * become non-busy without any further actions, therefore emit any
-	 * necessary flushes here.
-	 */
-	ret = i915_gem_object_flush_active(obj);
+	/* Count all active objects as busy, even if they are currently not used
+	 * by the gpu. Users of this interface expect objects to eventually
+	 * become non-busy without any further actions, therefore emit any
+	 * necessary flushes here.
+	 */
+	ret = i915_gem_object_flush_active(obj);
+
+	args->busy = 0;
+	if (obj->active) {
+		BUILD_BUG_ON(I915_NUM_ENGINES > 16);
+		args->busy |= 1;
+		for (i = 0; i < I915_NUM_ENGINES; i++)  {
+			if (obj->last_read[i].request == NULL)
+				continue;
+
+			args->busy |= 1 << (16 + i);
+		}
+	}
+
+	drm_gem_object_unreference(&obj->base);
+unlock:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+
+/* Throttle our rendering by waiting until the ring has completed our requests
+ * emitted over 20 msec ago.
+ *
+ * Note that if we were to use the current jiffies each time around the loop,
+ * we wouldn't escape the function with any frames outstanding if the time to
+ * render a frame was over 20ms.
+ *
+ * This should get us reasonable parallelism between CPU and GPU but also
+ * relatively low latency when blocking on a particular request to finish.
+ */
+int
+i915_gem_throttle_ioctl(struct drm_device *dev, void *data,
+			struct drm_file *file)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_file_private *file_priv = file->driver_priv;
+	unsigned long recent_enough = jiffies - msecs_to_jiffies(20);
+	struct i915_gem_request *rq, *tmp;
+	int ret;
+
+	ret = i915_gem_wait_for_error(&dev_priv->gpu_error);
+	if (ret)
+		return ret;
+
+	/* used for querying whethering the GPU is wedged by legacy userspace */
+	if (i915_terminally_wedged(&dev_priv->gpu_error))
+		return -EIO;
 
-	args->busy = obj->active;
-	if (obj->ring) {
-		BUILD_BUG_ON(I915_NUM_RINGS > 16);
-		args->busy |= intel_ring_flag(obj->ring) << 16;
+	spin_lock(&file_priv->mm.lock);
+	rq = NULL;
+	list_for_each_entry(tmp, &file_priv->mm.request_list, client_list) {
+		if (time_after_eq(tmp->emitted_jiffies, recent_enough))
+			break;
+		rq = tmp;
 	}
+	rq = i915_request_get(rq);
+	spin_unlock(&file_priv->mm.lock);
 
-	drm_gem_object_unreference(&obj->base);
-unlock:
-	mutex_unlock(&dev->struct_mutex);
-	return ret;
-}
+	if (rq != NULL) {
+		if (rq->breadcrumb[rq->engine->id] == 0) {
+			ret = i915_mutex_lock_interruptible(dev);
+			if (ret == 0) {
+				ret = i915_request_emit_breadcrumb(rq);
+				mutex_unlock(&dev->struct_mutex);
+			}
+		}
+		if (ret == 0)
+			ret = __i915_request_wait(rq, true, NULL, NULL);
+		i915_request_put__unlocked(rq);
+	}
 
-int
-i915_gem_throttle_ioctl(struct drm_device *dev, void *data,
-			struct drm_file *file_priv)
-{
-	return i915_gem_ring_throttle(dev, file_priv);
+	return ret;
 }
 
 int
@@ -4283,6 +4171,8 @@
 	switch (args->madv) {
 	case I915_MADV_DONTNEED:
 	case I915_MADV_WILLNEED:
+	case I915_MADV_POPULATE:
+	case I915_MADV_INVALIDATE:
 	    break;
 	default:
 	    return -EINVAL;
@@ -4298,21 +4188,21 @@
 		goto unlock;
 	}
 
-	if (i915_gem_obj_is_pinned(obj)) {
-		ret = -EINVAL;
-		goto out;
-	}
-
-	if (obj->madv != __I915_MADV_PURGED)
-		obj->madv = args->madv;
+	if (args->madv == I915_MADV_INVALIDATE) {
+		ret = drop_pages(obj);
+	} else if (args->madv == I915_MADV_POPULATE) {
+		ret = i915_gem_object_get_pages(obj);
+	} else {
+		if (obj->madv != __I915_MADV_PURGED)
+			obj->madv = args->madv;
 
-	/* if the object is no longer attached, discard its backing storage */
-	if (i915_gem_object_is_purgeable(obj) && obj->pages == NULL)
-		i915_gem_object_truncate(obj);
+		/* if the object is no longer attached, discard its backing storage */
+		if (obj->pages == NULL && i915_gem_object_is_purgeable(obj))
+			i915_gem_object_truncate(obj);
+	}
 
 	args->retained = obj->madv != __I915_MADV_PURGED;
 
-out:
 	drm_gem_object_unreference(&obj->base);
 unlock:
 	mutex_unlock(&dev->struct_mutex);
@@ -4322,8 +4212,13 @@
 void i915_gem_object_init(struct drm_i915_gem_object *obj,
 			  const struct drm_i915_gem_object_ops *ops)
 {
+	int i;
+
 	INIT_LIST_HEAD(&obj->global_list);
-	INIT_LIST_HEAD(&obj->ring_list);
+	INIT_LIST_HEAD(&obj->last_fence.engine_link);
+	INIT_LIST_HEAD(&obj->last_write.engine_link);
+	for (i = 0; i < I915_NUM_ENGINES; i++)
+		INIT_LIST_HEAD(&obj->last_read[i].engine_link);
 	INIT_LIST_HEAD(&obj->obj_exec_link);
 	INIT_LIST_HEAD(&obj->vma_list);
 
@@ -4331,8 +4226,6 @@
 
 	obj->fence_reg = I915_FENCE_REG_NONE;
 	obj->madv = I915_MADV_WILLNEED;
-	/* Avoid an unnecessary call to unbind on the first bind. */
-	obj->map_and_fenceable = true;
 
 	i915_gem_info_add_obj(obj->base.dev->dev_private, obj->base.size);
 }
@@ -4424,30 +4317,26 @@
 	struct drm_i915_gem_object *obj = to_intel_bo(gem_obj);
 	struct drm_device *dev = obj->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct i915_vma *vma, *next;
+	bool was_interruptible;
 
 	intel_runtime_pm_get(dev_priv);
 
 	trace_i915_gem_object_destroy(obj);
 
-	list_for_each_entry_safe(vma, next, &obj->vma_list, vma_link) {
-		int ret;
-
-		vma->pin_count = 0;
-		ret = i915_vma_unbind(vma);
-		if (WARN_ON(ret == -ERESTARTSYS)) {
-			bool was_interruptible;
-
-			was_interruptible = dev_priv->mm.interruptible;
-			dev_priv->mm.interruptible = false;
+	was_interruptible = dev_priv->mm.interruptible;
+	dev_priv->mm.interruptible = false;
+	while (!list_empty(&obj->vma_list)) {
+		struct i915_vma *vma;
 
-			WARN_ON(i915_vma_unbind(vma));
+		vma = list_first_entry(&obj->vma_list, typeof(*vma), obj_link);
+		list_del_init(&vma->obj_link);
 
-			dev_priv->mm.interruptible = was_interruptible;
-		}
+		vma->pin_count = 0;
+		BUG_ON(i915_vma_unbind(vma));
+		i915_vma_put(vma);
 	}
-
-	i915_gem_object_detach_phys(obj);
+	dev_priv->mm.interruptible = was_interruptible;
+	BUG_ON(obj->fence_reg != I915_FENCE_REG_NONE);
 
 	/* Stolen objects don't hold a ref, but do hold pin count. Fix that up
 	 * before progressing. */
@@ -4484,35 +4373,140 @@
 				     struct i915_address_space *vm)
 {
 	struct i915_vma *vma;
-	list_for_each_entry(vma, &obj->vma_list, vma_link)
+	list_for_each_entry(vma, &obj->vma_list, obj_link)
 		if (vma->vm == vm)
 			return vma;
 
 	return NULL;
 }
 
-void i915_gem_vma_destroy(struct i915_vma *vma)
+static void
+i915_gem_cleanup_engines(struct drm_device *dev)
 {
-	WARN_ON(vma->node.allocated);
+	int i;
 
-	/* Keep the vma as a placeholder in the execbuffer reservation lists */
-	if (!list_empty(&vma->exec_list))
-		return;
+	/* Not the regular for_each_engine so we can cleanup a failed setup */
+	for (i = 0; i < I915_NUM_ENGINES; i++)
+		intel_engine_cleanup(&to_i915(dev)->engine[i]);
+}
+
+static int
+i915_gem_resume_engines(struct drm_device *dev)
+{
+	struct intel_engine_cs *engine;
+	int i, ret;
 
-	list_del(&vma->vma_link);
+	for_each_engine(engine, to_i915(dev), i) {
+		ret = intel_engine_resume(engine);
+		if (ret)
+			return ret;
+	}
 
-	kfree(vma);
+	return 0;
 }
 
-static void
-i915_gem_stop_ringbuffers(struct drm_device *dev)
+static int
+i915_gem_suspend_engines(struct drm_device *dev)
+{
+	struct intel_engine_cs *engine;
+	int i, ret;
+
+	for_each_engine(engine, to_i915(dev), i) {
+		ret = intel_engine_suspend(engine);
+		if (ret)
+			return ret;
+	}
+
+	return 0;
+}
+
+static bool
+intel_enable_blt(struct drm_i915_private *dev_priv)
+{
+	if (!HAS_BLT(dev_priv))
+		return false;
+
+	/* The blitter was dysfunctional on early prototypes */
+	if (IS_GEN6(dev_priv) && dev_priv->dev->pdev->revision < 8) {
+		DRM_INFO("BLT not supported on this pre-production hardware;"
+			 " graphics performance will be degraded.\n");
+		return false;
+	}
+
+	return true;
+}
+
+static void stop_unused_ring(struct drm_i915_private *dev_priv, u32 base)
+{
+	I915_WRITE(RING_CTL(base), 0);
+	I915_WRITE(RING_HEAD(base), 0);
+	I915_WRITE(RING_TAIL(base), 0);
+	I915_WRITE(RING_START(base), 0);
+}
+
+static void stop_unused_rings(struct drm_i915_private *dev_priv)
+{
+	if (IS_I830(dev_priv)) {
+		stop_unused_ring(dev_priv, PRB1_BASE);
+		stop_unused_ring(dev_priv, SRB0_BASE);
+		stop_unused_ring(dev_priv, SRB1_BASE);
+		stop_unused_ring(dev_priv, SRB2_BASE);
+		stop_unused_ring(dev_priv, SRB3_BASE);
+	} else if (IS_GEN2(dev_priv)) {
+		stop_unused_ring(dev_priv, SRB0_BASE);
+		stop_unused_ring(dev_priv, SRB1_BASE);
+	} else if (IS_GEN3(dev_priv)) {
+		stop_unused_ring(dev_priv, PRB1_BASE);
+		stop_unused_ring(dev_priv, PRB2_BASE);
+	}
+}
+
+static int i915_gem_setup_engines(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
-	int i;
+	int ret;
+
+	/*
+	 * At least 830 can leave some of the unused rings
+	 * "active" (ie. head != tail) after resume which
+	 * will prevent c3 entry. Makes sure all unused rings
+	 * are totally idle.
+	 */
+	stop_unused_rings(dev_priv);
+
+	ret = intel_init_render_engine(dev_priv);
+	if (ret)
+		goto cleanup;
+
+	if (HAS_BSD(dev_priv)) {
+		ret = intel_init_bsd_engine(dev_priv);
+		if (ret)
+			goto cleanup;
+	}
+
+	if (intel_enable_blt(dev_priv)) {
+		ret = intel_init_blt_engine(dev_priv);
+		if (ret)
+			goto cleanup;
+	}
+
+	if (HAS_VEBOX(dev_priv)) {
+		ret = intel_init_vebox_engine(dev_priv);
+		if (ret)
+			goto cleanup;
+	}
+
+	if (HAS_BSD2(dev_priv)) {
+		ret = intel_init_bsd2_engine(dev_priv);
+		if (ret)
+			goto cleanup;
+	}
 
-	for_each_ring(ring, dev_priv, i)
-		intel_stop_ring_buffer(ring);
+	return 0;
+
+cleanup:
+	i915_gem_cleanup_engines(dev);
+	return ret;
 }
 
 int
@@ -4522,9 +4516,6 @@
 	int ret = 0;
 
 	mutex_lock(&dev->struct_mutex);
-	if (dev_priv->ums.mm_suspended)
-		goto err;
-
 	ret = i915_gpu_idle(dev);
 	if (ret)
 		goto err;
@@ -4535,18 +4526,13 @@
 	if (!drm_core_check_feature(dev, DRIVER_MODESET))
 		i915_gem_evict_everything(dev);
 
-	i915_kernel_lost_context(dev);
-	i915_gem_stop_ringbuffers(dev);
+	ret = i915_gem_suspend_engines(dev);
+	if (ret)
+		goto err;
 
-	/* Hack!  Don't let anybody do execbuf while we don't control the chip.
-	 * We need to replace this with a semaphore, or something.
-	 * And not confound ums.mm_suspended!
-	 */
-	dev_priv->ums.mm_suspended = !drm_core_check_feature(dev,
-							     DRIVER_MODESET);
 	mutex_unlock(&dev->struct_mutex);
 
-	del_timer_sync(&dev_priv->gpu_error.hangcheck_timer);
+	cancel_delayed_work_sync(&dev_priv->gpu_error.hangcheck_work);
 	cancel_delayed_work_sync(&dev_priv->mm.retire_work);
 	flush_delayed_work(&dev_priv->mm.idle_work);
 
@@ -4557,37 +4543,6 @@
 	return ret;
 }
 
-int i915_gem_l3_remap(struct intel_engine_cs *ring, int slice)
-{
-	struct drm_device *dev = ring->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	u32 reg_base = GEN7_L3LOG_BASE + (slice * 0x200);
-	u32 *remap_info = dev_priv->l3_parity.remap_info[slice];
-	int i, ret;
-
-	if (!HAS_L3_DPF(dev) || !remap_info)
-		return 0;
-
-	ret = intel_ring_begin(ring, GEN7_L3LOG_SIZE / 4 * 3);
-	if (ret)
-		return ret;
-
-	/*
-	 * Note: We do not worry about the concurrent register cacheline hang
-	 * here because no other code should access these registers other than
-	 * at initialization time.
-	 */
-	for (i = 0; i < GEN7_L3LOG_SIZE; i += 4) {
-		intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
-		intel_ring_emit(ring, reg_base + i);
-		intel_ring_emit(ring, remap_info[i/4]);
-	}
-
-	intel_ring_advance(ring);
-
-	return ret;
-}
-
 void i915_gem_init_swizzling(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -4613,80 +4568,11 @@
 		BUG();
 }
 
-static bool
-intel_enable_blt(struct drm_device *dev)
-{
-	if (!HAS_BLT(dev))
-		return false;
-
-	/* The blitter was dysfunctional on early prototypes */
-	if (IS_GEN6(dev) && dev->pdev->revision < 8) {
-		DRM_INFO("BLT not supported on this pre-production hardware;"
-			 " graphics performance will be degraded.\n");
-		return false;
-	}
-
-	return true;
-}
-
-static int i915_gem_init_rings(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	int ret;
-
-	ret = intel_init_render_ring_buffer(dev);
-	if (ret)
-		return ret;
-
-	if (HAS_BSD(dev)) {
-		ret = intel_init_bsd_ring_buffer(dev);
-		if (ret)
-			goto cleanup_render_ring;
-	}
-
-	if (intel_enable_blt(dev)) {
-		ret = intel_init_blt_ring_buffer(dev);
-		if (ret)
-			goto cleanup_bsd_ring;
-	}
-
-	if (HAS_VEBOX(dev)) {
-		ret = intel_init_vebox_ring_buffer(dev);
-		if (ret)
-			goto cleanup_blt_ring;
-	}
-
-	if (HAS_BSD2(dev)) {
-		ret = intel_init_bsd2_ring_buffer(dev);
-		if (ret)
-			goto cleanup_vebox_ring;
-	}
-
-	ret = i915_gem_set_seqno(dev, ((u32)~0 - 0x1000));
-	if (ret)
-		goto cleanup_bsd2_ring;
-
-	return 0;
-
-cleanup_bsd2_ring:
-	intel_cleanup_ring_buffer(&dev_priv->ring[VCS2]);
-cleanup_vebox_ring:
-	intel_cleanup_ring_buffer(&dev_priv->ring[VECS]);
-cleanup_blt_ring:
-	intel_cleanup_ring_buffer(&dev_priv->ring[BCS]);
-cleanup_bsd_ring:
-	intel_cleanup_ring_buffer(&dev_priv->ring[VCS]);
-cleanup_render_ring:
-	intel_cleanup_ring_buffer(&dev_priv->ring[RCS]);
-
-	return ret;
-}
-
 int
 i915_gem_init_hw(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	int ret, i;
+	int ret;
 
 	if (INTEL_INFO(dev)->gen < 6 && !intel_enable_gtt())
 		return -EIO;
@@ -4712,25 +4598,11 @@
 
 	i915_gem_init_swizzling(dev);
 
-	ret = i915_gem_init_rings(dev);
-	if (ret)
-		return ret;
-
-	for (i = 0; i < NUM_L3_SLICES(dev); i++)
-		i915_gem_l3_remap(&dev_priv->ring[RCS], i);
-
-	/*
-	 * XXX: Contexts should only be initialized once. Doing a switch to the
-	 * default context switch however is something we'd like to do after
-	 * reset or thaw (the latter may not actually be necessary for HW, but
-	 * goes with our code better). Context switching requires rings (for
-	 * the do_switch), but before enabling PPGTT. So don't move this.
-	 */
-	ret = i915_gem_context_enable(dev_priv);
-	if (ret && ret != -EIO) {
-		DRM_ERROR("Context enable failed %d\n", ret);
-		i915_gem_cleanup_ringbuffer(dev);
-	}
+	ret = i915_ppgtt_init_hw(dev);
+	if (ret == 0)
+		ret = i915_gem_context_enable(dev_priv);
+	if (ret == 0)
+		ret = i915_gem_resume_engines(dev);
 
 	return ret;
 }
@@ -4753,13 +4625,12 @@
 	i915_gem_init_userptr(dev);
 	i915_gem_init_global_gtt(dev);
 
-	ret = i915_gem_context_init(dev);
-	if (ret) {
-		mutex_unlock(&dev->struct_mutex);
-		return ret;
-	}
-
-	ret = i915_gem_init_hw(dev);
+	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
+	ret = i915_gem_setup_engines(dev);
+	if (ret == 0)
+		ret = i915_gem_context_init(dev);
+	if (ret == 0)
+		ret = i915_gem_init_hw(dev);
 	if (ret == -EIO) {
 		/* Allow ring initialisation to fail by marking the GPU as
 		 * wedged. But we only want to do this where the GPU is angry,
@@ -4769,103 +4640,32 @@
 		atomic_set_mask(I915_WEDGED, &dev_priv->gpu_error.reset_counter);
 		ret = 0;
 	}
-	mutex_unlock(&dev->struct_mutex);
-
-	/* Allow hardware batchbuffers unless told otherwise, but not for KMS. */
-	if (!drm_core_check_feature(dev, DRIVER_MODESET))
-		dev_priv->dri1.allow_batchbuffer = 1;
-	return ret;
-}
-
-void
-i915_gem_cleanup_ringbuffer(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
-	int i;
-
-	for_each_ring(ring, dev_priv, i)
-		intel_cleanup_ring_buffer(ring);
-}
-
-int
-i915_gem_entervt_ioctl(struct drm_device *dev, void *data,
-		       struct drm_file *file_priv)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	int ret;
-
-	if (drm_core_check_feature(dev, DRIVER_MODESET))
-		return 0;
-
-	if (i915_reset_in_progress(&dev_priv->gpu_error)) {
-		DRM_ERROR("Reenabling wedged hardware, good luck\n");
-		atomic_set(&dev_priv->gpu_error.reset_counter, 0);
-	}
-
-	mutex_lock(&dev->struct_mutex);
-	dev_priv->ums.mm_suspended = 0;
-
-	ret = i915_gem_init_hw(dev);
-	if (ret != 0) {
-		mutex_unlock(&dev->struct_mutex);
-		return ret;
-	}
-
-	BUG_ON(!list_empty(&dev_priv->gtt.base.active_list));
-
-	ret = drm_irq_install(dev, dev->pdev->irq);
-	if (ret)
-		goto cleanup_ringbuffer;
-	mutex_unlock(&dev->struct_mutex);
-
-	return 0;
-
-cleanup_ringbuffer:
-	i915_gem_cleanup_ringbuffer(dev);
-	dev_priv->ums.mm_suspended = 1;
+	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
 	mutex_unlock(&dev->struct_mutex);
 
 	return ret;
 }
 
-int
-i915_gem_leavevt_ioctl(struct drm_device *dev, void *data,
-		       struct drm_file *file_priv)
-{
-	if (drm_core_check_feature(dev, DRIVER_MODESET))
-		return 0;
-
-	mutex_lock(&dev->struct_mutex);
-	drm_irq_uninstall(dev);
-	mutex_unlock(&dev->struct_mutex);
-
-	return i915_gem_suspend(dev);
-}
-
-void
-i915_gem_lastclose(struct drm_device *dev)
+void i915_gem_fini(struct drm_device *dev)
 {
-	int ret;
-
-	if (drm_core_check_feature(dev, DRIVER_MODESET))
-		return;
-
-	ret = i915_gem_suspend(dev);
-	if (ret)
-		DRM_ERROR("failed to idle hardware: %d\n", ret);
+	i915_gem_context_fini(dev);
+	i915_gem_cleanup_engines(dev);
 }
 
 static void
-init_ring_lists(struct intel_engine_cs *ring)
+init_null_engine(struct intel_engine_cs *engine)
 {
-	INIT_LIST_HEAD(&ring->active_list);
-	INIT_LIST_HEAD(&ring->request_list);
+	INIT_LIST_HEAD(&engine->read_list);
+	INIT_LIST_HEAD(&engine->write_list);
+	INIT_LIST_HEAD(&engine->fence_list);
+	INIT_LIST_HEAD(&engine->requests);
+	INIT_LIST_HEAD(&engine->rings);
 }
 
 void i915_init_vm(struct drm_i915_private *dev_priv,
 		  struct i915_address_space *vm)
 {
+	kref_init(&vm->ref);
 	if (!i915_is_ggtt(vm))
 		drm_mm_init(&vm->mm, vm->start, vm->total);
 	vm->dev = dev_priv->dev;
@@ -4894,8 +4694,8 @@
 	INIT_LIST_HEAD(&dev_priv->mm.unbound_list);
 	INIT_LIST_HEAD(&dev_priv->mm.bound_list);
 	INIT_LIST_HEAD(&dev_priv->mm.fence_list);
-	for (i = 0; i < I915_NUM_RINGS; i++)
-		init_ring_lists(&dev_priv->ring[i]);
+	for (i = 0; i < I915_NUM_ENGINES; i++)
+		init_null_engine(&dev_priv->engine[i]);
 	for (i = 0; i < I915_MAX_NUM_FENCES; i++)
 		INIT_LIST_HEAD(&dev_priv->fence_regs[i].lru_list);
 	INIT_DELAYED_WORK(&dev_priv->mm.retire_work,
@@ -4947,32 +4747,27 @@
 {
 	struct drm_i915_file_private *file_priv = file->driver_priv;
 
-	cancel_delayed_work_sync(&file_priv->mm.idle_work);
-
 	/* Clean up our request list when the client is going away, so that
 	 * later retire_requests won't dereference our soon-to-be-gone
 	 * file_priv.
 	 */
 	spin_lock(&file_priv->mm.lock);
 	while (!list_empty(&file_priv->mm.request_list)) {
-		struct drm_i915_gem_request *request;
+		struct i915_gem_request *rq;
 
-		request = list_first_entry(&file_priv->mm.request_list,
-					   struct drm_i915_gem_request,
-					   client_list);
-		list_del(&request->client_list);
-		request->file_priv = NULL;
+		rq = list_first_entry(&file_priv->mm.request_list,
+				      struct i915_gem_request,
+				      client_list);
+		list_del(&rq->client_list);
+		rq->file_priv = NULL;
 	}
 	spin_unlock(&file_priv->mm.lock);
-}
-
-static void
-i915_gem_file_idle_work_handler(struct work_struct *work)
-{
-	struct drm_i915_file_private *file_priv =
-		container_of(work, typeof(*file_priv), mm.idle_work.work);
 
-	atomic_set(&file_priv->rps_wait_boost, false);
+	if (!list_empty(&file_priv->rps_boost)) {
+		mutex_lock(&to_i915(dev)->rps.hw_lock);
+		list_del(&file_priv->rps_boost);
+		mutex_unlock(&to_i915(dev)->rps.hw_lock);
+	}
 }
 
 int i915_gem_open(struct drm_device *dev, struct drm_file *file)
@@ -4989,11 +4784,10 @@
 	file->driver_priv = file_priv;
 	file_priv->dev_priv = dev->dev_private;
 	file_priv->file = file;
+	INIT_LIST_HEAD(&file_priv->rps_boost);
 
 	spin_lock_init(&file_priv->mm.lock);
 	INIT_LIST_HEAD(&file_priv->mm.request_list);
-	INIT_DELAYED_WORK(&file_priv->mm.idle_work,
-			  i915_gem_file_idle_work_handler);
 
 	ret = i915_gem_context_open(dev, file);
 	if (ret)
@@ -5002,6 +4796,15 @@
 	return ret;
 }
 
+/**
+ * i915_gem_track_fb - update frontbuffer tracking
+ * old: current GEM buffer for the frontbuffer slots
+ * new: new GEM buffer for the frontbuffer slots
+ * frontbuffer_bits: bitmask of frontbuffer slots
+ *
+ * This updates the frontbuffer tracking bits @frontbuffer_bits by clearing them
+ * from @old and setting them in @new. Both @old and @new can be NULL.
+ */
 void i915_gem_track_fb(struct drm_i915_gem_object *old,
 		       struct drm_i915_gem_object *new,
 		       unsigned frontbuffer_bits)
@@ -5053,7 +4856,7 @@
 	struct i915_vma *vma;
 	int count = 0;
 
-	list_for_each_entry(vma, &obj->vma_list, vma_link)
+	list_for_each_entry(vma, &obj->vma_list, obj_link)
 		if (drm_mm_node_allocated(&vma->node))
 			count++;
 
@@ -5097,11 +4900,9 @@
 	struct drm_i915_private *dev_priv = o->base.dev->dev_private;
 	struct i915_vma *vma;
 
-	if (!dev_priv->mm.aliasing_ppgtt ||
-	    vm == &dev_priv->mm.aliasing_ppgtt->base)
-		vm = &dev_priv->gtt.base;
+	WARN_ON(vm == &dev_priv->mm.aliasing_ppgtt->base);
 
-	list_for_each_entry(vma, &o->vma_list, vma_link) {
+	list_for_each_entry(vma, &o->vma_list, obj_link) {
 		if (vma->vm == vm)
 			return vma->node.start;
 
@@ -5116,7 +4917,7 @@
 {
 	struct i915_vma *vma;
 
-	list_for_each_entry(vma, &o->vma_list, vma_link)
+	list_for_each_entry(vma, &o->vma_list, obj_link)
 		if (vma->vm == vm && drm_mm_node_allocated(&vma->node))
 			return true;
 
@@ -5127,7 +4928,7 @@
 {
 	struct i915_vma *vma;
 
-	list_for_each_entry(vma, &o->vma_list, vma_link)
+	list_for_each_entry(vma, &o->vma_list, obj_link)
 		if (drm_mm_node_allocated(&vma->node))
 			return true;
 
@@ -5140,13 +4941,11 @@
 	struct drm_i915_private *dev_priv = o->base.dev->dev_private;
 	struct i915_vma *vma;
 
-	if (!dev_priv->mm.aliasing_ppgtt ||
-	    vm == &dev_priv->mm.aliasing_ppgtt->base)
-		vm = &dev_priv->gtt.base;
+	WARN_ON(vm == &dev_priv->mm.aliasing_ppgtt->base);
 
 	BUG_ON(list_empty(&o->vma_list));
 
-	list_for_each_entry(vma, &o->vma_list, vma_link)
+	list_for_each_entry(vma, &o->vma_list, obj_link)
 		if (vma->vm == vm)
 			return vma->node.size;
 
@@ -5165,11 +4964,16 @@
 	if (!i915_gem_shrinker_lock(dev, &unlock))
 		return SHRINK_STOP;
 
-	freed = i915_gem_purge(dev_priv, sc->nr_to_scan);
+	freed = i915_gem_shrink(dev_priv,
+				sc->nr_to_scan,
+				I915_SHRINK_BOUND |
+				I915_SHRINK_UNBOUND |
+				I915_SHRINK_PURGEABLE);
 	if (freed < sc->nr_to_scan)
-		freed += __i915_gem_shrink(dev_priv,
-					   sc->nr_to_scan - freed,
-					   false);
+		freed += i915_gem_shrink(dev_priv,
+					 sc->nr_to_scan - freed,
+					 I915_SHRINK_BOUND |
+					 I915_SHRINK_UNBOUND);
 	if (unlock)
 		mutex_unlock(&dev->struct_mutex);
 
@@ -5184,7 +4988,7 @@
 	struct drm_device *dev = dev_priv->dev;
 	struct drm_i915_gem_object *obj;
 	unsigned long timeout = msecs_to_jiffies(5000) + 1;
-	unsigned long pinned, bound, unbound, freed;
+	unsigned long pinned, bound, unbound, freed_pages;
 	bool was_interruptible;
 	bool unlock;
 
@@ -5201,7 +5005,7 @@
 	was_interruptible = dev_priv->mm.interruptible;
 	dev_priv->mm.interruptible = false;
 
-	freed = i915_gem_shrink_all(dev_priv);
+	freed_pages = i915_gem_shrink_all(dev_priv);
 
 	dev_priv->mm.interruptible = was_interruptible;
 
@@ -5232,14 +5036,15 @@
 	if (unlock)
 		mutex_unlock(&dev->struct_mutex);
 
-	pr_info("Purging GPU memory, %lu bytes freed, %lu bytes still pinned.\n",
-		freed, pinned);
+	if (freed_pages || unbound || bound)
+		pr_info("Purging GPU memory, %lu bytes freed, %lu bytes still pinned.\n",
+			freed_pages << PAGE_SHIFT, pinned);
 	if (unbound || bound)
 		pr_err("%lu and %lu bytes still available in the "
 		       "bound and unbound GPU page lists.\n",
 		       bound, unbound);
 
-	*(unsigned long *)ptr += freed;
+	*(unsigned long *)ptr += freed_pages;
 	return NOTIFY_DONE;
 }
 
@@ -5247,14 +5052,8 @@
 {
 	struct i915_vma *vma;
 
-	/* This WARN has probably outlived its usefulness (callers already
-	 * WARN if they don't find the GGTT vma they expect). When removing,
-	 * remember to remove the pre-check in is_pin_display() as well */
-	if (WARN_ON(list_empty(&obj->vma_list)))
-		return NULL;
-
-	vma = list_first_entry(&obj->vma_list, typeof(*vma), vma_link);
-	if (vma->vm != obj_to_ggtt(obj))
+	vma = list_first_entry(&obj->vma_list, typeof(*vma), obj_link);
+	if (vma->vm != i915_obj_to_ggtt(obj))
 		return NULL;
 
 	return vma;
diff -urN a/drivers/gpu/drm/i915/i915_gem_context.c b/drivers/gpu/drm/i915/i915_gem_context.c
--- a/drivers/gpu/drm/i915/i915_gem_context.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_gem_context.c	2014-11-22 14:37:49.330700418 -0700
@@ -88,65 +88,7 @@
 #include <drm/drmP.h>
 #include <drm/i915_drm.h>
 #include "i915_drv.h"
-
-/* This is a HW constraint. The value below is the largest known requirement
- * I've seen in a spec to date, and that was a workaround for a non-shipping
- * part. It should be safe to decrease this, but it's more future proof as is.
- */
-#define GEN6_CONTEXT_ALIGN (64<<10)
-#define GEN7_CONTEXT_ALIGN 4096
-
-static void do_ppgtt_cleanup(struct i915_hw_ppgtt *ppgtt)
-{
-	struct drm_device *dev = ppgtt->base.dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct i915_address_space *vm = &ppgtt->base;
-
-	if (ppgtt == dev_priv->mm.aliasing_ppgtt ||
-	    (list_empty(&vm->active_list) && list_empty(&vm->inactive_list))) {
-		ppgtt->base.cleanup(&ppgtt->base);
-		return;
-	}
-
-	/*
-	 * Make sure vmas are unbound before we take down the drm_mm
-	 *
-	 * FIXME: Proper refcounting should take care of this, this shouldn't be
-	 * needed at all.
-	 */
-	if (!list_empty(&vm->active_list)) {
-		struct i915_vma *vma;
-
-		list_for_each_entry(vma, &vm->active_list, mm_list)
-			if (WARN_ON(list_empty(&vma->vma_link) ||
-				    list_is_singular(&vma->vma_link)))
-				break;
-
-		i915_gem_evict_vm(&ppgtt->base, true);
-	} else {
-		i915_gem_retire_requests(dev);
-		i915_gem_evict_vm(&ppgtt->base, false);
-	}
-
-	ppgtt->base.cleanup(&ppgtt->base);
-}
-
-static void ppgtt_release(struct kref *kref)
-{
-	struct i915_hw_ppgtt *ppgtt =
-		container_of(kref, struct i915_hw_ppgtt, ref);
-
-	do_ppgtt_cleanup(ppgtt);
-	kfree(ppgtt);
-}
-
-static size_t get_context_alignment(struct drm_device *dev)
-{
-	if (IS_GEN6(dev))
-		return GEN6_CONTEXT_ALIGN;
-
-	return GEN7_CONTEXT_ALIGN;
-}
+#include "i915_trace.h"
 
 static int get_context_size(struct drm_device *dev)
 {
@@ -155,6 +97,9 @@
 	u32 reg;
 
 	switch (INTEL_INFO(dev)->gen) {
+	case 5:
+		ret = ILK_CXT_TOTAL_SIZE;
+		break;
 	case 6:
 		reg = I915_READ(CXT_SIZE);
 		ret = GEN6_CXT_TOTAL_SIZE(reg) * 64;
@@ -170,39 +115,44 @@
 		ret = GEN8_CXT_TOTAL_SIZE;
 		break;
 	default:
-		BUG();
+		WARN(1,
+		     "context size not known for gen %d\n",
+		     INTEL_INFO(dev)->gen);
+		ret = -1;
+		break;
 	}
 
 	return ret;
 }
 
-void i915_gem_context_free(struct kref *ctx_ref)
+void __i915_gem_context_free(struct kref *ctx_ref)
 {
-	struct intel_context *ctx = container_of(ctx_ref,
-						   typeof(*ctx), ref);
-	struct i915_hw_ppgtt *ppgtt = NULL;
-
-	if (ctx->legacy_hw_ctx.rcs_state) {
-		/* We refcount even the aliasing PPGTT to keep the code symmetric */
-		if (USES_PPGTT(ctx->legacy_hw_ctx.rcs_state->base.dev))
-			ppgtt = ctx_to_ppgtt(ctx);
-	}
-
-	if (ppgtt)
-		kref_put(&ppgtt->ref, ppgtt_release);
-	if (ctx->legacy_hw_ctx.rcs_state)
-		drm_gem_object_unreference(&ctx->legacy_hw_ctx.rcs_state->base);
+	struct intel_context *ctx =
+		container_of(ctx_ref, typeof(*ctx), ref);
+	struct intel_engine_cs *engine;
+	int i;
+
+	DRM_DEBUG_DRIVER("HW context %d freed\n", ctx->user_handle);
+	trace_i915_context_free(ctx);
+
+	for_each_engine(engine, ctx->i915, i)
+		engine->free_context(engine, ctx);
+
+	i915_vm_put(&ctx->ppgtt->base);
+
 	list_del(&ctx->link);
 	kfree(ctx);
 }
 
-static struct drm_i915_gem_object *
+struct drm_i915_gem_object *
 i915_gem_alloc_context_obj(struct drm_device *dev, size_t size)
 {
 	struct drm_i915_gem_object *obj;
 	int ret;
 
-	obj = i915_gem_alloc_object(dev, size);
+	obj = i915_gem_object_create_stolen(dev, size);
+	if (obj == NULL)
+		obj = i915_gem_alloc_object(dev, size);
 	if (obj == NULL)
 		return ERR_PTR(-ENOMEM);
 
@@ -226,49 +176,38 @@
 	return obj;
 }
 
-static struct i915_hw_ppgtt *
-create_vm_for_ctx(struct drm_device *dev, struct intel_context *ctx)
-{
-	struct i915_hw_ppgtt *ppgtt;
-	int ret;
-
-	ppgtt = kzalloc(sizeof(*ppgtt), GFP_KERNEL);
-	if (!ppgtt)
-		return ERR_PTR(-ENOMEM);
-
-	ret = i915_gem_init_ppgtt(dev, ppgtt);
-	if (ret) {
-		kfree(ppgtt);
-		return ERR_PTR(ret);
-	}
-
-	ppgtt->ctx = ctx;
-	return ppgtt;
-}
-
 static struct intel_context *
-__create_hw_context(struct drm_device *dev,
-		  struct drm_i915_file_private *file_priv)
+i915_gem_create_context(struct drm_device *dev,
+			struct drm_i915_file_private *file_priv,
+			bool is_default)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = to_i915(dev);
 	struct intel_context *ctx;
 	int ret;
 
+	BUG_ON(!mutex_is_locked(&dev->struct_mutex));
+
 	ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);
 	if (ctx == NULL)
 		return ERR_PTR(-ENOMEM);
 
 	kref_init(&ctx->ref);
 	list_add_tail(&ctx->link, &dev_priv->context_list);
+	ctx->i915 = dev_priv;
 
 	if (dev_priv->hw_context_size) {
-		struct drm_i915_gem_object *obj =
-				i915_gem_alloc_context_obj(dev, dev_priv->hw_context_size);
+		struct drm_i915_gem_object *obj;
+
+		if (is_default) {
+			obj = RCS_ENGINE(dev_priv)->default_context->ring[RCS].state;
+			drm_gem_object_reference(&obj->base);
+		} else
+			obj = i915_gem_alloc_context_obj(dev, dev_priv->hw_context_size);
 		if (IS_ERR(obj)) {
 			ret = PTR_ERR(obj);
-			goto err_out;
+			goto err;
 		}
-		ctx->legacy_hw_ctx.rcs_state = obj;
+		ctx->ring[RCS].state = obj;
 	}
 
 	/* Default context will never have a file_priv */
@@ -276,10 +215,12 @@
 		ret = idr_alloc(&file_priv->context_idr, ctx,
 				DEFAULT_CONTEXT_HANDLE, 0, GFP_KERNEL);
 		if (ret < 0)
-			goto err_out;
+			goto err;
 	} else
 		ret = DEFAULT_CONTEXT_HANDLE;
 
+	BUG_ON(is_default && ret != DEFAULT_CONTEXT_HANDLE);
+
 	ctx->file_priv = file_priv;
 	ctx->user_handle = ret;
 	/* NB: Mark all slices as needing a remap so that when the context first
@@ -287,137 +228,48 @@
 	 * is no remap info, it will be a NOP. */
 	ctx->remap_slice = (1 << NUM_L3_SLICES(dev)) - 1;
 
-	return ctx;
+	ctx->hang_stats.ban_period_seconds = DRM_I915_CTX_BAN_PERIOD;
 
-err_out:
-	i915_gem_context_unreference(ctx);
-	return ERR_PTR(ret);
-}
-
-/**
- * The default context needs to exist per ring that uses contexts. It stores the
- * context state of the GPU for applications that don't utilize HW contexts, as
- * well as an idle case.
- */
-static struct intel_context *
-i915_gem_create_context(struct drm_device *dev,
-			struct drm_i915_file_private *file_priv,
-			bool create_vm)
-{
-	const bool is_global_default_ctx = file_priv == NULL;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_context *ctx;
-	int ret = 0;
-
-	BUG_ON(!mutex_is_locked(&dev->struct_mutex));
-
-	ctx = __create_hw_context(dev, file_priv);
-	if (IS_ERR(ctx))
-		return ctx;
-
-	if (is_global_default_ctx && ctx->legacy_hw_ctx.rcs_state) {
-		/* We may need to do things with the shrinker which
-		 * require us to immediately switch back to the default
-		 * context. This can cause a problem as pinning the
-		 * default context also requires GTT space which may not
-		 * be available. To avoid this we always pin the default
-		 * context.
-		 */
-		ret = i915_gem_obj_ggtt_pin(ctx->legacy_hw_ctx.rcs_state,
-					    get_context_alignment(dev), 0);
-		if (ret) {
-			DRM_DEBUG_DRIVER("Couldn't pin %d\n", ret);
-			goto err_destroy;
-		}
-	}
+	if (USES_FULL_PPGTT(dev)) {
+		struct i915_hw_ppgtt *ppgtt;
 
-	if (create_vm) {
-		struct i915_hw_ppgtt *ppgtt = create_vm_for_ctx(dev, ctx);
+		if (is_default) {
+			ppgtt = RCS_ENGINE(dev_priv)->default_context->ppgtt;
+			i915_vm_get(&ppgtt->base);
+		} else
+			ppgtt = i915_ppgtt_create(dev);
 
-		if (IS_ERR_OR_NULL(ppgtt)) {
+		if (IS_ERR(ppgtt)) {
 			DRM_DEBUG_DRIVER("PPGTT setup failed (%ld)\n",
 					 PTR_ERR(ppgtt));
+			idr_remove(&file_priv->context_idr, ctx->user_handle);
 			ret = PTR_ERR(ppgtt);
-			goto err_unpin;
-		} else
-			ctx->vm = &ppgtt->base;
+			goto err;
+		}
 
-		/* This case is reserved for the global default context and
-		 * should only happen once. */
-		if (is_global_default_ctx) {
-			if (WARN_ON(dev_priv->mm.aliasing_ppgtt)) {
-				ret = -EEXIST;
-				goto err_unpin;
-			}
+		ctx->ppgtt = ppgtt;
+	}
 
-			dev_priv->mm.aliasing_ppgtt = ppgtt;
-		}
-	} else if (USES_PPGTT(dev)) {
-		/* For platforms which only have aliasing PPGTT, we fake the
-		 * address space and refcounting. */
-		ctx->vm = &dev_priv->mm.aliasing_ppgtt->base;
-		kref_get(&dev_priv->mm.aliasing_ppgtt->ref);
-	} else
-		ctx->vm = &dev_priv->gtt.base;
+	trace_i915_context_create(ctx);
 
 	return ctx;
 
-err_unpin:
-	if (is_global_default_ctx && ctx->legacy_hw_ctx.rcs_state)
-		i915_gem_object_ggtt_unpin(ctx->legacy_hw_ctx.rcs_state);
-err_destroy:
+err:
 	i915_gem_context_unreference(ctx);
 	return ERR_PTR(ret);
 }
 
-void i915_gem_context_reset(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	int i;
-
-	/* Prevent the hardware from restoring the last context (which hung) on
-	 * the next switch */
-	for (i = 0; i < I915_NUM_RINGS; i++) {
-		struct intel_engine_cs *ring = &dev_priv->ring[i];
-		struct intel_context *dctx = ring->default_context;
-		struct intel_context *lctx = ring->last_context;
-
-		/* Do a fake switch to the default context */
-		if (lctx == dctx)
-			continue;
-
-		if (!lctx)
-			continue;
-
-		if (dctx->legacy_hw_ctx.rcs_state && i == RCS) {
-			WARN_ON(i915_gem_obj_ggtt_pin(dctx->legacy_hw_ctx.rcs_state,
-						      get_context_alignment(dev), 0));
-			/* Fake a finish/inactive */
-			dctx->legacy_hw_ctx.rcs_state->base.write_domain = 0;
-			dctx->legacy_hw_ctx.rcs_state->active = 0;
-		}
-
-		if (lctx->legacy_hw_ctx.rcs_state && i == RCS)
-			i915_gem_object_ggtt_unpin(lctx->legacy_hw_ctx.rcs_state);
-
-		i915_gem_context_unreference(lctx);
-		i915_gem_context_reference(dctx);
-		ring->last_context = dctx;
-	}
-}
-
 int i915_gem_context_init(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_context *ctx;
 	int i;
 
-	/* Init should only be called once per module load. Eventually the
-	 * restriction on the context_disabled check can be loosened. */
-	if (WARN_ON(dev_priv->ring[RCS].default_context))
-		return 0;
-
-	if (HAS_HW_CONTEXTS(dev)) {
+	if (RCS_ENGINE(dev_priv)->execlists_enabled) {
+		/* NB: intentionally left blank. We will allocate our own
+		 * backing objects as we need them, thank you very much */
+		dev_priv->hw_context_size = 0;
+	} else if (HAS_HW_CONTEXTS(dev)) {
 		dev_priv->hw_context_size = round_up(get_context_size(dev), 4096);
 		if (dev_priv->hw_context_size > (1<<20)) {
 			DRM_DEBUG_DRIVER("Disabling HW Contexts; invalid size %d\n",
@@ -426,86 +278,106 @@
 		}
 	}
 
-	ctx = i915_gem_create_context(dev, NULL, USES_PPGTT(dev));
+	/**
+	 * The default context needs to exist per ring that uses contexts.
+	 * It stores the context state of the GPU for applications that don't
+	 * utilize HW contexts or per-process VM, as well as an idle case.
+	 */
+	ctx = i915_gem_create_context(dev, NULL, false);
 	if (IS_ERR(ctx)) {
 		DRM_ERROR("Failed to create default global context (error %ld)\n",
 			  PTR_ERR(ctx));
 		return PTR_ERR(ctx);
 	}
 
-	/* NB: RCS will hold a ref for all rings */
-	for (i = 0; i < I915_NUM_RINGS; i++)
-		dev_priv->ring[i].default_context = ctx;
+	/* We may need to do things with the shrinker which
+	 * require us to immediately switch back to the default
+	 * context. This can cause a problem as pinning the
+	 * default context also requires GTT space which may not
+	 * be available. To avoid this we always pin the default
+	 * context.
+	 */
+	for (i = 0; i < I915_NUM_ENGINES; i++) {
+		struct intel_engine_cs *engine = &dev_priv->engine[i];
+		struct intel_ringbuffer *ring;
+
+		if (engine->i915 == NULL)
+			continue;
+
+		engine->default_context = ctx;
+		i915_gem_context_reference(ctx);
 
-	DRM_DEBUG_DRIVER("%s context support initialized\n", dev_priv->hw_context_size ? "HW" : "fake");
+		ring = engine->pin_context(engine, ctx);
+		if (IS_ERR(ring)) {
+			DRM_ERROR("Failed to pin global default context (%s)\n",
+				  engine->name);
+			i915_gem_context_unreference(ctx);
+			return PTR_ERR(ring);
+		}
+	}
+	i915_gem_context_unreference(ctx);
+
+	DRM_DEBUG_DRIVER("%s context support initialized\n",
+			 RCS_ENGINE(dev_priv)->execlists_enabled ? "LR" :
+			 dev_priv->hw_context_size ? "HW" : "fake");
 	return 0;
 }
 
 void i915_gem_context_fini(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_context *dctx = dev_priv->ring[RCS].default_context;
+	struct intel_engine_cs *engine;
 	int i;
 
-	if (dctx->legacy_hw_ctx.rcs_state) {
-		/* The only known way to stop the gpu from accessing the hw context is
-		 * to reset it. Do this as the very last operation to avoid confusing
-		 * other code, leading to spurious errors. */
+	if (dev_priv->hw_context_size)
+		/* The only known way to stop the gpu from accessing
+		 * the hw context is to reset it. Do this as the very
+		 * last operation to avoid confusing other code,
+		 * leading to spurious errors.
+		 */
 		intel_gpu_reset(dev);
 
-		/* When default context is created and switched to, base object refcount
-		 * will be 2 (+1 from object creation and +1 from do_switch()).
-		 * i915_gem_context_fini() will be called after gpu_idle() has switched
-		 * to default context. So we need to unreference the base object once
-		 * to offset the do_switch part, so that i915_gem_context_unreference()
-		 * can then free the base object correctly. */
-		WARN_ON(!dev_priv->ring[RCS].last_context);
-		if (dev_priv->ring[RCS].last_context == dctx) {
-			/* Fake switch to NULL context */
-			WARN_ON(dctx->legacy_hw_ctx.rcs_state->active);
-			i915_gem_object_ggtt_unpin(dctx->legacy_hw_ctx.rcs_state);
-			i915_gem_context_unreference(dctx);
-			dev_priv->ring[RCS].last_context = NULL;
-		}
-
-		i915_gem_object_ggtt_unpin(dctx->legacy_hw_ctx.rcs_state);
-	}
-
-	for (i = 0; i < I915_NUM_RINGS; i++) {
-		struct intel_engine_cs *ring = &dev_priv->ring[i];
-
-		if (ring->last_context)
-			i915_gem_context_unreference(ring->last_context);
-
-		ring->default_context = NULL;
-		ring->last_context = NULL;
+	for_each_engine(engine, dev_priv, i) {
+		engine->unpin_context(engine, engine->default_context);
+		i915_gem_context_unreference(engine->default_context);
+		engine->default_context = NULL;
 	}
-
-	i915_gem_context_unreference(dctx);
 }
 
 int i915_gem_context_enable(struct drm_i915_private *dev_priv)
 {
-	struct intel_engine_cs *ring;
+	struct intel_engine_cs *engine;
 	int ret, i;
 
-	/* This is the only place the aliasing PPGTT gets enabled, which means
-	 * it has to happen before we bail on reset */
-	if (dev_priv->mm.aliasing_ppgtt) {
-		struct i915_hw_ppgtt *ppgtt = dev_priv->mm.aliasing_ppgtt;
-		ppgtt->enable(ppgtt);
-	}
-
-	/* FIXME: We should make this work, even in reset */
-	if (i915_reset_in_progress(&dev_priv->gpu_error))
-		return 0;
-
-	BUG_ON(!dev_priv->ring[RCS].default_context);
+	for_each_engine(engine, dev_priv, i) {
+		struct intel_context *ctx = engine->default_context;
+		struct i915_gem_request *rq;
+
+		if (HAS_L3_DPF(dev_priv))
+			ctx->remap_slice = (1 << NUM_L3_SLICES(dev_priv)) - 1;
+
+		rq = i915_request_create(ctx, engine);
+		if (IS_ERR(rq)) {
+			ret = PTR_ERR(rq);
+			goto err;
+		}
 
-	for_each_ring(ring, dev_priv, i) {
-		ret = i915_switch_context(ring, ring->default_context);
-		if (ret)
+		ret = 0;
+		/*
+		 * Workarounds applied in this fn are part of register state context,
+		 * they need to be re-initialized followed by gpu reset, suspend/resume,
+		 * module reload.
+		 */
+		if (engine->init_context)
+			ret = engine->init_context(rq);
+		if (ret == 0)
+			ret = i915_request_commit(rq);
+		i915_request_put(rq);
+		if (ret) {
+err:
+			DRM_ERROR("failed to enabled contexts (%s): %d\n", engine->name, ret);
 			return ret;
+		}
 	}
 
 	return 0;
@@ -515,7 +387,42 @@
 {
 	struct intel_context *ctx = p;
 
+	if (ctx->ppgtt && !i915_gem_context_is_default(ctx)) {
+		struct list_head *list;
+		struct i915_vma *vma;
+
+		/* Decouple the remaining vma to keep the next lookup fast */
+		list = &ctx->ppgtt->base.vma_list;
+		while (!list_empty(list)) {
+			vma = list_first_entry(list, typeof(*vma), vm_link);
+			list_del_init(&vma->vm_link);
+			list_del_init(&vma->obj_link);
+			i915_vma_put(vma);
+		}
+
+		/* Drop active references to this vm upon retire */
+		ctx->ppgtt->base.closed = true;
+
+		/* Drop all inactive references (via vma->vm reference) */
+		list = &ctx->ppgtt->base.inactive_list;
+		while (!list_empty(list)) {
+			struct drm_i915_gem_object *obj;
+			int ret;
+
+			vma = list_first_entry(list, typeof(*vma), mm_list);
+			obj = vma->obj;
+
+			drm_gem_object_reference(&obj->base);
+			ret = i915_vma_unbind(vma);
+			drm_gem_object_unreference(&obj->base);
+			if (WARN_ON(ret))
+				break;
+		}
+	}
+
+	ctx->file_priv = NULL;
 	i915_gem_context_unreference(ctx);
+
 	return 0;
 }
 
@@ -527,7 +434,7 @@
 	idr_init(&file_priv->context_idr);
 
 	mutex_lock(&dev->struct_mutex);
-	ctx = i915_gem_create_context(dev, file_priv, USES_FULL_PPGTT(dev));
+	ctx = i915_gem_create_context(dev, file_priv, true);
 	mutex_unlock(&dev->struct_mutex);
 
 	if (IS_ERR(ctx)) {
@@ -559,136 +466,173 @@
 }
 
 static inline int
-mi_set_context(struct intel_engine_cs *ring,
-	       struct intel_context *new_context,
-	       u32 hw_flags)
+mi_set_context(struct i915_gem_request *rq,
+	       struct intel_engine_context *new_context,
+	       u32 flags)
 {
-	int ret;
+	struct intel_ringbuffer *ring;
+	int len;
 
 	/* w/a: If Flush TLB Invalidation Mode is enabled, driver must do a TLB
 	 * invalidation prior to MI_SET_CONTEXT. On GEN6 we don't set the value
-	 * explicitly, so we rely on the value at ring init, stored in
+	 * explicitly, so we rely on the value at engine init, stored in
 	 * itlb_before_ctx_switch.
 	 */
-	if (IS_GEN6(ring->dev)) {
-		ret = ring->flush(ring, I915_GEM_GPU_DOMAINS, 0);
+	if (IS_GEN6(rq->i915))
+		rq->pending_flush |= I915_INVALIDATE_CACHES;
+	if ((flags & MI_RESTORE_INHIBIT) == 0) {
+		int ret = i915_request_emit_flush(rq, I915_COMMAND_BARRIER);
 		if (ret)
 			return ret;
 	}
 
-	ret = intel_ring_begin(ring, 6);
-	if (ret)
-		return ret;
+	len = 3;
+	switch (INTEL_INFO(rq->i915)->gen) {
+	case 8:
+	case 7:
+	case 5: len += 2;
+		break;
+	}
+
+	ring = intel_ring_begin(rq, len);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
-	/* WaProgramMiArbOnOffAroundMiSetContext:ivb,vlv,hsw,bdw,chv */
-	if (INTEL_INFO(ring->dev)->gen >= 7)
+	switch (INTEL_INFO(rq->i915)->gen) {
+	case 8:
+	case 7:
+		/* WaProgramMiArbOnOffAroundMiSetContext:ivb,vlv,hsw,bdw,chv */
 		intel_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_DISABLE);
-	else
-		intel_ring_emit(ring, MI_NOOP);
+		break;
+	case 5:
+		intel_ring_emit(ring, MI_SUSPEND_FLUSH | MI_SUSPEND_FLUSH_EN);
+		break;
+	}
 
-	intel_ring_emit(ring, MI_NOOP);
 	intel_ring_emit(ring, MI_SET_CONTEXT);
-	intel_ring_emit(ring, i915_gem_obj_ggtt_offset(new_context->legacy_hw_ctx.rcs_state) |
+	intel_ring_emit(ring,
+			i915_gem_obj_ggtt_offset(new_context->state) |
 			MI_MM_SPACE_GTT |
-			MI_SAVE_EXT_STATE_EN |
-			MI_RESTORE_EXT_STATE_EN |
-			hw_flags);
+			flags);
 	/*
 	 * w/a: MI_SET_CONTEXT must always be followed by MI_NOOP
 	 * WaMiSetContext_Hang:snb,ivb,vlv
 	 */
 	intel_ring_emit(ring, MI_NOOP);
 
-	if (INTEL_INFO(ring->dev)->gen >= 7)
+	switch (INTEL_INFO(rq->i915)->gen) {
+	case 8:
+	case 7:
 		intel_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_ENABLE);
-	else
-		intel_ring_emit(ring, MI_NOOP);
+		break;
+	case 5:
+		intel_ring_emit(ring, MI_SUSPEND_FLUSH);
+		break;
+	}
 
 	intel_ring_advance(ring);
 
-	return ret;
+	rq->pending_flush &= ~I915_COMMAND_BARRIER;
+	return 0;
 }
 
-static int do_switch(struct intel_engine_cs *ring,
-		     struct intel_context *to)
+static int l3_remap(struct i915_gem_request *rq, int slice)
 {
-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
-	struct intel_context *from = ring->last_context;
-	struct i915_hw_ppgtt *ppgtt = ctx_to_ppgtt(to);
-	u32 hw_flags = 0;
-	bool uninitialized = false;
-	int ret, i;
-
-	if (from != NULL && ring == &dev_priv->ring[RCS]) {
-		BUG_ON(from->legacy_hw_ctx.rcs_state == NULL);
-		BUG_ON(!i915_gem_obj_is_pinned(from->legacy_hw_ctx.rcs_state));
-	}
+	const u32 reg_base = GEN7_L3LOG_BASE + (slice * 0x200);
+	const u32 *remap_info;
+	struct intel_ringbuffer *ring;
+	int i;
 
-	if (from == to && !to->remap_slice)
+	remap_info = rq->i915->l3_parity.remap_info[slice];
+	if (remap_info == NULL)
 		return 0;
 
-	/* Trying to pin first makes error handling easier. */
-	if (ring == &dev_priv->ring[RCS]) {
-		ret = i915_gem_obj_ggtt_pin(to->legacy_hw_ctx.rcs_state,
-					    get_context_alignment(ring->dev), 0);
-		if (ret)
-			return ret;
-	}
+	ring = intel_ring_begin(rq, GEN7_L3LOG_SIZE / 4 * 3);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
 	/*
-	 * Pin can switch back to the default context if we end up calling into
-	 * evict_everything - as a last ditch gtt defrag effort that also
-	 * switches to the default context. Hence we need to reload from here.
+	 * Note: We do not worry about the concurrent register cacheline hang
+	 * here because no other code should access these registers other than
+	 * at initialization time.
 	 */
-	from = ring->last_context;
-
-	if (USES_FULL_PPGTT(ring->dev)) {
-		ret = ppgtt->switch_mm(ppgtt, ring, false);
-		if (ret)
-			goto unpin_out;
+	for (i = 0; i < GEN7_L3LOG_SIZE; i += 4) {
+		intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
+		intel_ring_emit(ring, reg_base + i);
+		intel_ring_emit(ring, remap_info[i/4]);
 	}
 
-	if (ring != &dev_priv->ring[RCS]) {
-		if (from)
-			i915_gem_context_unreference(from);
-		goto done;
-	}
+	intel_ring_advance(ring);
+	return 0;
+}
 
-	/*
-	 * Clear this page out of any CPU caches for coherent swap-in/out. Note
-	 * that thanks to write = false in this call and us not setting any gpu
-	 * write domains when putting a context object onto the active list
-	 * (when switching away from it), this won't block.
+/**
+ * i915_request_switch_context() - perform a GPU context switch.
+ * @rq: request and ring/ctx for which we'll execute the context switch
+ *
+ * The context life cycle is simple. The context refcount is incremented and
+ * decremented by 1 and create and destroy. If the context is in use by the GPU,
+ * it will have a refoucnt > 1. This allows us to destroy the context abstract
+ * object while letting the normal object tracking destroy the backing BO.
+ */
+int i915_request_switch_context(struct i915_gem_request *rq)
+{
+	struct intel_context *to = rq->ctx;
+	struct intel_context *from = rq->engine->last_context;
+	int ret, i;
+
+	lockdep_assert_held(&rq->i915->dev->struct_mutex);
+
+	if (from == to && !to->remap_slice)
+		return 0;
+
+	/* With execlists enabled, the ring, vm and logical state are
+	 * interwined and we do not need to explicitly load the mm or
+	 * logical state as it is loaded along with the LRCA.
 	 *
-	 * XXX: We need a real interface to do this instead of trickery.
+	 * But we still want to pin the state (for global usage tracking)
+	 * whilst in use and reload the l3 mapping if it has changed.
 	 */
-	ret = i915_gem_object_set_to_gtt_domain(to->legacy_hw_ctx.rcs_state, false);
-	if (ret)
-		goto unpin_out;
+	if (!rq->engine->execlists_enabled) {
+		struct intel_engine_context *ctx = &to->ring[rq->engine->id];
 
-	if (!to->legacy_hw_ctx.rcs_state->has_global_gtt_mapping) {
-		struct i915_vma *vma = i915_gem_obj_to_vma(to->legacy_hw_ctx.rcs_state,
-							   &dev_priv->gtt.base);
-		vma->bind_vma(vma, to->legacy_hw_ctx.rcs_state->cache_level, GLOBAL_BIND);
-	}
+		if (ctx->state != (from ? from->ring[rq->engine->id].state : NULL)) {
+			u32 flags;
 
-	if (!to->legacy_hw_ctx.initialized || i915_gem_context_is_default(to))
-		hw_flags |= MI_RESTORE_INHIBIT;
+			flags = 0;
+			if (!ctx->initialized || i915_gem_context_is_default(to))
+				flags |= MI_RESTORE_INHIBIT;
+
+			/* These flags are for resource streamer on HSW+ */
+			if (!IS_HASWELL(rq->i915) && INTEL_INFO(rq->i915)->gen < 8) {
+				if (ctx->initialized)
+					flags |= MI_RESTORE_EXT_STATE_EN;
+				flags |= MI_SAVE_EXT_STATE_EN;
+			}
 
-	ret = mi_set_context(ring, to, hw_flags);
-	if (ret)
-		goto unpin_out;
+			trace_i915_gem_ring_switch_context(rq->engine, to, flags);
+			ret = mi_set_context(rq, ctx, flags);
+			if (ret)
+				return ret;
+
+			rq->pending_flush &= ~I915_COMMAND_BARRIER;
+		}
+
+		if (to->ppgtt != (from ? from->ppgtt : NULL)) {
+			trace_i915_gem_request_switch_mm(rq);
+			ret = to->ppgtt->switch_mm(rq, to->ppgtt);
+			if (ret)
+				return ret;
+		}
+	}
 
 	for (i = 0; i < MAX_L3_SLICES; i++) {
-		if (!(to->remap_slice & (1<<i)))
+		if (!(to->remap_slice & (1 << i)))
 			continue;
 
-		ret = i915_gem_l3_remap(ring, i);
 		/* If it failed, try again next round */
-		if (ret)
-			DRM_DEBUG_DRIVER("L3 remapping failed\n");
-		else
-			to->remap_slice &= ~(1<<i);
+		if (l3_remap(rq, i) == 0)
+			rq->remap_l3 |= 1 << i;
 	}
 
 	/* The backing object for the context is done after switching to the
@@ -697,78 +641,36 @@
 	 * is a bit suboptimal because the retiring can occur simply after the
 	 * MI_SET_CONTEXT instead of when the next seqno has completed.
 	 */
-	if (from != NULL) {
-		from->legacy_hw_ctx.rcs_state->base.read_domains = I915_GEM_DOMAIN_INSTRUCTION;
-		i915_vma_move_to_active(i915_gem_obj_to_ggtt(from->legacy_hw_ctx.rcs_state), ring);
-		/* As long as MI_SET_CONTEXT is serializing, ie. it flushes the
-		 * whole damn pipeline, we don't need to explicitly mark the
-		 * object dirty. The only exception is that the context must be
-		 * correct in case the object gets swapped out. Ideally we'd be
-		 * able to defer doing this until we know the object would be
-		 * swapped, but there is no way to do that yet.
-		 */
-		from->legacy_hw_ctx.rcs_state->dirty = 1;
-		BUG_ON(from->legacy_hw_ctx.rcs_state->ring != ring);
-
-		/* obj is kept alive until the next request by its active ref */
-		i915_gem_object_ggtt_unpin(from->legacy_hw_ctx.rcs_state);
-		i915_gem_context_unreference(from);
-	}
-
-	uninitialized = !to->legacy_hw_ctx.initialized && from == NULL;
-	to->legacy_hw_ctx.initialized = true;
-
-done:
-	i915_gem_context_reference(to);
-	ring->last_context = to;
-
-	if (uninitialized) {
-		ret = i915_gem_render_state_init(ring);
-		if (ret)
-			DRM_ERROR("init render state: %d\n", ret);
-	}
+	if (from)
+		rq->engine->add_context(rq, from);
 
+	rq->has_ctx_switch = true;
 	return 0;
-
-unpin_out:
-	if (ring->id == RCS)
-		i915_gem_object_ggtt_unpin(to->legacy_hw_ctx.rcs_state);
-	return ret;
 }
 
 /**
- * i915_switch_context() - perform a GPU context switch.
- * @ring: ring for which we'll execute the context switch
- * @to: the context to switch to
- *
- * The context life cycle is simple. The context refcount is incremented and
- * decremented by 1 and create and destroy. If the context is in use by the GPU,
- * it will have a refoucnt > 1. This allows us to destroy the context abstract
- * object while letting the normal object tracking destroy the backing BO.
+ * i915_request_switch_context__commit() - commit the context sitch
+ * @rq: request for which we have executed the context switch
  */
-int i915_switch_context(struct intel_engine_cs *ring,
-			struct intel_context *to)
+void i915_request_switch_context__commit(struct i915_gem_request *rq)
 {
-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
+	lockdep_assert_held(&rq->i915->dev->struct_mutex);
 
-	WARN_ON(!mutex_is_locked(&dev_priv->dev->struct_mutex));
+	if (!rq->has_ctx_switch)
+		return;
 
-	if (to->legacy_hw_ctx.rcs_state == NULL) { /* We have the fake context */
-		if (to != ring->last_context) {
-			i915_gem_context_reference(to);
-			if (ring->last_context)
-				i915_gem_context_unreference(ring->last_context);
-			ring->last_context = to;
-		}
-		return 0;
-	}
+	rq->ctx->remap_slice &= ~rq->remap_l3;
+	rq->ctx->ring[rq->engine->id].initialized = true;
 
-	return do_switch(ring, to);
+	rq->has_ctx_switch = false;
 }
 
-static bool hw_context_enabled(struct drm_device *dev)
+static bool contexts_enabled(struct drm_i915_private *dev_priv)
 {
-	return to_i915(dev)->hw_context_size;
+	if (RCS_ENGINE(dev_priv)->execlists_enabled)
+		return true;
+
+	return dev_priv->hw_context_size;
 }
 
 int i915_gem_context_create_ioctl(struct drm_device *dev, void *data,
@@ -779,14 +681,14 @@
 	struct intel_context *ctx;
 	int ret;
 
-	if (!hw_context_enabled(dev))
+	if (!contexts_enabled(to_i915(dev)))
 		return -ENODEV;
 
 	ret = i915_mutex_lock_interruptible(dev);
 	if (ret)
 		return ret;
 
-	ctx = i915_gem_create_context(dev, file_priv, USES_FULL_PPGTT(dev));
+	ctx = i915_gem_create_context(dev, file_priv, false);
 	mutex_unlock(&dev->struct_mutex);
 	if (IS_ERR(ctx))
 		return PTR_ERR(ctx);
@@ -819,9 +721,142 @@
 	}
 
 	idr_remove(&ctx->file_priv->context_idr, ctx->user_handle);
-	i915_gem_context_unreference(ctx);
+	context_idr_cleanup(ctx->user_handle, ctx, NULL);
 	mutex_unlock(&dev->struct_mutex);
 
 	DRM_DEBUG_DRIVER("HW context %d destroyed\n", args->ctx_id);
 	return 0;
 }
+
+int i915_gem_context_getparam_ioctl(struct drm_device *dev, void *data,
+				    struct drm_file *file)
+{
+	struct drm_i915_file_private *file_priv = file->driver_priv;
+	struct drm_i915_gem_context_param *args = data;
+	struct intel_context *ctx;
+	int ret;
+
+	ret = i915_mutex_lock_interruptible(dev);
+	if (ret)
+		return ret;
+
+	ctx = i915_gem_context_get(file_priv, args->ctx_id);
+	if (IS_ERR(ctx)) {
+		mutex_unlock(&dev->struct_mutex);
+		return PTR_ERR(ctx);
+	}
+
+	args->size = 0;
+	switch (args->param) {
+	case I915_CONTEXT_PARAM_BAN_PERIOD:
+		args->value = ctx->hang_stats.ban_period_seconds;
+		break;
+	default:
+		ret = -EINVAL;
+		break;
+	}
+	mutex_unlock(&dev->struct_mutex);
+
+	return ret;
+}
+
+int i915_gem_context_setparam_ioctl(struct drm_device *dev, void *data,
+				    struct drm_file *file)
+{
+	struct drm_i915_file_private *file_priv = file->driver_priv;
+	struct drm_i915_gem_context_param *args = data;
+	struct intel_context *ctx;
+	int ret;
+
+	ret = i915_mutex_lock_interruptible(dev);
+	if (ret)
+		return ret;
+
+	ctx = i915_gem_context_get(file_priv, args->ctx_id);
+	if (IS_ERR(ctx)) {
+		mutex_unlock(&dev->struct_mutex);
+		return PTR_ERR(ctx);
+	}
+
+	switch (args->param) {
+	case I915_CONTEXT_PARAM_BAN_PERIOD:
+		if (args->size)
+			ret = -EINVAL;
+		else if (args->value < ctx->hang_stats.ban_period_seconds &&
+			 !capable(CAP_SYS_ADMIN))
+			ret = -EPERM;
+		else
+			ctx->hang_stats.ban_period_seconds = args->value;
+		break;
+	default:
+		ret = -EINVAL;
+		break;
+	}
+	mutex_unlock(&dev->struct_mutex);
+
+	return ret;
+}
+
+int i915_gem_context_dump_ioctl(struct drm_device *dev, void *data,
+				struct drm_file *file)
+{
+	struct drm_i915_file_private *file_priv = file->driver_priv;
+	struct drm_i915_gem_context_dump *args = data;
+	struct intel_context *ctx;
+	int ret;
+
+	ret = i915_mutex_lock_interruptible(dev);
+	if (ret)
+		return ret;
+
+	ctx = i915_gem_context_get(file_priv, args->ctx_id);
+	if (IS_ERR(ctx)) {
+		mutex_unlock(&dev->struct_mutex);
+		return PTR_ERR(ctx);
+	}
+
+	if (args->size >= ctx->ring[RCS].state->base.size) {
+		struct intel_engine_cs *rcs = RCS_ENGINE(dev);
+		struct drm_i915_gem_object *obj = ctx->ring[RCS].state;
+		struct sg_page_iter sg_iter;
+		struct i915_gem_request *rq;
+		int n = 0;
+
+		rq = i915_request_create(ctx, rcs);
+		if (IS_ERR(rq))
+			ret = PTR_ERR(rq);
+
+		/* Force a restore to itself in order to save the ctx */
+		if (ret == 0)
+			ret = mi_set_context(rq, &ctx->ring[RCS], 0);
+		if (ret == 0)
+			ret = i915_request_wait(rq);
+		i915_request_put(rq);
+		if (ret) {
+			mutex_unlock(&dev->struct_mutex);
+			return ret;
+		}
+
+		for_each_sg_page(obj->pages->sgl, &sg_iter, obj->pages->nents, 0) {
+			struct page *page = sg_page_iter_page(&sg_iter);
+			void *vaddr;
+
+			vaddr = kmap(page);
+			drm_clflush_virt_range(vaddr, PAGE_SIZE);
+			ret = copy_to_user(to_user_ptr(args->ptr + n),
+					   vaddr, PAGE_SIZE);
+			kunmap(page);
+			if (ret) {
+				mutex_unlock(&dev->struct_mutex);
+				return -EFAULT;
+			}
+
+			n += PAGE_SIZE;
+		}
+	}
+
+	args->size = ctx->ring[RCS].state->base.size;
+	mutex_unlock(&dev->struct_mutex);
+
+	return 0;
+}
diff -urN a/drivers/gpu/drm/i915/i915_gem_evict.c b/drivers/gpu/drm/i915/i915_gem_evict.c
--- a/drivers/gpu/drm/i915/i915_gem_evict.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_gem_evict.c	2014-11-22 14:37:49.330700418 -0700
@@ -39,10 +39,10 @@
 	if (vma->pin_count)
 		return false;
 
-	if (WARN_ON(!list_empty(&vma->exec_list)))
+	if (WARN_ON(!list_empty(&vma->exec_link)))
 		return false;
 
-	list_add(&vma->exec_list, unwind);
+	list_add(&vma->exec_link, unwind);
 	return drm_mm_scan_add_block(&vma->node);
 }
 
@@ -130,11 +130,11 @@
 	while (!list_empty(&unwind_list)) {
 		vma = list_first_entry(&unwind_list,
 				       struct i915_vma,
-				       exec_list);
+				       exec_link);
 		ret = drm_mm_scan_remove_block(&vma->node);
 		BUG_ON(ret);
 
-		list_del_init(&vma->exec_list);
+		list_del_init(&vma->exec_link);
 	}
 
 	/* Can we unpin some objects such as idle hw contents,
@@ -167,24 +167,73 @@
 	while (!list_empty(&unwind_list)) {
 		vma = list_first_entry(&unwind_list,
 				       struct i915_vma,
-				       exec_list);
-		if (drm_mm_scan_remove_block(&vma->node)) {
-			list_move(&vma->exec_list, &eviction_list);
-			drm_gem_object_reference(&vma->obj->base);
+				       exec_link);
+		list_del_init(&vma->exec_link);
+
+		if (drm_mm_scan_remove_block(&vma->node))
+			list_add(&i915_vma_get(vma)->exec_link,
+				  &eviction_list);
+	}
+
+	/* Unbinding will emit any required flushes */
+	while (!list_empty(&eviction_list)) {
+		vma = list_first_entry(&eviction_list,
+				       struct i915_vma,
+				       exec_link);
+		list_del_init(&vma->exec_link);
+
+		if (ret == 0)
+			ret = i915_vma_unbind(vma);
+
+		i915_vma_put(vma);
+	}
+
+	return ret;
+}
+
+int
+i915_gem_evict_range(struct drm_device *dev, struct i915_address_space *vm,
+		     unsigned long start, unsigned long end)
+{
+	struct drm_mm_node *node;
+	struct list_head eviction_list;
+	int ret = 0;
+
+	INIT_LIST_HEAD(&eviction_list);
+	drm_mm_for_each_node(node, &vm->mm) {
+		struct i915_vma *vma;
+
+		if (node->start + node->size <= start)
 			continue;
+		if (node->start >= end)
+			break;
+
+		vma = container_of(node, typeof(*vma), node);
+		if (vma->pin_count) {
+			ret = -EBUSY;
+			break;
 		}
-		list_del_init(&vma->exec_list);
+
+		if (WARN_ON(!list_empty(&vma->exec_link))) {
+			ret = -EINVAL;
+			break;
+		}
+
+		drm_gem_object_reference(&vma->obj->base);
+		list_add(&vma->exec_link, &eviction_list);
 	}
 
-	/* Unbinding will emit any required flushes */
 	while (!list_empty(&eviction_list)) {
+		struct i915_vma *vma;
 		struct drm_gem_object *obj;
+
 		vma = list_first_entry(&eviction_list,
 				       struct i915_vma,
-				       exec_list);
+				       exec_link);
+
+		obj = &vma->obj->base;
 
-		obj =  &vma->obj->base;
-		list_del_init(&vma->exec_list);
+		list_del_init(&vma->exec_link);
 		if (ret == 0)
 			ret = i915_vma_unbind(vma);
 
@@ -198,7 +247,6 @@
  * i915_gem_evict_vm - Evict all idle vmas from a vm
  *
  * @vm: Address space to cleanse
- * @do_idle: Boolean directing whether to idle first.
  *
  * This function evicts all idles vmas from a vm. If all unpinned vmas should be
  * evicted the @do_idle needs to be set to true.
@@ -209,15 +257,14 @@
  * To clarify: This is for freeing up virtual address space, not for freeing
  * memory in e.g. the shrinker.
  */
-int i915_gem_evict_vm(struct i915_address_space *vm, bool do_idle)
+int i915_gem_evict_vm(struct i915_address_space *vm)
 {
 	struct i915_vma *vma, *next;
-	int ret;
 
 	trace_i915_gem_evict_vm(vm);
 
-	if (do_idle) {
-		ret = i915_gpu_idle(vm->dev);
+	if (!list_empty(&vm->active_list)) {
+		int ret = i915_gpu_idle(vm->dev);
 		if (ret)
 			return ret;
 
@@ -243,18 +290,10 @@
 i915_gem_evict_everything(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct i915_address_space *vm;
-	bool lists_empty = true;
+	struct list_head still_in_list;
 	int ret;
 
-	list_for_each_entry(vm, &dev_priv->vm_list, global_link) {
-		lists_empty = (list_empty(&vm->inactive_list) &&
-			       list_empty(&vm->active_list));
-		if (!lists_empty)
-			lists_empty = false;
-	}
-
-	if (lists_empty)
+	if (list_empty(&dev_priv->mm.bound_list))
 		return -ENOSPC;
 
 	trace_i915_gem_evict_everything(dev);
@@ -270,8 +309,32 @@
 	i915_gem_retire_requests(dev);
 
 	/* Having flushed everything, unbind() should never raise an error */
-	list_for_each_entry(vm, &dev_priv->vm_list, global_link)
-		WARN_ON(i915_gem_evict_vm(vm, false));
+	INIT_LIST_HEAD(&still_in_list);
+	while (!list_empty(&dev_priv->mm.bound_list)) {
+		struct list_head vma_list;
+		struct drm_i915_gem_object *obj;
+
+		obj = list_first_entry(&dev_priv->mm.bound_list,
+				       typeof(*obj), global_list);
+		list_move_tail(&obj->global_list, &still_in_list);
+
+		drm_gem_object_reference(&obj->base);
+
+		INIT_LIST_HEAD(&vma_list);
+		while (!list_empty(&obj->vma_list)) {
+			struct i915_vma *vma;
+
+			vma = list_first_entry(&obj->vma_list,
+					       typeof(*vma), obj_link);
+			list_move_tail(&vma->obj_link, &vma_list);
+			if (i915_vma_unbind(vma))
+				break;
+		}
+		list_splice(&vma_list, &obj->vma_list);
+
+		drm_gem_object_unreference(&obj->base);
+	}
+	list_splice(&still_in_list, &dev_priv->mm.bound_list);
 
 	return 0;
 }
diff -urN a/drivers/gpu/drm/i915/i915_gem_execbuffer.c b/drivers/gpu/drm/i915/i915_gem_execbuffer.c
--- a/drivers/gpu/drm/i915/i915_gem_execbuffer.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_gem_execbuffer.c	2014-11-22 14:37:49.330700418 -0700
@@ -35,12 +35,15 @@
 
 #define  __EXEC_OBJECT_HAS_PIN (1<<31)
 #define  __EXEC_OBJECT_HAS_FENCE (1<<30)
+#define  __EXEC_OBJECT_NEEDS_MAP (1<<29)
 #define  __EXEC_OBJECT_NEEDS_BIAS (1<<28)
 
 #define BATCH_OFFSET_BIAS (256*1024)
 
 struct eb_vmas {
 	struct list_head vmas;
+	struct i915_address_space *vm;
+	struct i915_vma *batch;
 	int and;
 	union {
 		struct i915_vma *lut[0];
@@ -87,14 +90,33 @@
 		memset(eb->buckets, 0, (eb->and+1)*sizeof(struct hlist_head));
 }
 
+static struct i915_vma *
+eb_get_batch(struct eb_vmas *eb)
+{
+	struct i915_vma *vma =
+		list_entry(eb->vmas.prev, typeof(*vma), exec_link);
+
+	/*
+	 * SNA is doing fancy tricks with compressing batch buffers, which leads
+	 * to negative relocation deltas. Usually that works out ok since the
+	 * relocate address is still positive, except when the batch is placed
+	 * very low in the GTT. Ensure this doesn't happen.
+	 *
+	 * Note that actual hangs have only been observed on gen7, but for
+	 * paranoia do it everywhere.
+	 */
+	if ((vma->exec_entry->flags & EXEC_OBJECT_PINNED) == 0)
+		vma->exec_entry->flags |= __EXEC_OBJECT_NEEDS_BIAS;
+
+	return vma;
+}
+
 static int
 eb_lookup_vmas(struct eb_vmas *eb,
 	       struct drm_i915_gem_exec_object2 *exec,
 	       const struct drm_i915_gem_execbuffer2 *args,
-	       struct i915_address_space *vm,
 	       struct drm_file *file)
 {
-	struct drm_i915_private *dev_priv = vm->dev->dev_private;
 	struct drm_i915_gem_object *obj;
 	struct list_head objects;
 	int i, ret;
@@ -129,20 +151,6 @@
 	i = 0;
 	while (!list_empty(&objects)) {
 		struct i915_vma *vma;
-		struct i915_address_space *bind_vm = vm;
-
-		if (exec[i].flags & EXEC_OBJECT_NEEDS_GTT &&
-		    USES_FULL_PPGTT(vm->dev)) {
-			ret = -EINVAL;
-			goto err;
-		}
-
-		/* If we have secure dispatch, or the userspace assures us that
-		 * they know what they're doing, use the GGTT VM.
-		 */
-		if (((args->flags & I915_EXEC_SECURE) &&
-		    (i == (args->buffer_count - 1))))
-			bind_vm = &dev_priv->gtt.base;
 
 		obj = list_first_entry(&objects,
 				       struct drm_i915_gem_object,
@@ -156,7 +164,7 @@
 		 * from the (obj, vm) we don't run the risk of creating
 		 * duplicated vmas for the same vm.
 		 */
-		vma = i915_gem_obj_lookup_or_create_vma(obj, bind_vm);
+		vma = i915_gem_obj_get_vma(obj, eb->vm);
 		if (IS_ERR(vma)) {
 			DRM_DEBUG("Failed to lookup VMA\n");
 			ret = PTR_ERR(vma);
@@ -164,7 +172,7 @@
 		}
 
 		/* Transfer ownership from the objects list to the vmas list. */
-		list_add_tail(&vma->exec_list, &eb->vmas);
+		list_add_tail(&vma->exec_link, &eb->vmas);
 		list_del_init(&obj->obj_exec_link);
 
 		vma->exec_entry = &exec[i];
@@ -179,6 +187,9 @@
 		++i;
 	}
 
+	/* take note of the batch buffer before we might reorder the lists */
+	eb->batch = eb_get_batch(eb);
+
 	return 0;
 
 
@@ -221,37 +232,39 @@
 }
 
 static void
-i915_gem_execbuffer_unreserve_vma(struct i915_vma *vma)
+__i915_vma_unreserve(struct i915_vma *vma)
 {
-	struct drm_i915_gem_exec_object2 *entry;
-	struct drm_i915_gem_object *obj = vma->obj;
-
-	if (!drm_mm_node_allocated(&vma->node))
-		return;
-
-	entry = vma->exec_entry;
+	struct drm_i915_gem_exec_object2 *entry = vma->exec_entry;
 
 	if (entry->flags & __EXEC_OBJECT_HAS_FENCE)
-		i915_gem_object_unpin_fence(obj);
+		i915_gem_object_unpin_fence(vma->obj);
 
-	if (entry->flags & __EXEC_OBJECT_HAS_PIN)
+	if (entry->flags & __EXEC_OBJECT_HAS_PIN) {
+		BUG_ON(vma->pin_count == 0);
 		vma->pin_count--;
+	}
 
 	entry->flags &= ~(__EXEC_OBJECT_HAS_FENCE | __EXEC_OBJECT_HAS_PIN);
 }
 
-static void eb_destroy(struct eb_vmas *eb)
+void
+i915_vma_unreserve(struct i915_vma *vma)
 {
-	while (!list_empty(&eb->vmas)) {
-		struct i915_vma *vma;
-
-		vma = list_first_entry(&eb->vmas,
-				       struct i915_vma,
-				       exec_list);
-		list_del_init(&vma->exec_list);
-		i915_gem_execbuffer_unreserve_vma(vma);
-		drm_gem_object_unreference(&vma->obj->base);
+	list_del_init(&vma->exec_link);
+	if (vma->exec_entry) {
+		__i915_vma_unreserve(vma);
+		vma->exec_entry = NULL;
 	}
+	drm_gem_object_unreference(&vma->obj->base);
+	i915_vma_put(vma);
+}
+
+static void eb_destroy(struct eb_vmas *eb)
+{
+	while (!list_empty(&eb->vmas))
+		i915_vma_unreserve(list_first_entry(&eb->vmas,
+						    struct i915_vma,
+						    exec_link));
 	kfree(eb);
 }
 
@@ -270,7 +283,7 @@
 {
 	struct drm_device *dev = obj->base.dev;
 	uint32_t page_offset = offset_in_page(reloc->offset);
-	uint64_t delta = reloc->delta + target_offset;
+	uint64_t delta = (int)reloc->delta + target_offset;
 	char *vaddr;
 	int ret;
 
@@ -306,8 +319,8 @@
 {
 	struct drm_device *dev = obj->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	uint64_t delta = reloc->delta + target_offset;
-	uint32_t __iomem *reloc_entry;
+	uint64_t delta = (int)reloc->delta + target_offset;
+	uint64_t offset;
 	void __iomem *reloc_page;
 	int ret;
 
@@ -320,25 +333,24 @@
 		return ret;
 
 	/* Map the page containing the relocation we're going to perform.  */
-	reloc->offset += i915_gem_obj_ggtt_offset(obj);
+	offset = i915_gem_obj_ggtt_offset(obj);
+	offset += reloc->offset;
 	reloc_page = io_mapping_map_atomic_wc(dev_priv->gtt.mappable,
-			reloc->offset & PAGE_MASK);
-	reloc_entry = (uint32_t __iomem *)
-		(reloc_page + offset_in_page(reloc->offset));
-	iowrite32(lower_32_bits(delta), reloc_entry);
+					      offset & PAGE_MASK);
+	iowrite32(lower_32_bits(delta), reloc_page + offset_in_page(offset));
 
 	if (INTEL_INFO(dev)->gen >= 8) {
-		reloc_entry += 1;
+		offset += sizeof(uint32_t);
 
-		if (offset_in_page(reloc->offset + sizeof(uint32_t)) == 0) {
+		if (offset_in_page(offset) == 0) {
 			io_mapping_unmap_atomic(reloc_page);
-			reloc_page = io_mapping_map_atomic_wc(
-					dev_priv->gtt.mappable,
-					reloc->offset + sizeof(uint32_t));
-			reloc_entry = reloc_page;
+			reloc_page =
+				io_mapping_map_atomic_wc(dev_priv->gtt.mappable,
+							 offset);
 		}
 
-		iowrite32(upper_32_bits(delta), reloc_entry);
+		iowrite32(upper_32_bits(delta),
+			  reloc_page + offset_in_page(offset));
 	}
 
 	io_mapping_unmap_atomic(reloc_page);
@@ -346,6 +358,24 @@
 	return 0;
 }
 
+static bool obj_active(struct drm_i915_gem_object *obj)
+{
+	int i;
+
+	if (!obj->active)
+		return false;
+
+	for (i = 0; i < I915_NUM_ENGINES; i++) {
+		if (obj->last_read[i].request == NULL)
+			continue;
+
+		if (!i915_request_complete(obj->last_read[i].request))
+			return true;
+	}
+
+	return false;
+}
+
 static int
 i915_gem_execbuffer_relocate_entry(struct drm_i915_gem_object *obj,
 				   struct eb_vmas *eb,
@@ -371,13 +401,8 @@
 	 * pipe_control writes because the gpu doesn't properly redirect them
 	 * through the ppgtt for non_secure batchbuffers. */
 	if (unlikely(IS_GEN6(dev) &&
-	    reloc->write_domain == I915_GEM_DOMAIN_INSTRUCTION &&
-	    !target_i915_obj->has_global_gtt_mapping)) {
-		struct i915_vma *vma =
-			list_first_entry(&target_i915_obj->vma_list,
-					 typeof(*vma), vma_link);
-		vma->bind_vma(vma, target_i915_obj->cache_level, GLOBAL_BIND);
-	}
+		     reloc->write_domain == I915_GEM_DOMAIN_INSTRUCTION))
+		target_vma->exec_entry->flags |= EXEC_OBJECT_NEEDS_GTT;
 
 	/* Validate that the target is in a valid r/w GPU domain */
 	if (unlikely(reloc->write_domain & (reloc->write_domain - 1))) {
@@ -430,20 +455,18 @@
 	}
 
 	/* We can't wait for rendering with pagefaults disabled */
-	if (obj->active && in_atomic())
+	if (in_atomic() && obj_active(obj))
 		return -EFAULT;
 
 	if (use_cpu_reloc(obj))
 		ret = relocate_entry_cpu(obj, reloc, target_offset);
 	else
 		ret = relocate_entry_gtt(obj, reloc, target_offset);
-
 	if (ret)
 		return ret;
 
 	/* and update the user's relocation entry */
 	reloc->presumed_offset = target_offset;
-
 	return 0;
 }
 
@@ -524,7 +547,7 @@
 	 * lockdep complains vehemently.
 	 */
 	pagefault_disable();
-	list_for_each_entry(vma, &eb->vmas, exec_list) {
+	list_for_each_entry(vma, &eb->vmas, exec_link) {
 		ret = i915_gem_execbuffer_relocate_vma(vma, eb);
 		if (ret)
 			break;
@@ -535,56 +558,38 @@
 }
 
 static int
-need_reloc_mappable(struct i915_vma *vma)
-{
-	struct drm_i915_gem_exec_object2 *entry = vma->exec_entry;
-	return entry->relocation_count && !use_cpu_reloc(vma->obj) &&
-		i915_is_ggtt(vma->vm);
-}
-
-static int
 i915_gem_execbuffer_reserve_vma(struct i915_vma *vma,
-				struct intel_engine_cs *ring,
+				struct intel_engine_cs *engine,
 				bool *need_reloc)
 {
 	struct drm_i915_gem_object *obj = vma->obj;
 	struct drm_i915_gem_exec_object2 *entry = vma->exec_entry;
-	bool has_fenced_gpu_access = INTEL_INFO(ring->dev)->gen < 4;
-	bool need_fence;
 	uint64_t flags;
 	int ret;
 
-	flags = 0;
-
-	need_fence =
-		has_fenced_gpu_access &&
-		entry->flags & EXEC_OBJECT_NEEDS_FENCE &&
-		obj->tiling_mode != I915_TILING_NONE;
-	if (need_fence || need_reloc_mappable(vma))
-		flags |= PIN_MAPPABLE;
-
+	flags = PIN_LOCAL;
+	if (entry->flags & __EXEC_OBJECT_NEEDS_MAP)
+		flags |= PIN_GLOBAL | PIN_MAPPABLE;
 	if (entry->flags & EXEC_OBJECT_NEEDS_GTT)
 		flags |= PIN_GLOBAL;
 	if (entry->flags & __EXEC_OBJECT_NEEDS_BIAS)
 		flags |= BATCH_OFFSET_BIAS | PIN_OFFSET_BIAS;
+	if (entry->flags & EXEC_OBJECT_PINNED)
+		flags |= entry->offset | PIN_OFFSET_FIXED;
 
-	ret = i915_gem_object_pin(obj, vma->vm, entry->alignment, flags);
+	ret = i915_vma_pin(vma, entry->alignment, flags);
 	if (ret)
 		return ret;
 
 	entry->flags |= __EXEC_OBJECT_HAS_PIN;
 
-	if (has_fenced_gpu_access) {
-		if (entry->flags & EXEC_OBJECT_NEEDS_FENCE) {
-			ret = i915_gem_object_get_fence(obj);
-			if (ret)
-				return ret;
-
-			if (i915_gem_object_pin_fence(obj))
-				entry->flags |= __EXEC_OBJECT_HAS_FENCE;
+	if (entry->flags & EXEC_OBJECT_NEEDS_FENCE) {
+		ret = i915_gem_object_get_fence(obj);
+		if (ret)
+			return ret;
 
-			obj->pending_fenced_gpu_access = true;
-		}
+		if (i915_gem_object_pin_fence(obj))
+			entry->flags |= __EXEC_OBJECT_HAS_FENCE;
 	}
 
 	if (entry->offset != vma->node.start) {
@@ -601,26 +606,39 @@
 }
 
 static bool
-eb_vma_misplaced(struct i915_vma *vma, bool has_fenced_gpu_access)
+need_reloc_mappable(struct i915_vma *vma)
 {
-	struct drm_i915_gem_exec_object2 *entry = vma->exec_entry;
-	struct drm_i915_gem_object *obj = vma->obj;
-	bool need_fence, need_mappable;
+	if (vma->exec_entry->relocation_count == 0)
+		return false;
 
-	need_fence =
-		has_fenced_gpu_access &&
-		entry->flags & EXEC_OBJECT_NEEDS_FENCE &&
-		obj->tiling_mode != I915_TILING_NONE;
-	need_mappable = need_fence || need_reloc_mappable(vma);
+	if (!i915_is_ggtt(vma->vm))
+		return false;
 
-	WARN_ON((need_mappable || need_fence) &&
-	       !i915_is_ggtt(vma->vm));
+	/* See also use_cpu_reloc() */
+	if (HAS_LLC(vma->obj->base.dev))
+		return false;
+
+	if (vma->obj->base.write_domain == I915_GEM_DOMAIN_CPU)
+		return false;
+
+	return true;
+}
+
+static bool
+eb_vma_misplaced(struct i915_vma *vma)
+{
+	struct drm_i915_gem_exec_object2 *entry = vma->exec_entry;
+	struct drm_i915_gem_object *obj = vma->obj;
 
 	if (entry->alignment &&
 	    vma->node.start & (entry->alignment - 1))
 		return true;
 
-	if (need_mappable && !obj->map_and_fenceable)
+	if (entry->flags & EXEC_OBJECT_PINNED &&
+	    vma->node.start != entry->offset)
+		return true;
+
+	if (entry->flags & __EXEC_OBJECT_NEEDS_MAP && !obj->map_and_fenceable)
 		return true;
 
 	if (entry->flags & __EXEC_OBJECT_NEEDS_BIAS &&
@@ -631,49 +649,44 @@
 }
 
 static int
-i915_gem_execbuffer_reserve(struct intel_engine_cs *ring,
-			    struct list_head *vmas,
+i915_gem_execbuffer_reserve(struct intel_engine_cs *engine,
+			    struct eb_vmas *eb,
 			    bool *need_relocs)
 {
 	struct drm_i915_gem_object *obj;
 	struct i915_vma *vma;
-	struct i915_address_space *vm;
 	struct list_head ordered_vmas;
-	bool has_fenced_gpu_access = INTEL_INFO(ring->dev)->gen < 4;
+	bool has_fenced_gpu_access = INTEL_INFO(engine->i915)->gen < 4;
 	int retry;
 
-	if (list_empty(vmas))
-		return 0;
-
-	i915_gem_retire_requests_ring(ring);
-
-	vm = list_first_entry(vmas, struct i915_vma, exec_list)->vm;
+	i915_gem_retire_requests__engine(engine);
 
 	INIT_LIST_HEAD(&ordered_vmas);
-	while (!list_empty(vmas)) {
+	while (!list_empty(&eb->vmas)) {
 		struct drm_i915_gem_exec_object2 *entry;
 		bool need_fence, need_mappable;
 
-		vma = list_first_entry(vmas, struct i915_vma, exec_list);
+		vma = list_first_entry(&eb->vmas, struct i915_vma, exec_link);
 		obj = vma->obj;
 		entry = vma->exec_entry;
 
+		if (!has_fenced_gpu_access)
+			entry->flags &= ~EXEC_OBJECT_NEEDS_FENCE;
 		need_fence =
-			has_fenced_gpu_access &&
 			entry->flags & EXEC_OBJECT_NEEDS_FENCE &&
 			obj->tiling_mode != I915_TILING_NONE;
 		need_mappable = need_fence || need_reloc_mappable(vma);
 
-		if (need_mappable)
-			list_move(&vma->exec_list, &ordered_vmas);
-		else
-			list_move_tail(&vma->exec_list, &ordered_vmas);
+		if (need_mappable) {
+			entry->flags |= __EXEC_OBJECT_NEEDS_MAP;
+			list_move(&vma->exec_link, &ordered_vmas);
+		} else
+			list_move_tail(&vma->exec_link, &ordered_vmas);
 
 		obj->base.pending_read_domains = I915_GEM_GPU_DOMAINS & ~I915_GEM_DOMAIN_COMMAND;
 		obj->base.pending_write_domain = 0;
-		obj->pending_fenced_gpu_access = false;
 	}
-	list_splice(&ordered_vmas, vmas);
+	list_splice(&ordered_vmas, &eb->vmas);
 
 	/* Attempt to pin all of the buffers into the GTT.
 	 * This is done in 3 phases:
@@ -692,24 +705,24 @@
 		int ret = 0;
 
 		/* Unbind any ill-fitting objects or pin. */
-		list_for_each_entry(vma, vmas, exec_list) {
+		list_for_each_entry(vma, &eb->vmas, exec_link) {
 			if (!drm_mm_node_allocated(&vma->node))
 				continue;
 
-			if (eb_vma_misplaced(vma, has_fenced_gpu_access))
+			if (eb_vma_misplaced(vma))
 				ret = i915_vma_unbind(vma);
 			else
-				ret = i915_gem_execbuffer_reserve_vma(vma, ring, need_relocs);
+				ret = i915_gem_execbuffer_reserve_vma(vma, engine, need_relocs);
 			if (ret)
 				goto err;
 		}
 
 		/* Bind fresh objects */
-		list_for_each_entry(vma, vmas, exec_list) {
+		list_for_each_entry(vma, &eb->vmas, exec_link) {
 			if (drm_mm_node_allocated(&vma->node))
 				continue;
 
-			ret = i915_gem_execbuffer_reserve_vma(vma, ring, need_relocs);
+			ret = i915_gem_execbuffer_reserve_vma(vma, engine, need_relocs);
 			if (ret)
 				goto err;
 		}
@@ -719,45 +732,37 @@
 			return ret;
 
 		/* Decrement pin count for bound objects */
-		list_for_each_entry(vma, vmas, exec_list)
-			i915_gem_execbuffer_unreserve_vma(vma);
+		list_for_each_entry(vma, &eb->vmas, exec_link)
+			__i915_vma_unreserve(vma);
 
-		ret = i915_gem_evict_vm(vm, true);
+		ret = i915_gem_evict_vm(eb->vm);
 		if (ret)
 			return ret;
 	} while (1);
 }
 
 static int
-i915_gem_execbuffer_relocate_slow(struct drm_device *dev,
+i915_gem_execbuffer_relocate_slow(struct drm_i915_private *i915,
 				  struct drm_i915_gem_execbuffer2 *args,
 				  struct drm_file *file,
-				  struct intel_engine_cs *ring,
+				  struct intel_engine_cs *engine,
 				  struct eb_vmas *eb,
 				  struct drm_i915_gem_exec_object2 *exec)
 {
 	struct drm_i915_gem_relocation_entry *reloc;
-	struct i915_address_space *vm;
 	struct i915_vma *vma;
 	bool need_relocs;
 	int *reloc_offset;
 	int i, total, ret;
 	unsigned count = args->buffer_count;
 
-	if (WARN_ON(list_empty(&eb->vmas)))
-		return 0;
-
-	vm = list_first_entry(&eb->vmas, struct i915_vma, exec_list)->vm;
-
 	/* We may process another execbuffer during the unlock... */
 	while (!list_empty(&eb->vmas)) {
-		vma = list_first_entry(&eb->vmas, struct i915_vma, exec_list);
-		list_del_init(&vma->exec_list);
-		i915_gem_execbuffer_unreserve_vma(vma);
-		drm_gem_object_unreference(&vma->obj->base);
+		vma = list_first_entry(&eb->vmas, struct i915_vma, exec_link);
+		i915_vma_unreserve(vma);
 	}
 
-	mutex_unlock(&dev->struct_mutex);
+	mutex_unlock(&i915->dev->struct_mutex);
 
 	total = 0;
 	for (i = 0; i < count; i++)
@@ -768,7 +773,7 @@
 	if (reloc == NULL || reloc_offset == NULL) {
 		drm_free_large(reloc);
 		drm_free_large(reloc_offset);
-		mutex_lock(&dev->struct_mutex);
+		mutex_lock(&i915->dev->struct_mutex);
 		return -ENOMEM;
 	}
 
@@ -783,7 +788,7 @@
 		if (copy_from_user(reloc+total, user_relocs,
 				   exec[i].relocation_count * sizeof(*reloc))) {
 			ret = -EFAULT;
-			mutex_lock(&dev->struct_mutex);
+			mutex_lock(&i915->dev->struct_mutex);
 			goto err;
 		}
 
@@ -801,7 +806,7 @@
 					   &invalid_offset,
 					   sizeof(invalid_offset))) {
 				ret = -EFAULT;
-				mutex_lock(&dev->struct_mutex);
+				mutex_lock(&i915->dev->struct_mutex);
 				goto err;
 			}
 		}
@@ -810,24 +815,24 @@
 		total += exec[i].relocation_count;
 	}
 
-	ret = i915_mutex_lock_interruptible(dev);
+	ret = i915_mutex_lock_interruptible(i915->dev);
 	if (ret) {
-		mutex_lock(&dev->struct_mutex);
+		mutex_lock(&i915->dev->struct_mutex);
 		goto err;
 	}
 
 	/* reacquire the objects */
 	eb_reset(eb);
-	ret = eb_lookup_vmas(eb, exec, args, vm, file);
+	ret = eb_lookup_vmas(eb, exec, args, file);
 	if (ret)
 		goto err;
 
 	need_relocs = (args->flags & I915_EXEC_NO_RELOC) == 0;
-	ret = i915_gem_execbuffer_reserve(ring, &eb->vmas, &need_relocs);
+	ret = i915_gem_execbuffer_reserve(engine, eb, &need_relocs);
 	if (ret)
 		goto err;
 
-	list_for_each_entry(vma, &eb->vmas, exec_list) {
+	list_for_each_entry(vma, &eb->vmas, exec_link) {
 		int offset = vma->exec_entry - exec;
 		ret = i915_gem_execbuffer_relocate_vma_slow(vma, eb,
 							    reloc + reloc_offset[offset]);
@@ -848,17 +853,19 @@
 }
 
 static int
-i915_gem_execbuffer_move_to_gpu(struct intel_engine_cs *ring,
-				struct list_head *vmas)
+vmas_move_to_rq(struct eb_vmas *eb,
+		struct i915_gem_request *rq)
 {
 	struct i915_vma *vma;
 	uint32_t flush_domains = 0;
 	bool flush_chipset = false;
 	int ret;
 
-	list_for_each_entry(vma, vmas, exec_list) {
+	/* 1: flush/serialise damage from other sources */
+	list_for_each_entry(vma, &eb->vmas, exec_link) {
 		struct drm_i915_gem_object *obj = vma->obj;
-		ret = i915_gem_object_sync(obj, ring);
+
+		ret = i915_gem_object_sync(obj, rq);
 		if (ret)
 			return ret;
 
@@ -866,42 +873,65 @@
 			flush_chipset |= i915_gem_clflush_object(obj, false);
 
 		flush_domains |= obj->base.write_domain;
+		if (obj->last_read[rq->engine->id].request == NULL)
+			rq->pending_flush |= I915_INVALIDATE_CACHES;
 	}
 
 	if (flush_chipset)
-		i915_gem_chipset_flush(ring->dev);
+		i915_gem_chipset_flush(rq->i915->dev);
 
 	if (flush_domains & I915_GEM_DOMAIN_GTT)
 		wmb();
 
-	/* Unconditionally invalidate gpu caches and ensure that we do flush
-	 * any residual writes from the previous batch.
-	 */
-	return intel_ring_invalidate_all_caches(ring);
-}
+	/* 2: invalidate the caches from this ring after emitting semaphores */
+	ret = i915_request_emit_flush(rq, I915_INVALIDATE_CACHES);
+	if (ret)
+		return ret;
 
-static bool
-i915_gem_check_execbuffer(struct drm_i915_gem_execbuffer2 *exec)
-{
-	if (exec->flags & __I915_EXEC_UNKNOWN_FLAGS)
-		return false;
+	/* 3: track flushes and objects for this rq */
+	while (!list_empty(&eb->vmas)) {
+		unsigned fenced;
+
+		vma = list_first_entry(&eb->vmas, typeof(*vma), exec_link);
 
-	return ((exec->batch_start_offset | exec->batch_len) & 0x7) == 0;
+		fenced = 0;
+		if (vma->exec_entry->flags & EXEC_OBJECT_NEEDS_FENCE) {
+			fenced |= VMA_IS_FENCED;
+			if (vma->exec_entry->flags & __EXEC_OBJECT_HAS_FENCE)
+				fenced |= VMA_HAS_FENCE;
+		}
+
+		i915_request_add_vma(rq, vma, fenced);
+	}
+
+	return 0;
 }
 
 static int
-validate_exec_list(struct drm_i915_gem_exec_object2 *exec,
-		   int count)
+validate_exec_list(struct drm_i915_private *dev_priv,
+		   struct drm_i915_gem_execbuffer2 *args,
+		   struct drm_i915_gem_exec_object2 *exec)
 {
-	int i;
 	unsigned relocs_total = 0;
 	unsigned relocs_max = UINT_MAX / sizeof(struct drm_i915_gem_relocation_entry);
+	unsigned invalid_flags;
+	int i;
 
-	for (i = 0; i < count; i++) {
+	if (args->flags & __I915_EXEC_UNKNOWN_FLAGS)
+		return -EINVAL;
+
+	if ((args->batch_start_offset | args->batch_len) & 0x7)
+		return -EINVAL;
+
+	invalid_flags = __EXEC_OBJECT_UNKNOWN_FLAGS;
+	if (USES_FULL_PPGTT(dev_priv))
+		invalid_flags |= EXEC_OBJECT_NEEDS_GTT;
+
+	for (i = 0; i < args->buffer_count; i++) {
 		char __user *ptr = to_user_ptr(exec[i].relocs_ptr);
 		int length; /* limited by fault_in_pages_readable() */
 
-		if (exec[i].flags & __EXEC_OBJECT_UNKNOWN_FLAGS)
+		if (exec[i].flags & invalid_flags)
 			return -EINVAL;
 
 		/* First check for malicious input causing overflow in
@@ -922,7 +952,7 @@
 		if (!access_ok(VERIFY_WRITE, ptr, length))
 			return -EFAULT;
 
-		if (likely(!i915.prefault_disable)) {
+		if (likely(!i915_module.prefault_disable)) {
 			if (fault_in_multipages_readable(ptr, length))
 				return -EFAULT;
 		}
@@ -932,13 +962,14 @@
 }
 
 static struct intel_context *
-i915_gem_validate_context(struct drm_device *dev, struct drm_file *file,
-			  struct intel_engine_cs *ring, const u32 ctx_id)
+i915_gem_validate_context(struct drm_file *file,
+			  struct intel_engine_cs *engine,
+			  const u32 ctx_id)
 {
 	struct intel_context *ctx = NULL;
 	struct i915_ctx_hang_stats *hs;
 
-	if (ring->id != RCS && ctx_id != DEFAULT_CONTEXT_HANDLE)
+	if (engine->id != RCS && ctx_id != DEFAULT_CONTEXT_HANDLE)
 		return ERR_PTR(-EINVAL);
 
 	ctx = i915_gem_context_get(file->driver_priv, ctx_id);
@@ -954,109 +985,153 @@
 	return ctx;
 }
 
-static void
-i915_gem_execbuffer_move_to_active(struct list_head *vmas,
-				   struct intel_engine_cs *ring)
+static int
+reset_sol_offsets(struct i915_gem_request *rq)
 {
-	struct i915_vma *vma;
-
-	list_for_each_entry(vma, vmas, exec_list) {
-		struct drm_i915_gem_object *obj = vma->obj;
-		u32 old_read = obj->base.read_domains;
-		u32 old_write = obj->base.write_domain;
+	struct intel_ringbuffer *ring;
+	int i;
 
-		obj->base.write_domain = obj->base.pending_write_domain;
-		if (obj->base.write_domain == 0)
-			obj->base.pending_read_domains |= obj->base.read_domains;
-		obj->base.read_domains = obj->base.pending_read_domains;
-		obj->fenced_gpu_access = obj->pending_fenced_gpu_access;
+	if (!IS_GEN7(rq->i915) || rq->engine->id != RCS) {
+		DRM_DEBUG("sol reset is gen7/rcs only\n");
+		return -EINVAL;
+	}
 
-		i915_vma_move_to_active(vma, ring);
-		if (obj->base.write_domain) {
-			obj->dirty = 1;
-			obj->last_write_seqno = intel_ring_get_seqno(ring);
+	ring = intel_ring_begin(rq, 4 * 3);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
-			intel_fb_obj_invalidate(obj, ring);
+	for (i = 0; i < 4; i++) {
+		intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
+		intel_ring_emit(ring, GEN7_SO_WRITE_OFFSET(i));
+		intel_ring_emit(ring, 0);
+	}
 
-			/* update for the implicit flush after a batch */
-			obj->base.write_domain &= ~I915_GEM_GPU_DOMAINS;
-		}
+	intel_ring_advance(ring);
+	return 0;
+}
 
-		trace_i915_gem_object_change_domain(obj, old_read, old_write);
+static int
+emit_box(struct i915_gem_request *rq,
+	 struct drm_clip_rect *box,
+	 int DR1, int DR4)
+{
+	struct intel_ringbuffer *ring;
+
+	if (box->y2 <= box->y1 || box->x2 <= box->x1 ||
+	    box->y2 <= 0 || box->x2 <= 0) {
+		DRM_DEBUG("Bad box %d,%d..%d,%d\n",
+			  box->x1, box->y1, box->x2, box->y2);
+		return -EINVAL;
 	}
-}
 
-static void
-i915_gem_execbuffer_retire_commands(struct drm_device *dev,
-				    struct drm_file *file,
-				    struct intel_engine_cs *ring,
-				    struct drm_i915_gem_object *obj)
-{
-	/* Unconditionally force add_request to emit a full flush. */
-	ring->gpu_caches_dirty = true;
+	if (INTEL_INFO(rq->i915)->gen >= 4) {
+		ring = intel_ring_begin(rq, 4);
+		if (IS_ERR(ring))
+			return PTR_ERR(ring);
 
-	/* Add a breadcrumb for the completion of the batch buffer */
-	(void)__i915_add_request(ring, file, obj, NULL);
+		intel_ring_emit(ring, GFX_OP_DRAWRECT_INFO_I965);
+	} else {
+		ring = intel_ring_begin(rq, 5);
+		if (IS_ERR(ring))
+			return PTR_ERR(ring);
+
+		intel_ring_emit(ring, GFX_OP_DRAWRECT_INFO);
+		intel_ring_emit(ring, DR1);
+	}
+	intel_ring_emit(ring, (box->x1 & 0xffff) | box->y1 << 16);
+	intel_ring_emit(ring, ((box->x2 - 1) & 0xffff) | (box->y2 - 1) << 16);
+	intel_ring_emit(ring, DR4);
+	intel_ring_advance(ring);
+
+	return 0;
 }
 
-static int
-i915_reset_gen7_sol_offsets(struct drm_device *dev,
-			    struct intel_engine_cs *ring)
+static int set_contants_base(struct i915_gem_request *rq,
+			     struct drm_i915_gem_execbuffer2 *args)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	int ret, i;
+	int mode = args->flags & I915_EXEC_CONSTANTS_MASK;
+	u32 mask = I915_EXEC_CONSTANTS_MASK;
 
-	if (!IS_GEN7(dev) || ring != &dev_priv->ring[RCS]) {
-		DRM_DEBUG("sol reset is gen7/rcs only\n");
+	switch (mode) {
+	case I915_EXEC_CONSTANTS_REL_GENERAL:
+	case I915_EXEC_CONSTANTS_ABSOLUTE:
+	case I915_EXEC_CONSTANTS_REL_SURFACE:
+		if (mode != 0 && rq->engine->id != RCS) {
+			DRM_DEBUG("non-0 rel constants mode on non-RCS\n");
+			return -EINVAL;
+		}
+
+		if (mode != rq->engine->i915->relative_constants_mode) {
+			if (INTEL_INFO(rq->engine->i915)->gen < 4) {
+				DRM_DEBUG("no rel constants on pre-gen4\n");
+				return -EINVAL;
+			}
+
+			if (INTEL_INFO(rq->engine->i915)->gen > 5 &&
+			    mode == I915_EXEC_CONSTANTS_REL_SURFACE) {
+				DRM_DEBUG("rel surface constants mode invalid on gen5+\n");
+				return -EINVAL;
+			}
+
+			/* The HW changed the meaning on this bit on gen6 */
+			if (INTEL_INFO(rq->i915)->gen >= 6)
+				mask &= ~I915_EXEC_CONSTANTS_REL_SURFACE;
+		}
+		break;
+	default:
+		DRM_DEBUG("execbuf with unknown constants: %d\n", mode);
 		return -EINVAL;
 	}
 
-	ret = intel_ring_begin(ring, 4 * 3);
-	if (ret)
-		return ret;
+	/* XXX INSTPM is per-context not global etc */
+	if (rq->engine->id == RCS && mode != rq->i915->relative_constants_mode) {
+		struct intel_ringbuffer *ring;
+
+		ring = intel_ring_begin(rq, 3);
+		if (IS_ERR(ring))
+			return PTR_ERR(ring);
 
-	for (i = 0; i < 4; i++) {
 		intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
-		intel_ring_emit(ring, GEN7_SO_WRITE_OFFSET(i));
-		intel_ring_emit(ring, 0);
-	}
+		intel_ring_emit(ring, INSTPM);
+		intel_ring_emit(ring, mask << 16 | mode);
+		intel_ring_advance(ring);
 
-	intel_ring_advance(ring);
+		rq->i915->relative_constants_mode = mode;
+	}
 
 	return 0;
 }
 
 static int
-legacy_ringbuffer_submission(struct drm_device *dev, struct drm_file *file,
-			     struct intel_engine_cs *ring,
-			     struct intel_context *ctx,
-			     struct drm_i915_gem_execbuffer2 *args,
-			     struct list_head *vmas,
-			     struct drm_i915_gem_object *batch_obj,
-			     u64 exec_start, u32 flags)
+submit_execbuf(struct intel_engine_cs *engine,
+	       struct intel_context *ctx,
+	       struct drm_i915_gem_execbuffer2 *args,
+	       struct eb_vmas *eb,
+	       u32 flags)
 {
 	struct drm_clip_rect *cliprects = NULL;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	u64 exec_len;
-	int instp_mode;
-	u32 instp_mask;
-	int i, ret = 0;
+	struct i915_gem_request *rq = NULL;
+	u64 exec_start;
+	int i, ret;
 
 	if (args->num_cliprects != 0) {
-		if (ring != &dev_priv->ring[RCS]) {
+		if (engine->id != RCS) {
 			DRM_DEBUG("clip rectangles are only valid with the render ring\n");
-			return -EINVAL;
+			ret = -EINVAL;
+			goto out;
 		}
 
-		if (INTEL_INFO(dev)->gen >= 5) {
+		if (INTEL_INFO(engine->i915)->gen >= 5) {
 			DRM_DEBUG("clip rectangles are only valid on pre-gen5\n");
-			return -EINVAL;
+			ret = -EINVAL;
+			goto out;
 		}
 
 		if (args->num_cliprects > UINT_MAX / sizeof(*cliprects)) {
 			DRM_DEBUG("execbuf with %u cliprects\n",
 				  args->num_cliprects);
-			return -EINVAL;
+			ret = -EINVAL;
+			goto out;
 		}
 
 		cliprects = kcalloc(args->num_cliprects,
@@ -1064,14 +1139,14 @@
 				    GFP_KERNEL);
 		if (cliprects == NULL) {
 			ret = -ENOMEM;
-			goto error;
+			goto out;
 		}
 
 		if (copy_from_user(cliprects,
 				   to_user_ptr(args->cliprects_ptr),
 				   sizeof(*cliprects)*args->num_cliprects)) {
 			ret = -EFAULT;
-			goto error;
+			goto out_cliprects;
 		}
 	} else {
 		if (args->DR4 == 0xffffffff) {
@@ -1081,181 +1156,144 @@
 
 		if (args->DR1 || args->DR4 || args->cliprects_ptr) {
 			DRM_DEBUG("0 cliprects but dirt in cliprects fields\n");
-			return -EINVAL;
-		}
-	}
-
-	ret = i915_gem_execbuffer_move_to_gpu(ring, vmas);
-	if (ret)
-		goto error;
-
-	ret = i915_switch_context(ring, ctx);
-	if (ret)
-		goto error;
-
-	instp_mode = args->flags & I915_EXEC_CONSTANTS_MASK;
-	instp_mask = I915_EXEC_CONSTANTS_MASK;
-	switch (instp_mode) {
-	case I915_EXEC_CONSTANTS_REL_GENERAL:
-	case I915_EXEC_CONSTANTS_ABSOLUTE:
-	case I915_EXEC_CONSTANTS_REL_SURFACE:
-		if (instp_mode != 0 && ring != &dev_priv->ring[RCS]) {
-			DRM_DEBUG("non-0 rel constants mode on non-RCS\n");
 			ret = -EINVAL;
-			goto error;
-		}
-
-		if (instp_mode != dev_priv->relative_constants_mode) {
-			if (INTEL_INFO(dev)->gen < 4) {
-				DRM_DEBUG("no rel constants on pre-gen4\n");
-				ret = -EINVAL;
-				goto error;
-			}
-
-			if (INTEL_INFO(dev)->gen > 5 &&
-			    instp_mode == I915_EXEC_CONSTANTS_REL_SURFACE) {
-				DRM_DEBUG("rel surface constants mode invalid on gen5+\n");
-				ret = -EINVAL;
-				goto error;
-			}
-
-			/* The HW changed the meaning on this bit on gen6 */
-			if (INTEL_INFO(dev)->gen >= 6)
-				instp_mask &= ~I915_EXEC_CONSTANTS_REL_SURFACE;
+			goto out;
 		}
-		break;
-	default:
-		DRM_DEBUG("execbuf with unknown constants: %d\n", instp_mode);
-		ret = -EINVAL;
-		goto error;
 	}
 
-	if (ring == &dev_priv->ring[RCS] &&
-			instp_mode != dev_priv->relative_constants_mode) {
-		ret = intel_ring_begin(ring, 4);
-		if (ret)
-			goto error;
+	rq = i915_request_create(ctx, engine);
+	if (IS_ERR(rq)) {
+		ret = PTR_ERR(rq);
+		goto out_cliprects;
+	}
 
-		intel_ring_emit(ring, MI_NOOP);
-		intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
-		intel_ring_emit(ring, INSTPM);
-		intel_ring_emit(ring, instp_mask << 16 | instp_mode);
-		intel_ring_advance(ring);
+	ret = vmas_move_to_rq(eb, rq);
+	if (ret)
+		goto out_rq;
 
-		dev_priv->relative_constants_mode = instp_mode;
-	}
+	ret = set_contants_base(rq, args);
+	if (ret)
+		goto out_rq;
 
 	if (args->flags & I915_EXEC_GEN7_SOL_RESET) {
-		ret = i915_reset_gen7_sol_offsets(dev, ring);
+		ret = reset_sol_offsets(rq);
 		if (ret)
-			goto error;
+			goto out_rq;
 	}
 
-	exec_len = args->batch_len;
+	/* snb/ivb/vlv conflate the "batch in ppgtt" bit with the "non-secure
+	 * batch" bit. Hence we need to pin secure batches into the global gtt.
+	 * hsw should have this fixed, but bdw mucks it up again.
+	 */
+	exec_start = args->batch_start_offset;
+	if (flags & I915_DISPATCH_SECURE) {
+		struct i915_vma *ggtt;
+
+		/*
+		 * So on first glance it looks freaky that we pin the batch here
+		 * outside of the reservation loop. But:
+		 * - The batch is already pinned into the relevant ppgtt, so we
+		 *   already have the backing storage fully allocated.
+		 * - No other BO uses the global gtt (well contexts, but meh),
+		 *   so we don't really have issues with mutliple objects not
+		 *   fitting due to fragmentation.
+		 * So this is actually safe.
+		 */
+		ggtt = i915_gem_obj_get_ggtt(eb->batch->obj);
+		ret = i915_vma_pin(ggtt, 0, PIN_GLOBAL);
+		if (ret) {
+			i915_vma_put(ggtt);
+			goto out_rq;
+		}
+		exec_start += ggtt->node.start;
+
+		if (!i915_is_ggtt(eb->vm)) {
+			drm_gem_object_reference(&eb->batch->obj->base);
+			i915_request_add_vma(rq, ggtt, 0);
+		} else
+			i915_vma_put(ggtt);
+	} else
+		exec_start += eb->batch->node.start;
+
 	if (cliprects) {
 		for (i = 0; i < args->num_cliprects; i++) {
-			ret = i915_emit_box(dev, &cliprects[i],
-					    args->DR1, args->DR4);
+			ret = emit_box(rq, &cliprects[i],
+				       args->DR1, args->DR4);
 			if (ret)
-				goto error;
+				goto out_rq;
 
-			ret = ring->dispatch_execbuffer(ring,
-							exec_start, exec_len,
-							flags);
+			ret = i915_request_emit_batchbuffer(rq, eb->batch,
+							    exec_start,
+							    args->batch_len,
+							    flags);
 			if (ret)
-				goto error;
+				goto out_rq;
 		}
 	} else {
-		ret = ring->dispatch_execbuffer(ring,
-						exec_start, exec_len,
-						flags);
+		ret = i915_request_emit_batchbuffer(rq, eb->batch,
+						    exec_start,
+						    args->batch_len,
+						    flags);
 		if (ret)
-			return ret;
+			goto out_rq;
 	}
 
-	trace_i915_gem_ring_dispatch(ring, intel_ring_get_seqno(ring), flags);
-
-	i915_gem_execbuffer_move_to_active(vmas, ring);
-	i915_gem_execbuffer_retire_commands(dev, file, ring, batch_obj);
+	ret = i915_request_commit(rq);
+	if (ret)
+		goto out_rq;
 
-error:
+out_rq:
+	i915_request_put(rq);
+out_cliprects:
 	kfree(cliprects);
+out:
 	return ret;
 }
 
 /**
  * Find one BSD ring to dispatch the corresponding BSD command.
- * The Ring ID is returned.
  */
-static int gen8_dispatch_bsd_ring(struct drm_device *dev,
-				  struct drm_file *file)
+static struct intel_engine_cs *
+gen8_select_bsd_engine(struct drm_i915_private *dev_priv,
+		       struct drm_file *file)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_i915_file_private *file_priv = file->driver_priv;
 
-	/* Check whether the file_priv is using one ring */
-	if (file_priv->bsd_ring)
-		return file_priv->bsd_ring->id;
-	else {
-		/* If no, use the ping-pong mechanism to select one ring */
-		int ring_id;
+	/* Use the ping-pong mechanism to select one ring for this client */
+	if (file_priv->bsd_engine == NULL) {
+		int id;
 
-		mutex_lock(&dev->struct_mutex);
+		mutex_lock(&dev_priv->dev->struct_mutex);
 		if (dev_priv->mm.bsd_ring_dispatch_index == 0) {
-			ring_id = VCS;
+			id = VCS;
 			dev_priv->mm.bsd_ring_dispatch_index = 1;
 		} else {
-			ring_id = VCS2;
+			id = VCS2;
 			dev_priv->mm.bsd_ring_dispatch_index = 0;
 		}
-		file_priv->bsd_ring = &dev_priv->ring[ring_id];
-		mutex_unlock(&dev->struct_mutex);
-		return ring_id;
+		file_priv->bsd_engine = &dev_priv->engine[id];
+		mutex_unlock(&dev_priv->dev->struct_mutex);
 	}
-}
-
-static struct drm_i915_gem_object *
-eb_get_batch(struct eb_vmas *eb)
-{
-	struct i915_vma *vma = list_entry(eb->vmas.prev, typeof(*vma), exec_list);
-
-	/*
-	 * SNA is doing fancy tricks with compressing batch buffers, which leads
-	 * to negative relocation deltas. Usually that works out ok since the
-	 * relocate address is still positive, except when the batch is placed
-	 * very low in the GTT. Ensure this doesn't happen.
-	 *
-	 * Note that actual hangs have only been observed on gen7, but for
-	 * paranoia do it everywhere.
-	 */
-	vma->exec_entry->flags |= __EXEC_OBJECT_NEEDS_BIAS;
 
-	return vma->obj;
+	return file_priv->bsd_engine;
 }
 
 static int
-i915_gem_do_execbuffer(struct drm_device *dev, void *data,
+i915_gem_do_execbuffer(struct drm_i915_private *dev_priv, void *data,
 		       struct drm_file *file,
 		       struct drm_i915_gem_execbuffer2 *args,
 		       struct drm_i915_gem_exec_object2 *exec)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct eb_vmas *eb;
-	struct drm_i915_gem_object *batch_obj;
-	struct intel_engine_cs *ring;
+	struct intel_engine_cs *engine;
 	struct intel_context *ctx;
-	struct i915_address_space *vm;
 	const u32 ctx_id = i915_execbuffer2_get_context_id(*args);
-	u64 exec_start = args->batch_start_offset;
 	u32 flags;
 	int ret;
 	bool need_relocs;
 
-	if (!i915_gem_check_execbuffer(args))
-		return -EINVAL;
-
-	ret = validate_exec_list(exec, args->buffer_count);
-	if (ret)
+	ret = validate_exec_list(dev_priv, args, exec);
+	if (unlikely(ret))
 		return ret;
 
 	flags = 0;
@@ -1275,18 +1313,16 @@
 	}
 
 	if ((args->flags & I915_EXEC_RING_MASK) == I915_EXEC_DEFAULT)
-		ring = &dev_priv->ring[RCS];
+		engine = &dev_priv->engine[RCS];
 	else if ((args->flags & I915_EXEC_RING_MASK) == I915_EXEC_BSD) {
-		if (HAS_BSD2(dev)) {
-			int ring_id;
-			ring_id = gen8_dispatch_bsd_ring(dev, file);
-			ring = &dev_priv->ring[ring_id];
-		} else
-			ring = &dev_priv->ring[VCS];
+		if (HAS_BSD2(dev_priv))
+			engine = gen8_select_bsd_engine(dev_priv, file);
+		else
+			engine = &dev_priv->engine[VCS];
 	} else
-		ring = &dev_priv->ring[(args->flags & I915_EXEC_RING_MASK) - 1];
+		engine = &dev_priv->engine[(args->flags & I915_EXEC_RING_MASK) - 1];
 
-	if (!intel_ring_initialized(ring)) {
+	if (!intel_engine_initialized(engine)) {
 		DRM_DEBUG("execbuf with invalid ring: %d\n",
 			  (int)(args->flags & I915_EXEC_RING_MASK));
 		return -EINVAL;
@@ -1299,48 +1335,36 @@
 
 	intel_runtime_pm_get(dev_priv);
 
-	ret = i915_mutex_lock_interruptible(dev);
+	ret = i915_mutex_lock_interruptible(dev_priv->dev);
 	if (ret)
 		goto pre_mutex_err;
 
-	if (dev_priv->ums.mm_suspended) {
-		mutex_unlock(&dev->struct_mutex);
-		ret = -EBUSY;
-		goto pre_mutex_err;
-	}
-
-	ctx = i915_gem_validate_context(dev, file, ring, ctx_id);
+	ctx = i915_gem_validate_context(file, engine, ctx_id);
 	if (IS_ERR(ctx)) {
-		mutex_unlock(&dev->struct_mutex);
+		mutex_unlock(&dev_priv->dev->struct_mutex);
 		ret = PTR_ERR(ctx);
 		goto pre_mutex_err;
 	}
 
 	i915_gem_context_reference(ctx);
 
-	vm = ctx->vm;
-	if (!USES_FULL_PPGTT(dev))
-		vm = &dev_priv->gtt.base;
-
 	eb = eb_create(args);
 	if (eb == NULL) {
 		i915_gem_context_unreference(ctx);
-		mutex_unlock(&dev->struct_mutex);
+		mutex_unlock(&dev_priv->dev->struct_mutex);
 		ret = -ENOMEM;
 		goto pre_mutex_err;
 	}
+	eb->vm = ctx->ppgtt ? &ctx->ppgtt->base : &dev_priv->gtt.base;
 
 	/* Look up object handles */
-	ret = eb_lookup_vmas(eb, exec, args, vm, file);
+	ret = eb_lookup_vmas(eb, exec, args, file);
 	if (ret)
 		goto err;
 
-	/* take note of the batch buffer before we might reorder the lists */
-	batch_obj = eb_get_batch(eb);
-
 	/* Move the objects en-masse into the GTT, evicting if necessary. */
 	need_relocs = (args->flags & I915_EXEC_NO_RELOC) == 0;
-	ret = i915_gem_execbuffer_reserve(ring, &eb->vmas, &need_relocs);
+	ret = i915_gem_execbuffer_reserve(engine, eb, &need_relocs);
 	if (ret)
 		goto err;
 
@@ -1349,68 +1373,29 @@
 		ret = i915_gem_execbuffer_relocate(eb);
 	if (ret) {
 		if (ret == -EFAULT) {
-			ret = i915_gem_execbuffer_relocate_slow(dev, args, file, ring,
+			ret = i915_gem_execbuffer_relocate_slow(dev_priv, args, file, engine,
 								eb, exec);
-			BUG_ON(!mutex_is_locked(&dev->struct_mutex));
+			BUG_ON(!mutex_is_locked(&dev_priv->dev->struct_mutex));
 		}
 		if (ret)
 			goto err;
 	}
 
 	/* Set the pending read domains for the batch buffer to COMMAND */
-	if (batch_obj->base.pending_write_domain) {
+	if (eb->batch->obj->base.pending_write_domain) {
 		DRM_DEBUG("Attempting to use self-modifying batch buffer\n");
 		ret = -EINVAL;
 		goto err;
 	}
-	batch_obj->base.pending_read_domains |= I915_GEM_DOMAIN_COMMAND;
-
-	if (i915_needs_cmd_parser(ring)) {
-		ret = i915_parse_cmds(ring,
-				      batch_obj,
-				      args->batch_start_offset,
-				      file->is_master);
-		if (ret)
-			goto err;
-
-		/*
-		 * XXX: Actually do this when enabling batch copy...
-		 *
-		 * Set the DISPATCH_SECURE bit to remove the NON_SECURE bit
-		 * from MI_BATCH_BUFFER_START commands issued in the
-		 * dispatch_execbuffer implementations. We specifically don't
-		 * want that set when the command parser is enabled.
-		 */
-	}
-
-	/* snb/ivb/vlv conflate the "batch in ppgtt" bit with the "non-secure
-	 * batch" bit. Hence we need to pin secure batches into the global gtt.
-	 * hsw should have this fixed, but bdw mucks it up again. */
-	if (flags & I915_DISPATCH_SECURE &&
-	    !batch_obj->has_global_gtt_mapping) {
-		/* When we have multiple VMs, we'll need to make sure that we
-		 * allocate space first */
-		struct i915_vma *vma = i915_gem_obj_to_ggtt(batch_obj);
-		BUG_ON(!vma);
-		vma->bind_vma(vma, batch_obj->cache_level, GLOBAL_BIND);
-	}
-
-	if (flags & I915_DISPATCH_SECURE)
-		exec_start += i915_gem_obj_ggtt_offset(batch_obj);
-	else
-		exec_start += i915_gem_obj_offset(batch_obj, vm);
-
-	ret = legacy_ringbuffer_submission(dev, file, ring, ctx,
-			args, &eb->vmas, batch_obj, exec_start, flags);
-	if (ret)
-		goto err;
+	eb->batch->obj->base.pending_read_domains |= I915_GEM_DOMAIN_COMMAND;
 
+	ret = submit_execbuf(engine, ctx, args, eb, flags);
 err:
 	/* the request owns the ref now */
 	i915_gem_context_unreference(ctx);
 	eb_destroy(eb);
 
-	mutex_unlock(&dev->struct_mutex);
+	mutex_unlock(&dev_priv->dev->struct_mutex);
 
 pre_mutex_err:
 	/* intel_gpu_busy should also get a ref, so it will free when the device
@@ -1482,7 +1467,7 @@
 	exec2.flags = I915_EXEC_RENDER;
 	i915_execbuffer2_set_context_id(exec2, 0);
 
-	ret = i915_gem_do_execbuffer(dev, data, file, &exec2, exec2_list);
+	ret = i915_gem_do_execbuffer(to_i915(dev), data, file, &exec2, exec2_list);
 	if (!ret) {
 		struct drm_i915_gem_exec_object __user *user_exec_list =
 			to_user_ptr(args->buffers_ptr);
@@ -1546,7 +1531,7 @@
 		return -EFAULT;
 	}
 
-	ret = i915_gem_do_execbuffer(dev, data, file, args, exec2_list);
+	ret = i915_gem_do_execbuffer(to_i915(dev), data, file, args, exec2_list);
 	if (!ret) {
 		/* Copy the new buffer offsets back to the user's exec list. */
 		struct drm_i915_gem_exec_object2 __user *user_exec_list =
diff -urN a/drivers/gpu/drm/i915/i915_gem_gtt.c b/drivers/gpu/drm/i915/i915_gem_gtt.c
--- a/drivers/gpu/drm/i915/i915_gem_gtt.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_gem_gtt.c	2014-11-24 06:25:44.103887450 -0700
@@ -33,26 +33,28 @@
 static void bdw_setup_private_ppat(struct drm_i915_private *dev_priv);
 static void chv_setup_private_ppat(struct drm_i915_private *dev_priv);
 
-bool intel_enable_ppgtt(struct drm_device *dev, bool full)
+static int sanitize_enable_ppgtt(struct drm_device *dev, int enable_ppgtt)
 {
-	if (i915.enable_ppgtt == 0)
-		return false;
-
-	if (i915.enable_ppgtt == 1 && full)
-		return false;
+	bool has_aliasing_ppgtt;
+	bool has_full_ppgtt;
 
-	return true;
-}
+	has_aliasing_ppgtt = INTEL_INFO(dev)->gen >= 6;
+	has_full_ppgtt = INTEL_INFO(dev)->gen >= 7;
+	if (IS_GEN8(dev))
+		has_full_ppgtt = false; /* XXX why? */
 
-static int sanitize_enable_ppgtt(struct drm_device *dev, int enable_ppgtt)
-{
-	if (enable_ppgtt == 0 || !HAS_ALIASING_PPGTT(dev))
+	/*
+	 * We don't allow disabling PPGTT for gen9+ as it's a requirement for
+	 * execlists, the sole mechanism available to submit work.
+	 */
+	if (INTEL_INFO(dev)->gen < 9 &&
+	    (enable_ppgtt == 0 || !has_aliasing_ppgtt))
 		return 0;
 
 	if (enable_ppgtt == 1)
 		return 1;
 
-	if (enable_ppgtt == 2 && HAS_PPGTT(dev))
+	if (enable_ppgtt == 2 && has_full_ppgtt)
 		return 2;
 
 #ifdef CONFIG_INTEL_IOMMU
@@ -70,16 +72,9 @@
 		return 0;
 	}
 
-	return HAS_ALIASING_PPGTT(dev) ? 1 : 0;
+	return has_aliasing_ppgtt ? 1 : 0;
 }
 
-
-static void ppgtt_bind_vma(struct i915_vma *vma,
-			   enum i915_cache_level cache_level,
-			   u32 flags);
-static void ppgtt_unbind_vma(struct i915_vma *vma);
-static int gen8_ppgtt_enable(struct i915_hw_ppgtt *ppgtt);
-
 static inline gen8_gtt_pte_t gen8_pte_encode(dma_addr_t addr,
 					     enum i915_cache_level level,
 					     bool valid)
@@ -168,9 +163,6 @@
 	gen6_gtt_pte_t pte = valid ? GEN6_PTE_VALID : 0;
 	pte |= GEN6_PTE_ADDR_ENCODE(addr);
 
-	/* Mark the page as writeable.  Other platforms don't have a
-	 * setting for read-only/writable, so this matches that behavior.
-	 */
 	if (!(flags & PTE_READ_ONLY))
 		pte |= BYT_PTE_WRITEABLE;
 
@@ -215,38 +207,28 @@
 }
 
 /* Broadwell Page Directory Pointer Descriptors */
-static int gen8_write_pdp(struct intel_engine_cs *ring, unsigned entry,
-			   uint64_t val, bool synchronous)
+static int gen8_write_pdp(struct i915_gem_request *rq, unsigned entry, uint64_t val)
 {
-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
-	int ret;
+	struct intel_ringbuffer *ring;
 
 	BUG_ON(entry >= 4);
 
-	if (synchronous) {
-		I915_WRITE(GEN8_RING_PDP_UDW(ring, entry), val >> 32);
-		I915_WRITE(GEN8_RING_PDP_LDW(ring, entry), (u32)val);
-		return 0;
-	}
-
-	ret = intel_ring_begin(ring, 6);
-	if (ret)
-		return ret;
+	ring = intel_ring_begin(rq, 5);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
-	intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
-	intel_ring_emit(ring, GEN8_RING_PDP_UDW(ring, entry));
-	intel_ring_emit(ring, (u32)(val >> 32));
-	intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
-	intel_ring_emit(ring, GEN8_RING_PDP_LDW(ring, entry));
-	intel_ring_emit(ring, (u32)(val));
+	intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(2));
+	intel_ring_emit(ring, GEN8_RING_PDP_UDW(rq->engine, entry));
+	intel_ring_emit(ring, upper_32_bits(val));
+	intel_ring_emit(ring, GEN8_RING_PDP_LDW(rq->engine, entry));
+	intel_ring_emit(ring, lower_32_bits(val));
 	intel_ring_advance(ring);
 
 	return 0;
 }
 
-static int gen8_mm_switch(struct i915_hw_ppgtt *ppgtt,
-			  struct intel_engine_cs *ring,
-			  bool synchronous)
+static int gen8_mm_switch(struct i915_gem_request *rq,
+			  struct i915_hw_ppgtt *ppgtt)
 {
 	int i, ret;
 
@@ -255,7 +237,7 @@
 
 	for (i = used_pd - 1; i >= 0; i--) {
 		dma_addr_t addr = ppgtt->pd_dma_addr[i];
-		ret = gen8_write_pdp(ring, i, addr, synchronous);
+		ret = gen8_write_pdp(rq, i, addr);
 		if (ret)
 			return ret;
 	}
@@ -263,10 +245,10 @@
 	return 0;
 }
 
-static void gen8_ppgtt_clear_range(struct i915_address_space *vm,
-				   uint64_t start,
-				   uint64_t length,
-				   bool use_scratch)
+static int gen8_ppgtt_clear_range(struct i915_address_space *vm,
+				  uint64_t start,
+				  uint64_t length,
+				  bool use_scratch)
 {
 	struct i915_hw_ppgtt *ppgtt =
 		container_of(vm, struct i915_hw_ppgtt, base);
@@ -304,12 +286,14 @@
 			pde = 0;
 		}
 	}
+
+	return 0;
 }
 
-static void gen8_ppgtt_insert_entries(struct i915_address_space *vm,
-				      struct sg_table *pages,
-				      uint64_t start,
-				      enum i915_cache_level cache_level, u32 unused)
+static int gen8_ppgtt_insert_entries(struct i915_address_space *vm,
+				     struct sg_table *pages,
+				     uint64_t start,
+				     enum i915_cache_level cache_level, u32 unused)
 {
 	struct i915_hw_ppgtt *ppgtt =
 		container_of(vm, struct i915_hw_ppgtt, base);
@@ -348,6 +332,8 @@
 			drm_clflush_virt_range(pt_vaddr, PAGE_SIZE);
 		kunmap_atomic(pt_vaddr);
 	}
+
+	return 0;
 }
 
 static void gen8_free_page_tables(struct page **pt_pages)
@@ -403,9 +389,6 @@
 	struct i915_hw_ppgtt *ppgtt =
 		container_of(vm, struct i915_hw_ppgtt, base);
 
-	list_del(&vm->global_link);
-	drm_mm_takedown(&vm->mm);
-
 	gen8_ppgtt_unmap_pages(ppgtt);
 	gen8_ppgtt_free(ppgtt);
 }
@@ -615,7 +598,6 @@
 		kunmap_atomic(pd_vaddr);
 	}
 
-	ppgtt->enable = gen8_ppgtt_enable;
 	ppgtt->switch_mm = gen8_mm_switch;
 	ppgtt->base.clear_range = gen8_ppgtt_clear_range;
 	ppgtt->base.insert_entries = gen8_ppgtt_insert_entries;
@@ -640,35 +622,19 @@
 
 static void gen6_dump_ppgtt(struct i915_hw_ppgtt *ppgtt, struct seq_file *m)
 {
-	struct drm_i915_private *dev_priv = ppgtt->base.dev->dev_private;
 	struct i915_address_space *vm = &ppgtt->base;
-	gen6_gtt_pte_t __iomem *pd_addr;
 	gen6_gtt_pte_t scratch_pte;
-	uint32_t pd_entry;
 	int pte, pde;
 
-	scratch_pte = vm->pte_encode(vm->scratch.addr, I915_CACHE_LLC, true, 0);
+	if (ppgtt->state->pages == NULL)
+		return;
 
-	pd_addr = (gen6_gtt_pte_t __iomem *)dev_priv->gtt.gsm +
-		ppgtt->pd_offset / sizeof(gen6_gtt_pte_t);
+	scratch_pte = vm->pte_encode(vm->scratch.addr, I915_CACHE_LLC, true, 0);
 
-	seq_printf(m, "  VM %p (pd_offset %x-%x):\n", vm,
-		   ppgtt->pd_offset, ppgtt->pd_offset + ppgtt->num_pd_entries);
 	for (pde = 0; pde < ppgtt->num_pd_entries; pde++) {
-		u32 expected;
 		gen6_gtt_pte_t *pt_vaddr;
-		dma_addr_t pt_addr = ppgtt->pt_dma_addr[pde];
-		pd_entry = readl(pd_addr + pde);
-		expected = (GEN6_PDE_ADDR_ENCODE(pt_addr) | GEN6_PDE_VALID);
-
-		if (pd_entry != expected)
-			seq_printf(m, "\tPDE #%d mismatch: Actual PDE: %x Expected PDE: %x\n",
-				   pde,
-				   pd_entry,
-				   expected);
-		seq_printf(m, "\tPDE: %x\n", pd_entry);
 
-		pt_vaddr = kmap_atomic(ppgtt->pt_pages[pde]);
+		pt_vaddr = kmap_atomic(i915_gem_object_get_page(ppgtt->state, pde));
 		for (pte = 0; pte < I915_PPGTT_PT_ENTRIES; pte+=4) {
 			unsigned long va =
 				(pde * PAGE_SIZE * I915_PPGTT_PT_ENTRIES) +
@@ -694,183 +660,80 @@
 	}
 }
 
-static void gen6_write_pdes(struct i915_hw_ppgtt *ppgtt)
-{
-	struct drm_i915_private *dev_priv = ppgtt->base.dev->dev_private;
-	gen6_gtt_pte_t __iomem *pd_addr;
-	uint32_t pd_entry;
-	int i;
-
-	WARN_ON(ppgtt->pd_offset & 0x3f);
-	pd_addr = (gen6_gtt_pte_t __iomem*)dev_priv->gtt.gsm +
-		ppgtt->pd_offset / sizeof(gen6_gtt_pte_t);
-	for (i = 0; i < ppgtt->num_pd_entries; i++) {
-		dma_addr_t pt_addr;
-
-		pt_addr = ppgtt->pt_dma_addr[i];
-		pd_entry = GEN6_PDE_ADDR_ENCODE(pt_addr);
-		pd_entry |= GEN6_PDE_VALID;
-
-		writel(pd_entry, pd_addr + i);
-	}
-	readl(pd_addr);
-}
-
 static uint32_t get_pd_offset(struct i915_hw_ppgtt *ppgtt)
 {
-	BUG_ON(ppgtt->pd_offset & 0x3f);
-
-	return (ppgtt->pd_offset / 64) << 16;
+	uint64_t offset = i915_gem_obj_to_ggtt(ppgtt->state)->node.start / PAGE_SIZE;
+	return (offset * sizeof(gen6_gtt_pte_t) / 64) << 16;
 }
 
-static int hsw_mm_switch(struct i915_hw_ppgtt *ppgtt,
-			 struct intel_engine_cs *ring,
-			 bool synchronous)
+static int gen7_mm_switch(struct i915_gem_request *rq,
+			  struct i915_hw_ppgtt *ppgtt)
 {
-	struct drm_device *dev = ppgtt->base.dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_ringbuffer *ring;
 	int ret;
 
-	/* If we're in reset, we can assume the GPU is sufficiently idle to
-	 * manually frob these bits. Ideally we could use the ring functions,
-	 * except our error handling makes it quite difficult (can't use
-	 * intel_ring_begin, ring->flush, or intel_ring_advance)
-	 *
-	 * FIXME: We should try not to special case reset
-	 */
-	if (synchronous ||
-	    i915_reset_in_progress(&dev_priv->gpu_error)) {
-		WARN_ON(ppgtt != dev_priv->mm.aliasing_ppgtt);
-		I915_WRITE(RING_PP_DIR_DCLV(ring), PP_DIR_DCLV_2G);
-		I915_WRITE(RING_PP_DIR_BASE(ring), get_pd_offset(ppgtt));
-		POSTING_READ(RING_PP_DIR_BASE(ring));
-		return 0;
-	}
-
-	/* NB: TLBs must be flushed and invalidated before a switch */
-	ret = ring->flush(ring, I915_GEM_GPU_DOMAINS, I915_GEM_GPU_DOMAINS);
+	rq->pending_flush = ~0; /* XXX force the flush */
+	ret = i915_request_emit_flush(rq, I915_COMMAND_BARRIER);
 	if (ret)
 		return ret;
 
-	ret = intel_ring_begin(ring, 6);
-	if (ret)
-		return ret;
+	ring = intel_ring_begin(rq, 5);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
 	intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(2));
-	intel_ring_emit(ring, RING_PP_DIR_DCLV(ring));
+	intel_ring_emit(ring, RING_PP_DIR_DCLV(rq->engine));
 	intel_ring_emit(ring, PP_DIR_DCLV_2G);
-	intel_ring_emit(ring, RING_PP_DIR_BASE(ring));
+	intel_ring_emit(ring, RING_PP_DIR_BASE(rq->engine));
 	intel_ring_emit(ring, get_pd_offset(ppgtt));
-	intel_ring_emit(ring, MI_NOOP);
 	intel_ring_advance(ring);
 
-	return 0;
-}
-
-static int gen7_mm_switch(struct i915_hw_ppgtt *ppgtt,
-			  struct intel_engine_cs *ring,
-			  bool synchronous)
-{
-	struct drm_device *dev = ppgtt->base.dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	int ret;
-
-	/* If we're in reset, we can assume the GPU is sufficiently idle to
-	 * manually frob these bits. Ideally we could use the ring functions,
-	 * except our error handling makes it quite difficult (can't use
-	 * intel_ring_begin, ring->flush, or intel_ring_advance)
-	 *
-	 * FIXME: We should try not to special case reset
-	 */
-	if (synchronous ||
-	    i915_reset_in_progress(&dev_priv->gpu_error)) {
-		WARN_ON(ppgtt != dev_priv->mm.aliasing_ppgtt);
-		I915_WRITE(RING_PP_DIR_DCLV(ring), PP_DIR_DCLV_2G);
-		I915_WRITE(RING_PP_DIR_BASE(ring), get_pd_offset(ppgtt));
-		POSTING_READ(RING_PP_DIR_BASE(ring));
-		return 0;
-	}
-
-	/* NB: TLBs must be flushed and invalidated before a switch */
-	ret = ring->flush(ring, I915_GEM_GPU_DOMAINS, I915_GEM_GPU_DOMAINS);
-	if (ret)
-		return ret;
-
-	ret = intel_ring_begin(ring, 6);
-	if (ret)
-		return ret;
-
-	intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(2));
-	intel_ring_emit(ring, RING_PP_DIR_DCLV(ring));
-	intel_ring_emit(ring, PP_DIR_DCLV_2G);
-	intel_ring_emit(ring, RING_PP_DIR_BASE(ring));
-	intel_ring_emit(ring, get_pd_offset(ppgtt));
-	intel_ring_emit(ring, MI_NOOP);
-	intel_ring_advance(ring);
-
-	/* XXX: RCS is the only one to auto invalidate the TLBs? */
-	if (ring->id != RCS) {
-		ret = ring->flush(ring, I915_GEM_GPU_DOMAINS, I915_GEM_GPU_DOMAINS);
-		if (ret)
-			return ret;
-	}
+	rq->pending_flush |= I915_INVALIDATE_CACHES;
 
 	return 0;
 }
 
-static int gen6_mm_switch(struct i915_hw_ppgtt *ppgtt,
-			  struct intel_engine_cs *ring,
-			  bool synchronous)
+static int gen6_mm_switch(struct i915_gem_request *rq,
+			  struct i915_hw_ppgtt *ppgtt)
 {
-	struct drm_device *dev = ppgtt->base.dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-
-	if (!synchronous)
-		return 0;
-
-	I915_WRITE(RING_PP_DIR_DCLV(ring), PP_DIR_DCLV_2G);
-	I915_WRITE(RING_PP_DIR_BASE(ring), get_pd_offset(ppgtt));
-
-	POSTING_READ(RING_PP_DIR_DCLV(ring));
-
-	return 0;
+	return -ENODEV;
 }
 
-static int gen8_ppgtt_enable(struct i915_hw_ppgtt *ppgtt)
+static int gen8_ppgtt_enable(struct drm_device *dev)
 {
-	struct drm_device *dev = ppgtt->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
-	int j, ret;
+	struct intel_engine_cs *engine;
+	int j;
 
-	for_each_ring(ring, dev_priv, j) {
-		I915_WRITE(RING_MODE_GEN7(ring),
-			   _MASKED_BIT_ENABLE(GFX_PPGTT_ENABLE));
+	for_each_engine(engine, dev_priv, j) {
+		I915_WRITE(RING_MODE_GEN7(engine),
+				_MASKED_BIT_ENABLE(GFX_PPGTT_ENABLE));
 
-		/* We promise to do a switch later with FULL PPGTT. If this is
-		 * aliasing, this is the one and only switch we'll do */
-		if (USES_FULL_PPGTT(dev))
-			continue;
+		if (dev_priv->mm.aliasing_ppgtt) {
+			struct i915_gem_request *rq;
+			int ret;
 
-		ret = ppgtt->switch_mm(ppgtt, ring, true);
-		if (ret)
-			goto err_out;
+			rq = i915_request_create(engine->default_context,
+						 engine);
+			if (IS_ERR(rq))
+				return PTR_ERR(rq);
+
+			ret = gen8_mm_switch(rq, dev_priv->mm.aliasing_ppgtt);
+			if (ret == 0)
+				ret = i915_request_commit(rq);
+			i915_request_put(rq);
+			if (ret)
+				return ret;
+		}
 	}
 
 	return 0;
-
-err_out:
-	for_each_ring(ring, dev_priv, j)
-		I915_WRITE(RING_MODE_GEN7(ring),
-			   _MASKED_BIT_DISABLE(GFX_PPGTT_ENABLE));
-	return ret;
 }
 
-static int gen7_ppgtt_enable(struct i915_hw_ppgtt *ppgtt)
+static int gen7_ppgtt_enable(struct drm_device *dev)
 {
-	struct drm_device *dev = ppgtt->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
+	struct intel_engine_cs *engine;
 	uint32_t ecochk, ecobits;
 	int i;
 
@@ -886,31 +749,26 @@
 	}
 	I915_WRITE(GAM_ECOCHK, ecochk);
 
-	for_each_ring(ring, dev_priv, i) {
-		int ret;
+	for_each_engine(engine, dev_priv, i) {
 		/* GFX_MODE is per-ring on gen7+ */
-		I915_WRITE(RING_MODE_GEN7(ring),
+		I915_WRITE(RING_MODE_GEN7(engine),
 			   _MASKED_BIT_ENABLE(GFX_PPGTT_ENABLE));
 
-		/* We promise to do a switch later with FULL PPGTT. If this is
-		 * aliasing, this is the one and only switch we'll do */
-		if (USES_FULL_PPGTT(dev))
-			continue;
+		if (dev_priv->mm.aliasing_ppgtt) {
+			I915_WRITE(RING_PP_DIR_DCLV(engine), PP_DIR_DCLV_2G);
+			I915_WRITE(RING_PP_DIR_BASE(engine), get_pd_offset(dev_priv->mm.aliasing_ppgtt));
+		}
 
-		ret = ppgtt->switch_mm(ppgtt, ring, true);
-		if (ret)
-			return ret;
 	}
-
+	POSTING_READ(RING_PP_DIR_DCLV(RCS_ENGINE(dev_priv)));
 	return 0;
 }
 
-static int gen6_ppgtt_enable(struct i915_hw_ppgtt *ppgtt)
+static int gen6_ppgtt_enable(struct drm_device *dev)
 {
-	struct drm_device *dev = ppgtt->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
 	uint32_t ecochk, gab_ctl, ecobits;
+	struct intel_engine_cs *engine;
 	int i;
 
 	ecobits = I915_READ(GAC_ECO_BITS);
@@ -925,20 +783,27 @@
 
 	I915_WRITE(GFX_MODE, _MASKED_BIT_ENABLE(GFX_PPGTT_ENABLE));
 
-	for_each_ring(ring, dev_priv, i) {
-		int ret = ppgtt->switch_mm(ppgtt, ring, true);
-		if (ret)
-			return ret;
-	}
+	for_each_engine(engine, dev_priv, i) {
+		if (dev_priv->mm.aliasing_ppgtt) {
+			I915_WRITE(RING_PP_DIR_DCLV(engine), PP_DIR_DCLV_2G);
+			I915_WRITE(RING_PP_DIR_BASE(engine), get_pd_offset(dev_priv->mm.aliasing_ppgtt));
+		}
 
+	}
+	POSTING_READ(RING_PP_DIR_DCLV(RCS_ENGINE(dev_priv)));
 	return 0;
 }
 
 /* PPGTT support for Sandybdrige/Gen6 and later */
-static void gen6_ppgtt_clear_range(struct i915_address_space *vm,
-				   uint64_t start,
-				   uint64_t length,
-				   bool use_scratch)
+static inline void *kmap_pt(struct i915_hw_ppgtt *ppgtt, int pt)
+{
+	return kmap_atomic(i915_gem_object_get_page(ppgtt->state, pt));
+}
+
+static int gen6_ppgtt_clear_range(struct i915_address_space *vm,
+				  uint64_t start,
+				  uint64_t length,
+				  bool use_scratch)
 {
 	struct i915_hw_ppgtt *ppgtt =
 		container_of(vm, struct i915_hw_ppgtt, base);
@@ -948,6 +813,11 @@
 	unsigned act_pt = first_entry / I915_PPGTT_PT_ENTRIES;
 	unsigned first_pte = first_entry % I915_PPGTT_PT_ENTRIES;
 	unsigned last_pte, i;
+	int ret;
+
+	ret = i915_gem_object_get_pages(ppgtt->state);
+	if (ret)
+		return ret;
 
 	scratch_pte = vm->pte_encode(vm->scratch.addr, I915_CACHE_LLC, true, 0);
 
@@ -956,7 +826,7 @@
 		if (last_pte > I915_PPGTT_PT_ENTRIES)
 			last_pte = I915_PPGTT_PT_ENTRIES;
 
-		pt_vaddr = kmap_atomic(ppgtt->pt_pages[act_pt]);
+		pt_vaddr = kmap_pt(ppgtt, act_pt);
 
 		for (i = first_pte; i < last_pte; i++)
 			pt_vaddr[i] = scratch_pte;
@@ -967,12 +837,14 @@
 		first_pte = 0;
 		act_pt++;
 	}
+
+	return 0;
 }
 
-static void gen6_ppgtt_insert_entries(struct i915_address_space *vm,
-				      struct sg_table *pages,
-				      uint64_t start,
-				      enum i915_cache_level cache_level, u32 flags)
+static int gen6_ppgtt_insert_entries(struct i915_address_space *vm,
+				     struct sg_table *pages, uint64_t start,
+				     enum i915_cache_level cache_level,
+				     u32 flags)
 {
 	struct i915_hw_ppgtt *ppgtt =
 		container_of(vm, struct i915_hw_ppgtt, base);
@@ -981,11 +853,16 @@
 	unsigned act_pt = first_entry / I915_PPGTT_PT_ENTRIES;
 	unsigned act_pte = first_entry % I915_PPGTT_PT_ENTRIES;
 	struct sg_page_iter sg_iter;
+	int ret;
+
+	ret = i915_gem_object_get_pages(ppgtt->state);
+	if (ret)
+		return ret;
 
 	pt_vaddr = NULL;
 	for_each_sg_page(pages->sgl, &sg_iter, pages->nents, 0) {
 		if (pt_vaddr == NULL)
-			pt_vaddr = kmap_atomic(ppgtt->pt_pages[act_pt]);
+			pt_vaddr = kmap_pt(ppgtt, act_pt);
 
 		pt_vaddr[act_pte] =
 			vm->pte_encode(sg_page_iter_dma_address(&sg_iter),
@@ -1000,28 +877,8 @@
 	}
 	if (pt_vaddr)
 		kunmap_atomic(pt_vaddr);
-}
-
-static void gen6_ppgtt_unmap_pages(struct i915_hw_ppgtt *ppgtt)
-{
-	int i;
-
-	if (ppgtt->pt_dma_addr) {
-		for (i = 0; i < ppgtt->num_pd_entries; i++)
-			pci_unmap_page(ppgtt->base.dev->pdev,
-				       ppgtt->pt_dma_addr[i],
-				       4096, PCI_DMA_BIDIRECTIONAL);
-	}
-}
 
-static void gen6_ppgtt_free(struct i915_hw_ppgtt *ppgtt)
-{
-	int i;
-
-	kfree(ppgtt->pt_dma_addr);
-	for (i = 0; i < ppgtt->num_pd_entries; i++)
-		__free_page(ppgtt->pt_pages[i]);
-	kfree(ppgtt->pt_pages);
+	return 0;
 }
 
 static void gen6_ppgtt_cleanup(struct i915_address_space *vm)
@@ -1029,117 +886,23 @@
 	struct i915_hw_ppgtt *ppgtt =
 		container_of(vm, struct i915_hw_ppgtt, base);
 
-	list_del(&vm->global_link);
-	drm_mm_takedown(&ppgtt->base.mm);
-	drm_mm_remove_node(&ppgtt->node);
-
-	gen6_ppgtt_unmap_pages(ppgtt);
-	gen6_ppgtt_free(ppgtt);
+	drm_gem_object_unreference(&ppgtt->state->base);
 }
 
-static int gen6_ppgtt_allocate_page_directories(struct i915_hw_ppgtt *ppgtt)
+static int gen6_ppgtt_alloc(struct i915_hw_ppgtt *ppgtt)
 {
-	struct drm_device *dev = ppgtt->base.dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	bool retried = false;
-	int ret;
-
 	/* PPGTT PDEs reside in the GGTT and consists of 512 entries. The
 	 * allocator works in address space sizes, so it's multiplied by page
 	 * size. We allocate at the top of the GTT to avoid fragmentation.
 	 */
-	BUG_ON(!drm_mm_initialized(&dev_priv->gtt.base.mm));
-alloc:
-	ret = drm_mm_insert_node_in_range_generic(&dev_priv->gtt.base.mm,
-						  &ppgtt->node, GEN6_PD_SIZE,
-						  GEN6_PD_ALIGN, 0,
-						  0, dev_priv->gtt.base.total,
-						  DRM_MM_TOPDOWN);
-	if (ret == -ENOSPC && !retried) {
-		ret = i915_gem_evict_something(dev, &dev_priv->gtt.base,
-					       GEN6_PD_SIZE, GEN6_PD_ALIGN,
-					       I915_CACHE_NONE,
-					       0, dev_priv->gtt.base.total,
-					       0);
-		if (ret)
-			return ret;
-
-		retried = true;
-		goto alloc;
-	}
-
-	if (ppgtt->node.start < dev_priv->gtt.mappable_end)
-		DRM_DEBUG("Forced to use aperture for PDEs\n");
-
-	ppgtt->num_pd_entries = GEN6_PPGTT_PD_ENTRIES;
-	return ret;
-}
-
-static int gen6_ppgtt_allocate_page_tables(struct i915_hw_ppgtt *ppgtt)
-{
-	int i;
-
-	ppgtt->pt_pages = kcalloc(ppgtt->num_pd_entries, sizeof(struct page *),
-				  GFP_KERNEL);
-
-	if (!ppgtt->pt_pages)
+	ppgtt->state = i915_gem_alloc_object(ppgtt->base.dev, GEN6_PD_SIZE);
+	if (ppgtt->state == NULL)
 		return -ENOMEM;
 
-	for (i = 0; i < ppgtt->num_pd_entries; i++) {
-		ppgtt->pt_pages[i] = alloc_page(GFP_KERNEL);
-		if (!ppgtt->pt_pages[i]) {
-			gen6_ppgtt_free(ppgtt);
-			return -ENOMEM;
-		}
-	}
-
-	return 0;
-}
-
-static int gen6_ppgtt_alloc(struct i915_hw_ppgtt *ppgtt)
-{
-	int ret;
-
-	ret = gen6_ppgtt_allocate_page_directories(ppgtt);
-	if (ret)
-		return ret;
-
-	ret = gen6_ppgtt_allocate_page_tables(ppgtt);
-	if (ret) {
-		drm_mm_remove_node(&ppgtt->node);
-		return ret;
-	}
-
-	ppgtt->pt_dma_addr = kcalloc(ppgtt->num_pd_entries, sizeof(dma_addr_t),
-				     GFP_KERNEL);
-	if (!ppgtt->pt_dma_addr) {
-		drm_mm_remove_node(&ppgtt->node);
-		gen6_ppgtt_free(ppgtt);
-		return -ENOMEM;
-	}
-
-	return 0;
-}
-
-static int gen6_ppgtt_setup_page_tables(struct i915_hw_ppgtt *ppgtt)
-{
-	struct drm_device *dev = ppgtt->base.dev;
-	int i;
-
-	for (i = 0; i < ppgtt->num_pd_entries; i++) {
-		dma_addr_t pt_addr;
-
-		pt_addr = pci_map_page(dev->pdev, ppgtt->pt_pages[i], 0, 4096,
-				       PCI_DMA_BIDIRECTIONAL);
-
-		if (pci_dma_mapping_error(dev->pdev, pt_addr)) {
-			gen6_ppgtt_unmap_pages(ppgtt);
-			return -EIO;
-		}
-
-		ppgtt->pt_dma_addr[i] = pt_addr;
-	}
+	ppgtt->state->pde = true;
 
+	ppgtt->num_pd_entries = GEN6_PPGTT_PD_ENTRIES;
+	ppgtt->alignment = GEN6_PD_ALIGN;
 	return 0;
 }
 
@@ -1151,13 +914,8 @@
 
 	ppgtt->base.pte_encode = dev_priv->gtt.base.pte_encode;
 	if (IS_GEN6(dev)) {
-		ppgtt->enable = gen6_ppgtt_enable;
 		ppgtt->switch_mm = gen6_mm_switch;
-	} else if (IS_HASWELL(dev)) {
-		ppgtt->enable = gen7_ppgtt_enable;
-		ppgtt->switch_mm = hsw_mm_switch;
 	} else if (IS_GEN7(dev)) {
-		ppgtt->enable = gen7_ppgtt_enable;
 		ppgtt->switch_mm = gen7_mm_switch;
 	} else
 		BUG();
@@ -1166,12 +924,6 @@
 	if (ret)
 		return ret;
 
-	ret = gen6_ppgtt_setup_page_tables(ppgtt);
-	if (ret) {
-		gen6_ppgtt_free(ppgtt);
-		return ret;
-	}
-
 	ppgtt->base.clear_range = gen6_ppgtt_clear_range;
 	ppgtt->base.insert_entries = gen6_ppgtt_insert_entries;
 	ppgtt->base.cleanup = gen6_ppgtt_cleanup;
@@ -1179,68 +931,197 @@
 	ppgtt->base.total =  ppgtt->num_pd_entries * I915_PPGTT_PT_ENTRIES * PAGE_SIZE;
 	ppgtt->debug_dump = gen6_dump_ppgtt;
 
-	ppgtt->pd_offset =
-		ppgtt->node.start / PAGE_SIZE * sizeof(gen6_gtt_pte_t);
+	return ppgtt->base.clear_range(&ppgtt->base,
+				       0, ppgtt->base.total,
+				       true);
+}
 
-	ppgtt->base.clear_range(&ppgtt->base, 0, ppgtt->base.total, true);
+static void i915_ggtt_flush(struct drm_i915_private *dev_priv)
+{
+	if (INTEL_INFO(dev_priv->dev)->gen < 6) {
+		intel_gtt_chipset_flush();
+	} else {
+		I915_WRITE(GFX_FLSH_CNTL_GEN6, GFX_FLSH_CNTL_EN);
+		POSTING_READ(GFX_FLSH_CNTL_GEN6);
+	}
+}
 
-	DRM_DEBUG_DRIVER("Allocated pde space (%ldM) at GTT entry: %lx\n",
-			 ppgtt->node.size >> 20,
-			 ppgtt->node.start / PAGE_SIZE);
+static int pde_bind_vma(struct i915_vma *vma,
+			enum i915_cache_level cache_level,
+			u32 flags)
+{
+	struct sg_table *pages = vma->obj->pages;
+	struct drm_i915_private *i915 = to_i915(vma->vm->dev);
+	gen6_gtt_pte_t __iomem *pd_addr;
+	struct sg_page_iter sg_iter;
+
+	if (flags != GLOBAL_BIND)
+		return -EINVAL;
+
+	if (vma->node.start & 0x3f)
+		return -EINVAL;
+
+	if (vma->bound & GLOBAL_BIND)
+		return 0;
+
+	pd_addr = (gen6_gtt_pte_t __iomem*)i915->gtt.gsm;
+	pd_addr += vma->node.start >> PAGE_SHIFT;
+	for_each_sg_page(pages->sgl, &sg_iter, pages->nents, 0) {
+		dma_addr_t addr = (sg_dma_address(sg_iter.sg) +
+				   (sg_iter.sg_pgoffset << PAGE_SHIFT));
+		writel(GEN6_PDE_ADDR_ENCODE(addr) | GEN6_PDE_VALID, pd_addr++);
+	}
+	i915_ggtt_flush(i915);
+
+	/* Skips a second call when pinning */
+	vma->bound = GLOBAL_BIND;
+	return 0;
+}
 
+static int pde_unbind_vma(struct i915_vma *vma)
+{
+	vma->bound = 0;
 	return 0;
 }
 
-int i915_gem_init_ppgtt(struct drm_device *dev, struct i915_hw_ppgtt *ppgtt)
+static int __hw_ppgtt_init(struct drm_device *dev, struct i915_hw_ppgtt *ppgtt)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	int ret = 0;
 
 	ppgtt->base.dev = dev;
 	ppgtt->base.scratch = dev_priv->gtt.base.scratch;
+	INIT_LIST_HEAD(&ppgtt->base.vma_list);
 
 	if (INTEL_INFO(dev)->gen < 8)
-		ret = gen6_ppgtt_init(ppgtt);
-	else if (IS_GEN8(dev))
-		ret = gen8_ppgtt_init(ppgtt, dev_priv->gtt.base.total);
+		return gen6_ppgtt_init(ppgtt);
+	else if (IS_GEN8(dev) || IS_GEN9(dev))
+		return gen8_ppgtt_init(ppgtt, dev_priv->gtt.base.total);
 	else
 		BUG();
+}
+int i915_ppgtt_init(struct drm_device *dev, struct i915_hw_ppgtt *ppgtt)
+{
+	int ret;
 
-	if (!ret) {
-		struct drm_i915_private *dev_priv = dev->dev_private;
-		kref_init(&ppgtt->ref);
-		drm_mm_init(&ppgtt->base.mm, ppgtt->base.start,
-			    ppgtt->base.total);
-		i915_init_vm(dev_priv, &ppgtt->base);
-		if (INTEL_INFO(dev)->gen < 8) {
-			gen6_write_pdes(ppgtt);
-			DRM_DEBUG("Adding PPGTT at offset %x\n",
-				  ppgtt->pd_offset << 10);
-		}
+	ret = __hw_ppgtt_init(dev, ppgtt);
+	if (ret)
+		return ret;
+
+	drm_mm_init(&ppgtt->base.mm, ppgtt->base.start, ppgtt->base.total);
+	i915_init_vm(to_i915(dev), &ppgtt->base);
+
+	return 0;
+}
+
+int i915_ppgtt_init_hw(struct drm_device *dev)
+{
+	int ret;
+
+	if (!USES_PPGTT(dev))
+		return 0;
+
+	/* In the case of execlists, PPGTT is enabled by the context descriptor
+	 * and the PDPs are contained within the context itself.  We don't
+	 * need to do anything here. */
+	if (RCS_ENGINE(dev)->execlists_enabled)
+		return 0;
+
+	if (IS_GEN6(dev))
+		ret = gen6_ppgtt_enable(dev);
+	else if (IS_GEN7(dev))
+		ret = gen7_ppgtt_enable(dev);
+	else if (INTEL_INFO(dev)->gen >= 8)
+		ret = gen8_ppgtt_enable(dev);
+	else {
+		WARN_ON(1);
+		ret = -ENODEV;
 	}
 
 	return ret;
 }
 
-static void
+struct i915_hw_ppgtt *
+i915_ppgtt_create(struct drm_device *dev)
+{
+	struct i915_hw_ppgtt *ppgtt;
+	int ret;
+
+	ppgtt = kzalloc(sizeof(*ppgtt), GFP_KERNEL);
+	if (!ppgtt)
+		return ERR_PTR(-ENOMEM);
+
+	ret = i915_ppgtt_init(dev, ppgtt);
+	if (ret) {
+		kfree(ppgtt);
+		return ERR_PTR(ret);
+	}
+
+	trace_i915_vm_create(&ppgtt->base);
+
+	return ppgtt;
+}
+
+void __i915_vm_free(struct kref *kref)
+{
+	struct i915_address_space *vm =
+		container_of(kref, struct i915_address_space, ref);
+
+	DRM_DEBUG_DRIVER("VM %p freed\n", vm);
+	trace_i915_vm_free(vm);
+
+	/* vmas should already be unbound */
+	WARN_ON(!list_empty(&vm->vma_list));
+	WARN_ON(!list_empty(&vm->active_list));
+	WARN_ON(!list_empty(&vm->inactive_list));
+
+	list_del(&vm->global_link);
+	drm_mm_takedown(&vm->mm);
+
+	vm->cleanup(vm);
+	kfree(vm);
+}
+
+static int
 ppgtt_bind_vma(struct i915_vma *vma,
 	       enum i915_cache_level cache_level,
 	       u32 flags)
 {
+	int ret;
+
+	if (WARN_ON(flags & GLOBAL_BIND))
+		return -EINVAL;
+
 	/* Currently applicable only to VLV */
 	if (vma->obj->gt_ro)
 		flags |= PTE_READ_ONLY;
 
-	vma->vm->insert_entries(vma->vm, vma->obj->pages, vma->node.start,
-				cache_level, flags);
+	if (flags == vma->bound)
+		return 0;
+
+	if (flags == REBIND) {
+		flags &= ~REBIND;
+		flags |= LOCAL_BIND;
+	}
+
+	ret = vma->vm->insert_entries(vma->vm,
+				      vma->obj->pages, vma->node.start,
+				      cache_level, flags);
+	if (ret)
+		return ret;
+
+	vma->vm->dirty = true;
+	vma->bound = flags;
+	return 0;
 }
 
-static void ppgtt_unbind_vma(struct i915_vma *vma)
+static int ppgtt_unbind_vma(struct i915_vma *vma)
 {
 	vma->vm->clear_range(vma->vm,
 			     vma->node.start,
 			     vma->obj->base.size,
 			     true);
+	vma->bound = 0;
+	return 0;
 }
 
 extern int intel_iommu_gfx_mapped;
@@ -1284,18 +1165,18 @@
 void i915_check_and_clear_faults(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
+	struct intel_engine_cs *engine;
 	int i;
 
 	if (INTEL_INFO(dev)->gen < 6)
 		return;
 
-	for_each_ring(ring, dev_priv, i) {
+	for_each_engine(engine, dev_priv, i) {
 		u32 fault_reg;
-		fault_reg = I915_READ(RING_FAULT_REG(ring));
+		fault_reg = I915_READ(RING_FAULT_REG(engine));
 		if (fault_reg & RING_FAULT_VALID) {
 			DRM_DEBUG_DRIVER("Unexpected fault\n"
-					 "\tAddr: 0x%08lx\\n"
+					 "\tAddr: 0x%08lx\n"
 					 "\tAddress space: %s\n"
 					 "\tSource ID: %d\n"
 					 "\tType: %d\n",
@@ -1303,21 +1184,11 @@
 					 fault_reg & RING_FAULT_GTTSEL_MASK ? "GGTT" : "PPGTT",
 					 RING_FAULT_SRCID(fault_reg),
 					 RING_FAULT_FAULT_TYPE(fault_reg));
-			I915_WRITE(RING_FAULT_REG(ring),
+			I915_WRITE(RING_FAULT_REG(engine),
 				   fault_reg & ~RING_FAULT_VALID);
 		}
 	}
-	POSTING_READ(RING_FAULT_REG(&dev_priv->ring[RCS]));
-}
-
-static void i915_ggtt_flush(struct drm_i915_private *dev_priv)
-{
-	if (INTEL_INFO(dev_priv->dev)->gen < 6) {
-		intel_gtt_chipset_flush();
-	} else {
-		I915_WRITE(GFX_FLSH_CNTL_GEN6, GFX_FLSH_CNTL_EN);
-		POSTING_READ(GFX_FLSH_CNTL_GEN6);
-	}
+	POSTING_READ(RING_FAULT_REG(RCS_ENGINE(dev_priv)));
 }
 
 void i915_gem_suspend_gtt_mappings(struct drm_device *dev)
@@ -1344,7 +1215,6 @@
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_i915_gem_object *obj;
-	struct i915_address_space *vm;
 
 	i915_check_and_clear_faults(dev);
 
@@ -1361,12 +1231,7 @@
 			continue;
 
 		i915_gem_clflush_object(obj, obj->pin_display);
-		/* The bind_vma code tries to be smart about tracking mappings.
-		 * Unfortunately above, we've just wiped out the mappings
-		 * without telling our object about it. So we need to fake it.
-		 */
-		obj->has_global_gtt_mapping = 0;
-		vma->bind_vma(vma, obj->cache_level, GLOBAL_BIND);
+		vma->bind_vma(vma, obj->cache_level, REBIND);
 	}
 
 
@@ -1379,17 +1244,6 @@
 		return;
 	}
 
-	list_for_each_entry(vm, &dev_priv->vm_list, global_link) {
-		/* TODO: Perhaps it shouldn't be gen6 specific */
-		if (i915_is_ggtt(vm)) {
-			if (dev_priv->mm.aliasing_ppgtt)
-				gen6_write_pdes(dev_priv->mm.aliasing_ppgtt);
-			continue;
-		}
-
-		gen6_write_pdes(container_of(vm, struct i915_hw_ppgtt, base));
-	}
-
 	i915_ggtt_flush(dev_priv);
 }
 
@@ -1408,18 +1262,18 @@
 
 static inline void gen8_set_pte(void __iomem *addr, gen8_gtt_pte_t pte)
 {
-#ifdef writeq
+#if defined(writeq)
 	writeq(pte, addr);
 #else
-	iowrite32((u32)pte, addr);
-	iowrite32(pte >> 32, addr + 4);
+	iowrite32(lower_32_bits(pte), addr);
+	iowrite32(upper_32_bits(pte), addr + 4);
 #endif
 }
 
-static void gen8_ggtt_insert_entries(struct i915_address_space *vm,
-				     struct sg_table *st,
-				     uint64_t start,
-				     enum i915_cache_level level, u32 unused)
+static int gen8_ggtt_insert_entries(struct i915_address_space *vm,
+				    struct sg_table *st,
+				    uint64_t start,
+				    enum i915_cache_level level, u32 unused)
 {
 	struct drm_i915_private *dev_priv = vm->dev->dev_private;
 	unsigned first_entry = start >> PAGE_SHIFT;
@@ -1454,6 +1308,8 @@
 	 */
 	I915_WRITE(GFX_FLSH_CNTL_GEN6, GFX_FLSH_CNTL_EN);
 	POSTING_READ(GFX_FLSH_CNTL_GEN6);
+
+	return 0;
 }
 
 /*
@@ -1462,10 +1318,10 @@
  * within the global GTT as well as accessible by the GPU through the GMADR
  * mapped BAR (dev_priv->mm.gtt->gtt).
  */
-static void gen6_ggtt_insert_entries(struct i915_address_space *vm,
-				     struct sg_table *st,
-				     uint64_t start,
-				     enum i915_cache_level level, u32 flags)
+static int gen6_ggtt_insert_entries(struct i915_address_space *vm,
+				    struct sg_table *st,
+				    uint64_t start,
+				    enum i915_cache_level level, u32 flags)
 {
 	struct drm_i915_private *dev_priv = vm->dev->dev_private;
 	unsigned first_entry = start >> PAGE_SHIFT;
@@ -1498,12 +1354,14 @@
 	 */
 	I915_WRITE(GFX_FLSH_CNTL_GEN6, GFX_FLSH_CNTL_EN);
 	POSTING_READ(GFX_FLSH_CNTL_GEN6);
+
+	return 0;
 }
 
-static void gen8_ggtt_clear_range(struct i915_address_space *vm,
-				  uint64_t start,
-				  uint64_t length,
-				  bool use_scratch)
+static int gen8_ggtt_clear_range(struct i915_address_space *vm,
+				 uint64_t start,
+				 uint64_t length,
+				 bool use_scratch)
 {
 	struct drm_i915_private *dev_priv = vm->dev->dev_private;
 	unsigned first_entry = start >> PAGE_SHIFT;
@@ -1524,12 +1382,14 @@
 	for (i = 0; i < num_entries; i++)
 		gen8_set_pte(&gtt_base[i], scratch_pte);
 	readl(gtt_base);
+
+	return 0;
 }
 
-static void gen6_ggtt_clear_range(struct i915_address_space *vm,
-				  uint64_t start,
-				  uint64_t length,
-				  bool use_scratch)
+static int gen6_ggtt_clear_range(struct i915_address_space *vm,
+				 uint64_t start,
+				 uint64_t length,
+				 bool use_scratch)
 {
 	struct drm_i915_private *dev_priv = vm->dev->dev_private;
 	unsigned first_entry = start >> PAGE_SHIFT;
@@ -1549,49 +1409,117 @@
 	for (i = 0; i < num_entries; i++)
 		iowrite32(scratch_pte, &gtt_base[i]);
 	readl(gtt_base);
+
+	return 0;
 }
 
+static void mark_global_obj(struct i915_vma *vma)
+{
+	struct drm_i915_gem_object *obj = vma->obj;
+	bool mappable, fenceable;
+	u32 fence_size, fence_alignment;
+
+	fence_size = i915_gem_get_gtt_size(obj->base.dev,
+					   obj->base.size,
+					   obj->tiling_mode);
+	fence_alignment = i915_gem_get_gtt_alignment(obj->base.dev,
+						     obj->base.size,
+						     obj->tiling_mode,
+						     true);
+
+	fenceable = (vma->node.size == fence_size &&
+		     (vma->node.start & (fence_alignment - 1)) == 0);
 
-static void i915_ggtt_bind_vma(struct i915_vma *vma,
-			       enum i915_cache_level cache_level,
-			       u32 unused)
+	mappable = (vma->node.start + obj->base.size <=
+		    to_i915(obj->base.dev)->gtt.mappable_end);
+
+	obj->map_and_fenceable = mappable && fenceable;
+}
+
+static int i915_ggtt_bind_vma(struct i915_vma *vma,
+			      enum i915_cache_level cache_level,
+			      u32 unused)
 {
 	const unsigned long entry = vma->node.start >> PAGE_SHIFT;
 	unsigned int flags = (cache_level == I915_CACHE_NONE) ?
 		AGP_USER_MEMORY : AGP_USER_CACHED_MEMORY;
 
+	if (vma->bound && (flags & REBIND) == 0)
+		return 0;
+
 	BUG_ON(!i915_is_ggtt(vma->vm));
 	intel_gtt_insert_sg_entries(vma->obj->pages, entry, flags);
-	vma->obj->has_global_gtt_mapping = 1;
+	vma->bound = GLOBAL_BIND;
+	vma->vm->dirty = true;
+
+	mark_global_obj(vma);
+
+	return 0;
 }
 
-static void i915_ggtt_clear_range(struct i915_address_space *vm,
-				  uint64_t start,
-				  uint64_t length,
-				  bool unused)
+static int i915_ggtt_clear_range(struct i915_address_space *vm,
+				 uint64_t start,
+				 uint64_t length,
+				 bool unused)
 {
 	unsigned first_entry = start >> PAGE_SHIFT;
 	unsigned num_entries = length >> PAGE_SHIFT;
+
 	intel_gtt_clear_range(first_entry, num_entries);
+	return 0;
+}
+
+static int nop_clear_range(struct i915_address_space *vm,
+			   uint64_t start,
+			   uint64_t length,
+			   bool use_scratch)
+{
+	return 0;
 }
 
-static void i915_ggtt_unbind_vma(struct i915_vma *vma)
+static int i915_ggtt_unbind_vma(struct i915_vma *vma)
 {
+	struct drm_i915_gem_object *obj = vma->obj;
 	const unsigned int first = vma->node.start >> PAGE_SHIFT;
-	const unsigned int size = vma->obj->base.size >> PAGE_SHIFT;
+	const unsigned int size = obj->base.size >> PAGE_SHIFT;
+	int ret;
 
 	BUG_ON(!i915_is_ggtt(vma->vm));
-	vma->obj->has_global_gtt_mapping = 0;
+	BUG_ON(vma->bound == 0);
+
+	i915_gem_object_finish_gtt(obj);
+
+	/* release the fence reg _after_ flushing */
+	ret = i915_gem_object_put_fence(obj);
+	if (WARN_ON(ret)) /* should be idle already */
+		return ret;
+
+	obj->map_and_fenceable = false;
+
 	intel_gtt_clear_range(first, size);
+	vma->bound = 0;
+
+	return 0;
 }
 
-static void ggtt_bind_vma(struct i915_vma *vma,
-			  enum i915_cache_level cache_level,
-			  u32 flags)
+static int ggtt_bind_vma(struct i915_vma *vma,
+			 enum i915_cache_level cache_level,
+			 u32 flags)
 {
 	struct drm_device *dev = vma->vm->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_i915_gem_object *obj = vma->obj;
+	int ret;
+
+	if (flags == REBIND) {
+		flags = vma->bound;
+		vma->bound = 0;
+	}
+
+	if (!dev_priv->mm.aliasing_ppgtt) {
+		flags |= GLOBAL_BIND;
+		flags &= ~LOCAL_BIND;
+	}
 
 	/* Currently applicable only to VLV */
 	if (obj->gt_ro)
@@ -1608,50 +1536,69 @@
 	 * "gtt mappable", SNB errata, or if specified via special execbuf
 	 * flags. At all other times, the GPU will use the aliasing PPGTT.
 	 */
-	if (!dev_priv->mm.aliasing_ppgtt || flags & GLOBAL_BIND) {
-		if (!obj->has_global_gtt_mapping ||
-		    (cache_level != obj->cache_level)) {
-			vma->vm->insert_entries(vma->vm, obj->pages,
-						vma->node.start,
-						cache_level, flags);
-			obj->has_global_gtt_mapping = 1;
-		}
+	if ((flags & ~vma->bound) & GLOBAL_BIND) {
+		ret = vma->vm->insert_entries(vma->vm, obj->pages,
+					      vma->node.start,
+					      cache_level, flags);
+		if (ret)
+			return ret;
+
+		vma->bound |= GLOBAL_BIND;
+		vma->vm->dirty = true;
+		mark_global_obj(vma);
 	}
 
-	if (dev_priv->mm.aliasing_ppgtt &&
-	    (!obj->has_aliasing_ppgtt_mapping ||
-	     (cache_level != obj->cache_level))) {
+	if ((flags & ~vma->bound) & LOCAL_BIND) {
 		struct i915_hw_ppgtt *appgtt = dev_priv->mm.aliasing_ppgtt;
-		appgtt->base.insert_entries(&appgtt->base,
-					    vma->obj->pages,
-					    vma->node.start,
-					    cache_level, flags);
-		vma->obj->has_aliasing_ppgtt_mapping = 1;
+
+		ret = appgtt->base.insert_entries(&appgtt->base,
+						  obj->pages,
+						  vma->node.start,
+						  cache_level, flags);
+		if (ret)
+			return ret;
+
+		vma->bound |= LOCAL_BIND;
+		vma->vm->dirty = true;
 	}
+
+	vma->bound |= flags;
+	return 0;
 }
 
-static void ggtt_unbind_vma(struct i915_vma *vma)
+static int ggtt_unbind_vma(struct i915_vma *vma)
 {
 	struct drm_device *dev = vma->vm->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_i915_gem_object *obj = vma->obj;
+	int ret;
+
+	if (vma->bound & GLOBAL_BIND) {
+		i915_gem_object_finish_gtt(obj);
+
+		/* release the fence reg _after_ flushing */
+		ret = i915_gem_object_put_fence(obj);
+		if (WARN_ON(ret)) /* should be idle already */
+			return ret;
+
+		obj->map_and_fenceable = false;
 
-	if (obj->has_global_gtt_mapping) {
 		vma->vm->clear_range(vma->vm,
 				     vma->node.start,
 				     obj->base.size,
 				     true);
-		obj->has_global_gtt_mapping = 0;
 	}
 
-	if (obj->has_aliasing_ppgtt_mapping) {
+	if (vma->bound & LOCAL_BIND) {
 		struct i915_hw_ppgtt *appgtt = dev_priv->mm.aliasing_ppgtt;
 		appgtt->base.clear_range(&appgtt->base,
 					 vma->node.start,
 					 obj->base.size,
 					 true);
-		obj->has_aliasing_ppgtt_mapping = 0;
 	}
+
+	vma->bound = 0;
+	return 0;
 }
 
 void i915_gem_gtt_finish_object(struct drm_i915_gem_object *obj)
@@ -1687,10 +1634,10 @@
 	}
 }
 
-void i915_gem_setup_global_gtt(struct drm_device *dev,
-			       unsigned long start,
-			       unsigned long mappable_end,
-			       unsigned long end)
+static int i915_gem_setup_global_gtt(struct drm_device *dev,
+				     unsigned long start,
+				     unsigned long mappable_end,
+				     unsigned long end)
 {
 	/* Let GEM Manage all of the aperture.
 	 *
@@ -1702,45 +1649,71 @@
 	 * of the aperture.
 	 */
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct i915_address_space *ggtt_vm = &dev_priv->gtt.base;
+	struct i915_address_space *ggtt = &dev_priv->gtt.base;
 	struct drm_mm_node *entry;
 	struct drm_i915_gem_object *obj;
 	unsigned long hole_start, hole_end;
+	int ret;
 
 	BUG_ON(mappable_end > end);
 
 	/* Subtract the guard page ... */
-	drm_mm_init(&ggtt_vm->mm, start, end - start - PAGE_SIZE);
+	drm_mm_init(&ggtt->mm, start, end - start - PAGE_SIZE);
 	if (!HAS_LLC(dev))
 		dev_priv->gtt.base.mm.color_adjust = i915_gtt_color_adjust;
 
 	/* Mark any preallocated objects as occupied */
 	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list) {
-		struct i915_vma *vma = i915_gem_obj_to_vma(obj, ggtt_vm);
-		int ret;
+		struct i915_vma *vma = i915_gem_obj_to_ggtt(obj);
+
+		WARN_ON(drm_mm_node_allocated(&vma->node));
+
 		DRM_DEBUG_KMS("reserving preallocated space: %lx + %zx\n",
-			      i915_gem_obj_ggtt_offset(obj), obj->base.size);
+			      vma->node.start, obj->base.size);
+		ret = drm_mm_reserve_node(&ggtt->mm, &vma->node);
+		if (ret) {
+			DRM_DEBUG_KMS("Reservation failed: %i\n", ret);
+			return ret;
+		}
+		vma->bound |= GLOBAL_BIND;
+		mark_global_obj(vma);
+	}
 
-		WARN_ON(i915_gem_obj_ggtt_bound(obj));
-		ret = drm_mm_reserve_node(&ggtt_vm->mm, &vma->node);
-		if (ret)
-			DRM_DEBUG_KMS("Reservation failed\n");
-		obj->has_global_gtt_mapping = 1;
+	ggtt->start = start;
+	ggtt->total = end - start;
+
+	if (USES_PPGTT(dev) && !USES_FULL_PPGTT(dev)) {
+		struct i915_hw_ppgtt *ppgtt;
+
+		ppgtt = kzalloc(sizeof(*ppgtt), GFP_KERNEL);
+		if (!ppgtt)
+			return -ENOMEM;
+
+		ret = __hw_ppgtt_init(dev, ppgtt);
+		if (ret== 0 && ppgtt->state)
+			ret = i915_gem_object_ggtt_pin(ppgtt->state,
+						       ppgtt->alignment,
+						       0);
+		if (ret == 0)
+			dev_priv->mm.aliasing_ppgtt = ppgtt;
 	}
 
-	dev_priv->gtt.base.start = start;
-	dev_priv->gtt.base.total = end - start;
+	if (USES_FULL_PPGTT(dev) || dev_priv->mm.aliasing_ppgtt)
+		ggtt->clear_range = nop_clear_range;
 
 	/* Clear any non-preallocated blocks */
-	drm_mm_for_each_hole(entry, &ggtt_vm->mm, hole_start, hole_end) {
+	drm_mm_for_each_hole(entry, &ggtt->mm, hole_start, hole_end) {
 		DRM_DEBUG_KMS("clearing unused GTT space: [%lx, %lx]\n",
 			      hole_start, hole_end);
-		ggtt_vm->clear_range(ggtt_vm, hole_start,
-				     hole_end - hole_start, true);
+		ggtt->clear_range(ggtt,
+				  hole_start, hole_end - hole_start,
+				  true);
 	}
 
 	/* And finally clear the reserved guard page */
-	ggtt_vm->clear_range(ggtt_vm, end - PAGE_SIZE, PAGE_SIZE, true);
+	ggtt->clear_range(ggtt, end - PAGE_SIZE, PAGE_SIZE, true);
+
+	return 0;
 }
 
 void i915_gem_init_global_gtt(struct drm_device *dev)
@@ -1754,6 +1727,27 @@
 	i915_gem_setup_global_gtt(dev, 0, mappable_size, gtt_size);
 }
 
+void i915_global_gtt_cleanup(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct i915_address_space *vm = &dev_priv->gtt.base;
+
+	lockdep_assert_held(&dev->struct_mutex);
+
+	if (dev_priv->mm.aliasing_ppgtt) {
+		struct i915_hw_ppgtt *ppgtt = dev_priv->mm.aliasing_ppgtt;
+
+		ppgtt->base.cleanup(&ppgtt->base);
+	}
+
+	if (drm_mm_initialized(&vm->mm)) {
+		drm_mm_takedown(&vm->mm);
+		list_del(&vm->global_link);
+	}
+
+	vm->cleanup(vm);
+}
+
 static int setup_scratch_page(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -1763,7 +1757,6 @@
 	page = alloc_page(GFP_KERNEL | GFP_DMA32 | __GFP_ZERO);
 	if (page == NULL)
 		return -ENOMEM;
-	get_page(page);
 	set_pages_uc(page, 1);
 
 #ifdef CONFIG_INTEL_IOMMU
@@ -1788,7 +1781,6 @@
 	set_pages_wb(page, 1);
 	pci_unmap_page(dev->pdev, dev_priv->gtt.base.scratch.addr,
 		       PAGE_SIZE, PCI_DMA_BIDIRECTIONAL);
-	put_page(page);
 	__free_page(page);
 }
 
@@ -1858,6 +1850,18 @@
 		return (gmch_ctrl - 0x17 + 9) << 22;
 }
 
+static size_t gen9_get_stolen_size(u16 gen9_gmch_ctl)
+{
+	gen9_gmch_ctl >>= BDW_GMCH_GMS_SHIFT;
+	gen9_gmch_ctl &= BDW_GMCH_GMS_MASK;
+
+	if (gen9_gmch_ctl < 0xf0)
+		return gen9_gmch_ctl << 25; /* 32 MB units */
+	else
+		/* 4MB increments starting at 0xf0 for 4MB */
+		return (gen9_gmch_ctl - 0xf0 + 1) << 22;
+}
+
 static int ggtt_probe_common(struct drm_device *dev,
 			     size_t gtt_size)
 {
@@ -1933,9 +1937,17 @@
 	 * Only the snoop bit has meaning for CHV, the rest is
 	 * ignored.
 	 *
-	 * Note that the harware enforces snooping for all page
-	 * table accesses. The snoop bit is actually ignored for
-	 * PDEs.
+	 * The hardware will never snoop for certain types of accesses:
+	 * - CPU GTT (GMADR->GGTT->no snoop->memory)
+	 * - PPGTT page tables
+	 * - some other special cycles
+	 *
+	 * As with BDW, we also need to consider the following for GT accesses:
+	 * "For GGTT, there is NO pat_sel[2:0] from the entry,
+	 * so RTL will always use the value corresponding to
+	 * pat_sel = 000".
+	 * Which means we must set the snoop bit in PAT entry 0
+	 * in order to keep the global status page working.
 	 */
 	pat = GEN8_PPAT(0, CHV_PPAT_SNOOP) |
 	      GEN8_PPAT(1, 0) |
@@ -1970,7 +1982,10 @@
 
 	pci_read_config_word(dev->pdev, SNB_GMCH_CTRL, &snb_gmch_ctl);
 
-	if (IS_CHERRYVIEW(dev)) {
+	if (INTEL_INFO(dev)->gen >= 9) {
+		*stolen = gen9_get_stolen_size(snb_gmch_ctl);
+		gtt_size = gen8_get_total_gtt_size(snb_gmch_ctl);
+	} else if (IS_CHERRYVIEW(dev)) {
 		*stolen = chv_get_stolen_size(snb_gmch_ctl);
 		gtt_size = chv_get_total_gtt_size(snb_gmch_ctl);
 	} else {
@@ -2038,10 +2053,6 @@
 
 	struct i915_gtt *gtt = container_of(vm, struct i915_gtt, base);
 
-	if (drm_mm_initialized(&vm->mm)) {
-		drm_mm_takedown(&vm->mm);
-		list_del(&vm->global_link);
-	}
 	iounmap(gtt->gsm);
 	teardown_scratch_page(vm->dev);
 }
@@ -2074,10 +2085,6 @@
 
 static void i915_gmch_remove(struct i915_address_space *vm)
 {
-	if (drm_mm_initialized(&vm->mm)) {
-		drm_mm_takedown(&vm->mm);
-		list_del(&vm->global_link);
-	}
 	intel_gmch_remove();
 }
 
@@ -2087,6 +2094,9 @@
 	struct i915_gtt *gtt = &dev_priv->gtt;
 	int ret;
 
+	INIT_LIST_HEAD(&gtt->base.vma_list);
+	kref_init(&gtt->base.ref);
+
 	if (INTEL_INFO(dev)->gen <= 5) {
 		gtt->gtt_probe = i915_gmch_probe;
 		gtt->base.cleanup = i915_gmch_remove;
@@ -2130,30 +2140,45 @@
 	 * do this now so that we can print out any log messages once rather
 	 * than every time we check intel_enable_ppgtt().
 	 */
-	i915.enable_ppgtt = sanitize_enable_ppgtt(dev, i915.enable_ppgtt);
-	DRM_DEBUG_DRIVER("ppgtt mode: %i\n", i915.enable_ppgtt);
+	i915_module.enable_ppgtt =
+	       	sanitize_enable_ppgtt(dev, i915_module.enable_ppgtt);
+	DRM_DEBUG_DRIVER("ppgtt mode: %i\n", i915_module.enable_ppgtt);
 
 	return 0;
 }
 
-static struct i915_vma *__i915_gem_vma_create(struct drm_i915_gem_object *obj,
-					      struct i915_address_space *vm)
+static struct i915_vma *__i915_vma_create(struct drm_i915_gem_object *obj,
+					  struct i915_address_space *vm)
 {
-	struct i915_vma *vma = kzalloc(sizeof(*vma), GFP_KERNEL);
+	struct i915_vma *vma;
+	int n;
+
+	vma = kzalloc(sizeof(*vma), GFP_KERNEL);
 	if (vma == NULL)
 		return ERR_PTR(-ENOMEM);
 
-	INIT_LIST_HEAD(&vma->vma_link);
-	INIT_LIST_HEAD(&vma->mm_list);
-	INIT_LIST_HEAD(&vma->exec_list);
-	vma->vm = vm;
+	kref_init(&vma->kref);
+	vma->vm = i915_vm_get(vm);
 	vma->obj = obj;
+	list_add(&vma->vm_link, &vm->vma_list);
+
+	for (n = 0; n < I915_NUM_ENGINES; n++)
+		INIT_LIST_HEAD(&vma->last_read[n].engine_link);
+
+	INIT_LIST_HEAD(&vma->mm_list);
+	INIT_LIST_HEAD(&vma->exec_link);
 
 	switch (INTEL_INFO(vm->dev)->gen) {
+	case 9:
 	case 8:
 	case 7:
 	case 6:
-		if (i915_is_ggtt(vm)) {
+		if (obj->pde) {
+			if (WARN_ON(!i915_is_ggtt(vm)))
+				return ERR_PTR(-EINVAL);
+			vma->unbind_vma = pde_unbind_vma;
+			vma->bind_vma = pde_bind_vma;
+		} else if (i915_is_ggtt(vm)) {
 			vma->unbind_vma = ggtt_unbind_vma;
 			vma->bind_vma = ggtt_bind_vma;
 		} else {
@@ -2175,22 +2200,37 @@
 
 	/* Keep GGTT vmas first to make debug easier */
 	if (i915_is_ggtt(vm))
-		list_add(&vma->vma_link, &obj->vma_list);
+		list_add(&vma->obj_link, &obj->vma_list);
 	else
-		list_add_tail(&vma->vma_link, &obj->vma_list);
+		list_add_tail(&vma->obj_link, &obj->vma_list);
 
 	return vma;
 }
 
+void __i915_vma_free(struct kref *kref)
+{
+	struct i915_vma *vma = container_of(kref, typeof(*vma), kref);
+
+	WARN_ON(drm_mm_node_allocated(&vma->node));
+	WARN_ON(vma->bound);
+	WARN_ON(!list_empty(&vma->mm_list));
+	WARN_ON(!list_empty(&vma->exec_link));
+	WARN_ON(!list_empty(&vma->obj_link));
+
+	list_del(&vma->vm_link);
+	i915_vm_put(vma->vm);
+	kfree(vma);
+}
+
 struct i915_vma *
-i915_gem_obj_lookup_or_create_vma(struct drm_i915_gem_object *obj,
-				  struct i915_address_space *vm)
+i915_gem_obj_get_vma(struct drm_i915_gem_object *obj,
+		     struct i915_address_space *vm)
 {
 	struct i915_vma *vma;
 
 	vma = i915_gem_obj_to_vma(obj, vm);
-	if (!vma)
-		vma = __i915_gem_vma_create(obj, vm);
+	if (vma == NULL)
+		vma = __i915_vma_create(obj, vm);
 
-	return vma;
+	return i915_vma_get(vma);
 }
diff -urN a/drivers/gpu/drm/i915/i915_gem_gtt.c~ b/drivers/gpu/drm/i915/i915_gem_gtt.c~
--- a/drivers/gpu/drm/i915/i915_gem_gtt.c~	1969-12-31 17:00:00.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_gem_gtt.c~	2014-11-22 14:37:49.334700418 -0700
@@ -0,0 +1,2236 @@
+/*
+ * Copyright  2010 Daniel Vetter
+ * Copyright  2011-2014 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ *
+ */
+
+#include <linux/seq_file.h>
+#include <drm/drmP.h>
+#include <drm/i915_drm.h>
+#include "i915_drv.h"
+#include "i915_trace.h"
+#include "intel_drv.h"
+
+static void bdw_setup_private_ppat(struct drm_i915_private *dev_priv);
+static void chv_setup_private_ppat(struct drm_i915_private *dev_priv);
+
+static int sanitize_enable_ppgtt(struct drm_device *dev, int enable_ppgtt)
+{
+	bool has_aliasing_ppgtt;
+	bool has_full_ppgtt;
+
+	has_aliasing_ppgtt = INTEL_INFO(dev)->gen >= 6;
+	has_full_ppgtt = INTEL_INFO(dev)->gen >= 7;
+	if (IS_GEN8(dev))
+		has_full_ppgtt = false; /* XXX why? */
+
+	/*
+	 * We don't allow disabling PPGTT for gen9+ as it's a requirement for
+	 * execlists, the sole mechanism available to submit work.
+	 */
+	if (INTEL_INFO(dev)->gen < 9 &&
+	    (enable_ppgtt == 0 || !has_aliasing_ppgtt))
+		return 0;
+
+	if (enable_ppgtt == 1)
+		return 1;
+
+	if (enable_ppgtt == 2 && has_full_ppgtt)
+		return 2;
+
+#ifdef CONFIG_INTEL_IOMMU
+	/* Disable ppgtt on SNB if VT-d is on. */
+	if (INTEL_INFO(dev)->gen == 6 && intel_iommu_gfx_mapped) {
+		DRM_INFO("Disabling PPGTT because VT-d is on\n");
+		return 0;
+	}
+#endif
+
+	/* Early VLV doesn't have this */
+	if (IS_VALLEYVIEW(dev) && !IS_CHERRYVIEW(dev) &&
+	    dev->pdev->revision < 0xb) {
+		DRM_DEBUG_DRIVER("disabling PPGTT on pre-B3 step VLV\n");
+		return 0;
+	}
+
+	return has_aliasing_ppgtt ? 1 : 0;
+}
+
+static inline gen8_gtt_pte_t gen8_pte_encode(dma_addr_t addr,
+					     enum i915_cache_level level,
+					     bool valid)
+{
+	gen8_gtt_pte_t pte = valid ? _PAGE_PRESENT | _PAGE_RW : 0;
+	pte |= addr;
+
+	switch (level) {
+	case I915_CACHE_NONE:
+		pte |= PPAT_UNCACHED_INDEX;
+		break;
+	case I915_CACHE_WT:
+		pte |= PPAT_DISPLAY_ELLC_INDEX;
+		break;
+	default:
+		pte |= PPAT_CACHED_INDEX;
+		break;
+	}
+
+	return pte;
+}
+
+static inline gen8_ppgtt_pde_t gen8_pde_encode(struct drm_device *dev,
+					     dma_addr_t addr,
+					     enum i915_cache_level level)
+{
+	gen8_ppgtt_pde_t pde = _PAGE_PRESENT | _PAGE_RW;
+	pde |= addr;
+	if (level != I915_CACHE_NONE)
+		pde |= PPAT_CACHED_PDE_INDEX;
+	else
+		pde |= PPAT_UNCACHED_INDEX;
+	return pde;
+}
+
+static gen6_gtt_pte_t snb_pte_encode(dma_addr_t addr,
+				     enum i915_cache_level level,
+				     bool valid, u32 unused)
+{
+	gen6_gtt_pte_t pte = valid ? GEN6_PTE_VALID : 0;
+	pte |= GEN6_PTE_ADDR_ENCODE(addr);
+
+	switch (level) {
+	case I915_CACHE_L3_LLC:
+	case I915_CACHE_LLC:
+		pte |= GEN6_PTE_CACHE_LLC;
+		break;
+	case I915_CACHE_NONE:
+		pte |= GEN6_PTE_UNCACHED;
+		break;
+	default:
+		WARN_ON(1);
+	}
+
+	return pte;
+}
+
+static gen6_gtt_pte_t ivb_pte_encode(dma_addr_t addr,
+				     enum i915_cache_level level,
+				     bool valid, u32 unused)
+{
+	gen6_gtt_pte_t pte = valid ? GEN6_PTE_VALID : 0;
+	pte |= GEN6_PTE_ADDR_ENCODE(addr);
+
+	switch (level) {
+	case I915_CACHE_L3_LLC:
+		pte |= GEN7_PTE_CACHE_L3_LLC;
+		break;
+	case I915_CACHE_LLC:
+		pte |= GEN6_PTE_CACHE_LLC;
+		break;
+	case I915_CACHE_NONE:
+		pte |= GEN6_PTE_UNCACHED;
+		break;
+	default:
+		WARN_ON(1);
+	}
+
+	return pte;
+}
+
+static gen6_gtt_pte_t byt_pte_encode(dma_addr_t addr,
+				     enum i915_cache_level level,
+				     bool valid, u32 flags)
+{
+	gen6_gtt_pte_t pte = valid ? GEN6_PTE_VALID : 0;
+	pte |= GEN6_PTE_ADDR_ENCODE(addr);
+
+	if (!(flags & PTE_READ_ONLY))
+		pte |= BYT_PTE_WRITEABLE;
+
+	if (level != I915_CACHE_NONE)
+		pte |= BYT_PTE_SNOOPED_BY_CPU_CACHES;
+
+	return pte;
+}
+
+static gen6_gtt_pte_t hsw_pte_encode(dma_addr_t addr,
+				     enum i915_cache_level level,
+				     bool valid, u32 unused)
+{
+	gen6_gtt_pte_t pte = valid ? GEN6_PTE_VALID : 0;
+	pte |= HSW_PTE_ADDR_ENCODE(addr);
+
+	if (level != I915_CACHE_NONE)
+		pte |= HSW_WB_LLC_AGE3;
+
+	return pte;
+}
+
+static gen6_gtt_pte_t iris_pte_encode(dma_addr_t addr,
+				      enum i915_cache_level level,
+				      bool valid, u32 unused)
+{
+	gen6_gtt_pte_t pte = valid ? GEN6_PTE_VALID : 0;
+	pte |= HSW_PTE_ADDR_ENCODE(addr);
+
+	switch (level) {
+	case I915_CACHE_NONE:
+		break;
+	case I915_CACHE_WT:
+		pte |= HSW_WT_ELLC_LLC_AGE3;
+		break;
+	default:
+		pte |= HSW_WB_ELLC_LLC_AGE3;
+		break;
+	}
+
+	return pte;
+}
+
+/* Broadwell Page Directory Pointer Descriptors */
+static int gen8_write_pdp(struct i915_gem_request *rq, unsigned entry, uint64_t val)
+{
+	struct intel_ringbuffer *ring;
+
+	BUG_ON(entry >= 4);
+
+	ring = intel_ring_begin(rq, 5);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
+
+	intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(2));
+	intel_ring_emit(ring, GEN8_RING_PDP_UDW(rq->engine, entry));
+	intel_ring_emit(ring, upper_32_bits(val));
+	intel_ring_emit(ring, GEN8_RING_PDP_LDW(rq->engine, entry));
+	intel_ring_emit(ring, lower_32_bits(val));
+	intel_ring_advance(ring);
+
+	return 0;
+}
+
+static int gen8_mm_switch(struct i915_gem_request *rq,
+			  struct i915_hw_ppgtt *ppgtt)
+{
+	int i, ret;
+
+	/* bit of a hack to find the actual last used pd */
+	int used_pd = ppgtt->num_pd_entries / GEN8_PDES_PER_PAGE;
+
+	for (i = used_pd - 1; i >= 0; i--) {
+		dma_addr_t addr = ppgtt->pd_dma_addr[i];
+		ret = gen8_write_pdp(rq, i, addr);
+		if (ret)
+			return ret;
+	}
+
+	return 0;
+}
+
+static int gen8_ppgtt_clear_range(struct i915_address_space *vm,
+				  uint64_t start,
+				  uint64_t length,
+				  bool use_scratch)
+{
+	struct i915_hw_ppgtt *ppgtt =
+		container_of(vm, struct i915_hw_ppgtt, base);
+	gen8_gtt_pte_t *pt_vaddr, scratch_pte;
+	unsigned pdpe = start >> GEN8_PDPE_SHIFT & GEN8_PDPE_MASK;
+	unsigned pde = start >> GEN8_PDE_SHIFT & GEN8_PDE_MASK;
+	unsigned pte = start >> GEN8_PTE_SHIFT & GEN8_PTE_MASK;
+	unsigned num_entries = length >> PAGE_SHIFT;
+	unsigned last_pte, i;
+
+	scratch_pte = gen8_pte_encode(ppgtt->base.scratch.addr,
+				      I915_CACHE_LLC, use_scratch);
+
+	while (num_entries) {
+		struct page *page_table = ppgtt->gen8_pt_pages[pdpe][pde];
+
+		last_pte = pte + num_entries;
+		if (last_pte > GEN8_PTES_PER_PAGE)
+			last_pte = GEN8_PTES_PER_PAGE;
+
+		pt_vaddr = kmap_atomic(page_table);
+
+		for (i = pte; i < last_pte; i++) {
+			pt_vaddr[i] = scratch_pte;
+			num_entries--;
+		}
+
+		if (!HAS_LLC(ppgtt->base.dev))
+			drm_clflush_virt_range(pt_vaddr, PAGE_SIZE);
+		kunmap_atomic(pt_vaddr);
+
+		pte = 0;
+		if (++pde == GEN8_PDES_PER_PAGE) {
+			pdpe++;
+			pde = 0;
+		}
+	}
+
+	return 0;
+}
+
+static int gen8_ppgtt_insert_entries(struct i915_address_space *vm,
+				     struct sg_table *pages,
+				     uint64_t start,
+				     enum i915_cache_level cache_level, u32 unused)
+{
+	struct i915_hw_ppgtt *ppgtt =
+		container_of(vm, struct i915_hw_ppgtt, base);
+	gen8_gtt_pte_t *pt_vaddr;
+	unsigned pdpe = start >> GEN8_PDPE_SHIFT & GEN8_PDPE_MASK;
+	unsigned pde = start >> GEN8_PDE_SHIFT & GEN8_PDE_MASK;
+	unsigned pte = start >> GEN8_PTE_SHIFT & GEN8_PTE_MASK;
+	struct sg_page_iter sg_iter;
+
+	pt_vaddr = NULL;
+
+	for_each_sg_page(pages->sgl, &sg_iter, pages->nents, 0) {
+		if (WARN_ON(pdpe >= GEN8_LEGACY_PDPS))
+			break;
+
+		if (pt_vaddr == NULL)
+			pt_vaddr = kmap_atomic(ppgtt->gen8_pt_pages[pdpe][pde]);
+
+		pt_vaddr[pte] =
+			gen8_pte_encode(sg_page_iter_dma_address(&sg_iter),
+					cache_level, true);
+		if (++pte == GEN8_PTES_PER_PAGE) {
+			if (!HAS_LLC(ppgtt->base.dev))
+				drm_clflush_virt_range(pt_vaddr, PAGE_SIZE);
+			kunmap_atomic(pt_vaddr);
+			pt_vaddr = NULL;
+			if (++pde == GEN8_PDES_PER_PAGE) {
+				pdpe++;
+				pde = 0;
+			}
+			pte = 0;
+		}
+	}
+	if (pt_vaddr) {
+		if (!HAS_LLC(ppgtt->base.dev))
+			drm_clflush_virt_range(pt_vaddr, PAGE_SIZE);
+		kunmap_atomic(pt_vaddr);
+	}
+
+	return 0;
+}
+
+static void gen8_free_page_tables(struct page **pt_pages)
+{
+	int i;
+
+	if (pt_pages == NULL)
+		return;
+
+	for (i = 0; i < GEN8_PDES_PER_PAGE; i++)
+		if (pt_pages[i])
+			__free_pages(pt_pages[i], 0);
+}
+
+static void gen8_ppgtt_free(const struct i915_hw_ppgtt *ppgtt)
+{
+	int i;
+
+	for (i = 0; i < ppgtt->num_pd_pages; i++) {
+		gen8_free_page_tables(ppgtt->gen8_pt_pages[i]);
+		kfree(ppgtt->gen8_pt_pages[i]);
+		kfree(ppgtt->gen8_pt_dma_addr[i]);
+	}
+
+	__free_pages(ppgtt->pd_pages, get_order(ppgtt->num_pd_pages << PAGE_SHIFT));
+}
+
+static void gen8_ppgtt_unmap_pages(struct i915_hw_ppgtt *ppgtt)
+{
+	struct pci_dev *hwdev = ppgtt->base.dev->pdev;
+	int i, j;
+
+	for (i = 0; i < ppgtt->num_pd_pages; i++) {
+		/* TODO: In the future we'll support sparse mappings, so this
+		 * will have to change. */
+		if (!ppgtt->pd_dma_addr[i])
+			continue;
+
+		pci_unmap_page(hwdev, ppgtt->pd_dma_addr[i], PAGE_SIZE,
+			       PCI_DMA_BIDIRECTIONAL);
+
+		for (j = 0; j < GEN8_PDES_PER_PAGE; j++) {
+			dma_addr_t addr = ppgtt->gen8_pt_dma_addr[i][j];
+			if (addr)
+				pci_unmap_page(hwdev, addr, PAGE_SIZE,
+					       PCI_DMA_BIDIRECTIONAL);
+		}
+	}
+}
+
+static void gen8_ppgtt_cleanup(struct i915_address_space *vm)
+{
+	struct i915_hw_ppgtt *ppgtt =
+		container_of(vm, struct i915_hw_ppgtt, base);
+
+	gen8_ppgtt_unmap_pages(ppgtt);
+	gen8_ppgtt_free(ppgtt);
+}
+
+static struct page **__gen8_alloc_page_tables(void)
+{
+	struct page **pt_pages;
+	int i;
+
+	pt_pages = kcalloc(GEN8_PDES_PER_PAGE, sizeof(struct page *), GFP_KERNEL);
+	if (!pt_pages)
+		return ERR_PTR(-ENOMEM);
+
+	for (i = 0; i < GEN8_PDES_PER_PAGE; i++) {
+		pt_pages[i] = alloc_page(GFP_KERNEL);
+		if (!pt_pages[i])
+			goto bail;
+	}
+
+	return pt_pages;
+
+bail:
+	gen8_free_page_tables(pt_pages);
+	kfree(pt_pages);
+	return ERR_PTR(-ENOMEM);
+}
+
+static int gen8_ppgtt_allocate_page_tables(struct i915_hw_ppgtt *ppgtt,
+					   const int max_pdp)
+{
+	struct page **pt_pages[GEN8_LEGACY_PDPS];
+	int i, ret;
+
+	for (i = 0; i < max_pdp; i++) {
+		pt_pages[i] = __gen8_alloc_page_tables();
+		if (IS_ERR(pt_pages[i])) {
+			ret = PTR_ERR(pt_pages[i]);
+			goto unwind_out;
+		}
+	}
+
+	/* NB: Avoid touching gen8_pt_pages until last to keep the allocation,
+	 * "atomic" - for cleanup purposes.
+	 */
+	for (i = 0; i < max_pdp; i++)
+		ppgtt->gen8_pt_pages[i] = pt_pages[i];
+
+	return 0;
+
+unwind_out:
+	while (i--) {
+		gen8_free_page_tables(pt_pages[i]);
+		kfree(pt_pages[i]);
+	}
+
+	return ret;
+}
+
+static int gen8_ppgtt_allocate_dma(struct i915_hw_ppgtt *ppgtt)
+{
+	int i;
+
+	for (i = 0; i < ppgtt->num_pd_pages; i++) {
+		ppgtt->gen8_pt_dma_addr[i] = kcalloc(GEN8_PDES_PER_PAGE,
+						     sizeof(dma_addr_t),
+						     GFP_KERNEL);
+		if (!ppgtt->gen8_pt_dma_addr[i])
+			return -ENOMEM;
+	}
+
+	return 0;
+}
+
+static int gen8_ppgtt_allocate_page_directories(struct i915_hw_ppgtt *ppgtt,
+						const int max_pdp)
+{
+	ppgtt->pd_pages = alloc_pages(GFP_KERNEL, get_order(max_pdp << PAGE_SHIFT));
+	if (!ppgtt->pd_pages)
+		return -ENOMEM;
+
+	ppgtt->num_pd_pages = 1 << get_order(max_pdp << PAGE_SHIFT);
+	BUG_ON(ppgtt->num_pd_pages > GEN8_LEGACY_PDPS);
+
+	return 0;
+}
+
+static int gen8_ppgtt_alloc(struct i915_hw_ppgtt *ppgtt,
+			    const int max_pdp)
+{
+	int ret;
+
+	ret = gen8_ppgtt_allocate_page_directories(ppgtt, max_pdp);
+	if (ret)
+		return ret;
+
+	ret = gen8_ppgtt_allocate_page_tables(ppgtt, max_pdp);
+	if (ret) {
+		__free_pages(ppgtt->pd_pages, get_order(max_pdp << PAGE_SHIFT));
+		return ret;
+	}
+
+	ppgtt->num_pd_entries = max_pdp * GEN8_PDES_PER_PAGE;
+
+	ret = gen8_ppgtt_allocate_dma(ppgtt);
+	if (ret)
+		gen8_ppgtt_free(ppgtt);
+
+	return ret;
+}
+
+static int gen8_ppgtt_setup_page_directories(struct i915_hw_ppgtt *ppgtt,
+					     const int pd)
+{
+	dma_addr_t pd_addr;
+	int ret;
+
+	pd_addr = pci_map_page(ppgtt->base.dev->pdev,
+			       &ppgtt->pd_pages[pd], 0,
+			       PAGE_SIZE, PCI_DMA_BIDIRECTIONAL);
+
+	ret = pci_dma_mapping_error(ppgtt->base.dev->pdev, pd_addr);
+	if (ret)
+		return ret;
+
+	ppgtt->pd_dma_addr[pd] = pd_addr;
+
+	return 0;
+}
+
+static int gen8_ppgtt_setup_page_tables(struct i915_hw_ppgtt *ppgtt,
+					const int pd,
+					const int pt)
+{
+	dma_addr_t pt_addr;
+	struct page *p;
+	int ret;
+
+	p = ppgtt->gen8_pt_pages[pd][pt];
+	pt_addr = pci_map_page(ppgtt->base.dev->pdev,
+			       p, 0, PAGE_SIZE, PCI_DMA_BIDIRECTIONAL);
+	ret = pci_dma_mapping_error(ppgtt->base.dev->pdev, pt_addr);
+	if (ret)
+		return ret;
+
+	ppgtt->gen8_pt_dma_addr[pd][pt] = pt_addr;
+
+	return 0;
+}
+
+/**
+ * GEN8 legacy ppgtt programming is accomplished through a max 4 PDP registers
+ * with a net effect resembling a 2-level page table in normal x86 terms. Each
+ * PDP represents 1GB of memory 4 * 512 * 512 * 4096 = 4GB legacy 32b address
+ * space.
+ *
+ * FIXME: split allocation into smaller pieces. For now we only ever do this
+ * once, but with full PPGTT, the multiple contiguous allocations will be bad.
+ * TODO: Do something with the size parameter
+ */
+static int gen8_ppgtt_init(struct i915_hw_ppgtt *ppgtt, uint64_t size)
+{
+	const int max_pdp = DIV_ROUND_UP(size, 1 << 30);
+	const int min_pt_pages = GEN8_PDES_PER_PAGE * max_pdp;
+	int i, j, ret;
+
+	if (size % (1<<30))
+		DRM_INFO("Pages will be wasted unless GTT size (%llu) is divisible by 1GB\n", size);
+
+	/* 1. Do all our allocations for page directories and page tables. */
+	ret = gen8_ppgtt_alloc(ppgtt, max_pdp);
+	if (ret)
+		return ret;
+
+	/*
+	 * 2. Create DMA mappings for the page directories and page tables.
+	 */
+	for (i = 0; i < max_pdp; i++) {
+		ret = gen8_ppgtt_setup_page_directories(ppgtt, i);
+		if (ret)
+			goto bail;
+
+		for (j = 0; j < GEN8_PDES_PER_PAGE; j++) {
+			ret = gen8_ppgtt_setup_page_tables(ppgtt, i, j);
+			if (ret)
+				goto bail;
+		}
+	}
+
+	/*
+	 * 3. Map all the page directory entires to point to the page tables
+	 * we've allocated.
+	 *
+	 * For now, the PPGTT helper functions all require that the PDEs are
+	 * plugged in correctly. So we do that now/here. For aliasing PPGTT, we
+	 * will never need to touch the PDEs again.
+	 */
+	for (i = 0; i < max_pdp; i++) {
+		gen8_ppgtt_pde_t *pd_vaddr;
+		pd_vaddr = kmap_atomic(&ppgtt->pd_pages[i]);
+		for (j = 0; j < GEN8_PDES_PER_PAGE; j++) {
+			dma_addr_t addr = ppgtt->gen8_pt_dma_addr[i][j];
+			pd_vaddr[j] = gen8_pde_encode(ppgtt->base.dev, addr,
+						      I915_CACHE_LLC);
+		}
+		if (!HAS_LLC(ppgtt->base.dev))
+			drm_clflush_virt_range(pd_vaddr, PAGE_SIZE);
+		kunmap_atomic(pd_vaddr);
+	}
+
+	ppgtt->switch_mm = gen8_mm_switch;
+	ppgtt->base.clear_range = gen8_ppgtt_clear_range;
+	ppgtt->base.insert_entries = gen8_ppgtt_insert_entries;
+	ppgtt->base.cleanup = gen8_ppgtt_cleanup;
+	ppgtt->base.start = 0;
+	ppgtt->base.total = ppgtt->num_pd_entries * GEN8_PTES_PER_PAGE * PAGE_SIZE;
+
+	ppgtt->base.clear_range(&ppgtt->base, 0, ppgtt->base.total, true);
+
+	DRM_DEBUG_DRIVER("Allocated %d pages for page directories (%d wasted)\n",
+			 ppgtt->num_pd_pages, ppgtt->num_pd_pages - max_pdp);
+	DRM_DEBUG_DRIVER("Allocated %d pages for page tables (%lld wasted)\n",
+			 ppgtt->num_pd_entries,
+			 (ppgtt->num_pd_entries - min_pt_pages) + size % (1<<30));
+	return 0;
+
+bail:
+	gen8_ppgtt_unmap_pages(ppgtt);
+	gen8_ppgtt_free(ppgtt);
+	return ret;
+}
+
+static void gen6_dump_ppgtt(struct i915_hw_ppgtt *ppgtt, struct seq_file *m)
+{
+	struct i915_address_space *vm = &ppgtt->base;
+	gen6_gtt_pte_t scratch_pte;
+	int pte, pde;
+
+	if (ppgtt->state->pages == NULL)
+		return;
+
+	scratch_pte = vm->pte_encode(vm->scratch.addr, I915_CACHE_LLC, true, 0);
+
+	for (pde = 0; pde < ppgtt->num_pd_entries; pde++) {
+		gen6_gtt_pte_t *pt_vaddr;
+
+		pt_vaddr = kmap_atomic(i915_gem_object_get_page(ppgtt->state, pde));
+		for (pte = 0; pte < I915_PPGTT_PT_ENTRIES; pte+=4) {
+			unsigned long va =
+				(pde * PAGE_SIZE * I915_PPGTT_PT_ENTRIES) +
+				(pte * PAGE_SIZE);
+			int i;
+			bool found = false;
+			for (i = 0; i < 4; i++)
+				if (pt_vaddr[pte + i] != scratch_pte)
+					found = true;
+			if (!found)
+				continue;
+
+			seq_printf(m, "\t\t0x%lx [%03d,%04d]: =", va, pde, pte);
+			for (i = 0; i < 4; i++) {
+				if (pt_vaddr[pte + i] != scratch_pte)
+					seq_printf(m, " %08x", pt_vaddr[pte + i]);
+				else
+					seq_puts(m, "  SCRATCH ");
+			}
+			seq_puts(m, "\n");
+		}
+		kunmap_atomic(pt_vaddr);
+	}
+}
+
+static uint32_t get_pd_offset(struct i915_hw_ppgtt *ppgtt)
+{
+	uint64_t offset = i915_gem_obj_to_ggtt(ppgtt->state)->node.start / PAGE_SIZE;
+	return (offset * sizeof(gen6_gtt_pte_t) / 64) << 16;
+}
+
+static int gen7_mm_switch(struct i915_gem_request *rq,
+			  struct i915_hw_ppgtt *ppgtt)
+{
+	struct intel_ringbuffer *ring;
+	int ret;
+
+	rq->pending_flush = ~0; /* XXX force the flush */
+	ret = i915_request_emit_flush(rq, I915_COMMAND_BARRIER);
+	if (ret)
+		return ret;
+
+	ring = intel_ring_begin(rq, 5);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
+
+	intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(2));
+	intel_ring_emit(ring, RING_PP_DIR_DCLV(rq->engine));
+	intel_ring_emit(ring, PP_DIR_DCLV_2G);
+	intel_ring_emit(ring, RING_PP_DIR_BASE(rq->engine));
+	intel_ring_emit(ring, get_pd_offset(ppgtt));
+	intel_ring_advance(ring);
+
+	rq->pending_flush |= I915_INVALIDATE_CACHES;
+
+	return 0;
+}
+
+static int gen6_mm_switch(struct i915_gem_request *rq,
+			  struct i915_hw_ppgtt *ppgtt)
+{
+	return -ENODEV;
+}
+
+static int gen8_ppgtt_enable(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_engine_cs *engine;
+	int j;
+
+	for_each_engine(engine, dev_priv, j) {
+		I915_WRITE(RING_MODE_GEN7(engine),
+				_MASKED_BIT_ENABLE(GFX_PPGTT_ENABLE));
+
+		if (dev_priv->mm.aliasing_ppgtt) {
+			struct i915_gem_request *rq;
+			int ret;
+
+			rq = i915_request_create(engine->default_context,
+						 engine);
+			if (IS_ERR(rq))
+				return PTR_ERR(rq);
+
+			ret = gen8_mm_switch(rq, dev_priv->mm.aliasing_ppgtt);
+			if (ret == 0)
+				ret = i915_request_commit(rq);
+			i915_request_put(rq);
+			if (ret)
+				return ret;
+		}
+	}
+
+	return 0;
+}
+
+static int gen7_ppgtt_enable(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_engine_cs *engine;
+	uint32_t ecochk, ecobits;
+	int i;
+
+	ecobits = I915_READ(GAC_ECO_BITS);
+	I915_WRITE(GAC_ECO_BITS, ecobits | ECOBITS_PPGTT_CACHE64B);
+
+	ecochk = I915_READ(GAM_ECOCHK);
+	if (IS_HASWELL(dev)) {
+		ecochk |= ECOCHK_PPGTT_WB_HSW;
+	} else {
+		ecochk |= ECOCHK_PPGTT_LLC_IVB;
+		ecochk &= ~ECOCHK_PPGTT_GFDT_IVB;
+	}
+	I915_WRITE(GAM_ECOCHK, ecochk);
+
+	for_each_engine(engine, dev_priv, i) {
+		/* GFX_MODE is per-ring on gen7+ */
+		I915_WRITE(RING_MODE_GEN7(engine),
+			   _MASKED_BIT_ENABLE(GFX_PPGTT_ENABLE));
+
+		if (dev_priv->mm.aliasing_ppgtt) {
+			I915_WRITE(RING_PP_DIR_DCLV(engine), PP_DIR_DCLV_2G);
+			I915_WRITE(RING_PP_DIR_BASE(engine), get_pd_offset(dev_priv->mm.aliasing_ppgtt));
+		}
+
+	}
+	POSTING_READ(RING_PP_DIR_DCLV(RCS_ENGINE(dev_priv)));
+	return 0;
+}
+
+static int gen6_ppgtt_enable(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	uint32_t ecochk, gab_ctl, ecobits;
+	struct intel_engine_cs *engine;
+	int i;
+
+	ecobits = I915_READ(GAC_ECO_BITS);
+	I915_WRITE(GAC_ECO_BITS, ecobits | ECOBITS_SNB_BIT |
+		   ECOBITS_PPGTT_CACHE64B);
+
+	gab_ctl = I915_READ(GAB_CTL);
+	I915_WRITE(GAB_CTL, gab_ctl | GAB_CTL_CONT_AFTER_PAGEFAULT);
+
+	ecochk = I915_READ(GAM_ECOCHK);
+	I915_WRITE(GAM_ECOCHK, ecochk | ECOCHK_SNB_BIT | ECOCHK_PPGTT_CACHE64B);
+
+	I915_WRITE(GFX_MODE, _MASKED_BIT_ENABLE(GFX_PPGTT_ENABLE));
+
+	for_each_engine(engine, dev_priv, i) {
+		if (dev_priv->mm.aliasing_ppgtt) {
+			I915_WRITE(RING_PP_DIR_DCLV(engine), PP_DIR_DCLV_2G);
+			I915_WRITE(RING_PP_DIR_BASE(engine), get_pd_offset(dev_priv->mm.aliasing_ppgtt));
+		}
+
+	}
+	POSTING_READ(RING_PP_DIR_DCLV(RCS_ENGINE(dev_priv)));
+	return 0;
+}
+
+/* PPGTT support for Sandybdrige/Gen6 and later */
+static inline void *kmap_pt(struct i915_hw_ppgtt *ppgtt, int pt)
+{
+	return kmap_atomic(i915_gem_object_get_page(ppgtt->state, pt));
+}
+
+static int gen6_ppgtt_clear_range(struct i915_address_space *vm,
+				  uint64_t start,
+				  uint64_t length,
+				  bool use_scratch)
+{
+	struct i915_hw_ppgtt *ppgtt =
+		container_of(vm, struct i915_hw_ppgtt, base);
+	gen6_gtt_pte_t *pt_vaddr, scratch_pte;
+	unsigned first_entry = start >> PAGE_SHIFT;
+	unsigned num_entries = length >> PAGE_SHIFT;
+	unsigned act_pt = first_entry / I915_PPGTT_PT_ENTRIES;
+	unsigned first_pte = first_entry % I915_PPGTT_PT_ENTRIES;
+	unsigned last_pte, i;
+	int ret;
+
+	ret = i915_gem_object_get_pages(ppgtt->state);
+	if (ret)
+		return ret;
+
+	scratch_pte = vm->pte_encode(vm->scratch.addr, I915_CACHE_LLC, true, 0);
+
+	while (num_entries) {
+		last_pte = first_pte + num_entries;
+		if (last_pte > I915_PPGTT_PT_ENTRIES)
+			last_pte = I915_PPGTT_PT_ENTRIES;
+
+		pt_vaddr = kmap_pt(ppgtt, act_pt);
+
+		for (i = first_pte; i < last_pte; i++)
+			pt_vaddr[i] = scratch_pte;
+
+		kunmap_atomic(pt_vaddr);
+
+		num_entries -= last_pte - first_pte;
+		first_pte = 0;
+		act_pt++;
+	}
+
+	return 0;
+}
+
+static int gen6_ppgtt_insert_entries(struct i915_address_space *vm,
+				     struct sg_table *pages, uint64_t start,
+				     enum i915_cache_level cache_level,
+				     u32 flags)
+{
+	struct i915_hw_ppgtt *ppgtt =
+		container_of(vm, struct i915_hw_ppgtt, base);
+	gen6_gtt_pte_t *pt_vaddr;
+	unsigned first_entry = start >> PAGE_SHIFT;
+	unsigned act_pt = first_entry / I915_PPGTT_PT_ENTRIES;
+	unsigned act_pte = first_entry % I915_PPGTT_PT_ENTRIES;
+	struct sg_page_iter sg_iter;
+	int ret;
+
+	ret = i915_gem_object_get_pages(ppgtt->state);
+	if (ret)
+		return ret;
+
+	pt_vaddr = NULL;
+	for_each_sg_page(pages->sgl, &sg_iter, pages->nents, 0) {
+		if (pt_vaddr == NULL)
+			pt_vaddr = kmap_pt(ppgtt, act_pt);
+
+		pt_vaddr[act_pte] =
+			vm->pte_encode(sg_page_iter_dma_address(&sg_iter),
+				       cache_level, true, flags);
+
+		if (++act_pte == I915_PPGTT_PT_ENTRIES) {
+			kunmap_atomic(pt_vaddr);
+			pt_vaddr = NULL;
+			act_pt++;
+			act_pte = 0;
+		}
+	}
+	if (pt_vaddr)
+		kunmap_atomic(pt_vaddr);
+
+	return 0;
+}
+
+static void gen6_ppgtt_cleanup(struct i915_address_space *vm)
+{
+	struct i915_hw_ppgtt *ppgtt =
+		container_of(vm, struct i915_hw_ppgtt, base);
+
+	drm_gem_object_unreference(&ppgtt->state->base);
+}
+
+static int gen6_ppgtt_alloc(struct i915_hw_ppgtt *ppgtt)
+{
+	/* PPGTT PDEs reside in the GGTT and consists of 512 entries. The
+	 * allocator works in address space sizes, so it's multiplied by page
+	 * size. We allocate at the top of the GTT to avoid fragmentation.
+	 */
+	ppgtt->state = i915_gem_alloc_object(ppgtt->base.dev, GEN6_PD_SIZE);
+	if (ppgtt->state == NULL)
+		return -ENOMEM;
+
+	ppgtt->state->pde = true;
+
+	ppgtt->num_pd_entries = GEN6_PPGTT_PD_ENTRIES;
+	ppgtt->alignment = GEN6_PD_ALIGN;
+	return 0;
+}
+
+static int gen6_ppgtt_init(struct i915_hw_ppgtt *ppgtt)
+{
+	struct drm_device *dev = ppgtt->base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	int ret;
+
+	ppgtt->base.pte_encode = dev_priv->gtt.base.pte_encode;
+	if (IS_GEN6(dev)) {
+		ppgtt->switch_mm = gen6_mm_switch;
+	} else if (IS_GEN7(dev)) {
+		ppgtt->switch_mm = gen7_mm_switch;
+	} else
+		BUG();
+
+	ret = gen6_ppgtt_alloc(ppgtt);
+	if (ret)
+		return ret;
+
+	ppgtt->base.clear_range = gen6_ppgtt_clear_range;
+	ppgtt->base.insert_entries = gen6_ppgtt_insert_entries;
+	ppgtt->base.cleanup = gen6_ppgtt_cleanup;
+	ppgtt->base.start = 0;
+	ppgtt->base.total =  ppgtt->num_pd_entries * I915_PPGTT_PT_ENTRIES * PAGE_SIZE;
+	ppgtt->debug_dump = gen6_dump_ppgtt;
+
+	return ppgtt->base.clear_range(&ppgtt->base,
+				       0, ppgtt->base.total,
+				       true);
+}
+
+static void i915_ggtt_flush(struct drm_i915_private *dev_priv)
+{
+	if (INTEL_INFO(dev_priv->dev)->gen < 6) {
+		intel_gtt_chipset_flush();
+	} else {
+		I915_WRITE(GFX_FLSH_CNTL_GEN6, GFX_FLSH_CNTL_EN);
+		POSTING_READ(GFX_FLSH_CNTL_GEN6);
+	}
+}
+
+static int pde_bind_vma(struct i915_vma *vma,
+			enum i915_cache_level cache_level,
+			u32 flags)
+{
+	struct sg_table *pages = vma->obj->pages;
+	struct drm_i915_private *i915 = to_i915(vma->vm->dev);
+	gen6_gtt_pte_t __iomem *pd_addr;
+	struct sg_page_iter sg_iter;
+
+	if (WARN_ON(flags != GLOBAL_BIND))
+		return -EINVAL;
+
+	if (WARN_ON(vma->node.start & 0x3f))
+		return -EINVAL;
+
+	if (vma->bound & GLOBAL_BIND)
+		return 0;
+
+	pd_addr = (gen6_gtt_pte_t __iomem*)i915->gtt.gsm;
+	pd_addr += vma->node.start >> PAGE_SHIFT;
+	for_each_sg_page(pages->sgl, &sg_iter, pages->nents, 0) {
+		dma_addr_t addr = (sg_dma_address(sg_iter.sg) +
+				   (sg_iter.sg_pgoffset << PAGE_SHIFT));
+		writel(GEN6_PDE_ADDR_ENCODE(addr) | GEN6_PDE_VALID, pd_addr++);
+	}
+	i915_ggtt_flush(i915);
+
+	/* Skips a second call when pinning */
+	vma->bound = GLOBAL_BIND;
+	return 0;
+}
+
+static int pde_unbind_vma(struct i915_vma *vma)
+{
+	vma->bound = 0;
+	return 0;
+}
+
+static int __hw_ppgtt_init(struct drm_device *dev, struct i915_hw_ppgtt *ppgtt)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	ppgtt->base.dev = dev;
+	ppgtt->base.scratch = dev_priv->gtt.base.scratch;
+	INIT_LIST_HEAD(&ppgtt->base.vma_list);
+
+	if (INTEL_INFO(dev)->gen < 8)
+		return gen6_ppgtt_init(ppgtt);
+	else if (IS_GEN8(dev) || IS_GEN9(dev))
+		return gen8_ppgtt_init(ppgtt, dev_priv->gtt.base.total);
+	else
+		BUG();
+}
+int i915_ppgtt_init(struct drm_device *dev, struct i915_hw_ppgtt *ppgtt)
+{
+	int ret;
+
+	ret = __hw_ppgtt_init(dev, ppgtt);
+	if (ret)
+		return ret;
+
+	drm_mm_init(&ppgtt->base.mm, ppgtt->base.start, ppgtt->base.total);
+	i915_init_vm(to_i915(dev), &ppgtt->base);
+
+	return 0;
+}
+
+int i915_ppgtt_init_hw(struct drm_device *dev)
+{
+	int ret;
+
+	if (!USES_PPGTT(dev))
+		return 0;
+
+	/* In the case of execlists, PPGTT is enabled by the context descriptor
+	 * and the PDPs are contained within the context itself.  We don't
+	 * need to do anything here. */
+	if (RCS_ENGINE(dev)->execlists_enabled)
+		return 0;
+
+	if (IS_GEN6(dev))
+		ret = gen6_ppgtt_enable(dev);
+	else if (IS_GEN7(dev))
+		ret = gen7_ppgtt_enable(dev);
+	else if (INTEL_INFO(dev)->gen >= 8)
+		ret = gen8_ppgtt_enable(dev);
+	else {
+		WARN_ON(1);
+		ret = -ENODEV;
+	}
+
+	return ret;
+}
+
+struct i915_hw_ppgtt *
+i915_ppgtt_create(struct drm_device *dev)
+{
+	struct i915_hw_ppgtt *ppgtt;
+	int ret;
+
+	ppgtt = kzalloc(sizeof(*ppgtt), GFP_KERNEL);
+	if (!ppgtt)
+		return ERR_PTR(-ENOMEM);
+
+	ret = i915_ppgtt_init(dev, ppgtt);
+	if (ret) {
+		kfree(ppgtt);
+		return ERR_PTR(ret);
+	}
+
+	trace_i915_vm_create(&ppgtt->base);
+
+	return ppgtt;
+}
+
+void __i915_vm_free(struct kref *kref)
+{
+	struct i915_address_space *vm =
+		container_of(kref, struct i915_address_space, ref);
+
+	DRM_DEBUG_DRIVER("VM %p freed\n", vm);
+	trace_i915_vm_free(vm);
+
+	/* vmas should already be unbound */
+	WARN_ON(!list_empty(&vm->vma_list));
+	WARN_ON(!list_empty(&vm->active_list));
+	WARN_ON(!list_empty(&vm->inactive_list));
+
+	list_del(&vm->global_link);
+	drm_mm_takedown(&vm->mm);
+
+	vm->cleanup(vm);
+	kfree(vm);
+}
+
+static int
+ppgtt_bind_vma(struct i915_vma *vma,
+	       enum i915_cache_level cache_level,
+	       u32 flags)
+{
+	int ret;
+
+	if (WARN_ON(flags & GLOBAL_BIND))
+		return -EINVAL;
+
+	/* Currently applicable only to VLV */
+	if (vma->obj->gt_ro)
+		flags |= PTE_READ_ONLY;
+
+	if (flags == vma->bound)
+		return 0;
+
+	if (flags == REBIND) {
+		flags &= ~REBIND;
+		flags |= LOCAL_BIND;
+	}
+
+	ret = vma->vm->insert_entries(vma->vm,
+				      vma->obj->pages, vma->node.start,
+				      cache_level, flags);
+	if (ret)
+		return ret;
+
+	vma->vm->dirty = true;
+	vma->bound = flags;
+	return 0;
+}
+
+static int ppgtt_unbind_vma(struct i915_vma *vma)
+{
+	vma->vm->clear_range(vma->vm,
+			     vma->node.start,
+			     vma->obj->base.size,
+			     true);
+	vma->bound = 0;
+	return 0;
+}
+
+extern int intel_iommu_gfx_mapped;
+/* Certain Gen5 chipsets require require idling the GPU before
+ * unmapping anything from the GTT when VT-d is enabled.
+ */
+static inline bool needs_idle_maps(struct drm_device *dev)
+{
+#ifdef CONFIG_INTEL_IOMMU
+	/* Query intel_iommu to see if we need the workaround. Presumably that
+	 * was loaded first.
+	 */
+	if (IS_GEN5(dev) && IS_MOBILE(dev) && intel_iommu_gfx_mapped)
+		return true;
+#endif
+	return false;
+}
+
+static bool do_idling(struct drm_i915_private *dev_priv)
+{
+	bool ret = dev_priv->mm.interruptible;
+
+	if (unlikely(dev_priv->gtt.do_idle_maps)) {
+		dev_priv->mm.interruptible = false;
+		if (i915_gpu_idle(dev_priv->dev)) {
+			DRM_ERROR("Couldn't idle GPU\n");
+			/* Wait a bit, in hopes it avoids the hang */
+			udelay(10);
+		}
+	}
+
+	return ret;
+}
+
+static void undo_idling(struct drm_i915_private *dev_priv, bool interruptible)
+{
+	if (unlikely(dev_priv->gtt.do_idle_maps))
+		dev_priv->mm.interruptible = interruptible;
+}
+
+void i915_check_and_clear_faults(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_engine_cs *engine;
+	int i;
+
+	if (INTEL_INFO(dev)->gen < 6)
+		return;
+
+	for_each_engine(engine, dev_priv, i) {
+		u32 fault_reg;
+		fault_reg = I915_READ(RING_FAULT_REG(engine));
+		if (fault_reg & RING_FAULT_VALID) {
+			DRM_DEBUG_DRIVER("Unexpected fault\n"
+					 "\tAddr: 0x%08lx\n"
+					 "\tAddress space: %s\n"
+					 "\tSource ID: %d\n"
+					 "\tType: %d\n",
+					 fault_reg & PAGE_MASK,
+					 fault_reg & RING_FAULT_GTTSEL_MASK ? "GGTT" : "PPGTT",
+					 RING_FAULT_SRCID(fault_reg),
+					 RING_FAULT_FAULT_TYPE(fault_reg));
+			I915_WRITE(RING_FAULT_REG(engine),
+				   fault_reg & ~RING_FAULT_VALID);
+		}
+	}
+	POSTING_READ(RING_FAULT_REG(RCS_ENGINE(dev_priv)));
+}
+
+void i915_gem_suspend_gtt_mappings(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	/* Don't bother messing with faults pre GEN6 as we have little
+	 * documentation supporting that it's a good idea.
+	 */
+	if (INTEL_INFO(dev)->gen < 6)
+		return;
+
+	i915_check_and_clear_faults(dev);
+
+	dev_priv->gtt.base.clear_range(&dev_priv->gtt.base,
+				       dev_priv->gtt.base.start,
+				       dev_priv->gtt.base.total,
+				       true);
+
+	i915_ggtt_flush(dev_priv);
+}
+
+void i915_gem_restore_gtt_mappings(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_gem_object *obj;
+
+	i915_check_and_clear_faults(dev);
+
+	/* First fill our portion of the GTT with scratch pages */
+	dev_priv->gtt.base.clear_range(&dev_priv->gtt.base,
+				       dev_priv->gtt.base.start,
+				       dev_priv->gtt.base.total,
+				       true);
+
+	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list) {
+		struct i915_vma *vma = i915_gem_obj_to_vma(obj,
+							   &dev_priv->gtt.base);
+		if (!vma)
+			continue;
+
+		i915_gem_clflush_object(obj, obj->pin_display);
+		vma->bind_vma(vma, obj->cache_level, REBIND);
+	}
+
+
+	if (INTEL_INFO(dev)->gen >= 8) {
+		if (IS_CHERRYVIEW(dev))
+			chv_setup_private_ppat(dev_priv);
+		else
+			bdw_setup_private_ppat(dev_priv);
+
+		return;
+	}
+
+	i915_ggtt_flush(dev_priv);
+}
+
+int i915_gem_gtt_prepare_object(struct drm_i915_gem_object *obj)
+{
+	if (obj->has_dma_mapping)
+		return 0;
+
+	if (!dma_map_sg(&obj->base.dev->pdev->dev,
+			obj->pages->sgl, obj->pages->nents,
+			PCI_DMA_BIDIRECTIONAL))
+		return -ENOSPC;
+
+	return 0;
+}
+
+static inline void gen8_set_pte(void __iomem *addr, gen8_gtt_pte_t pte)
+{
+#if defined(writeq)
+	writeq(pte, addr);
+#else
+	iowrite32(lower_32_bits(pte), addr);
+	iowrite32(upper_32_bits(pte), addr + 4);
+#endif
+}
+
+static int gen8_ggtt_insert_entries(struct i915_address_space *vm,
+				    struct sg_table *st,
+				    uint64_t start,
+				    enum i915_cache_level level, u32 unused)
+{
+	struct drm_i915_private *dev_priv = vm->dev->dev_private;
+	unsigned first_entry = start >> PAGE_SHIFT;
+	gen8_gtt_pte_t __iomem *gtt_entries =
+		(gen8_gtt_pte_t __iomem *)dev_priv->gtt.gsm + first_entry;
+	int i = 0;
+	struct sg_page_iter sg_iter;
+	dma_addr_t addr = 0; /* shut up gcc */
+
+	for_each_sg_page(st->sgl, &sg_iter, st->nents, 0) {
+		addr = sg_dma_address(sg_iter.sg) +
+			(sg_iter.sg_pgoffset << PAGE_SHIFT);
+		gen8_set_pte(&gtt_entries[i],
+			     gen8_pte_encode(addr, level, true));
+		i++;
+	}
+
+	/*
+	 * XXX: This serves as a posting read to make sure that the PTE has
+	 * actually been updated. There is some concern that even though
+	 * registers and PTEs are within the same BAR that they are potentially
+	 * of NUMA access patterns. Therefore, even with the way we assume
+	 * hardware should work, we must keep this posting read for paranoia.
+	 */
+	if (i != 0)
+		WARN_ON(readq(&gtt_entries[i-1])
+			!= gen8_pte_encode(addr, level, true));
+
+	/* This next bit makes the above posting read even more important. We
+	 * want to flush the TLBs only after we're certain all the PTE updates
+	 * have finished.
+	 */
+	I915_WRITE(GFX_FLSH_CNTL_GEN6, GFX_FLSH_CNTL_EN);
+	POSTING_READ(GFX_FLSH_CNTL_GEN6);
+
+	return 0;
+}
+
+/*
+ * Binds an object into the global gtt with the specified cache level. The object
+ * will be accessible to the GPU via commands whose operands reference offsets
+ * within the global GTT as well as accessible by the GPU through the GMADR
+ * mapped BAR (dev_priv->mm.gtt->gtt).
+ */
+static int gen6_ggtt_insert_entries(struct i915_address_space *vm,
+				    struct sg_table *st,
+				    uint64_t start,
+				    enum i915_cache_level level, u32 flags)
+{
+	struct drm_i915_private *dev_priv = vm->dev->dev_private;
+	unsigned first_entry = start >> PAGE_SHIFT;
+	gen6_gtt_pte_t __iomem *gtt_entries =
+		(gen6_gtt_pte_t __iomem *)dev_priv->gtt.gsm + first_entry;
+	int i = 0;
+	struct sg_page_iter sg_iter;
+	dma_addr_t addr = 0;
+
+	for_each_sg_page(st->sgl, &sg_iter, st->nents, 0) {
+		addr = sg_page_iter_dma_address(&sg_iter);
+		iowrite32(vm->pte_encode(addr, level, true, flags), &gtt_entries[i]);
+		i++;
+	}
+
+	/* XXX: This serves as a posting read to make sure that the PTE has
+	 * actually been updated. There is some concern that even though
+	 * registers and PTEs are within the same BAR that they are potentially
+	 * of NUMA access patterns. Therefore, even with the way we assume
+	 * hardware should work, we must keep this posting read for paranoia.
+	 */
+	if (i != 0) {
+		unsigned long gtt = readl(&gtt_entries[i-1]);
+		WARN_ON(gtt != vm->pte_encode(addr, level, true, flags));
+	}
+
+	/* This next bit makes the above posting read even more important. We
+	 * want to flush the TLBs only after we're certain all the PTE updates
+	 * have finished.
+	 */
+	I915_WRITE(GFX_FLSH_CNTL_GEN6, GFX_FLSH_CNTL_EN);
+	POSTING_READ(GFX_FLSH_CNTL_GEN6);
+
+	return 0;
+}
+
+static int gen8_ggtt_clear_range(struct i915_address_space *vm,
+				 uint64_t start,
+				 uint64_t length,
+				 bool use_scratch)
+{
+	struct drm_i915_private *dev_priv = vm->dev->dev_private;
+	unsigned first_entry = start >> PAGE_SHIFT;
+	unsigned num_entries = length >> PAGE_SHIFT;
+	gen8_gtt_pte_t scratch_pte, __iomem *gtt_base =
+		(gen8_gtt_pte_t __iomem *) dev_priv->gtt.gsm + first_entry;
+	const int max_entries = gtt_total_entries(dev_priv->gtt) - first_entry;
+	int i;
+
+	if (WARN(num_entries > max_entries,
+		 "First entry = %d; Num entries = %d (max=%d)\n",
+		 first_entry, num_entries, max_entries))
+		num_entries = max_entries;
+
+	scratch_pte = gen8_pte_encode(vm->scratch.addr,
+				      I915_CACHE_LLC,
+				      use_scratch);
+	for (i = 0; i < num_entries; i++)
+		gen8_set_pte(&gtt_base[i], scratch_pte);
+	readl(gtt_base);
+
+	return 0;
+}
+
+static int gen6_ggtt_clear_range(struct i915_address_space *vm,
+				 uint64_t start,
+				 uint64_t length,
+				 bool use_scratch)
+{
+	struct drm_i915_private *dev_priv = vm->dev->dev_private;
+	unsigned first_entry = start >> PAGE_SHIFT;
+	unsigned num_entries = length >> PAGE_SHIFT;
+	gen6_gtt_pte_t scratch_pte, __iomem *gtt_base =
+		(gen6_gtt_pte_t __iomem *) dev_priv->gtt.gsm + first_entry;
+	const int max_entries = gtt_total_entries(dev_priv->gtt) - first_entry;
+	int i;
+
+	if (WARN(num_entries > max_entries,
+		 "First entry = %d; Num entries = %d (max=%d)\n",
+		 first_entry, num_entries, max_entries))
+		num_entries = max_entries;
+
+	scratch_pte = vm->pte_encode(vm->scratch.addr, I915_CACHE_LLC, use_scratch, 0);
+
+	for (i = 0; i < num_entries; i++)
+		iowrite32(scratch_pte, &gtt_base[i]);
+	readl(gtt_base);
+
+	return 0;
+}
+
+static void mark_global_obj(struct i915_vma *vma)
+{
+	struct drm_i915_gem_object *obj = vma->obj;
+	bool mappable, fenceable;
+	u32 fence_size, fence_alignment;
+
+	fence_size = i915_gem_get_gtt_size(obj->base.dev,
+					   obj->base.size,
+					   obj->tiling_mode);
+	fence_alignment = i915_gem_get_gtt_alignment(obj->base.dev,
+						     obj->base.size,
+						     obj->tiling_mode,
+						     true);
+
+	fenceable = (vma->node.size == fence_size &&
+		     (vma->node.start & (fence_alignment - 1)) == 0);
+
+	mappable = (vma->node.start + obj->base.size <=
+		    to_i915(obj->base.dev)->gtt.mappable_end);
+
+	obj->map_and_fenceable = mappable && fenceable;
+}
+
+static int i915_ggtt_bind_vma(struct i915_vma *vma,
+			      enum i915_cache_level cache_level,
+			      u32 unused)
+{
+	const unsigned long entry = vma->node.start >> PAGE_SHIFT;
+	unsigned int flags = (cache_level == I915_CACHE_NONE) ?
+		AGP_USER_MEMORY : AGP_USER_CACHED_MEMORY;
+
+	if (vma->bound && (flags & REBIND) == 0)
+		return 0;
+
+	BUG_ON(!i915_is_ggtt(vma->vm));
+	intel_gtt_insert_sg_entries(vma->obj->pages, entry, flags);
+	vma->bound = GLOBAL_BIND;
+	vma->vm->dirty = true;
+
+	mark_global_obj(vma);
+
+	return 0;
+}
+
+static int i915_ggtt_clear_range(struct i915_address_space *vm,
+				 uint64_t start,
+				 uint64_t length,
+				 bool unused)
+{
+	unsigned first_entry = start >> PAGE_SHIFT;
+	unsigned num_entries = length >> PAGE_SHIFT;
+
+	intel_gtt_clear_range(first_entry, num_entries);
+	return 0;
+}
+
+static int nop_clear_range(struct i915_address_space *vm,
+			   uint64_t start,
+			   uint64_t length,
+			   bool use_scratch)
+{
+	return 0;
+}
+
+static int i915_ggtt_unbind_vma(struct i915_vma *vma)
+{
+	struct drm_i915_gem_object *obj = vma->obj;
+	const unsigned int first = vma->node.start >> PAGE_SHIFT;
+	const unsigned int size = obj->base.size >> PAGE_SHIFT;
+	int ret;
+
+	BUG_ON(!i915_is_ggtt(vma->vm));
+	BUG_ON(vma->bound == 0);
+
+	i915_gem_object_finish_gtt(obj);
+
+	/* release the fence reg _after_ flushing */
+	ret = i915_gem_object_put_fence(obj);
+	if (WARN_ON(ret)) /* should be idle already */
+		return ret;
+
+	obj->map_and_fenceable = false;
+
+	intel_gtt_clear_range(first, size);
+	vma->bound = 0;
+
+	return 0;
+}
+
+static int ggtt_bind_vma(struct i915_vma *vma,
+			 enum i915_cache_level cache_level,
+			 u32 flags)
+{
+	struct drm_device *dev = vma->vm->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_gem_object *obj = vma->obj;
+	int ret;
+
+	if (flags == REBIND) {
+		flags = vma->bound;
+		vma->bound = 0;
+	}
+
+	if (!dev_priv->mm.aliasing_ppgtt) {
+		flags |= GLOBAL_BIND;
+		flags &= ~LOCAL_BIND;
+	}
+
+	/* Currently applicable only to VLV */
+	if (obj->gt_ro)
+		flags |= PTE_READ_ONLY;
+
+	/* If there is no aliasing PPGTT, or the caller needs a global mapping,
+	 * or we have a global mapping already but the cacheability flags have
+	 * changed, set the global PTEs.
+	 *
+	 * If there is an aliasing PPGTT it is anecdotally faster, so use that
+	 * instead if none of the above hold true.
+	 *
+	 * NB: A global mapping should only be needed for special regions like
+	 * "gtt mappable", SNB errata, or if specified via special execbuf
+	 * flags. At all other times, the GPU will use the aliasing PPGTT.
+	 */
+	if ((flags & ~vma->bound) & GLOBAL_BIND) {
+		ret = vma->vm->insert_entries(vma->vm, obj->pages,
+					      vma->node.start,
+					      cache_level, flags);
+		if (ret)
+			return ret;
+
+		vma->bound |= GLOBAL_BIND;
+		vma->vm->dirty = true;
+		mark_global_obj(vma);
+	}
+
+	if ((flags & ~vma->bound) & LOCAL_BIND) {
+		struct i915_hw_ppgtt *appgtt = dev_priv->mm.aliasing_ppgtt;
+
+		ret = appgtt->base.insert_entries(&appgtt->base,
+						  obj->pages,
+						  vma->node.start,
+						  cache_level, flags);
+		if (ret)
+			return ret;
+
+		vma->bound |= LOCAL_BIND;
+		vma->vm->dirty = true;
+	}
+
+	vma->bound |= flags;
+	return 0;
+}
+
+static int ggtt_unbind_vma(struct i915_vma *vma)
+{
+	struct drm_device *dev = vma->vm->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_gem_object *obj = vma->obj;
+	int ret;
+
+	if (vma->bound & GLOBAL_BIND) {
+		i915_gem_object_finish_gtt(obj);
+
+		/* release the fence reg _after_ flushing */
+		ret = i915_gem_object_put_fence(obj);
+		if (WARN_ON(ret)) /* should be idle already */
+			return ret;
+
+		obj->map_and_fenceable = false;
+
+		vma->vm->clear_range(vma->vm,
+				     vma->node.start,
+				     obj->base.size,
+				     true);
+	}
+
+	if (vma->bound & LOCAL_BIND) {
+		struct i915_hw_ppgtt *appgtt = dev_priv->mm.aliasing_ppgtt;
+		appgtt->base.clear_range(&appgtt->base,
+					 vma->node.start,
+					 obj->base.size,
+					 true);
+	}
+
+	vma->bound = 0;
+	return 0;
+}
+
+void i915_gem_gtt_finish_object(struct drm_i915_gem_object *obj)
+{
+	struct drm_device *dev = obj->base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	bool interruptible;
+
+	interruptible = do_idling(dev_priv);
+
+	if (!obj->has_dma_mapping)
+		dma_unmap_sg(&dev->pdev->dev,
+			     obj->pages->sgl, obj->pages->nents,
+			     PCI_DMA_BIDIRECTIONAL);
+
+	undo_idling(dev_priv, interruptible);
+}
+
+static void i915_gtt_color_adjust(struct drm_mm_node *node,
+				  unsigned long color,
+				  unsigned long *start,
+				  unsigned long *end)
+{
+	if (node->color != color)
+		*start += 4096;
+
+	if (!list_empty(&node->node_list)) {
+		node = list_entry(node->node_list.next,
+				  struct drm_mm_node,
+				  node_list);
+		if (node->allocated && node->color != color)
+			*end -= 4096;
+	}
+}
+
+static int i915_gem_setup_global_gtt(struct drm_device *dev,
+				     unsigned long start,
+				     unsigned long mappable_end,
+				     unsigned long end)
+{
+	/* Let GEM Manage all of the aperture.
+	 *
+	 * However, leave one page at the end still bound to the scratch page.
+	 * There are a number of places where the hardware apparently prefetches
+	 * past the end of the object, and we've seen multiple hangs with the
+	 * GPU head pointer stuck in a batchbuffer bound at the last page of the
+	 * aperture.  One page should be enough to keep any prefetching inside
+	 * of the aperture.
+	 */
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct i915_address_space *ggtt = &dev_priv->gtt.base;
+	struct drm_mm_node *entry;
+	struct drm_i915_gem_object *obj;
+	unsigned long hole_start, hole_end;
+	int ret;
+
+	BUG_ON(mappable_end > end);
+
+	/* Subtract the guard page ... */
+	drm_mm_init(&ggtt->mm, start, end - start - PAGE_SIZE);
+	if (!HAS_LLC(dev))
+		dev_priv->gtt.base.mm.color_adjust = i915_gtt_color_adjust;
+
+	/* Mark any preallocated objects as occupied */
+	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list) {
+		struct i915_vma *vma = i915_gem_obj_to_ggtt(obj);
+
+		WARN_ON(drm_mm_node_allocated(&vma->node));
+
+		DRM_DEBUG_KMS("reserving preallocated space: %lx + %zx\n",
+			      vma->node.start, obj->base.size);
+		ret = drm_mm_reserve_node(&ggtt->mm, &vma->node);
+		if (ret) {
+			DRM_DEBUG_KMS("Reservation failed: %i\n", ret);
+			return ret;
+		}
+		vma->bound |= GLOBAL_BIND;
+		mark_global_obj(vma);
+	}
+
+	ggtt->start = start;
+	ggtt->total = end - start;
+
+	if (USES_PPGTT(dev) && !USES_FULL_PPGTT(dev)) {
+		struct i915_hw_ppgtt *ppgtt;
+
+		ppgtt = kzalloc(sizeof(*ppgtt), GFP_KERNEL);
+		if (!ppgtt)
+			return -ENOMEM;
+
+		ret = __hw_ppgtt_init(dev, ppgtt);
+		if (ret== 0 && ppgtt->state)
+			ret = i915_gem_object_ggtt_pin(ppgtt->state,
+						       ppgtt->alignment,
+						       0);
+		if (ret == 0)
+			dev_priv->mm.aliasing_ppgtt = ppgtt;
+	}
+
+	if (USES_FULL_PPGTT(dev) || dev_priv->mm.aliasing_ppgtt)
+		ggtt->clear_range = nop_clear_range;
+
+	/* Clear any non-preallocated blocks */
+	drm_mm_for_each_hole(entry, &ggtt->mm, hole_start, hole_end) {
+		DRM_DEBUG_KMS("clearing unused GTT space: [%lx, %lx]\n",
+			      hole_start, hole_end);
+		ggtt->clear_range(ggtt,
+				  hole_start, hole_end - hole_start,
+				  true);
+	}
+
+	/* And finally clear the reserved guard page */
+	ggtt->clear_range(ggtt, end - PAGE_SIZE, PAGE_SIZE, true);
+
+	return 0;
+}
+
+void i915_gem_init_global_gtt(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	unsigned long gtt_size, mappable_size;
+
+	gtt_size = dev_priv->gtt.base.total;
+	mappable_size = dev_priv->gtt.mappable_end;
+
+	i915_gem_setup_global_gtt(dev, 0, mappable_size, gtt_size);
+}
+
+void i915_global_gtt_cleanup(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct i915_address_space *vm = &dev_priv->gtt.base;
+
+	lockdep_assert_held(&dev->struct_mutex);
+
+	if (dev_priv->mm.aliasing_ppgtt) {
+		struct i915_hw_ppgtt *ppgtt = dev_priv->mm.aliasing_ppgtt;
+
+		ppgtt->base.cleanup(&ppgtt->base);
+	}
+
+	if (drm_mm_initialized(&vm->mm)) {
+		drm_mm_takedown(&vm->mm);
+		list_del(&vm->global_link);
+	}
+
+	vm->cleanup(vm);
+}
+
+static int setup_scratch_page(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct page *page;
+	dma_addr_t dma_addr;
+
+	page = alloc_page(GFP_KERNEL | GFP_DMA32 | __GFP_ZERO);
+	if (page == NULL)
+		return -ENOMEM;
+	set_pages_uc(page, 1);
+
+#ifdef CONFIG_INTEL_IOMMU
+	dma_addr = pci_map_page(dev->pdev, page, 0, PAGE_SIZE,
+				PCI_DMA_BIDIRECTIONAL);
+	if (pci_dma_mapping_error(dev->pdev, dma_addr))
+		return -EINVAL;
+#else
+	dma_addr = page_to_phys(page);
+#endif
+	dev_priv->gtt.base.scratch.page = page;
+	dev_priv->gtt.base.scratch.addr = dma_addr;
+
+	return 0;
+}
+
+static void teardown_scratch_page(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct page *page = dev_priv->gtt.base.scratch.page;
+
+	set_pages_wb(page, 1);
+	pci_unmap_page(dev->pdev, dev_priv->gtt.base.scratch.addr,
+		       PAGE_SIZE, PCI_DMA_BIDIRECTIONAL);
+	__free_page(page);
+}
+
+static inline unsigned int gen6_get_total_gtt_size(u16 snb_gmch_ctl)
+{
+	snb_gmch_ctl >>= SNB_GMCH_GGMS_SHIFT;
+	snb_gmch_ctl &= SNB_GMCH_GGMS_MASK;
+	return snb_gmch_ctl << 20;
+}
+
+static inline unsigned int gen8_get_total_gtt_size(u16 bdw_gmch_ctl)
+{
+	bdw_gmch_ctl >>= BDW_GMCH_GGMS_SHIFT;
+	bdw_gmch_ctl &= BDW_GMCH_GGMS_MASK;
+	if (bdw_gmch_ctl)
+		bdw_gmch_ctl = 1 << bdw_gmch_ctl;
+
+#ifdef CONFIG_X86_32
+	/* Limit 32b platforms to a 2GB GGTT: 4 << 20 / pte size * PAGE_SIZE */
+	if (bdw_gmch_ctl > 4)
+		bdw_gmch_ctl = 4;
+#endif
+
+	return bdw_gmch_ctl << 20;
+}
+
+static inline unsigned int chv_get_total_gtt_size(u16 gmch_ctrl)
+{
+	gmch_ctrl >>= SNB_GMCH_GGMS_SHIFT;
+	gmch_ctrl &= SNB_GMCH_GGMS_MASK;
+
+	if (gmch_ctrl)
+		return 1 << (20 + gmch_ctrl);
+
+	return 0;
+}
+
+static inline size_t gen6_get_stolen_size(u16 snb_gmch_ctl)
+{
+	snb_gmch_ctl >>= SNB_GMCH_GMS_SHIFT;
+	snb_gmch_ctl &= SNB_GMCH_GMS_MASK;
+	return snb_gmch_ctl << 25; /* 32 MB units */
+}
+
+static inline size_t gen8_get_stolen_size(u16 bdw_gmch_ctl)
+{
+	bdw_gmch_ctl >>= BDW_GMCH_GMS_SHIFT;
+	bdw_gmch_ctl &= BDW_GMCH_GMS_MASK;
+	return bdw_gmch_ctl << 25; /* 32 MB units */
+}
+
+static size_t chv_get_stolen_size(u16 gmch_ctrl)
+{
+	gmch_ctrl >>= SNB_GMCH_GMS_SHIFT;
+	gmch_ctrl &= SNB_GMCH_GMS_MASK;
+
+	/*
+	 * 0x0  to 0x10: 32MB increments starting at 0MB
+	 * 0x11 to 0x16: 4MB increments starting at 8MB
+	 * 0x17 to 0x1d: 4MB increments start at 36MB
+	 */
+	if (gmch_ctrl < 0x11)
+		return gmch_ctrl << 25;
+	else if (gmch_ctrl < 0x17)
+		return (gmch_ctrl - 0x11 + 2) << 22;
+	else
+		return (gmch_ctrl - 0x17 + 9) << 22;
+}
+
+static size_t gen9_get_stolen_size(u16 gen9_gmch_ctl)
+{
+	gen9_gmch_ctl >>= BDW_GMCH_GMS_SHIFT;
+	gen9_gmch_ctl &= BDW_GMCH_GMS_MASK;
+
+	if (gen9_gmch_ctl < 0xf0)
+		return gen9_gmch_ctl << 25; /* 32 MB units */
+	else
+		/* 4MB increments starting at 0xf0 for 4MB */
+		return (gen9_gmch_ctl - 0xf0 + 1) << 22;
+}
+
+static int ggtt_probe_common(struct drm_device *dev,
+			     size_t gtt_size)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	phys_addr_t gtt_phys_addr;
+	int ret;
+
+	/* For Modern GENs the PTEs and register space are split in the BAR */
+	gtt_phys_addr = pci_resource_start(dev->pdev, 0) +
+		(pci_resource_len(dev->pdev, 0) / 2);
+
+	dev_priv->gtt.gsm = ioremap_wc(gtt_phys_addr, gtt_size);
+	if (!dev_priv->gtt.gsm) {
+		DRM_ERROR("Failed to map the gtt page table\n");
+		return -ENOMEM;
+	}
+
+	ret = setup_scratch_page(dev);
+	if (ret) {
+		DRM_ERROR("Scratch setup failed\n");
+		/* iounmap will also get called at remove, but meh */
+		iounmap(dev_priv->gtt.gsm);
+	}
+
+	return ret;
+}
+
+/* The GGTT and PPGTT need a private PPAT setup in order to handle cacheability
+ * bits. When using advanced contexts each context stores its own PAT, but
+ * writing this data shouldn't be harmful even in those cases. */
+static void bdw_setup_private_ppat(struct drm_i915_private *dev_priv)
+{
+	uint64_t pat;
+
+	pat = GEN8_PPAT(0, GEN8_PPAT_WB | GEN8_PPAT_LLC)     | /* for normal objects, no eLLC */
+	      GEN8_PPAT(1, GEN8_PPAT_WC | GEN8_PPAT_LLCELLC) | /* for something pointing to ptes? */
+	      GEN8_PPAT(2, GEN8_PPAT_WT | GEN8_PPAT_LLCELLC) | /* for scanout with eLLC */
+	      GEN8_PPAT(3, GEN8_PPAT_UC)                     | /* Uncached objects, mostly for scanout */
+	      GEN8_PPAT(4, GEN8_PPAT_WB | GEN8_PPAT_LLCELLC | GEN8_PPAT_AGE(0)) |
+	      GEN8_PPAT(5, GEN8_PPAT_WB | GEN8_PPAT_LLCELLC | GEN8_PPAT_AGE(1)) |
+	      GEN8_PPAT(6, GEN8_PPAT_WB | GEN8_PPAT_LLCELLC | GEN8_PPAT_AGE(2)) |
+	      GEN8_PPAT(7, GEN8_PPAT_WB | GEN8_PPAT_LLCELLC | GEN8_PPAT_AGE(3));
+
+	if (!USES_PPGTT(dev_priv->dev))
+		/* Spec: "For GGTT, there is NO pat_sel[2:0] from the entry,
+		 * so RTL will always use the value corresponding to
+		 * pat_sel = 000".
+		 * So let's disable cache for GGTT to avoid screen corruptions.
+		 * MOCS still can be used though.
+		 * - System agent ggtt writes (i.e. cpu gtt mmaps) already work
+		 * before this patch, i.e. the same uncached + snooping access
+		 * like on gen6/7 seems to be in effect.
+		 * - So this just fixes blitter/render access. Again it looks
+		 * like it's not just uncached access, but uncached + snooping.
+		 * So we can still hold onto all our assumptions wrt cpu
+		 * clflushing on LLC machines.
+		 */
+		pat = GEN8_PPAT(0, GEN8_PPAT_UC);
+
+	/* XXX: spec defines this as 2 distinct registers. It's unclear if a 64b
+	 * write would work. */
+	I915_WRITE(GEN8_PRIVATE_PAT, pat);
+	I915_WRITE(GEN8_PRIVATE_PAT + 4, pat >> 32);
+}
+
+static void chv_setup_private_ppat(struct drm_i915_private *dev_priv)
+{
+	uint64_t pat;
+
+	/*
+	 * Map WB on BDW to snooped on CHV.
+	 *
+	 * Only the snoop bit has meaning for CHV, the rest is
+	 * ignored.
+	 *
+	 * The hardware will never snoop for certain types of accesses:
+	 * - CPU GTT (GMADR->GGTT->no snoop->memory)
+	 * - PPGTT page tables
+	 * - some other special cycles
+	 *
+	 * As with BDW, we also need to consider the following for GT accesses:
+	 * "For GGTT, there is NO pat_sel[2:0] from the entry,
+	 * so RTL will always use the value corresponding to
+	 * pat_sel = 000".
+	 * Which means we must set the snoop bit in PAT entry 0
+	 * in order to keep the global status page working.
+	 */
+	pat = GEN8_PPAT(0, CHV_PPAT_SNOOP) |
+	      GEN8_PPAT(1, 0) |
+	      GEN8_PPAT(2, 0) |
+	      GEN8_PPAT(3, 0) |
+	      GEN8_PPAT(4, CHV_PPAT_SNOOP) |
+	      GEN8_PPAT(5, CHV_PPAT_SNOOP) |
+	      GEN8_PPAT(6, CHV_PPAT_SNOOP) |
+	      GEN8_PPAT(7, CHV_PPAT_SNOOP);
+
+	I915_WRITE(GEN8_PRIVATE_PAT, pat);
+	I915_WRITE(GEN8_PRIVATE_PAT + 4, pat >> 32);
+}
+
+static int gen8_gmch_probe(struct drm_device *dev,
+			   size_t *gtt_total,
+			   size_t *stolen,
+			   phys_addr_t *mappable_base,
+			   unsigned long *mappable_end)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	unsigned int gtt_size;
+	u16 snb_gmch_ctl;
+	int ret;
+
+	/* TODO: We're not aware of mappable constraints on gen8 yet */
+	*mappable_base = pci_resource_start(dev->pdev, 2);
+	*mappable_end = pci_resource_len(dev->pdev, 2);
+
+	if (!pci_set_dma_mask(dev->pdev, DMA_BIT_MASK(39)))
+		pci_set_consistent_dma_mask(dev->pdev, DMA_BIT_MASK(39));
+
+	pci_read_config_word(dev->pdev, SNB_GMCH_CTRL, &snb_gmch_ctl);
+
+	if (INTEL_INFO(dev)->gen >= 9) {
+		*stolen = gen9_get_stolen_size(snb_gmch_ctl);
+		gtt_size = gen8_get_total_gtt_size(snb_gmch_ctl);
+	} else if (IS_CHERRYVIEW(dev)) {
+		*stolen = chv_get_stolen_size(snb_gmch_ctl);
+		gtt_size = chv_get_total_gtt_size(snb_gmch_ctl);
+	} else {
+		*stolen = gen8_get_stolen_size(snb_gmch_ctl);
+		gtt_size = gen8_get_total_gtt_size(snb_gmch_ctl);
+	}
+
+	*gtt_total = (gtt_size / sizeof(gen8_gtt_pte_t)) << PAGE_SHIFT;
+
+	if (IS_CHERRYVIEW(dev))
+		chv_setup_private_ppat(dev_priv);
+	else
+		bdw_setup_private_ppat(dev_priv);
+
+	ret = ggtt_probe_common(dev, gtt_size);
+
+	dev_priv->gtt.base.clear_range = gen8_ggtt_clear_range;
+	dev_priv->gtt.base.insert_entries = gen8_ggtt_insert_entries;
+
+	return ret;
+}
+
+static int gen6_gmch_probe(struct drm_device *dev,
+			   size_t *gtt_total,
+			   size_t *stolen,
+			   phys_addr_t *mappable_base,
+			   unsigned long *mappable_end)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	unsigned int gtt_size;
+	u16 snb_gmch_ctl;
+	int ret;
+
+	*mappable_base = pci_resource_start(dev->pdev, 2);
+	*mappable_end = pci_resource_len(dev->pdev, 2);
+
+	/* 64/512MB is the current min/max we actually know of, but this is just
+	 * a coarse sanity check.
+	 */
+	if ((*mappable_end < (64<<20) || (*mappable_end > (512<<20)))) {
+		DRM_ERROR("Unknown GMADR size (%lx)\n",
+			  dev_priv->gtt.mappable_end);
+		return -ENXIO;
+	}
+
+	if (!pci_set_dma_mask(dev->pdev, DMA_BIT_MASK(40)))
+		pci_set_consistent_dma_mask(dev->pdev, DMA_BIT_MASK(40));
+	pci_read_config_word(dev->pdev, SNB_GMCH_CTRL, &snb_gmch_ctl);
+
+	*stolen = gen6_get_stolen_size(snb_gmch_ctl);
+
+	gtt_size = gen6_get_total_gtt_size(snb_gmch_ctl);
+	*gtt_total = (gtt_size / sizeof(gen6_gtt_pte_t)) << PAGE_SHIFT;
+
+	ret = ggtt_probe_common(dev, gtt_size);
+
+	dev_priv->gtt.base.clear_range = gen6_ggtt_clear_range;
+	dev_priv->gtt.base.insert_entries = gen6_ggtt_insert_entries;
+
+	return ret;
+}
+
+static void gen6_gmch_remove(struct i915_address_space *vm)
+{
+
+	struct i915_gtt *gtt = container_of(vm, struct i915_gtt, base);
+
+	iounmap(gtt->gsm);
+	teardown_scratch_page(vm->dev);
+}
+
+static int i915_gmch_probe(struct drm_device *dev,
+			   size_t *gtt_total,
+			   size_t *stolen,
+			   phys_addr_t *mappable_base,
+			   unsigned long *mappable_end)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	int ret;
+
+	ret = intel_gmch_probe(dev_priv->bridge_dev, dev_priv->dev->pdev, NULL);
+	if (!ret) {
+		DRM_ERROR("failed to set up gmch\n");
+		return -EIO;
+	}
+
+	intel_gtt_get(gtt_total, stolen, mappable_base, mappable_end);
+
+	dev_priv->gtt.do_idle_maps = needs_idle_maps(dev_priv->dev);
+	dev_priv->gtt.base.clear_range = i915_ggtt_clear_range;
+
+	if (unlikely(dev_priv->gtt.do_idle_maps))
+		DRM_INFO("applying Ironlake quirks for intel_iommu\n");
+
+	return 0;
+}
+
+static void i915_gmch_remove(struct i915_address_space *vm)
+{
+	intel_gmch_remove();
+}
+
+int i915_gem_gtt_init(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct i915_gtt *gtt = &dev_priv->gtt;
+	int ret;
+
+	INIT_LIST_HEAD(&gtt->base.vma_list);
+	kref_init(&gtt->base.ref);
+
+	if (INTEL_INFO(dev)->gen <= 5) {
+		gtt->gtt_probe = i915_gmch_probe;
+		gtt->base.cleanup = i915_gmch_remove;
+	} else if (INTEL_INFO(dev)->gen < 8) {
+		gtt->gtt_probe = gen6_gmch_probe;
+		gtt->base.cleanup = gen6_gmch_remove;
+		if (IS_HASWELL(dev) && dev_priv->ellc_size)
+			gtt->base.pte_encode = iris_pte_encode;
+		else if (IS_HASWELL(dev))
+			gtt->base.pte_encode = hsw_pte_encode;
+		else if (IS_VALLEYVIEW(dev))
+			gtt->base.pte_encode = byt_pte_encode;
+		else if (INTEL_INFO(dev)->gen >= 7)
+			gtt->base.pte_encode = ivb_pte_encode;
+		else
+			gtt->base.pte_encode = snb_pte_encode;
+	} else {
+		dev_priv->gtt.gtt_probe = gen8_gmch_probe;
+		dev_priv->gtt.base.cleanup = gen6_gmch_remove;
+	}
+
+	ret = gtt->gtt_probe(dev, &gtt->base.total, &gtt->stolen_size,
+			     &gtt->mappable_base, &gtt->mappable_end);
+	if (ret)
+		return ret;
+
+	gtt->base.dev = dev;
+
+	/* GMADR is the PCI mmio aperture into the global GTT. */
+	DRM_INFO("Memory usable by graphics device = %zdM\n",
+		 gtt->base.total >> 20);
+	DRM_DEBUG_DRIVER("GMADR size = %ldM\n", gtt->mappable_end >> 20);
+	DRM_DEBUG_DRIVER("GTT stolen size = %zdM\n", gtt->stolen_size >> 20);
+#ifdef CONFIG_INTEL_IOMMU
+	if (intel_iommu_gfx_mapped)
+		DRM_INFO("VT-d active for gfx access\n");
+#endif
+	/*
+	 * i915.enable_ppgtt is read-only, so do an early pass to validate the
+	 * user's requested state against the hardware/driver capabilities.  We
+	 * do this now so that we can print out any log messages once rather
+	 * than every time we check intel_enable_ppgtt().
+	 */
+	i915_module.enable_ppgtt =
+	       	sanitize_enable_ppgtt(dev, i915_module.enable_ppgtt);
+	DRM_DEBUG_DRIVER("ppgtt mode: %i\n", i915_module.enable_ppgtt);
+
+	return 0;
+}
+
+static struct i915_vma *__i915_vma_create(struct drm_i915_gem_object *obj,
+					  struct i915_address_space *vm)
+{
+	struct i915_vma *vma;
+	int n;
+
+	vma = kzalloc(sizeof(*vma), GFP_KERNEL);
+	if (vma == NULL)
+		return ERR_PTR(-ENOMEM);
+
+	kref_init(&vma->kref);
+	vma->vm = i915_vm_get(vm);
+	vma->obj = obj;
+	list_add(&vma->vm_link, &vm->vma_list);
+
+	for (n = 0; n < I915_NUM_ENGINES; n++)
+		INIT_LIST_HEAD(&vma->last_read[n].engine_link);
+
+	INIT_LIST_HEAD(&vma->mm_list);
+	INIT_LIST_HEAD(&vma->exec_link);
+
+	switch (INTEL_INFO(vm->dev)->gen) {
+	case 9:
+	case 8:
+	case 7:
+	case 6:
+		if (obj->pde) {
+			if (WARN_ON(!i915_is_ggtt(vm)))
+				return ERR_PTR(-EINVAL);
+			vma->unbind_vma = pde_unbind_vma;
+			vma->bind_vma = pde_bind_vma;
+		} else if (i915_is_ggtt(vm)) {
+			vma->unbind_vma = ggtt_unbind_vma;
+			vma->bind_vma = ggtt_bind_vma;
+		} else {
+			vma->unbind_vma = ppgtt_unbind_vma;
+			vma->bind_vma = ppgtt_bind_vma;
+		}
+		break;
+	case 5:
+	case 4:
+	case 3:
+	case 2:
+		BUG_ON(!i915_is_ggtt(vm));
+		vma->unbind_vma = i915_ggtt_unbind_vma;
+		vma->bind_vma = i915_ggtt_bind_vma;
+		break;
+	default:
+		BUG();
+	}
+
+	/* Keep GGTT vmas first to make debug easier */
+	if (i915_is_ggtt(vm))
+		list_add(&vma->obj_link, &obj->vma_list);
+	else
+		list_add_tail(&vma->obj_link, &obj->vma_list);
+
+	return vma;
+}
+
+void __i915_vma_free(struct kref *kref)
+{
+	struct i915_vma *vma = container_of(kref, typeof(*vma), kref);
+
+	WARN_ON(drm_mm_node_allocated(&vma->node));
+	WARN_ON(vma->bound);
+	WARN_ON(!list_empty(&vma->mm_list));
+	WARN_ON(!list_empty(&vma->exec_link));
+	WARN_ON(!list_empty(&vma->obj_link));
+
+	list_del(&vma->vm_link);
+	i915_vm_put(vma->vm);
+	kfree(vma);
+}
+
+struct i915_vma *
+i915_gem_obj_get_vma(struct drm_i915_gem_object *obj,
+		     struct i915_address_space *vm)
+{
+	struct i915_vma *vma;
+
+	vma = i915_gem_obj_to_vma(obj, vm);
+	if (vma == NULL)
+		vma = __i915_vma_create(obj, vm);
+
+	return i915_vma_get(vma);
+}
diff -urN a/drivers/gpu/drm/i915/i915_gem_gtt.h b/drivers/gpu/drm/i915/i915_gem_gtt.h
--- a/drivers/gpu/drm/i915/i915_gem_gtt.h	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_gem_gtt.h	2014-11-22 14:37:49.334700418 -0700
@@ -117,17 +117,20 @@
  * will always be <= an objects lifetime. So object refcounting should cover us.
  */
 struct i915_vma {
+	struct kref kref;
 	struct drm_mm_node node;
 	struct drm_i915_gem_object *obj;
 	struct i915_address_space *vm;
 
+	struct list_head vm_link;
+
 	/** This object's place on the active/inactive lists */
 	struct list_head mm_list;
 
-	struct list_head vma_link; /* Link in the object's VMA list */
+	struct list_head obj_link; /* Link in the object's VMA list */
 
 	/** This vma's place in the batchbuffer or on the eviction list */
-	struct list_head exec_list;
+	struct list_head exec_link;
 
 	/**
 	 * Used for performing relocations during execbuffer insertion.
@@ -136,43 +139,55 @@
 	unsigned long exec_handle;
 	struct drm_i915_gem_exec_object2 *exec_entry;
 
-	/**
-	 * How many users have pinned this object in GTT space. The following
-	 * users can each hold at most one reference: pwrite/pread, pin_ioctl
-	 * (via user_pin_count), execbuffer (objects are not allowed multiple
-	 * times for the same batchbuffer), and the framebuffer code. When
-	 * switching/pageflipping, the framebuffer code has at most two buffers
-	 * pinned per crtc.
-	 *
-	 * In the worst case this is 1 + 1 + 1 + 2*2 = 7. That would fit into 3
-	 * bits with absolutely no headroom. So use 4 bits. */
-	unsigned int pin_count:4;
-#define DRM_I915_GEM_OBJECT_MAX_PIN_COUNT 0xf
+	struct {
+		struct i915_gem_request *request;
+		struct list_head engine_link;
+	} last_read[I915_NUM_ENGINES];
+
+	unsigned int pin_count;
+
+	/** Flags and address space this VMA is bound to */
+	unsigned int bound:4;
+#define LOCAL_BIND	(1<<0)
+#define GLOBAL_BIND	(1<<1)
+#define PTE_READ_ONLY	(1<<2)
+#define REBIND		(1<<3)
+	unsigned int active:I915_NUM_ENGINE_BITS;
+	unsigned int exec_read:1;
+	unsigned int exec_write:1;
+	unsigned int exec_fence:2;
 
 	/** Unmap an object from an address space. This usually consists of
 	 * setting the valid PTE entries to a reserved scratch page. */
-	void (*unbind_vma)(struct i915_vma *vma);
+	int (*unbind_vma)(struct i915_vma *vma);
 	/* Map an object into an address space with the given cache flags. */
-#define GLOBAL_BIND (1<<0)
-#define PTE_READ_ONLY (1<<1)
-	void (*bind_vma)(struct i915_vma *vma,
-			 enum i915_cache_level cache_level,
-			 u32 flags);
+	int (*bind_vma)(struct i915_vma *vma,
+			enum i915_cache_level cache_level,
+			unsigned flags);
 };
 
 struct i915_address_space {
+	struct kref ref;
 	struct drm_mm mm;
 	struct drm_device *dev;
 	struct list_head global_link;
 	unsigned long start;		/* Start offset always 0 for dri2 */
 	size_t total;		/* size addr space maps (ex. 2GB for ggtt) */
 
+	bool dirty;
+	bool closed;
+
 	struct {
 		dma_addr_t addr;
 		struct page *page;
 	} scratch;
 
 	/**
+	 * List of all allocated vma.
+	 */
+	struct list_head vma_list;
+
+	/**
 	 * List of objects currently involved in rendering.
 	 *
 	 * Includes buffers having the contents of their GPU caches
@@ -199,14 +214,15 @@
 	gen6_gtt_pte_t (*pte_encode)(dma_addr_t addr,
 				     enum i915_cache_level level,
 				     bool valid, u32 flags); /* Create a valid PTE */
-	void (*clear_range)(struct i915_address_space *vm,
-			    uint64_t start,
-			    uint64_t length,
-			    bool use_scratch);
-	void (*insert_entries)(struct i915_address_space *vm,
-			       struct sg_table *st,
-			       uint64_t start,
-			       enum i915_cache_level cache_level, u32 flags);
+	int (*clear_range)(struct i915_address_space *vm,
+			   uint64_t start,
+			   uint64_t length,
+			   bool use_scratch);
+	int (*insert_entries)(struct i915_address_space *vm,
+			      struct sg_table *st,
+			      uint64_t start,
+			      enum i915_cache_level cache_level,
+			      u32 flags);
 	void (*cleanup)(struct i915_address_space *vm);
 };
 
@@ -240,40 +256,65 @@
 
 struct i915_hw_ppgtt {
 	struct i915_address_space base;
-	struct kref ref;
-	struct drm_mm_node node;
+	struct drm_i915_gem_object *state;
+	unsigned alignment;
 	unsigned num_pd_entries;
 	unsigned num_pd_pages; /* gen8+ */
 	union {
-		struct page **pt_pages;
 		struct page **gen8_pt_pages[GEN8_LEGACY_PDPS];
 	};
 	struct page *pd_pages;
 	union {
-		uint32_t pd_offset;
 		dma_addr_t pd_dma_addr[GEN8_LEGACY_PDPS];
 	};
 	union {
-		dma_addr_t *pt_dma_addr;
 		dma_addr_t *gen8_pt_dma_addr[4];
 	};
 
-	struct intel_context *ctx;
-
 	int (*enable)(struct i915_hw_ppgtt *ppgtt);
-	int (*switch_mm)(struct i915_hw_ppgtt *ppgtt,
-			 struct intel_engine_cs *ring,
-			 bool synchronous);
+	int (*switch_mm)(struct i915_gem_request *rq,
+			 struct i915_hw_ppgtt *ppgtt);
 	void (*debug_dump)(struct i915_hw_ppgtt *ppgtt, struct seq_file *m);
 };
 
+void __i915_vma_free(struct kref *kref);
+
+static inline struct i915_vma *
+i915_vma_get(struct i915_vma *vma)
+{
+	if (vma)
+		kref_get(&vma->kref);
+	return vma;
+}
+
+static inline void
+i915_vma_put(struct i915_vma *vma)
+{
+	if (vma)
+		kref_put(&vma->kref, __i915_vma_free);
+}
+
 int i915_gem_gtt_init(struct drm_device *dev);
 void i915_gem_init_global_gtt(struct drm_device *dev);
-void i915_gem_setup_global_gtt(struct drm_device *dev, unsigned long start,
-			       unsigned long mappable_end, unsigned long end);
+void i915_global_gtt_cleanup(struct drm_device *dev);
+
 
-bool intel_enable_ppgtt(struct drm_device *dev, bool full);
-int i915_gem_init_ppgtt(struct drm_device *dev, struct i915_hw_ppgtt *ppgtt);
+int i915_ppgtt_init(struct drm_device *dev, struct i915_hw_ppgtt *ppgtt);
+int i915_ppgtt_init_hw(struct drm_device *dev);
+struct i915_hw_ppgtt *i915_ppgtt_create(struct drm_device *dev);
+void __i915_vm_free(struct kref *kref);
+static inline struct i915_address_space *
+i915_vm_get(struct i915_address_space *vm)
+{
+	if (vm)
+		kref_get(&vm->ref);
+	return vm;
+}
+static inline void i915_vm_put(struct i915_address_space *vm)
+{
+	if (vm)
+		kref_put(&vm->ref, __i915_vm_free);
+}
 
 void i915_check_and_clear_faults(struct drm_device *dev);
 void i915_gem_suspend_gtt_mappings(struct drm_device *dev);
diff -urN a/drivers/gpu/drm/i915/i915_gem_render_state.c b/drivers/gpu/drm/i915/i915_gem_render_state.c
--- a/drivers/gpu/drm/i915/i915_gem_render_state.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_gem_render_state.c	2014-11-22 14:37:49.334700418 -0700
@@ -33,10 +33,11 @@
 	struct drm_i915_gem_object *obj;
 	u64 ggtt_offset;
 	int gen;
+	unsigned batch_length;
 };
 
 static const struct intel_renderstate_rodata *
-render_state_get_rodata(struct drm_device *dev, const int gen)
+render_state_get_rodata(const int gen)
 {
 	switch (gen) {
 	case 6:
@@ -45,28 +46,30 @@
 		return &gen7_null_state;
 	case 8:
 		return &gen8_null_state;
+	case 9:
+		return &gen9_null_state;
 	}
 
 	return NULL;
 }
 
-static int render_state_init(struct render_state *so, struct drm_device *dev)
+static int render_state_init(struct render_state *so, struct i915_gem_request *rq)
 {
 	int ret;
 
-	so->gen = INTEL_INFO(dev)->gen;
-	so->rodata = render_state_get_rodata(dev, so->gen);
+	so->gen = INTEL_INFO(rq->i915)->gen;
+	so->rodata = render_state_get_rodata(so->gen);
 	if (so->rodata == NULL)
 		return 0;
 
 	if (so->rodata->batch_items * 4 > 4096)
 		return -EINVAL;
 
-	so->obj = i915_gem_alloc_object(dev, 4096);
+	so->obj = i915_gem_alloc_object(rq->i915->dev, 4096);
 	if (so->obj == NULL)
 		return -ENOMEM;
 
-	ret = i915_gem_obj_ggtt_pin(so->obj, 4096, 0);
+	ret = i915_gem_object_ggtt_pin(so->obj, 4096, 0);
 	if (ret)
 		goto free_gem;
 
@@ -93,9 +96,13 @@
 	page = sg_page(so->obj->pages->sgl);
 	d = kmap(page);
 
+	so->batch_length = 0;
 	while (i < rodata->batch_items) {
 		u32 s = rodata->batch[i];
 
+		if (so->batch_length == 0 && s == MI_BATCH_BUFFER_END)
+			so->batch_length = sizeof(u32) * ALIGN(i, 2);
+
 		if (i * 4  == rodata->reloc[reloc_index]) {
 			u64 r = s + so->ggtt_offset;
 			s = lower_32_bits(r);
@@ -115,10 +122,6 @@
 	}
 	kunmap(page);
 
-	ret = i915_gem_object_set_to_gtt_domain(so->obj, false);
-	if (ret)
-		return ret;
-
 	if (rodata->reloc[reloc_index] != -1) {
 		DRM_ERROR("only %d relocs resolved\n", reloc_index);
 		return -EINVAL;
@@ -133,15 +136,15 @@
 	drm_gem_object_unreference(&so->obj->base);
 }
 
-int i915_gem_render_state_init(struct intel_engine_cs *ring)
+int i915_gem_render_state_init(struct i915_gem_request *rq)
 {
 	struct render_state so;
 	int ret;
 
-	if (WARN_ON(ring->id != RCS))
+	if (WARN_ON(rq->engine->id != RCS))
 		return -ENOENT;
 
-	ret = render_state_init(&so, ring->dev);
+	ret = render_state_init(&so, rq);
 	if (ret)
 		return ret;
 
@@ -152,16 +155,20 @@
 	if (ret)
 		goto out;
 
-	ret = ring->dispatch_execbuffer(ring,
-					so.ggtt_offset,
-					so.rodata->batch_items * 4,
-					I915_DISPATCH_SECURE);
+	if (i915_gem_clflush_object(so.obj, false))
+		i915_gem_chipset_flush(rq->i915->dev);
+
+	ret = i915_request_emit_batchbuffer(rq, NULL,
+					    so.ggtt_offset,
+					    so.batch_length,
+					    I915_DISPATCH_SECURE);
 	if (ret)
 		goto out;
 
-	i915_vma_move_to_active(i915_gem_obj_to_ggtt(so.obj), ring);
+	so.obj->base.pending_read_domains = I915_GEM_DOMAIN_COMMAND;
+	drm_gem_object_reference(&so.obj->base);
+	i915_request_add_vma(rq, i915_gem_obj_get_ggtt(so.obj), 0);
 
-	ret = __i915_add_request(ring, NULL, so.obj, NULL);
 	/* __i915_add_request moves object to inactive if it fails */
 out:
 	render_state_fini(&so);
diff -urN a/drivers/gpu/drm/i915/i915_gem_request.c b/drivers/gpu/drm/i915/i915_gem_request.c
--- a/drivers/gpu/drm/i915/i915_gem_request.c	1969-12-31 17:00:00.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_gem_request.c	2014-11-22 14:37:49.334700418 -0700
@@ -0,0 +1,733 @@
+/*
+ * Copyright  2014 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ *
+ */
+
+#include <drm/drmP.h>
+#include "i915_drv.h"
+#include <drm/i915_drm.h>
+#include "i915_trace.h"
+#include "intel_drv.h"
+
+static bool check_reset(struct i915_gem_request *rq)
+{
+	unsigned reset = atomic_read(&rq->i915->gpu_error.reset_counter);
+	return likely(reset == rq->reset_counter);
+}
+
+static u32
+next_seqno(struct drm_i915_private *i915)
+{
+	/* reserve 0 for non-seqno */
+	if (++i915->next_seqno == 0)
+		++i915->next_seqno;
+	return i915->next_seqno;
+}
+
+struct i915_gem_request *
+i915_request_create(struct intel_context *ctx,
+		    struct intel_engine_cs *engine)
+{
+	struct intel_ringbuffer *ring;
+	struct i915_gem_request *rq;
+	int ret, n;
+
+	lockdep_assert_held(&engine->i915->dev->struct_mutex);
+
+	/* Pin first in case we need to recurse */
+	ring = engine->pin_context(engine, ctx);
+	if (IS_ERR(ring))
+		return ERR_CAST(ring);
+
+	rq = kzalloc(sizeof(*rq), GFP_KERNEL);
+	if (rq == NULL) {
+		ret = -ENOMEM;
+		goto err;
+	}
+
+	kref_init(&rq->kref);
+	INIT_LIST_HEAD(&rq->vmas);
+	INIT_LIST_HEAD(&rq->breadcrumb_link);
+
+	rq->i915 = engine->i915;
+	rq->ring = ring;
+	rq->engine = engine;
+
+	rq->reset_counter = atomic_read(&rq->i915->gpu_error.reset_counter);
+	if (rq->reset_counter & (I915_RESET_IN_PROGRESS_FLAG | I915_WEDGED)) {
+		ret = rq->reset_counter & I915_WEDGED ? -EIO : -EAGAIN;
+		goto err_rq;
+	}
+
+	rq->seqno = next_seqno(rq->i915);
+	memcpy(rq->semaphore, engine->semaphore.sync, sizeof(rq->semaphore));
+	for (n = 0; n < ARRAY_SIZE(rq->semaphore); n++)
+		if (__i915_seqno_passed(rq->semaphore[n], rq->seqno))
+			rq->semaphore[n] = 0;
+	rq->head = ring->tail;
+	rq->outstanding = true;
+	rq->pending_flush = ring->pending_flush;
+
+	rq->ctx = ctx;
+	i915_gem_context_reference(rq->ctx);
+
+	ret = i915_request_switch_context(rq);
+	if (ret)
+		goto err_ctx;
+
+	return rq;
+
+err_ctx:
+	i915_gem_context_unreference(ctx);
+err_rq:
+	kfree(rq);
+err:
+	engine->unpin_context(engine, ctx);
+	return ERR_PTR(ret);
+}
+
+void
+i915_request_add_vma(struct i915_gem_request *rq,
+		     struct i915_vma *vma,
+		     unsigned fenced)
+{
+	struct drm_i915_gem_object *obj = vma->obj;
+	u32 old_read = obj->base.read_domains;
+	u32 old_write = obj->base.write_domain;
+
+	lockdep_assert_held(&rq->i915->dev->struct_mutex);
+	BUG_ON(!drm_mm_node_allocated(&vma->node));
+
+	obj->base.write_domain = obj->base.pending_write_domain;
+	if (obj->base.write_domain == 0)
+		obj->base.pending_read_domains |= obj->base.read_domains;
+	obj->base.read_domains = obj->base.pending_read_domains;
+
+	obj->base.pending_read_domains = 0;
+	obj->base.pending_write_domain = 0;
+
+	trace_i915_gem_object_change_domain(obj, old_read, old_write);
+	list_move_tail(&vma->exec_link, &rq->vmas);
+
+	if (obj->base.read_domains) {
+		vma->exec_read = 1;
+		vma->exec_fence = fenced;
+		vma->exec_write = !!(obj->base.write_domain & I915_GEM_GPU_DOMAINS);
+
+		if (vma->exec_write) {
+			rq->pending_flush |= I915_FLUSH_CACHES;
+			intel_fb_obj_invalidate(obj, rq);
+		}
+	} else
+		vma->exec_read = 0;
+
+	/* update for the implicit flush after the rq */
+	obj->base.write_domain &= ~I915_GEM_GPU_DOMAINS;
+}
+
+int
+i915_request_emit_flush(struct i915_gem_request *rq,
+			unsigned flags)
+{
+	struct intel_engine_cs *engine = rq->engine;
+	int ret;
+
+	lockdep_assert_held(&rq->i915->dev->struct_mutex);
+
+	if ((flags & rq->pending_flush) == 0)
+		return 0;
+
+	trace_i915_gem_request_emit_flush(rq);
+	ret = engine->emit_flush(rq, rq->pending_flush);
+	if (ret)
+		return ret;
+
+	rq->pending_flush = 0;
+	return 0;
+}
+
+int
+__i915_request_emit_breadcrumb(struct i915_gem_request *rq, int id)
+{
+	struct intel_engine_cs *engine = rq->engine;
+	u32 seqno;
+	int ret;
+
+	lockdep_assert_held(&rq->i915->dev->struct_mutex);
+
+	if (rq->breadcrumb[id])
+		return 0;
+
+	if (rq->outstanding) {
+		ret = i915_request_emit_flush(rq, I915_FLUSH_CACHES | I915_COMMAND_BARRIER);
+		if (ret)
+			return ret;
+
+		trace_i915_gem_request_emit_breadcrumb(rq);
+		if (id == engine->id)
+			ret = engine->emit_breadcrumb(rq);
+		else
+			ret = engine->semaphore.signal(rq, id);
+		if (ret)
+			return ret;
+
+		seqno = rq->seqno;
+	} else if (engine->breadcrumb[id] == 0 ||
+		   __i915_seqno_passed(rq->seqno, engine->breadcrumb[id])) {
+		struct i915_gem_request *tmp;
+
+		tmp = i915_request_create(engine->last_context, engine);
+		if (IS_ERR(tmp))
+			return PTR_ERR(tmp);
+
+		/* Masquerade as a continuation of the earlier request */
+		tmp->reset_counter = rq->reset_counter;
+
+		ret = __i915_request_emit_breadcrumb(tmp, id);
+		if (ret == 0 && id != engine->id) {
+			/* semaphores are unstable across a wrap */
+			if (tmp->seqno < engine->breadcrumb[id])
+				ret = i915_request_wait(tmp);
+		}
+		if (ret == 0)
+			ret = i915_request_commit(tmp);
+
+		i915_request_put(tmp);
+		if (ret)
+			return ret;
+
+		seqno = tmp->seqno;
+	} else
+		seqno = engine->breadcrumb[id];
+
+	rq->breadcrumb[id] = seqno;
+	return 0;
+}
+
+int
+i915_request_emit_batchbuffer(struct i915_gem_request *rq,
+			      struct i915_vma *batch,
+			      uint64_t start, uint32_t len,
+			      unsigned flags)
+{
+	struct intel_engine_cs *engine = rq->engine;
+	int ret;
+
+	lockdep_assert_held(&rq->i915->dev->struct_mutex);
+
+	trace_i915_gem_request_emit_batch(rq);
+	rq->batch = batch;
+
+	ret = engine->emit_batchbuffer(rq, start, len, flags);
+	if (ret)
+		return ret;
+
+	/* We track the associated batch vma for debugging and error capture.
+	 * Whilst this request exists, the batch obj will be on the active_list,
+	 * and so will hold the active reference. Only when this request is
+	 * retired will the the batch be moved onto the inactive_list and lose
+	 * its active reference. Hence we do not need to explicitly hold
+	 * another reference here.
+	 */
+	rq->pending_flush |= I915_COMMAND_BARRIER;
+	return 0;
+}
+
+/* Track the batches submitted by clients for throttling */
+static void
+add_to_client(struct i915_gem_request *rq)
+{
+	struct drm_i915_file_private *file_priv = rq->ctx->file_priv;
+
+	if (file_priv) {
+		spin_lock(&file_priv->mm.lock);
+		list_add_tail(&rq->client_list,
+			      &file_priv->mm.request_list);
+		rq->file_priv = file_priv;
+		rq->emitted_jiffies = jiffies;
+		spin_unlock(&file_priv->mm.lock);
+	}
+}
+
+static void
+remove_from_client(struct i915_gem_request *rq)
+{
+	struct drm_i915_file_private *file_priv = rq->file_priv;
+
+	if (!file_priv)
+		return;
+
+	spin_lock(&file_priv->mm.lock);
+	if (rq->file_priv) {
+		list_del(&rq->client_list);
+		rq->file_priv = NULL;
+	}
+	spin_unlock(&file_priv->mm.lock);
+}
+
+/* Activity tracking on the object so that we can serialise CPU access to
+ * the object's memory with the GPU.
+ */
+static void
+add_to_obj(struct i915_gem_request *rq, struct i915_vma *vma)
+{
+	struct drm_i915_gem_object *obj = vma->obj;
+	struct intel_engine_cs *engine = rq->engine;
+
+	if (!vma->exec_read)
+		return;
+
+	if (vma->last_read[engine->id].request == NULL && vma->active++ == 0) {
+		drm_gem_object_reference(&obj->base);
+		i915_vma_get(vma);
+	}
+
+	i915_request_put(vma->last_read[engine->id].request);
+	vma->last_read[engine->id].request = i915_request_get(rq);
+
+	list_move_tail(&vma->last_read[engine->id].engine_link,
+		       &engine->vma_list);
+
+	/* Add a reference if we're newly entering the active list. */
+	if (obj->last_read[engine->id].request == NULL && obj->active++ == 0)
+		drm_gem_object_reference(&obj->base);
+
+	if (vma->exec_write) {
+		obj->dirty = 1;
+		i915_request_put(obj->last_write.request);
+		obj->last_write.request = i915_request_get(rq);
+		list_move_tail(&obj->last_write.engine_link,
+			       &engine->write_list);
+
+		if (obj->active > 1) {
+			int i;
+
+			for (i = 0; i < I915_NUM_ENGINES; i++) {
+				if (obj->last_read[i].request == NULL)
+					continue;
+
+				list_del_init(&obj->last_read[i].engine_link);
+				i915_request_put(obj->last_read[i].request);
+				obj->last_read[i].request = NULL;
+			}
+
+			obj->active = 1;
+		}
+	}
+
+	if (vma->exec_fence & VMA_IS_FENCED) {
+		i915_request_put(obj->last_fence.request);
+		obj->last_fence.request = i915_request_get(rq);
+		list_move_tail(&obj->last_fence.engine_link,
+			       &engine->fence_list);
+		if (vma->exec_fence & VMA_HAS_FENCE)
+			list_move_tail(&rq->i915->fence_regs[obj->fence_reg].lru_list,
+					&rq->i915->mm.fence_list);
+	}
+
+	i915_request_put(obj->last_read[engine->id].request);
+	obj->last_read[engine->id].request = i915_request_get(rq);
+	list_move_tail(&obj->last_read[engine->id].engine_link,
+		       &engine->read_list);
+
+	BUG_ON(!drm_mm_node_allocated(&vma->node));
+	list_move_tail(&vma->mm_list, &vma->vm->active_list);
+}
+
+static bool leave_breadcrumb(struct i915_gem_request *rq)
+{
+	if (rq->breadcrumb[rq->engine->id])
+		return false;
+
+	/* Auto-report HEAD every 4k to make sure that we can always wait on
+	 * some available ring space in the future. This also caps the
+	 * latency of future waits for missed breadcrumbs.
+	 */
+	if (__intel_ring_space(rq->ring->tail, rq->ring->breadcrumb_tail,
+			       rq->ring->size, 0) >= PAGE_SIZE)
+		return true;
+
+	return false;
+}
+
+static bool simulated_hang(struct intel_engine_cs *engine)
+{
+	return test_and_clear_bit(engine->id,
+				  &engine->i915->gpu_error.stop_rings);
+}
+
+int i915_request_commit(struct i915_gem_request *rq)
+{
+	int ret, n;
+
+	lockdep_assert_held(&rq->i915->dev->struct_mutex);
+
+	if (!rq->outstanding)
+		return 0;
+
+	if (rq->head == rq->ring->tail) {
+		rq->completed = true;
+		return 0;
+	}
+
+	if (simulated_hang(rq->engine))
+		i915_handle_error(rq->i915->dev,
+				  I915_HANG_RESET | I915_HANG_SIMULATED,
+				  "Simulated hang");
+
+	if (!check_reset(rq))
+		return rq->i915->mm.interruptible ? -EAGAIN : -EIO;
+
+	if (leave_breadcrumb(rq)) {
+		ret = i915_request_emit_breadcrumb(rq);
+		if (ret)
+			return ret;
+	}
+
+	/* TAIL must be aligned to a qword */
+	if ((rq->ring->tail / sizeof (uint32_t)) & 1) {
+		intel_ring_emit(rq->ring, MI_NOOP);
+		intel_ring_advance(rq->ring);
+	}
+	rq->tail = rq->ring->tail;
+
+	trace_i915_gem_request_commit(rq);
+	ret = rq->engine->add_request(rq);
+	if (ret)
+		return ret;
+
+	i915_request_get(rq);
+
+	rq->outstanding = false;
+	if (rq->breadcrumb[rq->engine->id]) {
+		list_add_tail(&rq->breadcrumb_link, &rq->ring->breadcrumbs);
+		rq->ring->breadcrumb_tail = rq->tail;
+	}
+
+	memcpy(rq->engine->semaphore.sync,
+	       rq->semaphore,
+	       sizeof(rq->semaphore));
+	for (n = 0; n < ARRAY_SIZE(rq->breadcrumb); n++)
+		if (rq->breadcrumb[n])
+			rq->engine->breadcrumb[n] = rq->breadcrumb[n];
+
+	rq->ring->pending_flush = rq->pending_flush;
+
+	if (rq->batch) {
+		add_to_client(rq);
+		rq->batch->vm->dirty = false;
+
+		i915_queue_hangcheck(rq->i915->dev);
+	}
+
+	rq->engine->last_request = rq;
+	intel_mark_busy(rq->i915->dev);
+
+	cancel_delayed_work_sync(&rq->i915->mm.idle_work);
+	mod_delayed_work(rq->i915->wq,
+			 &rq->i915->mm.retire_work,
+			 round_jiffies_up_relative(HZ));
+
+	while (!list_empty(&rq->vmas)) {
+		struct i915_vma *vma =
+			list_first_entry(&rq->vmas,
+					 typeof(*vma),
+					 exec_link);
+
+		add_to_obj(rq, vma);
+		i915_vma_unreserve(vma);
+	}
+
+	i915_request_switch_context__commit(rq);
+
+	if (rq->engine->last_context) {
+		rq->engine->unpin_context(rq->engine, rq->engine->last_context);
+		i915_gem_context_unreference(rq->engine->last_context);
+	}
+
+	rq->engine->last_request = rq;
+	rq->engine->last_context = rq->ctx;
+	return 0;
+}
+
+static void fake_irq(unsigned long data)
+{
+	wake_up_process((struct task_struct *)data);
+}
+
+static bool missed_irq(struct i915_gem_request *rq)
+{
+	return test_bit(rq->engine->id, &rq->i915->gpu_error.missed_irq_rings);
+}
+
+bool __i915_request_complete__wa(struct i915_gem_request *rq)
+{
+	struct drm_i915_private *dev_priv = rq->i915;
+	unsigned head, tail;
+
+	if (i915_request_complete(rq))
+		return true;
+
+	/* With execlists, we rely on interrupts to track request completion */
+	if (rq->engine->execlists_enabled)
+		return false;
+
+	/* As we may not emit a breadcrumb with every request, we
+	 * often have unflushed requests. In the event of an emergency,
+	 * just assume that if the RING_HEAD has reached the tail, then
+	 * the request is complete. However, note that the RING_HEAD
+	 * advances before the instruction completes, so this is quite lax,
+	 * and should only be used carefully. To compensate, we only treat
+	 * it as completed if the request flushed.
+	 *
+	 * As we treat this as only an advisory completion, we forgo
+	 * marking the request as actually complete.
+	 */
+	head = __intel_ring_space(I915_READ_HEAD(rq->engine) & HEAD_ADDR,
+				  rq->ring->tail, rq->ring->size,
+				  rq->pending_flush & I915_COMMAND_BARRIER ? 8 : 0);
+	tail = __intel_ring_space(rq->tail,
+				  rq->ring->tail, rq->ring->size, 0);
+	return head >= tail;
+}
+
+/**
+ * __i915_request_wait - wait until execution of request has finished
+ * @request: the request to wait upon
+ * @interruptible: do an interruptible wait (normally yes)
+ * @timeout_ns: in - how long to wait (NULL forever); out - how much time remaining
+ *
+ * Returns 0 if the request was completed within the alloted time. Else returns the
+ * errno with remaining time filled in timeout argument.
+ */
+int __i915_request_wait(struct i915_gem_request *rq,
+			bool interruptible,
+			s64 *timeout_ns,
+			struct drm_i915_file_private *file_priv)
+{
+	const bool irq_test_in_progress =
+		ACCESS_ONCE(rq->i915->gpu_error.test_irq_rings) & intel_engine_flag(rq->engine);
+	DEFINE_WAIT(wait);
+	unsigned long timeout_expire;
+	unsigned long before, now;
+	int ret = 0;
+
+	WARN(!intel_irqs_enabled(rq->i915), "IRQs disabled");
+
+	if (i915_request_complete(rq))
+		return 0;
+
+	timeout_expire = timeout_ns ? jiffies + nsecs_to_jiffies((u64)*timeout_ns) : 0;
+
+	if (rq->engine->id == RCS && INTEL_INFO(rq->i915)->gen >= 6)
+		gen6_rps_boost(rq->i915, file_priv);
+
+	if (!irq_test_in_progress) {
+		if (WARN_ON(!intel_irqs_enabled(rq->i915)))
+			return -ENODEV;
+
+		rq->engine->irq_get(rq->engine);
+	}
+
+	/* Record current time in case interrupted by signal, or wedged */
+	trace_i915_gem_request_wait_begin(rq);
+	before = jiffies;
+	for (;;) {
+		struct timer_list timer;
+
+		prepare_to_wait(&rq->engine->irq_queue, &wait,
+				interruptible ? TASK_INTERRUPTIBLE : TASK_UNINTERRUPTIBLE);
+
+		if (!check_reset(rq))
+			break;
+
+		rq->engine->irq_barrier(rq->engine);
+
+		if (i915_request_complete(rq))
+			break;
+
+		if (timeout_ns && time_after_eq(jiffies, timeout_expire)) {
+			ret = -ETIME;
+			break;
+		}
+
+		if (interruptible && signal_pending(current)) {
+			ret = -ERESTARTSYS;
+			break;
+		}
+
+		/* Paranoid kick of hangcheck so that we never wait forever */
+		i915_queue_hangcheck(rq->i915->dev);
+
+		timer.function = NULL;
+		if (timeout_ns || missed_irq(rq)) {
+			unsigned long expire;
+
+			setup_timer_on_stack(&timer, fake_irq, (unsigned long)current);
+			expire = missed_irq(rq) ? jiffies + 1 : timeout_expire;
+			mod_timer(&timer, expire);
+		}
+
+		io_schedule();
+
+		if (timer.function) {
+			del_singleshot_timer_sync(&timer);
+			destroy_timer_on_stack(&timer);
+		}
+	}
+	now = jiffies;
+	trace_i915_gem_request_wait_end(rq);
+
+	if (!irq_test_in_progress)
+		rq->engine->irq_put(rq->engine);
+
+	finish_wait(&rq->engine->irq_queue, &wait);
+
+	if (timeout_ns) {
+		s64 tres = *timeout_ns - jiffies_to_nsecs(now - before);
+		*timeout_ns = tres <= 0 ? 0 : tres;
+	}
+
+	return ret;
+}
+
+struct i915_gem_request *
+i915_request_get_breadcrumb(struct i915_gem_request *rq)
+{
+	struct list_head *list;
+	u32 seqno;
+	int ret;
+
+	if (i915_request_complete(rq))
+		return i915_request_get(rq);
+
+	/* Writes are only coherent from the cpu (in the general case) when
+	 * the interrupt following the write to memory is complete. That is
+	 * when the breadcrumb after the write request is complete.
+	 *
+	 * Reads are only complete when then command streamer barrier is
+	 * passed.
+	 *
+	 * In both cases, the CPU needs to wait upon the subsequent breadcrumb,
+	 * which ensures that all pending flushes have been emitted and are
+	 * complete, before reporting that the request is finished and
+	 * the CPU's view of memory is coherent with the GPU.
+	 */
+
+	ret = i915_request_emit_breadcrumb(rq);
+	if (ret)
+		return ERR_PTR(ret);
+
+	ret = i915_request_commit(rq);
+	if (ret)
+		return ERR_PTR(ret);
+
+	if (!list_empty(&rq->breadcrumb_link))
+		return i915_request_get(rq);
+
+	seqno = rq->breadcrumb[rq->engine->id];
+	list = &rq->ring->breadcrumbs;
+	list_for_each_entry_reverse(rq, list, breadcrumb_link) {
+		if (rq->seqno == seqno)
+			return i915_request_get(rq);
+	}
+
+	return ERR_PTR(-EIO);
+}
+
+int
+i915_request_wait(struct i915_gem_request *rq)
+{
+	int ret;
+
+	lockdep_assert_held(&rq->i915->dev->struct_mutex);
+
+	rq = i915_request_get_breadcrumb(rq);
+	if (IS_ERR(rq))
+		return PTR_ERR(rq);
+
+	ret = __i915_request_wait(rq, rq->i915->mm.interruptible,
+				  NULL, NULL);
+	i915_request_put(rq);
+
+	return ret;
+}
+
+void
+i915_request_retire(struct i915_gem_request *rq)
+{
+	lockdep_assert_held(&rq->i915->dev->struct_mutex);
+
+	if (!rq->completed) {
+		trace_i915_gem_request_complete(rq);
+		rq->completed = true;
+	}
+	trace_i915_gem_request_retire(rq);
+
+	/* We know the GPU must have read the request to have
+	 * sent us the seqno + interrupt, we can use the position
+	 * of tail of the request to update the last known position
+	 * of the GPU head.
+	 */
+	if (!list_empty(&rq->breadcrumb_link))
+		rq->ring->retired_head = rq->tail;
+
+	rq->batch = NULL;
+
+	/* We need to protect against simultaneous hangcheck/capture */
+	spin_lock(&rq->engine->lock);
+	if (rq->engine->last_request == rq)
+		rq->engine->last_request = NULL;
+	list_del(&rq->engine_link);
+	spin_unlock(&rq->engine->lock);
+
+	list_del(&rq->breadcrumb_link);
+	remove_from_client(rq);
+
+	i915_request_put(rq);
+}
+
+void
+__i915_request_free(struct kref *kref)
+{
+	struct i915_gem_request *rq = container_of(kref, struct i915_gem_request, kref);
+
+	lockdep_assert_held(&rq->i915->dev->struct_mutex);
+
+	if (rq->outstanding) {
+		/* Rollback this partial transaction as we never committed
+		 * the request to the hardware queue.
+		 */
+		rq->ring->tail = rq->head;
+		rq->ring->space = intel_ring_space(rq->ring);
+
+		while (!list_empty(&rq->vmas))
+			i915_vma_unreserve(list_first_entry(&rq->vmas,
+							    struct i915_vma,
+							    exec_link));
+
+		rq->engine->unpin_context(rq->engine, rq->ctx);
+		i915_gem_context_unreference(rq->ctx);
+	}
+
+	kfree(rq);
+}
diff -urN a/drivers/gpu/drm/i915/i915_gem_stolen.c b/drivers/gpu/drm/i915/i915_gem_stolen.c
--- a/drivers/gpu/drm/i915/i915_gem_stolen.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_gem_stolen.c	2014-11-22 14:37:49.334700418 -0700
@@ -30,6 +30,9 @@
 #include <drm/i915_drm.h>
 #include "i915_drv.h"
 
+#define KB(x) ((x) * 1024)
+#define MB(x) (KB(x) * 1024)
+
 /*
  * The BIOS typically reserves some of the system's memory for the exclusive
  * use of the integrated graphics. This memory is no longer available for
@@ -51,24 +54,90 @@
 	/* Almost universally we can find the Graphics Base of Stolen Memory
 	 * at offset 0x5c in the igfx configuration space. On a few (desktop)
 	 * machines this is also mirrored in the bridge device at different
-	 * locations, or in the MCHBAR. On gen2, the layout is again slightly
-	 * different with the Graphics Segment immediately following Top of
-	 * Memory (or Top of Usable DRAM). Note it appears that TOUD is only
-	 * reported by 865g, so we just use the top of memory as determined
-	 * by the e820 probe.
+	 * locations, or in the MCHBAR.
+	 *
+	 * On 865 we just check the TOUD register.
+	 *
+	 * On 830/845/85x the stolen memory base isn't available in any
+	 * register. We need to calculate it as TOM-TSEG_SIZE-stolen_size.
 	 *
-	 * XXX However gen2 requires an unavailable symbol.
 	 */
 	base = 0;
 	if (INTEL_INFO(dev)->gen >= 3) {
 		/* Read Graphics Base of Stolen Memory directly */
 		pci_read_config_dword(dev->pdev, 0x5c, &base);
 		base &= ~((1<<20) - 1);
-	} else { /* GEN2 */
-#if 0
-		/* Stolen is immediately above Top of Memory */
-		base = max_low_pfn_mapped << PAGE_SHIFT;
-#endif
+	} else if (IS_I865G(dev)) {
+		u16 toud = 0;
+
+		/*
+		 * FIXME is the graphics stolen memory region
+		 * always at TOUD? Ie. is it always the last
+		 * one to be allocated by the BIOS?
+		 */
+		pci_bus_read_config_word(dev->pdev->bus, PCI_DEVFN(0, 0),
+					 I865_TOUD, &toud);
+
+		base = toud << 16;
+	} else if (IS_I85X(dev)) {
+		u32 tseg_size = 0;
+		u32 tom;
+		u8 tmp;
+
+		pci_bus_read_config_byte(dev->pdev->bus, PCI_DEVFN(0, 0), I85X_ESMRAMC, &tmp);
+
+		if (tmp & TSEG_ENABLE)
+			tseg_size = MB(1);
+
+		pci_bus_read_config_byte(dev->pdev->bus, PCI_DEVFN(0, 1),
+					 I85X_DRB3, &tmp);
+		tom = tmp * MB(32);
+
+		base = tom - tseg_size - dev_priv->gtt.stolen_size;
+	} if (IS_845G(dev)) {
+		u32 tseg_size = 0;
+		u32 tom;
+		u8 tmp;
+
+		pci_bus_read_config_byte(dev->pdev->bus, PCI_DEVFN(0, 0),
+					 I845_ESMRAMC, &tmp);
+
+		if (tmp & TSEG_ENABLE) {
+			switch (tmp & I845_TSEG_SIZE_MASK) {
+			case I845_TSEG_SIZE_512K:
+				tseg_size = KB(512);
+				break;
+			case I845_TSEG_SIZE_1M:
+				tseg_size = MB(1);
+				break;
+			}
+		}
+
+		pci_bus_read_config_byte(dev->pdev->bus, PCI_DEVFN(0, 0),
+					 I830_DRB3, &tmp);
+		tom = tmp * MB(32);
+
+		base = tom - tseg_size - dev_priv->gtt.stolen_size;
+	} else if (IS_I830(dev)) {
+		u32 tseg_size = 0;
+		u32 tom;
+		u8 tmp;
+
+		pci_bus_read_config_byte(dev->pdev->bus, PCI_DEVFN(0, 0),
+					 I830_ESMRAMC, &tmp);
+
+		if (tmp & TSEG_ENABLE) {
+			if (tmp & I830_TSEG_SIZE_1M)
+				tseg_size = MB(1);
+			else
+				tseg_size = KB(512);
+		}
+
+		pci_bus_read_config_byte(dev->pdev->bus, PCI_DEVFN(0, 0),
+					 I830_DRB3, &tmp);
+		tom = tmp * MB(32);
+
+		base = tom - tseg_size - dev_priv->gtt.stolen_size;
 	}
 
 	if (base == 0)
@@ -289,6 +358,7 @@
 int i915_gem_init_stolen(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
+	u32 tmp;
 	int bios_reserved = 0;
 
 #ifdef CONFIG_INTEL_IOMMU
@@ -308,8 +378,16 @@
 	DRM_DEBUG_KMS("found %zd bytes of stolen memory at %08lx\n",
 		      dev_priv->gtt.stolen_size, dev_priv->mm.stolen_base);
 
-	if (IS_VALLEYVIEW(dev))
-		bios_reserved = 1024*1024; /* top 1M on VLV/BYT */
+	if (INTEL_INFO(dev)->gen >= 8) {
+		tmp = I915_READ(GEN7_BIOS_RESERVED);
+		tmp >>= GEN8_BIOS_RESERVED_SHIFT;
+		tmp &= GEN8_BIOS_RESERVED_MASK;
+		bios_reserved = (1024*1024) << tmp;
+	} else if (IS_GEN7(dev)) {
+		tmp = I915_READ(GEN7_BIOS_RESERVED);
+		bios_reserved = tmp & GEN7_BIOS_RESERVED_256K ?
+			256*1024 : 1024*1024;
+	}
 
 	if (WARN_ON(bios_reserved > dev_priv->gtt.stolen_size))
 		return 0;
@@ -417,18 +495,28 @@
 	return NULL;
 }
 
-struct drm_i915_gem_object *
-i915_gem_object_create_stolen(struct drm_device *dev, u32 size)
+static bool mark_free(struct drm_i915_gem_object *obj, struct list_head *unwind)
+{
+	if (obj->stolen == NULL)
+		return false;
+
+	if (obj->madv != I915_MADV_DONTNEED)
+		return false;
+
+	if (i915_gem_obj_is_pinned(obj))
+		return false;
+
+	list_add(&obj->obj_exec_link, unwind);
+	return drm_mm_scan_add_block(obj->stolen);
+}
+
+static struct drm_mm_node *stolen_alloc(struct drm_i915_private *dev_priv, u32 size)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_i915_gem_object *obj;
 	struct drm_mm_node *stolen;
+	struct drm_i915_gem_object *obj;
+	struct list_head unwind, evict;
 	int ret;
 
-	if (!drm_mm_initialized(&dev_priv->mm.stolen))
-		return NULL;
-
-	DRM_DEBUG_KMS("creating stolen object: size=%x\n", size);
 	if (size == 0)
 		return NULL;
 
@@ -438,11 +526,99 @@
 
 	ret = drm_mm_insert_node(&dev_priv->mm.stolen, stolen, size,
 				 4096, DRM_MM_SEARCH_DEFAULT);
-	if (ret) {
-		kfree(stolen);
-		return NULL;
+	if (ret == 0)
+		return stolen;
+
+	/* No more stolen memory available, or too fragmented.
+	 * Try evicting purgeable objects and search again.
+	 */
+
+	drm_mm_init_scan(&dev_priv->mm.stolen, size, 4096, 0);
+	INIT_LIST_HEAD(&unwind);
+
+	list_for_each_entry(obj, &dev_priv->mm.unbound_list, global_list)
+		if (mark_free(obj, &unwind))
+			goto found;
+
+	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list)
+		if (mark_free(obj, &unwind))
+			goto found;
+
+found:
+	INIT_LIST_HEAD(&evict);
+	while (!list_empty(&unwind)) {
+		obj = list_first_entry(&unwind,
+				       struct drm_i915_gem_object,
+				       obj_exec_link);
+		list_del_init(&obj->obj_exec_link);
+
+		if (drm_mm_scan_remove_block(obj->stolen)) {
+			list_add(&obj->obj_exec_link, &evict);
+			drm_gem_object_reference(&obj->base);
+		}
+	}
+
+	ret = 0;
+	while (!list_empty(&evict)) {
+		obj = list_first_entry(&evict,
+				       struct drm_i915_gem_object,
+				       obj_exec_link);
+		list_del_init(&obj->obj_exec_link);
+
+		if (ret == 0) {
+			struct i915_vma *vma, *vma_next;
+
+			list_for_each_entry_safe(vma, vma_next,
+						 &obj->vma_list,
+						 obj_link)
+				if (i915_vma_unbind(vma))
+					break;
+
+			/* Stolen pins its pages to prevent the
+			 * normal shrinker from processing stolen
+			 * objects.
+			 */
+			i915_gem_object_unpin_pages(obj);
+
+			ret = i915_gem_object_put_pages(obj);
+			if (ret == 0) {
+				i915_gem_object_release_stolen(obj);
+				obj->madv = __I915_MADV_PURGED;
+			} else
+				i915_gem_object_pin_pages(obj);
+		}
+
+		drm_gem_object_unreference(&obj->base);
 	}
 
+	if (ret == 0)
+		ret = drm_mm_insert_node(&dev_priv->mm.stolen, stolen, size,
+					 4096, DRM_MM_SEARCH_DEFAULT);
+	if (ret == 0)
+		return stolen;
+
+	kfree(stolen);
+	return NULL;
+}
+
+struct drm_i915_gem_object *
+i915_gem_object_create_stolen(struct drm_device *dev, u32 size)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_gem_object *obj;
+	struct drm_mm_node *stolen;
+
+	lockdep_assert_held(&dev->struct_mutex);
+
+	if (!drm_mm_initialized(&dev_priv->mm.stolen))
+		return NULL;
+
+	DRM_DEBUG_KMS("creating stolen object: size=%x\n", size);
+
+	stolen = stolen_alloc(dev_priv, size);
+	if (stolen == NULL)
+		return NULL;
+
 	obj = _i915_gem_object_create_stolen(dev, stolen);
 	if (obj)
 		return obj;
@@ -503,7 +679,7 @@
 	if (gtt_offset == I915_GTT_OFFSET_NONE)
 		return obj;
 
-	vma = i915_gem_obj_lookup_or_create_vma(obj, ggtt);
+	vma = i915_gem_obj_get_vma(obj, ggtt);
 	if (IS_ERR(vma)) {
 		ret = PTR_ERR(vma);
 		goto err_out;
@@ -524,7 +700,7 @@
 		}
 	}
 
-	obj->has_global_gtt_mapping = 1;
+	vma->bound |= GLOBAL_BIND;
 
 	list_add_tail(&obj->global_list, &dev_priv->mm.bound_list);
 	list_add_tail(&vma->mm_list, &ggtt->inactive_list);
@@ -533,7 +709,7 @@
 	return obj;
 
 err_vma:
-	i915_gem_vma_destroy(vma);
+	i915_vma_put(vma);
 err_out:
 	drm_mm_remove_node(stolen);
 	kfree(stolen);
diff -urN a/drivers/gpu/drm/i915/i915_gem_tiling.c b/drivers/gpu/drm/i915/i915_gem_tiling.c
--- a/drivers/gpu/drm/i915/i915_gem_tiling.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_gem_tiling.c	2014-11-22 14:37:49.334700418 -0700
@@ -91,26 +91,44 @@
 	uint32_t swizzle_x = I915_BIT_6_SWIZZLE_UNKNOWN;
 	uint32_t swizzle_y = I915_BIT_6_SWIZZLE_UNKNOWN;
 
-	if (IS_VALLEYVIEW(dev)) {
+	if (INTEL_INFO(dev)->gen >= 8 || IS_VALLEYVIEW(dev)) {
+		/*
+		 * On BDW+, swizzling is not used. We leave the CPU memory
+		 * controller in charge of optimizing memory accesses without
+		 * the extra address manipulation GPU side.
+		 *
+		 * VLV and CHV don't have GPU swizzling.
+		 */
 		swizzle_x = I915_BIT_6_SWIZZLE_NONE;
 		swizzle_y = I915_BIT_6_SWIZZLE_NONE;
 	} else if (INTEL_INFO(dev)->gen >= 6) {
-		uint32_t dimm_c0, dimm_c1;
-		dimm_c0 = I915_READ(MAD_DIMM_C0);
-		dimm_c1 = I915_READ(MAD_DIMM_C1);
-		dimm_c0 &= MAD_DIMM_A_SIZE_MASK | MAD_DIMM_B_SIZE_MASK;
-		dimm_c1 &= MAD_DIMM_A_SIZE_MASK | MAD_DIMM_B_SIZE_MASK;
-		/* Enable swizzling when the channels are populated with
-		 * identically sized dimms. We don't need to check the 3rd
-		 * channel because no cpu with gpu attached ships in that
-		 * configuration. Also, swizzling only makes sense for 2
-		 * channels anyway. */
-		if (dimm_c0 == dimm_c1) {
-			swizzle_x = I915_BIT_6_SWIZZLE_9_10;
-			swizzle_y = I915_BIT_6_SWIZZLE_9;
+		if (dev_priv->preserve_bios_swizzle) {
+			if (I915_READ(DISP_ARB_CTL) &
+			    DISP_TILE_SURFACE_SWIZZLING) {
+				swizzle_x = I915_BIT_6_SWIZZLE_9_10;
+				swizzle_y = I915_BIT_6_SWIZZLE_9;
+			} else {
+				swizzle_x = I915_BIT_6_SWIZZLE_NONE;
+				swizzle_y = I915_BIT_6_SWIZZLE_NONE;
+			}
 		} else {
-			swizzle_x = I915_BIT_6_SWIZZLE_NONE;
-			swizzle_y = I915_BIT_6_SWIZZLE_NONE;
+			uint32_t dimm_c0, dimm_c1;
+			dimm_c0 = I915_READ(MAD_DIMM_C0);
+			dimm_c1 = I915_READ(MAD_DIMM_C1);
+			dimm_c0 &= MAD_DIMM_A_SIZE_MASK | MAD_DIMM_B_SIZE_MASK;
+			dimm_c1 &= MAD_DIMM_A_SIZE_MASK | MAD_DIMM_B_SIZE_MASK;
+			/* Enable swizzling when the channels are populated
+			 * with identically sized dimms. We don't need to check
+			 * the 3rd channel because no cpu with gpu attached
+			 * ships in that configuration. Also, swizzling only
+			 * makes sense for 2 channels anyway. */
+			if (dimm_c0 == dimm_c1) {
+				swizzle_x = I915_BIT_6_SWIZZLE_9_10;
+				swizzle_y = I915_BIT_6_SWIZZLE_9;
+			} else {
+				swizzle_x = I915_BIT_6_SWIZZLE_NONE;
+				swizzle_y = I915_BIT_6_SWIZZLE_NONE;
+			}
 		}
 	} else if (IS_GEN5(dev)) {
 		/* On Ironlake whatever DRAM config, GPU always do
@@ -357,26 +375,12 @@
 		 * has to also include the unfenced register the GPU uses
 		 * whilst executing a fenced command for an untiled object.
 		 */
-
-		obj->map_and_fenceable =
-			!i915_gem_obj_ggtt_bound(obj) ||
-			(i915_gem_obj_ggtt_offset(obj) +
-			 obj->base.size <= dev_priv->gtt.mappable_end &&
-			 i915_gem_object_fence_ok(obj, args->tiling_mode));
-
-		/* Rebind if we need a change of alignment */
-		if (!obj->map_and_fenceable) {
-			u32 unfenced_align =
-				i915_gem_get_gtt_alignment(dev, obj->base.size,
-							    args->tiling_mode,
-							    false);
-			if (i915_gem_obj_ggtt_offset(obj) & (unfenced_align - 1))
-				ret = i915_gem_object_ggtt_unbind(obj);
-		}
-
+		if (obj->map_and_fenceable &&
+		    !i915_gem_object_fence_ok(obj, args->tiling_mode))
+			ret = i915_vma_unbind(i915_gem_obj_to_ggtt(obj));
 		if (ret == 0) {
 			obj->fence_dirty =
-				obj->fenced_gpu_access ||
+				obj->last_fence.request ||
 				obj->fence_reg != I915_FENCE_REG_NONE;
 
 			obj->tiling_mode = args->tiling_mode;
@@ -440,6 +444,7 @@
 	}
 
 	/* Hide bit 17 from the user -- see comment in i915_gem_set_tiling */
+	args->phys_swizzle_mode = args->swizzle_mode;
 	if (args->swizzle_mode == I915_BIT_6_SWIZZLE_9_17)
 		args->swizzle_mode = I915_BIT_6_SWIZZLE_9;
 	if (args->swizzle_mode == I915_BIT_6_SWIZZLE_9_10_17)
diff -urN a/drivers/gpu/drm/i915/i915_gem_userptr.c b/drivers/gpu/drm/i915/i915_gem_userptr.c
--- a/drivers/gpu/drm/i915/i915_gem_userptr.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_gem_userptr.c	2014-11-22 14:37:49.334700418 -0700
@@ -79,7 +79,7 @@
 		was_interruptible = dev_priv->mm.interruptible;
 		dev_priv->mm.interruptible = false;
 
-		list_for_each_entry_safe(vma, tmp, &obj->vma_list, vma_link) {
+		list_for_each_entry_safe(vma, tmp, &obj->vma_list, obj_link) {
 			int ret = i915_vma_unbind(vma);
 			WARN_ON(ret && ret != -EIO);
 		}
@@ -828,11 +828,10 @@
 	return 0;
 }
 
-int
+void
 i915_gem_init_userptr(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = to_i915(dev);
 	mutex_init(&dev_priv->mm_lock);
 	hash_init(dev_priv->mm_structs);
-	return 0;
 }
diff -urN a/drivers/gpu/drm/i915/i915_gpu_error.c b/drivers/gpu/drm/i915/i915_gpu_error.c
--- a/drivers/gpu/drm/i915/i915_gpu_error.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_gpu_error.c	2014-11-22 14:37:49.334700418 -0700
@@ -28,6 +28,7 @@
  */
 
 #include <generated/utsrelease.h>
+#include <linux/zlib.h>
 #include "i915_drv.h"
 
 static const char *yesno(int v)
@@ -192,15 +193,18 @@
 				struct drm_i915_error_buffer *err,
 				int count)
 {
-	err_printf(m, "%s [%d]:\n", name, count);
+	int n;
 
+	err_printf(m, "  %s [%d]:\n", name, count);
 	while (count--) {
-		err_printf(m, "  %08x %8u %02x %02x %x %x",
+		err_printf(m, "    %08x %8u %02x %02x [",
 			   err->gtt_offset,
 			   err->size,
 			   err->read_domains,
-			   err->write_domain,
-			   err->rseqno, err->wseqno);
+			   err->write_domain);
+		for (n = 0; n < ARRAY_SIZE(err->rseqno); n++)
+			err_printf(m, " %x", err->rseqno[n]);
+		err_printf(m, " ] %x %x ", err->wseqno, err->fseqno);
 		err_puts(m, pin_flag(err->pinned));
 		err_puts(m, tiling_flag(err->tiling));
 		err_puts(m, dirty_flag(err->dirty));
@@ -208,7 +212,7 @@
 		err_puts(m, err->userptr ? " userptr" : "");
 		err_puts(m, err->ring != -1 ? " " : "");
 		err_puts(m, ring_str(err->ring));
-		err_puts(m, i915_cache_level_str(err->cache_level));
+		err_puts(m, i915_cache_level_str(m->i915, err->cache_level));
 
 		if (err->name)
 			err_printf(m, " (name: %d)", err->name);
@@ -220,11 +224,13 @@
 	}
 }
 
-static const char *hangcheck_action_to_str(enum intel_ring_hangcheck_action a)
+static const char *hangcheck_action_to_str(enum intel_engine_hangcheck_action a)
 {
 	switch (a) {
 	case HANGCHECK_IDLE:
 		return "idle";
+	case HANGCHECK_IDLE_WAITERS:
+		return "idle (with waiters)";
 	case HANGCHECK_WAIT:
 		return "wait";
 	case HANGCHECK_ACTIVE:
@@ -244,13 +250,19 @@
 				  struct drm_device *dev,
 				  struct drm_i915_error_ring *ring)
 {
+	int n;
+
 	if (!ring->valid)
 		return;
 
-	err_printf(m, "  HEAD: 0x%08x\n", ring->head);
-	err_printf(m, "  TAIL: 0x%08x\n", ring->tail);
-	err_printf(m, "  CTL: 0x%08x\n", ring->ctl);
-	err_printf(m, "  HWS: 0x%08x\n", ring->hws);
+	err_printf(m, "%s command stream:\n", ring_str(ring->id));
+
+	err_printf(m, "  START: 0x%08x\n", ring->start);
+	err_printf(m, "  HEAD:  0x%08x\n", ring->head);
+	err_printf(m, "  TAIL:  0x%08x\n", ring->tail);
+	err_printf(m, "  CTL:   0x%08x\n", ring->ctl);
+	err_printf(m, "  MODE:  0x%08x [idle? %d]\n", ring->mode, !!(ring->mode & MODE_IDLE));
+	err_printf(m, "  HWS:   0x%08x\n", ring->hws);
 	err_printf(m, "  ACTHD: 0x%08x %08x\n", (u32)(ring->acthd>>32), (u32)ring->acthd);
 	err_printf(m, "  IPEIR: 0x%08x\n", ring->ipeir);
 	err_printf(m, "  IPEHR: 0x%08x\n", ring->ipehr);
@@ -266,17 +278,13 @@
 	if (INTEL_INFO(dev)->gen >= 6) {
 		err_printf(m, "  RC PSMI: 0x%08x\n", ring->rc_psmi);
 		err_printf(m, "  FAULT_REG: 0x%08x\n", ring->fault_reg);
-		err_printf(m, "  SYNC_0: 0x%08x [last synced 0x%08x]\n",
-			   ring->semaphore_mboxes[0],
-			   ring->semaphore_seqno[0]);
-		err_printf(m, "  SYNC_1: 0x%08x [last synced 0x%08x]\n",
-			   ring->semaphore_mboxes[1],
-			   ring->semaphore_seqno[1]);
-		if (HAS_VEBOX(dev)) {
-			err_printf(m, "  SYNC_2: 0x%08x [last synced 0x%08x]\n",
-				   ring->semaphore_mboxes[2],
-				   ring->semaphore_seqno[2]);
-		}
+		err_printf(m, "  SYNC_0: 0x%08x\n",
+			   ring->semaphore_mboxes[0]);
+		err_printf(m, "  SYNC_1: 0x%08x\n",
+			   ring->semaphore_mboxes[1]);
+		if (HAS_VEBOX(dev))
+			err_printf(m, "  SYNC_2: 0x%08x\n",
+				   ring->semaphore_mboxes[2]);
 	}
 	if (USES_PPGTT(dev)) {
 		err_printf(m, "  GFX_MODE: 0x%08x\n", ring->vm_info.gfx_mode);
@@ -291,8 +299,20 @@
 				   ring->vm_info.pp_dir_base);
 		}
 	}
-	err_printf(m, "  seqno: 0x%08x\n", ring->seqno);
-	err_printf(m, "  waiting: %s\n", yesno(ring->waiting));
+	err_printf(m, "  tag: 0x%04x\n", ring->tag);
+	err_printf(m, "  seqno: 0x%08x [hangcheck 0x%08x, breadcrumb 0x%08x, request 0x%08x]\n",
+		   ring->seqno, ring->hangcheck, ring->breadcrumb[ring->id], ring->request);
+	err_printf(m, "  sem.signal: [");
+	for (n = 0; n < ARRAY_SIZE(ring->breadcrumb); n++)
+		err_printf(m, " %s%08x", n == ring->id ? "*" : "", ring->breadcrumb[n]);
+	err_printf(m, " ]\n");
+	err_printf(m, "  sem.waited: [");
+	for (n = 0; n < ARRAY_SIZE(ring->semaphore_sync); n++)
+		err_printf(m, " %s%08x", n == ring->id ? "*" : "", ring->semaphore_sync[n]);
+	err_printf(m, " ]\n");
+	err_printf(m, "  waiting: %s [irq count %d]\n",
+		   yesno(ring->waiting), ring->irq_count);
+	err_printf(m, "  interrupts: %d\n", ring->interrupts);
 	err_printf(m, "  ring->head: 0x%08x\n", ring->cpu_ring_head);
 	err_printf(m, "  ring->tail: 0x%08x\n", ring->cpu_ring_tail);
 	err_printf(m, "  hangcheck: %s [%d]\n",
@@ -309,18 +329,44 @@
 	va_end(args);
 }
 
+static bool
+ascii85_encode(uint32_t in, char *out)
+{
+	int i;
+
+	if (in == 0)
+		return false;
+
+	out[5] = '\0';
+	for (i = 5; i--; ) {
+		int digit = in % 85;
+		out[i] = digit + 33;
+		in /= 85;
+	}
+
+	return true;
+}
+
 static void print_error_obj(struct drm_i915_error_state_buf *m,
 			    struct drm_i915_error_object *obj)
 {
-	int page, offset, elt;
+	char out[6];
+	int page;
 
-	for (page = offset = 0; page < obj->page_count; page++) {
-		for (elt = 0; elt < PAGE_SIZE/4; elt++) {
-			err_printf(m, "%08x :  %08x\n", offset,
-				   obj->pages[page][elt]);
-			offset += 4;
+	err_puts(m, ":"); /* indicate compressed data */
+	for (page = 0; page < obj->page_count; page++) {
+		int i, len = PAGE_SIZE;
+		if (page == obj->page_count - 1)
+			len -= obj->unused;
+		len = (len + 3) / 4;
+		for (i = 0; i < len; i++) {
+			if (ascii85_encode(obj->pages[page][i], out))
+				err_puts(m, out);
+			else
+				err_puts(m, "z");
 		}
 	}
+	err_puts(m, "\n");
 }
 
 int i915_error_state_to_str(struct drm_i915_error_state_buf *m,
@@ -330,8 +376,8 @@
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_i915_error_state *error = error_priv->error;
 	struct drm_i915_error_object *obj;
-	int i, j, offset, elt;
 	int max_hangcheck_score;
+	int i, j;
 
 	if (!error) {
 		err_printf(m, "no error state collected\n");
@@ -362,11 +408,16 @@
 	err_printf(m, "EIR: 0x%08x\n", error->eir);
 	err_printf(m, "IER: 0x%08x\n", error->ier);
 	if (INTEL_INFO(dev)->gen >= 8) {
-		for (i = 0; i < 4; i++)
+		for (i = 0; i < 4; i++) {
 			err_printf(m, "GTIER gt %d: 0x%08x\n", i,
 				   error->gtier[i]);
-	} else if (HAS_PCH_SPLIT(dev) || IS_VALLEYVIEW(dev))
+			err_printf(m, "GTIMR gt %d: 0x%08x\n", i,
+				   error->gtimr[i]);
+		}
+	} else if (HAS_PCH_SPLIT(dev) || IS_VALLEYVIEW(dev)) {
 		err_printf(m, "GTIER: 0x%08x\n", error->gtier[0]);
+		err_printf(m, "GTIMR: 0x%08x\n", error->gtimr[0]);
+	}
 	err_printf(m, "PGTBL_ER: 0x%08x\n", error->pgtbl_er);
 	err_printf(m, "FORCEWAKE: 0x%08x\n", error->forcewake);
 	err_printf(m, "DERRMR: 0x%08x\n", error->derrmr);
@@ -388,94 +439,82 @@
 	if (INTEL_INFO(dev)->gen == 7)
 		err_printf(m, "ERR_INT: 0x%08x\n", error->err_int);
 
-	for (i = 0; i < ARRAY_SIZE(error->ring); i++) {
-		err_printf(m, "%s command stream:\n", ring_str(i));
+	for (i = 0; i < ARRAY_SIZE(error->ring); i++)
 		i915_ring_error_state(m, dev, &error->ring[i]);
-	}
 
-	if (error->active_bo)
+	for (i = 0; i < error->vm_count; i++) {
+		err_printf(m, "vm[%d]\n", i);
+
 		print_error_buffers(m, "Active",
-				    error->active_bo[0],
-				    error->active_bo_count[0]);
+				    error->active_bo[i],
+				    error->active_bo_count[i]);
 
-	if (error->pinned_bo)
 		print_error_buffers(m, "Pinned",
-				    error->pinned_bo[0],
-				    error->pinned_bo_count[0]);
+				    error->pinned_bo[i],
+				    error->pinned_bo_count[i]);
+	}
 
 	for (i = 0; i < ARRAY_SIZE(error->ring); i++) {
-		obj = error->ring[i].batchbuffer;
+		const struct drm_i915_error_ring *ering = &error->ring[i];
+		const char *name = dev_priv->engine[ering->id].name;
+
+		obj = ering->batchbuffer;
 		if (obj) {
-			err_puts(m, dev_priv->ring[i].name);
-			if (error->ring[i].pid != -1)
+			err_puts(m, name);
+			if (ering->pid != -1)
 				err_printf(m, " (submitted by %s [%d])",
-					   error->ring[i].comm,
-					   error->ring[i].pid);
+					   ering->comm, ering->pid);
 			err_printf(m, " --- gtt_offset = 0x%08x\n",
 				   obj->gtt_offset);
 			print_error_obj(m, obj);
 		}
 
-		obj = error->ring[i].wa_batchbuffer;
+		obj = ering->wa_batchbuffer;
 		if (obj) {
 			err_printf(m, "%s (w/a) --- gtt_offset = 0x%08x\n",
-				   dev_priv->ring[i].name, obj->gtt_offset);
+				   name, obj->gtt_offset);
 			print_error_obj(m, obj);
 		}
 
-		if (error->ring[i].num_requests) {
-			err_printf(m, "%s --- %d requests\n",
-				   dev_priv->ring[i].name,
-				   error->ring[i].num_requests);
-			for (j = 0; j < error->ring[i].num_requests; j++) {
-				err_printf(m, "  seqno 0x%08x, emitted %ld, tail 0x%08x\n",
-					   error->ring[i].requests[j].seqno,
-					   error->ring[i].requests[j].jiffies,
-					   error->ring[i].requests[j].tail);
+		if (ering->num_batches) {
+			err_printf(m, "%s --- %d batches\n",
+				   name, ering->num_batches);
+			for (j = 0; j < ering->num_batches; j++) {
+				err_printf(m, "  pid %ld, seqno 0x%08x, tag 0x%04x, emitted %dms ago (at %ld jiffies), head 0x%08x, tail 0x%08x, batch 0x%08x, complete? %d\n",
+					   ering->batches[j].pid,
+					   ering->batches[j].seqno,
+					   ering->batches[j].tag,
+					   jiffies_to_usecs(jiffies - ering->batches[j].jiffies) / 1000,
+					   ering->batches[j].jiffies,
+					   ering->batches[j].head,
+					   ering->batches[j].tail,
+					   ering->batches[j].batch,
+					   ering->batches[j].complete);
 			}
 		}
 
-		if ((obj = error->ring[i].ringbuffer)) {
+		if ((obj = ering->ringbuffer)) {
 			err_printf(m, "%s --- ringbuffer = 0x%08x\n",
-				   dev_priv->ring[i].name,
-				   obj->gtt_offset);
+				   name, obj->gtt_offset);
 			print_error_obj(m, obj);
 		}
 
-		if ((obj = error->ring[i].hws_page)) {
+		if ((obj = ering->hws_page)) {
 			err_printf(m, "%s --- HW Status = 0x%08x\n",
-				   dev_priv->ring[i].name,
-				   obj->gtt_offset);
-			offset = 0;
-			for (elt = 0; elt < PAGE_SIZE/16; elt += 4) {
-				err_printf(m, "[%04x] %08x %08x %08x %08x\n",
-					   offset,
-					   obj->pages[0][elt],
-					   obj->pages[0][elt+1],
-					   obj->pages[0][elt+2],
-					   obj->pages[0][elt+3]);
-					offset += 16;
-			}
+				   name, obj->gtt_offset);
+			print_error_obj(m, obj);
 		}
 
 		if ((obj = error->ring[i].ctx)) {
 			err_printf(m, "%s --- HW Context = 0x%08x\n",
-				   dev_priv->ring[i].name,
-				   obj->gtt_offset);
+				   name, obj->gtt_offset);
 			print_error_obj(m, obj);
 		}
 	}
 
 	if ((obj = error->semaphore_obj)) {
 		err_printf(m, "Semaphore page = 0x%08x\n", obj->gtt_offset);
-		for (elt = 0; elt < PAGE_SIZE/16; elt += 4) {
-			err_printf(m, "[%04x] %08x %08x %08x %08x\n",
-				   elt * 4,
-				   obj->pages[0][elt],
-				   obj->pages[0][elt+1],
-				   obj->pages[0][elt+2],
-				   obj->pages[0][elt+3]);
-		}
+		print_error_obj(m, obj);
 	}
 
 	if (error->overlay)
@@ -492,9 +531,11 @@
 }
 
 int i915_error_state_buf_init(struct drm_i915_error_state_buf *ebuf,
+			      struct drm_i915_private *i915,
 			      size_t count, loff_t pos)
 {
 	memset(ebuf, 0, sizeof(*ebuf));
+	ebuf->i915 = i915;
 
 	/* We need to have enough room to store any i915_error_state printf
 	 * so that we can move it to start position.
@@ -529,7 +570,7 @@
 		return;
 
 	for (page = 0; page < obj->page_count; page++)
-		kfree(obj->pages[page]);
+		free_page((unsigned long)obj->pages[page]);
 
 	kfree(obj);
 }
@@ -545,7 +586,7 @@
 		i915_error_object_free(error->ring[i].ringbuffer);
 		i915_error_object_free(error->ring[i].hws_page);
 		i915_error_object_free(error->ring[i].ctx);
-		kfree(error->ring[i].requests);
+		kfree(error->ring[i].batches);
 	}
 
 	i915_error_object_free(error->semaphore_obj);
@@ -555,102 +596,157 @@
 	kfree(error);
 }
 
+static int compress_page(struct z_stream_s *zstream,
+			 void *src,
+			 struct drm_i915_error_object *dst)
+{
+	zstream->next_in = src;
+	zstream->avail_in = PAGE_SIZE;
+
+	do {
+		if (zstream->avail_out == 0) {
+			zstream->next_out = (void *)__get_free_page(GFP_ATOMIC);
+			if (zstream->next_out == NULL)
+				return -ENOMEM;
+
+			dst->pages[dst->page_count++] = (void *)zstream->next_out;
+			zstream->avail_out = PAGE_SIZE;
+		}
+
+		if (zlib_deflate(zstream, Z_SYNC_FLUSH) != Z_OK)
+			return -EIO;
+
+#if 0
+		if (zstream->total_out > zstream->total_in)
+			return -E2BIG;
+#endif
+	} while (zstream->avail_in);
+
+	return 0;
+}
+
 static struct drm_i915_error_object *
-i915_error_object_create_sized(struct drm_i915_private *dev_priv,
-			       struct drm_i915_gem_object *src,
-			       struct i915_address_space *vm,
-			       const int num_pages)
+i915_error_object_create(struct drm_i915_private *dev_priv,
+			 struct i915_vma *vma)
 {
+	struct drm_i915_gem_object *src;
 	struct drm_i915_error_object *dst;
-	int i;
+	int num_pages;
+	bool use_ggtt;
 	u32 reloc_offset;
+	struct z_stream_s zstream;
+
+	if (vma == NULL)
+		return NULL;
 
-	if (src == NULL || src->pages == NULL)
+	src = vma->obj;
+	if (src->pages == NULL)
 		return NULL;
 
-	dst = kmalloc(sizeof(*dst) + num_pages * sizeof(u32 *), GFP_ATOMIC);
+	num_pages = src->base.size >> PAGE_SHIFT;
+
+	dst = kmalloc(sizeof(*dst) + (10 * num_pages * sizeof(u32 *) >> 3), GFP_ATOMIC);
 	if (dst == NULL)
 		return NULL;
 
-	reloc_offset = dst->gtt_offset = i915_gem_obj_offset(src, vm);
-	for (i = 0; i < num_pages; i++) {
-		unsigned long flags;
-		void *d;
+	dst->gtt_offset = vma->node.start;
+	dst->page_count = 0;
+	dst->unused = 0;
+
+	memset(&zstream, 0, sizeof(zstream));
+	zstream.workspace = kmalloc(zlib_deflate_workspacesize(MAX_WBITS, MAX_MEM_LEVEL),
+				    GFP_ATOMIC);
+	if (zstream.workspace == NULL ||
+	    zlib_deflateInit(&zstream, Z_DEFAULT_COMPRESSION) != Z_OK) {
+		kfree(dst);
+		return NULL;
+	}
 
-		d = kmalloc(PAGE_SIZE, GFP_ATOMIC);
-		if (d == NULL)
-			goto unwind;
+	reloc_offset = dst->gtt_offset;
+	use_ggtt = (src->cache_level == I915_CACHE_NONE &&
+		    vma->bound & GLOBAL_BIND &&
+		    reloc_offset + num_pages * PAGE_SIZE <= dev_priv->gtt.mappable_end);
+
+	/* Cannot access stolen address directly, try to use the aperture */
+	if (src->stolen && !use_ggtt)
+		goto unwind;
+
+	/* Cannot access snooped pages through the aperture */
+	if (use_ggtt && src->cache_level != I915_CACHE_NONE && !HAS_LLC(dev_priv))
+		goto unwind;
 
-		local_irq_save(flags);
-		if (src->cache_level == I915_CACHE_NONE &&
-		    reloc_offset < dev_priv->gtt.mappable_end &&
-		    src->has_global_gtt_mapping &&
-		    i915_is_ggtt(vm)) {
-			void __iomem *s;
+	if (!use_ggtt)
+		reloc_offset = 0;
+
+	while (num_pages--) {
+		unsigned long flags;
+		void *s;
+		int ret;
 
+		local_irq_save(flags);
+		if (use_ggtt) {
 			/* Simply ignore tiling or any overlapping fence.
 			 * It's part of the error state, and this hopefully
 			 * captures what the GPU read.
 			 */
-
-			s = io_mapping_map_atomic_wc(dev_priv->gtt.mappable,
-						     reloc_offset);
-			memcpy_fromio(d, s, PAGE_SIZE);
+			s = (void *__force)
+				io_mapping_map_atomic_wc(dev_priv->gtt.mappable,
+							 reloc_offset);
+			ret = compress_page(&zstream, s, dst);
 			io_mapping_unmap_atomic(s);
-		} else if (src->stolen) {
-			unsigned long offset;
-
-			offset = dev_priv->mm.stolen_base;
-			offset += src->stolen->start;
-			offset += i << PAGE_SHIFT;
-
-			memcpy_fromio(d, (void __iomem *) offset, PAGE_SIZE);
 		} else {
-			struct page *page;
-			void *s;
-
-			page = i915_gem_object_get_page(src, i);
-
-			drm_clflush_pages(&page, 1);
-
-			s = kmap_atomic(page);
-			memcpy(d, s, PAGE_SIZE);
+			s = kmap_atomic(i915_gem_object_get_page(src, reloc_offset >> PAGE_SHIFT));
+			if (!HAS_LLC(dev_priv))
+				drm_clflush_virt_range(s, PAGE_SIZE);
+			ret = compress_page(&zstream, s, dst);
 			kunmap_atomic(s);
-
-			drm_clflush_pages(&page, 1);
 		}
 		local_irq_restore(flags);
-
-		dst->pages[i] = d;
-
+		if (ret)
+			goto unwind;
 		reloc_offset += PAGE_SIZE;
 	}
-	dst->page_count = num_pages;
+	zlib_deflate(&zstream, Z_FINISH);
+	dst->unused = zstream.avail_out;
+	zlib_deflateEnd(&zstream);
+	kfree(zstream.workspace);
 
 	return dst;
 
 unwind:
-	while (i--)
-		kfree(dst->pages[i]);
+	while (dst->page_count--)
+		free_page((unsigned long)dst->pages[dst->page_count]);
+	zlib_deflateEnd(&zstream);
+	kfree(zstream.workspace);
 	kfree(dst);
 	return NULL;
 }
-#define i915_error_object_create(dev_priv, src, vm) \
-	i915_error_object_create_sized((dev_priv), (src), (vm), \
-				       (src)->base.size>>PAGE_SHIFT)
-
-#define i915_error_ggtt_object_create(dev_priv, src) \
-	i915_error_object_create_sized((dev_priv), (src), &(dev_priv)->gtt.base, \
-				       (src)->base.size>>PAGE_SHIFT)
+
+static inline struct drm_i915_error_object *
+i915_error_ggtt_object_create(struct drm_i915_private *i915,
+			      struct drm_i915_gem_object *src)
+{
+	if (src == NULL)
+		return NULL;
+
+	return i915_error_object_create(i915,
+					i915_gem_obj_to_vma(src,
+							    &i915->gtt.base));
+}
 
 static void capture_bo(struct drm_i915_error_buffer *err,
-		       struct drm_i915_gem_object *obj)
+		       struct i915_vma *vma)
 {
+	struct drm_i915_gem_object *obj = vma->obj;
+	int n;
+
 	err->size = obj->base.size;
 	err->name = obj->base.name;
-	err->rseqno = obj->last_read_seqno;
-	err->wseqno = obj->last_write_seqno;
-	err->gtt_offset = i915_gem_obj_ggtt_offset(obj);
+	for (n = 0; n < ARRAY_SIZE(obj->last_read); n++)
+		err->rseqno[n] = i915_request_seqno(obj->last_read[n].request);
+	err->wseqno = i915_request_seqno(obj->last_write.request);
+	err->fseqno = i915_request_seqno(obj->last_fence.request);
+	err->gtt_offset = vma->node.start;
 	err->read_domains = obj->base.read_domains;
 	err->write_domain = obj->base.write_domain;
 	err->fence_reg = obj->fence_reg;
@@ -663,7 +759,7 @@
 	err->dirty = obj->dirty;
 	err->purgeable = obj->madv != I915_MADV_WILLNEED;
 	err->userptr = obj->userptr.mm != NULL;
-	err->ring = obj->ring ? obj->ring->id : -1;
+	err->ring = i915_request_engine_id(obj->last_write.request);
 	err->cache_level = obj->cache_level;
 }
 
@@ -674,7 +770,7 @@
 	int i = 0;
 
 	list_for_each_entry(vma, head, mm_list) {
-		capture_bo(err++, vma->obj);
+		capture_bo(err++, vma);
 		if (++i == count)
 			break;
 	}
@@ -683,21 +779,27 @@
 }
 
 static u32 capture_pinned_bo(struct drm_i915_error_buffer *err,
-			     int count, struct list_head *head)
+			     int count, struct list_head *head,
+			     struct i915_address_space *vm)
 {
 	struct drm_i915_gem_object *obj;
-	int i = 0;
+	struct drm_i915_error_buffer * const first = err;
+	struct drm_i915_error_buffer * const last = err + count;
 
 	list_for_each_entry(obj, head, global_list) {
-		if (!i915_gem_obj_is_pinned(obj))
-			continue;
+		struct i915_vma *vma;
 
-		capture_bo(err++, obj);
-		if (++i == count)
+		if (err == last)
 			break;
+
+		list_for_each_entry(vma, &obj->vma_list, obj_link)
+			if (vma->vm == vm && vma->pin_count > 0) {
+				capture_bo(err++, vma);
+				break;
+			}
 	}
 
-	return i;
+	return err - first;
 }
 
 /* Generate a semi-unique error code. The code is not meant to have meaning, The
@@ -721,7 +823,7 @@
 	 * synchronization commands which almost always appear in the case
 	 * strictly a client bug. Use instdone to differentiate those some.
 	 */
-	for (i = 0; i < I915_NUM_RINGS; i++) {
+	for (i = 0; i < I915_NUM_ENGINES; i++) {
 		if (error->ring[i].hangcheck_action == HANGCHECK_HUNG) {
 			if (ring_id)
 				*ring_id = i;
@@ -741,6 +843,7 @@
 
 	/* Fences */
 	switch (INTEL_INFO(dev)->gen) {
+	case 9:
 	case 8:
 	case 7:
 	case 6:
@@ -769,83 +872,77 @@
 
 static void gen8_record_semaphore_state(struct drm_i915_private *dev_priv,
 					struct drm_i915_error_state *error,
-					struct intel_engine_cs *ring,
+					struct intel_engine_cs *engine,
 					struct drm_i915_error_ring *ering)
 {
 	struct intel_engine_cs *to;
+	u32 *mbox;
 	int i;
 
-	if (!i915_semaphore_is_enabled(dev_priv->dev))
+	if (dev_priv->semaphore_obj == NULL)
 		return;
 
-	if (!error->semaphore_obj)
+	if (error->semaphore_obj == NULL)
 		error->semaphore_obj =
-			i915_error_object_create(dev_priv,
-						 dev_priv->semaphore_obj,
-						 &dev_priv->gtt.base);
-
-	for_each_ring(to, dev_priv, i) {
-		int idx;
-		u16 signal_offset;
-		u32 *tmp;
+			i915_error_ggtt_object_create(dev_priv,
+						      dev_priv->semaphore_obj);
+	if (error->semaphore_obj == NULL)
+		return;
 
-		if (ring == to)
+	mbox = error->semaphore_obj->pages[0];
+	for_each_engine(to, dev_priv, i) {
+		if (engine == to)
 			continue;
 
-		signal_offset = (GEN8_SIGNAL_OFFSET(ring, i) & (PAGE_SIZE - 1))
-				/ 4;
-		tmp = error->semaphore_obj->pages[0];
-		idx = intel_ring_sync_index(ring, to);
-
-		ering->semaphore_mboxes[idx] = tmp[signal_offset];
-		ering->semaphore_seqno[idx] = ring->semaphore.sync_seqno[idx];
+		ering->semaphore_mboxes[i] =
+			mbox[GEN8_SEMAPHORE_OFFSET(dev_priv,
+						   engine->id,
+						   i) & (PAGE_SIZE - 1) / 4];
 	}
 }
 
 static void gen6_record_semaphore_state(struct drm_i915_private *dev_priv,
-					struct intel_engine_cs *ring,
+					struct intel_engine_cs *engine,
 					struct drm_i915_error_ring *ering)
 {
-	ering->semaphore_mboxes[0] = I915_READ(RING_SYNC_0(ring->mmio_base));
-	ering->semaphore_mboxes[1] = I915_READ(RING_SYNC_1(ring->mmio_base));
-	ering->semaphore_seqno[0] = ring->semaphore.sync_seqno[0];
-	ering->semaphore_seqno[1] = ring->semaphore.sync_seqno[1];
-
+	ering->semaphore_mboxes[0] = I915_READ(RING_SYNC_0(engine->mmio_base));
+	ering->semaphore_mboxes[1] = I915_READ(RING_SYNC_1(engine->mmio_base));
 	if (HAS_VEBOX(dev_priv->dev)) {
 		ering->semaphore_mboxes[2] =
-			I915_READ(RING_SYNC_2(ring->mmio_base));
-		ering->semaphore_seqno[2] = ring->semaphore.sync_seqno[2];
+			I915_READ(RING_SYNC_2(engine->mmio_base));
 	}
 }
 
 static void i915_record_ring_state(struct drm_device *dev,
 				   struct drm_i915_error_state *error,
-				   struct intel_engine_cs *ring,
+				   struct intel_engine_cs *engine,
+				   struct i915_gem_request *rq,
 				   struct drm_i915_error_ring *ering)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_ringbuffer *ring;
 
 	if (INTEL_INFO(dev)->gen >= 6) {
-		ering->rc_psmi = I915_READ(ring->mmio_base + 0x50);
-		ering->fault_reg = I915_READ(RING_FAULT_REG(ring));
+		ering->rc_psmi = I915_READ(engine->mmio_base + 0x50);
+		ering->fault_reg = I915_READ(RING_FAULT_REG(engine));
 		if (INTEL_INFO(dev)->gen >= 8)
-			gen8_record_semaphore_state(dev_priv, error, ring, ering);
+			gen8_record_semaphore_state(dev_priv, error, engine, ering);
 		else
-			gen6_record_semaphore_state(dev_priv, ring, ering);
+			gen6_record_semaphore_state(dev_priv, engine, ering);
 	}
 
 	if (INTEL_INFO(dev)->gen >= 4) {
-		ering->faddr = I915_READ(RING_DMA_FADD(ring->mmio_base));
-		ering->ipeir = I915_READ(RING_IPEIR(ring->mmio_base));
-		ering->ipehr = I915_READ(RING_IPEHR(ring->mmio_base));
-		ering->instdone = I915_READ(RING_INSTDONE(ring->mmio_base));
-		ering->instps = I915_READ(RING_INSTPS(ring->mmio_base));
-		ering->bbaddr = I915_READ(RING_BBADDR(ring->mmio_base));
+		ering->faddr = I915_READ(RING_DMA_FADD(engine->mmio_base));
+		ering->ipeir = I915_READ(RING_IPEIR(engine->mmio_base));
+		ering->ipehr = I915_READ(RING_IPEHR(engine->mmio_base));
+		ering->instdone = I915_READ(RING_INSTDONE(engine->mmio_base));
+		ering->instps = I915_READ(RING_INSTPS(engine->mmio_base));
+		ering->bbaddr = I915_READ(RING_BBADDR(engine->mmio_base));
 		if (INTEL_INFO(dev)->gen >= 8) {
-			ering->faddr |= (u64) I915_READ(RING_DMA_FADD_UDW(ring->mmio_base)) << 32;
-			ering->bbaddr |= (u64) I915_READ(RING_BBADDR_UDW(ring->mmio_base)) << 32;
+			ering->faddr |= (u64) I915_READ(RING_DMA_FADD_UDW(engine->mmio_base)) << 32;
+			ering->bbaddr |= (u64) I915_READ(RING_BBADDR_UDW(engine->mmio_base)) << 32;
 		}
-		ering->bbstate = I915_READ(RING_BBSTATE(ring->mmio_base));
+		ering->bbstate = I915_READ(RING_BBSTATE(engine->mmio_base));
 	} else {
 		ering->faddr = I915_READ(DMA_FADD_I8XX);
 		ering->ipeir = I915_READ(IPEIR);
@@ -853,19 +950,29 @@
 		ering->instdone = I915_READ(INSTDONE);
 	}
 
-	ering->waiting = waitqueue_active(&ring->irq_queue);
-	ering->instpm = I915_READ(RING_INSTPM(ring->mmio_base));
-	ering->seqno = ring->get_seqno(ring, false);
-	ering->acthd = intel_ring_get_active_head(ring);
-	ering->head = I915_READ_HEAD(ring);
-	ering->tail = I915_READ_TAIL(ring);
-	ering->ctl = I915_READ_CTL(ring);
+	ering->waiting = waitqueue_active(&engine->irq_queue);
+	ering->instpm = I915_READ(RING_INSTPM(engine->mmio_base));
+	ering->acthd = intel_engine_get_active_head(engine);
+	ering->seqno = intel_engine_get_seqno(engine);
+	ering->request = engine->last_request ? engine->last_request->seqno : 0;
+	ering->hangcheck = engine->hangcheck.seqno;
+	memcpy(ering->breadcrumb, engine->breadcrumb, sizeof(ering->breadcrumb));
+	memcpy(ering->semaphore_sync, engine->semaphore.sync, sizeof(ering->semaphore_sync));
+	ering->tag = engine->tag;
+	ering->interrupts = atomic_read(&engine->interrupts);
+	ering->irq_count = engine->irq_refcount;
+	ering->start = I915_READ_START(engine);
+	ering->head = I915_READ_HEAD(engine);
+	ering->tail = I915_READ_TAIL(engine);
+	ering->ctl = I915_READ_CTL(engine);
+	if (!IS_GEN2(dev_priv))
+		ering->mode = I915_READ_MODE(engine);
 
 	if (I915_NEED_GFX_HWS(dev)) {
 		int mmio;
 
 		if (IS_GEN7(dev)) {
-			switch (ring->id) {
+			switch (engine->id) {
 			default:
 			case RCS:
 				mmio = RENDER_HWS_PGA_GEN7;
@@ -880,59 +987,68 @@
 				mmio = VEBOX_HWS_PGA_GEN7;
 				break;
 			}
-		} else if (IS_GEN6(ring->dev)) {
-			mmio = RING_HWS_PGA_GEN6(ring->mmio_base);
+		} else if (IS_GEN6(engine->i915)) {
+			mmio = RING_HWS_PGA_GEN6(engine->mmio_base);
 		} else {
 			/* XXX: gen8 returns to sanity */
-			mmio = RING_HWS_PGA(ring->mmio_base);
+			mmio = RING_HWS_PGA(engine->mmio_base);
 		}
 
 		ering->hws = I915_READ(mmio);
 	}
 
-	ering->cpu_ring_head = ring->buffer->head;
-	ering->cpu_ring_tail = ring->buffer->tail;
+	ring = rq ? rq->ctx->ring[engine->id].ring : engine->default_context->ring[engine->id].ring;
+	if (ring) {
+		ering->cpu_ring_head = ring->head;
+		ering->cpu_ring_tail = ring->tail;
+		ering->ringbuffer =
+			i915_error_ggtt_object_create(dev_priv, ring->obj);
+	}
 
-	ering->hangcheck_score = ring->hangcheck.score;
-	ering->hangcheck_action = ring->hangcheck.action;
+	ering->hws_page =
+		i915_error_ggtt_object_create(dev_priv,
+					      engine->status_page.obj);
+
+	ering->hangcheck_score = engine->hangcheck.score;
+	ering->hangcheck_action = engine->hangcheck.action;
 
 	if (USES_PPGTT(dev)) {
 		int i;
 
-		ering->vm_info.gfx_mode = I915_READ(RING_MODE_GEN7(ring));
+		ering->vm_info.gfx_mode = I915_READ(RING_MODE_GEN7(engine));
 
 		switch (INTEL_INFO(dev)->gen) {
+		case 9:
 		case 8:
 			for (i = 0; i < 4; i++) {
 				ering->vm_info.pdp[i] =
-					I915_READ(GEN8_RING_PDP_UDW(ring, i));
+					I915_READ(GEN8_RING_PDP_UDW(engine, i));
 				ering->vm_info.pdp[i] <<= 32;
 				ering->vm_info.pdp[i] |=
-					I915_READ(GEN8_RING_PDP_LDW(ring, i));
+					I915_READ(GEN8_RING_PDP_LDW(engine, i));
 			}
 			break;
 		case 7:
 			ering->vm_info.pp_dir_base =
-				I915_READ(RING_PP_DIR_BASE(ring));
+				I915_READ(RING_PP_DIR_BASE(engine));
 			break;
 		case 6:
 			ering->vm_info.pp_dir_base =
-				I915_READ(RING_PP_DIR_BASE_READ(ring));
+				I915_READ(RING_PP_DIR_BASE_READ(engine));
 			break;
 		}
 	}
 }
 
-
-static void i915_gem_record_active_context(struct intel_engine_cs *ring,
+static void i915_gem_record_active_context(struct intel_engine_cs *engine,
 					   struct drm_i915_error_state *error,
 					   struct drm_i915_error_ring *ering)
 {
-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
+	struct drm_i915_private *dev_priv = engine->i915;
 	struct drm_i915_gem_object *obj;
 
 	/* Currently render ring is the only HW context user */
-	if (ring->id != RCS || !error->ccid)
+	if (engine->id != RCS || !error->ccid)
 		return;
 
 	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list) {
@@ -950,45 +1066,40 @@
 				  struct drm_i915_error_state *error)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_i915_gem_request *request;
+	struct i915_gem_request *rq;
 	int i, count;
 
-	for (i = 0; i < I915_NUM_RINGS; i++) {
-		struct intel_engine_cs *ring = &dev_priv->ring[i];
+	for (i = 0; i < I915_NUM_ENGINES; i++) {
+		struct intel_engine_cs *engine = &dev_priv->engine[i];
 
 		error->ring[i].pid = -1;
 
-		if (ring->dev == NULL)
+		if (engine->i915 == NULL)
 			continue;
 
 		error->ring[i].valid = true;
+		error->ring[i].id = i;
 
-		i915_record_ring_state(dev, error, ring, &error->ring[i]);
-
-		request = i915_gem_find_active_request(ring);
-		if (request) {
+		spin_lock(&engine->lock);
+		rq = intel_engine_find_active_batch(engine);
+		if (rq) {
 			/* We need to copy these to an anonymous buffer
 			 * as the simplest method to avoid being overwritten
 			 * by userspace.
 			 */
 			error->ring[i].batchbuffer =
-				i915_error_object_create(dev_priv,
-							 request->batch_obj,
-							 request->ctx ?
-							 request->ctx->vm :
-							 &dev_priv->gtt.base);
+				i915_error_object_create(dev_priv, rq->batch);
 
-			if (HAS_BROKEN_CS_TLB(dev_priv->dev) &&
-			    ring->scratch.obj)
+			if (HAS_BROKEN_CS_TLB(dev_priv->dev))
 				error->ring[i].wa_batchbuffer =
 					i915_error_ggtt_object_create(dev_priv,
-							     ring->scratch.obj);
+							     engine->scratch.obj);
 
-			if (request->file_priv) {
+			if (rq->file_priv) {
 				struct task_struct *task;
 
 				rcu_read_lock();
-				task = pid_task(request->file_priv->file->pid,
+				task = pid_task(rq->file_priv->file->pid,
 						PIDTYPE_PID);
 				if (task) {
 					strcpy(error->ring[i].comm, task->comm);
@@ -998,37 +1109,50 @@
 			}
 		}
 
-		error->ring[i].ringbuffer =
-			i915_error_ggtt_object_create(dev_priv, ring->buffer->obj);
+		i915_record_ring_state(dev, error, engine, rq, &error->ring[i]);
 
-		if (ring->status_page.obj)
-			error->ring[i].hws_page =
-				i915_error_ggtt_object_create(dev_priv, ring->status_page.obj);
-
-		i915_gem_record_active_context(ring, error, &error->ring[i]);
+		i915_gem_record_active_context(engine, error, &error->ring[i]);
 
 		count = 0;
-		list_for_each_entry(request, &ring->request_list, list)
-			count++;
+		list_for_each_entry(rq, &engine->requests, engine_link)
+			count += rq->batch != NULL;
 
-		error->ring[i].num_requests = count;
-		error->ring[i].requests =
-			kcalloc(count, sizeof(*error->ring[i].requests),
+		error->ring[i].num_batches = count;
+		error->ring[i].batches =
+			kcalloc(count, sizeof(struct drm_i915_error_request),
 				GFP_ATOMIC);
-		if (error->ring[i].requests == NULL) {
-			error->ring[i].num_requests = 0;
+		if (error->ring[i].batches == NULL) {
+			error->ring[i].num_batches = 0;
 			continue;
 		}
 
 		count = 0;
-		list_for_each_entry(request, &ring->request_list, list) {
+		list_for_each_entry(rq, &engine->requests, engine_link) {
 			struct drm_i915_error_request *erq;
+			struct task_struct *task;
+
+			if (rq->batch == NULL)
+				continue;
+
+			if (count == error->ring[i].num_batches)
+				break;
 
-			erq = &error->ring[i].requests[count++];
-			erq->seqno = request->seqno;
-			erq->jiffies = request->emitted_jiffies;
-			erq->tail = request->tail;
+			erq = &error->ring[i].batches[count++];
+			erq->seqno = rq->seqno;
+			erq->jiffies = rq->emitted_jiffies;
+			erq->head = rq->head;
+			erq->tail = rq->tail;
+			erq->batch = rq->batch->node.start;
+			memcpy(erq->breadcrumb, rq->breadcrumb, sizeof(rq->breadcrumb));
+			erq->complete = i915_request_complete(rq);
+			erq->tag = rq->tag;
+
+			rcu_read_lock();
+			task = rq->file_priv ? pid_task(rq->file_priv->file->pid, PIDTYPE_PID) : NULL;
+			erq->pid = task ? task->pid : 0;
+			rcu_read_unlock();
 		}
+		spin_unlock(&engine->lock);
 	}
 }
 
@@ -1049,9 +1173,14 @@
 	list_for_each_entry(vma, &vm->active_list, mm_list)
 		i++;
 	error->active_bo_count[ndx] = i;
-	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list)
-		if (i915_gem_obj_is_pinned(obj))
-			i++;
+
+	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list) {
+		list_for_each_entry(vma, &obj->vma_list, obj_link)
+			if (vma->vm == vm && vma->pin_count > 0) {
+				i++;
+				break;
+			}
+	}
 	error->pinned_bo_count[ndx] = i - error->active_bo_count[ndx];
 
 	if (i) {
@@ -1070,7 +1199,7 @@
 		error->pinned_bo_count[ndx] =
 			capture_pinned_bo(pinned_bo,
 					  error->pinned_bo_count[ndx],
-					  &dev_priv->mm.bound_list);
+					  &dev_priv->mm.bound_list, vm);
 	error->active_bo[ndx] = active_bo;
 	error->pinned_bo[ndx] = pinned_bo;
 }
@@ -1091,8 +1220,25 @@
 	error->pinned_bo_count = kcalloc(cnt, sizeof(*error->pinned_bo_count),
 					 GFP_ATOMIC);
 
-	list_for_each_entry(vm, &dev_priv->vm_list, global_link)
-		i915_gem_capture_vm(dev_priv, error, vm, i++);
+	if (error->active_bo == NULL ||
+	    error->pinned_bo == NULL ||
+	    error->active_bo_count == NULL ||
+	    error->pinned_bo_count == NULL) {
+		kfree(error->active_bo);
+		kfree(error->active_bo_count);
+		kfree(error->pinned_bo);
+		kfree(error->pinned_bo_count);
+
+		error->active_bo = NULL;
+		error->active_bo_count = NULL;
+		error->pinned_bo = NULL;
+		error->pinned_bo_count = NULL;
+	} else {
+		list_for_each_entry(vm, &dev_priv->vm_list, global_link)
+			i915_gem_capture_vm(dev_priv, error, vm, i++);
+
+		error->vm_count = cnt;
+	}
 }
 
 /* Capture all registers which don't fit into another category. */
@@ -1113,6 +1259,7 @@
 	/* 1: Registers specific to a single generation */
 	if (IS_VALLEYVIEW(dev)) {
 		error->gtier[0] = I915_READ(GTIER);
+		error->gtimr[0] = I915_READ(GTIMR);
 		error->ier = I915_READ(VLV_IER);
 		error->forcewake = I915_READ(FORCEWAKE_VLV);
 	}
@@ -1148,11 +1295,14 @@
 
 	if (INTEL_INFO(dev)->gen >= 8) {
 		error->ier = I915_READ(GEN8_DE_MISC_IER);
-		for (i = 0; i < 4; i++)
+		for (i = 0; i < 4; i++) {
 			error->gtier[i] = I915_READ(GEN8_GT_IER(i));
+			error->gtimr[i] = I915_READ(GEN8_GT_IMR(i));
+		}
 	} else if (HAS_PCH_SPLIT(dev)) {
 		error->ier = I915_READ(DEIER);
 		error->gtier[0] = I915_READ(GTIER);
+		error->gtimr[0] = I915_READ(GTIMR);
 	} else if (IS_GEN2(dev)) {
 		error->ier = I915_READ16(IER);
 	} else if (!IS_VALLEYVIEW(dev)) {
@@ -1176,7 +1326,8 @@
 	ecode = i915_error_generate_code(dev_priv, error, &ring_id);
 
 	len = scnprintf(error->error_msg, sizeof(error->error_msg),
-			"GPU HANG: ecode %d:0x%08x", ring_id, ecode);
+			"GPU HANG: ecode %d:%d:0x%08x",
+			INTEL_INFO(dev)->gen, ring_id, ecode);
 
 	if (ring_id != -1 && error->ring[ring_id].pid != -1)
 		len += scnprintf(error->error_msg + len,
@@ -1264,13 +1415,12 @@
 			  struct i915_error_state_file_priv *error_priv)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	unsigned long flags;
 
-	spin_lock_irqsave(&dev_priv->gpu_error.lock, flags);
+	spin_lock_irq(&dev_priv->gpu_error.lock);
 	error_priv->error = dev_priv->gpu_error.first_error;
 	if (error_priv->error)
 		kref_get(&error_priv->error->ref);
-	spin_unlock_irqrestore(&dev_priv->gpu_error.lock, flags);
+	spin_unlock_irq(&dev_priv->gpu_error.lock);
 
 }
 
@@ -1284,22 +1434,21 @@
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_i915_error_state *error;
-	unsigned long flags;
 
-	spin_lock_irqsave(&dev_priv->gpu_error.lock, flags);
+	spin_lock_irq(&dev_priv->gpu_error.lock);
 	error = dev_priv->gpu_error.first_error;
 	dev_priv->gpu_error.first_error = NULL;
-	spin_unlock_irqrestore(&dev_priv->gpu_error.lock, flags);
+	spin_unlock_irq(&dev_priv->gpu_error.lock);
 
 	if (error)
 		kref_put(&error->ref, i915_error_state_free);
 }
 
-const char *i915_cache_level_str(int type)
+const char *i915_cache_level_str(struct drm_i915_private *i915, int type)
 {
 	switch (type) {
 	case I915_CACHE_NONE: return " uncached";
-	case I915_CACHE_LLC: return " snooped or LLC";
+	case I915_CACHE_LLC: return HAS_LLC(i915) ? " LLC" : " snooped";
 	case I915_CACHE_L3_LLC: return " L3+LLC";
 	case I915_CACHE_WT: return " WT";
 	default: return "";
@@ -1327,6 +1476,7 @@
 		WARN_ONCE(1, "Unsupported platform\n");
 	case 7:
 	case 8:
+	case 9:
 		instdone[0] = I915_READ(GEN7_INSTDONE_1);
 		instdone[1] = I915_READ(GEN7_SC_INSTDONE);
 		instdone[2] = I915_READ(GEN7_SAMPLER_INSTDONE);
diff -urN a/drivers/gpu/drm/i915/i915_ioc32.c b/drivers/gpu/drm/i915/i915_ioc32.c
--- a/drivers/gpu/drm/i915/i915_ioc32.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_ioc32.c	2014-11-22 14:37:49.334700418 -0700
@@ -189,7 +189,6 @@
 	[DRM_I915_ALLOC] = compat_i915_alloc
 };
 
-#ifdef CONFIG_COMPAT
 /**
  * Called whenever a 32-bit process running under a 64-bit kernel
  * performs an ioctl on /dev/dri/card<n>.
@@ -218,4 +217,3 @@
 
 	return ret;
 }
-#endif
diff -urN a/drivers/gpu/drm/i915/i915_irq.c b/drivers/gpu/drm/i915/i915_irq.c
--- a/drivers/gpu/drm/i915/i915_irq.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_irq.c	2014-11-22 14:37:49.334700418 -0700
@@ -37,6 +37,28 @@
 #include "i915_trace.h"
 #include "intel_drv.h"
 
+/**
+ * DOC: interrupt handling
+ *
+ * These functions provide the basic support for enabling and disabling the
+ * interrupt handling support. There's a lot more functionality in i915_irq.c
+ * and related files, but that will be described in separate chapters.
+ */
+
+#define __raw_i915_read8(dev_priv__, reg__) readb((dev_priv__)->regs + (reg__))
+#define __raw_i915_write8(dev_priv__, reg__, val__) writeb(val__, (dev_priv__)->regs + (reg__))
+
+#define __raw_i915_read16(dev_priv__, reg__) readw((dev_priv__)->regs + (reg__))
+#define __raw_i915_write16(dev_priv__, reg__, val__) writew(val__, (dev_priv__)->regs + (reg__))
+
+#define __raw_i915_read32(dev_priv__, reg__) readl((dev_priv__)->regs + (reg__))
+#define __raw_i915_write32(dev_priv__, reg__, val__) writel(val__, (dev_priv__)->regs + (reg__))
+
+#define __raw_i915_read64(dev_priv__, reg__) readq((dev_priv__)->regs + (reg__))
+#define __raw_i915_write64(dev_priv__, reg__, val__) writeq(val__, (dev_priv__)->regs + (reg__))
+
+#define __raw_i915_posting_read(dev_priv__, reg__) (void)__raw_i915_read32(dev_priv__, reg__)
+
 static const u32 hpd_ibx[] = {
 	[HPD_CRT] = SDE_CRT_HOTPLUG,
 	[HPD_SDVO_B] = SDE_SDVOB_HOTPLUG,
@@ -118,20 +140,22 @@
 
 #define GEN8_IRQ_INIT_NDX(type, which, imr_val, ier_val) do { \
 	GEN5_ASSERT_IIR_IS_ZERO(GEN8_##type##_IIR(which)); \
-	I915_WRITE(GEN8_##type##_IMR(which), (imr_val)); \
 	I915_WRITE(GEN8_##type##_IER(which), (ier_val)); \
-	POSTING_READ(GEN8_##type##_IER(which)); \
+	I915_WRITE(GEN8_##type##_IMR(which), (imr_val)); \
+	POSTING_READ(GEN8_##type##_IMR(which)); \
 } while (0)
 
 #define GEN5_IRQ_INIT(type, imr_val, ier_val) do { \
 	GEN5_ASSERT_IIR_IS_ZERO(type##IIR); \
-	I915_WRITE(type##IMR, (imr_val)); \
 	I915_WRITE(type##IER, (ier_val)); \
-	POSTING_READ(type##IER); \
+	I915_WRITE(type##IMR, (imr_val)); \
+	POSTING_READ(type##IMR); \
 } while (0)
 
+static void gen6_rps_irq_handler(struct drm_i915_private *dev_priv, u32 pm_iir);
+
 /* For display hotplug interrupt */
-static void
+void
 ironlake_enable_display_irq(struct drm_i915_private *dev_priv, u32 mask)
 {
 	assert_spin_locked(&dev_priv->irq_lock);
@@ -146,12 +170,12 @@
 	}
 }
 
-static void
+void
 ironlake_disable_display_irq(struct drm_i915_private *dev_priv, u32 mask)
 {
 	assert_spin_locked(&dev_priv->irq_lock);
 
-	if (!intel_irqs_enabled(dev_priv))
+	if (WARN_ON(!intel_irqs_enabled(dev_priv)))
 		return;
 
 	if ((dev_priv->irq_mask & mask) != mask) {
@@ -192,71 +216,28 @@
 	ilk_update_gt_irq(dev_priv, mask, 0);
 }
 
-/**
-  * snb_update_pm_irq - update GEN6_PMIMR
-  * @dev_priv: driver private
-  * @interrupt_mask: mask of interrupt bits to update
-  * @enabled_irq_mask: mask of interrupt bits to enable
-  */
-static void snb_update_pm_irq(struct drm_i915_private *dev_priv,
-			      uint32_t interrupt_mask,
-			      uint32_t enabled_irq_mask)
-{
-	uint32_t new_val;
-
-	assert_spin_locked(&dev_priv->irq_lock);
-
-	if (WARN_ON(!intel_irqs_enabled(dev_priv)))
-		return;
-
-	new_val = dev_priv->pm_irq_mask;
-	new_val &= ~interrupt_mask;
-	new_val |= (~enabled_irq_mask & interrupt_mask);
-
-	if (new_val != dev_priv->pm_irq_mask) {
-		dev_priv->pm_irq_mask = new_val;
-		I915_WRITE(GEN6_PMIMR, dev_priv->pm_irq_mask);
-		POSTING_READ(GEN6_PMIMR);
-	}
-}
-
-void gen6_enable_pm_irq(struct drm_i915_private *dev_priv, uint32_t mask)
+static u32 gen6_pm_iir(struct drm_i915_private *dev_priv)
 {
-	snb_update_pm_irq(dev_priv, mask, mask);
+	return INTEL_INFO(dev_priv)->gen >= 8 ? GEN8_GT_IIR(2) : GEN6_PMIIR;
 }
 
-void gen6_disable_pm_irq(struct drm_i915_private *dev_priv, uint32_t mask)
+static u32 gen6_pm_imr(struct drm_i915_private *dev_priv)
 {
-	snb_update_pm_irq(dev_priv, mask, 0);
+	return INTEL_INFO(dev_priv)->gen >= 8 ? GEN8_GT_IMR(2) : GEN6_PMIMR;
 }
 
-static bool ivb_can_enable_err_int(struct drm_device *dev)
+static u32 gen6_pm_ier(struct drm_i915_private *dev_priv)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_crtc *crtc;
-	enum pipe pipe;
-
-	assert_spin_locked(&dev_priv->irq_lock);
-
-	for_each_pipe(pipe) {
-		crtc = to_intel_crtc(dev_priv->pipe_to_crtc_mapping[pipe]);
-
-		if (crtc->cpu_fifo_underrun_disabled)
-			return false;
-	}
-
-	return true;
+	return INTEL_INFO(dev_priv)->gen >= 8 ? GEN8_GT_IER(2) : GEN6_PMIER;
 }
 
 /**
-  * bdw_update_pm_irq - update GT interrupt 2
+  * snb_update_pm_irq - update GEN6_PMIMR
   * @dev_priv: driver private
   * @interrupt_mask: mask of interrupt bits to update
   * @enabled_irq_mask: mask of interrupt bits to enable
-  *
-  * Copied from the snb function, updated with relevant register offsets
   */
-static void bdw_update_pm_irq(struct drm_i915_private *dev_priv,
+static void snb_update_pm_irq(struct drm_i915_private *dev_priv,
 			      uint32_t interrupt_mask,
 			      uint32_t enabled_irq_mask)
 {
@@ -273,135 +254,65 @@
 
 	if (new_val != dev_priv->pm_irq_mask) {
 		dev_priv->pm_irq_mask = new_val;
-		I915_WRITE(GEN8_GT_IMR(2), dev_priv->pm_irq_mask);
-		POSTING_READ(GEN8_GT_IMR(2));
+		I915_WRITE(gen6_pm_imr(dev_priv), dev_priv->pm_irq_mask);
+		POSTING_READ(gen6_pm_imr(dev_priv));
 	}
 }
 
-void gen8_enable_pm_irq(struct drm_i915_private *dev_priv, uint32_t mask)
-{
-	bdw_update_pm_irq(dev_priv, mask, mask);
-}
-
-void gen8_disable_pm_irq(struct drm_i915_private *dev_priv, uint32_t mask)
-{
-	bdw_update_pm_irq(dev_priv, mask, 0);
-}
-
-static bool cpt_can_enable_serr_int(struct drm_device *dev)
+void gen6_enable_pm_irq(struct drm_i915_private *dev_priv, uint32_t mask)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	enum pipe pipe;
-	struct intel_crtc *crtc;
-
-	assert_spin_locked(&dev_priv->irq_lock);
-
-	for_each_pipe(pipe) {
-		crtc = to_intel_crtc(dev_priv->pipe_to_crtc_mapping[pipe]);
-
-		if (crtc->pch_fifo_underrun_disabled)
-			return false;
-	}
-
-	return true;
+	snb_update_pm_irq(dev_priv, mask, mask);
 }
 
-void i9xx_check_fifo_underruns(struct drm_device *dev)
+void gen6_disable_pm_irq(struct drm_i915_private *dev_priv, uint32_t mask)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_crtc *crtc;
-	unsigned long flags;
-
-	spin_lock_irqsave(&dev_priv->irq_lock, flags);
-
-	for_each_intel_crtc(dev, crtc) {
-		u32 reg = PIPESTAT(crtc->pipe);
-		u32 pipestat;
-
-		if (crtc->cpu_fifo_underrun_disabled)
-			continue;
-
-		pipestat = I915_READ(reg) & 0xffff0000;
-		if ((pipestat & PIPE_FIFO_UNDERRUN_STATUS) == 0)
-			continue;
-
-		I915_WRITE(reg, pipestat | PIPE_FIFO_UNDERRUN_STATUS);
-		POSTING_READ(reg);
-
-		DRM_ERROR("pipe %c underrun\n", pipe_name(crtc->pipe));
-	}
-
-	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
+	snb_update_pm_irq(dev_priv, mask, 0);
 }
 
-static void i9xx_set_fifo_underrun_reporting(struct drm_device *dev,
-					     enum pipe pipe,
-					     bool enable, bool old)
+void gen6_reset_rps_interrupts(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	u32 reg = PIPESTAT(pipe);
-	u32 pipestat = I915_READ(reg) & 0xffff0000;
+	uint32_t reg = gen6_pm_iir(dev_priv);
 
-	assert_spin_locked(&dev_priv->irq_lock);
-
-	if (enable) {
-		I915_WRITE(reg, pipestat | PIPE_FIFO_UNDERRUN_STATUS);
-		POSTING_READ(reg);
-	} else {
-		if (old && pipestat & PIPE_FIFO_UNDERRUN_STATUS)
-			DRM_ERROR("pipe %c underrun\n", pipe_name(pipe));
-	}
+	spin_lock_irq(&dev_priv->irq_lock);
+	I915_WRITE(reg, dev_priv->rps.pm_events);
+	I915_WRITE(reg, dev_priv->rps.pm_events);
+	POSTING_READ(reg);
+	spin_unlock_irq(&dev_priv->irq_lock);
 }
 
-static void ironlake_set_fifo_underrun_reporting(struct drm_device *dev,
-						 enum pipe pipe, bool enable)
+void gen6_enable_rps_interrupts(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	uint32_t bit = (pipe == PIPE_A) ? DE_PIPEA_FIFO_UNDERRUN :
-					  DE_PIPEB_FIFO_UNDERRUN;
 
-	if (enable)
-		ironlake_enable_display_irq(dev_priv, bit);
-	else
-		ironlake_disable_display_irq(dev_priv, bit);
+	spin_lock_irq(&dev_priv->irq_lock);
+	WARN_ON(dev_priv->rps.pm_iir);
+	WARN_ON(I915_READ(gen6_pm_iir(dev_priv)) & dev_priv->rps.pm_events);
+	dev_priv->rps.interrupts_enabled = true;
+	gen6_enable_pm_irq(dev_priv, dev_priv->rps.pm_events);
+	spin_unlock_irq(&dev_priv->irq_lock);
 }
 
-static void ivybridge_set_fifo_underrun_reporting(struct drm_device *dev,
-						  enum pipe pipe,
-						  bool enable, bool old)
+void gen6_disable_rps_interrupts(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	if (enable) {
-		I915_WRITE(GEN7_ERR_INT, ERR_INT_FIFO_UNDERRUN(pipe));
-
-		if (!ivb_can_enable_err_int(dev))
-			return;
 
-		ironlake_enable_display_irq(dev_priv, DE_ERR_INT_IVB);
-	} else {
-		ironlake_disable_display_irq(dev_priv, DE_ERR_INT_IVB);
+	spin_lock_irq(&dev_priv->irq_lock);
+	dev_priv->rps.interrupts_enabled = false;
+	spin_unlock_irq(&dev_priv->irq_lock);
 
-		if (old &&
-		    I915_READ(GEN7_ERR_INT) & ERR_INT_FIFO_UNDERRUN(pipe)) {
-			DRM_ERROR("uncleared fifo underrun on pipe %c\n",
-				  pipe_name(pipe));
-		}
-	}
-}
+	cancel_work_sync(&dev_priv->rps.work);
 
-static void broadwell_set_fifo_underrun_reporting(struct drm_device *dev,
-						  enum pipe pipe, bool enable)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	I915_WRITE(GEN6_PMINTRMSK, INTEL_INFO(dev_priv)->gen >= 8 ?
+		   ~GEN8_PMINTR_REDIRECT_TO_NON_DISP : ~0);
+	I915_WRITE(gen6_pm_ier(dev_priv), I915_READ(gen6_pm_ier(dev_priv)) &
+				~dev_priv->rps.pm_events);
 
-	assert_spin_locked(&dev_priv->irq_lock);
+	spin_lock_irq(&dev_priv->irq_lock);
+	dev_priv->rps.pm_iir = 0;
+	spin_unlock_irq(&dev_priv->irq_lock);
 
-	if (enable)
-		dev_priv->de_irq_mask[pipe] &= ~GEN8_PIPE_FIFO_UNDERRUN;
-	else
-		dev_priv->de_irq_mask[pipe] |= GEN8_PIPE_FIFO_UNDERRUN;
-	I915_WRITE(GEN8_DE_PIPE_IMR(pipe), dev_priv->de_irq_mask[pipe]);
-	POSTING_READ(GEN8_DE_PIPE_IMR(pipe));
+	I915_WRITE(gen6_pm_iir(dev_priv), dev_priv->rps.pm_events);
 }
 
 /**
@@ -410,9 +321,9 @@
  * @interrupt_mask: mask of interrupt bits to update
  * @enabled_irq_mask: mask of interrupt bits to enable
  */
-static void ibx_display_interrupt_update(struct drm_i915_private *dev_priv,
-					 uint32_t interrupt_mask,
-					 uint32_t enabled_irq_mask)
+void ibx_display_interrupt_update(struct drm_i915_private *dev_priv,
+				  uint32_t interrupt_mask,
+				  uint32_t enabled_irq_mask)
 {
 	uint32_t sdeimr = I915_READ(SDEIMR);
 	sdeimr &= ~interrupt_mask;
@@ -426,160 +337,6 @@
 	I915_WRITE(SDEIMR, sdeimr);
 	POSTING_READ(SDEIMR);
 }
-#define ibx_enable_display_interrupt(dev_priv, bits) \
-	ibx_display_interrupt_update((dev_priv), (bits), (bits))
-#define ibx_disable_display_interrupt(dev_priv, bits) \
-	ibx_display_interrupt_update((dev_priv), (bits), 0)
-
-static void ibx_set_fifo_underrun_reporting(struct drm_device *dev,
-					    enum transcoder pch_transcoder,
-					    bool enable)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	uint32_t bit = (pch_transcoder == TRANSCODER_A) ?
-		       SDE_TRANSA_FIFO_UNDER : SDE_TRANSB_FIFO_UNDER;
-
-	if (enable)
-		ibx_enable_display_interrupt(dev_priv, bit);
-	else
-		ibx_disable_display_interrupt(dev_priv, bit);
-}
-
-static void cpt_set_fifo_underrun_reporting(struct drm_device *dev,
-					    enum transcoder pch_transcoder,
-					    bool enable, bool old)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-
-	if (enable) {
-		I915_WRITE(SERR_INT,
-			   SERR_INT_TRANS_FIFO_UNDERRUN(pch_transcoder));
-
-		if (!cpt_can_enable_serr_int(dev))
-			return;
-
-		ibx_enable_display_interrupt(dev_priv, SDE_ERROR_CPT);
-	} else {
-		ibx_disable_display_interrupt(dev_priv, SDE_ERROR_CPT);
-
-		if (old && I915_READ(SERR_INT) &
-		    SERR_INT_TRANS_FIFO_UNDERRUN(pch_transcoder)) {
-			DRM_ERROR("uncleared pch fifo underrun on pch transcoder %c\n",
-				  transcoder_name(pch_transcoder));
-		}
-	}
-}
-
-/**
- * intel_set_cpu_fifo_underrun_reporting - enable/disable FIFO underrun messages
- * @dev: drm device
- * @pipe: pipe
- * @enable: true if we want to report FIFO underrun errors, false otherwise
- *
- * This function makes us disable or enable CPU fifo underruns for a specific
- * pipe. Notice that on some Gens (e.g. IVB, HSW), disabling FIFO underrun
- * reporting for one pipe may also disable all the other CPU error interruts for
- * the other pipes, due to the fact that there's just one interrupt mask/enable
- * bit for all the pipes.
- *
- * Returns the previous state of underrun reporting.
- */
-static bool __intel_set_cpu_fifo_underrun_reporting(struct drm_device *dev,
-						    enum pipe pipe, bool enable)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_crtc *crtc = dev_priv->pipe_to_crtc_mapping[pipe];
-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
-	bool old;
-
-	assert_spin_locked(&dev_priv->irq_lock);
-
-	old = !intel_crtc->cpu_fifo_underrun_disabled;
-	intel_crtc->cpu_fifo_underrun_disabled = !enable;
-
-	if (INTEL_INFO(dev)->gen < 5 || IS_VALLEYVIEW(dev))
-		i9xx_set_fifo_underrun_reporting(dev, pipe, enable, old);
-	else if (IS_GEN5(dev) || IS_GEN6(dev))
-		ironlake_set_fifo_underrun_reporting(dev, pipe, enable);
-	else if (IS_GEN7(dev))
-		ivybridge_set_fifo_underrun_reporting(dev, pipe, enable, old);
-	else if (IS_GEN8(dev))
-		broadwell_set_fifo_underrun_reporting(dev, pipe, enable);
-
-	return old;
-}
-
-bool intel_set_cpu_fifo_underrun_reporting(struct drm_device *dev,
-					   enum pipe pipe, bool enable)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	unsigned long flags;
-	bool ret;
-
-	spin_lock_irqsave(&dev_priv->irq_lock, flags);
-	ret = __intel_set_cpu_fifo_underrun_reporting(dev, pipe, enable);
-	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
-
-	return ret;
-}
-
-static bool __cpu_fifo_underrun_reporting_enabled(struct drm_device *dev,
-						  enum pipe pipe)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_crtc *crtc = dev_priv->pipe_to_crtc_mapping[pipe];
-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
-
-	return !intel_crtc->cpu_fifo_underrun_disabled;
-}
-
-/**
- * intel_set_pch_fifo_underrun_reporting - enable/disable FIFO underrun messages
- * @dev: drm device
- * @pch_transcoder: the PCH transcoder (same as pipe on IVB and older)
- * @enable: true if we want to report FIFO underrun errors, false otherwise
- *
- * This function makes us disable or enable PCH fifo underruns for a specific
- * PCH transcoder. Notice that on some PCHs (e.g. CPT/PPT), disabling FIFO
- * underrun reporting for one transcoder may also disable all the other PCH
- * error interruts for the other transcoders, due to the fact that there's just
- * one interrupt mask/enable bit for all the transcoders.
- *
- * Returns the previous state of underrun reporting.
- */
-bool intel_set_pch_fifo_underrun_reporting(struct drm_device *dev,
-					   enum transcoder pch_transcoder,
-					   bool enable)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_crtc *crtc = dev_priv->pipe_to_crtc_mapping[pch_transcoder];
-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
-	unsigned long flags;
-	bool old;
-
-	/*
-	 * NOTE: Pre-LPT has a fixed cpu pipe -> pch transcoder mapping, but LPT
-	 * has only one pch transcoder A that all pipes can use. To avoid racy
-	 * pch transcoder -> pipe lookups from interrupt code simply store the
-	 * underrun statistics in crtc A. Since we never expose this anywhere
-	 * nor use it outside of the fifo underrun code here using the "wrong"
-	 * crtc on LPT won't cause issues.
-	 */
-
-	spin_lock_irqsave(&dev_priv->irq_lock, flags);
-
-	old = !intel_crtc->pch_fifo_underrun_disabled;
-	intel_crtc->pch_fifo_underrun_disabled = !enable;
-
-	if (HAS_PCH_IBX(dev))
-		ibx_set_fifo_underrun_reporting(dev, pch_transcoder, enable);
-	else
-		cpt_set_fifo_underrun_reporting(dev, pch_transcoder, enable, old);
-
-	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
-	return old;
-}
-
 
 static void
 __i915_enable_pipestat(struct drm_i915_private *dev_priv, enum pipe pipe,
@@ -589,6 +346,7 @@
 	u32 pipestat = I915_READ(reg) & PIPESTAT_INT_ENABLE_MASK;
 
 	assert_spin_locked(&dev_priv->irq_lock);
+	WARN_ON(!intel_irqs_enabled(dev_priv));
 
 	if (WARN_ONCE(enable_mask & ~PIPESTAT_INT_ENABLE_MASK ||
 		      status_mask & ~PIPESTAT_INT_STATUS_MASK,
@@ -615,6 +373,7 @@
 	u32 pipestat = I915_READ(reg) & PIPESTAT_INT_ENABLE_MASK;
 
 	assert_spin_locked(&dev_priv->irq_lock);
+	WARN_ON(!intel_irqs_enabled(dev_priv));
 
 	if (WARN_ONCE(enable_mask & ~PIPESTAT_INT_ENABLE_MASK ||
 		      status_mask & ~PIPESTAT_INT_STATUS_MASK,
@@ -694,19 +453,18 @@
 static void i915_enable_asle_pipestat(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	unsigned long irqflags;
 
 	if (!dev_priv->opregion.asle || !IS_MOBILE(dev))
 		return;
 
-	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
+	spin_lock_irq(&dev_priv->irq_lock);
 
 	i915_enable_pipestat(dev_priv, PIPE_B, PIPE_LEGACY_BLC_EVENT_STATUS);
 	if (INTEL_INFO(dev)->gen >= 4)
 		i915_enable_pipestat(dev_priv, PIPE_A,
 				     PIPE_LEGACY_BLC_EVENT_STATUS);
 
-	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
+	spin_unlock_irq(&dev_priv->irq_lock);
 }
 
 /**
@@ -1020,7 +778,7 @@
 
 	/* In vblank? */
 	if (in_vbl)
-		ret |= DRM_SCANOUTPOS_INVBL;
+		ret |= DRM_SCANOUTPOS_IN_VBLANK;
 
 	return ret;
 }
@@ -1094,46 +852,42 @@
 {
 	struct drm_i915_private *dev_priv =
 		container_of(work, struct drm_i915_private, dig_port_work);
-	unsigned long irqflags;
-	u32 long_port_mask, short_port_mask;
-	struct intel_digital_port *intel_dig_port;
-	int i, ret;
-	u32 old_bits = 0;
+	u32 long_mask, valid_mask, old_bits = 0;
+	int i;
 
-	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
-	long_port_mask = dev_priv->long_hpd_port_mask;
+	spin_lock_irq(&dev_priv->irq_lock);
+
+	long_mask = dev_priv->long_hpd_port_mask;
 	dev_priv->long_hpd_port_mask = 0;
-	short_port_mask = dev_priv->short_hpd_port_mask;
+
+	valid_mask = dev_priv->short_hpd_port_mask;
 	dev_priv->short_hpd_port_mask = 0;
-	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
+
+	valid_mask |= long_mask;
+	valid_mask &= ~dev_priv->hpd_event_bits;
+	spin_unlock_irq(&dev_priv->irq_lock);
 
 	for (i = 0; i < I915_MAX_PORTS; i++) {
-		bool valid = false;
-		bool long_hpd = false;
-		intel_dig_port = dev_priv->hpd_irq_port[i];
-		if (!intel_dig_port || !intel_dig_port->hpd_pulse)
+		struct intel_digital_port *port;
+
+		port = dev_priv->hpd_irq_port[i];
+		if (!port || !port->hpd_pulse)
 			continue;
 
-		if (long_port_mask & (1 << i))  {
-			valid = true;
-			long_hpd = true;
-		} else if (short_port_mask & (1 << i))
-			valid = true;
-
-		if (valid) {
-			ret = intel_dig_port->hpd_pulse(intel_dig_port, long_hpd);
-			if (ret == true) {
-				/* if we get true fallback to old school hpd */
-				old_bits |= (1 << intel_dig_port->base.hpd_pin);
-			}
-		}
+		if (valid_mask & (1 << i) &&
+		    port->hpd_pulse(port, long_mask & (1 << i)))
+			/* unhandled, fallback to old school hpd */
+			old_bits |= 1 << port->base.hpd_pin;
 	}
 
 	if (old_bits) {
-		spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
+		spin_lock_irq(&dev_priv->irq_lock);
 		dev_priv->hpd_event_bits |= old_bits;
-		spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
-		schedule_work(&dev_priv->hotplug_work);
+		spin_unlock_irq(&dev_priv->irq_lock);
+
+		mod_delayed_work(system_wq,
+				 &dev_priv->hotplug_work,
+				 msecs_to_jiffies(50));
 	}
 }
 
@@ -1145,13 +899,12 @@
 static void i915_hotplug_work_func(struct work_struct *work)
 {
 	struct drm_i915_private *dev_priv =
-		container_of(work, struct drm_i915_private, hotplug_work);
+		container_of(work, struct drm_i915_private, hotplug_work.work);
 	struct drm_device *dev = dev_priv->dev;
 	struct drm_mode_config *mode_config = &dev->mode_config;
 	struct intel_connector *intel_connector;
 	struct intel_encoder *intel_encoder;
 	struct drm_connector *connector;
-	unsigned long irqflags;
 	bool hpd_disabled = false;
 	bool changed = false;
 	u32 hpd_event_bits;
@@ -1159,7 +912,7 @@
 	mutex_lock(&mode_config->mutex);
 	DRM_DEBUG_KMS("running encoder hotplug functions\n");
 
-	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
+	spin_lock_irq(&dev_priv->irq_lock);
 
 	hpd_event_bits = dev_priv->hpd_event_bits;
 	dev_priv->hpd_event_bits = 0;
@@ -1193,7 +946,7 @@
 				 msecs_to_jiffies(I915_REENABLE_HOTPLUG_DELAY));
 	}
 
-	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
+	spin_unlock_irq(&dev_priv->irq_lock);
 
 	list_for_each_entry(connector, &mode_config->connector_list, head) {
 		intel_connector = to_intel_connector(connector);
@@ -1253,143 +1006,72 @@
 }
 
 static void notify_ring(struct drm_device *dev,
-			struct intel_engine_cs *ring)
+			struct intel_engine_cs *engine)
 {
-	if (!intel_ring_initialized(ring))
+	if (!intel_engine_initialized(engine))
 		return;
 
-	trace_i915_gem_request_complete(ring);
-
-	if (drm_core_check_feature(dev, DRIVER_MODESET))
-		intel_notify_mmio_flip(ring);
+	trace_i915_gem_ring_complete(engine);
+	atomic_inc(&engine->interrupts);
 
-	wake_up_all(&ring->irq_queue);
-	i915_queue_hangcheck(dev);
+	wake_up_all(&engine->irq_queue);
 }
 
-static u32 vlv_c0_residency(struct drm_i915_private *dev_priv,
-			    struct intel_rps_ei *rps_ei)
+static void vlv_c0_read(struct drm_i915_private *dev_priv,
+			struct intel_rps_ei *ei)
 {
-	u32 cz_ts, cz_freq_khz;
-	u32 render_count, media_count;
-	u32 elapsed_render, elapsed_media, elapsed_time;
-	u32 residency = 0;
-
-	cz_ts = vlv_punit_read(dev_priv, PUNIT_REG_CZ_TIMESTAMP);
-	cz_freq_khz = DIV_ROUND_CLOSEST(dev_priv->mem_freq * 1000, 4);
-
-	render_count = I915_READ(VLV_RENDER_C0_COUNT_REG);
-	media_count = I915_READ(VLV_MEDIA_C0_COUNT_REG);
-
-	if (rps_ei->cz_clock == 0) {
-		rps_ei->cz_clock = cz_ts;
-		rps_ei->render_c0 = render_count;
-		rps_ei->media_c0 = media_count;
-
-		return dev_priv->rps.cur_freq;
-	}
-
-	elapsed_time = cz_ts - rps_ei->cz_clock;
-	rps_ei->cz_clock = cz_ts;
-
-	elapsed_render = render_count - rps_ei->render_c0;
-	rps_ei->render_c0 = render_count;
-
-	elapsed_media = media_count - rps_ei->media_c0;
-	rps_ei->media_c0 = media_count;
-
-	/* Convert all the counters into common unit of milli sec */
-	elapsed_time /= VLV_CZ_CLOCK_TO_MILLI_SEC;
-	elapsed_render /=  cz_freq_khz;
-	elapsed_media /= cz_freq_khz;
-
-	/*
-	 * Calculate overall C0 residency percentage
-	 * only if elapsed time is non zero
-	 */
-	if (elapsed_time) {
-		residency =
-			((max(elapsed_render, elapsed_media) * 100)
-				/ elapsed_time);
-	}
-
-	return residency;
+	ei->cz_clock = vlv_punit_read(dev_priv, PUNIT_REG_CZ_TIMESTAMP);
+	ei->render_c0 = I915_READ(VLV_RENDER_C0_COUNT);
+	ei->media_c0 = I915_READ(VLV_MEDIA_C0_COUNT);
 }
 
-/**
- * vlv_calc_delay_from_C0_counters - Increase/Decrease freq based on GPU
- * busy-ness calculated from C0 counters of render & media power wells
- * @dev_priv: DRM device private
- *
- */
-static u32 vlv_calc_delay_from_C0_counters(struct drm_i915_private *dev_priv)
+static bool vlv_c0_above(struct drm_i915_private *dev_priv,
+			 const struct intel_rps_ei *old,
+			 const struct intel_rps_ei *now,
+			 int threshold)
 {
-	u32 residency_C0_up = 0, residency_C0_down = 0;
-	u8 new_delay, adj;
-
-	dev_priv->rps.ei_interrupt_count++;
-
-	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
+	u64 time = now->cz_clock - old->cz_clock;
+	u64 c0 = max(now->render_c0 - old->render_c0,
+		     now->media_c0 - old->media_c0);
 
+	c0 *= 100 * VLV_CZ_CLOCK_TO_MILLI_SEC * 4 / 1000;
+	time *= threshold * dev_priv->mem_freq;
+	return c0 >= time;
+}
 
-	if (dev_priv->rps.up_ei.cz_clock == 0) {
-		vlv_c0_residency(dev_priv, &dev_priv->rps.up_ei);
-		vlv_c0_residency(dev_priv, &dev_priv->rps.down_ei);
-		return dev_priv->rps.cur_freq;
-	}
+void gen6_rps_reset_ei(struct drm_i915_private *dev_priv)
+{
+	vlv_c0_read(dev_priv, &dev_priv->rps.down_ei);
+	dev_priv->rps.up_ei = dev_priv->rps.down_ei;
+}
 
+static u32 vlv_wa_c0_ei(struct drm_i915_private *dev_priv, u32 pm_iir)
+{
+	struct intel_rps_ei now;
+	u32 events = 0;
 
-	/*
-	 * To down throttle, C0 residency should be less than down threshold
-	 * for continous EI intervals. So calculate down EI counters
-	 * once in VLV_INT_COUNT_FOR_DOWN_EI
-	 */
-	if (dev_priv->rps.ei_interrupt_count == VLV_INT_COUNT_FOR_DOWN_EI) {
+	if ((pm_iir & (GEN6_PM_RP_DOWN_EI_EXPIRED | GEN6_PM_RP_UP_EI_EXPIRED)) == 0)
+		return 0;
 
-		dev_priv->rps.ei_interrupt_count = 0;
+	vlv_c0_read(dev_priv, &now);
 
-		residency_C0_down = vlv_c0_residency(dev_priv,
-						     &dev_priv->rps.down_ei);
-	} else {
-		residency_C0_up = vlv_c0_residency(dev_priv,
-						   &dev_priv->rps.up_ei);
+	if (pm_iir & GEN6_PM_RP_DOWN_EI_EXPIRED) {
+		if (!vlv_c0_above(dev_priv,
+				  &dev_priv->rps.down_ei, &now,
+				  dev_priv->rps.down_threshold))
+			events |= GEN6_PM_RP_DOWN_THRESHOLD;
+		dev_priv->rps.down_ei = now;
 	}
 
-	new_delay = dev_priv->rps.cur_freq;
-
-	adj = dev_priv->rps.last_adj;
-	/* C0 residency is greater than UP threshold. Increase Frequency */
-	if (residency_C0_up >= VLV_RP_UP_EI_THRESHOLD) {
-		if (adj > 0)
-			adj *= 2;
-		else
-			adj = 1;
-
-		if (dev_priv->rps.cur_freq < dev_priv->rps.max_freq_softlimit)
-			new_delay = dev_priv->rps.cur_freq + adj;
-
-		/*
-		 * For better performance, jump directly
-		 * to RPe if we're below it.
-		 */
-		if (new_delay < dev_priv->rps.efficient_freq)
-			new_delay = dev_priv->rps.efficient_freq;
-
-	} else if (!dev_priv->rps.ei_interrupt_count &&
-			(residency_C0_down < VLV_RP_DOWN_EI_THRESHOLD)) {
-		if (adj < 0)
-			adj *= 2;
-		else
-			adj = -1;
-		/*
-		 * This means, C0 residency is less than down threshold over
-		 * a period of VLV_INT_COUNT_FOR_DOWN_EI. So, reduce the freq
-		 */
-		if (dev_priv->rps.cur_freq > dev_priv->rps.min_freq_softlimit)
-			new_delay = dev_priv->rps.cur_freq + adj;
+	if (pm_iir & GEN6_PM_RP_UP_EI_EXPIRED) {
+		if (vlv_c0_above(dev_priv,
+				 &dev_priv->rps.up_ei, &now,
+				 dev_priv->rps.up_threshold))
+			events |= GEN6_PM_RP_UP_THRESHOLD;
+		dev_priv->rps.up_ei = now;
 	}
 
-	return new_delay;
+	return events;
 }
 
 static void gen6_pm_rps_work(struct work_struct *work)
@@ -1400,60 +1082,62 @@
 	int new_delay, adj;
 
 	spin_lock_irq(&dev_priv->irq_lock);
+	/* Speed up work cancelation during disabling rps interrupts. */
+	if (!dev_priv->rps.interrupts_enabled) {
+		spin_unlock_irq(&dev_priv->irq_lock);
+		return;
+	}
 	pm_iir = dev_priv->rps.pm_iir;
 	dev_priv->rps.pm_iir = 0;
-	if (INTEL_INFO(dev_priv->dev)->gen >= 8)
-		gen8_enable_pm_irq(dev_priv, dev_priv->pm_rps_events);
-	else {
-		/* Make sure not to corrupt PMIMR state used by ringbuffer */
-		gen6_enable_pm_irq(dev_priv, dev_priv->pm_rps_events);
-	}
+	/* Make sure not to corrupt PMIMR state used by ringbuffer on GEN6 */
+	gen6_enable_pm_irq(dev_priv, dev_priv->rps.pm_events);
 	spin_unlock_irq(&dev_priv->irq_lock);
 
 	/* Make sure we didn't queue anything we're not going to process. */
-	WARN_ON(pm_iir & ~dev_priv->pm_rps_events);
+	WARN_ON(pm_iir & ~dev_priv->rps.pm_events);
 
-	if ((pm_iir & dev_priv->pm_rps_events) == 0)
+	if ((pm_iir & dev_priv->rps.pm_events) == 0)
 		return;
 
 	mutex_lock(&dev_priv->rps.hw_lock);
 
+	pm_iir |= vlv_wa_c0_ei(dev_priv, pm_iir);
+
 	adj = dev_priv->rps.last_adj;
+	new_delay = dev_priv->rps.cur_freq;
 	if (pm_iir & GEN6_PM_RP_UP_THRESHOLD) {
 		if (adj > 0)
 			adj *= 2;
-		else {
-			/* CHV needs even encode values */
-			adj = IS_CHERRYVIEW(dev_priv->dev) ? 2 : 1;
-		}
-		new_delay = dev_priv->rps.cur_freq + adj;
-
+		else
+			adj = 1;
 		/*
 		 * For better performance, jump directly
 		 * to RPe if we're below it.
 		 */
-		if (new_delay < dev_priv->rps.efficient_freq)
+		if (new_delay < dev_priv->rps.efficient_freq - adj) {
 			new_delay = dev_priv->rps.efficient_freq;
+			adj = 0;
+		}
 	} else if (pm_iir & GEN6_PM_RP_DOWN_TIMEOUT) {
 		if (dev_priv->rps.cur_freq > dev_priv->rps.efficient_freq)
 			new_delay = dev_priv->rps.efficient_freq;
 		else
 			new_delay = dev_priv->rps.min_freq_softlimit;
 		adj = 0;
-	} else if (pm_iir & GEN6_PM_RP_UP_EI_EXPIRED) {
-		new_delay = vlv_calc_delay_from_C0_counters(dev_priv);
 	} else if (pm_iir & GEN6_PM_RP_DOWN_THRESHOLD) {
 		if (adj < 0)
 			adj *= 2;
-		else {
-			/* CHV needs even encode values */
-			adj = IS_CHERRYVIEW(dev_priv->dev) ? -2 : -1;
-		}
-		new_delay = dev_priv->rps.cur_freq + adj;
+		else
+			adj = -1;
 	} else { /* unknown event */
-		new_delay = dev_priv->rps.cur_freq;
+		adj = 0;
 	}
 
+	/* CHV needs even encode values */
+	if (IS_CHERRYVIEW(dev_priv->dev))
+		adj <<= 1;
+	new_delay += adj;
+
 	/* sysfs frequency interfaces may have snuck in while servicing the
 	 * interrupt
 	 */
@@ -1488,7 +1172,6 @@
 	u32 error_status, row, bank, subbank;
 	char *parity_event[6];
 	uint32_t misccpctl;
-	unsigned long flags;
 	uint8_t slice = 0;
 
 	/* We must turn off DOP level clock gating to access the L3 registers.
@@ -1547,9 +1230,9 @@
 
 out:
 	WARN_ON(dev_priv->l3_parity.which_slice);
-	spin_lock_irqsave(&dev_priv->irq_lock, flags);
+	spin_lock_irq(&dev_priv->irq_lock);
 	gen5_enable_gt_irq(dev_priv, GT_PARITY_ERROR(dev_priv->dev));
-	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
+	spin_unlock_irq(&dev_priv->irq_lock);
 
 	mutex_unlock(&dev_priv->dev->struct_mutex);
 }
@@ -1581,9 +1264,9 @@
 {
 	if (gt_iir &
 	    (GT_RENDER_USER_INTERRUPT | GT_RENDER_PIPECTL_NOTIFY_INTERRUPT))
-		notify_ring(dev, &dev_priv->ring[RCS]);
+		notify_ring(dev, &dev_priv->engine[RCS]);
 	if (gt_iir & ILK_BSD_USER_INTERRUPT)
-		notify_ring(dev, &dev_priv->ring[VCS]);
+		notify_ring(dev, &dev_priv->engine[VCS]);
 }
 
 static void snb_gt_irq_handler(struct drm_device *dev,
@@ -1593,40 +1276,28 @@
 
 	if (gt_iir &
 	    (GT_RENDER_USER_INTERRUPT | GT_RENDER_PIPECTL_NOTIFY_INTERRUPT))
-		notify_ring(dev, &dev_priv->ring[RCS]);
+		notify_ring(dev, &dev_priv->engine[RCS]);
 	if (gt_iir & GT_BSD_USER_INTERRUPT)
-		notify_ring(dev, &dev_priv->ring[VCS]);
+		notify_ring(dev, &dev_priv->engine[VCS]);
 	if (gt_iir & GT_BLT_USER_INTERRUPT)
-		notify_ring(dev, &dev_priv->ring[BCS]);
+		notify_ring(dev, &dev_priv->engine[BCS]);
 
 	if (gt_iir & (GT_BLT_CS_ERROR_INTERRUPT |
 		      GT_BSD_CS_ERROR_INTERRUPT |
 		      GT_RENDER_CS_MASTER_ERROR_INTERRUPT)) {
-		i915_handle_error(dev, false, "GT error interrupt 0x%08x",
-				  gt_iir);
+		i915_handle_error(dev, 0,
+				  "GT error interrupt 0x%08x", gt_iir);
 	}
 
 	if (gt_iir & GT_PARITY_ERROR(dev))
 		ivybridge_parity_error_irq_handler(dev, gt_iir);
 }
 
-static void gen8_rps_irq_handler(struct drm_i915_private *dev_priv, u32 pm_iir)
-{
-	if ((pm_iir & dev_priv->pm_rps_events) == 0)
-		return;
-
-	spin_lock(&dev_priv->irq_lock);
-	dev_priv->rps.pm_iir |= pm_iir & dev_priv->pm_rps_events;
-	gen8_disable_pm_irq(dev_priv, pm_iir & dev_priv->pm_rps_events);
-	spin_unlock(&dev_priv->irq_lock);
-
-	queue_work(dev_priv->wq, &dev_priv->rps.work);
-}
-
 static irqreturn_t gen8_gt_irq_handler(struct drm_device *dev,
 				       struct drm_i915_private *dev_priv,
 				       u32 master_ctl)
 {
+	struct intel_engine_cs *engine;
 	u32 rcs, bcs, vcs;
 	uint32_t tmp = 0;
 	irqreturn_t ret = IRQ_NONE;
@@ -1636,12 +1307,20 @@
 		if (tmp) {
 			I915_WRITE(GEN8_GT_IIR(0), tmp);
 			ret = IRQ_HANDLED;
+
 			rcs = tmp >> GEN8_RCS_IRQ_SHIFT;
-			bcs = tmp >> GEN8_BCS_IRQ_SHIFT;
+			engine = &dev_priv->engine[RCS];
 			if (rcs & GT_RENDER_USER_INTERRUPT)
-				notify_ring(dev, &dev_priv->ring[RCS]);
+				notify_ring(dev, engine);
+			if (rcs & GT_CONTEXT_SWITCH_INTERRUPT)
+				intel_execlists_irq_handler(engine);
+
+			bcs = tmp >> GEN8_BCS_IRQ_SHIFT;
+			engine = &dev_priv->engine[BCS];
 			if (bcs & GT_RENDER_USER_INTERRUPT)
-				notify_ring(dev, &dev_priv->ring[BCS]);
+				notify_ring(dev, engine);
+			if (bcs & GT_CONTEXT_SWITCH_INTERRUPT)
+				intel_execlists_irq_handler(engine);
 		} else
 			DRM_ERROR("The master control interrupt lied (GT0)!\n");
 	}
@@ -1651,23 +1330,31 @@
 		if (tmp) {
 			I915_WRITE(GEN8_GT_IIR(1), tmp);
 			ret = IRQ_HANDLED;
+
 			vcs = tmp >> GEN8_VCS1_IRQ_SHIFT;
+			engine = &dev_priv->engine[VCS];
 			if (vcs & GT_RENDER_USER_INTERRUPT)
-				notify_ring(dev, &dev_priv->ring[VCS]);
+				notify_ring(dev, engine);
+			if (vcs & GT_CONTEXT_SWITCH_INTERRUPT)
+				intel_execlists_irq_handler(engine);
+
 			vcs = tmp >> GEN8_VCS2_IRQ_SHIFT;
+			engine = &dev_priv->engine[VCS2];
 			if (vcs & GT_RENDER_USER_INTERRUPT)
-				notify_ring(dev, &dev_priv->ring[VCS2]);
+				notify_ring(dev, engine);
+			if (vcs & GT_CONTEXT_SWITCH_INTERRUPT)
+				intel_execlists_irq_handler(engine);
 		} else
 			DRM_ERROR("The master control interrupt lied (GT1)!\n");
 	}
 
 	if (master_ctl & GEN8_GT_PM_IRQ) {
 		tmp = I915_READ(GEN8_GT_IIR(2));
-		if (tmp & dev_priv->pm_rps_events) {
+		if (tmp & dev_priv->rps.pm_events) {
 			I915_WRITE(GEN8_GT_IIR(2),
-				   tmp & dev_priv->pm_rps_events);
+				   tmp & dev_priv->rps.pm_events);
 			ret = IRQ_HANDLED;
-			gen8_rps_irq_handler(dev_priv, tmp);
+			gen6_rps_irq_handler(dev_priv, tmp);
 		} else
 			DRM_ERROR("The master control interrupt lied (PM)!\n");
 	}
@@ -1677,9 +1364,13 @@
 		if (tmp) {
 			I915_WRITE(GEN8_GT_IIR(3), tmp);
 			ret = IRQ_HANDLED;
+
 			vcs = tmp >> GEN8_VECS_IRQ_SHIFT;
+			engine = &dev_priv->engine[VECS];
 			if (vcs & GT_RENDER_USER_INTERRUPT)
-				notify_ring(dev, &dev_priv->ring[VECS]);
+				notify_ring(dev, engine);
+			if (vcs & GT_CONTEXT_SWITCH_INTERRUPT)
+				intel_execlists_irq_handler(engine);
 		} else
 			DRM_ERROR("The master control interrupt lied (GT3)!\n");
 	}
@@ -1690,7 +1381,7 @@
 #define HPD_STORM_DETECT_PERIOD 1000
 #define HPD_STORM_THRESHOLD 5
 
-static int ilk_port_to_hotplug_shift(enum port port)
+static int pch_port_to_hotplug_shift(enum port port)
 {
 	switch (port) {
 	case PORT_A:
@@ -1706,7 +1397,7 @@
 	}
 }
 
-static int g4x_port_to_hotplug_shift(enum port port)
+static int i915_port_to_hotplug_shift(enum port port)
 {
 	switch (port) {
 	case PORT_A:
@@ -1764,15 +1455,17 @@
 		if (port && dev_priv->hpd_irq_port[port]) {
 			bool long_hpd;
 
-			if (IS_G4X(dev)) {
-				dig_shift = g4x_port_to_hotplug_shift(port);
-				long_hpd = (hotplug_trigger >> dig_shift) & PORTB_HOTPLUG_LONG_DETECT;
-			} else {
-				dig_shift = ilk_port_to_hotplug_shift(port);
+			if (HAS_PCH_SPLIT(dev)) {
+				dig_shift = pch_port_to_hotplug_shift(port);
 				long_hpd = (dig_hotplug_reg >> dig_shift) & PORTB_HOTPLUG_LONG_DETECT;
+			} else {
+				dig_shift = i915_port_to_hotplug_shift(port);
+				long_hpd = (hotplug_trigger >> dig_shift) & PORTB_HOTPLUG_LONG_DETECT;
 			}
 
-			DRM_DEBUG_DRIVER("digital hpd port %d %d\n", port, long_hpd);
+			DRM_DEBUG_DRIVER("digital hpd port %c - %s\n",
+					 port_name(port),
+					 long_hpd ? "long" : "short");
 			/* for long HPD pulses we want to have the digital queue happen,
 			   but we still want HPD storm detection to function. */
 			if (long_hpd) {
@@ -1843,7 +1536,9 @@
 	if (queue_dig)
 		queue_work(dev_priv->dp_wq, &dev_priv->dig_port_work);
 	if (queue_hp)
-		schedule_work(&dev_priv->hotplug_work);
+		mod_delayed_work(system_wq,
+				 &dev_priv->hotplug_work,
+				 msecs_to_jiffies(50));
 }
 
 static void gmbus_irq_handler(struct drm_device *dev)
@@ -1961,21 +1656,30 @@
  * the work queue. */
 static void gen6_rps_irq_handler(struct drm_i915_private *dev_priv, u32 pm_iir)
 {
-	if (pm_iir & dev_priv->pm_rps_events) {
+	/* TODO: RPS on GEN9+ is not supported yet. */
+	if (WARN_ONCE(INTEL_INFO(dev_priv)->gen >= 9,
+		      "GEN9+: unexpected RPS IRQ\n"))
+		return;
+
+	if (pm_iir & dev_priv->rps.pm_events) {
 		spin_lock(&dev_priv->irq_lock);
-		dev_priv->rps.pm_iir |= pm_iir & dev_priv->pm_rps_events;
-		gen6_disable_pm_irq(dev_priv, pm_iir & dev_priv->pm_rps_events);
+		gen6_disable_pm_irq(dev_priv, pm_iir & dev_priv->rps.pm_events);
+		if (dev_priv->rps.interrupts_enabled) {
+			dev_priv->rps.pm_iir |= pm_iir & dev_priv->rps.pm_events;
+			queue_work(dev_priv->wq, &dev_priv->rps.work);
+		}
 		spin_unlock(&dev_priv->irq_lock);
-
-		queue_work(dev_priv->wq, &dev_priv->rps.work);
 	}
 
+	if (INTEL_INFO(dev_priv)->gen >= 8)
+		return;
+
 	if (HAS_VEBOX(dev_priv->dev)) {
 		if (pm_iir & PM_VEBOX_USER_INTERRUPT)
-			notify_ring(dev_priv->dev, &dev_priv->ring[VECS]);
+			notify_ring(dev_priv->dev, &dev_priv->engine[VECS]);
 
 		if (pm_iir & PM_VEBOX_CS_ERROR_INTERRUPT) {
-			i915_handle_error(dev_priv->dev, false,
+			i915_handle_error(dev_priv->dev, 0,
 					  "VEBOX CS error interrupt 0x%08x",
 					  pm_iir);
 		}
@@ -1984,14 +1688,9 @@
 
 static bool intel_pipe_handle_vblank(struct drm_device *dev, enum pipe pipe)
 {
-	struct intel_crtc *crtc;
-
 	if (!drm_handle_vblank(dev, pipe))
 		return false;
 
-	crtc = to_intel_crtc(intel_get_crtc_for_pipe(dev, pipe));
-	wake_up(&crtc->vbl_wait);
-
 	return true;
 }
 
@@ -2002,7 +1701,7 @@
 	int pipe;
 
 	spin_lock(&dev_priv->irq_lock);
-	for_each_pipe(pipe) {
+	for_each_pipe(dev_priv, pipe) {
 		int reg;
 		u32 mask, iir_bit = 0;
 
@@ -2013,9 +1712,9 @@
 		 * we need to be careful that we only handle what we want to
 		 * handle.
 		 */
-		mask = 0;
-		if (__cpu_fifo_underrun_reporting_enabled(dev, pipe))
-			mask |= PIPE_FIFO_UNDERRUN_STATUS;
+
+		/* fifo underruns are filterered in the underrun handler. */
+		mask = PIPE_FIFO_UNDERRUN_STATUS;
 
 		switch (pipe) {
 		case PIPE_A:
@@ -2047,9 +1746,10 @@
 	}
 	spin_unlock(&dev_priv->irq_lock);
 
-	for_each_pipe(pipe) {
-		if (pipe_stats[pipe] & PIPE_START_VBLANK_INTERRUPT_STATUS)
-			intel_pipe_handle_vblank(dev, pipe);
+	for_each_pipe(dev_priv, pipe) {
+		if (pipe_stats[pipe] & PIPE_START_VBLANK_INTERRUPT_STATUS &&
+		    intel_pipe_handle_vblank(dev, pipe))
+			intel_check_page_flip(dev, pipe);
 
 		if (pipe_stats[pipe] & PLANE_FLIP_DONE_INT_STATUS_VLV) {
 			intel_prepare_page_flip(dev, pipe);
@@ -2059,9 +1759,8 @@
 		if (pipe_stats[pipe] & PIPE_CRC_DONE_INTERRUPT_STATUS)
 			i9xx_pipe_crc_irq_handler(dev, pipe);
 
-		if (pipe_stats[pipe] & PIPE_FIFO_UNDERRUN_STATUS &&
-		    intel_set_cpu_fifo_underrun_reporting(dev, pipe, false))
-			DRM_ERROR("pipe %c underrun\n", pipe_name(pipe));
+		if (pipe_stats[pipe] & PIPE_FIFO_UNDERRUN_STATUS)
+			intel_cpu_fifo_underrun_irq_handler(dev_priv, pipe);
 	}
 
 	if (pipe_stats[0] & PIPE_GMBUS_INTERRUPT_STATUS)
@@ -2216,7 +1915,7 @@
 		DRM_ERROR("PCH poison interrupt\n");
 
 	if (pch_iir & SDE_FDI_MASK)
-		for_each_pipe(pipe)
+		for_each_pipe(dev_priv, pipe)
 			DRM_DEBUG_DRIVER("  pipe %c FDI IIR: 0x%08x\n",
 					 pipe_name(pipe),
 					 I915_READ(FDI_RX_IIR(pipe)));
@@ -2228,14 +1927,10 @@
 		DRM_DEBUG_DRIVER("PCH transcoder CRC error interrupt\n");
 
 	if (pch_iir & SDE_TRANSA_FIFO_UNDER)
-		if (intel_set_pch_fifo_underrun_reporting(dev, TRANSCODER_A,
-							  false))
-			DRM_ERROR("PCH transcoder A FIFO underrun\n");
+		intel_pch_fifo_underrun_irq_handler(dev_priv, TRANSCODER_A);
 
 	if (pch_iir & SDE_TRANSB_FIFO_UNDER)
-		if (intel_set_pch_fifo_underrun_reporting(dev, TRANSCODER_B,
-							  false))
-			DRM_ERROR("PCH transcoder B FIFO underrun\n");
+		intel_pch_fifo_underrun_irq_handler(dev_priv, TRANSCODER_B);
 }
 
 static void ivb_err_int_handler(struct drm_device *dev)
@@ -2247,13 +1942,9 @@
 	if (err_int & ERR_INT_POISON)
 		DRM_ERROR("Poison interrupt\n");
 
-	for_each_pipe(pipe) {
-		if (err_int & ERR_INT_FIFO_UNDERRUN(pipe)) {
-			if (intel_set_cpu_fifo_underrun_reporting(dev, pipe,
-								  false))
-				DRM_ERROR("Pipe %c FIFO underrun\n",
-					  pipe_name(pipe));
-		}
+	for_each_pipe(dev_priv, pipe) {
+		if (err_int & ERR_INT_FIFO_UNDERRUN(pipe))
+			intel_cpu_fifo_underrun_irq_handler(dev_priv, pipe);
 
 		if (err_int & ERR_INT_PIPE_CRC_DONE(pipe)) {
 			if (IS_IVYBRIDGE(dev))
@@ -2275,19 +1966,13 @@
 		DRM_ERROR("PCH poison interrupt\n");
 
 	if (serr_int & SERR_INT_TRANS_A_FIFO_UNDERRUN)
-		if (intel_set_pch_fifo_underrun_reporting(dev, TRANSCODER_A,
-							  false))
-			DRM_ERROR("PCH transcoder A FIFO underrun\n");
+		intel_pch_fifo_underrun_irq_handler(dev_priv, TRANSCODER_A);
 
 	if (serr_int & SERR_INT_TRANS_B_FIFO_UNDERRUN)
-		if (intel_set_pch_fifo_underrun_reporting(dev, TRANSCODER_B,
-							  false))
-			DRM_ERROR("PCH transcoder B FIFO underrun\n");
+		intel_pch_fifo_underrun_irq_handler(dev_priv, TRANSCODER_B);
 
 	if (serr_int & SERR_INT_TRANS_C_FIFO_UNDERRUN)
-		if (intel_set_pch_fifo_underrun_reporting(dev, TRANSCODER_C,
-							  false))
-			DRM_ERROR("PCH transcoder C FIFO underrun\n");
+		intel_pch_fifo_underrun_irq_handler(dev_priv, TRANSCODER_C);
 
 	I915_WRITE(SERR_INT, serr_int);
 }
@@ -2324,7 +2009,7 @@
 		DRM_DEBUG_DRIVER("Audio CP change interrupt\n");
 
 	if (pch_iir & SDE_FDI_MASK_CPT)
-		for_each_pipe(pipe)
+		for_each_pipe(dev_priv, pipe)
 			DRM_DEBUG_DRIVER("  pipe %c FDI IIR: 0x%08x\n",
 					 pipe_name(pipe),
 					 I915_READ(FDI_RX_IIR(pipe)));
@@ -2347,14 +2032,13 @@
 	if (de_iir & DE_POISON)
 		DRM_ERROR("Poison interrupt\n");
 
-	for_each_pipe(pipe) {
-		if (de_iir & DE_PIPE_VBLANK(pipe))
-			intel_pipe_handle_vblank(dev, pipe);
+	for_each_pipe(dev_priv, pipe) {
+		if (de_iir & DE_PIPE_VBLANK(pipe) &&
+		    intel_pipe_handle_vblank(dev, pipe))
+			intel_check_page_flip(dev, pipe);
 
 		if (de_iir & DE_PIPE_FIFO_UNDERRUN(pipe))
-			if (intel_set_cpu_fifo_underrun_reporting(dev, pipe, false))
-				DRM_ERROR("Pipe %c FIFO underrun\n",
-					  pipe_name(pipe));
+			intel_cpu_fifo_underrun_irq_handler(dev_priv, pipe);
 
 		if (de_iir & DE_PIPE_CRC_DONE(pipe))
 			i9xx_pipe_crc_irq_handler(dev, pipe);
@@ -2397,9 +2081,10 @@
 	if (de_iir & DE_GSE_IVB)
 		intel_opregion_asle_intr(dev);
 
-	for_each_pipe(pipe) {
-		if (de_iir & (DE_PIPE_VBLANK_IVB(pipe)))
-			intel_pipe_handle_vblank(dev, pipe);
+	for_each_pipe(dev_priv, pipe) {
+		if (de_iir & (DE_PIPE_VBLANK_IVB(pipe)) &&
+		    intel_pipe_handle_vblank(dev, pipe))
+			intel_check_page_flip(dev, pipe);
 
 		/* plane/pipes map 1:1 on ilk+ */
 		if (de_iir & DE_PLANE_FLIP_DONE_IVB(pipe)) {
@@ -2431,7 +2116,7 @@
 {
 	struct drm_device *dev = arg;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	u32 de_iir, gt_iir, de_ier, sde_ier = 0;
+	u32 de_iir, gt_iir;
 	irqreturn_t ret = IRQ_NONE;
 
 	/* We get interrupts on unclaimed registers, so check for this before we
@@ -2439,26 +2124,21 @@
 	intel_uncore_check_errors(dev);
 
 	/* disable master interrupt before clearing iir  */
-	de_ier = I915_READ(DEIER);
-	I915_WRITE(DEIER, de_ier & ~DE_MASTER_IRQ_CONTROL);
-	POSTING_READ(DEIER);
+	__raw_i915_write32(dev_priv, DEIER, dev_priv->irq_enable & ~DE_MASTER_IRQ_CONTROL);
 
 	/* Disable south interrupts. We'll only write to SDEIIR once, so further
 	 * interrupts will will be stored on its back queue, and then we'll be
 	 * able to process them after we restore SDEIER (as soon as we restore
 	 * it, we'll get an interrupt if SDEIIR still has something to process
 	 * due to its back queue). */
-	if (!HAS_PCH_NOP(dev)) {
-		sde_ier = I915_READ(SDEIER);
-		I915_WRITE(SDEIER, 0);
-		POSTING_READ(SDEIER);
-	}
+	if (!HAS_PCH_NOP(dev))
+		__raw_i915_write32(dev_priv, SDEIER, 0);
 
 	/* Find, clear, then process each source of interrupt */
 
-	gt_iir = I915_READ(GTIIR);
+	gt_iir = __raw_i915_read32(dev_priv, GTIIR);
 	if (gt_iir) {
-		I915_WRITE(GTIIR, gt_iir);
+		__raw_i915_write32(dev_priv, GTIIR, gt_iir);
 		ret = IRQ_HANDLED;
 		if (INTEL_INFO(dev)->gen >= 6)
 			snb_gt_irq_handler(dev, dev_priv, gt_iir);
@@ -2466,9 +2146,9 @@
 			ilk_gt_irq_handler(dev, dev_priv, gt_iir);
 	}
 
-	de_iir = I915_READ(DEIIR);
+	de_iir = __raw_i915_read32(dev_priv, DEIIR);
 	if (de_iir) {
-		I915_WRITE(DEIIR, de_iir);
+		__raw_i915_write32(dev_priv, DEIIR, de_iir);
 		ret = IRQ_HANDLED;
 		if (INTEL_INFO(dev)->gen >= 7)
 			ivb_display_irq_handler(dev, de_iir);
@@ -2477,20 +2157,18 @@
 	}
 
 	if (INTEL_INFO(dev)->gen >= 6) {
-		u32 pm_iir = I915_READ(GEN6_PMIIR);
+		u32 pm_iir = __raw_i915_read32(dev_priv, GEN6_PMIIR);
 		if (pm_iir) {
-			I915_WRITE(GEN6_PMIIR, pm_iir);
+			__raw_i915_write32(dev_priv, GEN6_PMIIR, pm_iir);
 			ret = IRQ_HANDLED;
 			gen6_rps_irq_handler(dev_priv, pm_iir);
 		}
 	}
 
-	I915_WRITE(DEIER, de_ier);
-	POSTING_READ(DEIER);
-	if (!HAS_PCH_NOP(dev)) {
-		I915_WRITE(SDEIER, sde_ier);
-		POSTING_READ(SDEIER);
-	}
+	__raw_i915_write32(dev_priv, DEIER, dev_priv->irq_enable);
+	if (!HAS_PCH_NOP(dev))
+		__raw_i915_write32(dev_priv, SDEIER, ~0);
+	__raw_i915_posting_read(dev_priv, DEIER);
 
 	return ret;
 }
@@ -2503,6 +2181,11 @@
 	irqreturn_t ret = IRQ_NONE;
 	uint32_t tmp = 0;
 	enum pipe pipe;
+	u32 aux_mask = GEN8_AUX_CHANNEL_A;
+
+	if (IS_GEN9(dev))
+		aux_mask |=  GEN9_AUX_CHANNEL_B | GEN9_AUX_CHANNEL_C |
+			GEN9_AUX_CHANNEL_D;
 
 	master_ctl = I915_READ(GEN8_MASTER_IRQ);
 	master_ctl &= ~GEN8_MASTER_IRQ_CONTROL;
@@ -2535,7 +2218,8 @@
 		if (tmp) {
 			I915_WRITE(GEN8_DE_PORT_IIR, tmp);
 			ret = IRQ_HANDLED;
-			if (tmp & GEN8_AUX_CHANNEL_A)
+
+			if (tmp & aux_mask)
 				dp_aux_irq_handler(dev);
 			else
 				DRM_ERROR("Unexpected DE Port interrupt\n");
@@ -2544,8 +2228,8 @@
 			DRM_ERROR("The master control interrupt lied (DE PORT)!\n");
 	}
 
-	for_each_pipe(pipe) {
-		uint32_t pipe_iir;
+	for_each_pipe(dev_priv, pipe) {
+		uint32_t pipe_iir, flip_done = 0, fault_errors = 0;
 
 		if (!(master_ctl & GEN8_DE_PIPE_IRQ(pipe)))
 			continue;
@@ -2554,10 +2238,17 @@
 		if (pipe_iir) {
 			ret = IRQ_HANDLED;
 			I915_WRITE(GEN8_DE_PIPE_IIR(pipe), pipe_iir);
-			if (pipe_iir & GEN8_PIPE_VBLANK)
-				intel_pipe_handle_vblank(dev, pipe);
 
-			if (pipe_iir & GEN8_PIPE_PRIMARY_FLIP_DONE) {
+			if (pipe_iir & GEN8_PIPE_VBLANK &&
+			    intel_pipe_handle_vblank(dev, pipe))
+				intel_check_page_flip(dev, pipe);
+
+			if (IS_GEN9(dev))
+				flip_done = pipe_iir & GEN9_PIPE_PLANE1_FLIP_DONE;
+			else
+				flip_done = pipe_iir & GEN8_PIPE_PRIMARY_FLIP_DONE;
+
+			if (flip_done) {
 				intel_prepare_page_flip(dev, pipe);
 				intel_finish_page_flip_plane(dev, pipe);
 			}
@@ -2565,18 +2256,20 @@
 			if (pipe_iir & GEN8_PIPE_CDCLK_CRC_DONE)
 				hsw_pipe_crc_irq_handler(dev, pipe);
 
-			if (pipe_iir & GEN8_PIPE_FIFO_UNDERRUN) {
-				if (intel_set_cpu_fifo_underrun_reporting(dev, pipe,
-									  false))
-					DRM_ERROR("Pipe %c FIFO underrun\n",
-						  pipe_name(pipe));
-			}
+			if (pipe_iir & GEN8_PIPE_FIFO_UNDERRUN)
+				intel_cpu_fifo_underrun_irq_handler(dev_priv,
+								    pipe);
+
+
+			if (IS_GEN9(dev))
+				fault_errors = pipe_iir & GEN9_DE_PIPE_IRQ_FAULT_ERRORS;
+			else
+				fault_errors = pipe_iir & GEN8_DE_PIPE_IRQ_FAULT_ERRORS;
 
-			if (pipe_iir & GEN8_DE_PIPE_IRQ_FAULT_ERRORS) {
+			if (fault_errors)
 				DRM_ERROR("Fault errors on pipe %c\n: 0x%08x",
 					  pipe_name(pipe),
 					  pipe_iir & GEN8_DE_PIPE_IRQ_FAULT_ERRORS);
-			}
 		} else
 			DRM_ERROR("The master control interrupt lied (DE PIPE)!\n");
 	}
@@ -2606,7 +2299,7 @@
 static void i915_error_wake_up(struct drm_i915_private *dev_priv,
 			       bool reset_completed)
 {
-	struct intel_engine_cs *ring;
+	struct intel_engine_cs *engine;
 	int i;
 
 	/*
@@ -2617,8 +2310,8 @@
 	 */
 
 	/* Wake up __wait_seqno, potentially holding dev->struct_mutex. */
-	for_each_ring(ring, dev_priv, i)
-		wake_up_all(&ring->irq_queue);
+	for_each_engine(engine, dev_priv, i)
+		wake_up_all(&engine->irq_queue);
 
 	/* Wake up intel_crtc_wait_for_pending_flips, holding crtc->mutex. */
 	wake_up_all(&dev_priv->pending_flip_queue);
@@ -2662,7 +2355,7 @@
 	 * the reset in-progress bit is only ever set by code outside of this
 	 * work we don't need to worry about any other races.
 	 */
-	if (i915_reset_in_progress(error) && !i915_terminally_wedged(error)) {
+	if (i915_recovery_pending(error)) {
 		DRM_DEBUG_DRIVER("resetting chip\n");
 		kobject_uevent_env(&dev->primary->kdev->kobj, KOBJ_CHANGE,
 				   reset_event);
@@ -2698,9 +2391,7 @@
 			 * updates before
 			 * the counter increment.
 			 */
-			smp_mb__before_atomic();
-			atomic_inc(&dev_priv->gpu_error.reset_counter);
-
+			smp_mb__after_atomic();
 			kobject_uevent_env(&dev->primary->kdev->kobj,
 					   KOBJ_CHANGE, reset_done_event);
 		} else {
@@ -2763,7 +2454,7 @@
 
 	if (eir & I915_ERROR_MEMORY_REFRESH) {
 		pr_err("memory refresh error:\n");
-		for_each_pipe(pipe)
+		for_each_pipe(dev_priv, pipe)
 			pr_err("pipe %c stat: 0x%08x\n",
 			       pipe_name(pipe), I915_READ(PIPESTAT(pipe)));
 		/* pipestat has already been acked */
@@ -2817,7 +2508,7 @@
  * so userspace knows something bad happened (should trigger collection
  * of a ring dump etc.).
  */
-void i915_handle_error(struct drm_device *dev, bool wedged,
+void i915_handle_error(struct drm_device *dev, unsigned flags,
 		       const char *fmt, ...)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -2828,10 +2519,11 @@
 	vscnprintf(error_msg, sizeof(error_msg), fmt, args);
 	va_end(args);
 
-	i915_capture_error_state(dev, wedged, error_msg);
+	if ((flags & I915_HANG_SIMULATED) == 0)
+		i915_capture_error_state(dev, flags, error_msg);
 	i915_report_and_clear_eir(dev);
 
-	if (wedged) {
+	if (flags & I915_HANG_RESET) {
 		atomic_set_mask(I915_RESET_IN_PROGRESS_FLAG,
 				&dev_priv->gpu_error.reset_counter);
 
@@ -2860,52 +2552,6 @@
 	schedule_work(&dev_priv->gpu_error.work);
 }
 
-static void __always_unused i915_pageflip_stall_check(struct drm_device *dev, int pipe)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_crtc *crtc = dev_priv->pipe_to_crtc_mapping[pipe];
-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
-	struct drm_i915_gem_object *obj;
-	struct intel_unpin_work *work;
-	unsigned long flags;
-	bool stall_detected;
-
-	/* Ignore early vblank irqs */
-	if (intel_crtc == NULL)
-		return;
-
-	spin_lock_irqsave(&dev->event_lock, flags);
-	work = intel_crtc->unpin_work;
-
-	if (work == NULL ||
-	    atomic_read(&work->pending) >= INTEL_FLIP_COMPLETE ||
-	    !work->enable_stall_check) {
-		/* Either the pending flip IRQ arrived, or we're too early. Don't check */
-		spin_unlock_irqrestore(&dev->event_lock, flags);
-		return;
-	}
-
-	/* Potential stall - if we see that the flip has happened, assume a missed interrupt */
-	obj = work->pending_flip_obj;
-	if (INTEL_INFO(dev)->gen >= 4) {
-		int dspsurf = DSPSURF(intel_crtc->plane);
-		stall_detected = I915_HI_DISPBASE(I915_READ(dspsurf)) ==
-					i915_gem_obj_ggtt_offset(obj);
-	} else {
-		int dspaddr = DSPADDR(intel_crtc->plane);
-		stall_detected = I915_READ(dspaddr) == (i915_gem_obj_ggtt_offset(obj) +
-							crtc->y * crtc->primary->fb->pitches[0] +
-							crtc->x * crtc->primary->fb->bits_per_pixel/8);
-	}
-
-	spin_unlock_irqrestore(&dev->event_lock, flags);
-
-	if (stall_detected) {
-		DRM_DEBUG_DRIVER("Pageflip stall detected\n");
-		intel_prepare_page_flip(dev, intel_crtc->plane);
-	}
-}
-
 /* Called from drm generic code, passed 'crtc' which
  * we use as a pipe index
  */
@@ -3031,24 +2677,24 @@
 	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
 }
 
-static u32
-ring_last_seqno(struct intel_engine_cs *ring)
-{
-	return list_entry(ring->request_list.prev,
-			  struct drm_i915_gem_request, list)->seqno;
-}
-
 static bool
-ring_idle(struct intel_engine_cs *ring, u32 seqno)
+engine_idle(struct intel_engine_cs *engine)
 {
-	return (list_empty(&ring->request_list) ||
-		i915_seqno_passed(seqno, ring_last_seqno(ring)));
+	bool ret = true;
+
+	spin_lock(&engine->lock);
+	if (engine->last_request &&
+	    !__i915_request_complete__wa(engine->last_request))
+		ret = engine->is_idle(engine);
+	spin_unlock(&engine->lock);
+
+	return ret;
 }
 
 static bool
-ipehr_is_semaphore_wait(struct drm_device *dev, u32 ipehr)
+ipehr_is_semaphore_wait(struct drm_i915_private *i915, u32 ipehr)
 {
-	if (INTEL_INFO(dev)->gen >= 8) {
+	if (INTEL_INFO(i915)->gen >= 8) {
 		return (ipehr >> 23) == 0x1c;
 	} else {
 		ipehr &= ~MI_SEMAPHORE_SYNC_MASK;
@@ -3058,48 +2704,54 @@
 }
 
 static struct intel_engine_cs *
-semaphore_wait_to_signaller_ring(struct intel_engine_cs *ring, u32 ipehr, u64 offset)
+semaphore_wait_to_signaller_engine(struct intel_engine_cs *engine, u32 ipehr, u64 offset)
 {
-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
+	struct drm_i915_private *dev_priv = engine->i915;
 	struct intel_engine_cs *signaller;
 	int i;
 
 	if (INTEL_INFO(dev_priv->dev)->gen >= 8) {
-		for_each_ring(signaller, dev_priv, i) {
-			if (ring == signaller)
+		for_each_engine(signaller, dev_priv, i) {
+			if (engine == signaller)
 				continue;
 
-			if (offset == signaller->semaphore.signal_ggtt[ring->id])
+			if (offset == GEN8_SEMAPHORE_OFFSET(dev_priv, signaller->id, engine->id))
 				return signaller;
 		}
 	} else {
 		u32 sync_bits = ipehr & MI_SEMAPHORE_SYNC_MASK;
 
-		for_each_ring(signaller, dev_priv, i) {
-			if(ring == signaller)
+		for_each_engine(signaller, dev_priv, i) {
+			if(engine == signaller)
 				continue;
 
-			if (sync_bits == signaller->semaphore.mbox.wait[ring->id])
+			if (sync_bits == signaller->semaphore.mbox.wait[engine->id])
 				return signaller;
 		}
 	}
 
 	DRM_ERROR("No signaller ring found for ring %i, ipehr 0x%08x, offset 0x%016llx\n",
-		  ring->id, ipehr, offset);
+		  engine->id, ipehr, offset);
 
 	return NULL;
 }
 
 static struct intel_engine_cs *
-semaphore_waits_for(struct intel_engine_cs *ring, u32 *seqno)
+semaphore_waits_for(struct intel_engine_cs *engine, u32 *seqno)
 {
-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
+	struct drm_i915_private *dev_priv = engine->i915;
+	struct intel_ringbuffer *ring;
 	u32 cmd, ipehr, head;
 	u64 offset = 0;
 	int i, backwards;
 
-	ipehr = I915_READ(RING_IPEHR(ring->mmio_base));
-	if (!ipehr_is_semaphore_wait(ring->dev, ipehr))
+	ipehr = I915_READ(RING_IPEHR(engine->mmio_base));
+	if (!ipehr_is_semaphore_wait(engine->i915, ipehr))
+		return NULL;
+
+	/* XXX execlists */
+	ring = engine->default_context->ring[RCS].ring;
+	if (ring == NULL)
 		return NULL;
 
 	/*
@@ -3110,19 +2762,19 @@
 	 * point at at batch, and semaphores are always emitted into the
 	 * ringbuffer itself.
 	 */
-	head = I915_READ_HEAD(ring) & HEAD_ADDR;
-	backwards = (INTEL_INFO(ring->dev)->gen >= 8) ? 5 : 4;
+	head = I915_READ_HEAD(engine) & HEAD_ADDR;
+	backwards = (INTEL_INFO(dev_priv)->gen >= 8) ? 5 : 4;
 
 	for (i = backwards; i; --i) {
 		/*
 		 * Be paranoid and presume the hw has gone off into the wild -
-		 * our ring is smaller than what the hardware (and hence
+		 * our engine is smaller than what the hardware (and hence
 		 * HEAD_ADDR) allows. Also handles wrap-around.
 		 */
-		head &= ring->buffer->size - 1;
+		head &= ring->size - 1;
 
 		/* This here seems to blow up */
-		cmd = ioread32(ring->buffer->virtual_start + head);
+		cmd = ioread32(ring->virtual_start + head);
 		if (cmd == ipehr)
 			break;
 
@@ -3132,32 +2784,37 @@
 	if (!i)
 		return NULL;
 
-	*seqno = ioread32(ring->buffer->virtual_start + head + 4) + 1;
-	if (INTEL_INFO(ring->dev)->gen >= 8) {
-		offset = ioread32(ring->buffer->virtual_start + head + 12);
+	*seqno = ioread32(ring->virtual_start + head + 4) + 1;
+	if (INTEL_INFO(dev_priv)->gen >= 8) {
+		offset = ioread32(ring->virtual_start + head + 12);
 		offset <<= 32;
-		offset = ioread32(ring->buffer->virtual_start + head + 8);
+		offset = ioread32(ring->virtual_start + head + 8);
 	}
-	return semaphore_wait_to_signaller_ring(ring, ipehr, offset);
+	return semaphore_wait_to_signaller_engine(engine, ipehr, offset);
 }
 
-static int semaphore_passed(struct intel_engine_cs *ring)
+static int semaphore_passed(struct intel_engine_cs *engine)
 {
-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
+	struct drm_i915_private *dev_priv = engine->i915;
 	struct intel_engine_cs *signaller;
+	struct i915_gem_request *rq;
 	u32 seqno;
 
-	ring->hangcheck.deadlock++;
+	engine->hangcheck.deadlock++;
 
-	signaller = semaphore_waits_for(ring, &seqno);
+	if (engine->semaphore.wait == NULL)
+		return -1;
+
+	signaller = semaphore_waits_for(engine, &seqno);
 	if (signaller == NULL)
 		return -1;
 
 	/* Prevent pathological recursion due to driver bugs */
-	if (signaller->hangcheck.deadlock >= I915_NUM_RINGS)
+	if (signaller->hangcheck.deadlock >= I915_NUM_ENGINES)
 		return -1;
 
-	if (i915_seqno_passed(signaller->get_seqno(signaller, false), seqno))
+	rq = intel_engine_seqno_to_request(signaller, seqno);
+	if (rq == NULL || i915_request_complete(rq))
 		return 1;
 
 	/* cursory check for an unkickable deadlock */
@@ -3170,30 +2827,29 @@
 
 static void semaphore_clear_deadlocks(struct drm_i915_private *dev_priv)
 {
-	struct intel_engine_cs *ring;
+	struct intel_engine_cs *engine;
 	int i;
 
-	for_each_ring(ring, dev_priv, i)
-		ring->hangcheck.deadlock = 0;
+	for_each_engine(engine, dev_priv, i)
+		engine->hangcheck.deadlock = 0;
 }
 
-static enum intel_ring_hangcheck_action
-ring_stuck(struct intel_engine_cs *ring, u64 acthd)
+static enum intel_engine_hangcheck_action
+engine_stuck(struct intel_engine_cs *engine, u64 acthd)
 {
-	struct drm_device *dev = ring->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = engine->i915;
 	u32 tmp;
 
-	if (acthd != ring->hangcheck.acthd) {
-		if (acthd > ring->hangcheck.max_acthd) {
-			ring->hangcheck.max_acthd = acthd;
+	if (acthd != engine->hangcheck.acthd) {
+		if (acthd > engine->hangcheck.max_acthd) {
+			engine->hangcheck.max_acthd = acthd;
 			return HANGCHECK_ACTIVE;
 		}
 
 		return HANGCHECK_ACTIVE_LOOP;
 	}
 
-	if (IS_GEN2(dev))
+	if (IS_GEN2(dev_priv))
 		return HANGCHECK_HUNG;
 
 	/* Is the chip hanging on a WAIT_FOR_EVENT?
@@ -3201,24 +2857,24 @@
 	 * and break the hang. This should work on
 	 * all but the second generation chipsets.
 	 */
-	tmp = I915_READ_CTL(ring);
+	tmp = I915_READ_CTL(engine);
 	if (tmp & RING_WAIT) {
-		i915_handle_error(dev, false,
+		i915_handle_error(dev_priv->dev, 0,
 				  "Kicking stuck wait on %s",
-				  ring->name);
-		I915_WRITE_CTL(ring, tmp);
+				  engine->name);
+		I915_WRITE_CTL(engine, tmp);
 		return HANGCHECK_KICK;
 	}
 
-	if (INTEL_INFO(dev)->gen >= 6 && tmp & RING_WAIT_SEMAPHORE) {
-		switch (semaphore_passed(ring)) {
+	if (INTEL_INFO(dev_priv)->gen >= 6 && tmp & RING_WAIT_SEMAPHORE) {
+		switch (semaphore_passed(engine)) {
 		default:
 			return HANGCHECK_HUNG;
 		case 1:
-			i915_handle_error(dev, false,
+			i915_handle_error(dev_priv->dev, 0,
 					  "Kicking stuck semaphore on %s",
-					  ring->name);
-			I915_WRITE_CTL(ring, tmp);
+					  engine->name);
+			I915_WRITE_CTL(engine, tmp);
 			return HANGCHECK_KICK;
 		case 0:
 			return HANGCHECK_WAIT;
@@ -3230,135 +2886,173 @@
 
 /**
  * This is called when the chip hasn't reported back with completed
- * batchbuffers in a long time. We keep track per ring seqno progress and
+ * batchbuffers in a long time. We keep track per engine seqno progress and
  * if there are no progress, hangcheck score for that ring is increased.
  * Further, acthd is inspected to see if the ring is stuck. On stuck case
  * we kick the ring. If we see no progress on three subsequent calls
  * we assume chip is wedged and try to fix it by resetting the chip.
  */
-static void i915_hangcheck_elapsed(unsigned long data)
+static void i915_hangcheck_elapsed(struct work_struct *work)
 {
-	struct drm_device *dev = (struct drm_device *)data;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
+	struct drm_i915_private *dev_priv =
+		container_of(work, typeof(*dev_priv),
+			     gpu_error.hangcheck_work.work);
+	struct intel_engine_cs *engine;
+	bool stuck[I915_NUM_ENGINES] = { 0 };
+	int busy_count = 0, hung = 0;
 	int i;
-	int busy_count = 0, rings_hung = 0;
-	bool stuck[I915_NUM_RINGS] = { 0 };
 #define BUSY 1
 #define KICK 5
 #define HUNG 20
 
-	if (!i915.enable_hangcheck)
+	if (!i915_module.enable_hangcheck)
 		return;
 
-	for_each_ring(ring, dev_priv, i) {
+	for_each_engine(engine, dev_priv, i) {
 		u64 acthd;
 		u32 seqno;
+		u32 interrupts;
 		bool busy = true;
 
 		semaphore_clear_deadlocks(dev_priv);
 
-		seqno = ring->get_seqno(ring, false);
-		acthd = intel_ring_get_active_head(ring);
+		acthd = intel_engine_get_active_head(engine);
+		seqno = intel_engine_get_seqno(engine);
+		interrupts = atomic_read(&engine->interrupts);
+
+		if (engine_idle(engine)) {
+			if (waitqueue_active(&engine->irq_queue)) {
+				/* Issue a wake-up to catch stuck h/w. */
+				if (engine->hangcheck.action == HANGCHECK_IDLE_WAITERS &&
+						engine->hangcheck.interrupts == interrupts &&
+						!test_and_set_bit(engine->id, &dev_priv->gpu_error.missed_irq_rings)) {
+					if (!(dev_priv->gpu_error.test_irq_rings & intel_engine_flag(engine)))
+						DRM_ERROR("Hangcheck timer elapsed... %s idle\n",
+								engine->name);
+					else
+						DRM_INFO("Fake missed irq on %s\n",
+								engine->name);
+					wake_up_all(&engine->irq_queue);
+				}
 
-		if (ring->hangcheck.seqno == seqno) {
-			if (ring_idle(ring, seqno)) {
-				ring->hangcheck.action = HANGCHECK_IDLE;
-
-				if (waitqueue_active(&ring->irq_queue)) {
-					/* Issue a wake-up to catch stuck h/w. */
-					if (!test_and_set_bit(ring->id, &dev_priv->gpu_error.missed_irq_rings)) {
-						if (!(dev_priv->gpu_error.test_irq_rings & intel_ring_flag(ring)))
-							DRM_ERROR("Hangcheck timer elapsed... %s idle\n",
-								  ring->name);
-						else
-							DRM_INFO("Fake missed irq on %s\n",
-								 ring->name);
-						wake_up_all(&ring->irq_queue);
-					}
-					/* Safeguard against driver failure */
-					ring->hangcheck.score += BUSY;
-				} else
-					busy = false;
+				/* Safeguard against driver failure */
+				engine->hangcheck.score += BUSY;
+				engine->hangcheck.action = HANGCHECK_IDLE_WAITERS;
 			} else {
-				/* We always increment the hangcheck score
-				 * if the ring is busy and still processing
-				 * the same request, so that no single request
-				 * can run indefinitely (such as a chain of
-				 * batches). The only time we do not increment
-				 * the hangcheck score on this ring, if this
-				 * ring is in a legitimate wait for another
-				 * ring. In that case the waiting ring is a
-				 * victim and we want to be sure we catch the
-				 * right culprit. Then every time we do kick
-				 * the ring, add a small increment to the
-				 * score so that we can catch a batch that is
-				 * being repeatedly kicked and so responsible
-				 * for stalling the machine.
-				 */
-				ring->hangcheck.action = ring_stuck(ring,
-								    acthd);
-
-				switch (ring->hangcheck.action) {
+				busy = false;
+				engine->hangcheck.action = HANGCHECK_IDLE;
+			}
+		} else if (engine->hangcheck.seqno == seqno) {
+			/* We always increment the hangcheck score
+			 * if the ring is busy and still processing
+			 * the same request, so that no single request
+			 * can run indefinitely (such as a chain of
+			 * batches). The only time we do not increment
+			 * the hangcheck score on this ring, if this
+			 * ring is in a legitimate wait for another
+			 * ring. In that case the waiting ring is a
+			 * victim and we want to be sure we catch the
+			 * right culprit. Then every time we do kick
+			 * the ring, add a small increment to the
+			 * score so that we can catch a batch that is
+			 * being repeatedly kicked and so responsible
+			 * for stalling the machine.
+			 */
+			engine->hangcheck.action = engine_stuck(engine, acthd);
+			switch (engine->hangcheck.action) {
 				case HANGCHECK_IDLE:
+				case HANGCHECK_IDLE_WAITERS:
 				case HANGCHECK_WAIT:
 				case HANGCHECK_ACTIVE:
 					break;
 				case HANGCHECK_ACTIVE_LOOP:
-					ring->hangcheck.score += BUSY;
+					engine->hangcheck.score += BUSY;
 					break;
 				case HANGCHECK_KICK:
-					ring->hangcheck.score += KICK;
+					engine->hangcheck.score += KICK;
 					break;
 				case HANGCHECK_HUNG:
-					ring->hangcheck.score += HUNG;
+					engine->hangcheck.score += HUNG;
 					stuck[i] = true;
 					break;
-				}
 			}
 		} else {
-			ring->hangcheck.action = HANGCHECK_ACTIVE;
+			engine->hangcheck.action = HANGCHECK_ACTIVE;
 
 			/* Gradually reduce the count so that we catch DoS
 			 * attempts across multiple batches.
 			 */
-			if (ring->hangcheck.score > 0)
-				ring->hangcheck.score--;
+			if (engine->hangcheck.score > 0)
+				engine->hangcheck.score--;
 
-			ring->hangcheck.acthd = ring->hangcheck.max_acthd = 0;
+			engine->hangcheck.acthd = engine->hangcheck.max_acthd = 0;
 		}
 
-		ring->hangcheck.seqno = seqno;
-		ring->hangcheck.acthd = acthd;
+		engine->hangcheck.interrupts = interrupts;
+		engine->hangcheck.seqno = seqno;
+		engine->hangcheck.acthd = acthd;
 		busy_count += busy;
+
+		hung += engine->hangcheck.score >= HANGCHECK_SCORE_RING_HUNG;
 	}
 
-	for_each_ring(ring, dev_priv, i) {
-		if (ring->hangcheck.score >= HANGCHECK_SCORE_RING_HUNG) {
-			DRM_INFO("%s on %s\n",
-				 stuck[i] ? "stuck" : "no progress",
-				 ring->name);
-			rings_hung++;
+	if (hung) {
+		char msg[512];
+		int rings_stall, rings_stuck, len;
+
+		len = rings_stall = rings_stuck = 0;
+		for_each_engine(engine, dev_priv, i) {
+			if (engine->hangcheck.score >= HANGCHECK_SCORE_RING_HUNG &&
+			    stuck[i]) {
+				if (rings_stuck == 0)
+					len += snprintf(msg + len,
+							sizeof(msg)-len,
+							"Stuck on");
+				len += snprintf(msg + len, sizeof(msg)-len,
+						" %s,", engine->name);
+				rings_stuck++;
+			}
 		}
-	}
+		if (rings_stuck)
+			msg[--len] = '\0';
+
+		for_each_engine(engine, dev_priv, i) {
+			if (engine->hangcheck.score >= HANGCHECK_SCORE_RING_HUNG &&
+			    !stuck[i]) {
+				if (rings_stall == 0) {
+					if (rings_stuck)
+						len += snprintf(msg + len,
+								sizeof(msg)-len,
+								"; ");
+					len += snprintf(msg + len,
+							sizeof(msg)-len,
+							"No progress on");
+				}
+				len += snprintf(msg + len, sizeof(msg)-len,
+						" %s,", engine->name);
+				rings_stall++;
+			}
+		}
+		if (rings_stall)
+			msg[--len] = '\0';
 
-	if (rings_hung)
-		return i915_handle_error(dev, true, "Ring hung");
+		return i915_handle_error(dev_priv->dev, I915_HANG_RESET, msg);
+	}
 
 	if (busy_count)
 		/* Reset timer case chip hangs without another request
 		 * being added */
-		i915_queue_hangcheck(dev);
+		i915_queue_hangcheck(dev_priv->dev);
 }
 
 void i915_queue_hangcheck(struct drm_device *dev)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	if (!i915.enable_hangcheck)
+	if (!i915_module.enable_hangcheck)
 		return;
 
-	mod_timer(&dev_priv->gpu_error.hangcheck_timer,
-		  round_jiffies_up(jiffies + DRM_I915_HANGCHECK_JIFFIES));
+	/* Don't continually defer the hangcheck, but make sure it is active */
+	schedule_delayed_work(&to_i915(dev)->gpu_error.hangcheck_work,
+			      round_jiffies_up_relative(DRM_I915_HANGCHECK_JIFFIES));
 }
 
 static void ibx_irq_reset(struct drm_device *dev)
@@ -3409,7 +3103,7 @@
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	I915_WRITE(HWSTAM, 0xffffffff);
+	I915_WRITE(HWSTAM, ~0);
 
 	GEN5_IRQ_RESET(DE);
 	if (IS_GEN7(dev))
@@ -3420,10 +3114,22 @@
 	ibx_irq_reset(dev);
 }
 
+static void vlv_display_irq_reset(struct drm_i915_private *dev_priv)
+{
+	enum pipe pipe;
+
+	I915_WRITE(PORT_HOTPLUG_EN, 0);
+	I915_WRITE(PORT_HOTPLUG_STAT, I915_READ(PORT_HOTPLUG_STAT));
+
+	for_each_pipe(dev_priv, pipe)
+		I915_WRITE(PIPESTAT(pipe), 0xffff);
+
+	GEN5_IRQ_RESET(VLV_);
+}
+
 static void valleyview_irq_preinstall(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	int pipe;
 
 	/* VLV magic */
 	I915_WRITE(VLV_IMR, 0);
@@ -3431,22 +3137,11 @@
 	I915_WRITE(RING_IMR(GEN6_BSD_RING_BASE), 0);
 	I915_WRITE(RING_IMR(BLT_RING_BASE), 0);
 
-	/* and GT */
-	I915_WRITE(GTIIR, I915_READ(GTIIR));
-	I915_WRITE(GTIIR, I915_READ(GTIIR));
-
 	gen5_gt_irq_reset(dev);
 
-	I915_WRITE(DPINVGTT, 0xff);
+	I915_WRITE(DPINVGTT, DPINVGTT_STATUS_MASK);
 
-	I915_WRITE(PORT_HOTPLUG_EN, 0);
-	I915_WRITE(PORT_HOTPLUG_STAT, I915_READ(PORT_HOTPLUG_STAT));
-	for_each_pipe(pipe)
-		I915_WRITE(PIPESTAT(pipe), 0xffff);
-	I915_WRITE(VLV_IIR, 0xffffffff);
-	I915_WRITE(VLV_IMR, 0xffffffff);
-	I915_WRITE(VLV_IER, 0x0);
-	POSTING_READ(VLV_IER);
+	vlv_display_irq_reset(dev_priv);
 }
 
 static void gen8_gt_irq_reset(struct drm_i915_private *dev_priv)
@@ -3467,9 +3162,9 @@
 
 	gen8_gt_irq_reset(dev_priv);
 
-	for_each_pipe(pipe)
-		if (intel_display_power_enabled(dev_priv,
-						POWER_DOMAIN_PIPE(pipe)))
+	for_each_pipe(dev_priv, pipe)
+		if (intel_display_power_is_enabled(dev_priv,
+						   POWER_DOMAIN_PIPE(pipe)))
 			GEN8_IRQ_RESET_NDX(DE_PIPE, pipe);
 
 	GEN5_IRQ_RESET(GEN8_DE_PORT_);
@@ -3481,21 +3176,19 @@
 
 void gen8_irq_power_well_post_enable(struct drm_i915_private *dev_priv)
 {
-	unsigned long irqflags;
 	uint32_t extra_ier = GEN8_PIPE_VBLANK | GEN8_PIPE_FIFO_UNDERRUN;
 
-	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
+	spin_lock_irq(&dev_priv->irq_lock);
 	GEN8_IRQ_INIT_NDX(DE_PIPE, PIPE_B, dev_priv->de_irq_mask[PIPE_B],
 			  ~dev_priv->de_irq_mask[PIPE_B] | extra_ier);
 	GEN8_IRQ_INIT_NDX(DE_PIPE, PIPE_C, dev_priv->de_irq_mask[PIPE_C],
 			  ~dev_priv->de_irq_mask[PIPE_C] | extra_ier);
-	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
+	spin_unlock_irq(&dev_priv->irq_lock);
 }
 
 static void cherryview_irq_preinstall(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	int pipe;
 
 	I915_WRITE(GEN8_MASTER_IRQ, 0);
 	POSTING_READ(GEN8_MASTER_IRQ);
@@ -3504,37 +3197,25 @@
 
 	GEN5_IRQ_RESET(GEN8_PCU_);
 
-	POSTING_READ(GEN8_PCU_IIR);
-
 	I915_WRITE(DPINVGTT, DPINVGTT_STATUS_MASK_CHV);
 
-	I915_WRITE(PORT_HOTPLUG_EN, 0);
-	I915_WRITE(PORT_HOTPLUG_STAT, I915_READ(PORT_HOTPLUG_STAT));
-
-	for_each_pipe(pipe)
-		I915_WRITE(PIPESTAT(pipe), 0xffff);
-
-	I915_WRITE(VLV_IMR, 0xffffffff);
-	I915_WRITE(VLV_IER, 0x0);
-	I915_WRITE(VLV_IIR, 0xffffffff);
-	POSTING_READ(VLV_IIR);
+	vlv_display_irq_reset(dev_priv);
 }
 
 static void ibx_hpd_irq_setup(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_mode_config *mode_config = &dev->mode_config;
 	struct intel_encoder *intel_encoder;
 	u32 hotplug_irqs, hotplug, enabled_irqs = 0;
 
 	if (HAS_PCH_IBX(dev)) {
 		hotplug_irqs = SDE_HOTPLUG_MASK;
-		list_for_each_entry(intel_encoder, &mode_config->encoder_list, base.head)
+		for_each_intel_encoder(dev, intel_encoder)
 			if (dev_priv->hpd_stats[intel_encoder->hpd_pin].hpd_mark == HPD_ENABLED)
 				enabled_irqs |= hpd_ibx[intel_encoder->hpd_pin];
 	} else {
 		hotplug_irqs = SDE_HOTPLUG_MASK_CPT;
-		list_for_each_entry(intel_encoder, &mode_config->encoder_list, base.head)
+		for_each_intel_encoder(dev, intel_encoder)
 			if (dev_priv->hpd_stats[intel_encoder->hpd_pin].hpd_mark == HPD_ENABLED)
 				enabled_irqs |= hpd_cpt[intel_encoder->hpd_pin];
 	}
@@ -3597,7 +3278,7 @@
 	GEN5_IRQ_INIT(GT, dev_priv->gt_irq_mask, gt_irqs);
 
 	if (INTEL_INFO(dev)->gen >= 6) {
-		pm_irqs |= dev_priv->pm_rps_events;
+		pm_irqs |= dev_priv->rps.pm_events;
 
 		if (HAS_VEBOX(dev))
 			pm_irqs |= PM_VEBOX_USER_INTERRUPT;
@@ -3609,7 +3290,6 @@
 
 static int ironlake_irq_postinstall(struct drm_device *dev)
 {
-	unsigned long irqflags;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	u32 display_mask, extra_mask;
 
@@ -3631,8 +3311,9 @@
 	}
 
 	dev_priv->irq_mask = ~display_mask;
+	dev_priv->irq_enable = display_mask | extra_mask;
 
-	I915_WRITE(HWSTAM, 0xeffe);
+	I915_WRITE(HWSTAM, ~0);
 
 	ibx_irq_pre_postinstall(dev);
 
@@ -3648,9 +3329,9 @@
 		 * spinlocking not required here for correctness since interrupt
 		 * setup is guaranteed to run in single-threaded context. But we
 		 * need it to make the assert_spin_locked happy. */
-		spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
+		spin_lock_irq(&dev_priv->irq_lock);
 		ironlake_enable_display_irq(dev_priv, DE_PCU_EVENT);
-		spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
+		spin_unlock_irq(&dev_priv->irq_lock);
 	}
 
 	return 0;
@@ -3660,45 +3341,51 @@
 {
 	u32 pipestat_mask;
 	u32 iir_mask;
+	enum pipe pipe;
 
 	pipestat_mask = PIPESTAT_INT_STATUS_MASK |
 			PIPE_FIFO_UNDERRUN_STATUS;
 
-	I915_WRITE(PIPESTAT(PIPE_A), pipestat_mask);
-	I915_WRITE(PIPESTAT(PIPE_B), pipestat_mask);
+	for_each_pipe(dev_priv, pipe)
+		I915_WRITE(PIPESTAT(pipe), pipestat_mask);
 	POSTING_READ(PIPESTAT(PIPE_A));
 
 	pipestat_mask = PLANE_FLIP_DONE_INT_STATUS_VLV |
 			PIPE_CRC_DONE_INTERRUPT_STATUS;
 
-	i915_enable_pipestat(dev_priv, PIPE_A, pipestat_mask |
-					       PIPE_GMBUS_INTERRUPT_STATUS);
-	i915_enable_pipestat(dev_priv, PIPE_B, pipestat_mask);
+	i915_enable_pipestat(dev_priv, PIPE_A, PIPE_GMBUS_INTERRUPT_STATUS);
+	for_each_pipe(dev_priv, pipe)
+		      i915_enable_pipestat(dev_priv, pipe, pipestat_mask);
 
 	iir_mask = I915_DISPLAY_PORT_INTERRUPT |
 		   I915_DISPLAY_PIPE_A_EVENT_INTERRUPT |
 		   I915_DISPLAY_PIPE_B_EVENT_INTERRUPT;
+	if (IS_CHERRYVIEW(dev_priv))
+		iir_mask |= I915_DISPLAY_PIPE_C_EVENT_INTERRUPT;
 	dev_priv->irq_mask &= ~iir_mask;
 
 	I915_WRITE(VLV_IIR, iir_mask);
 	I915_WRITE(VLV_IIR, iir_mask);
-	I915_WRITE(VLV_IMR, dev_priv->irq_mask);
 	I915_WRITE(VLV_IER, ~dev_priv->irq_mask);
-	POSTING_READ(VLV_IER);
+	I915_WRITE(VLV_IMR, dev_priv->irq_mask);
+	POSTING_READ(VLV_IMR);
 }
 
 static void valleyview_display_irqs_uninstall(struct drm_i915_private *dev_priv)
 {
 	u32 pipestat_mask;
 	u32 iir_mask;
+	enum pipe pipe;
 
 	iir_mask = I915_DISPLAY_PORT_INTERRUPT |
 		   I915_DISPLAY_PIPE_A_EVENT_INTERRUPT |
 		   I915_DISPLAY_PIPE_B_EVENT_INTERRUPT;
+	if (IS_CHERRYVIEW(dev_priv))
+		iir_mask |= I915_DISPLAY_PIPE_C_EVENT_INTERRUPT;
 
 	dev_priv->irq_mask |= iir_mask;
-	I915_WRITE(VLV_IER, ~dev_priv->irq_mask);
 	I915_WRITE(VLV_IMR, dev_priv->irq_mask);
+	I915_WRITE(VLV_IER, ~dev_priv->irq_mask);
 	I915_WRITE(VLV_IIR, iir_mask);
 	I915_WRITE(VLV_IIR, iir_mask);
 	POSTING_READ(VLV_IIR);
@@ -3706,14 +3393,15 @@
 	pipestat_mask = PLANE_FLIP_DONE_INT_STATUS_VLV |
 			PIPE_CRC_DONE_INTERRUPT_STATUS;
 
-	i915_disable_pipestat(dev_priv, PIPE_A, pipestat_mask |
-					        PIPE_GMBUS_INTERRUPT_STATUS);
-	i915_disable_pipestat(dev_priv, PIPE_B, pipestat_mask);
+	i915_disable_pipestat(dev_priv, PIPE_A, PIPE_GMBUS_INTERRUPT_STATUS);
+	for_each_pipe(dev_priv, pipe)
+		i915_disable_pipestat(dev_priv, pipe, pipestat_mask);
 
 	pipestat_mask = PIPESTAT_INT_STATUS_MASK |
 			PIPE_FIFO_UNDERRUN_STATUS;
-	I915_WRITE(PIPESTAT(PIPE_A), pipestat_mask);
-	I915_WRITE(PIPESTAT(PIPE_B), pipestat_mask);
+
+	for_each_pipe(dev_priv, pipe)
+		I915_WRITE(PIPESTAT(pipe), pipestat_mask);
 	POSTING_READ(PIPESTAT(PIPE_A));
 }
 
@@ -3726,7 +3414,7 @@
 
 	dev_priv->display_irqs_enabled = true;
 
-	if (dev_priv->dev->irq_enabled)
+	if (intel_irqs_enabled(dev_priv))
 		valleyview_display_irqs_install(dev_priv);
 }
 
@@ -3739,34 +3427,36 @@
 
 	dev_priv->display_irqs_enabled = false;
 
-	if (dev_priv->dev->irq_enabled)
+	if (intel_irqs_enabled(dev_priv))
 		valleyview_display_irqs_uninstall(dev_priv);
 }
 
-static int valleyview_irq_postinstall(struct drm_device *dev)
+static void vlv_display_irq_postinstall(struct drm_i915_private *dev_priv)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	unsigned long irqflags;
-
 	dev_priv->irq_mask = ~0;
 
 	I915_WRITE(PORT_HOTPLUG_EN, 0);
 	POSTING_READ(PORT_HOTPLUG_EN);
 
-	I915_WRITE(VLV_IMR, dev_priv->irq_mask);
-	I915_WRITE(VLV_IER, ~dev_priv->irq_mask);
 	I915_WRITE(VLV_IIR, 0xffffffff);
-	POSTING_READ(VLV_IER);
+	I915_WRITE(VLV_IIR, 0xffffffff);
+	I915_WRITE(VLV_IER, ~dev_priv->irq_mask);
+	I915_WRITE(VLV_IMR, dev_priv->irq_mask);
+	POSTING_READ(VLV_IMR);
 
 	/* Interrupt setup is already guaranteed to be single-threaded, this is
 	 * just to make the assert_spin_locked check happy. */
-	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
+	spin_lock_irq(&dev_priv->irq_lock);
 	if (dev_priv->display_irqs_enabled)
 		valleyview_display_irqs_install(dev_priv);
-	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
+	spin_unlock_irq(&dev_priv->irq_lock);
+}
 
-	I915_WRITE(VLV_IIR, 0xffffffff);
-	I915_WRITE(VLV_IIR, 0xffffffff);
+static int valleyview_irq_postinstall(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	vlv_display_irq_postinstall(dev_priv);
 
 	gen5_gt_irq_postinstall(dev);
 
@@ -3783,46 +3473,60 @@
 
 static void gen8_gt_irq_postinstall(struct drm_i915_private *dev_priv)
 {
-	int i;
-
 	/* These are interrupts we'll toggle with the ring mask register */
 	uint32_t gt_interrupts[] = {
 		GT_RENDER_USER_INTERRUPT << GEN8_RCS_IRQ_SHIFT |
+			GT_CONTEXT_SWITCH_INTERRUPT << GEN8_RCS_IRQ_SHIFT |
 			GT_RENDER_L3_PARITY_ERROR_INTERRUPT |
-			GT_RENDER_USER_INTERRUPT << GEN8_BCS_IRQ_SHIFT,
+			GT_RENDER_USER_INTERRUPT << GEN8_BCS_IRQ_SHIFT |
+			GT_CONTEXT_SWITCH_INTERRUPT << GEN8_BCS_IRQ_SHIFT,
 		GT_RENDER_USER_INTERRUPT << GEN8_VCS1_IRQ_SHIFT |
-			GT_RENDER_USER_INTERRUPT << GEN8_VCS2_IRQ_SHIFT,
+			GT_CONTEXT_SWITCH_INTERRUPT << GEN8_VCS1_IRQ_SHIFT |
+			GT_RENDER_USER_INTERRUPT << GEN8_VCS2_IRQ_SHIFT |
+			GT_CONTEXT_SWITCH_INTERRUPT << GEN8_VCS2_IRQ_SHIFT,
 		0,
-		GT_RENDER_USER_INTERRUPT << GEN8_VECS_IRQ_SHIFT
+		GT_RENDER_USER_INTERRUPT << GEN8_VECS_IRQ_SHIFT |
+			GT_CONTEXT_SWITCH_INTERRUPT << GEN8_VECS_IRQ_SHIFT
 		};
 
-	for (i = 0; i < ARRAY_SIZE(gt_interrupts); i++)
-		GEN8_IRQ_INIT_NDX(GT, i, ~gt_interrupts[i], gt_interrupts[i]);
-
 	dev_priv->pm_irq_mask = 0xffffffff;
+	GEN8_IRQ_INIT_NDX(GT, 0, ~gt_interrupts[0], gt_interrupts[0]);
+	GEN8_IRQ_INIT_NDX(GT, 1, ~gt_interrupts[1], gt_interrupts[1]);
+	GEN8_IRQ_INIT_NDX(GT, 2, dev_priv->pm_irq_mask, dev_priv->rps.pm_events);
+	GEN8_IRQ_INIT_NDX(GT, 3, ~gt_interrupts[3], gt_interrupts[3]);
 }
 
 static void gen8_de_irq_postinstall(struct drm_i915_private *dev_priv)
 {
-	struct drm_device *dev = dev_priv->dev;
-	uint32_t de_pipe_masked = GEN8_PIPE_PRIMARY_FLIP_DONE |
-		GEN8_PIPE_CDCLK_CRC_DONE |
-		GEN8_DE_PIPE_IRQ_FAULT_ERRORS;
-	uint32_t de_pipe_enables = de_pipe_masked | GEN8_PIPE_VBLANK |
-		GEN8_PIPE_FIFO_UNDERRUN;
+	uint32_t de_pipe_masked = GEN8_PIPE_CDCLK_CRC_DONE;
+	uint32_t de_pipe_enables;
 	int pipe;
+	u32 aux_en = GEN8_AUX_CHANNEL_A;
+
+	if (IS_GEN9(dev_priv)) {
+		de_pipe_masked |= GEN9_PIPE_PLANE1_FLIP_DONE |
+				  GEN9_DE_PIPE_IRQ_FAULT_ERRORS;
+		aux_en |= GEN9_AUX_CHANNEL_B | GEN9_AUX_CHANNEL_C |
+			GEN9_AUX_CHANNEL_D;
+	} else
+		de_pipe_masked |= GEN8_PIPE_PRIMARY_FLIP_DONE |
+				  GEN8_DE_PIPE_IRQ_FAULT_ERRORS;
+
+	de_pipe_enables = de_pipe_masked | GEN8_PIPE_VBLANK |
+					   GEN8_PIPE_FIFO_UNDERRUN;
+
 	dev_priv->de_irq_mask[PIPE_A] = ~de_pipe_masked;
 	dev_priv->de_irq_mask[PIPE_B] = ~de_pipe_masked;
 	dev_priv->de_irq_mask[PIPE_C] = ~de_pipe_masked;
 
-	for_each_pipe(pipe)
-		if (intel_display_power_enabled(dev_priv,
+	for_each_pipe(dev_priv, pipe)
+		if (intel_display_power_is_enabled(dev_priv,
 				POWER_DOMAIN_PIPE(pipe)))
 			GEN8_IRQ_INIT_NDX(DE_PIPE, pipe,
 					  dev_priv->de_irq_mask[pipe],
 					  de_pipe_enables);
 
-	GEN5_IRQ_INIT(GEN8_DE_PORT_, ~GEN8_AUX_CHANNEL_A, GEN8_AUX_CHANNEL_A);
+	GEN5_IRQ_INIT(GEN8_DE_PORT_, ~aux_en, aux_en);
 }
 
 static int gen8_irq_postinstall(struct drm_device *dev)
@@ -3845,33 +3549,8 @@
 static int cherryview_irq_postinstall(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	u32 enable_mask = I915_DISPLAY_PORT_INTERRUPT |
-		I915_DISPLAY_PIPE_A_EVENT_INTERRUPT |
-		I915_DISPLAY_PIPE_B_EVENT_INTERRUPT |
-		I915_DISPLAY_PIPE_C_EVENT_INTERRUPT;
-	u32 pipestat_enable = PLANE_FLIP_DONE_INT_STATUS_VLV |
-		PIPE_CRC_DONE_INTERRUPT_STATUS;
-	unsigned long irqflags;
-	int pipe;
-
-	/*
-	 * Leave vblank interrupts masked initially.  enable/disable will
-	 * toggle them based on usage.
-	 */
-	dev_priv->irq_mask = ~enable_mask;
-
-	for_each_pipe(pipe)
-		I915_WRITE(PIPESTAT(pipe), 0xffff);
-
-	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
-	i915_enable_pipestat(dev_priv, PIPE_A, PIPE_GMBUS_INTERRUPT_STATUS);
-	for_each_pipe(pipe)
-		i915_enable_pipestat(dev_priv, pipe, pipestat_enable);
-	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
 
-	I915_WRITE(VLV_IIR, 0xffffffff);
-	I915_WRITE(VLV_IMR, dev_priv->irq_mask);
-	I915_WRITE(VLV_IER, enable_mask);
+	vlv_display_irq_postinstall(dev_priv);
 
 	gen8_gt_irq_postinstall(dev_priv);
 
@@ -3891,41 +3570,39 @@
 	gen8_irq_reset(dev);
 }
 
+static void vlv_display_irq_uninstall(struct drm_i915_private *dev_priv)
+{
+	/* Interrupt setup is already guaranteed to be single-threaded, this is
+	 * just to make the assert_spin_locked check happy. */
+	spin_lock_irq(&dev_priv->irq_lock);
+	if (dev_priv->display_irqs_enabled)
+		valleyview_display_irqs_uninstall(dev_priv);
+	spin_unlock_irq(&dev_priv->irq_lock);
+
+	vlv_display_irq_reset(dev_priv);
+
+	dev_priv->irq_mask = 0;
+}
+
 static void valleyview_irq_uninstall(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	unsigned long irqflags;
-	int pipe;
 
 	if (!dev_priv)
 		return;
 
 	I915_WRITE(VLV_MASTER_IER, 0);
 
-	for_each_pipe(pipe)
-		I915_WRITE(PIPESTAT(pipe), 0xffff);
-
-	I915_WRITE(HWSTAM, 0xffffffff);
-	I915_WRITE(PORT_HOTPLUG_EN, 0);
-	I915_WRITE(PORT_HOTPLUG_STAT, I915_READ(PORT_HOTPLUG_STAT));
-
-	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
-	if (dev_priv->display_irqs_enabled)
-		valleyview_display_irqs_uninstall(dev_priv);
-	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
+	gen5_gt_irq_reset(dev);
 
-	dev_priv->irq_mask = 0;
+	I915_WRITE(HWSTAM, ~0);
 
-	I915_WRITE(VLV_IIR, 0xffffffff);
-	I915_WRITE(VLV_IMR, 0xffffffff);
-	I915_WRITE(VLV_IER, 0x0);
-	POSTING_READ(VLV_IER);
+	vlv_display_irq_uninstall(dev_priv);
 }
 
 static void cherryview_irq_uninstall(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	int pipe;
 
 	if (!dev_priv)
 		return;
@@ -3933,44 +3610,11 @@
 	I915_WRITE(GEN8_MASTER_IRQ, 0);
 	POSTING_READ(GEN8_MASTER_IRQ);
 
-#define GEN8_IRQ_FINI_NDX(type, which)				\
-do {								\
-	I915_WRITE(GEN8_##type##_IMR(which), 0xffffffff);	\
-	I915_WRITE(GEN8_##type##_IER(which), 0);		\
-	I915_WRITE(GEN8_##type##_IIR(which), 0xffffffff);	\
-	POSTING_READ(GEN8_##type##_IIR(which));			\
-	I915_WRITE(GEN8_##type##_IIR(which), 0xffffffff);	\
-} while (0)
-
-#define GEN8_IRQ_FINI(type)				\
-do {							\
-	I915_WRITE(GEN8_##type##_IMR, 0xffffffff);	\
-	I915_WRITE(GEN8_##type##_IER, 0);		\
-	I915_WRITE(GEN8_##type##_IIR, 0xffffffff);	\
-	POSTING_READ(GEN8_##type##_IIR);		\
-	I915_WRITE(GEN8_##type##_IIR, 0xffffffff);	\
-} while (0)
-
-	GEN8_IRQ_FINI_NDX(GT, 0);
-	GEN8_IRQ_FINI_NDX(GT, 1);
-	GEN8_IRQ_FINI_NDX(GT, 2);
-	GEN8_IRQ_FINI_NDX(GT, 3);
-
-	GEN8_IRQ_FINI(PCU);
-
-#undef GEN8_IRQ_FINI
-#undef GEN8_IRQ_FINI_NDX
-
-	I915_WRITE(PORT_HOTPLUG_EN, 0);
-	I915_WRITE(PORT_HOTPLUG_STAT, I915_READ(PORT_HOTPLUG_STAT));
+	gen8_gt_irq_reset(dev_priv);
 
-	for_each_pipe(pipe)
-		I915_WRITE(PIPESTAT(pipe), 0xffff);
+	GEN5_IRQ_RESET(GEN8_PCU_);
 
-	I915_WRITE(VLV_IMR, 0xffffffff);
-	I915_WRITE(VLV_IER, 0x0);
-	I915_WRITE(VLV_IIR, 0xffffffff);
-	POSTING_READ(VLV_IIR);
+	vlv_display_irq_uninstall(dev_priv);
 }
 
 static void ironlake_irq_uninstall(struct drm_device *dev)
@@ -3988,7 +3632,7 @@
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	int pipe;
 
-	for_each_pipe(pipe)
+	for_each_pipe(dev_priv, pipe)
 		I915_WRITE(PIPESTAT(pipe), 0);
 	I915_WRITE16(IMR, 0xffff);
 	I915_WRITE16(IER, 0x0);
@@ -3998,7 +3642,6 @@
 static int i8xx_irq_postinstall(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	unsigned long irqflags;
 
 	I915_WRITE16(EMR,
 		     ~(I915_ERROR_PAGE_TABLE | I915_ERROR_MEMORY_REFRESH));
@@ -4021,10 +3664,10 @@
 
 	/* Interrupt setup is already guaranteed to be single-threaded, this is
 	 * just to make the assert_spin_locked check happy. */
-	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
+	spin_lock_irq(&dev_priv->irq_lock);
 	i915_enable_pipestat(dev_priv, PIPE_A, PIPE_CRC_DONE_INTERRUPT_STATUS);
 	i915_enable_pipestat(dev_priv, PIPE_B, PIPE_CRC_DONE_INTERRUPT_STATUS);
-	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
+	spin_unlock_irq(&dev_priv->irq_lock);
 
 	return 0;
 }
@@ -4042,7 +3685,7 @@
 		return false;
 
 	if ((iir & flip_pending) == 0)
-		return false;
+		goto check_page_flip;
 
 	intel_prepare_page_flip(dev, plane);
 
@@ -4053,11 +3696,14 @@
 	 * an interrupt per se, we watch for the change at vblank.
 	 */
 	if (I915_READ16(ISR) & flip_pending)
-		return false;
+		goto check_page_flip;
 
 	intel_finish_page_flip(dev, pipe);
-
 	return true;
+
+check_page_flip:
+	intel_check_page_flip(dev, pipe);
+	return false;
 }
 
 static irqreturn_t i8xx_irq_handler(int irq, void *arg)
@@ -4066,7 +3712,6 @@
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	u16 iir, new_iir;
 	u32 pipe_stats[2];
-	unsigned long irqflags;
 	int pipe;
 	u16 flip_mask =
 		I915_DISPLAY_PLANE_A_FLIP_PENDING_INTERRUPT |
@@ -4082,13 +3727,13 @@
 		 * It doesn't set the bit in iir again, but it still produces
 		 * interrupts (for non-MSI).
 		 */
-		spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
+		spin_lock(&dev_priv->irq_lock);
 		if (iir & I915_RENDER_COMMAND_PARSER_ERROR_INTERRUPT)
-			i915_handle_error(dev, false,
+			i915_handle_error(dev, 0,
 					  "Command parser error, iir 0x%08x",
 					  iir);
 
-		for_each_pipe(pipe) {
+		for_each_pipe(dev_priv, pipe) {
 			int reg = PIPESTAT(pipe);
 			pipe_stats[pipe] = I915_READ(reg);
 
@@ -4098,17 +3743,15 @@
 			if (pipe_stats[pipe] & 0x8000ffff)
 				I915_WRITE(reg, pipe_stats[pipe]);
 		}
-		spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
+		spin_unlock(&dev_priv->irq_lock);
 
 		I915_WRITE16(IIR, iir & ~flip_mask);
 		new_iir = I915_READ16(IIR); /* Flush posted writes */
 
-		i915_update_dri1_breadcrumb(dev);
-
 		if (iir & I915_USER_INTERRUPT)
-			notify_ring(dev, &dev_priv->ring[RCS]);
+			notify_ring(dev, &dev_priv->engine[RCS]);
 
-		for_each_pipe(pipe) {
+		for_each_pipe(dev_priv, pipe) {
 			int plane = pipe;
 			if (HAS_FBC(dev))
 				plane = !plane;
@@ -4120,9 +3763,9 @@
 			if (pipe_stats[pipe] & PIPE_CRC_DONE_INTERRUPT_STATUS)
 				i9xx_pipe_crc_irq_handler(dev, pipe);
 
-			if (pipe_stats[pipe] & PIPE_FIFO_UNDERRUN_STATUS &&
-			    intel_set_cpu_fifo_underrun_reporting(dev, pipe, false))
-				DRM_ERROR("pipe %c underrun\n", pipe_name(pipe));
+			if (pipe_stats[pipe] & PIPE_FIFO_UNDERRUN_STATUS)
+				intel_cpu_fifo_underrun_irq_handler(dev_priv,
+								    pipe);
 		}
 
 		iir = new_iir;
@@ -4136,7 +3779,7 @@
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	int pipe;
 
-	for_each_pipe(pipe) {
+	for_each_pipe(dev_priv, pipe) {
 		/* Clear enable bits; then clear status bits */
 		I915_WRITE(PIPESTAT(pipe), 0);
 		I915_WRITE(PIPESTAT(pipe), I915_READ(PIPESTAT(pipe)));
@@ -4156,8 +3799,8 @@
 		I915_WRITE(PORT_HOTPLUG_STAT, I915_READ(PORT_HOTPLUG_STAT));
 	}
 
-	I915_WRITE16(HWSTAM, 0xeffe);
-	for_each_pipe(pipe)
+	I915_WRITE16(HWSTAM, ~0);
+	for_each_pipe(dev_priv, pipe)
 		I915_WRITE(PIPESTAT(pipe), 0);
 	I915_WRITE(IMR, 0xffffffff);
 	I915_WRITE(IER, 0x0);
@@ -4168,7 +3811,6 @@
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	u32 enable_mask;
-	unsigned long irqflags;
 
 	I915_WRITE(EMR, ~(I915_ERROR_PAGE_TABLE | I915_ERROR_MEMORY_REFRESH));
 
@@ -4206,10 +3848,10 @@
 
 	/* Interrupt setup is already guaranteed to be single-threaded, this is
 	 * just to make the assert_spin_locked check happy. */
-	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
+	spin_lock_irq(&dev_priv->irq_lock);
 	i915_enable_pipestat(dev_priv, PIPE_A, PIPE_CRC_DONE_INTERRUPT_STATUS);
 	i915_enable_pipestat(dev_priv, PIPE_B, PIPE_CRC_DONE_INTERRUPT_STATUS);
-	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
+	spin_unlock_irq(&dev_priv->irq_lock);
 
 	return 0;
 }
@@ -4227,7 +3869,7 @@
 		return false;
 
 	if ((iir & flip_pending) == 0)
-		return false;
+		goto check_page_flip;
 
 	intel_prepare_page_flip(dev, plane);
 
@@ -4238,11 +3880,14 @@
 	 * an interrupt per se, we watch for the change at vblank.
 	 */
 	if (I915_READ(ISR) & flip_pending)
-		return false;
+		goto check_page_flip;
 
 	intel_finish_page_flip(dev, pipe);
-
 	return true;
+
+check_page_flip:
+	intel_check_page_flip(dev, pipe);
+	return false;
 }
 
 static irqreturn_t i915_irq_handler(int irq, void *arg)
@@ -4250,7 +3895,6 @@
 	struct drm_device *dev = arg;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	u32 iir, new_iir, pipe_stats[I915_MAX_PIPES];
-	unsigned long irqflags;
 	u32 flip_mask =
 		I915_DISPLAY_PLANE_A_FLIP_PENDING_INTERRUPT |
 		I915_DISPLAY_PLANE_B_FLIP_PENDING_INTERRUPT;
@@ -4266,13 +3910,13 @@
 		 * It doesn't set the bit in iir again, but it still produces
 		 * interrupts (for non-MSI).
 		 */
-		spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
+		spin_lock(&dev_priv->irq_lock);
 		if (iir & I915_RENDER_COMMAND_PARSER_ERROR_INTERRUPT)
-			i915_handle_error(dev, false,
+			i915_handle_error(dev, 0,
 					  "Command parser error, iir 0x%08x",
 					  iir);
 
-		for_each_pipe(pipe) {
+		for_each_pipe(dev_priv, pipe) {
 			int reg = PIPESTAT(pipe);
 			pipe_stats[pipe] = I915_READ(reg);
 
@@ -4282,7 +3926,7 @@
 				irq_received = true;
 			}
 		}
-		spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
+		spin_unlock(&dev_priv->irq_lock);
 
 		if (!irq_received)
 			break;
@@ -4296,9 +3940,9 @@
 		new_iir = I915_READ(IIR); /* Flush posted writes */
 
 		if (iir & I915_USER_INTERRUPT)
-			notify_ring(dev, &dev_priv->ring[RCS]);
+			notify_ring(dev, &dev_priv->engine[RCS]);
 
-		for_each_pipe(pipe) {
+		for_each_pipe(dev_priv, pipe) {
 			int plane = pipe;
 			if (HAS_FBC(dev))
 				plane = !plane;
@@ -4313,9 +3957,9 @@
 			if (pipe_stats[pipe] & PIPE_CRC_DONE_INTERRUPT_STATUS)
 				i9xx_pipe_crc_irq_handler(dev, pipe);
 
-			if (pipe_stats[pipe] & PIPE_FIFO_UNDERRUN_STATUS &&
-			    intel_set_cpu_fifo_underrun_reporting(dev, pipe, false))
-				DRM_ERROR("pipe %c underrun\n", pipe_name(pipe));
+			if (pipe_stats[pipe] & PIPE_FIFO_UNDERRUN_STATUS)
+				intel_cpu_fifo_underrun_irq_handler(dev_priv,
+								    pipe);
 		}
 
 		if (blc_event || (iir & I915_ASLE_INTERRUPT))
@@ -4340,8 +3984,6 @@
 		iir = new_iir;
 	} while (iir & ~flip_mask);
 
-	i915_update_dri1_breadcrumb(dev);
-
 	return ret;
 }
 
@@ -4355,8 +3997,8 @@
 		I915_WRITE(PORT_HOTPLUG_STAT, I915_READ(PORT_HOTPLUG_STAT));
 	}
 
-	I915_WRITE16(HWSTAM, 0xffff);
-	for_each_pipe(pipe) {
+	I915_WRITE16(HWSTAM, ~0);
+	for_each_pipe(dev_priv, pipe) {
 		/* Clear enable bits; then clear status bits */
 		I915_WRITE(PIPESTAT(pipe), 0);
 		I915_WRITE(PIPESTAT(pipe), I915_READ(PIPESTAT(pipe)));
@@ -4375,8 +4017,8 @@
 	I915_WRITE(PORT_HOTPLUG_EN, 0);
 	I915_WRITE(PORT_HOTPLUG_STAT, I915_READ(PORT_HOTPLUG_STAT));
 
-	I915_WRITE(HWSTAM, 0xeffe);
-	for_each_pipe(pipe)
+	I915_WRITE(HWSTAM, ~0);
+	for_each_pipe(dev_priv, pipe)
 		I915_WRITE(PIPESTAT(pipe), 0);
 	I915_WRITE(IMR, 0xffffffff);
 	I915_WRITE(IER, 0x0);
@@ -4388,7 +4030,6 @@
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	u32 enable_mask;
 	u32 error_mask;
-	unsigned long irqflags;
 
 	/* Unmask the interrupts that we always want on. */
 	dev_priv->irq_mask = ~(I915_ASLE_INTERRUPT |
@@ -4409,11 +4050,11 @@
 
 	/* Interrupt setup is already guaranteed to be single-threaded, this is
 	 * just to make the assert_spin_locked check happy. */
-	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
+	spin_lock_irq(&dev_priv->irq_lock);
 	i915_enable_pipestat(dev_priv, PIPE_A, PIPE_GMBUS_INTERRUPT_STATUS);
 	i915_enable_pipestat(dev_priv, PIPE_A, PIPE_CRC_DONE_INTERRUPT_STATUS);
 	i915_enable_pipestat(dev_priv, PIPE_B, PIPE_CRC_DONE_INTERRUPT_STATUS);
-	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
+	spin_unlock_irq(&dev_priv->irq_lock);
 
 	/*
 	 * Enable some error detection, note the instruction error mask
@@ -4445,7 +4086,6 @@
 static void i915_hpd_irq_setup(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_mode_config *mode_config = &dev->mode_config;
 	struct intel_encoder *intel_encoder;
 	u32 hotplug_en;
 
@@ -4456,7 +4096,7 @@
 		hotplug_en &= ~HOTPLUG_INT_EN_MASK;
 		/* Note HDMI and DP share hotplug bits */
 		/* enable bits are the same for all generations */
-		list_for_each_entry(intel_encoder, &mode_config->encoder_list, base.head)
+		for_each_intel_encoder(dev, intel_encoder)
 			if (dev_priv->hpd_stats[intel_encoder->hpd_pin].hpd_mark == HPD_ENABLED)
 				hotplug_en |= hpd_mask_i915[intel_encoder->hpd_pin];
 		/* Programming the CRT detection parameters tends
@@ -4479,7 +4119,6 @@
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	u32 iir, new_iir;
 	u32 pipe_stats[I915_MAX_PIPES];
-	unsigned long irqflags;
 	int ret = IRQ_NONE, pipe;
 	u32 flip_mask =
 		I915_DISPLAY_PLANE_A_FLIP_PENDING_INTERRUPT |
@@ -4496,13 +4135,13 @@
 		 * It doesn't set the bit in iir again, but it still produces
 		 * interrupts (for non-MSI).
 		 */
-		spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
+		spin_lock(&dev_priv->irq_lock);
 		if (iir & I915_RENDER_COMMAND_PARSER_ERROR_INTERRUPT)
-			i915_handle_error(dev, false,
+			i915_handle_error(dev, 0,
 					  "Command parser error, iir 0x%08x",
 					  iir);
 
-		for_each_pipe(pipe) {
+		for_each_pipe(dev_priv, pipe) {
 			int reg = PIPESTAT(pipe);
 			pipe_stats[pipe] = I915_READ(reg);
 
@@ -4514,7 +4153,7 @@
 				irq_received = true;
 			}
 		}
-		spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
+		spin_unlock(&dev_priv->irq_lock);
 
 		if (!irq_received)
 			break;
@@ -4529,11 +4168,11 @@
 		new_iir = I915_READ(IIR); /* Flush posted writes */
 
 		if (iir & I915_USER_INTERRUPT)
-			notify_ring(dev, &dev_priv->ring[RCS]);
+			notify_ring(dev, &dev_priv->engine[RCS]);
 		if (iir & I915_BSD_USER_INTERRUPT)
-			notify_ring(dev, &dev_priv->ring[VCS]);
+			notify_ring(dev, &dev_priv->engine[VCS]);
 
-		for_each_pipe(pipe) {
+		for_each_pipe(dev_priv, pipe) {
 			if (pipe_stats[pipe] & PIPE_START_VBLANK_INTERRUPT_STATUS &&
 			    i915_handle_vblank(dev, pipe, pipe, iir))
 				flip_mask &= ~DISPLAY_PLANE_FLIP_PENDING(pipe);
@@ -4544,9 +4183,8 @@
 			if (pipe_stats[pipe] & PIPE_CRC_DONE_INTERRUPT_STATUS)
 				i9xx_pipe_crc_irq_handler(dev, pipe);
 
-			if (pipe_stats[pipe] & PIPE_FIFO_UNDERRUN_STATUS &&
-			    intel_set_cpu_fifo_underrun_reporting(dev, pipe, false))
-				DRM_ERROR("pipe %c underrun\n", pipe_name(pipe));
+			if (pipe_stats[pipe] & PIPE_FIFO_UNDERRUN_STATUS)
+				intel_cpu_fifo_underrun_irq_handler(dev_priv, pipe);
 		}
 
 		if (blc_event || (iir & I915_ASLE_INTERRUPT))
@@ -4573,8 +4211,6 @@
 		iir = new_iir;
 	}
 
-	i915_update_dri1_breadcrumb(dev);
-
 	return ret;
 }
 
@@ -4589,31 +4225,30 @@
 	I915_WRITE(PORT_HOTPLUG_EN, 0);
 	I915_WRITE(PORT_HOTPLUG_STAT, I915_READ(PORT_HOTPLUG_STAT));
 
-	I915_WRITE(HWSTAM, 0xffffffff);
-	for_each_pipe(pipe)
+	I915_WRITE(HWSTAM, ~0);
+	for_each_pipe(dev_priv, pipe)
 		I915_WRITE(PIPESTAT(pipe), 0);
 	I915_WRITE(IMR, 0xffffffff);
 	I915_WRITE(IER, 0x0);
 
-	for_each_pipe(pipe)
+	for_each_pipe(dev_priv, pipe)
 		I915_WRITE(PIPESTAT(pipe),
 			   I915_READ(PIPESTAT(pipe)) & 0x8000ffff);
 	I915_WRITE(IIR, I915_READ(IIR));
 }
 
-static void intel_hpd_irq_reenable(struct work_struct *work)
+static void intel_hpd_irq_reenable_work(struct work_struct *work)
 {
 	struct drm_i915_private *dev_priv =
 		container_of(work, typeof(*dev_priv),
 			     hotplug_reenable_work.work);
 	struct drm_device *dev = dev_priv->dev;
 	struct drm_mode_config *mode_config = &dev->mode_config;
-	unsigned long irqflags;
 	int i;
 
 	intel_runtime_pm_get(dev_priv);
 
-	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
+	spin_lock_irq(&dev_priv->irq_lock);
 	for (i = (HPD_NONE + 1); i < HPD_NUM_PINS; i++) {
 		struct drm_connector *connector;
 
@@ -4637,43 +4272,46 @@
 	}
 	if (dev_priv->display.hpd_irq_setup)
 		dev_priv->display.hpd_irq_setup(dev);
-	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
+	spin_unlock_irq(&dev_priv->irq_lock);
 
 	intel_runtime_pm_put(dev_priv);
 }
 
-void intel_irq_init(struct drm_device *dev)
+/**
+ * intel_irq_init - initializes irq support
+ * @dev_priv: i915 device instance
+ *
+ * This function initializes all the irq support including work items, timers
+ * and all the vtables. It does not setup the interrupt itself though.
+ */
+void intel_irq_init(struct drm_i915_private *dev_priv)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_device *dev = dev_priv->dev;
 
-	INIT_WORK(&dev_priv->hotplug_work, i915_hotplug_work_func);
+	INIT_DELAYED_WORK(&dev_priv->hotplug_work, i915_hotplug_work_func);
 	INIT_WORK(&dev_priv->dig_port_work, i915_digport_work_func);
 	INIT_WORK(&dev_priv->gpu_error.work, i915_error_work_func);
 	INIT_WORK(&dev_priv->rps.work, gen6_pm_rps_work);
 	INIT_WORK(&dev_priv->l3_parity.error_work, ivybridge_parity_work);
 
 	/* Let's track the enabled rps events */
-	if (IS_VALLEYVIEW(dev))
-		/* WaGsvRC0ResidenncyMethod:VLV */
-		dev_priv->pm_rps_events = GEN6_PM_RP_UP_EI_EXPIRED;
+	if (IS_VALLEYVIEW(dev_priv) && !IS_CHERRYVIEW(dev_priv))
+		/* WaGsvRC0ResidencyMethod:vlv */
+		dev_priv->rps.pm_events = GEN6_PM_RP_DOWN_EI_EXPIRED | GEN6_PM_RP_UP_EI_EXPIRED;
 	else
-		dev_priv->pm_rps_events = GEN6_PM_RPS_EVENTS;
+		dev_priv->rps.pm_events = GEN6_PM_RPS_EVENTS;
 
-	setup_timer(&dev_priv->gpu_error.hangcheck_timer,
-		    i915_hangcheck_elapsed,
-		    (unsigned long) dev);
+	INIT_DELAYED_WORK(&dev_priv->gpu_error.hangcheck_work,
+			  i915_hangcheck_elapsed);
 	INIT_DELAYED_WORK(&dev_priv->hotplug_reenable_work,
-			  intel_hpd_irq_reenable);
+			  intel_hpd_irq_reenable_work);
 
 	pm_qos_add_request(&dev_priv->pm_qos, PM_QOS_CPU_DMA_LATENCY, PM_QOS_DEFAULT_VALUE);
 
-	/* Haven't installed the IRQ handler yet */
-	dev_priv->pm._irqs_disabled = true;
-
-	if (IS_GEN2(dev)) {
+	if (IS_GEN2(dev_priv)) {
 		dev->max_vblank_count = 0;
 		dev->driver->get_vblank_counter = i8xx_get_vblank_counter;
-	} else if (IS_G4X(dev) || INTEL_INFO(dev)->gen >= 5) {
+	} else if (IS_G4X(dev_priv) || INTEL_INFO(dev_priv)->gen >= 5) {
 		dev->max_vblank_count = 0xffffffff; /* full 32 bit counter */
 		dev->driver->get_vblank_counter = gm45_get_vblank_counter;
 	} else {
@@ -4681,12 +4319,20 @@
 		dev->max_vblank_count = 0xffffff; /* only 24 bits of frame count */
 	}
 
+	/*
+	 * Opt out of the vblank disable timer on everything except gen2.
+	 * Gen2 doesn't have a hardware frame counter and so depends on
+	 * vblank interrupts to produce sane vblank seuquence numbers.
+	 */
+	if (!IS_GEN2(dev_priv))
+		dev->vblank_disable_immediate = true;
+
 	if (drm_core_check_feature(dev, DRIVER_MODESET)) {
 		dev->driver->get_vblank_timestamp = i915_get_vblank_timestamp;
 		dev->driver->get_scanout_position = i915_get_crtc_scanoutpos;
 	}
 
-	if (IS_CHERRYVIEW(dev)) {
+	if (IS_CHERRYVIEW(dev_priv)) {
 		dev->driver->irq_handler = cherryview_irq_handler;
 		dev->driver->irq_preinstall = cherryview_irq_preinstall;
 		dev->driver->irq_postinstall = cherryview_irq_postinstall;
@@ -4694,7 +4340,7 @@
 		dev->driver->enable_vblank = valleyview_enable_vblank;
 		dev->driver->disable_vblank = valleyview_disable_vblank;
 		dev_priv->display.hpd_irq_setup = i915_hpd_irq_setup;
-	} else if (IS_VALLEYVIEW(dev)) {
+	} else if (IS_VALLEYVIEW(dev_priv)) {
 		dev->driver->irq_handler = valleyview_irq_handler;
 		dev->driver->irq_preinstall = valleyview_irq_preinstall;
 		dev->driver->irq_postinstall = valleyview_irq_postinstall;
@@ -4702,7 +4348,7 @@
 		dev->driver->enable_vblank = valleyview_enable_vblank;
 		dev->driver->disable_vblank = valleyview_disable_vblank;
 		dev_priv->display.hpd_irq_setup = i915_hpd_irq_setup;
-	} else if (IS_GEN8(dev)) {
+	} else if (INTEL_INFO(dev_priv)->gen >= 8) {
 		dev->driver->irq_handler = gen8_irq_handler;
 		dev->driver->irq_preinstall = gen8_irq_reset;
 		dev->driver->irq_postinstall = gen8_irq_postinstall;
@@ -4719,12 +4365,12 @@
 		dev->driver->disable_vblank = ironlake_disable_vblank;
 		dev_priv->display.hpd_irq_setup = ibx_hpd_irq_setup;
 	} else {
-		if (INTEL_INFO(dev)->gen == 2) {
+		if (INTEL_INFO(dev_priv)->gen == 2) {
 			dev->driver->irq_preinstall = i8xx_irq_preinstall;
 			dev->driver->irq_postinstall = i8xx_irq_postinstall;
 			dev->driver->irq_handler = i8xx_irq_handler;
 			dev->driver->irq_uninstall = i8xx_irq_uninstall;
-		} else if (INTEL_INFO(dev)->gen == 3) {
+		} else if (INTEL_INFO(dev_priv)->gen == 3) {
 			dev->driver->irq_preinstall = i915_irq_preinstall;
 			dev->driver->irq_postinstall = i915_irq_postinstall;
 			dev->driver->irq_uninstall = i915_irq_uninstall;
@@ -4742,12 +4388,23 @@
 	}
 }
 
-void intel_hpd_init(struct drm_device *dev)
+/**
+ * intel_hpd_init - initializes and enables hpd support
+ * @dev_priv: i915 device instance
+ *
+ * This function enables the hotplug support. It requires that interrupts have
+ * already been enabled with intel_irq_init_hw(). From this point on hotplug and
+ * poll request can run concurrently to other code, so locking rules must be
+ * obeyed.
+ *
+ * This is a separate step from interrupt enabling to simplify the locking rules
+ * in the driver load and resume code.
+ */
+void intel_hpd_init(struct drm_i915_private *dev_priv)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_device *dev = dev_priv->dev;
 	struct drm_mode_config *mode_config = &dev->mode_config;
 	struct drm_connector *connector;
-	unsigned long irqflags;
 	int i;
 
 	for (i = 1; i < HPD_NUM_PINS; i++) {
@@ -4765,27 +4422,72 @@
 
 	/* Interrupt setup is already guaranteed to be single-threaded, this is
 	 * just to make the assert_spin_locked checks happy. */
-	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
+	spin_lock_irq(&dev_priv->irq_lock);
 	if (dev_priv->display.hpd_irq_setup)
 		dev_priv->display.hpd_irq_setup(dev);
-	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
+	spin_unlock_irq(&dev_priv->irq_lock);
 }
 
-/* Disable interrupts so we can allow runtime PM. */
-void intel_runtime_pm_disable_interrupts(struct drm_device *dev)
+/**
+ * intel_irq_install - enables the hardware interrupt
+ * @dev_priv: i915 device instance
+ *
+ * This function enables the hardware interrupt handling, but leaves the hotplug
+ * handling still disabled. It is called after intel_irq_init().
+ *
+ * In the driver load and resume code we need working interrupts in a few places
+ * but don't want to deal with the hassle of concurrent probe and hotplug
+ * workers. Hence the split into this two-stage approach.
+ */
+int intel_irq_install(struct drm_i915_private *dev_priv)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	/*
+	 * We enable some interrupt sources in our postinstall hooks, so mark
+	 * interrupts as enabled _before_ actually enabling them to avoid
+	 * special cases in our ordering checks.
+	 */
+	dev_priv->pm.irqs_enabled = true;
 
-	dev->driver->irq_uninstall(dev);
-	dev_priv->pm._irqs_disabled = true;
+	return drm_irq_install(dev_priv->dev, dev_priv->dev->pdev->irq);
 }
 
-/* Restore interrupts so we can recover from runtime PM. */
-void intel_runtime_pm_restore_interrupts(struct drm_device *dev)
+/**
+ * intel_irq_uninstall - finilizes all irq handling
+ * @dev_priv: i915 device instance
+ *
+ * This stops interrupt and hotplug handling and unregisters and frees all
+ * resources acquired in the init functions.
+ */
+void intel_irq_uninstall(struct drm_i915_private *dev_priv)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	drm_irq_uninstall(dev_priv->dev);
+	intel_hpd_cancel_work(dev_priv);
+	dev_priv->pm.irqs_enabled = false;
+}
+
+/**
+ * intel_runtime_pm_disable_interrupts - runtime interrupt disabling
+ * @dev_priv: i915 device instance
+ *
+ * This function is used to disable interrupts at runtime, both in the runtime
+ * pm and the system suspend/resume code.
+ */
+void intel_runtime_pm_disable_interrupts(struct drm_i915_private *dev_priv)
+{
+	dev_priv->dev->driver->irq_uninstall(dev_priv->dev);
+	dev_priv->pm.irqs_enabled = false;
+}
 
-	dev_priv->pm._irqs_disabled = false;
-	dev->driver->irq_preinstall(dev);
-	dev->driver->irq_postinstall(dev);
+/**
+ * intel_runtime_pm_enable_interrupts - runtime interrupt enabling
+ * @dev_priv: i915 device instance
+ *
+ * This function is used to enable interrupts at runtime, both in the runtime
+ * pm and the system suspend/resume code.
+ */
+void intel_runtime_pm_enable_interrupts(struct drm_i915_private *dev_priv)
+{
+	dev_priv->pm.irqs_enabled = true;
+	dev_priv->dev->driver->irq_preinstall(dev_priv->dev);
+	dev_priv->dev->driver->irq_postinstall(dev_priv->dev);
 }
diff -urN a/drivers/gpu/drm/i915/i915_params.c b/drivers/gpu/drm/i915/i915_params.c
--- a/drivers/gpu/drm/i915/i915_params.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_params.c	2014-11-22 18:37:18.021014051 -0700
@@ -24,7 +24,7 @@
 
 #include "i915_drv.h"
 
-struct i915_params i915 __read_mostly = {
+struct i915_module_parameters i915_module __read_mostly = {
 	.modeset = -1,
 	.panel_ignore_lid = 1,
 	.powersave = 1,
@@ -35,6 +35,7 @@
 	.vbt_sdvo_panel_type = -1,
 	.enable_rc6 = -1,
 	.enable_fbc = -1,
+	.enable_execlists = 0,
 	.enable_hangcheck = true,
 	.enable_ppgtt = -1,
 	.enable_psr = 0,
@@ -52,26 +53,26 @@
 	.mmio_debug = 0,
 };
 
-module_param_named(modeset, i915.modeset, int, 0400);
+module_param_named(modeset, i915_module.modeset, int, 0400);
 MODULE_PARM_DESC(modeset,
 	"Use kernel modesetting [KMS] (0=DRM_I915_KMS from .config, "
 	"1=on, -1=force vga console preference [default])");
 
-module_param_named(panel_ignore_lid, i915.panel_ignore_lid, int, 0600);
+module_param_named(panel_ignore_lid, i915_module.panel_ignore_lid, int, 0600);
 MODULE_PARM_DESC(panel_ignore_lid,
 	"Override lid status (0=autodetect, 1=autodetect disabled [default], "
 	"-1=force lid closed, -2=force lid open)");
 
-module_param_named(powersave, i915.powersave, int, 0600);
+module_param_named(powersave, i915_module.powersave, int, 0600);
 MODULE_PARM_DESC(powersave,
 	"Enable powersavings, fbc, downclocking, etc. (default: true)");
 
-module_param_named(semaphores, i915.semaphores, int, 0400);
+module_param_named(semaphores, i915_module.semaphores, int, 0400);
 MODULE_PARM_DESC(semaphores,
 	"Use semaphores for inter-ring sync "
 	"(default: -1 (use per-chip defaults))");
 
-module_param_named(enable_rc6, i915.enable_rc6, int, 0400);
+module_param_named(enable_rc6, i915_module.enable_rc6, int, 0400);
 MODULE_PARM_DESC(enable_rc6,
 	"Enable power-saving render C-state 6. "
 	"Different stages can be selected via bitmask values "
@@ -79,69 +80,74 @@
 	"For example, 3 would enable rc6 and deep rc6, and 7 would enable everything. "
 	"default: -1 (use per-chip default)");
 
-module_param_named(enable_fbc, i915.enable_fbc, int, 0600);
+module_param_named(enable_fbc, i915_module.enable_fbc, int, 0600);
 MODULE_PARM_DESC(enable_fbc,
 	"Enable frame buffer compression for power savings "
 	"(default: -1 (use per-chip default))");
 
-module_param_named(lvds_downclock, i915.lvds_downclock, int, 0400);
+module_param_named(lvds_downclock, i915_module.lvds_downclock, int, 0400);
 MODULE_PARM_DESC(lvds_downclock,
 	"Use panel (LVDS/eDP) downclocking for power savings "
 	"(default: false)");
 
-module_param_named(lvds_channel_mode, i915.lvds_channel_mode, int, 0600);
+module_param_named(lvds_channel_mode, i915_module.lvds_channel_mode, int, 0600);
 MODULE_PARM_DESC(lvds_channel_mode,
 	 "Specify LVDS channel mode "
 	 "(0=probe BIOS [default], 1=single-channel, 2=dual-channel)");
 
-module_param_named(lvds_use_ssc, i915.panel_use_ssc, int, 0600);
+module_param_named(lvds_use_ssc, i915_module.panel_use_ssc, int, 0600);
 MODULE_PARM_DESC(lvds_use_ssc,
 	"Use Spread Spectrum Clock with panels [LVDS/eDP] "
 	"(default: auto from VBT)");
 
-module_param_named(vbt_sdvo_panel_type, i915.vbt_sdvo_panel_type, int, 0600);
+module_param_named(vbt_sdvo_panel_type, i915_module.vbt_sdvo_panel_type, int, 0600);
 MODULE_PARM_DESC(vbt_sdvo_panel_type,
 	"Override/Ignore selection of SDVO panel mode in the VBT "
 	"(-2=ignore, -1=auto [default], index in VBT BIOS table)");
 
-module_param_named(reset, i915.reset, bool, 0600);
+module_param_named(reset, i915_module.reset, bool, 0600);
 MODULE_PARM_DESC(reset, "Attempt GPU resets (default: true)");
 
-module_param_named(enable_hangcheck, i915.enable_hangcheck, bool, 0644);
+module_param_named(enable_hangcheck, i915_module.enable_hangcheck, bool, 0644);
 MODULE_PARM_DESC(enable_hangcheck,
 	"Periodically check GPU activity for detecting hangs. "
 	"WARNING: Disabling this can cause system wide hangs. "
 	"(default: true)");
 
-module_param_named(enable_ppgtt, i915.enable_ppgtt, int, 0400);
+module_param_named(enable_ppgtt, i915_module.enable_ppgtt, int, 0400);
 MODULE_PARM_DESC(enable_ppgtt,
 	"Override PPGTT usage. "
 	"(-1=auto [default], 0=disabled, 1=aliasing, 2=full)");
 
-module_param_named(enable_psr, i915.enable_psr, int, 0600);
+module_param_named(enable_execlists, i915_module.enable_execlists, int, 0400);
+MODULE_PARM_DESC(enable_execlists,
+	"Override execlists usage. "
+	"(-1=auto, 0=disabled [default], 1=enabled)");
+
+module_param_named(enable_psr, i915_module.enable_psr, int, 0600);
 MODULE_PARM_DESC(enable_psr, "Enable PSR (default: false)");
 
-module_param_named(preliminary_hw_support, i915.preliminary_hw_support, int, 0600);
+module_param_named(preliminary_hw_support, i915_module.preliminary_hw_support, int, 0600);
 MODULE_PARM_DESC(preliminary_hw_support,
 	"Enable preliminary hardware support.");
 
-module_param_named(disable_power_well, i915.disable_power_well, int, 0600);
+module_param_named(disable_power_well, i915_module.disable_power_well, int, 0600);
 MODULE_PARM_DESC(disable_power_well,
 	"Disable the power well when possible (default: true)");
 
-module_param_named(enable_ips, i915.enable_ips, int, 0600);
+module_param_named(enable_ips, i915_module.enable_ips, int, 0600);
 MODULE_PARM_DESC(enable_ips, "Enable IPS (default: true)");
 
-module_param_named(fastboot, i915.fastboot, bool, 0600);
+module_param_named(fastboot, i915_module.fastboot, bool, 0600);
 MODULE_PARM_DESC(fastboot,
 	"Try to skip unnecessary mode sets at boot time (default: false)");
 
-module_param_named(prefault_disable, i915.prefault_disable, bool, 0600);
+module_param_named(prefault_disable, i915_module.prefault_disable, bool, 0600);
 MODULE_PARM_DESC(prefault_disable,
 	"Disable page prefaulting for pread/pwrite/reloc (default:false). "
 	"For developers only.");
 
-module_param_named(invert_brightness, i915.invert_brightness, int, 0600);
+module_param_named(invert_brightness, i915_module.invert_brightness, int, 0600);
 MODULE_PARM_DESC(invert_brightness,
 	"Invert backlight brightness "
 	"(-1 force normal, 0 machine defaults, 1 force inversion), please "
@@ -149,21 +155,21 @@
 	"to dri-devel@lists.freedesktop.org, if your machine needs it. "
 	"It will then be included in an upcoming module version.");
 
-module_param_named(disable_display, i915.disable_display, bool, 0600);
+module_param_named(disable_display, i915_module.disable_display, bool, 0600);
 MODULE_PARM_DESC(disable_display, "Disable display (default: false)");
 
-module_param_named(disable_vtd_wa, i915.disable_vtd_wa, bool, 0600);
+module_param_named(disable_vtd_wa, i915_module.disable_vtd_wa, bool, 0600);
 MODULE_PARM_DESC(disable_vtd_wa, "Disable all VT-d workarounds (default: false)");
 
-module_param_named(enable_cmd_parser, i915.enable_cmd_parser, int, 0600);
+module_param_named(enable_cmd_parser, i915_module.enable_cmd_parser, int, 0600);
 MODULE_PARM_DESC(enable_cmd_parser,
 		 "Enable command parsing (1=enabled [default], 0=disabled)");
 
-module_param_named(use_mmio_flip, i915.use_mmio_flip, int, 0600);
+module_param_named(use_mmio_flip, i915_module.use_mmio_flip, int, 0600);
 MODULE_PARM_DESC(use_mmio_flip,
 		 "use MMIO flips (-1=never, 0=driver discretion [default], 1=always)");
 
-module_param_named(mmio_debug, i915.mmio_debug, bool, 0600);
+module_param_named(mmio_debug, i915_module.mmio_debug, bool, 0600);
 MODULE_PARM_DESC(mmio_debug,
 	"Enable the MMIO debug code (default: false). This may negatively "
 	"affect performance.");
diff -urN a/drivers/gpu/drm/i915/i915_perf.c b/drivers/gpu/drm/i915/i915_perf.c
--- a/drivers/gpu/drm/i915/i915_perf.c	1969-12-31 17:00:00.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_perf.c	2014-11-22 14:37:49.334700418 -0700
@@ -0,0 +1,567 @@
+#include <linux/perf_event.h>
+#include <linux/pm_runtime.h>
+
+#include "i915_drv.h"
+#include "intel_ringbuffer.h"
+
+#define FREQUENCY 200
+#define PERIOD max_t(u64, 10000, NSEC_PER_SEC / FREQUENCY)
+
+#define RING_MASK 0xffffffff
+#define RING_MAX 32
+
+#define INSTDONE_ENABLE 0x8
+
+static bool gpu_active(struct drm_i915_private *i915)
+{
+	struct intel_engine_cs *engine;
+	int i;
+
+	if (!pm_runtime_active(&i915->dev->pdev->dev))
+		return false;
+
+	for_each_engine(engine, i915, i) {
+		if (engine->last_request == NULL)
+			continue;
+
+		if (!i915_request_complete(engine->last_request))
+			return true;
+	}
+
+	return false;
+}
+
+static void engines_sample(struct drm_i915_private *dev_priv)
+{
+	struct intel_engine_cs *engine;
+	int i;
+
+	if ((dev_priv->pmu.enable & RING_MASK) == 0)
+		return;
+
+	if (!gpu_active(dev_priv))
+		return;
+
+	if (dev_priv->info.gen >= 6)
+		gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
+
+	for_each_engine(engine, dev_priv, i) {
+		u32 head, tail, ctrl;
+
+		if ((dev_priv->pmu.enable & (0x7 << (4*i))) == 0)
+			continue;
+
+		if (engine->last_request == NULL)
+			continue;
+
+		head = I915_READ_NOTRACE(RING_HEAD((engine)->mmio_base));
+		tail = I915_READ_NOTRACE(RING_TAIL((engine)->mmio_base));
+		ctrl = I915_READ_NOTRACE(RING_CTL((engine)->mmio_base));
+
+		if ((head ^ tail) & HEAD_ADDR)
+			engine->pmu_sample[I915_SAMPLE_BUSY] += PERIOD;
+
+		if (ctrl & ((dev_priv->info.gen == 2) ? RING_WAIT_I8XX : RING_WAIT))
+			engine->pmu_sample[I915_SAMPLE_WAIT] += PERIOD;
+
+		if (ctrl & RING_WAIT_SEMAPHORE)
+			engine->pmu_sample[I915_SAMPLE_SEMA] += PERIOD;
+	}
+
+	if (dev_priv->pmu.enable & INSTDONE_ENABLE) {
+		u64 instdone;
+
+		if (dev_priv->info.gen < 4) {
+			instdone = I915_READ_NOTRACE(INSTDONE);
+		} else if (dev_priv->info.gen < 7) {
+			instdone  = I915_READ_NOTRACE(INSTDONE_I965);
+			instdone |= (u64)I915_READ_NOTRACE(INSTDONE1) << 32;
+		} else {
+			instdone  = I915_READ_NOTRACE(GEN7_INSTDONE_1);
+			instdone |= (u64)(I915_READ_NOTRACE(GEN7_SC_INSTDONE) & 0xff) << 32;
+			instdone |= (u64)(I915_READ_NOTRACE(GEN7_SAMPLER_INSTDONE) & 0xff) << 40;
+			instdone |= (u64)(I915_READ_NOTRACE(GEN7_ROW_INSTDONE) & 0xff) << 48;
+		}
+
+		for (instdone = ~instdone & dev_priv->pmu.instdone, i = 0;
+		     instdone;
+		     instdone >>= 1, i++) {
+			if ((instdone & 1) == 0)
+				continue;
+
+			dev_priv->pmu.sample[__I915_SAMPLE_INSTDONE_0 + i] += PERIOD;
+		}
+	}
+
+	if (dev_priv->info.gen >= 6)
+		gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
+}
+
+static void frequency_sample(struct drm_i915_private *dev_priv)
+{
+	if (dev_priv->pmu.enable & ((u64)1 << I915_PERF_ACTUAL_FREQUENCY)) {
+		u64 val;
+
+		if (gpu_active(dev_priv)) {
+			if (dev_priv->info.is_valleyview) {
+				mutex_lock(&dev_priv->rps.hw_lock);
+				val = vlv_punit_read(dev_priv, PUNIT_REG_GPU_FREQ_STS);
+				mutex_unlock(&dev_priv->rps.hw_lock);
+				val = vlv_gpu_freq(dev_priv, (val >> 8) & 0xff);
+			} else {
+				val = I915_READ_NOTRACE(GEN6_RPSTAT1);
+				if (dev_priv->info.is_haswell)
+					val = (val & HSW_CAGF_MASK) >> HSW_CAGF_SHIFT;
+				else
+					val = (val & GEN6_CAGF_MASK) >> GEN6_CAGF_SHIFT;
+				val *= GT_FREQUENCY_MULTIPLIER;
+			}
+		} else {
+			val = dev_priv->rps.cur_freq; /* minor white lie to save power */
+			if (dev_priv->info.is_valleyview)
+				val = vlv_gpu_freq(dev_priv, val);
+			else
+				val *= GT_FREQUENCY_MULTIPLIER;
+		}
+
+		dev_priv->pmu.sample[__I915_SAMPLE_FREQ_ACT] += val * PERIOD;
+	}
+
+	if (dev_priv->pmu.enable & ((u64)1 << I915_PERF_REQUESTED_FREQUENCY)) {
+		u64 val = dev_priv->rps.cur_freq;
+		if (dev_priv->info.is_valleyview)
+			val = vlv_gpu_freq(dev_priv, val);
+		else
+			val *= GT_FREQUENCY_MULTIPLIER;
+		dev_priv->pmu.sample[__I915_SAMPLE_FREQ_REQ] += val * PERIOD;
+	}
+}
+
+static enum hrtimer_restart i915_sample(struct hrtimer *hrtimer)
+{
+	struct drm_i915_private *i915 =
+		container_of(hrtimer, struct drm_i915_private, pmu.timer);
+
+	if (i915->pmu.enable == 0)
+		return HRTIMER_NORESTART;
+
+	engines_sample(i915);
+	frequency_sample(i915);
+
+	hrtimer_forward_now(hrtimer, ns_to_ktime(PERIOD));
+	return HRTIMER_RESTART;
+}
+
+static void i915_perf_event_destroy(struct perf_event *event)
+{
+	WARN_ON(event->parent);
+}
+
+static int engine_event_init(struct perf_event *event)
+{
+	struct drm_i915_private *i915 =
+		container_of(event->pmu, typeof(*i915), pmu.base);
+	int engine = event->attr.config >> 2;
+	int sample = event->attr.config & 3;
+
+	switch (sample) {
+	case I915_SAMPLE_BUSY:
+	case I915_SAMPLE_WAIT:
+		break;
+	case I915_SAMPLE_SEMA:
+		if (i915->info.gen < 6)
+			return -ENODEV;
+		break;
+	default:
+		return -ENOENT;
+	}
+
+	if (engine >= I915_NUM_ENGINES)
+		return -ENOENT;
+
+	if (!intel_engine_initialized(&i915->engine[engine]))
+		return -ENODEV;
+
+	return 0;
+}
+
+static enum hrtimer_restart hrtimer_sample(struct hrtimer *hrtimer)
+{
+	struct pt_regs *regs;
+	struct perf_sample_data data;
+	struct perf_event *event;
+	u64 period;
+
+	event = container_of(hrtimer, struct perf_event, hw.hrtimer);
+	if (event->state != PERF_EVENT_STATE_ACTIVE)
+		return HRTIMER_NORESTART;
+
+	event->pmu->read(event);
+
+	perf_sample_data_init(&data, 0, event->hw.last_period);
+	regs = get_irq_regs();
+
+	perf_event_overflow(event, &data, NULL);
+
+	period = max_t(u64, 10000, event->hw.sample_period);
+	hrtimer_forward_now(hrtimer, ns_to_ktime(period));
+	return HRTIMER_RESTART;
+}
+
+static void init_hrtimer(struct perf_event *event)
+{
+	struct hw_perf_event *hwc = &event->hw;
+
+	printk(KERN_ERR "%s %d, is-sampling-event? %d\n", __func__, (int)event->attr.config, is_sampling_event(event));
+
+	if (!is_sampling_event(event))
+		return;
+
+	hrtimer_init(&hwc->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	hwc->hrtimer.function = hrtimer_sample;
+
+	if (event->attr.freq) {
+		long freq = event->attr.sample_freq;
+
+		event->attr.sample_period = NSEC_PER_SEC / freq;
+		hwc->sample_period = event->attr.sample_period;
+		local64_set(&hwc->period_left, hwc->sample_period);
+		hwc->last_period = hwc->sample_period;
+		event->attr.freq = 0;
+	}
+}
+
+static int i915_perf_event_init(struct perf_event *event)
+{
+	struct drm_i915_private *i915 =
+		container_of(event->pmu, typeof(*i915), pmu.base);
+	int ret;
+
+	/* XXX ideally only want pid == -1 && cpu == -1 */
+
+	if (event->attr.type != event->pmu->type)
+		return -ENOENT;
+
+	if (has_branch_stack(event))
+		return -EOPNOTSUPP;
+
+	ret = 0;
+	if (event->attr.config < RING_MAX) {
+		ret = engine_event_init(event);
+	} else switch (event->attr.config) {
+	case I915_PERF_ACTUAL_FREQUENCY:
+	case I915_PERF_REQUESTED_FREQUENCY:
+	case I915_PERF_ENERGY:
+	case I915_PERF_RC6_RESIDENCY:
+	case I915_PERF_RC6p_RESIDENCY:
+	case I915_PERF_RC6pp_RESIDENCY:
+		if (i915->info.gen < 6)
+			ret = -ENODEV;
+		break;
+	case I915_PERF_STATISTIC_0...I915_PERF_STATISTIC_8:
+		if (i915->info.gen < 4)
+			ret = -ENODEV;
+		break;
+	case I915_PERF_INSTDONE_0...I915_PERF_INSTDONE_63:
+		if (i915->info.gen < 4 &&
+		    event->attr.config - I915_PERF_INSTDONE_0 >= 32) {
+			ret = -ENODEV;
+			break;
+		}
+	}
+	if (ret)
+		return ret;
+
+	if (!event->parent) {
+		event->destroy = i915_perf_event_destroy;
+	}
+
+	init_hrtimer(event);
+
+	return 0;
+}
+
+static inline bool is_instdone_event(struct perf_event *event)
+{
+	return (event->attr.config >= I915_PERF_INSTDONE_0 &&
+		event->attr.config <= I915_PERF_INSTDONE_63);
+}
+
+static void i915_perf_timer_start(struct perf_event *event)
+{
+	struct hw_perf_event *hwc = &event->hw;
+	s64 period;
+
+	if (!is_sampling_event(event))
+		return;
+
+	period = local64_read(&hwc->period_left);
+	if (period) {
+		if (period < 0)
+			period = 10000;
+
+		local64_set(&hwc->period_left, 0);
+	} else {
+		period = max_t(u64, 10000, hwc->sample_period);
+	}
+
+	__hrtimer_start_range_ns(&hwc->hrtimer,
+				 ns_to_ktime(period), 0,
+				 HRTIMER_MODE_REL_PINNED, 0);
+}
+
+static void i915_perf_timer_cancel(struct perf_event *event)
+{
+	struct hw_perf_event *hwc = &event->hw;
+
+	if (!is_sampling_event(event))
+		return;
+
+	local64_set(&hwc->period_left,
+		    ktime_to_ns(hrtimer_get_remaining(&hwc->hrtimer)));
+	hrtimer_cancel(&hwc->hrtimer);
+}
+
+static void i915_perf_enable(struct perf_event *event)
+{
+	struct drm_i915_private *i915 =
+		container_of(event->pmu, typeof(*i915), pmu.base);
+	u64 mask;
+
+	if (i915->pmu.enable == 0)
+		__hrtimer_start_range_ns(&i915->pmu.timer,
+					 ns_to_ktime(PERIOD), 0,
+					 HRTIMER_MODE_REL_PINNED, 0);
+
+	if (is_instdone_event(event)) {
+		i915->pmu.instdone |= (u64)1 << (event->attr.config - I915_PERF_INSTDONE_0);
+		mask = INSTDONE_ENABLE;
+	} else
+		mask = (u64)1 << event->attr.config;
+
+	i915->pmu.enable |= mask;
+
+	i915_perf_timer_start(event);
+}
+
+static void i915_perf_disable(struct perf_event *event)
+{
+	struct drm_i915_private *i915 =
+		container_of(event->pmu, typeof(*i915), pmu.base);
+	u64 mask;
+
+	if (is_instdone_event(event)) {
+		i915->pmu.instdone &= ~((u64)1 << (event->attr.config - I915_PERF_INSTDONE_0));
+		mask = i915->pmu.instdone == 0 ? INSTDONE_ENABLE : 0;
+	} else
+		mask = (u64)1 << event->attr.config;
+
+	i915->pmu.enable &= ~mask;
+
+	i915_perf_timer_cancel(event);
+}
+
+static int i915_perf_event_add(struct perf_event *event, int flags)
+{
+	struct hw_perf_event *hwc = &event->hw;
+
+	printk(KERN_ERR "%s %d\n", __func__, (int)event->attr.config);
+
+	if (flags & PERF_EF_START)
+		i915_perf_enable(event);
+
+	hwc->state = !(flags & PERF_EF_START);
+
+	return 0;
+}
+
+static void i915_perf_event_del(struct perf_event *event, int flags)
+{
+	printk(KERN_ERR "%s %d\n", __func__, (int)event->attr.config);
+
+	i915_perf_disable(event);
+}
+
+static void i915_perf_event_start(struct perf_event *event, int flags)
+{
+	printk(KERN_ERR "%s %d\n", __func__, (int)event->attr.config);
+
+	i915_perf_enable(event);
+}
+
+static void i915_perf_event_stop(struct perf_event *event, int flags)
+{
+	printk(KERN_ERR "%s %d\n", __func__, (int)event->attr.config);
+
+	i915_perf_disable(event);
+}
+
+static u64 read_energy_uJ(struct drm_i915_private *dev_priv)
+{
+	u64 power;
+	u32 units;
+
+	if (dev_priv->info.gen < 6)
+		return 0;
+
+	rdmsrl(MSR_RAPL_POWER_UNIT, power);
+	power = (power & 0x1f00) >> 8;
+	units = 1000000 / (1 << power); /* convert to uJ */
+	power = I915_READ_NOTRACE(MCH_SECP_NRG_STTS);
+	power *= units;
+
+	return power;
+}
+
+static inline u64 calc_residency(struct drm_i915_private *dev_priv, const u32 reg)
+{
+	if (dev_priv->info.gen >= 6) {
+		u64 val, units = 128, div = 100000;
+		if (dev_priv->info.is_valleyview) {
+			u32 clock;
+
+			clock = I915_READ_NOTRACE(VLV_CLK_CTL2) >> CLK_CTL2_CZCOUNT_30NS_SHIFT;
+			if (clock) {
+				units = DIV_ROUND_UP(30 * 1000, clock);
+				if (I915_READ_NOTRACE(VLV_COUNTER_CONTROL) & VLV_COUNT_RANGE_HIGH)
+					units <<= 8;
+			} else
+				units = 0;
+
+			div *= 1000;
+		}
+		val = I915_READ_NOTRACE(reg);
+		val *= units;
+		return DIV_ROUND_UP_ULL(val, div);
+	} else
+		return 0;
+}
+
+static inline u64 read_statistic(struct drm_i915_private *dev_priv,
+				 const int statistic)
+{
+	const u32 reg = 0x2310 + 8 *statistic;
+	u32 high, low;
+
+	do {
+		high = I915_READ_NOTRACE(reg + 4);
+		low = I915_READ_NOTRACE(reg);
+	} while (high != I915_READ_NOTRACE(reg + 4));
+
+	return (u64)high << 32 | low;
+}
+
+static u64 count_interrupts(struct drm_i915_private *i915)
+{
+	/* open-coded kstat_irqs() */
+	struct irq_desc *desc = irq_to_desc(i915->dev->pdev->irq);
+	u64 sum = 0;
+	int cpu;
+
+	if (!desc || !desc->kstat_irqs)
+		return 0;
+
+	for_each_possible_cpu(cpu)
+		sum += *per_cpu_ptr(desc->kstat_irqs, cpu);
+
+	return sum;
+}
+
+static void i915_perf_event_read(struct perf_event *event)
+{
+	struct drm_i915_private *i915 =
+		container_of(event->pmu, typeof(*i915), pmu.base);
+	u64 val = 0;
+
+	if (event->attr.config < 32) {
+		int engine = event->attr.config >> 2;
+		int sample = event->attr.config & 3;
+		val = i915->engine[engine].pmu_sample[sample];
+	} else switch (event->attr.config) {
+	case I915_PERF_ACTUAL_FREQUENCY:
+		val = i915->pmu.sample[__I915_SAMPLE_FREQ_ACT];
+		break;
+	case I915_PERF_REQUESTED_FREQUENCY:
+		val = i915->pmu.sample[__I915_SAMPLE_FREQ_REQ];
+		break;
+	case I915_PERF_ENERGY:
+		val = read_energy_uJ(i915);
+		break;
+	case I915_PERF_INTERRUPTS:
+		val = count_interrupts(i915);
+		break;
+
+	case I915_PERF_RC6_RESIDENCY:
+		if (!pm_runtime_active(&i915->dev->pdev->dev))
+			return;
+
+		val = calc_residency(i915, i915->info.is_valleyview ? VLV_GT_RENDER_RC6 : GEN6_GT_GFX_RC6);
+		break;
+
+	case I915_PERF_RC6p_RESIDENCY:
+		if (!pm_runtime_active(&i915->dev->pdev->dev))
+			return;
+
+		if (!i915->info.is_valleyview)
+			val = calc_residency(i915, GEN6_GT_GFX_RC6p);
+		break;
+
+	case I915_PERF_RC6pp_RESIDENCY:
+		if (!pm_runtime_active(&i915->dev->pdev->dev))
+			return;
+
+		if (!i915->info.is_valleyview)
+			val = calc_residency(i915, GEN6_GT_GFX_RC6pp);
+		break;
+
+	case I915_PERF_STATISTIC_0...I915_PERF_STATISTIC_8:
+		if (!pm_runtime_active(&i915->dev->pdev->dev))
+			return;
+
+		val = read_statistic(i915, event->attr.config - I915_PERF_STATISTIC_0);
+		break;
+
+	case I915_PERF_INSTDONE_0...I915_PERF_INSTDONE_63:
+		val = i915->pmu.sample[event->attr.config - I915_PERF_INSTDONE_0 + __I915_SAMPLE_INSTDONE_0];
+		break;
+	}
+
+	local64_set(&event->count, val);
+}
+
+static int i915_perf_event_event_idx(struct perf_event *event)
+{
+	return 0;
+}
+
+void i915_perf_register(struct drm_device *dev)
+{
+	struct drm_i915_private *i915 = to_i915(dev);
+
+	i915->pmu.base.task_ctx_nr	= perf_sw_context;
+	i915->pmu.base.event_init	= i915_perf_event_init;
+	i915->pmu.base.add		= i915_perf_event_add;
+	i915->pmu.base.del		= i915_perf_event_del;
+	i915->pmu.base.start		= i915_perf_event_start;
+	i915->pmu.base.stop		= i915_perf_event_stop;
+	i915->pmu.base.read		= i915_perf_event_read;
+	i915->pmu.base.event_idx	= i915_perf_event_event_idx;
+
+	hrtimer_init(&i915->pmu.timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	i915->pmu.timer.function = i915_sample;
+	i915->pmu.enable = 0;
+
+	if (perf_pmu_register(&i915->pmu.base, "i915", -1))
+		i915->pmu.base.event_init = NULL;
+}
+
+void i915_perf_unregister(struct drm_device *dev)
+{
+	struct drm_i915_private *i915 = to_i915(dev);
+
+	if (i915->pmu.base.event_init == NULL)
+		return;
+
+	perf_pmu_unregister(&i915->pmu.base);
+	i915->pmu.base.event_init = NULL;
+}
diff -urN a/drivers/gpu/drm/i915/i915_reg.h b/drivers/gpu/drm/i915/i915_reg.h
--- a/drivers/gpu/drm/i915/i915_reg.h	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_reg.h	2014-11-22 14:37:49.334700418 -0700
@@ -26,8 +26,8 @@
 #define _I915_REG_H_
 
 #define _PIPE(pipe, a, b) ((a) + (pipe)*((b)-(a)))
+#define _PLANE(plane, a, b) _PIPE(plane, a, b)
 #define _TRANSCODER(tran, a, b) ((a) + (tran)*((b)-(a)))
-
 #define _PORT(port, a, b) ((a) + (port)*((b)-(a)))
 #define _PIPE3(pipe, a, b, c) ((pipe) == PIPE_A ? (a) : \
 			       (pipe) == PIPE_B ? (b) : (c))
@@ -143,6 +143,14 @@
 #define GAB_CTL				0x24000
 #define   GAB_CTL_CONT_AFTER_PAGEFAULT	(1<<8)
 
+#define GEN7_BIOS_RESERVED		0x1082C0
+#define GEN7_BIOS_RESERVED_1M		(0 << 5)
+#define GEN7_BIOS_RESERVED_256K		(1 << 5)
+#define GEN8_BIOS_RESERVED_SHIFT       7
+#define GEN7_BIOS_RESERVED_MASK        0x1
+#define GEN8_BIOS_RESERVED_MASK        0x3
+
+
 /* VGA stuff */
 
 #define VGA_ST01_MDA 0x3ba
@@ -232,6 +240,7 @@
 #define MI_LOAD_SCAN_LINES_INCL MI_INSTR(0x12, 0)
 #define MI_DISPLAY_FLIP		MI_INSTR(0x14, 2)
 #define MI_DISPLAY_FLIP_I915	MI_INSTR(0x14, 1)
+#define   MI_DISPLAY_FLIP_ASYNC	(1 << 22)
 #define   MI_DISPLAY_FLIP_PLANE(n) ((n) << 20)
 /* IVB has funny definitions for which plane to flip. */
 #define   MI_DISPLAY_FLIP_IVB_PLANE_A  (0 << 19)
@@ -240,6 +249,21 @@
 #define   MI_DISPLAY_FLIP_IVB_SPRITE_B (3 << 19)
 #define   MI_DISPLAY_FLIP_IVB_PLANE_C  (4 << 19)
 #define   MI_DISPLAY_FLIP_IVB_SPRITE_C (5 << 19)
+/* SKL ones */
+#define   MI_DISPLAY_FLIP_SKL_PLANE_1_A	(0 << 8)
+#define   MI_DISPLAY_FLIP_SKL_PLANE_1_B	(1 << 8)
+#define   MI_DISPLAY_FLIP_SKL_PLANE_1_C	(2 << 8)
+#define   MI_DISPLAY_FLIP_SKL_PLANE_2_A	(4 << 8)
+#define   MI_DISPLAY_FLIP_SKL_PLANE_2_B	(5 << 8)
+#define   MI_DISPLAY_FLIP_SKL_PLANE_2_C	(6 << 8)
+#define   MI_DISPLAY_FLIP_SKL_PLANE_3_A	(7 << 8)
+#define   MI_DISPLAY_FLIP_SKL_PLANE_3_B	(8 << 8)
+#define   MI_DISPLAY_FLIP_SKL_PLANE_3_C	(9 << 8)
+/* These go in the bottom of the base address value */
+#define   MI_DISPLAY_FLIP_TYPE_SYNC    (0 << 0)
+#define   MI_DISPLAY_FLIP_TYPE_ASYNC   (1 << 0)
+#define   MI_DISPLAY_FLIP_TYPE_STEREO  (2 << 0)
+
 #define MI_SEMAPHORE_MBOX	MI_INSTR(0x16, 1) /* gen6, gen7 */
 #define   MI_SEMAPHORE_GLOBAL_GTT    (1<<22)
 #define   MI_SEMAPHORE_UPDATE	    (1<<21)
@@ -272,6 +296,7 @@
 #define   MI_SEMAPHORE_POLL		(1<<15)
 #define   MI_SEMAPHORE_SAD_GTE_SDD	(1<<12)
 #define MI_STORE_DWORD_IMM	MI_INSTR(0x20, 1)
+#define MI_STORE_DWORD_IMM_GEN8	MI_INSTR(0x20, 2)
 #define   MI_MEM_VIRTUAL	(1 << 22) /* 965+ only */
 #define MI_STORE_DWORD_INDEX	MI_INSTR(0x21, 1)
 #define   MI_STORE_DWORD_INDEX_SHIFT 2
@@ -282,10 +307,11 @@
  *   address/value pairs. Don't overdue it, though, x <= 2^4 must hold!
  */
 #define MI_LOAD_REGISTER_IMM(x)	MI_INSTR(0x22, 2*(x)-1)
+#define   MI_LRI_FORCE_POSTED		(1<<12)
 #define MI_STORE_REGISTER_MEM(x) MI_INSTR(0x24, 2*(x)-1)
 #define MI_STORE_REGISTER_MEM_GEN8(x) MI_INSTR(0x24, 3*(x)-1)
 #define   MI_SRM_LRM_GLOBAL_GTT		(1<<22)
-#define MI_FLUSH_DW		MI_INSTR(0x26, 1) /* for GEN6 */
+#define MI_FLUSH_DW		MI_INSTR(0x26, 0) /* for GEN6 */
 #define   MI_FLUSH_DW_STORE_INDEX	(1<<21)
 #define   MI_INVALIDATE_TLB		(1<<18)
 #define   MI_FLUSH_DW_OP_STOREDW	(1<<14)
@@ -304,6 +330,8 @@
 #define   MI_BATCH_GTT		    (2<<6) /* aliased with (1<<7) on gen4 */
 #define MI_BATCH_BUFFER_START_GEN8	MI_INSTR(0x31, 1)
 
+#define MI_PREDICATE_SRC0	(0x2400)
+#define MI_PREDICATE_SRC1	(0x2408)
 
 #define MI_PREDICATE_RESULT_2	(0x2214)
 #define  LOWER_SLICE_ENABLED	(1<<0)
@@ -501,10 +529,26 @@
 #define BUNIT_REG_BISOC				0x11
 
 #define PUNIT_REG_DSPFREQ			0x36
+#define   DSPFREQSTAT_SHIFT_CHV			24
+#define   DSPFREQSTAT_MASK_CHV			(0x1f << DSPFREQSTAT_SHIFT_CHV)
+#define   DSPFREQGUAR_SHIFT_CHV			8
+#define   DSPFREQGUAR_MASK_CHV			(0x1f << DSPFREQGUAR_SHIFT_CHV)
 #define   DSPFREQSTAT_SHIFT			30
 #define   DSPFREQSTAT_MASK			(0x3 << DSPFREQSTAT_SHIFT)
 #define   DSPFREQGUAR_SHIFT			14
 #define   DSPFREQGUAR_MASK			(0x3 << DSPFREQGUAR_SHIFT)
+#define   _DP_SSC(val, pipe)			((val) << (2 * (pipe)))
+#define   DP_SSC_MASK(pipe)			_DP_SSC(0x3, (pipe))
+#define   DP_SSC_PWR_ON(pipe)			_DP_SSC(0x0, (pipe))
+#define   DP_SSC_CLK_GATE(pipe)			_DP_SSC(0x1, (pipe))
+#define   DP_SSC_RESET(pipe)			_DP_SSC(0x2, (pipe))
+#define   DP_SSC_PWR_GATE(pipe)			_DP_SSC(0x3, (pipe))
+#define   _DP_SSS(val, pipe)			((val) << (2 * (pipe) + 16))
+#define   DP_SSS_MASK(pipe)			_DP_SSS(0x3, (pipe))
+#define   DP_SSS_PWR_ON(pipe)			_DP_SSS(0x0, (pipe))
+#define   DP_SSS_CLK_GATE(pipe)			_DP_SSS(0x1, (pipe))
+#define   DP_SSS_RESET(pipe)			_DP_SSS(0x2, (pipe))
+#define   DP_SSS_PWR_GATE(pipe)			_DP_SSS(0x3, (pipe))
 
 /* See the PUNIT HAS v0.8 for the below bits */
 enum punit_power_well {
@@ -518,6 +562,11 @@
 	PUNIT_POWER_WELL_DPIO_TX_C_LANES_23	= 9,
 	PUNIT_POWER_WELL_DPIO_RX0		= 10,
 	PUNIT_POWER_WELL_DPIO_RX1		= 11,
+	PUNIT_POWER_WELL_DPIO_CMN_D		= 12,
+	/* FIXME: guesswork below */
+	PUNIT_POWER_WELL_DPIO_TX_D_LANES_01	= 13,
+	PUNIT_POWER_WELL_DPIO_TX_D_LANES_23	= 14,
+	PUNIT_POWER_WELL_DPIO_RX2		= 15,
 
 	PUNIT_POWER_WELL_NUM,
 };
@@ -533,6 +582,7 @@
 #define PUNIT_REG_GPU_LFM			0xd3
 #define PUNIT_REG_GPU_FREQ_REQ			0xd4
 #define PUNIT_REG_GPU_FREQ_STS			0xd8
+#define   GPLLENABLE				(1<<4)
 #define   GENFREQSTATUS				(1<<0)
 #define PUNIT_REG_MEDIA_TURBO_FREQ_REQ		0xdc
 #define PUNIT_REG_CZ_TIMESTAMP			0xce
@@ -562,9 +612,6 @@
 #define   FB_FMAX_VMIN_FREQ_LO_MASK		0xf8000000
 
 #define VLV_CZ_CLOCK_TO_MILLI_SEC		100000
-#define VLV_RP_UP_EI_THRESHOLD			90
-#define VLV_RP_DOWN_EI_THRESHOLD		70
-#define VLV_INT_COUNT_FOR_DOWN_EI		5
 
 /* vlv2 north clock has */
 #define CCK_FUSE_REG				0x8
@@ -641,7 +688,7 @@
  * need to be accessed during AUX communication,
  *
  * Generally the common lane corresponds to the pipe and
- * the spline (PCS/TX) correponds to the port.
+ * the spline (PCS/TX) corresponds to the port.
  *
  * For dual channel PHY (VLV/CHV):
  *
@@ -765,6 +812,8 @@
 #define _VLV_PCS_DW0_CH1		0x8400
 #define   DPIO_PCS_TX_LANE2_RESET	(1<<16)
 #define   DPIO_PCS_TX_LANE1_RESET	(1<<7)
+#define   DPIO_LEFT_TXFIFO_RST_MASTER2	(1<<4)
+#define   DPIO_RIGHT_TXFIFO_RST_MASTER2	(1<<3)
 #define VLV_PCS_DW0(ch) _PORT(ch, _VLV_PCS_DW0_CH0, _VLV_PCS_DW0_CH1)
 
 #define _VLV_PCS01_DW0_CH0		0x200
@@ -805,12 +854,31 @@
 
 #define _VLV_PCS_DW9_CH0		0x8224
 #define _VLV_PCS_DW9_CH1		0x8424
+#define   DPIO_PCS_TX2MARGIN_MASK	(0x7<<13)
+#define   DPIO_PCS_TX2MARGIN_000	(0<<13)
+#define   DPIO_PCS_TX2MARGIN_101	(1<<13)
+#define   DPIO_PCS_TX1MARGIN_MASK	(0x7<<10)
+#define   DPIO_PCS_TX1MARGIN_000	(0<<10)
+#define   DPIO_PCS_TX1MARGIN_101	(1<<10)
 #define	VLV_PCS_DW9(ch) _PORT(ch, _VLV_PCS_DW9_CH0, _VLV_PCS_DW9_CH1)
 
+#define _VLV_PCS01_DW9_CH0		0x224
+#define _VLV_PCS23_DW9_CH0		0x424
+#define _VLV_PCS01_DW9_CH1		0x2624
+#define _VLV_PCS23_DW9_CH1		0x2824
+#define VLV_PCS01_DW9(ch) _PORT(ch, _VLV_PCS01_DW9_CH0, _VLV_PCS01_DW9_CH1)
+#define VLV_PCS23_DW9(ch) _PORT(ch, _VLV_PCS23_DW9_CH0, _VLV_PCS23_DW9_CH1)
+
 #define _CHV_PCS_DW10_CH0		0x8228
 #define _CHV_PCS_DW10_CH1		0x8428
 #define   DPIO_PCS_SWING_CALC_TX0_TX2	(1<<30)
 #define   DPIO_PCS_SWING_CALC_TX1_TX3	(1<<31)
+#define   DPIO_PCS_TX2DEEMP_MASK	(0xf<<24)
+#define   DPIO_PCS_TX2DEEMP_9P5		(0<<24)
+#define   DPIO_PCS_TX2DEEMP_6P0		(2<<24)
+#define   DPIO_PCS_TX1DEEMP_MASK	(0xf<<16)
+#define   DPIO_PCS_TX1DEEMP_9P5		(0<<16)
+#define   DPIO_PCS_TX1DEEMP_6P0		(2<<16)
 #define CHV_PCS_DW10(ch) _PORT(ch, _CHV_PCS_DW10_CH0, _CHV_PCS_DW10_CH1)
 
 #define _VLV_PCS01_DW10_CH0		0x0228
@@ -822,8 +890,18 @@
 
 #define _VLV_PCS_DW11_CH0		0x822c
 #define _VLV_PCS_DW11_CH1		0x842c
+#define   DPIO_LANEDESKEW_STRAP_OVRD	(1<<3)
+#define   DPIO_LEFT_TXFIFO_RST_MASTER	(1<<1)
+#define   DPIO_RIGHT_TXFIFO_RST_MASTER	(1<<0)
 #define VLV_PCS_DW11(ch) _PORT(ch, _VLV_PCS_DW11_CH0, _VLV_PCS_DW11_CH1)
 
+#define _VLV_PCS01_DW11_CH0		0x022c
+#define _VLV_PCS23_DW11_CH0		0x042c
+#define _VLV_PCS01_DW11_CH1		0x262c
+#define _VLV_PCS23_DW11_CH1		0x282c
+#define VLV_PCS01_DW11(ch) _PORT(ch, _VLV_PCS01_DW11_CH0, _VLV_PCS01_DW11_CH1)
+#define VLV_PCS23_DW11(ch) _PORT(ch, _VLV_PCS23_DW11_CH0, _VLV_PCS23_DW11_CH1)
+
 #define _VLV_PCS_DW12_CH0		0x8230
 #define _VLV_PCS_DW12_CH1		0x8430
 #define VLV_PCS_DW12(ch) _PORT(ch, _VLV_PCS_DW12_CH0, _VLV_PCS_DW12_CH1)
@@ -838,8 +916,8 @@
 
 #define _VLV_TX_DW2_CH0			0x8288
 #define _VLV_TX_DW2_CH1			0x8488
-#define   DPIO_SWING_MARGIN_SHIFT	16
-#define   DPIO_SWING_MARGIN_MASK	(0xff << DPIO_SWING_MARGIN_SHIFT)
+#define   DPIO_SWING_MARGIN000_SHIFT	16
+#define   DPIO_SWING_MARGIN000_MASK	(0xff << DPIO_SWING_MARGIN000_SHIFT)
 #define   DPIO_UNIQ_TRANS_SCALE_SHIFT	8
 #define VLV_TX_DW2(ch) _PORT(ch, _VLV_TX_DW2_CH0, _VLV_TX_DW2_CH1)
 
@@ -847,12 +925,16 @@
 #define _VLV_TX_DW3_CH1			0x848c
 /* The following bit for CHV phy */
 #define   DPIO_TX_UNIQ_TRANS_SCALE_EN	(1<<27)
+#define   DPIO_SWING_MARGIN101_SHIFT	16
+#define   DPIO_SWING_MARGIN101_MASK	(0xff << DPIO_SWING_MARGIN101_SHIFT)
 #define VLV_TX_DW3(ch) _PORT(ch, _VLV_TX_DW3_CH0, _VLV_TX_DW3_CH1)
 
 #define _VLV_TX_DW4_CH0			0x8290
 #define _VLV_TX_DW4_CH1			0x8490
 #define   DPIO_SWING_DEEMPH9P5_SHIFT	24
 #define   DPIO_SWING_DEEMPH9P5_MASK	(0xff << DPIO_SWING_DEEMPH9P5_SHIFT)
+#define   DPIO_SWING_DEEMPH6P0_SHIFT	16
+#define   DPIO_SWING_DEEMPH6P0_MASK	(0xff << DPIO_SWING_DEEMPH6P0_SHIFT)
 #define VLV_TX_DW4(ch) _PORT(ch, _VLV_TX_DW4_CH0, _VLV_TX_DW4_CH1)
 
 #define _VLV_TX3_DW4_CH0		0x690
@@ -1003,6 +1085,13 @@
 #define   PGTBL_ADDRESS_LO_MASK	0xfffff000 /* bits [31:12] */
 #define   PGTBL_ADDRESS_HI_MASK	0x000000f0 /* bits [35:32] (gen4) */
 #define PGTBL_ER	0x02024
+#define PRB0_BASE (0x2030-0x30)
+#define PRB1_BASE (0x2040-0x30) /* 830,gen3 */
+#define PRB2_BASE (0x2050-0x30) /* gen3 */
+#define SRB0_BASE (0x2100-0x30) /* gen2 */
+#define SRB1_BASE (0x2110-0x30) /* gen2 */
+#define SRB2_BASE (0x2120-0x30) /* 830 */
+#define SRB3_BASE (0x2130-0x30) /* 830 */
 #define RENDER_RING_BASE	0x02000
 #define BSD_RING_BASE		0x04000
 #define GEN6_BSD_RING_BASE	0x12000
@@ -1064,6 +1153,7 @@
 #define RING_ACTHD_UDW(base)	((base)+0x5c)
 #define RING_NOPID(base)	((base)+0x94)
 #define RING_IMR(base)		((base)+0xa8)
+#define RING_HWSTAM(base)	((base)+0x98)
 #define RING_TIMESTAMP(base)	((base)+0x358)
 #define   TAIL_ADDR		0x001FFFF8
 #define   HEAD_WRAP_COUNT	0xFFE00000
@@ -1248,6 +1338,10 @@
 #define   INSTPM_TLB_INVALIDATE	(1<<9)
 #define   INSTPM_SYNC_FLUSH	(1<<5)
 #define ACTHD	        0x020c8
+#define MEM_MODE	0x020cc
+#define   MEM_DISPLAY_B_TRICKLE_FEED_DISABLE (1<<3) /* 830 only */
+#define   MEM_DISPLAY_A_TRICKLE_FEED_DISABLE (1<<2) /* 830/845 only */
+#define   MEM_DISPLAY_TRICKLE_FEED_DISABLE (1<<2) /* 85x only */
 #define FW_BLC		0x020d8
 #define FW_BLC2		0x020dc
 #define FW_BLC_SELF	0x020e0 /* 915+ only */
@@ -1380,6 +1474,7 @@
 #define GT_BSD_CS_ERROR_INTERRUPT		(1 << 15)
 #define GT_BSD_USER_INTERRUPT			(1 << 12)
 #define GT_RENDER_L3_PARITY_ERROR_INTERRUPT_S1	(1 << 11) /* hsw+; rsvd on snb, ivb, vlv */
+#define GT_CONTEXT_SWITCH_INTERRUPT		(1 <<  8)
 #define GT_RENDER_L3_PARITY_ERROR_INTERRUPT	(1 <<  5) /* !snb */
 #define GT_RENDER_PIPECTL_NOTIFY_INTERRUPT	(1 <<  4)
 #define GT_RENDER_CS_MASTER_ERROR_INTERRUPT	(1 <<  3)
@@ -1519,6 +1614,7 @@
 /* Framebuffer compression for Ironlake */
 #define ILK_DPFC_CB_BASE	0x43200
 #define ILK_DPFC_CONTROL	0x43208
+#define   FBC_CTL_FALSE_COLOR	(1<<10)
 /* The bit 28-8 is reserved */
 #define   DPFC_RESERVED		(0x1FFFFF00)
 #define ILK_DPFC_RECOMP_CTL	0x4320c
@@ -1675,12 +1771,9 @@
 #define DPIO_PHY_STATUS			(VLV_DISPLAY_BASE + 0x6240)
 #define   DPLL_PORTD_READY_MASK		(0xf)
 #define DISPLAY_PHY_CONTROL (VLV_DISPLAY_BASE + 0x60100)
-#define   PHY_COM_LANE_RESET_DEASSERT(phy, val) \
-				((phy == DPIO_PHY0) ? (val | 1) : (val | 2))
-#define   PHY_COM_LANE_RESET_ASSERT(phy, val) \
-				((phy == DPIO_PHY0) ? (val & ~1) : (val & ~2))
+#define   PHY_COM_LANE_RESET_DEASSERT(phy) (1 << (phy))
 #define DISPLAY_PHY_STATUS (VLV_DISPLAY_BASE + 0x60104)
-#define   PHY_POWERGOOD(phy)	((phy == DPIO_PHY0) ? (1<<31) : (1<<30))
+#define   PHY_POWERGOOD(phy)	(((phy) == DPIO_PHY0) ? (1<<31) : (1<<30))
 
 /*
  * The i830 generation, in LVDS mode, defines P1 as the bit number set within
@@ -2236,7 +2329,6 @@
 
 #define GEN6_GT_THREAD_STATUS_REG 0x13805c
 #define GEN6_GT_THREAD_STATUS_CORE_MASK 0x7
-#define GEN6_GT_THREAD_STATUS_CORE_MASK_HSW (0x7 | (0x07 << 16))
 
 #define GEN6_GT_PERF_STATUS	(MCHBAR_MIRROR_BASE_SNB + 0x5948)
 #define GEN6_RP_STATE_LIMITS	(MCHBAR_MIRROR_BASE_SNB + 0x5994)
@@ -2261,6 +2353,7 @@
  *   doesn't need saving on GT1
  */
 #define CXT_SIZE		0x21a0
+#define ILK_CXT_TOTAL_SIZE		(1 * PAGE_SIZE)
 #define GEN6_CXT_POWER_SIZE(cxt_reg)	((cxt_reg >> 24) & 0x3f)
 #define GEN6_CXT_RING_SIZE(cxt_reg)	((cxt_reg >> 18) & 0x3f)
 #define GEN6_CXT_RENDER_SIZE(cxt_reg)	((cxt_reg >> 12) & 0x3f)
@@ -2397,6 +2490,7 @@
 #define _PIPEASRC	0x6001c
 #define _BCLRPAT_A	0x60020
 #define _VSYNCSHIFT_A	0x60028
+#define _PIPE_MULT_A	0x6002c
 
 /* Pipe B timing regs */
 #define _HTOTAL_B	0x61000
@@ -2408,6 +2502,7 @@
 #define _PIPEBSRC	0x6101c
 #define _BCLRPAT_B	0x61020
 #define _VSYNCSHIFT_B	0x61028
+#define _PIPE_MULT_B	0x6102c
 
 #define TRANSCODER_A_OFFSET 0x60000
 #define TRANSCODER_B_OFFSET 0x61000
@@ -2428,6 +2523,7 @@
 #define BCLRPAT(trans) _TRANSCODER2(trans, _BCLRPAT_A)
 #define VSYNCSHIFT(trans) _TRANSCODER2(trans, _VSYNCSHIFT_A)
 #define PIPESRC(trans) _TRANSCODER2(trans, _PIPEASRC)
+#define PIPE_MULT(trans) _TRANSCODER2(trans, _PIPE_MULT_A)
 
 /* HSW+ eDP PSR registers */
 #define EDP_PSR_BASE(dev)                       (IS_HASWELL(dev) ? 0x64800 : 0x6f800)
@@ -2457,9 +2553,7 @@
 
 #define EDP_PSR_AUX_CTL(dev)			(EDP_PSR_BASE(dev) + 0x10)
 #define EDP_PSR_AUX_DATA1(dev)			(EDP_PSR_BASE(dev) + 0x14)
-#define   EDP_PSR_DPCD_COMMAND		0x80060000
 #define EDP_PSR_AUX_DATA2(dev)			(EDP_PSR_BASE(dev) + 0x18)
-#define   EDP_PSR_DPCD_NORMAL_OPERATION	(1<<24)
 #define EDP_PSR_AUX_DATA3(dev)			(EDP_PSR_BASE(dev) + 0x1c)
 #define EDP_PSR_AUX_DATA4(dev)			(EDP_PSR_BASE(dev) + 0x20)
 #define EDP_PSR_AUX_DATA5(dev)			(EDP_PSR_BASE(dev) + 0x24)
@@ -3476,6 +3570,8 @@
 #define   DP_LINK_TRAIN_OFF		(3 << 28)
 #define   DP_LINK_TRAIN_MASK		(3 << 28)
 #define   DP_LINK_TRAIN_SHIFT		28
+#define   DP_LINK_TRAIN_PAT_3_CHV	(1 << 14)
+#define   DP_LINK_TRAIN_MASK_CHV	((3 << 28)|(1<<14))
 
 /* CPT Link training mode */
 #define   DP_LINK_TRAIN_PAT_1_CPT	(0 << 8)
@@ -3594,6 +3690,7 @@
 #define   DP_AUX_CH_CTL_PRECHARGE_TEST	    (1 << 11)
 #define   DP_AUX_CH_CTL_BIT_CLOCK_2X_MASK    (0x7ff)
 #define   DP_AUX_CH_CTL_BIT_CLOCK_2X_SHIFT   0
+#define   DP_AUX_CH_CTL_SYNC_PULSE_SKL(c)   ((c) - 1)
 
 /*
  * Computing GMCH M and N values for the Display Port link
@@ -3732,7 +3829,6 @@
 #define   PIPE_VSYNC_INTERRUPT_STATUS		(1UL<<9)
 #define   PIPE_DISPLAY_LINE_COMPARE_STATUS	(1UL<<8)
 #define   PIPE_DPST_EVENT_STATUS		(1UL<<7)
-#define   PIPE_LEGACY_BLC_EVENT_STATUS		(1UL<<6)
 #define   PIPE_A_PSR_STATUS_VLV			(1UL<<6)
 #define   PIPE_LEGACY_BLC_EVENT_STATUS		(1UL<<6)
 #define   PIPE_ODD_FIELD_INTERRUPT_STATUS	(1UL<<5)
@@ -3842,73 +3938,152 @@
 #define   DSPARB_BEND_SHIFT	9 /* on 855 */
 #define   DSPARB_AEND_SHIFT	0
 
+/* pnv/gen4/g4x/vlv/chv */
 #define DSPFW1			(dev_priv->info.display_mmio_offset + 0x70034)
-#define   DSPFW_SR_SHIFT	23
-#define   DSPFW_SR_MASK		(0x1ff<<23)
-#define   DSPFW_CURSORB_SHIFT	16
-#define   DSPFW_CURSORB_MASK	(0x3f<<16)
-#define   DSPFW_PLANEB_SHIFT	8
-#define   DSPFW_PLANEB_MASK	(0x7f<<8)
-#define   DSPFW_PLANEA_MASK	(0x7f)
+#define   DSPFW_SR_SHIFT		23
+#define   DSPFW_SR_MASK			(0x1ff<<23)
+#define   DSPFW_CURSORB_SHIFT		16
+#define   DSPFW_CURSORB_MASK		(0x3f<<16)
+#define   DSPFW_PLANEB_SHIFT		8
+#define   DSPFW_PLANEB_MASK		(0x7f<<8)
+#define   DSPFW_PLANEB_MASK_VLV		(0xff<<8) /* vlv/chv */
+#define   DSPFW_PLANEA_SHIFT		0
+#define   DSPFW_PLANEA_MASK		(0x7f<<0)
+#define   DSPFW_PLANEA_MASK_VLV		(0xff<<0) /* vlv/chv */
 #define DSPFW2			(dev_priv->info.display_mmio_offset + 0x70038)
-#define   DSPFW_CURSORA_MASK	0x00003f00
-#define   DSPFW_CURSORA_SHIFT	8
-#define   DSPFW_PLANEC_MASK	(0x7f)
+#define   DSPFW_FBC_SR_EN		(1<<31)	  /* g4x */
+#define   DSPFW_FBC_SR_SHIFT		28
+#define   DSPFW_FBC_SR_MASK		(0x7<<28) /* g4x */
+#define   DSPFW_FBC_HPLL_SR_SHIFT	24
+#define   DSPFW_FBC_HPLL_SR_MASK	(0xf<<24) /* g4x */
+#define   DSPFW_SPRITEB_SHIFT		(16)
+#define   DSPFW_SPRITEB_MASK		(0x7f<<16) /* g4x */
+#define   DSPFW_SPRITEB_MASK_VLV	(0xff<<16) /* vlv/chv */
+#define   DSPFW_CURSORA_SHIFT		8
+#define   DSPFW_CURSORA_MASK		(0x3f<<8)
+#define   DSPFW_PLANEC_SHIFT_OLD	0
+#define   DSPFW_PLANEC_MASK_OLD		(0x7f<<0) /* pre-gen4 sprite C */
+#define   DSPFW_SPRITEA_SHIFT		0
+#define   DSPFW_SPRITEA_MASK		(0x7f<<0) /* g4x */
+#define   DSPFW_SPRITEA_MASK_VLV	(0xff<<0) /* vlv/chv */
 #define DSPFW3			(dev_priv->info.display_mmio_offset + 0x7003c)
-#define   DSPFW_HPLL_SR_EN	(1<<31)
-#define   DSPFW_CURSOR_SR_SHIFT	24
+#define   DSPFW_HPLL_SR_EN		(1<<31)
 #define   PINEVIEW_SELF_REFRESH_EN	(1<<30)
+#define   DSPFW_CURSOR_SR_SHIFT		24
 #define   DSPFW_CURSOR_SR_MASK		(0x3f<<24)
 #define   DSPFW_HPLL_CURSOR_SHIFT	16
 #define   DSPFW_HPLL_CURSOR_MASK	(0x3f<<16)
-#define   DSPFW_HPLL_SR_MASK		(0x1ff)
-#define DSPFW4			(dev_priv->info.display_mmio_offset + 0x70070)
-#define DSPFW7			(dev_priv->info.display_mmio_offset + 0x7007c)
+#define   DSPFW_HPLL_SR_SHIFT		0
+#define   DSPFW_HPLL_SR_MASK		(0x1ff<<0)
+
+/* vlv/chv */
+#define DSPFW4			(VLV_DISPLAY_BASE + 0x70070)
+#define   DSPFW_SPRITEB_WM1_SHIFT	16
+#define   DSPFW_SPRITEB_WM1_MASK	(0xff<<16)
+#define   DSPFW_CURSORA_WM1_SHIFT	8
+#define   DSPFW_CURSORA_WM1_MASK	(0x3f<<8)
+#define   DSPFW_SPRITEA_WM1_SHIFT	0
+#define   DSPFW_SPRITEA_WM1_MASK	(0xff<<0)
+#define DSPFW5			(VLV_DISPLAY_BASE + 0x70074)
+#define   DSPFW_PLANEB_WM1_SHIFT	24
+#define   DSPFW_PLANEB_WM1_MASK		(0xff<<24)
+#define   DSPFW_PLANEA_WM1_SHIFT	16
+#define   DSPFW_PLANEA_WM1_MASK		(0xff<<16)
+#define   DSPFW_CURSORB_WM1_SHIFT	8
+#define   DSPFW_CURSORB_WM1_MASK	(0x3f<<8)
+#define   DSPFW_CURSOR_SR_WM1_SHIFT	0
+#define   DSPFW_CURSOR_SR_WM1_MASK	(0x3f<<0)
+#define DSPFW6			(VLV_DISPLAY_BASE + 0x70078)
+#define   DSPFW_SR_WM1_SHIFT		0
+#define   DSPFW_SR_WM1_MASK		(0x1ff<<0)
+#define DSPFW7			(VLV_DISPLAY_BASE + 0x7007c)
+#define DSPFW7_CHV		(VLV_DISPLAY_BASE + 0x700b4) /* wtf #1? */
+#define   DSPFW_SPRITED_WM1_SHIFT	24
+#define   DSPFW_SPRITED_WM1_MASK	(0xff<<24)
+#define   DSPFW_SPRITED_SHIFT		16
+#define   DSPFW_SPRITED_MASK		(0xff<<16)
+#define   DSPFW_SPRITEC_WM1_SHIFT	8
+#define   DSPFW_SPRITEC_WM1_MASK	(0xff<<8)
+#define   DSPFW_SPRITEC_SHIFT		0
+#define   DSPFW_SPRITEC_MASK		(0xff<<0)
+#define DSPFW8_CHV		(VLV_DISPLAY_BASE + 0x700b8)
+#define   DSPFW_SPRITEF_WM1_SHIFT	24
+#define   DSPFW_SPRITEF_WM1_MASK	(0xff<<24)
+#define   DSPFW_SPRITEF_SHIFT		16
+#define   DSPFW_SPRITEF_MASK		(0xff<<16)
+#define   DSPFW_SPRITEE_WM1_SHIFT	8
+#define   DSPFW_SPRITEE_WM1_MASK	(0xff<<8)
+#define   DSPFW_SPRITEE_SHIFT		0
+#define   DSPFW_SPRITEE_MASK		(0xff<<0)
+#define DSPFW9_CHV		(VLV_DISPLAY_BASE + 0x7007c) /* wtf #2? */
+#define   DSPFW_PLANEC_WM1_SHIFT	24
+#define   DSPFW_PLANEC_WM1_MASK		(0xff<<24)
+#define   DSPFW_PLANEC_SHIFT		16
+#define   DSPFW_PLANEC_MASK		(0xff<<16)
+#define   DSPFW_CURSORC_WM1_SHIFT	8
+#define   DSPFW_CURSORC_WM1_MASK	(0x3f<<16)
+#define   DSPFW_CURSORC_SHIFT		0
+#define   DSPFW_CURSORC_MASK		(0x3f<<0)
+
+/* vlv/chv high order bits */
+#define DSPHOWM			(VLV_DISPLAY_BASE + 0x70064)
+#define   DSPFW_SR_HI_SHIFT		24
+#define   DSPFW_SR_HI_MASK		(1<<24)
+#define   DSPFW_SPRITEF_HI_SHIFT	23
+#define   DSPFW_SPRITEF_HI_MASK		(1<<23)
+#define   DSPFW_SPRITEE_HI_SHIFT	22
+#define   DSPFW_SPRITEE_HI_MASK		(1<<22)
+#define   DSPFW_PLANEC_HI_SHIFT		21
+#define   DSPFW_PLANEC_HI_MASK		(1<<21)
+#define   DSPFW_SPRITED_HI_SHIFT	20
+#define   DSPFW_SPRITED_HI_MASK		(1<<20)
+#define   DSPFW_SPRITEC_HI_SHIFT	16
+#define   DSPFW_SPRITEC_HI_MASK		(1<<16)
+#define   DSPFW_PLANEB_HI_SHIFT		12
+#define   DSPFW_PLANEB_HI_MASK		(1<<12)
+#define   DSPFW_SPRITEB_HI_SHIFT	8
+#define   DSPFW_SPRITEB_HI_MASK		(1<<8)
+#define   DSPFW_SPRITEA_HI_SHIFT	4
+#define   DSPFW_SPRITEA_HI_MASK		(1<<4)
+#define   DSPFW_PLANEA_HI_SHIFT		0
+#define   DSPFW_PLANEA_HI_MASK		(1<<0)
+#define DSPHOWM1		(VLV_DISPLAY_BASE + 0x70068)
+#define   DSPFW_SR_WM1_HI_SHIFT		24
+#define   DSPFW_SR_WM1_HI_MASK		(1<<24)
+#define   DSPFW_SPRITEF_WM1_HI_SHIFT	23
+#define   DSPFW_SPRITEF_WM1_HI_MASK	(1<<23)
+#define   DSPFW_SPRITEE_WM1_HI_SHIFT	22
+#define   DSPFW_SPRITEE_WM1_HI_MASK	(1<<22)
+#define   DSPFW_PLANEC_WM1_HI_SHIFT	21
+#define   DSPFW_PLANEC_WM1_HI_MASK	(1<<21)
+#define   DSPFW_SPRITED_WM1_HI_SHIFT	20
+#define   DSPFW_SPRITED_WM1_HI_MASK	(1<<20)
+#define   DSPFW_SPRITEC_WM1_HI_SHIFT	16
+#define   DSPFW_SPRITEC_WM1_HI_MASK	(1<<16)
+#define   DSPFW_PLANEB_WM1_HI_SHIFT	12
+#define   DSPFW_PLANEB_WM1_HI_MASK	(1<<12)
+#define   DSPFW_SPRITEB_WM1_HI_SHIFT	8
+#define   DSPFW_SPRITEB_WM1_HI_MASK	(1<<8)
+#define   DSPFW_SPRITEA_WM1_HI_SHIFT	4
+#define   DSPFW_SPRITEA_WM1_HI_MASK	(1<<4)
+#define   DSPFW_PLANEA_WM1_HI_SHIFT	0
+#define   DSPFW_PLANEA_WM1_HI_MASK	(1<<0)
 
 /* drain latency register values*/
+#define DRAIN_LATENCY_PRECISION_16	16
 #define DRAIN_LATENCY_PRECISION_32	32
 #define DRAIN_LATENCY_PRECISION_64	64
-#define VLV_DDL1			(VLV_DISPLAY_BASE + 0x70050)
-#define DDL_CURSORA_PRECISION_64	(1<<31)
-#define DDL_CURSORA_PRECISION_32	(0<<31)
-#define DDL_CURSORA_SHIFT		24
-#define DDL_SPRITEB_PRECISION_64	(1<<23)
-#define DDL_SPRITEB_PRECISION_32	(0<<23)
-#define DDL_SPRITEB_SHIFT		16
-#define DDL_SPRITEA_PRECISION_64	(1<<15)
-#define DDL_SPRITEA_PRECISION_32	(0<<15)
-#define DDL_SPRITEA_SHIFT		8
-#define DDL_PLANEA_PRECISION_64		(1<<7)
-#define DDL_PLANEA_PRECISION_32		(0<<7)
-#define DDL_PLANEA_SHIFT		0
-
-#define VLV_DDL2			(VLV_DISPLAY_BASE + 0x70054)
-#define DDL_CURSORB_PRECISION_64	(1<<31)
-#define DDL_CURSORB_PRECISION_32	(0<<31)
-#define DDL_CURSORB_SHIFT		24
-#define DDL_SPRITED_PRECISION_64	(1<<23)
-#define DDL_SPRITED_PRECISION_32	(0<<23)
-#define DDL_SPRITED_SHIFT		16
-#define DDL_SPRITEC_PRECISION_64	(1<<15)
-#define DDL_SPRITEC_PRECISION_32	(0<<15)
-#define DDL_SPRITEC_SHIFT		8
-#define DDL_PLANEB_PRECISION_64		(1<<7)
-#define DDL_PLANEB_PRECISION_32		(0<<7)
-#define DDL_PLANEB_SHIFT		0
-
-#define VLV_DDL3			(VLV_DISPLAY_BASE + 0x70058)
-#define DDL_CURSORC_PRECISION_64	(1<<31)
-#define DDL_CURSORC_PRECISION_32	(0<<31)
-#define DDL_CURSORC_SHIFT		24
-#define DDL_SPRITEF_PRECISION_64	(1<<23)
-#define DDL_SPRITEF_PRECISION_32	(0<<23)
-#define DDL_SPRITEF_SHIFT		16
-#define DDL_SPRITEE_PRECISION_64	(1<<15)
-#define DDL_SPRITEE_PRECISION_32	(0<<15)
-#define DDL_SPRITEE_SHIFT		8
-#define DDL_PLANEC_PRECISION_64		(1<<7)
-#define DDL_PLANEC_PRECISION_32		(0<<7)
-#define DDL_PLANEC_SHIFT		0
+#define VLV_DDL(pipe)			(VLV_DISPLAY_BASE + 0x70050 + 4 * (pipe))
+#define DDL_CURSOR_PRECISION_HIGH	(1<<31)
+#define DDL_CURSOR_PRECISION_LOW	(0<<31)
+#define DDL_CURSOR_SHIFT		24
+#define DDL_SPRITE_PRECISION_HIGH(sprite)	(1<<(15+8*(sprite)))
+#define DDL_SPRITE_PRECISION_LOW(sprite)	(0<<(15+8*(sprite)))
+#define DDL_SPRITE_SHIFT(sprite)	(8+8*(sprite))
+#define DDL_PLANE_PRECISION_HIGH	(1<<7)
+#define DDL_PLANE_PRECISION_LOW		(0<<7)
+#define DDL_PLANE_SHIFT			0
+#define DRAIN_LATENCY_MASK		0x7f
 
 /* FIFO watermark sizes etc */
 #define G4X_FIFO_LINE_SIZE	64
@@ -3943,6 +4118,41 @@
 #define I965_CURSOR_MAX_WM	32
 #define I965_CURSOR_DFT_WM	8
 
+/* Watermark register definitions for SKL */
+#define CUR_WM_A_0		0x70140
+#define CUR_WM_B_0		0x71140
+#define PLANE_WM_1_A_0		0x70240
+#define PLANE_WM_1_B_0		0x71240
+#define PLANE_WM_2_A_0		0x70340
+#define PLANE_WM_2_B_0		0x71340
+#define PLANE_WM_TRANS_1_A_0	0x70268
+#define PLANE_WM_TRANS_1_B_0	0x71268
+#define PLANE_WM_TRANS_2_A_0	0x70368
+#define PLANE_WM_TRANS_2_B_0	0x71368
+#define CUR_WM_TRANS_A_0	0x70168
+#define CUR_WM_TRANS_B_0	0x71168
+#define   PLANE_WM_EN		(1 << 31)
+#define   PLANE_WM_LINES_SHIFT	14
+#define   PLANE_WM_LINES_MASK	0x1f
+#define   PLANE_WM_BLOCKS_MASK	0x3ff
+
+#define CUR_WM_0(pipe) _PIPE(pipe, CUR_WM_A_0, CUR_WM_B_0)
+#define CUR_WM(pipe, level) (CUR_WM_0(pipe) + ((4) * (level)))
+#define CUR_WM_TRANS(pipe) _PIPE(pipe, CUR_WM_TRANS_A_0, CUR_WM_TRANS_B_0)
+
+#define _PLANE_WM_1(pipe) _PIPE(pipe, PLANE_WM_1_A_0, PLANE_WM_1_B_0)
+#define _PLANE_WM_2(pipe) _PIPE(pipe, PLANE_WM_2_A_0, PLANE_WM_2_B_0)
+#define _PLANE_WM_BASE(pipe, plane)	\
+			_PLANE(plane, _PLANE_WM_1(pipe), _PLANE_WM_2(pipe))
+#define PLANE_WM(pipe, plane, level)	\
+			(_PLANE_WM_BASE(pipe, plane) + ((4) * (level)))
+#define _PLANE_WM_TRANS_1(pipe)	\
+			_PIPE(pipe, PLANE_WM_TRANS_1_A_0, PLANE_WM_TRANS_1_B_0)
+#define _PLANE_WM_TRANS_2(pipe)	\
+			_PIPE(pipe, PLANE_WM_TRANS_2_A_0, PLANE_WM_TRANS_2_B_0)
+#define PLANE_WM_TRANS(pipe, plane)	\
+		_PLANE(plane, _PLANE_WM_TRANS_1(pipe), _PLANE_WM_TRANS_2(pipe))
+
 /* define the Watermark register on Ironlake */
 #define WM0_PIPEA_ILK		0x45100
 #define  WM0_PIPE_PLANE_MASK	(0xffff<<16)
@@ -4026,7 +4236,8 @@
 /* Old style CUR*CNTR flags (desktop 8xx) */
 #define   CURSOR_ENABLE		0x80000000
 #define   CURSOR_GAMMA_ENABLE	0x40000000
-#define   CURSOR_STRIDE_MASK	0x30000000
+#define   CURSOR_STRIDE_SHIFT	28
+#define   CURSOR_STRIDE(x)	((ffs(x)-9) << CURSOR_STRIDE_SHIFT) /* 256,512,1k,2k */
 #define   CURSOR_PIPE_CSC_ENABLE (1<<24)
 #define   CURSOR_FORMAT_SHIFT	24
 #define   CURSOR_FORMAT_MASK	(0x07 << CURSOR_FORMAT_SHIFT)
@@ -4048,6 +4259,7 @@
 #define   MCURSOR_PIPE_A	0x00
 #define   MCURSOR_PIPE_B	(1 << 28)
 #define   MCURSOR_GAMMA_ENABLE  (1 << 26)
+#define   CURSOR_ROTATE_180	(1<<15)
 #define   CURSOR_TRICKLE_FEED_DISABLE	(1 << 14)
 #define _CURABASE		0x70084
 #define _CURAPOS		0x70088
@@ -4111,8 +4323,11 @@
 #define   DISPPLANE_NO_LINE_DOUBLE		0
 #define   DISPPLANE_STEREO_POLARITY_FIRST	0
 #define   DISPPLANE_STEREO_POLARITY_SECOND	(1<<18)
+#define   DISPPLANE_ALPHA_PREMULTIPLY		(1<<16) /* CHV pipe B */
+#define   DISPPLANE_ROTATE_180			(1<<15)
 #define   DISPPLANE_TRICKLE_FEED_DISABLE	(1<<14) /* Ironlake */
 #define   DISPPLANE_TILED			(1<<10)
+#define   DISPPLANE_MIRROR			(1<<8) /* CHV pipe B */
 #define _DSPAADDR				0x70184
 #define _DSPASTRIDE				0x70188
 #define _DSPAPOS				0x7018C /* reserved */
@@ -4133,6 +4348,24 @@
 #define DSPOFFSET(plane) _PIPE2(plane, _DSPAOFFSET)
 #define DSPSURFLIVE(plane) _PIPE2(plane, _DSPASURFLIVE)
 
+/* CHV pipe B blender and primary plane */
+#define _CHV_BLEND_A		0x60a00
+#define   CHV_BLEND_LEGACY		(0<<30)
+#define   CHV_BLEND_ANDROID		(1<<30)
+#define   CHV_BLEND_MPO			(2<<30)
+#define   CHV_BLEND_MASK		(3<<30)
+#define _CHV_CANVAS_A		0x60a04
+#define _PRIMPOS_A		0x60a08
+#define _PRIMSIZE_A		0x60a0c
+#define _PRIMCNSTALPHA_A	0x60a10
+#define   PRIM_CONST_ALPHA_ENABLE	(1<<31)
+
+#define CHV_BLEND(pipe) _TRANSCODER2(pipe, _CHV_BLEND_A)
+#define CHV_CANVAS(pipe) _TRANSCODER2(pipe, _CHV_CANVAS_A)
+#define PRIMPOS(plane) _TRANSCODER2(plane, _PRIMPOS_A)
+#define PRIMSIZE(plane) _TRANSCODER2(plane, _PRIMSIZE_A)
+#define PRIMCNSTALPHA(plane) _TRANSCODER2(plane, _PRIMCNSTALPHA_A)
+
 /* Display/Sprite base address macros */
 #define DISP_BASEADDR_MASK	(0xfffff000)
 #define I915_LO_DISPBASE(val)	(val & ~DISP_BASEADDR_MASK)
@@ -4195,6 +4428,7 @@
 #define   DVS_YUV_ORDER_UYVY	(1<<16)
 #define   DVS_YUV_ORDER_YVYU	(2<<16)
 #define   DVS_YUV_ORDER_VYUY	(3<<16)
+#define   DVS_ROTATE_180	(1<<15)
 #define   DVS_DEST_KEY		(1<<2)
 #define   DVS_TRICKLE_FEED_DISABLE (1<<14)
 #define   DVS_TILED		(1<<10)
@@ -4265,6 +4499,7 @@
 #define   SPRITE_YUV_ORDER_UYVY		(1<<16)
 #define   SPRITE_YUV_ORDER_YVYU		(2<<16)
 #define   SPRITE_YUV_ORDER_VYUY		(3<<16)
+#define   SPRITE_ROTATE_180		(1<<15)
 #define   SPRITE_TRICKLE_FEED_DISABLE	(1<<14)
 #define   SPRITE_INT_GAMMA_ENABLE	(1<<13)
 #define   SPRITE_TILED			(1<<10)
@@ -4332,13 +4567,16 @@
 #define   SP_FORMAT_RGBA1010102		(9<<26)
 #define   SP_FORMAT_RGBX8888		(0xe<<26)
 #define   SP_FORMAT_RGBA8888		(0xf<<26)
+#define   SP_ALPHA_PREMULTIPLY		(1<<23) /* CHV pipe B */
 #define   SP_SOURCE_KEY			(1<<22)
 #define   SP_YUV_BYTE_ORDER_MASK	(3<<16)
 #define   SP_YUV_ORDER_YUYV		(0<<16)
 #define   SP_YUV_ORDER_UYVY		(1<<16)
 #define   SP_YUV_ORDER_YVYU		(2<<16)
 #define   SP_YUV_ORDER_VYUY		(3<<16)
+#define   SP_ROTATE_180			(1<<15)
 #define   SP_TILED			(1<<10)
+#define   SP_MIRROR			(1<<8) /* CHV pipe B */
 #define _SPALINOFF		(VLV_DISPLAY_BASE + 0x72184)
 #define _SPASTRIDE		(VLV_DISPLAY_BASE + 0x72188)
 #define _SPAPOS			(VLV_DISPLAY_BASE + 0x7218c)
@@ -4349,6 +4587,7 @@
 #define _SPAKEYMAXVAL		(VLV_DISPLAY_BASE + 0x721a0)
 #define _SPATILEOFF		(VLV_DISPLAY_BASE + 0x721a4)
 #define _SPACONSTALPHA		(VLV_DISPLAY_BASE + 0x721a8)
+#define   SP_CONST_ALPHA_ENABLE		(1<<31)
 #define _SPAGAMC		(VLV_DISPLAY_BASE + 0x721f4)
 
 #define _SPBCNTR		(VLV_DISPLAY_BASE + 0x72280)
@@ -4377,6 +4616,195 @@
 #define SPCONSTALPHA(pipe, plane) _PIPE(pipe * 2 + plane, _SPACONSTALPHA, _SPBCONSTALPHA)
 #define SPGAMC(pipe, plane) _PIPE(pipe * 2 + plane, _SPAGAMC, _SPBGAMC)
 
+/*
+ * CHV pipe B sprite CSC
+ *
+ * |cr|   |c0 c1 c2|   |cr + cr_ioff|   |cr_ooff|
+ * |yg| = |c3 c4 c5| x |yg + yg_ioff| + |yg_ooff|
+ * |cb|   |c6 c7 c8|   |cb + cr_ioff|   |cb_ooff|
+ */
+#define SPCSCYGOFF(sprite)	(VLV_DISPLAY_BASE + 0x6d900 + (sprite) * 0x1000)
+#define SPCSCCBOFF(sprite)	(VLV_DISPLAY_BASE + 0x6d904 + (sprite) * 0x1000)
+#define SPCSCCROFF(sprite)	(VLV_DISPLAY_BASE + 0x6d908 + (sprite) * 0x1000)
+#define  SPCSC_OOFF(x)		(((x) & 0x7ff) << 16) /* s11 */
+#define  SPCSC_IOFF(x)		(((x) & 0x7ff) << 0) /* s11 */
+
+#define SPCSCC01(sprite)	(VLV_DISPLAY_BASE + 0x6d90c + (sprite) * 0x1000)
+#define SPCSCC23(sprite)	(VLV_DISPLAY_BASE + 0x6d910 + (sprite) * 0x1000)
+#define SPCSCC45(sprite)	(VLV_DISPLAY_BASE + 0x6d914 + (sprite) * 0x1000)
+#define SPCSCC67(sprite)	(VLV_DISPLAY_BASE + 0x6d918 + (sprite) * 0x1000)
+#define SPCSCC8(sprite)		(VLV_DISPLAY_BASE + 0x6d91c + (sprite) * 0x1000)
+#define  SPCSC_C1(x)		(((x) & 0x7fff) << 16) /* s3.12 */
+#define  SPCSC_C0(x)		(((x) & 0x7fff) << 0) /* s3.12 */
+
+#define SPCSCYGICLAMP(sprite)	(VLV_DISPLAY_BASE + 0x6d920 + (sprite) * 0x1000)
+#define SPCSCCBICLAMP(sprite)	(VLV_DISPLAY_BASE + 0x6d924 + (sprite) * 0x1000)
+#define SPCSCCRICLAMP(sprite)	(VLV_DISPLAY_BASE + 0x6d928 + (sprite) * 0x1000)
+#define  SPCSC_IMAX(x)		(((x) & 0x7ff) << 16) /* s11 */
+#define  SPCSC_IMIN(x)		(((x) & 0x7ff) << 0) /* s11 */
+
+#define SPCSCYGOCLAMP(sprite)	(VLV_DISPLAY_BASE + 0x6d92c + (sprite) * 0x1000)
+#define SPCSCCBOCLAMP(sprite)	(VLV_DISPLAY_BASE + 0x6d930 + (sprite) * 0x1000)
+#define SPCSCCROCLAMP(sprite)	(VLV_DISPLAY_BASE + 0x6d934 + (sprite) * 0x1000)
+#define  SPCSC_OMAX(x)		((x) << 16) /* u10 */
+#define  SPCSC_OMIN(x)		((x) << 0) /* u10 */
+
+/* Skylake plane registers */
+
+#define _PLANE_CTL_1_A				0x70180
+#define _PLANE_CTL_2_A				0x70280
+#define _PLANE_CTL_3_A				0x70380
+#define   PLANE_CTL_ENABLE			(1 << 31)
+#define   PLANE_CTL_PIPE_GAMMA_ENABLE		(1 << 30)
+#define   PLANE_CTL_FORMAT_MASK			(0xf << 24)
+#define   PLANE_CTL_FORMAT_YUV422		(  0 << 24)
+#define   PLANE_CTL_FORMAT_NV12			(  1 << 24)
+#define   PLANE_CTL_FORMAT_XRGB_2101010		(  2 << 24)
+#define   PLANE_CTL_FORMAT_XRGB_8888		(  4 << 24)
+#define   PLANE_CTL_FORMAT_XRGB_16161616F	(  6 << 24)
+#define   PLANE_CTL_FORMAT_AYUV			(  8 << 24)
+#define   PLANE_CTL_FORMAT_INDEXED		( 12 << 24)
+#define   PLANE_CTL_FORMAT_RGB_565		( 14 << 24)
+#define   PLANE_CTL_PIPE_CSC_ENABLE		(1 << 23)
+#define   PLANE_CTL_KEY_ENABLE_MASK		(0x3 << 21)
+#define   PLANE_CTL_KEY_ENABLE_SOURCE		(  1 << 21)
+#define   PLANE_CTL_KEY_ENABLE_DESTINATION	(  2 << 21)
+#define   PLANE_CTL_ORDER_BGRX			(0 << 20)
+#define   PLANE_CTL_ORDER_RGBX			(1 << 20)
+#define   PLANE_CTL_YUV422_ORDER_MASK		(0x3 << 16)
+#define   PLANE_CTL_YUV422_YUYV			(  0 << 16)
+#define   PLANE_CTL_YUV422_UYVY			(  1 << 16)
+#define   PLANE_CTL_YUV422_YVYU			(  2 << 16)
+#define   PLANE_CTL_YUV422_VYUY			(  3 << 16)
+#define   PLANE_CTL_DECOMPRESSION_ENABLE	(1 << 15)
+#define   PLANE_CTL_TRICKLE_FEED_DISABLE	(1 << 14)
+#define   PLANE_CTL_PLANE_GAMMA_DISABLE		(1 << 13)
+#define   PLANE_CTL_TILED_MASK			(0x7 << 10)
+#define   PLANE_CTL_TILED_LINEAR		(  0 << 10)
+#define   PLANE_CTL_TILED_X			(  1 << 10)
+#define   PLANE_CTL_TILED_Y			(  4 << 10)
+#define   PLANE_CTL_TILED_YF			(  5 << 10)
+#define   PLANE_CTL_ALPHA_MASK			(0x3 << 4)
+#define   PLANE_CTL_ALPHA_DISABLE		(  0 << 4)
+#define   PLANE_CTL_ALPHA_SW_PREMULTIPLY	(  2 << 4)
+#define   PLANE_CTL_ALPHA_HW_PREMULTIPLY	(  3 << 4)
+#define   PLANE_CTL_ROTATE_MASK			0x3
+#define   PLANE_CTL_ROTATE_0			0x0
+#define   PLANE_CTL_ROTATE_180			0x2
+#define _PLANE_STRIDE_1_A			0x70188
+#define _PLANE_STRIDE_2_A			0x70288
+#define _PLANE_STRIDE_3_A			0x70388
+#define _PLANE_POS_1_A				0x7018c
+#define _PLANE_POS_2_A				0x7028c
+#define _PLANE_POS_3_A				0x7038c
+#define _PLANE_SIZE_1_A				0x70190
+#define _PLANE_SIZE_2_A				0x70290
+#define _PLANE_SIZE_3_A				0x70390
+#define _PLANE_SURF_1_A				0x7019c
+#define _PLANE_SURF_2_A				0x7029c
+#define _PLANE_SURF_3_A				0x7039c
+#define _PLANE_OFFSET_1_A			0x701a4
+#define _PLANE_OFFSET_2_A			0x702a4
+#define _PLANE_OFFSET_3_A			0x703a4
+#define _PLANE_KEYVAL_1_A			0x70194
+#define _PLANE_KEYVAL_2_A			0x70294
+#define _PLANE_KEYMSK_1_A			0x70198
+#define _PLANE_KEYMSK_2_A			0x70298
+#define _PLANE_KEYMAX_1_A			0x701a0
+#define _PLANE_KEYMAX_2_A			0x702a0
+#define _PLANE_BUF_CFG_1_A			0x7027c
+#define _PLANE_BUF_CFG_2_A			0x7037c
+
+#define _PLANE_CTL_1_B				0x71180
+#define _PLANE_CTL_2_B				0x71280
+#define _PLANE_CTL_3_B				0x71380
+#define _PLANE_CTL_1(pipe)	_PIPE(pipe, _PLANE_CTL_1_A, _PLANE_CTL_1_B)
+#define _PLANE_CTL_2(pipe)	_PIPE(pipe, _PLANE_CTL_2_A, _PLANE_CTL_2_B)
+#define _PLANE_CTL_3(pipe)	_PIPE(pipe, _PLANE_CTL_3_A, _PLANE_CTL_3_B)
+#define PLANE_CTL(pipe, plane)	\
+	_PLANE(plane, _PLANE_CTL_1(pipe), _PLANE_CTL_2(pipe))
+
+#define _PLANE_STRIDE_1_B			0x71188
+#define _PLANE_STRIDE_2_B			0x71288
+#define _PLANE_STRIDE_3_B			0x71388
+#define _PLANE_STRIDE_1(pipe)	\
+	_PIPE(pipe, _PLANE_STRIDE_1_A, _PLANE_STRIDE_1_B)
+#define _PLANE_STRIDE_2(pipe)	\
+	_PIPE(pipe, _PLANE_STRIDE_2_A, _PLANE_STRIDE_2_B)
+#define _PLANE_STRIDE_3(pipe)	\
+	_PIPE(pipe, _PLANE_STRIDE_3_A, _PLANE_STRIDE_3_B)
+#define PLANE_STRIDE(pipe, plane)	\
+	_PLANE(plane, _PLANE_STRIDE_1(pipe), _PLANE_STRIDE_2(pipe))
+
+#define _PLANE_POS_1_B				0x7118c
+#define _PLANE_POS_2_B				0x7128c
+#define _PLANE_POS_3_B				0x7138c
+#define _PLANE_POS_1(pipe)	_PIPE(pipe, _PLANE_POS_1_A, _PLANE_POS_1_B)
+#define _PLANE_POS_2(pipe)	_PIPE(pipe, _PLANE_POS_2_A, _PLANE_POS_2_B)
+#define _PLANE_POS_3(pipe)	_PIPE(pipe, _PLANE_POS_3_A, _PLANE_POS_3_B)
+#define PLANE_POS(pipe, plane)	\
+	_PLANE(plane, _PLANE_POS_1(pipe), _PLANE_POS_2(pipe))
+
+#define _PLANE_SIZE_1_B				0x71190
+#define _PLANE_SIZE_2_B				0x71290
+#define _PLANE_SIZE_3_B				0x71390
+#define _PLANE_SIZE_1(pipe)	_PIPE(pipe, _PLANE_SIZE_1_A, _PLANE_SIZE_1_B)
+#define _PLANE_SIZE_2(pipe)	_PIPE(pipe, _PLANE_SIZE_2_A, _PLANE_SIZE_2_B)
+#define _PLANE_SIZE_3(pipe)	_PIPE(pipe, _PLANE_SIZE_3_A, _PLANE_SIZE_3_B)
+#define PLANE_SIZE(pipe, plane)	\
+	_PLANE(plane, _PLANE_SIZE_1(pipe), _PLANE_SIZE_2(pipe))
+
+#define _PLANE_SURF_1_B				0x7119c
+#define _PLANE_SURF_2_B				0x7129c
+#define _PLANE_SURF_3_B				0x7139c
+#define _PLANE_SURF_1(pipe)	_PIPE(pipe, _PLANE_SURF_1_A, _PLANE_SURF_1_B)
+#define _PLANE_SURF_2(pipe)	_PIPE(pipe, _PLANE_SURF_2_A, _PLANE_SURF_2_B)
+#define _PLANE_SURF_3(pipe)	_PIPE(pipe, _PLANE_SURF_3_A, _PLANE_SURF_3_B)
+#define PLANE_SURF(pipe, plane)	\
+	_PLANE(plane, _PLANE_SURF_1(pipe), _PLANE_SURF_2(pipe))
+
+#define _PLANE_OFFSET_1_B			0x711a4
+#define _PLANE_OFFSET_2_B			0x712a4
+#define _PLANE_OFFSET_1(pipe) _PIPE(pipe, _PLANE_OFFSET_1_A, _PLANE_OFFSET_1_B)
+#define _PLANE_OFFSET_2(pipe) _PIPE(pipe, _PLANE_OFFSET_2_A, _PLANE_OFFSET_2_B)
+#define PLANE_OFFSET(pipe, plane)	\
+	_PLANE(plane, _PLANE_OFFSET_1(pipe), _PLANE_OFFSET_2(pipe))
+
+#define _PLANE_KEYVAL_1_B			0x71194
+#define _PLANE_KEYVAL_2_B			0x71294
+#define _PLANE_KEYVAL_1(pipe) _PIPE(pipe, _PLANE_KEYVAL_1_A, _PLANE_KEYVAL_1_B)
+#define _PLANE_KEYVAL_2(pipe) _PIPE(pipe, _PLANE_KEYVAL_2_A, _PLANE_KEYVAL_2_B)
+#define PLANE_KEYVAL(pipe, plane)	\
+	_PLANE(plane, _PLANE_KEYVAL_1(pipe), _PLANE_KEYVAL_2(pipe))
+
+#define _PLANE_KEYMSK_1_B			0x71198
+#define _PLANE_KEYMSK_2_B			0x71298
+#define _PLANE_KEYMSK_1(pipe) _PIPE(pipe, _PLANE_KEYMSK_1_A, _PLANE_KEYMSK_1_B)
+#define _PLANE_KEYMSK_2(pipe) _PIPE(pipe, _PLANE_KEYMSK_2_A, _PLANE_KEYMSK_2_B)
+#define PLANE_KEYMSK(pipe, plane)	\
+	_PLANE(plane, _PLANE_KEYMSK_1(pipe), _PLANE_KEYMSK_2(pipe))
+
+#define _PLANE_KEYMAX_1_B			0x711a0
+#define _PLANE_KEYMAX_2_B			0x712a0
+#define _PLANE_KEYMAX_1(pipe) _PIPE(pipe, _PLANE_KEYMAX_1_A, _PLANE_KEYMAX_1_B)
+#define _PLANE_KEYMAX_2(pipe) _PIPE(pipe, _PLANE_KEYMAX_2_A, _PLANE_KEYMAX_2_B)
+#define PLANE_KEYMAX(pipe, plane)	\
+	_PLANE(plane, _PLANE_KEYMAX_1(pipe), _PLANE_KEYMAX_2(pipe))
+
+#define _PLANE_BUF_CFG_1_B			0x7127c
+#define _PLANE_BUF_CFG_2_B			0x7137c
+#define _PLANE_BUF_CFG_1(pipe)	\
+	_PIPE(pipe, _PLANE_BUF_CFG_1_A, _PLANE_BUF_CFG_1_B)
+#define _PLANE_BUF_CFG_2(pipe)	\
+	_PIPE(pipe, _PLANE_BUF_CFG_2_A, _PLANE_BUF_CFG_2_B)
+#define PLANE_BUF_CFG(pipe, plane)	\
+	_PLANE(plane, _PLANE_BUF_CFG_1(pipe), _PLANE_BUF_CFG_2(pipe))
+
+/* SKL new cursor registers */
+#define _CUR_BUF_CFG_A				0x7017c
+#define _CUR_BUF_CFG_B				0x7117c
+#define CUR_BUF_CFG(pipe)	_PIPE(pipe, _CUR_BUF_CFG_A, _CUR_BUF_CFG_B)
+
 /* VBIOS regs */
 #define VGACNTRL		0x71400
 # define VGA_DISP_DISABLE			(1 << 31)
@@ -4492,6 +4920,18 @@
 #define PF_VSCALE(pipe)		_PIPE(pipe, _PFA_VSCALE, _PFB_VSCALE)
 #define PF_HSCALE(pipe)		_PIPE(pipe, _PFA_HSCALE, _PFB_HSCALE)
 
+#define _PSA_CTL		0x68180
+#define _PSB_CTL		0x68980
+#define PS_ENABLE		(1<<31)
+#define _PSA_WIN_SZ		0x68174
+#define _PSB_WIN_SZ		0x68974
+#define _PSA_WIN_POS		0x68170
+#define _PSB_WIN_POS		0x68970
+
+#define PS_CTL(pipe)		_PIPE(pipe, _PSA_CTL, _PSB_CTL)
+#define PS_WIN_SZ(pipe)		_PIPE(pipe, _PSA_WIN_SZ, _PSB_WIN_SZ)
+#define PS_WIN_POS(pipe)	_PIPE(pipe, _PSA_WIN_POS, _PSB_WIN_POS)
+
 /* legacy palette */
 #define _LGC_PALETTE_A           0x4a000
 #define _LGC_PALETTE_B           0x4a800
@@ -4613,16 +5053,32 @@
 #define  GEN8_PIPE_SCAN_LINE_EVENT	(1 << 2)
 #define  GEN8_PIPE_VSYNC		(1 << 1)
 #define  GEN8_PIPE_VBLANK		(1 << 0)
+#define  GEN9_PIPE_CURSOR_FAULT		(1 << 11)
+#define  GEN9_PIPE_PLANE3_FAULT		(1 << 9)
+#define  GEN9_PIPE_PLANE2_FAULT		(1 << 8)
+#define  GEN9_PIPE_PLANE1_FAULT		(1 << 7)
+#define  GEN9_PIPE_PLANE3_FLIP_DONE	(1 << 5)
+#define  GEN9_PIPE_PLANE2_FLIP_DONE	(1 << 4)
+#define  GEN9_PIPE_PLANE1_FLIP_DONE	(1 << 3)
+#define  GEN9_PIPE_PLANE_FLIP_DONE(p)	(1 << (3 + p))
 #define GEN8_DE_PIPE_IRQ_FAULT_ERRORS \
 	(GEN8_PIPE_CURSOR_FAULT | \
 	 GEN8_PIPE_SPRITE_FAULT | \
 	 GEN8_PIPE_PRIMARY_FAULT)
+#define GEN9_DE_PIPE_IRQ_FAULT_ERRORS \
+	(GEN9_PIPE_CURSOR_FAULT | \
+	 GEN9_PIPE_PLANE3_FAULT | \
+	 GEN9_PIPE_PLANE2_FAULT | \
+	 GEN9_PIPE_PLANE1_FAULT)
 
 #define GEN8_DE_PORT_ISR 0x44440
 #define GEN8_DE_PORT_IMR 0x44444
 #define GEN8_DE_PORT_IIR 0x44448
 #define GEN8_DE_PORT_IER 0x4444c
 #define  GEN8_PORT_DP_A_HOTPLUG		(1 << 3)
+#define  GEN9_AUX_CHANNEL_D		(1 << 27)
+#define  GEN9_AUX_CHANNEL_C		(1 << 26)
+#define  GEN9_AUX_CHANNEL_B		(1 << 25)
 #define  GEN8_AUX_CHANNEL_A		(1 << 0)
 
 #define GEN8_DE_MISC_ISR 0x44460
@@ -4706,6 +5162,8 @@
 /* GEN8 chicken */
 #define HDC_CHICKEN0				0x7300
 #define  HDC_FORCE_NON_COHERENT			(1<<4)
+#define  HDC_DONOT_FETCH_MEM_WHEN_MASKED	(1<<11)
+#define  HDC_FENCE_DEST_SLM_DISABLE		(1<<14)
 
 /* WaCatErrorRejectionIssue */
 #define GEN7_SQ_CHICKEN_MBCUNIT_CONFIG		0x9030
@@ -5246,8 +5704,7 @@
 #define PIPEA_PP_STATUS         (VLV_DISPLAY_BASE + 0x61200)
 #define PIPEA_PP_CONTROL        (VLV_DISPLAY_BASE + 0x61204)
 #define PIPEA_PP_ON_DELAYS      (VLV_DISPLAY_BASE + 0x61208)
-#define  PANEL_PORT_SELECT_DPB_VLV	(1 << 30)
-#define  PANEL_PORT_SELECT_DPC_VLV	(2 << 30)
+#define  PANEL_PORT_SELECT_VLV(port)	((port) << 30)
 #define PIPEA_PP_OFF_DELAYS     (VLV_DISPLAY_BASE + 0x6120c)
 #define PIPEA_PP_DIVISOR        (VLV_DISPLAY_BASE + 0x61210)
 
@@ -5407,8 +5864,13 @@
 #define   VLV_GTLC_ALLOWWAKEERR			(1 << 1)
 #define   VLV_GTLC_PW_MEDIA_STATUS_MASK		(1 << 5)
 #define   VLV_GTLC_PW_RENDER_STATUS_MASK	(1 << 7)
-#define VLV_GTLC_SURVIVABILITY_REG              0x130098
 #define  FORCEWAKE_MT				0xa188 /* multi-threaded */
+#define  FORCEWAKE_MEDIA_GEN9			0xa270
+#define  FORCEWAKE_RENDER_GEN9			0xa278
+#define  FORCEWAKE_BLITTER_GEN9			0xa188
+#define  FORCEWAKE_ACK_MEDIA_GEN9		0x0D88
+#define  FORCEWAKE_ACK_RENDER_GEN9		0x0D84
+#define  FORCEWAKE_ACK_BLITTER_GEN9		0x130044
 #define   FORCEWAKE_KERNEL			0x1
 #define   FORCEWAKE_USER			0x2
 #define  FORCEWAKE_MT_ACK			0x130040
@@ -5545,12 +6007,6 @@
 						 GEN6_PM_RP_DOWN_THRESHOLD | \
 						 GEN6_PM_RP_DOWN_TIMEOUT)
 
-#define CHV_CZ_CLOCK_FREQ_MODE_200			200
-#define CHV_CZ_CLOCK_FREQ_MODE_267			267
-#define CHV_CZ_CLOCK_FREQ_MODE_320			320
-#define CHV_CZ_CLOCK_FREQ_MODE_333			333
-#define CHV_CZ_CLOCK_FREQ_MODE_400			400
-
 #define GEN7_GT_SCRATCH_BASE			0x4F100
 #define GEN7_GT_SCRATCH_REG_NUM			8
 
@@ -5571,8 +6027,8 @@
 
 #define GEN6_GT_GFX_RC6p			0x13810C
 #define GEN6_GT_GFX_RC6pp			0x138110
-#define VLV_RENDER_C0_COUNT_REG		0x138118
-#define VLV_MEDIA_C0_COUNT_REG			0x13811C
+#define VLV_RENDER_C0_COUNT			0x138118
+#define VLV_MEDIA_C0_COUNT			0x13811C
 
 #define GEN6_PCODE_MAILBOX			0x138124
 #define   GEN6_PCODE_READY			(1<<31)
@@ -5589,6 +6045,13 @@
 #define GEN6_PCODE_DATA				0x138128
 #define   GEN6_PCODE_FREQ_IA_RATIO_SHIFT	8
 #define   GEN6_PCODE_FREQ_RING_RATIO_SHIFT	16
+#define GEN6_PCODE_DATA1			0x13812C
+
+#define   GEN9_PCODE_READ_MEM_LATENCY		0x6
+#define   GEN9_MEM_LATENCY_LEVEL_MASK		0xFF
+#define   GEN9_MEM_LATENCY_LEVEL_1_5_SHIFT	8
+#define   GEN9_MEM_LATENCY_LEVEL_2_6_SHIFT	16
+#define   GEN9_MEM_LATENCY_LEVEL_3_7_SHIFT	24
 
 #define GEN6_GT_CORE_STATUS		0x138060
 #define   GEN6_CORE_CPD_STATE_MASK	(7<<4)
@@ -5626,6 +6089,9 @@
 #define   GEN7_SINGLE_SUBSCAN_DISPATCH_ENABLE	(1<<10)
 #define   GEN7_PSD_SINGLE_PORT_DISPATCH_ENABLE	(1<<3)
 
+#define GEN9_HALF_SLICE_CHICKEN5	0xe188
+#define   GEN9_DG_MIRROR_FIX_ENABLE	(1<<5)
+
 #define GEN8_ROW_CHICKEN		0xe4f0
 #define   PARTIAL_INSTRUCTION_SHOOTDOWN_DISABLE	(1<<8)
 #define   STALL_DOP_GATING_DISABLE		(1<<5)
@@ -5641,57 +6107,58 @@
 #define   GEN8_CENTROID_PIXEL_OPT_DIS	(1<<8)
 #define   GEN8_SAMPLER_POWER_BYPASS_DIS	(1<<1)
 
+/* Audio */
 #define G4X_AUD_VID_DID			(dev_priv->info.display_mmio_offset + 0x62020)
-#define INTEL_AUDIO_DEVCL		0x808629FB
-#define INTEL_AUDIO_DEVBLC		0x80862801
-#define INTEL_AUDIO_DEVCTG		0x80862802
+#define   INTEL_AUDIO_DEVCL		0x808629FB
+#define   INTEL_AUDIO_DEVBLC		0x80862801
+#define   INTEL_AUDIO_DEVCTG		0x80862802
 
 #define G4X_AUD_CNTL_ST			0x620B4
-#define G4X_ELDV_DEVCL_DEVBLC		(1 << 13)
-#define G4X_ELDV_DEVCTG			(1 << 14)
-#define G4X_ELD_ADDR			(0xf << 5)
-#define G4X_ELD_ACK			(1 << 4)
+#define   G4X_ELDV_DEVCL_DEVBLC		(1 << 13)
+#define   G4X_ELDV_DEVCTG		(1 << 14)
+#define   G4X_ELD_ADDR_MASK		(0xf << 5)
+#define   G4X_ELD_ACK			(1 << 4)
 #define G4X_HDMIW_HDMIEDID		0x6210C
 
-#define IBX_HDMIW_HDMIEDID_A		0xE2050
-#define IBX_HDMIW_HDMIEDID_B		0xE2150
+#define _IBX_HDMIW_HDMIEDID_A		0xE2050
+#define _IBX_HDMIW_HDMIEDID_B		0xE2150
 #define IBX_HDMIW_HDMIEDID(pipe) _PIPE(pipe, \
-					IBX_HDMIW_HDMIEDID_A, \
-					IBX_HDMIW_HDMIEDID_B)
-#define IBX_AUD_CNTL_ST_A		0xE20B4
-#define IBX_AUD_CNTL_ST_B		0xE21B4
+					_IBX_HDMIW_HDMIEDID_A, \
+					_IBX_HDMIW_HDMIEDID_B)
+#define _IBX_AUD_CNTL_ST_A		0xE20B4
+#define _IBX_AUD_CNTL_ST_B		0xE21B4
 #define IBX_AUD_CNTL_ST(pipe) _PIPE(pipe, \
-					IBX_AUD_CNTL_ST_A, \
-					IBX_AUD_CNTL_ST_B)
-#define IBX_ELD_BUFFER_SIZE		(0x1f << 10)
-#define IBX_ELD_ADDRESS			(0x1f << 5)
-#define IBX_ELD_ACK			(1 << 4)
+					_IBX_AUD_CNTL_ST_A, \
+					_IBX_AUD_CNTL_ST_B)
+#define   IBX_ELD_BUFFER_SIZE_MASK	(0x1f << 10)
+#define   IBX_ELD_ADDRESS_MASK		(0x1f << 5)
+#define   IBX_ELD_ACK			(1 << 4)
 #define IBX_AUD_CNTL_ST2		0xE20C0
-#define IBX_ELD_VALIDB			(1 << 0)
-#define IBX_CP_READYB			(1 << 1)
+#define   IBX_CP_READY(port)		((1 << 1) << (((port) - 1) * 4))
+#define   IBX_ELD_VALID(port)		((1 << 0) << (((port) - 1) * 4))
 
-#define CPT_HDMIW_HDMIEDID_A		0xE5050
-#define CPT_HDMIW_HDMIEDID_B		0xE5150
+#define _CPT_HDMIW_HDMIEDID_A		0xE5050
+#define _CPT_HDMIW_HDMIEDID_B		0xE5150
 #define CPT_HDMIW_HDMIEDID(pipe) _PIPE(pipe, \
-					CPT_HDMIW_HDMIEDID_A, \
-					CPT_HDMIW_HDMIEDID_B)
-#define CPT_AUD_CNTL_ST_A		0xE50B4
-#define CPT_AUD_CNTL_ST_B		0xE51B4
+					_CPT_HDMIW_HDMIEDID_A, \
+					_CPT_HDMIW_HDMIEDID_B)
+#define _CPT_AUD_CNTL_ST_A		0xE50B4
+#define _CPT_AUD_CNTL_ST_B		0xE51B4
 #define CPT_AUD_CNTL_ST(pipe) _PIPE(pipe, \
-					CPT_AUD_CNTL_ST_A, \
-					CPT_AUD_CNTL_ST_B)
+					_CPT_AUD_CNTL_ST_A, \
+					_CPT_AUD_CNTL_ST_B)
 #define CPT_AUD_CNTRL_ST2		0xE50C0
 
-#define VLV_HDMIW_HDMIEDID_A		(VLV_DISPLAY_BASE + 0x62050)
-#define VLV_HDMIW_HDMIEDID_B		(VLV_DISPLAY_BASE + 0x62150)
+#define _VLV_HDMIW_HDMIEDID_A		(VLV_DISPLAY_BASE + 0x62050)
+#define _VLV_HDMIW_HDMIEDID_B		(VLV_DISPLAY_BASE + 0x62150)
 #define VLV_HDMIW_HDMIEDID(pipe) _PIPE(pipe, \
-					VLV_HDMIW_HDMIEDID_A, \
-					VLV_HDMIW_HDMIEDID_B)
-#define VLV_AUD_CNTL_ST_A		(VLV_DISPLAY_BASE + 0x620B4)
-#define VLV_AUD_CNTL_ST_B		(VLV_DISPLAY_BASE + 0x621B4)
+					_VLV_HDMIW_HDMIEDID_A, \
+					_VLV_HDMIW_HDMIEDID_B)
+#define _VLV_AUD_CNTL_ST_A		(VLV_DISPLAY_BASE + 0x620B4)
+#define _VLV_AUD_CNTL_ST_B		(VLV_DISPLAY_BASE + 0x621B4)
 #define VLV_AUD_CNTL_ST(pipe) _PIPE(pipe, \
-					VLV_AUD_CNTL_ST_A, \
-					VLV_AUD_CNTL_ST_B)
+					_VLV_AUD_CNTL_ST_A, \
+					_VLV_AUD_CNTL_ST_B)
 #define VLV_AUD_CNTL_ST2		(VLV_DISPLAY_BASE + 0x620C0)
 
 /* These are the 4 32-bit write offset registers for each stream
@@ -5700,28 +6167,28 @@
  */
 #define GEN7_SO_WRITE_OFFSET(n)		(0x5280 + (n) * 4)
 
-#define IBX_AUD_CONFIG_A			0xe2000
-#define IBX_AUD_CONFIG_B			0xe2100
+#define _IBX_AUD_CONFIG_A		0xe2000
+#define _IBX_AUD_CONFIG_B		0xe2100
 #define IBX_AUD_CFG(pipe) _PIPE(pipe, \
-					IBX_AUD_CONFIG_A, \
-					IBX_AUD_CONFIG_B)
-#define CPT_AUD_CONFIG_A			0xe5000
-#define CPT_AUD_CONFIG_B			0xe5100
+					_IBX_AUD_CONFIG_A, \
+					_IBX_AUD_CONFIG_B)
+#define _CPT_AUD_CONFIG_A		0xe5000
+#define _CPT_AUD_CONFIG_B		0xe5100
 #define CPT_AUD_CFG(pipe) _PIPE(pipe, \
-					CPT_AUD_CONFIG_A, \
-					CPT_AUD_CONFIG_B)
-#define VLV_AUD_CONFIG_A		(VLV_DISPLAY_BASE + 0x62000)
-#define VLV_AUD_CONFIG_B		(VLV_DISPLAY_BASE + 0x62100)
+					_CPT_AUD_CONFIG_A, \
+					_CPT_AUD_CONFIG_B)
+#define _VLV_AUD_CONFIG_A		(VLV_DISPLAY_BASE + 0x62000)
+#define _VLV_AUD_CONFIG_B		(VLV_DISPLAY_BASE + 0x62100)
 #define VLV_AUD_CFG(pipe) _PIPE(pipe, \
-					VLV_AUD_CONFIG_A, \
-					VLV_AUD_CONFIG_B)
+					_VLV_AUD_CONFIG_A, \
+					_VLV_AUD_CONFIG_B)
 
 #define   AUD_CONFIG_N_VALUE_INDEX		(1 << 29)
 #define   AUD_CONFIG_N_PROG_ENABLE		(1 << 28)
 #define   AUD_CONFIG_UPPER_N_SHIFT		20
-#define   AUD_CONFIG_UPPER_N_VALUE		(0xff << 20)
+#define   AUD_CONFIG_UPPER_N_MASK		(0xff << 20)
 #define   AUD_CONFIG_LOWER_N_SHIFT		4
-#define   AUD_CONFIG_LOWER_N_VALUE		(0xfff << 4)
+#define   AUD_CONFIG_LOWER_N_MASK		(0xfff << 4)
 #define   AUD_CONFIG_PIXEL_CLOCK_HDMI_SHIFT	16
 #define   AUD_CONFIG_PIXEL_CLOCK_HDMI_MASK	(0xf << 16)
 #define   AUD_CONFIG_PIXEL_CLOCK_HDMI_25175	(0 << 16)
@@ -5737,52 +6204,44 @@
 #define   AUD_CONFIG_DISABLE_NCTS		(1 << 3)
 
 /* HSW Audio */
-#define   HSW_AUD_CONFIG_A		0x65000 /* Audio Configuration Transcoder A */
-#define   HSW_AUD_CONFIG_B		0x65100 /* Audio Configuration Transcoder B */
-#define   HSW_AUD_CFG(pipe) _PIPE(pipe, \
-					HSW_AUD_CONFIG_A, \
-					HSW_AUD_CONFIG_B)
-
-#define   HSW_AUD_MISC_CTRL_A		0x65010 /* Audio Misc Control Convert 1 */
-#define   HSW_AUD_MISC_CTRL_B		0x65110 /* Audio Misc Control Convert 2 */
-#define   HSW_AUD_MISC_CTRL(pipe) _PIPE(pipe, \
-					HSW_AUD_MISC_CTRL_A, \
-					HSW_AUD_MISC_CTRL_B)
-
-#define   HSW_AUD_DIP_ELD_CTRL_ST_A	0x650b4 /* Audio DIP and ELD Control State Transcoder A */
-#define   HSW_AUD_DIP_ELD_CTRL_ST_B	0x651b4 /* Audio DIP and ELD Control State Transcoder B */
-#define   HSW_AUD_DIP_ELD_CTRL(pipe) _PIPE(pipe, \
-					HSW_AUD_DIP_ELD_CTRL_ST_A, \
-					HSW_AUD_DIP_ELD_CTRL_ST_B)
+#define _HSW_AUD_CONFIG_A		0x65000
+#define _HSW_AUD_CONFIG_B		0x65100
+#define HSW_AUD_CFG(pipe) _PIPE(pipe, \
+					_HSW_AUD_CONFIG_A, \
+					_HSW_AUD_CONFIG_B)
+
+#define _HSW_AUD_MISC_CTRL_A		0x65010
+#define _HSW_AUD_MISC_CTRL_B		0x65110
+#define HSW_AUD_MISC_CTRL(pipe) _PIPE(pipe, \
+					_HSW_AUD_MISC_CTRL_A, \
+					_HSW_AUD_MISC_CTRL_B)
+
+#define _HSW_AUD_DIP_ELD_CTRL_ST_A	0x650b4
+#define _HSW_AUD_DIP_ELD_CTRL_ST_B	0x651b4
+#define HSW_AUD_DIP_ELD_CTRL(pipe) _PIPE(pipe, \
+					_HSW_AUD_DIP_ELD_CTRL_ST_A, \
+					_HSW_AUD_DIP_ELD_CTRL_ST_B)
 
 /* Audio Digital Converter */
-#define   HSW_AUD_DIG_CNVT_1		0x65080 /* Audio Converter 1 */
-#define   HSW_AUD_DIG_CNVT_2		0x65180 /* Audio Converter 1 */
-#define   AUD_DIG_CNVT(pipe) _PIPE(pipe, \
-					HSW_AUD_DIG_CNVT_1, \
-					HSW_AUD_DIG_CNVT_2)
-#define   DIP_PORT_SEL_MASK		0x3
-
-#define   HSW_AUD_EDID_DATA_A		0x65050
-#define   HSW_AUD_EDID_DATA_B		0x65150
-#define   HSW_AUD_EDID_DATA(pipe) _PIPE(pipe, \
-					HSW_AUD_EDID_DATA_A, \
-					HSW_AUD_EDID_DATA_B)
-
-#define   HSW_AUD_PIPE_CONV_CFG		0x6507c /* Audio pipe and converter configs */
-#define   HSW_AUD_PIN_ELD_CP_VLD	0x650c0 /* Audio ELD and CP Ready Status */
-#define   AUDIO_INACTIVE_C		(1<<11)
-#define   AUDIO_INACTIVE_B		(1<<7)
-#define   AUDIO_INACTIVE_A		(1<<3)
-#define   AUDIO_OUTPUT_ENABLE_A		(1<<2)
-#define   AUDIO_OUTPUT_ENABLE_B		(1<<6)
-#define   AUDIO_OUTPUT_ENABLE_C		(1<<10)
-#define   AUDIO_ELD_VALID_A		(1<<0)
-#define   AUDIO_ELD_VALID_B		(1<<4)
-#define   AUDIO_ELD_VALID_C		(1<<8)
-#define   AUDIO_CP_READY_A		(1<<1)
-#define   AUDIO_CP_READY_B		(1<<5)
-#define   AUDIO_CP_READY_C		(1<<9)
+#define _HSW_AUD_DIG_CNVT_1		0x65080
+#define _HSW_AUD_DIG_CNVT_2		0x65180
+#define AUD_DIG_CNVT(pipe) _PIPE(pipe, \
+					_HSW_AUD_DIG_CNVT_1, \
+					_HSW_AUD_DIG_CNVT_2)
+#define DIP_PORT_SEL_MASK		0x3
+
+#define _HSW_AUD_EDID_DATA_A		0x65050
+#define _HSW_AUD_EDID_DATA_B		0x65150
+#define HSW_AUD_EDID_DATA(pipe) _PIPE(pipe, \
+					_HSW_AUD_EDID_DATA_A, \
+					_HSW_AUD_EDID_DATA_B)
+
+#define HSW_AUD_PIPE_CONV_CFG		0x6507c
+#define HSW_AUD_PIN_ELD_CP_VLD		0x650c0
+#define   AUDIO_INACTIVE(trans)		((1 << 3) << ((trans) * 4))
+#define   AUDIO_OUTPUT_ENABLE(trans)	((1 << 2) << ((trans) * 4))
+#define   AUDIO_CP_READY(trans)		((1 << 1) << ((trans) * 4))
+#define   AUDIO_ELD_VALID(trans)	((1 << 0) << ((trans) * 4))
 
 /* HSW Power Wells */
 #define HSW_PWR_WELL_BIOS			0x45400 /* CTL1 */
@@ -5866,15 +6325,7 @@
 #define DDI_BUF_CTL_B				0x64100
 #define DDI_BUF_CTL(port) _PORT(port, DDI_BUF_CTL_A, DDI_BUF_CTL_B)
 #define  DDI_BUF_CTL_ENABLE			(1<<31)
-#define  DDI_BUF_EMP_400MV_0DB_HSW		(0<<24)   /* Sel0 */
-#define  DDI_BUF_EMP_400MV_3_5DB_HSW		(1<<24)   /* Sel1 */
-#define  DDI_BUF_EMP_400MV_6DB_HSW		(2<<24)   /* Sel2 */
-#define  DDI_BUF_EMP_400MV_9_5DB_HSW		(3<<24)   /* Sel3 */
-#define  DDI_BUF_EMP_600MV_0DB_HSW		(4<<24)   /* Sel4 */
-#define  DDI_BUF_EMP_600MV_3_5DB_HSW		(5<<24)   /* Sel5 */
-#define  DDI_BUF_EMP_600MV_6DB_HSW		(6<<24)   /* Sel6 */
-#define  DDI_BUF_EMP_800MV_0DB_HSW		(7<<24)   /* Sel7 */
-#define  DDI_BUF_EMP_800MV_3_5DB_HSW		(8<<24)   /* Sel8 */
+#define  DDI_BUF_TRANS_SELECT(n)	((n) << 24)
 #define  DDI_BUF_EMP_MASK			(0xf<<24)
 #define  DDI_BUF_PORT_REVERSAL			(1<<16)
 #define  DDI_BUF_IS_IDLE			(1<<7)
@@ -6008,6 +6459,83 @@
 #define  LCPLL_CD_SOURCE_FCLK		(1<<21)
 #define  LCPLL_CD_SOURCE_FCLK_DONE	(1<<19)
 
+/*
+ * SKL Clocks
+ */
+
+/* CDCLK_CTL */
+#define CDCLK_CTL			0x46000
+#define  CDCLK_FREQ_SEL_MASK		(3<<26)
+#define  CDCLK_FREQ_450_432		(0<<26)
+#define  CDCLK_FREQ_540			(1<<26)
+#define  CDCLK_FREQ_337_308		(2<<26)
+#define  CDCLK_FREQ_675_617		(3<<26)
+#define  CDCLK_FREQ_DECIMAL_MASK	(0x7ff)
+
+/* LCPLL_CTL */
+#define LCPLL1_CTL		0x46010
+#define LCPLL2_CTL		0x46014
+#define  LCPLL_PLL_ENABLE	(1<<31)
+
+/* DPLL control1 */
+#define DPLL_CTRL1		0x6C058
+#define  DPLL_CTRL1_HDMI_MODE(id)		(1<<((id)*6+5))
+#define  DPLL_CTRL1_SSC(id)			(1<<((id)*6+4))
+#define  DPLL_CRTL1_LINK_RATE_MASK(id)		(7<<((id)*6+1))
+#define  DPLL_CRTL1_LINK_RATE_SHIFT(id)		((id)*6+1)
+#define  DPLL_CRTL1_LINK_RATE(linkrate, id)	((linkrate)<<((id)*6+1))
+#define  DPLL_CTRL1_OVERRIDE(id)		(1<<((id)*6))
+#define  DPLL_CRTL1_LINK_RATE_2700		0
+#define  DPLL_CRTL1_LINK_RATE_1350		1
+#define  DPLL_CRTL1_LINK_RATE_810		2
+#define  DPLL_CRTL1_LINK_RATE_1620		3
+#define  DPLL_CRTL1_LINK_RATE_1080		4
+#define  DPLL_CRTL1_LINK_RATE_2160		5
+
+/* DPLL control2 */
+#define DPLL_CTRL2				0x6C05C
+#define  DPLL_CTRL2_DDI_CLK_OFF(port)		(1<<(port+15))
+#define  DPLL_CTRL2_DDI_CLK_SEL_MASK(port)	(3<<((port)*3+1))
+#define  DPLL_CTRL2_DDI_CLK_SEL_SHIFT(port)    ((port)*3+1)
+#define  DPLL_CTRL2_DDI_CLK_SEL(clk, port)	(clk<<((port)*3+1))
+#define  DPLL_CTRL2_DDI_SEL_OVERRIDE(port)     (1<<((port)*3))
+
+/* DPLL Status */
+#define DPLL_STATUS	0x6C060
+#define  DPLL_LOCK(id) (1<<((id)*8))
+
+/* DPLL cfg */
+#define DPLL1_CFGCR1	0x6C040
+#define DPLL2_CFGCR1	0x6C048
+#define DPLL3_CFGCR1	0x6C050
+#define  DPLL_CFGCR1_FREQ_ENABLE	(1<<31)
+#define  DPLL_CFGCR1_DCO_FRACTION_MASK	(0x7fff<<9)
+#define  DPLL_CFGCR1_DCO_FRACTION(x)	(x<<9)
+#define  DPLL_CFGCR1_DCO_INTEGER_MASK	(0x1ff)
+
+#define DPLL1_CFGCR2	0x6C044
+#define DPLL2_CFGCR2	0x6C04C
+#define DPLL3_CFGCR2	0x6C054
+#define  DPLL_CFGCR2_QDIV_RATIO_MASK	(0xff<<8)
+#define  DPLL_CFGCR2_QDIV_RATIO(x)	(x<<8)
+#define  DPLL_CFGCR2_QDIV_MODE(x)	(x<<7)
+#define  DPLL_CFGCR2_KDIV_MASK		(3<<5)
+#define  DPLL_CFGCR2_KDIV(x)		(x<<5)
+#define  DPLL_CFGCR2_KDIV_5 (0<<5)
+#define  DPLL_CFGCR2_KDIV_2 (1<<5)
+#define  DPLL_CFGCR2_KDIV_3 (2<<5)
+#define  DPLL_CFGCR2_KDIV_1 (3<<5)
+#define  DPLL_CFGCR2_PDIV_MASK		(7<<2)
+#define  DPLL_CFGCR2_PDIV(x)		(x<<2)
+#define  DPLL_CFGCR2_PDIV_1 (0<<2)
+#define  DPLL_CFGCR2_PDIV_2 (1<<2)
+#define  DPLL_CFGCR2_PDIV_3 (2<<2)
+#define  DPLL_CFGCR2_PDIV_7 (4<<2)
+#define  DPLL_CFGCR2_CENTRAL_FREQ_MASK	(3)
+
+#define GET_CFG_CR1_REG(id) (DPLL1_CFGCR1 + (id - SKL_DPLL1) * 8)
+#define GET_CFG_CR2_REG(id) (DPLL1_CFGCR2 + (id - SKL_DPLL1) * 8)
+
 /* Please see hsw_read_dcomp() and hsw_write_dcomp() before using this register,
  * since on HSW we can't write to it using I915_WRITE. */
 #define D_COMP_HSW			(MCHBAR_MIRROR_BASE_SNB + 0x5F0C)
diff -urN a/drivers/gpu/drm/i915/i915_suspend.c b/drivers/gpu/drm/i915/i915_suspend.c
--- a/drivers/gpu/drm/i915/i915_suspend.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_suspend.c	2014-11-22 14:37:49.334700418 -0700
@@ -203,34 +203,19 @@
 		i915_save_display_reg(dev);
 
 	/* LVDS state */
-	if (HAS_PCH_SPLIT(dev)) {
-		dev_priv->regfile.savePP_CONTROL = I915_READ(PCH_PP_CONTROL);
-		if (HAS_PCH_IBX(dev) || HAS_PCH_CPT(dev))
-			dev_priv->regfile.saveLVDS = I915_READ(PCH_LVDS);
-	} else if (IS_VALLEYVIEW(dev)) {
-		dev_priv->regfile.savePP_CONTROL = I915_READ(PP_CONTROL);
-		dev_priv->regfile.savePFIT_PGM_RATIOS = I915_READ(PFIT_PGM_RATIOS);
-
-		dev_priv->regfile.saveBLC_HIST_CTL =
-			I915_READ(VLV_BLC_HIST_CTL(PIPE_A));
-		dev_priv->regfile.saveBLC_HIST_CTL_B =
-			I915_READ(VLV_BLC_HIST_CTL(PIPE_B));
-	} else {
-		dev_priv->regfile.savePP_CONTROL = I915_READ(PP_CONTROL);
-		dev_priv->regfile.savePFIT_PGM_RATIOS = I915_READ(PFIT_PGM_RATIOS);
-		dev_priv->regfile.saveBLC_HIST_CTL = I915_READ(BLC_HIST_CTL);
-		if (IS_MOBILE(dev) && !IS_I830(dev))
-			dev_priv->regfile.saveLVDS = I915_READ(LVDS);
-	}
-
-	if (!IS_I830(dev) && !IS_845G(dev) && !HAS_PCH_SPLIT(dev))
-		dev_priv->regfile.savePFIT_CONTROL = I915_READ(PFIT_CONTROL);
+	if (HAS_PCH_IBX(dev) || HAS_PCH_CPT(dev))
+		dev_priv->regfile.saveLVDS = I915_READ(PCH_LVDS);
+	else if (INTEL_INFO(dev)->gen <= 4 && IS_MOBILE(dev) && !IS_I830(dev))
+		dev_priv->regfile.saveLVDS = I915_READ(LVDS);
 
+	/* Panel power sequencer */
 	if (HAS_PCH_SPLIT(dev)) {
+		dev_priv->regfile.savePP_CONTROL = I915_READ(PCH_PP_CONTROL);
 		dev_priv->regfile.savePP_ON_DELAYS = I915_READ(PCH_PP_ON_DELAYS);
 		dev_priv->regfile.savePP_OFF_DELAYS = I915_READ(PCH_PP_OFF_DELAYS);
 		dev_priv->regfile.savePP_DIVISOR = I915_READ(PCH_PP_DIVISOR);
-	} else {
+	} else if (!IS_VALLEYVIEW(dev)) {
+		dev_priv->regfile.savePP_CONTROL = I915_READ(PP_CONTROL);
 		dev_priv->regfile.savePP_ON_DELAYS = I915_READ(PP_ON_DELAYS);
 		dev_priv->regfile.savePP_OFF_DELAYS = I915_READ(PP_OFF_DELAYS);
 		dev_priv->regfile.savePP_DIVISOR = I915_READ(PP_DIVISOR);
@@ -259,29 +244,19 @@
 	if (drm_core_check_feature(dev, DRIVER_MODESET))
 		mask = ~LVDS_PORT_EN;
 
+	/* LVDS state */
 	if (HAS_PCH_IBX(dev) || HAS_PCH_CPT(dev))
 		I915_WRITE(PCH_LVDS, dev_priv->regfile.saveLVDS & mask);
 	else if (INTEL_INFO(dev)->gen <= 4 && IS_MOBILE(dev) && !IS_I830(dev))
 		I915_WRITE(LVDS, dev_priv->regfile.saveLVDS & mask);
 
-	if (!IS_I830(dev) && !IS_845G(dev) && !HAS_PCH_SPLIT(dev))
-		I915_WRITE(PFIT_CONTROL, dev_priv->regfile.savePFIT_CONTROL);
-
+	/* Panel power sequencer */
 	if (HAS_PCH_SPLIT(dev)) {
 		I915_WRITE(PCH_PP_ON_DELAYS, dev_priv->regfile.savePP_ON_DELAYS);
 		I915_WRITE(PCH_PP_OFF_DELAYS, dev_priv->regfile.savePP_OFF_DELAYS);
 		I915_WRITE(PCH_PP_DIVISOR, dev_priv->regfile.savePP_DIVISOR);
 		I915_WRITE(PCH_PP_CONTROL, dev_priv->regfile.savePP_CONTROL);
-		I915_WRITE(RSTDBYCTL,
-			   dev_priv->regfile.saveMCHBAR_RENDER_STANDBY);
-	} else if (IS_VALLEYVIEW(dev)) {
-		I915_WRITE(VLV_BLC_HIST_CTL(PIPE_A),
-			   dev_priv->regfile.saveBLC_HIST_CTL);
-		I915_WRITE(VLV_BLC_HIST_CTL(PIPE_B),
-			   dev_priv->regfile.saveBLC_HIST_CTL);
-	} else {
-		I915_WRITE(PFIT_PGM_RATIOS, dev_priv->regfile.savePFIT_PGM_RATIOS);
-		I915_WRITE(BLC_HIST_CTL, dev_priv->regfile.saveBLC_HIST_CTL);
+	} else if (!IS_VALLEYVIEW(dev)) {
 		I915_WRITE(PP_ON_DELAYS, dev_priv->regfile.savePP_ON_DELAYS);
 		I915_WRITE(PP_OFF_DELAYS, dev_priv->regfile.savePP_OFF_DELAYS);
 		I915_WRITE(PP_DIVISOR, dev_priv->regfile.savePP_DIVISOR);
@@ -368,6 +343,8 @@
 			I915_WRITE(_FDI_RXA_IMR, dev_priv->regfile.saveFDI_RXA_IMR);
 			I915_WRITE(_FDI_RXB_IMR, dev_priv->regfile.saveFDI_RXB_IMR);
 			I915_WRITE(PCH_PORT_HOTPLUG, dev_priv->regfile.savePCH_PORT_HOTPLUG);
+			I915_WRITE(RSTDBYCTL,
+				   dev_priv->regfile.saveMCHBAR_RENDER_STANDBY);
 		} else {
 			I915_WRITE(IER, dev_priv->regfile.saveIER);
 			I915_WRITE(IMR, dev_priv->regfile.saveIMR);
diff -urN a/drivers/gpu/drm/i915/i915_sysfs.c b/drivers/gpu/drm/i915/i915_sysfs.c
--- a/drivers/gpu/drm/i915/i915_sysfs.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_sysfs.c	2014-11-22 14:37:49.334700418 -0700
@@ -139,8 +139,6 @@
 static struct attribute *rc6_attrs[] = {
 	&dev_attr_rc6_enable.attr,
 	&dev_attr_rc6_residency_ms.attr,
-	&dev_attr_rc6p_residency_ms.attr,
-	&dev_attr_rc6pp_residency_ms.attr,
 	NULL
 };
 
@@ -148,6 +146,17 @@
 	.name = power_group_name,
 	.attrs =  rc6_attrs
 };
+
+static struct attribute *rc6p_attrs[] = {
+	&dev_attr_rc6p_residency_ms.attr,
+	&dev_attr_rc6pp_residency_ms.attr,
+	NULL
+};
+
+static struct attribute_group rc6p_attr_group = {
+	.name = power_group_name,
+	.attrs =  rc6p_attrs
+};
 #endif
 
 static int l3_access_valid(struct drm_device *dev, loff_t offset)
@@ -540,7 +549,7 @@
 
 	memset(&error_priv, 0, sizeof(error_priv));
 
-	ret = i915_error_state_buf_init(&error_str, count, off);
+	ret = i915_error_state_buf_init(&error_str, to_i915(dev), count, off);
 	if (ret)
 		return ret;
 
@@ -595,12 +604,18 @@
 	int ret;
 
 #ifdef CONFIG_PM
-	if (INTEL_INFO(dev)->gen >= 6) {
+	if (HAS_RC6(dev)) {
 		ret = sysfs_merge_group(&dev->primary->kdev->kobj,
 					&rc6_attr_group);
 		if (ret)
 			DRM_ERROR("RC6 residency sysfs setup failed\n");
 	}
+	if (HAS_RC6p(dev)) {
+		ret = sysfs_merge_group(&dev->primary->kdev->kobj,
+					&rc6p_attr_group);
+		if (ret)
+			DRM_ERROR("RC6p residency sysfs setup failed\n");
+	}
 #endif
 	if (HAS_L3_DPF(dev)) {
 		ret = device_create_bin_file(dev->primary->kdev, &dpf_attrs);
@@ -640,5 +655,6 @@
 	device_remove_bin_file(dev->primary->kdev,  &dpf_attrs);
 #ifdef CONFIG_PM
 	sysfs_unmerge_group(&dev->primary->kdev->kobj, &rc6_attr_group);
+	sysfs_unmerge_group(&dev->primary->kdev->kobj, &rc6p_attr_group);
 #endif
 }
diff -urN a/drivers/gpu/drm/i915/i915_trace.h b/drivers/gpu/drm/i915/i915_trace.h
--- a/drivers/gpu/drm/i915/i915_trace.h	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_trace.h	2014-11-22 14:37:49.338700418 -0700
@@ -156,10 +156,12 @@
 		      __entry->obj, __entry->offset, __entry->size, __entry->vm)
 );
 
-TRACE_EVENT(i915_gem_object_change_domain,
+TRACE_EVENT_CONDITION(i915_gem_object_change_domain,
 	    TP_PROTO(struct drm_i915_gem_object *obj, u32 old_read, u32 old_write),
 	    TP_ARGS(obj, old_read, old_write),
 
+	    TP_CONDITION((old_read ^ obj->base.read_domains) | (old_write ^ obj->base.write_domain)),
+
 	    TP_STRUCT__entry(
 			     __field(struct drm_i915_gem_object *, obj)
 			     __field(u32, read_domains)
@@ -325,11 +327,10 @@
 	    TP_printk("dev=%d, vm=%p", __entry->dev, __entry->vm)
 );
 
-TRACE_EVENT(i915_gem_ring_sync_to,
-	    TP_PROTO(struct intel_engine_cs *from,
-		     struct intel_engine_cs *to,
-		     u32 seqno),
-	    TP_ARGS(from, to, seqno),
+TRACE_EVENT(i915_gem_ring_wait,
+	    TP_PROTO(struct i915_gem_request *waiter,
+		     struct i915_gem_request *signaller),
+	    TP_ARGS(waiter, signaller),
 
 	    TP_STRUCT__entry(
 			     __field(u32, dev)
@@ -339,18 +340,40 @@
 			     ),
 
 	    TP_fast_assign(
-			   __entry->dev = from->dev->primary->index;
-			   __entry->sync_from = from->id;
-			   __entry->sync_to = to->id;
-			   __entry->seqno = seqno;
+			   __entry->dev = waiter->i915->dev->primary->index;
+			   __entry->sync_from = waiter->engine->id;
+			   __entry->sync_to = signaller->engine->id;
+			   __entry->seqno = signaller->breadcrumb[waiter->engine->id];
 			   ),
 
-	    TP_printk("dev=%u, sync-from=%u, sync-to=%u, seqno=%u",
+	    TP_printk("dev=%u, sync-from=%u, sync-to=%u, seqno=%x",
 		      __entry->dev,
 		      __entry->sync_from, __entry->sync_to,
 		      __entry->seqno)
 );
 
+TRACE_EVENT(i915_gem_ring_switch_context,
+	    TP_PROTO(struct intel_engine_cs *engine, struct intel_context *ctx, u32 flags),
+	    TP_ARGS(engine, ctx, flags),
+
+	    TP_STRUCT__entry(
+			     __field(u32, dev)
+			     __field(u32, ring)
+			     __field(u32, ctx)
+			     __field(u32, flags)
+			     ),
+
+	    TP_fast_assign(
+			   __entry->dev = engine->i915->dev->primary->index;
+			   __entry->ring = engine->id;
+			   __entry->ctx = ctx->file_priv ? ctx->user_handle : -1;
+			   __entry->flags = flags;
+			   ),
+
+	    TP_printk("dev=%u, ring=%u, ctx=%d, flags=0x%08x",
+		      __entry->dev, __entry->ring, __entry->ctx, __entry->flags)
+);
+
 TRACE_EVENT(i915_gem_ring_dispatch,
 	    TP_PROTO(struct intel_engine_cs *ring, u32 seqno, u32 flags),
 	    TP_ARGS(ring, seqno, flags),
@@ -363,66 +386,84 @@
 			     ),
 
 	    TP_fast_assign(
-			   __entry->dev = ring->dev->primary->index;
+			   __entry->dev = ring->i915->dev->primary->index;
 			   __entry->ring = ring->id;
 			   __entry->seqno = seqno;
 			   __entry->flags = flags;
 			   i915_trace_irq_get(ring, seqno);
 			   ),
 
-	    TP_printk("dev=%u, ring=%u, seqno=%u, flags=%x",
+	    TP_printk("dev=%u, ring=%u, seqno=%x, flags=%x",
 		      __entry->dev, __entry->ring, __entry->seqno, __entry->flags)
 );
 
-TRACE_EVENT(i915_gem_ring_flush,
-	    TP_PROTO(struct intel_engine_cs *ring, u32 invalidate, u32 flush),
-	    TP_ARGS(ring, invalidate, flush),
+TRACE_EVENT(intel_ringbuffer_begin,
+	    TP_PROTO(struct intel_ringbuffer *ring, int need),
+	    TP_ARGS(ring, need),
 
 	    TP_STRUCT__entry(
 			     __field(u32, dev)
 			     __field(u32, ring)
-			     __field(u32, invalidate)
-			     __field(u32, flush)
+			     __field(u32, need)
+			     __field(u32, space)
 			     ),
 
 	    TP_fast_assign(
-			   __entry->dev = ring->dev->primary->index;
-			   __entry->ring = ring->id;
-			   __entry->invalidate = invalidate;
-			   __entry->flush = flush;
+			   __entry->dev = ring->engine->i915->dev->primary->index;
+			   __entry->ring = ring->engine->id;
+			   __entry->need = need;
+			   __entry->space = intel_ring_space(ring);
 			   ),
 
-	    TP_printk("dev=%u, ring=%x, invalidate=%04x, flush=%04x",
-		      __entry->dev, __entry->ring,
-		      __entry->invalidate, __entry->flush)
+	    TP_printk("dev=%u, ring=%u, need=%u, space=%u",
+		      __entry->dev, __entry->ring, __entry->need, __entry->space)
 );
 
-DECLARE_EVENT_CLASS(i915_gem_request,
-	    TP_PROTO(struct intel_engine_cs *ring, u32 seqno),
-	    TP_ARGS(ring, seqno),
+TRACE_EVENT(intel_ringbuffer_wait,
+	    TP_PROTO(struct intel_ringbuffer *ring, int need),
+	    TP_ARGS(ring, need),
 
 	    TP_STRUCT__entry(
 			     __field(u32, dev)
 			     __field(u32, ring)
-			     __field(u32, seqno)
+			     __field(u32, need)
+			     __field(u32, space)
 			     ),
 
 	    TP_fast_assign(
-			   __entry->dev = ring->dev->primary->index;
-			   __entry->ring = ring->id;
-			   __entry->seqno = seqno;
+			   __entry->dev = ring->engine->i915->dev->primary->index;
+			   __entry->ring = ring->engine->id;
+			   __entry->need = need;
+			   __entry->space = intel_ring_space(ring);
 			   ),
 
-	    TP_printk("dev=%u, ring=%u, seqno=%u",
-		      __entry->dev, __entry->ring, __entry->seqno)
+	    TP_printk("dev=%u, ring=%u, need=%u, space=%u",
+		      __entry->dev, __entry->ring, __entry->need, __entry->space)
 );
 
-DEFINE_EVENT(i915_gem_request, i915_gem_request_add,
-	    TP_PROTO(struct intel_engine_cs *ring, u32 seqno),
-	    TP_ARGS(ring, seqno)
+TRACE_EVENT(intel_ringbuffer_wrap,
+	    TP_PROTO(struct intel_ringbuffer *ring, int rem),
+	    TP_ARGS(ring, rem),
+
+	    TP_STRUCT__entry(
+			     __field(u32, dev)
+			     __field(u32, ring)
+			     __field(u32, rem)
+			     __field(u32, size)
+			     ),
+
+	    TP_fast_assign(
+			   __entry->dev = ring->engine->i915->dev->primary->index;
+			   __entry->ring = ring->engine->id;
+			   __entry->rem = rem;
+			   __entry->size = ring->effective_size;
+			   ),
+
+	    TP_printk("dev=%u, ring=%u, rem=%u, size=%u",
+		      __entry->dev, __entry->ring, __entry->rem, __entry->size)
 );
 
-TRACE_EVENT(i915_gem_request_complete,
+TRACE_EVENT(i915_gem_ring_complete,
 	    TP_PROTO(struct intel_engine_cs *ring),
 	    TP_ARGS(ring),
 
@@ -433,23 +474,80 @@
 			     ),
 
 	    TP_fast_assign(
-			   __entry->dev = ring->dev->primary->index;
+			   __entry->dev = ring->i915->dev->primary->index;
 			   __entry->ring = ring->id;
-			   __entry->seqno = ring->get_seqno(ring, false);
+			   __entry->seqno = intel_engine_get_seqno(ring);
+			   ),
+
+	    TP_printk("dev=%u, ring=%u, seqno=%x",
+		      __entry->dev, __entry->ring, __entry->seqno)
+);
+
+DECLARE_EVENT_CLASS(i915_gem_request,
+	    TP_PROTO(struct i915_gem_request *rq),
+	    TP_ARGS(rq),
+
+	    TP_STRUCT__entry(
+			     __field(u32, dev)
+			     __field(u32, ring)
+			     __field(u32, seqno)
+			     ),
+
+	    TP_fast_assign(
+			   __entry->dev = rq->i915->dev->primary->index;
+			   __entry->ring = rq->engine->id;
+			   __entry->seqno = rq->seqno;
 			   ),
 
-	    TP_printk("dev=%u, ring=%u, seqno=%u",
+	    TP_printk("dev=%u, ring=%u, seqno=%x",
 		      __entry->dev, __entry->ring, __entry->seqno)
 );
 
+/**
+ * DOC: switch_mm tracepoint
+ *
+ * This tracepoint allows tracking of the mm switch, which is an important point
+ * in the lifetime of the vm in the legacy submission path. This tracepoint is
+ * called only if full ppgtt is enabled.
+ */
+DEFINE_EVENT(i915_gem_request, i915_gem_request_switch_mm,
+	    TP_PROTO(struct i915_gem_request *rq),
+	    TP_ARGS(rq)
+);
+
+DEFINE_EVENT(i915_gem_request, i915_gem_request_emit_flush,
+	    TP_PROTO(struct i915_gem_request *rq),
+	    TP_ARGS(rq)
+);
+
+DEFINE_EVENT(i915_gem_request, i915_gem_request_emit_batch,
+	    TP_PROTO(struct i915_gem_request *rq),
+	    TP_ARGS(rq)
+);
+
+DEFINE_EVENT(i915_gem_request, i915_gem_request_emit_breadcrumb,
+	    TP_PROTO(struct i915_gem_request *rq),
+	    TP_ARGS(rq)
+);
+
+DEFINE_EVENT(i915_gem_request, i915_gem_request_commit,
+	    TP_PROTO(struct i915_gem_request *rq),
+	    TP_ARGS(rq)
+);
+
+DEFINE_EVENT(i915_gem_request, i915_gem_request_complete,
+	    TP_PROTO(struct i915_gem_request *rq),
+	    TP_ARGS(rq)
+);
+
 DEFINE_EVENT(i915_gem_request, i915_gem_request_retire,
-	    TP_PROTO(struct intel_engine_cs *ring, u32 seqno),
-	    TP_ARGS(ring, seqno)
+	    TP_PROTO(struct i915_gem_request *rq),
+	    TP_ARGS(rq)
 );
 
 TRACE_EVENT(i915_gem_request_wait_begin,
-	    TP_PROTO(struct intel_engine_cs *ring, u32 seqno),
-	    TP_ARGS(ring, seqno),
+	    TP_PROTO(struct i915_gem_request *rq),
+	    TP_ARGS(rq),
 
 	    TP_STRUCT__entry(
 			     __field(u32, dev)
@@ -465,47 +563,38 @@
 	     * less desirable.
 	     */
 	    TP_fast_assign(
-			   __entry->dev = ring->dev->primary->index;
-			   __entry->ring = ring->id;
-			   __entry->seqno = seqno;
-			   __entry->blocking = mutex_is_locked(&ring->dev->struct_mutex);
+			   __entry->dev = rq->i915->dev->primary->index;
+			   __entry->ring = rq->engine->id;
+			   __entry->seqno = rq->seqno;
+			   __entry->blocking = mutex_is_locked(&rq->i915->dev->struct_mutex);
 			   ),
 
-	    TP_printk("dev=%u, ring=%u, seqno=%u, blocking=%s",
+	    TP_printk("dev=%u, ring=%u, seqno=%x, blocking?=%s",
 		      __entry->dev, __entry->ring, __entry->seqno,
 		      __entry->blocking ?  "yes (NB)" : "no")
 );
 
-DEFINE_EVENT(i915_gem_request, i915_gem_request_wait_end,
-	    TP_PROTO(struct intel_engine_cs *ring, u32 seqno),
-	    TP_ARGS(ring, seqno)
-);
-
-DECLARE_EVENT_CLASS(i915_ring,
-	    TP_PROTO(struct intel_engine_cs *ring),
-	    TP_ARGS(ring),
+TRACE_EVENT(i915_gem_request_wait_end,
+	    TP_PROTO(struct i915_gem_request *rq),
+	    TP_ARGS(rq),
 
 	    TP_STRUCT__entry(
 			     __field(u32, dev)
 			     __field(u32, ring)
+			     __field(u32, seqno)
+			     __field(bool, completed)
 			     ),
 
 	    TP_fast_assign(
-			   __entry->dev = ring->dev->primary->index;
-			   __entry->ring = ring->id;
+			   __entry->dev = rq->i915->dev->primary->index;
+			   __entry->ring = rq->engine->id;
+			   __entry->seqno = rq->seqno;
+			   __entry->completed = rq->completed;
 			   ),
 
-	    TP_printk("dev=%u, ring=%u", __entry->dev, __entry->ring)
-);
-
-DEFINE_EVENT(i915_ring, i915_ring_wait_begin,
-	    TP_PROTO(struct intel_engine_cs *ring),
-	    TP_ARGS(ring)
-);
-
-DEFINE_EVENT(i915_ring, i915_ring_wait_end,
-	    TP_PROTO(struct intel_engine_cs *ring),
-	    TP_ARGS(ring)
+	    TP_printk("dev=%u, ring=%u, seqno=%x, completed=%s",
+		      __entry->dev, __entry->ring, __entry->seqno,
+		      __entry->completed ?  "yes" : "no")
 );
 
 TRACE_EVENT(i915_flip_request,
@@ -587,6 +676,80 @@
 	    TP_printk("new_freq=%u", __entry->freq)
 );
 
+/**
+ * DOC: i915_vm_create and i915_vm_free tracepoints
+ *
+ * With full ppgtt enabled each process using drm will allocate at least one
+ * translation table. With these traces it is possible to keep track of the
+ * allocation and of the lifetime of the tables; this can be used during
+ * testing/debug to verify that we are not leaking ppgtts.
+ * These traces identify the ppgtt through the vm pointer, which is also printed
+ * by the i915_vma_bind and i915_vma_unbind tracepoints.
+ */
+DECLARE_EVENT_CLASS(i915_vm,
+	TP_PROTO(struct i915_address_space *vm),
+	TP_ARGS(vm),
+
+	TP_STRUCT__entry(
+			__field(struct i915_address_space *, vm)
+			__field(u32, dev)
+	),
+
+	TP_fast_assign(
+			__entry->vm = vm;
+			__entry->dev = vm->dev->primary->index;
+	),
+
+	TP_printk("dev=%u, vm=%p", __entry->dev, __entry->vm)
+)
+
+DEFINE_EVENT(i915_vm, i915_vm_create,
+	TP_PROTO(struct i915_address_space *vm),
+	TP_ARGS(vm)
+);
+
+DEFINE_EVENT(i915_vm, i915_vm_free,
+	TP_PROTO(struct i915_address_space *vm),
+	TP_ARGS(vm)
+);
+
+/**
+ * DOC: i915_context_create and i915_context_free tracepoints
+ *
+ * These tracepoints are used to track creation and deletion of contexts.
+ * If full ppgtt is enabled, they also print the address of the vm assigned to
+ * the context.
+ */
+DECLARE_EVENT_CLASS(i915_context,
+	TP_PROTO(struct intel_context *ctx),
+	TP_ARGS(ctx),
+
+	TP_STRUCT__entry(
+			__field(u32, dev)
+			__field(struct intel_context *, ctx)
+			__field(struct i915_address_space *, vm)
+	),
+
+	TP_fast_assign(
+			__entry->ctx = ctx;
+			__entry->vm = ctx->ppgtt ? &ctx->ppgtt->base : NULL;
+			__entry->dev = ctx->file_priv->dev_priv->dev->primary->index;
+	),
+
+	TP_printk("dev=%u, ctx=%p, ctx_vm=%p",
+		  __entry->dev, __entry->ctx, __entry->vm)
+)
+
+DEFINE_EVENT(i915_context, i915_context_create,
+	TP_PROTO(struct intel_context *ctx),
+	TP_ARGS(ctx)
+);
+
+DEFINE_EVENT(i915_context, i915_context_free,
+	TP_PROTO(struct intel_context *ctx),
+	TP_ARGS(ctx)
+);
+
 #endif /* _I915_TRACE_H_ */
 
 /* This part must be outside protection */
diff -urN a/drivers/gpu/drm/i915/i915_ums.c b/drivers/gpu/drm/i915/i915_ums.c
--- a/drivers/gpu/drm/i915/i915_ums.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/i915_ums.c	2014-11-22 14:37:49.338700418 -0700
@@ -270,6 +270,12 @@
 	}
 	/* FIXME: regfile.save TV & SDVO state */
 
+	/* Panel fitter */
+	if (!IS_I830(dev) && !IS_845G(dev) && !HAS_PCH_SPLIT(dev)) {
+		dev_priv->regfile.savePFIT_CONTROL = I915_READ(PFIT_CONTROL);
+		dev_priv->regfile.savePFIT_PGM_RATIOS = I915_READ(PFIT_PGM_RATIOS);
+	}
+
 	/* Backlight */
 	if (INTEL_INFO(dev)->gen <= 4)
 		pci_read_config_byte(dev->pdev, PCI_LBPC,
@@ -284,6 +290,7 @@
 		dev_priv->regfile.saveBLC_PWM_CTL = I915_READ(BLC_PWM_CTL);
 		if (INTEL_INFO(dev)->gen >= 4)
 			dev_priv->regfile.saveBLC_PWM_CTL2 = I915_READ(BLC_PWM_CTL2);
+		dev_priv->regfile.saveBLC_HIST_CTL = I915_READ(BLC_HIST_CTL);
 	}
 
 	return;
@@ -313,6 +320,13 @@
 		if (INTEL_INFO(dev)->gen >= 4)
 			I915_WRITE(BLC_PWM_CTL2, dev_priv->regfile.saveBLC_PWM_CTL2);
 		I915_WRITE(BLC_PWM_CTL, dev_priv->regfile.saveBLC_PWM_CTL);
+		I915_WRITE(BLC_HIST_CTL, dev_priv->regfile.saveBLC_HIST_CTL);
+	}
+
+	/* Panel fitter */
+	if (!IS_I830(dev) && !IS_845G(dev) && !HAS_PCH_SPLIT(dev)) {
+		I915_WRITE(PFIT_PGM_RATIOS, dev_priv->regfile.savePFIT_PGM_RATIOS);
+		I915_WRITE(PFIT_CONTROL, dev_priv->regfile.savePFIT_CONTROL);
 	}
 
 	/* Display port ratios (must be done before clock is set) */
diff -urN a/drivers/gpu/drm/i915/intel_audio.c b/drivers/gpu/drm/i915/intel_audio.c
--- a/drivers/gpu/drm/i915/intel_audio.c	1969-12-31 17:00:00.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_audio.c	2014-11-22 14:37:49.338700418 -0700
@@ -0,0 +1,463 @@
+/*
+ * Copyright  2014 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#include <linux/kernel.h>
+
+#include <drm/drmP.h>
+#include <drm/drm_edid.h>
+#include "intel_drv.h"
+#include "i915_drv.h"
+
+/**
+ * DOC: High Definition Audio over HDMI and Display Port
+ *
+ * The graphics and audio drivers together support High Definition Audio over
+ * HDMI and Display Port. The audio programming sequences are divided into audio
+ * codec and controller enable and disable sequences. The graphics driver
+ * handles the audio codec sequences, while the audio driver handles the audio
+ * controller sequences.
+ *
+ * The disable sequences must be performed before disabling the transcoder or
+ * port. The enable sequences may only be performed after enabling the
+ * transcoder and port, and after completed link training.
+ *
+ * The codec and controller sequences could be done either parallel or serial,
+ * but generally the ELDV/PD change in the codec sequence indicates to the audio
+ * driver that the controller sequence should start. Indeed, most of the
+ * co-operation between the graphics and audio drivers is handled via audio
+ * related registers. (The notable exception is the power management, not
+ * covered here.)
+ */
+
+static const struct {
+	int clock;
+	u32 config;
+} hdmi_audio_clock[] = {
+	{ DIV_ROUND_UP(25200 * 1000, 1001), AUD_CONFIG_PIXEL_CLOCK_HDMI_25175 },
+	{ 25200, AUD_CONFIG_PIXEL_CLOCK_HDMI_25200 }, /* default per bspec */
+	{ 27000, AUD_CONFIG_PIXEL_CLOCK_HDMI_27000 },
+	{ 27000 * 1001 / 1000, AUD_CONFIG_PIXEL_CLOCK_HDMI_27027 },
+	{ 54000, AUD_CONFIG_PIXEL_CLOCK_HDMI_54000 },
+	{ 54000 * 1001 / 1000, AUD_CONFIG_PIXEL_CLOCK_HDMI_54054 },
+	{ DIV_ROUND_UP(74250 * 1000, 1001), AUD_CONFIG_PIXEL_CLOCK_HDMI_74176 },
+	{ 74250, AUD_CONFIG_PIXEL_CLOCK_HDMI_74250 },
+	{ DIV_ROUND_UP(148500 * 1000, 1001), AUD_CONFIG_PIXEL_CLOCK_HDMI_148352 },
+	{ 148500, AUD_CONFIG_PIXEL_CLOCK_HDMI_148500 },
+};
+
+/* get AUD_CONFIG_PIXEL_CLOCK_HDMI_* value for mode */
+static u32 audio_config_hdmi_pixel_clock(struct drm_display_mode *mode)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(hdmi_audio_clock); i++) {
+		if (mode->clock == hdmi_audio_clock[i].clock)
+			break;
+	}
+
+	if (i == ARRAY_SIZE(hdmi_audio_clock)) {
+		DRM_DEBUG_KMS("HDMI audio pixel clock setting for %d not found, falling back to defaults\n", mode->clock);
+		i = 1;
+	}
+
+	DRM_DEBUG_KMS("Configuring HDMI audio for pixel clock %d (0x%08x)\n",
+		      hdmi_audio_clock[i].clock,
+		      hdmi_audio_clock[i].config);
+
+	return hdmi_audio_clock[i].config;
+}
+
+static bool intel_eld_uptodate(struct drm_connector *connector,
+			       int reg_eldv, uint32_t bits_eldv,
+			       int reg_elda, uint32_t bits_elda,
+			       int reg_edid)
+{
+	struct drm_i915_private *dev_priv = connector->dev->dev_private;
+	uint8_t *eld = connector->eld;
+	uint32_t tmp;
+	int i;
+
+	tmp = I915_READ(reg_eldv);
+	tmp &= bits_eldv;
+
+	if (!tmp)
+		return false;
+
+	tmp = I915_READ(reg_elda);
+	tmp &= ~bits_elda;
+	I915_WRITE(reg_elda, tmp);
+
+	for (i = 0; i < drm_eld_size(eld) / 4; i++)
+		if (I915_READ(reg_edid) != *((uint32_t *)eld + i))
+			return false;
+
+	return true;
+}
+
+static void g4x_audio_codec_disable(struct intel_encoder *encoder)
+{
+	struct drm_i915_private *dev_priv = encoder->base.dev->dev_private;
+	uint32_t eldv, tmp;
+
+	DRM_DEBUG_KMS("Disable audio codec\n");
+
+	tmp = I915_READ(G4X_AUD_VID_DID);
+	if (tmp == INTEL_AUDIO_DEVBLC || tmp == INTEL_AUDIO_DEVCL)
+		eldv = G4X_ELDV_DEVCL_DEVBLC;
+	else
+		eldv = G4X_ELDV_DEVCTG;
+
+	/* Invalidate ELD */
+	tmp = I915_READ(G4X_AUD_CNTL_ST);
+	tmp &= ~eldv;
+	I915_WRITE(G4X_AUD_CNTL_ST, tmp);
+}
+
+static void g4x_audio_codec_enable(struct drm_connector *connector,
+				   struct intel_encoder *encoder,
+				   struct drm_display_mode *mode)
+{
+	struct drm_i915_private *dev_priv = connector->dev->dev_private;
+	uint8_t *eld = connector->eld;
+	uint32_t eldv;
+	uint32_t tmp;
+	int len, i;
+
+	DRM_DEBUG_KMS("Enable audio codec, %u bytes ELD\n", eld[2]);
+
+	tmp = I915_READ(G4X_AUD_VID_DID);
+	if (tmp == INTEL_AUDIO_DEVBLC || tmp == INTEL_AUDIO_DEVCL)
+		eldv = G4X_ELDV_DEVCL_DEVBLC;
+	else
+		eldv = G4X_ELDV_DEVCTG;
+
+	if (intel_eld_uptodate(connector,
+			       G4X_AUD_CNTL_ST, eldv,
+			       G4X_AUD_CNTL_ST, G4X_ELD_ADDR_MASK,
+			       G4X_HDMIW_HDMIEDID))
+		return;
+
+	tmp = I915_READ(G4X_AUD_CNTL_ST);
+	tmp &= ~(eldv | G4X_ELD_ADDR_MASK);
+	len = (tmp >> 9) & 0x1f;		/* ELD buffer size */
+	I915_WRITE(G4X_AUD_CNTL_ST, tmp);
+
+	len = min(drm_eld_size(eld) / 4, len);
+	DRM_DEBUG_DRIVER("ELD size %d\n", len);
+	for (i = 0; i < len; i++)
+		I915_WRITE(G4X_HDMIW_HDMIEDID, *((uint32_t *)eld + i));
+
+	tmp = I915_READ(G4X_AUD_CNTL_ST);
+	tmp |= eldv;
+	I915_WRITE(G4X_AUD_CNTL_ST, tmp);
+}
+
+static void hsw_audio_codec_disable(struct intel_encoder *encoder)
+{
+	struct drm_i915_private *dev_priv = encoder->base.dev->dev_private;
+	struct intel_crtc *intel_crtc = to_intel_crtc(encoder->base.crtc);
+	enum pipe pipe = intel_crtc->pipe;
+	uint32_t tmp;
+
+	DRM_DEBUG_KMS("Disable audio codec on pipe %c\n", pipe_name(pipe));
+
+	/* Disable timestamps */
+	tmp = I915_READ(HSW_AUD_CFG(pipe));
+	tmp &= ~AUD_CONFIG_N_VALUE_INDEX;
+	tmp |= AUD_CONFIG_N_PROG_ENABLE;
+	tmp &= ~AUD_CONFIG_UPPER_N_MASK;
+	tmp &= ~AUD_CONFIG_LOWER_N_MASK;
+	if (intel_pipe_has_type(intel_crtc, INTEL_OUTPUT_DISPLAYPORT))
+		tmp |= AUD_CONFIG_N_VALUE_INDEX;
+	I915_WRITE(HSW_AUD_CFG(pipe), tmp);
+
+	/* Invalidate ELD */
+	tmp = I915_READ(HSW_AUD_PIN_ELD_CP_VLD);
+	tmp &= ~AUDIO_ELD_VALID(pipe);
+	tmp &= ~AUDIO_OUTPUT_ENABLE(pipe);
+	I915_WRITE(HSW_AUD_PIN_ELD_CP_VLD, tmp);
+}
+
+static void hsw_audio_codec_enable(struct drm_connector *connector,
+				   struct intel_encoder *encoder,
+				   struct drm_display_mode *mode)
+{
+	struct drm_i915_private *dev_priv = connector->dev->dev_private;
+	struct intel_crtc *intel_crtc = to_intel_crtc(encoder->base.crtc);
+	enum pipe pipe = intel_crtc->pipe;
+	const uint8_t *eld = connector->eld;
+	uint32_t tmp;
+	int len, i;
+
+	DRM_DEBUG_KMS("Enable audio codec on pipe %c, %u bytes ELD\n",
+		      pipe_name(pipe), drm_eld_size(eld));
+
+	/* Enable audio presence detect, invalidate ELD */
+	tmp = I915_READ(HSW_AUD_PIN_ELD_CP_VLD);
+	tmp |= AUDIO_OUTPUT_ENABLE(pipe);
+	tmp &= ~AUDIO_ELD_VALID(pipe);
+	I915_WRITE(HSW_AUD_PIN_ELD_CP_VLD, tmp);
+
+	/*
+	 * FIXME: We're supposed to wait for vblank here, but we have vblanks
+	 * disabled during the mode set. The proper fix would be to push the
+	 * rest of the setup into a vblank work item, queued here, but the
+	 * infrastructure is not there yet.
+	 */
+
+	/* Reset ELD write address */
+	tmp = I915_READ(HSW_AUD_DIP_ELD_CTRL(pipe));
+	tmp &= ~IBX_ELD_ADDRESS_MASK;
+	I915_WRITE(HSW_AUD_DIP_ELD_CTRL(pipe), tmp);
+
+	/* Up to 84 bytes of hw ELD buffer */
+	len = min(drm_eld_size(eld), 84);
+	for (i = 0; i < len / 4; i++)
+		I915_WRITE(HSW_AUD_EDID_DATA(pipe), *((uint32_t *)eld + i));
+
+	/* ELD valid */
+	tmp = I915_READ(HSW_AUD_PIN_ELD_CP_VLD);
+	tmp |= AUDIO_ELD_VALID(pipe);
+	I915_WRITE(HSW_AUD_PIN_ELD_CP_VLD, tmp);
+
+	/* Enable timestamps */
+	tmp = I915_READ(HSW_AUD_CFG(pipe));
+	tmp &= ~AUD_CONFIG_N_VALUE_INDEX;
+	tmp &= ~AUD_CONFIG_N_PROG_ENABLE;
+	tmp &= ~AUD_CONFIG_PIXEL_CLOCK_HDMI_MASK;
+	if (intel_pipe_has_type(intel_crtc, INTEL_OUTPUT_DISPLAYPORT))
+		tmp |= AUD_CONFIG_N_VALUE_INDEX;
+	else
+		tmp |= audio_config_hdmi_pixel_clock(mode);
+	I915_WRITE(HSW_AUD_CFG(pipe), tmp);
+}
+
+static void ilk_audio_codec_disable(struct intel_encoder *encoder)
+{
+	struct drm_i915_private *dev_priv = encoder->base.dev->dev_private;
+	struct intel_crtc *intel_crtc = to_intel_crtc(encoder->base.crtc);
+	struct intel_digital_port *intel_dig_port =
+		enc_to_dig_port(&encoder->base);
+	enum port port = intel_dig_port->port;
+	enum pipe pipe = intel_crtc->pipe;
+	uint32_t tmp, eldv;
+	int aud_config;
+	int aud_cntrl_st2;
+
+	DRM_DEBUG_KMS("Disable audio codec on port %c, pipe %c\n",
+		      port_name(port), pipe_name(pipe));
+
+	if (HAS_PCH_IBX(dev_priv->dev)) {
+		aud_config = IBX_AUD_CFG(pipe);
+		aud_cntrl_st2 = IBX_AUD_CNTL_ST2;
+	} else if (IS_VALLEYVIEW(dev_priv)) {
+		aud_config = VLV_AUD_CFG(pipe);
+		aud_cntrl_st2 = VLV_AUD_CNTL_ST2;
+	} else {
+		aud_config = CPT_AUD_CFG(pipe);
+		aud_cntrl_st2 = CPT_AUD_CNTRL_ST2;
+	}
+
+	/* Disable timestamps */
+	tmp = I915_READ(aud_config);
+	tmp &= ~AUD_CONFIG_N_VALUE_INDEX;
+	tmp |= AUD_CONFIG_N_PROG_ENABLE;
+	tmp &= ~AUD_CONFIG_UPPER_N_MASK;
+	tmp &= ~AUD_CONFIG_LOWER_N_MASK;
+	if (intel_pipe_has_type(intel_crtc, INTEL_OUTPUT_DISPLAYPORT))
+		tmp |= AUD_CONFIG_N_VALUE_INDEX;
+	I915_WRITE(aud_config, tmp);
+
+	if (WARN_ON(!port)) {
+		eldv = IBX_ELD_VALID(PORT_B) | IBX_ELD_VALID(PORT_C) |
+			IBX_ELD_VALID(PORT_D);
+	} else {
+		eldv = IBX_ELD_VALID(port);
+	}
+
+	/* Invalidate ELD */
+	tmp = I915_READ(aud_cntrl_st2);
+	tmp &= ~eldv;
+	I915_WRITE(aud_cntrl_st2, tmp);
+}
+
+static void ilk_audio_codec_enable(struct drm_connector *connector,
+				   struct intel_encoder *encoder,
+				   struct drm_display_mode *mode)
+{
+	struct drm_i915_private *dev_priv = connector->dev->dev_private;
+	struct intel_crtc *intel_crtc = to_intel_crtc(encoder->base.crtc);
+	struct intel_digital_port *intel_dig_port =
+		enc_to_dig_port(&encoder->base);
+	enum port port = intel_dig_port->port;
+	enum pipe pipe = intel_crtc->pipe;
+	uint8_t *eld = connector->eld;
+	uint32_t eldv;
+	uint32_t tmp;
+	int len, i;
+	int hdmiw_hdmiedid;
+	int aud_config;
+	int aud_cntl_st;
+	int aud_cntrl_st2;
+
+	DRM_DEBUG_KMS("Enable audio codec on port %c, pipe %c, %u bytes ELD\n",
+		      port_name(port), pipe_name(pipe), drm_eld_size(eld));
+
+	/*
+	 * FIXME: We're supposed to wait for vblank here, but we have vblanks
+	 * disabled during the mode set. The proper fix would be to push the
+	 * rest of the setup into a vblank work item, queued here, but the
+	 * infrastructure is not there yet.
+	 */
+
+	if (HAS_PCH_IBX(connector->dev)) {
+		hdmiw_hdmiedid = IBX_HDMIW_HDMIEDID(pipe);
+		aud_config = IBX_AUD_CFG(pipe);
+		aud_cntl_st = IBX_AUD_CNTL_ST(pipe);
+		aud_cntrl_st2 = IBX_AUD_CNTL_ST2;
+	} else if (IS_VALLEYVIEW(connector->dev)) {
+		hdmiw_hdmiedid = VLV_HDMIW_HDMIEDID(pipe);
+		aud_config = VLV_AUD_CFG(pipe);
+		aud_cntl_st = VLV_AUD_CNTL_ST(pipe);
+		aud_cntrl_st2 = VLV_AUD_CNTL_ST2;
+	} else {
+		hdmiw_hdmiedid = CPT_HDMIW_HDMIEDID(pipe);
+		aud_config = CPT_AUD_CFG(pipe);
+		aud_cntl_st = CPT_AUD_CNTL_ST(pipe);
+		aud_cntrl_st2 = CPT_AUD_CNTRL_ST2;
+	}
+
+	if (WARN_ON(!port)) {
+		eldv = IBX_ELD_VALID(PORT_B) | IBX_ELD_VALID(PORT_C) |
+			IBX_ELD_VALID(PORT_D);
+	} else {
+		eldv = IBX_ELD_VALID(port);
+	}
+
+	/* Invalidate ELD */
+	tmp = I915_READ(aud_cntrl_st2);
+	tmp &= ~eldv;
+	I915_WRITE(aud_cntrl_st2, tmp);
+
+	/* Reset ELD write address */
+	tmp = I915_READ(aud_cntl_st);
+	tmp &= ~IBX_ELD_ADDRESS_MASK;
+	I915_WRITE(aud_cntl_st, tmp);
+
+	/* Up to 84 bytes of hw ELD buffer */
+	len = min(drm_eld_size(eld), 84);
+	for (i = 0; i < len / 4; i++)
+		I915_WRITE(hdmiw_hdmiedid, *((uint32_t *)eld + i));
+
+	/* ELD valid */
+	tmp = I915_READ(aud_cntrl_st2);
+	tmp |= eldv;
+	I915_WRITE(aud_cntrl_st2, tmp);
+
+	/* Enable timestamps */
+	tmp = I915_READ(aud_config);
+	tmp &= ~AUD_CONFIG_N_VALUE_INDEX;
+	tmp &= ~AUD_CONFIG_N_PROG_ENABLE;
+	tmp &= ~AUD_CONFIG_PIXEL_CLOCK_HDMI_MASK;
+	if (intel_pipe_has_type(intel_crtc, INTEL_OUTPUT_DISPLAYPORT))
+		tmp |= AUD_CONFIG_N_VALUE_INDEX;
+	else
+		tmp |= audio_config_hdmi_pixel_clock(mode);
+	I915_WRITE(aud_config, tmp);
+}
+
+/**
+ * intel_audio_codec_enable - Enable the audio codec for HD audio
+ * @intel_encoder: encoder on which to enable audio
+ *
+ * The enable sequences may only be performed after enabling the transcoder and
+ * port, and after completed link training.
+ */
+void intel_audio_codec_enable(struct intel_encoder *intel_encoder)
+{
+	struct drm_encoder *encoder = &intel_encoder->base;
+	struct intel_crtc *crtc = to_intel_crtc(encoder->crtc);
+	struct drm_display_mode *mode = &crtc->config.adjusted_mode;
+	struct drm_connector *connector;
+	struct drm_device *dev = encoder->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	connector = drm_select_eld(encoder, mode);
+	if (!connector)
+		return;
+
+	DRM_DEBUG_DRIVER("ELD on [CONNECTOR:%d:%s], [ENCODER:%d:%s]\n",
+			 connector->base.id,
+			 connector->name,
+			 connector->encoder->base.id,
+			 connector->encoder->name);
+
+	/* ELD Conn_Type */
+	connector->eld[5] &= ~(3 << 2);
+	if (intel_pipe_has_type(crtc, INTEL_OUTPUT_DISPLAYPORT))
+		connector->eld[5] |= (1 << 2);
+
+	connector->eld[6] = drm_av_sync_delay(connector, mode) / 2;
+
+	if (dev_priv->display.audio_codec_enable)
+		dev_priv->display.audio_codec_enable(connector, intel_encoder, mode);
+}
+
+/**
+ * intel_audio_codec_disable - Disable the audio codec for HD audio
+ * @encoder: encoder on which to disable audio
+ *
+ * The disable sequences must be performed before disabling the transcoder or
+ * port.
+ */
+void intel_audio_codec_disable(struct intel_encoder *encoder)
+{
+	struct drm_device *dev = encoder->base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	if (dev_priv->display.audio_codec_disable)
+		dev_priv->display.audio_codec_disable(encoder);
+}
+
+/**
+ * intel_init_audio - Set up chip specific audio functions
+ * @dev: drm device
+ */
+void intel_init_audio(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	if (IS_G4X(dev)) {
+		dev_priv->display.audio_codec_enable = g4x_audio_codec_enable;
+		dev_priv->display.audio_codec_disable = g4x_audio_codec_disable;
+	} else if (IS_VALLEYVIEW(dev)) {
+		dev_priv->display.audio_codec_enable = ilk_audio_codec_enable;
+		dev_priv->display.audio_codec_disable = ilk_audio_codec_disable;
+	} else if (IS_HASWELL(dev) || INTEL_INFO(dev)->gen >= 8) {
+		dev_priv->display.audio_codec_enable = hsw_audio_codec_enable;
+		dev_priv->display.audio_codec_disable = hsw_audio_codec_disable;
+	} else if (HAS_PCH_SPLIT(dev)) {
+		dev_priv->display.audio_codec_enable = ilk_audio_codec_enable;
+		dev_priv->display.audio_codec_disable = ilk_audio_codec_disable;
+	}
+}
diff -urN a/drivers/gpu/drm/i915/intel_bios.c b/drivers/gpu/drm/i915/intel_bios.c
--- a/drivers/gpu/drm/i915/intel_bios.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_bios.c	2014-11-22 14:37:49.338700418 -0700
@@ -287,7 +287,7 @@
 			downclock = dvo_timing->clock;
 	}
 
-	if (downclock < panel_dvo_timing->clock && i915.lvds_downclock) {
+	if (downclock < panel_dvo_timing->clock && i915_module.lvds_downclock) {
 		dev_priv->lvds_downclock_avail = 1;
 		dev_priv->lvds_downclock = downclock * 10;
 		DRM_DEBUG_KMS("LVDS downclock is found in VBT. "
@@ -354,7 +354,7 @@
 	struct drm_display_mode *panel_fixed_mode;
 	int index;
 
-	index = i915.vbt_sdvo_panel_type;
+	index = i915_module.vbt_sdvo_panel_type;
 	if (index == -2) {
 		DRM_DEBUG_KMS("Ignore SDVO panel mode from BIOS VBT tables.\n");
 		return;
@@ -627,16 +627,16 @@
 
 	switch (edp_link_params->preemphasis) {
 	case EDP_PREEMPHASIS_NONE:
-		dev_priv->vbt.edp_preemphasis = DP_TRAIN_PRE_EMPHASIS_0;
+		dev_priv->vbt.edp_preemphasis = DP_TRAIN_PRE_EMPH_LEVEL_0;
 		break;
 	case EDP_PREEMPHASIS_3_5dB:
-		dev_priv->vbt.edp_preemphasis = DP_TRAIN_PRE_EMPHASIS_3_5;
+		dev_priv->vbt.edp_preemphasis = DP_TRAIN_PRE_EMPH_LEVEL_1;
 		break;
 	case EDP_PREEMPHASIS_6dB:
-		dev_priv->vbt.edp_preemphasis = DP_TRAIN_PRE_EMPHASIS_6;
+		dev_priv->vbt.edp_preemphasis = DP_TRAIN_PRE_EMPH_LEVEL_2;
 		break;
 	case EDP_PREEMPHASIS_9_5dB:
-		dev_priv->vbt.edp_preemphasis = DP_TRAIN_PRE_EMPHASIS_9_5;
+		dev_priv->vbt.edp_preemphasis = DP_TRAIN_PRE_EMPH_LEVEL_3;
 		break;
 	default:
 		DRM_DEBUG_KMS("VBT has unknown eDP pre-emphasis value %u\n",
@@ -646,16 +646,16 @@
 
 	switch (edp_link_params->vswing) {
 	case EDP_VSWING_0_4V:
-		dev_priv->vbt.edp_vswing = DP_TRAIN_VOLTAGE_SWING_400;
+		dev_priv->vbt.edp_vswing = DP_TRAIN_VOLTAGE_SWING_LEVEL_0;
 		break;
 	case EDP_VSWING_0_6V:
-		dev_priv->vbt.edp_vswing = DP_TRAIN_VOLTAGE_SWING_600;
+		dev_priv->vbt.edp_vswing = DP_TRAIN_VOLTAGE_SWING_LEVEL_1;
 		break;
 	case EDP_VSWING_0_8V:
-		dev_priv->vbt.edp_vswing = DP_TRAIN_VOLTAGE_SWING_800;
+		dev_priv->vbt.edp_vswing = DP_TRAIN_VOLTAGE_SWING_LEVEL_2;
 		break;
 	case EDP_VSWING_1_2V:
-		dev_priv->vbt.edp_vswing = DP_TRAIN_VOLTAGE_SWING_1200;
+		dev_priv->vbt.edp_vswing = DP_TRAIN_VOLTAGE_SWING_LEVEL_3;
 		break;
 	default:
 		DRM_DEBUG_KMS("VBT has unknown eDP voltage swing value %u\n",
@@ -946,7 +946,7 @@
 		DRM_DEBUG_KMS("Analog port %c is also DP or TMDS compatible\n",
 			      port_name(port));
 	if (is_dvi && (port == PORT_A || port == PORT_E))
-		DRM_DEBUG_KMS("Port %c is TMDS compabile\n", port_name(port));
+		DRM_DEBUG_KMS("Port %c is TMDS compatible\n", port_name(port));
 	if (!is_dvi && !is_dp && !is_crt)
 		DRM_DEBUG_KMS("Port %c is not DP/TMDS/CRT compatible\n",
 			      port_name(port));
@@ -976,12 +976,10 @@
 	if (bdb->version >= 158) {
 		/* The VBT HDMI level shift values match the table we have. */
 		hdmi_level_shift = child->raw[7] & 0xF;
-		if (hdmi_level_shift < 0xC) {
-			DRM_DEBUG_KMS("VBT HDMI level shift for port %c: %d\n",
-				      port_name(port),
-				      hdmi_level_shift);
-			info->hdmi_level_shift = hdmi_level_shift;
-		}
+		DRM_DEBUG_KMS("VBT HDMI level shift for port %c: %d\n",
+			      port_name(port),
+			      hdmi_level_shift);
+		info->hdmi_level_shift = hdmi_level_shift;
 	}
 }
 
@@ -1114,8 +1112,7 @@
 		struct ddi_vbt_port_info *info =
 			&dev_priv->vbt.ddi_port_info[port];
 
-		/* Recommended BSpec default: 800mV 0dB. */
-		info->hdmi_level_shift = 6;
+		info->hdmi_level_shift = HDMI_LEVEL_SHIFT_UNKNOWN;
 
 		info->supports_dvi = (port != PORT_A && port != PORT_E);
 		info->supports_hdmi = info->supports_dvi;
diff -urN a/drivers/gpu/drm/i915/intel_bios.h b/drivers/gpu/drm/i915/intel_bios.h
--- a/drivers/gpu/drm/i915/intel_bios.h	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_bios.h	2014-11-22 14:37:49.338700418 -0700
@@ -46,7 +46,7 @@
 	u16 version;			/**< decimal */
 	u16 header_size;		/**< in bytes */
 	u16 bdb_size;			/**< in bytes */
-};
+} __packed;
 
 /* strictly speaking, this is a "skip" block, but it has interesting info */
 struct vbios_data {
@@ -252,7 +252,7 @@
 	/* This one should also be safe to use anywhere, even without version
 	 * checks. */
 	struct common_child_dev_config common;
-};
+} __packed;
 
 struct bdb_general_definitions {
 	/* DDC GPIO */
@@ -802,7 +802,8 @@
 
 	u16 rsvd4;
 
-	u8 rsvd5[5];
+	u8 rsvd5;
+	u32 target_burst_mode_freq;
 	u32 dsi_ddr_clk;
 	u32 bridge_ref_clk;
 
@@ -887,12 +888,12 @@
 	u16 bl_disable_delay;
 	u16 panel_off_delay;
 	u16 panel_power_cycle_delay;
-};
+} __packed;
 
 struct bdb_mipi_config {
 	struct mipi_config config[MAX_MIPI_CONFIGURATIONS];
 	struct mipi_pps_data pps[MAX_MIPI_CONFIGURATIONS];
-};
+} __packed;
 
 /* Block 53 contains MIPI sequences as needed by the panel
  * for enabling it. This block can be variable in size and
@@ -901,7 +902,7 @@
 struct bdb_mipi_sequence {
 	u8 version;
 	u8 data[0];
-};
+} __packed;
 
 /* MIPI Sequnece Block definitions */
 enum mipi_seq {
diff -urN a/drivers/gpu/drm/i915/intel_crt.c b/drivers/gpu/drm/i915/intel_crt.c
--- a/drivers/gpu/drm/i915/intel_crt.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_crt.c	2014-11-22 14:37:49.338700418 -0700
@@ -72,7 +72,7 @@
 	u32 tmp;
 
 	power_domain = intel_display_port_power_domain(encoder);
-	if (!intel_display_power_enabled(dev_priv, power_domain))
+	if (!intel_display_power_is_enabled(dev_priv, power_domain))
 		return false;
 
 	tmp = I915_READ(crt->adpa_reg);
@@ -775,7 +775,7 @@
 		I915_WRITE(crt->adpa_reg, adpa);
 		POSTING_READ(crt->adpa_reg);
 
-		DRM_DEBUG_KMS("pch crt adpa set to 0x%x\n", adpa);
+		DRM_DEBUG_KMS("crt adpa set to 0x%x\n", adpa);
 		crt->force_hotplug_required = 1;
 	}
 
diff -urN a/drivers/gpu/drm/i915/intel_ddi.c b/drivers/gpu/drm/i915/intel_ddi.c
--- a/drivers/gpu/drm/i915/intel_ddi.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_ddi.c	2014-11-22 14:37:49.338700418 -0700
@@ -28,87 +28,129 @@
 #include "i915_drv.h"
 #include "intel_drv.h"
 
+struct ddi_buf_trans {
+	u32 trans1;	/* balance leg enable, de-emph level */
+	u32 trans2;	/* vref sel, vswing */
+};
+
 /* HDMI/DVI modes ignore everything but the last 2 items. So we share
  * them for both DP and FDI transports, allowing those ports to
  * automatically adapt to HDMI connections as well
  */
-static const u32 hsw_ddi_translations_dp[] = {
-	0x00FFFFFF, 0x0006000E,		/* DP parameters */
-	0x00D75FFF, 0x0005000A,
-	0x00C30FFF, 0x00040006,
-	0x80AAAFFF, 0x000B0000,
-	0x00FFFFFF, 0x0005000A,
-	0x00D75FFF, 0x000C0004,
-	0x80C30FFF, 0x000B0000,
-	0x00FFFFFF, 0x00040006,
-	0x80D75FFF, 0x000B0000,
+static const struct ddi_buf_trans hsw_ddi_translations_dp[] = {
+	{ 0x00FFFFFF, 0x0006000E },
+	{ 0x00D75FFF, 0x0005000A },
+	{ 0x00C30FFF, 0x00040006 },
+	{ 0x80AAAFFF, 0x000B0000 },
+	{ 0x00FFFFFF, 0x0005000A },
+	{ 0x00D75FFF, 0x000C0004 },
+	{ 0x80C30FFF, 0x000B0000 },
+	{ 0x00FFFFFF, 0x00040006 },
+	{ 0x80D75FFF, 0x000B0000 },
+};
+
+static const struct ddi_buf_trans hsw_ddi_translations_fdi[] = {
+	{ 0x00FFFFFF, 0x0007000E },
+	{ 0x00D75FFF, 0x000F000A },
+	{ 0x00C30FFF, 0x00060006 },
+	{ 0x00AAAFFF, 0x001E0000 },
+	{ 0x00FFFFFF, 0x000F000A },
+	{ 0x00D75FFF, 0x00160004 },
+	{ 0x00C30FFF, 0x001E0000 },
+	{ 0x00FFFFFF, 0x00060006 },
+	{ 0x00D75FFF, 0x001E0000 },
+};
+
+static const struct ddi_buf_trans hsw_ddi_translations_hdmi[] = {
+					/* Idx	NT mV d	T mV d	db	*/
+	{ 0x00FFFFFF, 0x0006000E },	/* 0:	400	400	0	*/
+	{ 0x00E79FFF, 0x000E000C },	/* 1:	400	500	2	*/
+	{ 0x00D75FFF, 0x0005000A },	/* 2:	400	600	3.5	*/
+	{ 0x00FFFFFF, 0x0005000A },	/* 3:	600	600	0	*/
+	{ 0x00E79FFF, 0x001D0007 },	/* 4:	600	750	2	*/
+	{ 0x00D75FFF, 0x000C0004 },	/* 5:	600	900	3.5	*/
+	{ 0x00FFFFFF, 0x00040006 },	/* 6:	800	800	0	*/
+	{ 0x80E79FFF, 0x00030002 },	/* 7:	800	1000	2	*/
+	{ 0x00FFFFFF, 0x00140005 },	/* 8:	850	850	0	*/
+	{ 0x00FFFFFF, 0x000C0004 },	/* 9:	900	900	0	*/
+	{ 0x00FFFFFF, 0x001C0003 },	/* 10:	950	950	0	*/
+	{ 0x80FFFFFF, 0x00030002 },	/* 11:	1000	1000	0	*/
+};
+
+static const struct ddi_buf_trans bdw_ddi_translations_edp[] = {
+	{ 0x00FFFFFF, 0x00000012 },
+	{ 0x00EBAFFF, 0x00020011 },
+	{ 0x00C71FFF, 0x0006000F },
+	{ 0x00AAAFFF, 0x000E000A },
+	{ 0x00FFFFFF, 0x00020011 },
+	{ 0x00DB6FFF, 0x0005000F },
+	{ 0x00BEEFFF, 0x000A000C },
+	{ 0x00FFFFFF, 0x0005000F },
+	{ 0x00DB6FFF, 0x000A000C },
 };
 
-static const u32 hsw_ddi_translations_fdi[] = {
-	0x00FFFFFF, 0x0007000E,		/* FDI parameters */
-	0x00D75FFF, 0x000F000A,
-	0x00C30FFF, 0x00060006,
-	0x00AAAFFF, 0x001E0000,
-	0x00FFFFFF, 0x000F000A,
-	0x00D75FFF, 0x00160004,
-	0x00C30FFF, 0x001E0000,
-	0x00FFFFFF, 0x00060006,
-	0x00D75FFF, 0x001E0000,
+static const struct ddi_buf_trans bdw_ddi_translations_dp[] = {
+	{ 0x00FFFFFF, 0x0007000E },
+	{ 0x00D75FFF, 0x000E000A },
+	{ 0x00BEFFFF, 0x00140006 },
+	{ 0x80B2CFFF, 0x001B0002 },
+	{ 0x00FFFFFF, 0x000E000A },
+	{ 0x00DB6FFF, 0x00160005 },
+	{ 0x80C71FFF, 0x001A0002 },
+	{ 0x00F7DFFF, 0x00180004 },
+	{ 0x80D75FFF, 0x001B0002 },
 };
 
-static const u32 hsw_ddi_translations_hdmi[] = {
-				/* Idx	NT mV diff	T mV diff	db  */
-	0x00FFFFFF, 0x0006000E, /* 0:	400		400		0   */
-	0x00E79FFF, 0x000E000C, /* 1:	400		500		2   */
-	0x00D75FFF, 0x0005000A, /* 2:	400		600		3.5 */
-	0x00FFFFFF, 0x0005000A, /* 3:	600		600		0   */
-	0x00E79FFF, 0x001D0007, /* 4:	600		750		2   */
-	0x00D75FFF, 0x000C0004, /* 5:	600		900		3.5 */
-	0x00FFFFFF, 0x00040006, /* 6:	800		800		0   */
-	0x80E79FFF, 0x00030002, /* 7:	800		1000		2   */
-	0x00FFFFFF, 0x00140005, /* 8:	850		850		0   */
-	0x00FFFFFF, 0x000C0004, /* 9:	900		900		0   */
-	0x00FFFFFF, 0x001C0003, /* 10:	950		950		0   */
-	0x80FFFFFF, 0x00030002, /* 11:	1000		1000		0   */
+static const struct ddi_buf_trans bdw_ddi_translations_fdi[] = {
+	{ 0x00FFFFFF, 0x0001000E },
+	{ 0x00D75FFF, 0x0004000A },
+	{ 0x00C30FFF, 0x00070006 },
+	{ 0x00AAAFFF, 0x000C0000 },
+	{ 0x00FFFFFF, 0x0004000A },
+	{ 0x00D75FFF, 0x00090004 },
+	{ 0x00C30FFF, 0x000C0000 },
+	{ 0x00FFFFFF, 0x00070006 },
+	{ 0x00D75FFF, 0x000C0000 },
 };
 
-static const u32 bdw_ddi_translations_edp[] = {
-	0x00FFFFFF, 0x00000012,		/* eDP parameters */
-	0x00EBAFFF, 0x00020011,
-	0x00C71FFF, 0x0006000F,
-	0x00AAAFFF, 0x000E000A,
-	0x00FFFFFF, 0x00020011,
-	0x00DB6FFF, 0x0005000F,
-	0x00BEEFFF, 0x000A000C,
-	0x00FFFFFF, 0x0005000F,
-	0x00DB6FFF, 0x000A000C,
-	0x00FFFFFF, 0x00140006		/* HDMI parameters 800mV 0dB*/
+static const struct ddi_buf_trans bdw_ddi_translations_hdmi[] = {
+					/* Idx	NT mV d	T mV df	db	*/
+	{ 0x00FFFFFF, 0x0007000E },	/* 0:	400	400	0	*/
+	{ 0x00D75FFF, 0x000E000A },	/* 1:	400	600	3.5	*/
+	{ 0x00BEFFFF, 0x00140006 },	/* 2:	400	800	6	*/
+	{ 0x00FFFFFF, 0x0009000D },	/* 3:	450	450	0	*/
+	{ 0x00FFFFFF, 0x000E000A },	/* 4:	600	600	0	*/
+	{ 0x00D7FFFF, 0x00140006 },	/* 5:	600	800	2.5	*/
+	{ 0x80CB2FFF, 0x001B0002 },	/* 6:	600	1000	4.5	*/
+	{ 0x00FFFFFF, 0x00140006 },	/* 7:	800	800	0	*/
+	{ 0x80E79FFF, 0x001B0002 },	/* 8:	800	1000	2	*/
+	{ 0x80FFFFFF, 0x001B0002 },	/* 9:	1000	1000	0	*/
 };
 
-static const u32 bdw_ddi_translations_dp[] = {
-	0x00FFFFFF, 0x0007000E,		/* DP parameters */
-	0x00D75FFF, 0x000E000A,
-	0x00BEFFFF, 0x00140006,
-	0x80B2CFFF, 0x001B0002,
-	0x00FFFFFF, 0x000E000A,
-	0x00D75FFF, 0x00180004,
-	0x80CB2FFF, 0x001B0002,
-	0x00F7DFFF, 0x00180004,
-	0x80D75FFF, 0x001B0002,
-	0x00FFFFFF, 0x00140006		/* HDMI parameters 800mV 0dB*/
+static const struct ddi_buf_trans skl_ddi_translations_dp[] = {
+	{ 0x00000018, 0x000000a0 },
+	{ 0x00004014, 0x00000098 },
+	{ 0x00006012, 0x00000088 },
+	{ 0x00008010, 0x00000080 },
+	{ 0x00000018, 0x00000098 },
+	{ 0x00004014, 0x00000088 },
+	{ 0x00006012, 0x00000080 },
+	{ 0x00000018, 0x00000088 },
+	{ 0x00004014, 0x00000080 },
 };
 
-static const u32 bdw_ddi_translations_fdi[] = {
-	0x00FFFFFF, 0x0001000E,		/* FDI parameters */
-	0x00D75FFF, 0x0004000A,
-	0x00C30FFF, 0x00070006,
-	0x00AAAFFF, 0x000C0000,
-	0x00FFFFFF, 0x0004000A,
-	0x00D75FFF, 0x00090004,
-	0x00C30FFF, 0x000C0000,
-	0x00FFFFFF, 0x00070006,
-	0x00D75FFF, 0x000C0000,
-	0x00FFFFFF, 0x00140006		/* HDMI parameters 800mV 0dB*/
+static const struct ddi_buf_trans skl_ddi_translations_hdmi[] = {
+					/* Idx	NT mV   T mV    db  */
+	{ 0x00000018, 0x000000a0 },	/* 0:	400	400	0   */
+	{ 0x00004014, 0x00000098 },	/* 1:	400	600	3.5 */
+	{ 0x00006012, 0x00000088 },	/* 2:	400	800	6   */
+	{ 0x00000018, 0x0000003c },	/* 3:	450	450	0   */
+	{ 0x00000018, 0x00000098 },	/* 4:	600	600	0   */
+	{ 0x00003015, 0x00000088 },	/* 5:	600	800	2.5 */
+	{ 0x00005013, 0x00000080 },	/* 6:	600	1000	4.5 */
+	{ 0x00000018, 0x00000088 },	/* 7:	800	800	0   */
+	{ 0x00000096, 0x00000080 },	/* 8:	800	1000	2   */
+	{ 0x00000018, 0x00000080 },	/* 9:	1200	1200	0   */
 };
 
 enum port intel_ddi_get_encoder_port(struct intel_encoder *intel_encoder)
@@ -145,26 +187,43 @@
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	u32 reg;
-	int i;
+	int i, n_hdmi_entries, hdmi_800mV_0dB;
 	int hdmi_level = dev_priv->vbt.ddi_port_info[port].hdmi_level_shift;
-	const u32 *ddi_translations_fdi;
-	const u32 *ddi_translations_dp;
-	const u32 *ddi_translations_edp;
-	const u32 *ddi_translations;
-
-	if (IS_BROADWELL(dev)) {
+	const struct ddi_buf_trans *ddi_translations_fdi;
+	const struct ddi_buf_trans *ddi_translations_dp;
+	const struct ddi_buf_trans *ddi_translations_edp;
+	const struct ddi_buf_trans *ddi_translations_hdmi;
+	const struct ddi_buf_trans *ddi_translations;
+
+	if (IS_SKYLAKE(dev)) {
+		ddi_translations_fdi = NULL;
+		ddi_translations_dp = skl_ddi_translations_dp;
+		ddi_translations_edp = skl_ddi_translations_dp;
+		ddi_translations_hdmi = skl_ddi_translations_hdmi;
+		n_hdmi_entries = ARRAY_SIZE(skl_ddi_translations_hdmi);
+		hdmi_800mV_0dB = 7;
+	} else if (IS_BROADWELL(dev)) {
 		ddi_translations_fdi = bdw_ddi_translations_fdi;
 		ddi_translations_dp = bdw_ddi_translations_dp;
 		ddi_translations_edp = bdw_ddi_translations_edp;
+		ddi_translations_hdmi = bdw_ddi_translations_hdmi;
+		n_hdmi_entries = ARRAY_SIZE(bdw_ddi_translations_hdmi);
+		hdmi_800mV_0dB = 7;
 	} else if (IS_HASWELL(dev)) {
 		ddi_translations_fdi = hsw_ddi_translations_fdi;
 		ddi_translations_dp = hsw_ddi_translations_dp;
 		ddi_translations_edp = hsw_ddi_translations_dp;
+		ddi_translations_hdmi = hsw_ddi_translations_hdmi;
+		n_hdmi_entries = ARRAY_SIZE(hsw_ddi_translations_hdmi);
+		hdmi_800mV_0dB = 6;
 	} else {
 		WARN(1, "ddi translation table missing\n");
 		ddi_translations_edp = bdw_ddi_translations_dp;
 		ddi_translations_fdi = bdw_ddi_translations_fdi;
 		ddi_translations_dp = bdw_ddi_translations_dp;
+		ddi_translations_hdmi = bdw_ddi_translations_hdmi;
+		n_hdmi_entries = ARRAY_SIZE(bdw_ddi_translations_hdmi);
+		hdmi_800mV_0dB = 7;
 	}
 
 	switch (port) {
@@ -182,7 +241,10 @@
 			ddi_translations = ddi_translations_dp;
 		break;
 	case PORT_E:
-		ddi_translations = ddi_translations_fdi;
+		if (ddi_translations_fdi)
+			ddi_translations = ddi_translations_fdi;
+		else
+			ddi_translations = ddi_translations_dp;
 		break;
 	default:
 		BUG();
@@ -190,14 +252,22 @@
 
 	for (i = 0, reg = DDI_BUF_TRANS(port);
 	     i < ARRAY_SIZE(hsw_ddi_translations_fdi); i++) {
-		I915_WRITE(reg, ddi_translations[i]);
+		I915_WRITE(reg, ddi_translations[i].trans1);
 		reg += 4;
-	}
-	/* Entry 9 is for HDMI: */
-	for (i = 0; i < 2; i++) {
-		I915_WRITE(reg, hsw_ddi_translations_hdmi[hdmi_level * 2 + i]);
+		I915_WRITE(reg, ddi_translations[i].trans2);
 		reg += 4;
 	}
+
+	/* Choose a good default if VBT is badly populated */
+	if (hdmi_level == HDMI_LEVEL_SHIFT_UNKNOWN ||
+	    hdmi_level >= n_hdmi_entries)
+		hdmi_level = hdmi_800mV_0dB;
+
+	/* Entry 9 is for HDMI: */
+	I915_WRITE(reg, ddi_translations_hdmi[hdmi_level].trans1);
+	reg += 4;
+	I915_WRITE(reg, ddi_translations_hdmi[hdmi_level].trans2);
+	reg += 4;
 }
 
 /* Program DDI buffers translations for DP. By default, program ports A-D in DP
@@ -214,18 +284,6 @@
 		intel_prepare_ddi_buffers(dev, port);
 }
 
-static const long hsw_ddi_buf_ctl_values[] = {
-	DDI_BUF_EMP_400MV_0DB_HSW,
-	DDI_BUF_EMP_400MV_3_5DB_HSW,
-	DDI_BUF_EMP_400MV_6DB_HSW,
-	DDI_BUF_EMP_400MV_9_5DB_HSW,
-	DDI_BUF_EMP_600MV_0DB_HSW,
-	DDI_BUF_EMP_600MV_3_5DB_HSW,
-	DDI_BUF_EMP_600MV_6DB_HSW,
-	DDI_BUF_EMP_800MV_0DB_HSW,
-	DDI_BUF_EMP_800MV_3_5DB_HSW
-};
-
 static void intel_wait_ddi_buf_idle(struct drm_i915_private *dev_priv,
 				    enum port port)
 {
@@ -285,7 +343,7 @@
 
 	/* Start the training iterating through available voltages and emphasis,
 	 * testing each value twice. */
-	for (i = 0; i < ARRAY_SIZE(hsw_ddi_buf_ctl_values) * 2; i++) {
+	for (i = 0; i < ARRAY_SIZE(hsw_ddi_translations_fdi) * 2; i++) {
 		/* Configure DP_TP_CTL with auto-training */
 		I915_WRITE(DP_TP_CTL(PORT_E),
 					DP_TP_CTL_FDI_AUTOTRAIN |
@@ -300,7 +358,7 @@
 		I915_WRITE(DDI_BUF_CTL(PORT_E),
 			   DDI_BUF_CTL_ENABLE |
 			   ((intel_crtc->config.fdi_lanes - 1) << 1) |
-			   hsw_ddi_buf_ctl_values[i / 2]);
+			   DDI_BUF_TRANS_SELECT(i / 2));
 		POSTING_READ(DDI_BUF_CTL(PORT_E));
 
 		udelay(600);
@@ -375,7 +433,7 @@
 		enc_to_dig_port(&encoder->base);
 
 	intel_dp->DP = intel_dig_port->saved_port_bits |
-		DDI_BUF_CTL_ENABLE | DDI_BUF_EMP_400MV_0DB_HSW;
+		DDI_BUF_CTL_ENABLE | DDI_BUF_TRANS_SELECT(0);
 	intel_dp->DP |= DDI_PORT_WIDTH(intel_dp->lane_count);
 
 }
@@ -401,8 +459,29 @@
 	return ret;
 }
 
+static struct intel_encoder *
+intel_ddi_get_crtc_new_encoder(struct intel_crtc *crtc)
+{
+	struct drm_device *dev = crtc->base.dev;
+	struct intel_encoder *intel_encoder, *ret = NULL;
+	int num_encoders = 0;
+
+	for_each_intel_encoder(dev, intel_encoder) {
+		if (intel_encoder->new_crtc == crtc) {
+			ret = intel_encoder;
+			num_encoders++;
+		}
+	}
+
+	WARN(num_encoders != 1, "%d encoders on crtc for pipe %c\n", num_encoders,
+	     pipe_name(crtc->pipe));
+
+	BUG_ON(ret == NULL);
+	return ret;
+}
+
 #define LC_FREQ 2700
-#define LC_FREQ_2K (LC_FREQ * 2000)
+#define LC_FREQ_2K U64_C(LC_FREQ * 2000)
 
 #define P_MIN 2
 #define P_MAX 64
@@ -414,7 +493,11 @@
 #define VCO_MIN 2400
 #define VCO_MAX 4800
 
-#define ABS_DIFF(a, b) ((a > b) ? (a - b) : (b - a))
+#define abs_diff(a, b) ({			\
+	typeof(a) __a = (a);			\
+	typeof(b) __b = (b);			\
+	(void) (&__a == &__b);			\
+	__a > __b ? (__a - __b) : (__b - __a); })
 
 struct wrpll_rnp {
 	unsigned p, n2, r2;
@@ -524,9 +607,9 @@
 	 */
 	a = freq2k * budget * p * r2;
 	b = freq2k * budget * best->p * best->r2;
-	diff = ABS_DIFF((freq2k * p * r2), (LC_FREQ_2K * n2));
-	diff_best = ABS_DIFF((freq2k * best->p * best->r2),
-			     (LC_FREQ_2K * best->n2));
+	diff = abs_diff(freq2k * p * r2, LC_FREQ_2K * n2);
+	diff_best = abs_diff(freq2k * best->p * best->r2,
+			     LC_FREQ_2K * best->n2);
 	c = 1000000 * diff;
 	d = 1000000 * diff_best;
 
@@ -587,8 +670,113 @@
 	return (refclk * n * 100) / (p * r);
 }
 
-void intel_ddi_clock_get(struct intel_encoder *encoder,
-			 struct intel_crtc_config *pipe_config)
+static int skl_calc_wrpll_link(struct drm_i915_private *dev_priv,
+			       uint32_t dpll)
+{
+	uint32_t cfgcr1_reg, cfgcr2_reg;
+	uint32_t cfgcr1_val, cfgcr2_val;
+	uint32_t p0, p1, p2, dco_freq;
+
+	cfgcr1_reg = GET_CFG_CR1_REG(dpll);
+	cfgcr2_reg = GET_CFG_CR2_REG(dpll);
+
+	cfgcr1_val = I915_READ(cfgcr1_reg);
+	cfgcr2_val = I915_READ(cfgcr2_reg);
+
+	p0 = cfgcr2_val & DPLL_CFGCR2_PDIV_MASK;
+	p2 = cfgcr2_val & DPLL_CFGCR2_KDIV_MASK;
+
+	if (cfgcr2_val &  DPLL_CFGCR2_QDIV_MODE(1))
+		p1 = (cfgcr2_val & DPLL_CFGCR2_QDIV_RATIO_MASK) >> 8;
+	else
+		p1 = 1;
+
+
+	switch (p0) {
+	case DPLL_CFGCR2_PDIV_1:
+		p0 = 1;
+		break;
+	case DPLL_CFGCR2_PDIV_2:
+		p0 = 2;
+		break;
+	case DPLL_CFGCR2_PDIV_3:
+		p0 = 3;
+		break;
+	case DPLL_CFGCR2_PDIV_7:
+		p0 = 7;
+		break;
+	}
+
+	switch (p2) {
+	case DPLL_CFGCR2_KDIV_5:
+		p2 = 5;
+		break;
+	case DPLL_CFGCR2_KDIV_2:
+		p2 = 2;
+		break;
+	case DPLL_CFGCR2_KDIV_3:
+		p2 = 3;
+		break;
+	case DPLL_CFGCR2_KDIV_1:
+		p2 = 1;
+		break;
+	}
+
+	dco_freq = (cfgcr1_val & DPLL_CFGCR1_DCO_INTEGER_MASK) * 24 * 1000;
+
+	dco_freq += (((cfgcr1_val & DPLL_CFGCR1_DCO_FRACTION_MASK) >> 9) * 24 *
+		1000) / 0x8000;
+
+	return dco_freq / (p0 * p1 * p2 * 5);
+}
+
+
+static void skl_ddi_clock_get(struct intel_encoder *encoder,
+				struct intel_crtc_config *pipe_config)
+{
+	struct drm_i915_private *dev_priv = encoder->base.dev->dev_private;
+	int link_clock = 0;
+	uint32_t dpll_ctl1, dpll;
+
+	dpll = pipe_config->ddi_pll_sel;
+
+	dpll_ctl1 = I915_READ(DPLL_CTRL1);
+
+	if (dpll_ctl1 & DPLL_CTRL1_HDMI_MODE(dpll)) {
+		link_clock = skl_calc_wrpll_link(dev_priv, dpll);
+	} else {
+		link_clock = dpll_ctl1 & DPLL_CRTL1_LINK_RATE_MASK(dpll);
+		link_clock >>= DPLL_CRTL1_LINK_RATE_SHIFT(dpll);
+
+		switch (link_clock) {
+		case DPLL_CRTL1_LINK_RATE_810:
+			link_clock = 81000;
+			break;
+		case DPLL_CRTL1_LINK_RATE_1350:
+			link_clock = 135000;
+			break;
+		case DPLL_CRTL1_LINK_RATE_2700:
+			link_clock = 270000;
+			break;
+		default:
+			WARN(1, "Unsupported link rate\n");
+			break;
+		}
+		link_clock *= 2;
+	}
+
+	pipe_config->port_clock = link_clock;
+
+	if (pipe_config->has_dp_encoder)
+		pipe_config->adjusted_mode.crtc_clock =
+			intel_dotclock_calculate(pipe_config->port_clock,
+						 &pipe_config->dp_m_n);
+	else
+		pipe_config->adjusted_mode.crtc_clock = pipe_config->port_clock;
+}
+
+static void hsw_ddi_clock_get(struct intel_encoder *encoder,
+			      struct intel_crtc_config *pipe_config)
 {
 	struct drm_i915_private *dev_priv = encoder->base.dev->dev_private;
 	int link_clock = 0;
@@ -643,9 +831,15 @@
 		pipe_config->adjusted_mode.crtc_clock = pipe_config->port_clock;
 }
 
+void intel_ddi_clock_get(struct intel_encoder *encoder,
+			 struct intel_crtc_config *pipe_config)
+{
+	hsw_ddi_clock_get(encoder, pipe_config);
+}
+
 static void
-intel_ddi_calculate_wrpll(int clock /* in Hz */,
-			  unsigned *r2_out, unsigned *n2_out, unsigned *p_out)
+hsw_ddi_calculate_wrpll(int clock /* in Hz */,
+			unsigned *r2_out, unsigned *n2_out, unsigned *p_out)
 {
 	uint64_t freq2k;
 	unsigned p, n2, r2;
@@ -708,33 +902,23 @@
 	*r2_out = best.r2;
 }
 
-/*
- * Tries to find a PLL for the CRTC. If it finds, it increases the refcount and
- * stores it in intel_crtc->ddi_pll_sel, so other mode sets won't be able to
- * steal the selected PLL. You need to call intel_ddi_pll_enable to actually
- * enable the PLL.
- */
-bool intel_ddi_pll_select(struct intel_crtc *intel_crtc)
+static bool
+hsw_ddi_pll_select(struct intel_crtc *intel_crtc,
+		   struct intel_encoder *intel_encoder,
+		   int clock)
 {
-	struct drm_crtc *crtc = &intel_crtc->base;
-	struct intel_encoder *intel_encoder = intel_ddi_get_crtc_encoder(crtc);
-	int type = intel_encoder->type;
-	int clock = intel_crtc->config.port_clock;
-
-	intel_put_shared_dpll(intel_crtc);
-
-	if (type == INTEL_OUTPUT_HDMI) {
+	if (intel_encoder->type == INTEL_OUTPUT_HDMI) {
 		struct intel_shared_dpll *pll;
 		uint32_t val;
 		unsigned p, n2, r2;
 
-		intel_ddi_calculate_wrpll(clock * 1000, &r2, &n2, &p);
+		hsw_ddi_calculate_wrpll(clock * 1000, &r2, &n2, &p);
 
 		val = WRPLL_PLL_ENABLE | WRPLL_PLL_LCPLL |
 		      WRPLL_DIVIDER_REFERENCE(r2) | WRPLL_DIVIDER_FEEDBACK(n2) |
 		      WRPLL_DIVIDER_POST(p);
 
-		intel_crtc->config.dpll_hw_state.wrpll = val;
+		intel_crtc->new_config->dpll_hw_state.wrpll = val;
 
 		pll = intel_get_shared_dpll(intel_crtc);
 		if (pll == NULL) {
@@ -743,12 +927,255 @@
 			return false;
 		}
 
-		intel_crtc->config.ddi_pll_sel = PORT_CLK_SEL_WRPLL(pll->id);
+		intel_crtc->new_config->ddi_pll_sel = PORT_CLK_SEL_WRPLL(pll->id);
 	}
 
 	return true;
 }
 
+struct skl_wrpll_params {
+	uint32_t        dco_fraction;
+	uint32_t        dco_integer;
+	uint32_t        qdiv_ratio;
+	uint32_t        qdiv_mode;
+	uint32_t        kdiv;
+	uint32_t        pdiv;
+	uint32_t        central_freq;
+};
+
+static void
+skl_ddi_calculate_wrpll(int clock /* in Hz */,
+			struct skl_wrpll_params *wrpll_params)
+{
+	uint64_t afe_clock = clock * 5; /* AFE Clock is 5x Pixel clock */
+	uint64_t dco_central_freq[3] = {8400000000ULL,
+					9000000000ULL,
+					9600000000ULL};
+	uint32_t min_dco_deviation = 400;
+	uint32_t min_dco_index = 3;
+	uint32_t P0[4] = {1, 2, 3, 7};
+	uint32_t P2[4] = {1, 2, 3, 5};
+	bool found = false;
+	uint32_t candidate_p = 0;
+	uint32_t candidate_p0[3] = {0}, candidate_p1[3] = {0};
+	uint32_t candidate_p2[3] = {0};
+	uint32_t dco_central_freq_deviation[3];
+	uint32_t i, P1, k, dco_count;
+	bool retry_with_odd = false;
+	uint64_t dco_freq;
+
+	/* Determine P0, P1 or P2 */
+	for (dco_count = 0; dco_count < 3; dco_count++) {
+		found = false;
+		candidate_p =
+			div64_u64(dco_central_freq[dco_count], afe_clock);
+		if (retry_with_odd == false)
+			candidate_p = (candidate_p % 2 == 0 ?
+				candidate_p : candidate_p + 1);
+
+		for (P1 = 1; P1 < candidate_p; P1++) {
+			for (i = 0; i < 4; i++) {
+				if (!(P0[i] != 1 || P1 == 1))
+					continue;
+
+				for (k = 0; k < 4; k++) {
+					if (P1 != 1 && P2[k] != 2)
+						continue;
+
+					if (candidate_p == P0[i] * P1 * P2[k]) {
+						/* Found possible P0, P1, P2 */
+						found = true;
+						candidate_p0[dco_count] = P0[i];
+						candidate_p1[dco_count] = P1;
+						candidate_p2[dco_count] = P2[k];
+						goto found;
+					}
+
+				}
+			}
+		}
+
+found:
+		if (found) {
+			dco_central_freq_deviation[dco_count] =
+				div64_u64(10000 *
+					  abs_diff((candidate_p * afe_clock),
+						   dco_central_freq[dco_count]),
+					  dco_central_freq[dco_count]);
+
+			if (dco_central_freq_deviation[dco_count] <
+				min_dco_deviation) {
+				min_dco_deviation =
+					dco_central_freq_deviation[dco_count];
+				min_dco_index = dco_count;
+			}
+		}
+
+		if (min_dco_index > 2 && dco_count == 2) {
+			retry_with_odd = true;
+			dco_count = 0;
+		}
+	}
+
+	if (min_dco_index > 2) {
+		WARN(1, "No valid values found for the given pixel clock\n");
+	} else {
+		 wrpll_params->central_freq = dco_central_freq[min_dco_index];
+
+		 switch (dco_central_freq[min_dco_index]) {
+		 case 9600000000ULL:
+			wrpll_params->central_freq = 0;
+			break;
+		 case 9000000000ULL:
+			wrpll_params->central_freq = 1;
+			break;
+		 case 8400000000ULL:
+			wrpll_params->central_freq = 3;
+		 }
+
+		 switch (candidate_p0[min_dco_index]) {
+		 case 1:
+			wrpll_params->pdiv = 0;
+			break;
+		 case 2:
+			wrpll_params->pdiv = 1;
+			break;
+		 case 3:
+			wrpll_params->pdiv = 2;
+			break;
+		 case 7:
+			wrpll_params->pdiv = 4;
+			break;
+		 default:
+			WARN(1, "Incorrect PDiv\n");
+		 }
+
+		 switch (candidate_p2[min_dco_index]) {
+		 case 5:
+			wrpll_params->kdiv = 0;
+			break;
+		 case 2:
+			wrpll_params->kdiv = 1;
+			break;
+		 case 3:
+			wrpll_params->kdiv = 2;
+			break;
+		 case 1:
+			wrpll_params->kdiv = 3;
+			break;
+		 default:
+			WARN(1, "Incorrect KDiv\n");
+		 }
+
+		 wrpll_params->qdiv_ratio = candidate_p1[min_dco_index];
+		 wrpll_params->qdiv_mode =
+			(wrpll_params->qdiv_ratio == 1) ? 0 : 1;
+
+		 dco_freq = candidate_p0[min_dco_index] *
+			 candidate_p1[min_dco_index] *
+			 candidate_p2[min_dco_index] * afe_clock;
+
+		/*
+		* Intermediate values are in Hz.
+		* Divide by MHz to match bsepc
+		*/
+		 wrpll_params->dco_integer = div_u64(dco_freq, (24 * MHz(1)));
+		 wrpll_params->dco_fraction =
+			 div_u64(((div_u64(dco_freq, 24) -
+				   wrpll_params->dco_integer * MHz(1)) * 0x8000), MHz(1));
+
+	}
+}
+
+
+static bool
+skl_ddi_pll_select(struct intel_crtc *intel_crtc,
+		   struct intel_encoder *intel_encoder,
+		   int clock)
+{
+	struct intel_shared_dpll *pll;
+	uint32_t ctrl1, cfgcr1, cfgcr2;
+
+	/*
+	 * See comment in intel_dpll_hw_state to understand why we always use 0
+	 * as the DPLL id in this function.
+	 */
+
+	ctrl1 = DPLL_CTRL1_OVERRIDE(0);
+
+	if (intel_encoder->type == INTEL_OUTPUT_HDMI) {
+		struct skl_wrpll_params wrpll_params = { 0, };
+
+		ctrl1 |= DPLL_CTRL1_HDMI_MODE(0);
+
+		skl_ddi_calculate_wrpll(clock * 1000, &wrpll_params);
+
+		cfgcr1 = DPLL_CFGCR1_FREQ_ENABLE |
+			 DPLL_CFGCR1_DCO_FRACTION(wrpll_params.dco_fraction) |
+			 wrpll_params.dco_integer;
+
+		cfgcr2 = DPLL_CFGCR2_QDIV_RATIO(wrpll_params.qdiv_ratio) |
+			 DPLL_CFGCR2_QDIV_MODE(wrpll_params.qdiv_mode) |
+			 DPLL_CFGCR2_KDIV(wrpll_params.kdiv) |
+			 DPLL_CFGCR2_PDIV(wrpll_params.pdiv) |
+			 wrpll_params.central_freq;
+	} else if (intel_encoder->type == INTEL_OUTPUT_DISPLAYPORT) {
+		struct drm_encoder *encoder = &intel_encoder->base;
+		struct intel_dp *intel_dp = enc_to_intel_dp(encoder);
+
+		switch (intel_dp->link_bw) {
+		case DP_LINK_BW_1_62:
+			ctrl1 |= DPLL_CRTL1_LINK_RATE(DPLL_CRTL1_LINK_RATE_810, 0);
+			break;
+		case DP_LINK_BW_2_7:
+			ctrl1 |= DPLL_CRTL1_LINK_RATE(DPLL_CRTL1_LINK_RATE_1350, 0);
+			break;
+		case DP_LINK_BW_5_4:
+			ctrl1 |= DPLL_CRTL1_LINK_RATE(DPLL_CRTL1_LINK_RATE_2700, 0);
+			break;
+		}
+
+		cfgcr1 = cfgcr2 = 0;
+	} else /* eDP */
+		return true;
+
+	intel_crtc->new_config->dpll_hw_state.ctrl1 = ctrl1;
+	intel_crtc->new_config->dpll_hw_state.cfgcr1 = cfgcr1;
+	intel_crtc->new_config->dpll_hw_state.cfgcr2 = cfgcr2;
+
+	pll = intel_get_shared_dpll(intel_crtc);
+	if (pll == NULL) {
+		DRM_DEBUG_DRIVER("failed to find PLL for pipe %c\n",
+				 pipe_name(intel_crtc->pipe));
+		return false;
+	}
+
+	/* shared DPLL id 0 is DPLL 1 */
+	intel_crtc->new_config->ddi_pll_sel = pll->id + 1;
+
+	return true;
+}
+
+/*
+ * Tries to find a *shared* PLL for the CRTC and store it in
+ * intel_crtc->ddi_pll_sel.
+ *
+ * For private DPLLs, compute_config() should do the selection for us. This
+ * function should be folded into compute_config() eventually.
+ */
+bool intel_ddi_pll_select(struct intel_crtc *intel_crtc)
+{
+	struct drm_device *dev = intel_crtc->base.dev;
+	struct intel_encoder *intel_encoder =
+		intel_ddi_get_crtc_new_encoder(intel_crtc);
+	int clock = intel_crtc->new_config->port_clock;
+
+	if (IS_SKYLAKE(dev))
+		return skl_ddi_pll_select(intel_crtc, intel_encoder, clock);
+	else
+		return hsw_ddi_pll_select(intel_crtc, intel_encoder, clock);
+}
+
 void intel_ddi_set_pipe_settings(struct drm_crtc *crtc)
 {
 	struct drm_i915_private *dev_priv = crtc->dev->dev_private;
@@ -921,7 +1348,7 @@
 	uint32_t tmp;
 
 	power_domain = intel_display_port_power_domain(intel_encoder);
-	if (!intel_display_power_enabled(dev_priv, power_domain))
+	if (!intel_display_power_is_enabled(dev_priv, power_domain))
 		return false;
 
 	if (!intel_encoder->get_hw_state(intel_encoder, &pipe))
@@ -967,7 +1394,7 @@
 	int i;
 
 	power_domain = intel_display_port_power_domain(encoder);
-	if (!intel_display_power_enabled(dev_priv, power_domain))
+	if (!intel_display_power_is_enabled(dev_priv, power_domain))
 		return false;
 
 	tmp = I915_READ(DDI_BUF_CTL(port));
@@ -1038,27 +1465,53 @@
 static void intel_ddi_pre_enable(struct intel_encoder *intel_encoder)
 {
 	struct drm_encoder *encoder = &intel_encoder->base;
-	struct drm_i915_private *dev_priv = encoder->dev->dev_private;
+	struct drm_device *dev = encoder->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_crtc *crtc = to_intel_crtc(encoder->crtc);
 	enum port port = intel_ddi_get_encoder_port(intel_encoder);
 	int type = intel_encoder->type;
 
-	if (crtc->config.has_audio) {
-		DRM_DEBUG_DRIVER("Audio on pipe %c on DDI\n",
-				 pipe_name(crtc->pipe));
-
-		/* write eld */
-		DRM_DEBUG_DRIVER("DDI audio: write eld information\n");
-		intel_write_eld(encoder, &crtc->config.adjusted_mode);
-	}
-
 	if (type == INTEL_OUTPUT_EDP) {
 		struct intel_dp *intel_dp = enc_to_intel_dp(encoder);
 		intel_edp_panel_on(intel_dp);
 	}
 
-	WARN_ON(crtc->config.ddi_pll_sel == PORT_CLK_SEL_NONE);
-	I915_WRITE(PORT_CLK_SEL(port), crtc->config.ddi_pll_sel);
+	if (IS_SKYLAKE(dev)) {
+		uint32_t dpll = crtc->config.ddi_pll_sel;
+		uint32_t val;
+
+		/*
+		 * DPLL0 is used for eDP and is the only "private" DPLL (as
+		 * opposed to shared) on SKL
+		 */
+		if (type == INTEL_OUTPUT_EDP) {
+			WARN_ON(dpll != SKL_DPLL0);
+
+			val = I915_READ(DPLL_CTRL1);
+
+			val &= ~(DPLL_CTRL1_HDMI_MODE(dpll) |
+				 DPLL_CTRL1_SSC(dpll) |
+				 DPLL_CRTL1_LINK_RATE_MASK(dpll));
+			val |= crtc->config.dpll_hw_state.ctrl1 << (dpll * 6);
+
+			I915_WRITE(DPLL_CTRL1, val);
+			POSTING_READ(DPLL_CTRL1);
+		}
+
+		/* DDI -> PLL mapping  */
+		val = I915_READ(DPLL_CTRL2);
+
+		val &= ~(DPLL_CTRL2_DDI_CLK_OFF(port) |
+			DPLL_CTRL2_DDI_CLK_SEL_MASK(port));
+		val |= (DPLL_CTRL2_DDI_CLK_SEL(dpll, port) |
+			DPLL_CTRL2_DDI_SEL_OVERRIDE(port));
+
+		I915_WRITE(DPLL_CTRL2, val);
+
+	} else {
+		WARN_ON(crtc->config.ddi_pll_sel == PORT_CLK_SEL_NONE);
+		I915_WRITE(PORT_CLK_SEL(port), crtc->config.ddi_pll_sel);
+	}
 
 	if (type == INTEL_OUTPUT_DISPLAYPORT || type == INTEL_OUTPUT_EDP) {
 		struct intel_dp *intel_dp = enc_to_intel_dp(encoder);
@@ -1068,7 +1521,7 @@
 		intel_dp_sink_dpms(intel_dp, DRM_MODE_DPMS_ON);
 		intel_dp_start_link_train(intel_dp);
 		intel_dp_complete_link_train(intel_dp);
-		if (port != PORT_A)
+		if (port != PORT_A || INTEL_INFO(dev)->gen >= 9)
 			intel_dp_stop_link_train(intel_dp);
 	} else if (type == INTEL_OUTPUT_HDMI) {
 		struct intel_hdmi *intel_hdmi = enc_to_intel_hdmi(encoder);
@@ -1082,7 +1535,8 @@
 static void intel_ddi_post_disable(struct intel_encoder *intel_encoder)
 {
 	struct drm_encoder *encoder = &intel_encoder->base;
-	struct drm_i915_private *dev_priv = encoder->dev->dev_private;
+	struct drm_device *dev = encoder->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
 	enum port port = intel_ddi_get_encoder_port(intel_encoder);
 	int type = intel_encoder->type;
 	uint32_t val;
@@ -1110,7 +1564,11 @@
 		intel_edp_panel_off(intel_dp);
 	}
 
-	I915_WRITE(PORT_CLK_SEL(port), PORT_CLK_SEL_NONE);
+	if (IS_SKYLAKE(dev))
+		I915_WRITE(DPLL_CTRL2, (I915_READ(DPLL_CTRL2) |
+					DPLL_CTRL2_DDI_CLK_OFF(port)));
+	else
+		I915_WRITE(PORT_CLK_SEL(port), PORT_CLK_SEL_NONE);
 }
 
 static void intel_enable_ddi(struct intel_encoder *intel_encoder)
@@ -1118,12 +1576,10 @@
 	struct drm_encoder *encoder = &intel_encoder->base;
 	struct drm_crtc *crtc = encoder->crtc;
 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
-	int pipe = intel_crtc->pipe;
 	struct drm_device *dev = encoder->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	enum port port = intel_ddi_get_encoder_port(intel_encoder);
 	int type = intel_encoder->type;
-	uint32_t tmp;
 
 	if (type == INTEL_OUTPUT_HDMI) {
 		struct intel_digital_port *intel_dig_port =
@@ -1139,18 +1595,16 @@
 	} else if (type == INTEL_OUTPUT_EDP) {
 		struct intel_dp *intel_dp = enc_to_intel_dp(encoder);
 
-		if (port == PORT_A)
+		if (port == PORT_A && INTEL_INFO(dev)->gen < 9)
 			intel_dp_stop_link_train(intel_dp);
 
 		intel_edp_backlight_on(intel_dp);
-		intel_edp_psr_enable(intel_dp);
+		intel_psr_enable(intel_dp);
 	}
 
 	if (intel_crtc->config.has_audio) {
 		intel_display_power_get(dev_priv, POWER_DOMAIN_AUDIO);
-		tmp = I915_READ(HSW_AUD_PIN_ELD_CP_VLD);
-		tmp |= ((AUDIO_OUTPUT_ENABLE_A | AUDIO_ELD_VALID_A) << (pipe * 4));
-		I915_WRITE(HSW_AUD_PIN_ELD_CP_VLD, tmp);
+		intel_audio_codec_enable(intel_encoder);
 	}
 }
 
@@ -1159,61 +1613,126 @@
 	struct drm_encoder *encoder = &intel_encoder->base;
 	struct drm_crtc *crtc = encoder->crtc;
 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
-	int pipe = intel_crtc->pipe;
 	int type = intel_encoder->type;
 	struct drm_device *dev = encoder->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	uint32_t tmp;
 
-	/* We can't touch HSW_AUD_PIN_ELD_CP_VLD uncionditionally because this
-	 * register is part of the power well on Haswell. */
 	if (intel_crtc->config.has_audio) {
-		tmp = I915_READ(HSW_AUD_PIN_ELD_CP_VLD);
-		tmp &= ~((AUDIO_OUTPUT_ENABLE_A | AUDIO_ELD_VALID_A) <<
-			 (pipe * 4));
-		I915_WRITE(HSW_AUD_PIN_ELD_CP_VLD, tmp);
+		intel_audio_codec_disable(intel_encoder);
 		intel_display_power_put(dev_priv, POWER_DOMAIN_AUDIO);
 	}
 
 	if (type == INTEL_OUTPUT_EDP) {
 		struct intel_dp *intel_dp = enc_to_intel_dp(encoder);
 
-		intel_edp_psr_disable(intel_dp);
+		intel_psr_disable(intel_dp);
 		intel_edp_backlight_off(intel_dp);
 	}
 }
 
-int intel_ddi_get_cdclk_freq(struct drm_i915_private *dev_priv)
+static int skl_get_cdclk_freq(struct drm_i915_private *dev_priv)
+{
+	uint32_t lcpll1 = I915_READ(LCPLL1_CTL);
+	uint32_t cdctl = I915_READ(CDCLK_CTL);
+	uint32_t linkrate;
+
+	if (!(lcpll1 & LCPLL_PLL_ENABLE)) {
+		WARN(1, "LCPLL1 not enabled\n");
+		return 24000; /* 24MHz is the cd freq with NSSC ref */
+	}
+
+	if ((cdctl & CDCLK_FREQ_SEL_MASK) == CDCLK_FREQ_540)
+		return 540000;
+
+	linkrate = (I915_READ(DPLL_CTRL1) &
+		    DPLL_CRTL1_LINK_RATE_MASK(SKL_DPLL0)) >> 1;
+
+	if (linkrate == DPLL_CRTL1_LINK_RATE_2160 ||
+	    linkrate == DPLL_CRTL1_LINK_RATE_1080) {
+		/* vco 8640 */
+		switch (cdctl & CDCLK_FREQ_SEL_MASK) {
+		case CDCLK_FREQ_450_432:
+			return 432000;
+		case CDCLK_FREQ_337_308:
+			return 308570;
+		case CDCLK_FREQ_675_617:
+			return 617140;
+		default:
+			WARN(1, "Unknown cd freq selection\n");
+		}
+	} else {
+		/* vco 8100 */
+		switch (cdctl & CDCLK_FREQ_SEL_MASK) {
+		case CDCLK_FREQ_450_432:
+			return 450000;
+		case CDCLK_FREQ_337_308:
+			return 337500;
+		case CDCLK_FREQ_675_617:
+			return 675000;
+		default:
+			WARN(1, "Unknown cd freq selection\n");
+		}
+	}
+
+	/* error case, do as if DPLL0 isn't enabled */
+	return 24000;
+}
+
+static int bdw_get_cdclk_freq(struct drm_i915_private *dev_priv)
+{
+	uint32_t lcpll = I915_READ(LCPLL_CTL);
+	uint32_t freq = lcpll & LCPLL_CLK_FREQ_MASK;
+
+	if (lcpll & LCPLL_CD_SOURCE_FCLK)
+		return 800000;
+	else if (I915_READ(FUSE_STRAP) & HSW_CDCLK_LIMIT)
+		return 450000;
+	else if (freq == LCPLL_CLK_FREQ_450)
+		return 450000;
+	else if (freq == LCPLL_CLK_FREQ_54O_BDW)
+		return 540000;
+	else if (freq == LCPLL_CLK_FREQ_337_5_BDW)
+		return 337500;
+	else
+		return 675000;
+}
+
+static int hsw_get_cdclk_freq(struct drm_i915_private *dev_priv)
 {
 	struct drm_device *dev = dev_priv->dev;
 	uint32_t lcpll = I915_READ(LCPLL_CTL);
 	uint32_t freq = lcpll & LCPLL_CLK_FREQ_MASK;
 
-	if (lcpll & LCPLL_CD_SOURCE_FCLK) {
+	if (lcpll & LCPLL_CD_SOURCE_FCLK)
 		return 800000;
-	} else if (I915_READ(FUSE_STRAP) & HSW_CDCLK_LIMIT) {
+	else if (I915_READ(FUSE_STRAP) & HSW_CDCLK_LIMIT)
 		return 450000;
-	} else if (freq == LCPLL_CLK_FREQ_450) {
+	else if (freq == LCPLL_CLK_FREQ_450)
 		return 450000;
-	} else if (IS_HASWELL(dev)) {
-		if (IS_ULT(dev))
-			return 337500;
-		else
-			return 540000;
-	} else {
-		if (freq == LCPLL_CLK_FREQ_54O_BDW)
-			return 540000;
-		else if (freq == LCPLL_CLK_FREQ_337_5_BDW)
-			return 337500;
-		else
-			return 675000;
-	}
+	else if (IS_HSW_ULT(dev))
+		return 337500;
+	else
+		return 540000;
+}
+
+int intel_ddi_get_cdclk_freq(struct drm_i915_private *dev_priv)
+{
+	struct drm_device *dev = dev_priv->dev;
+
+	if (IS_SKYLAKE(dev))
+		return skl_get_cdclk_freq(dev_priv);
+
+	if (IS_BROADWELL(dev))
+		return bdw_get_cdclk_freq(dev_priv);
+
+	/* Haswell */
+	return hsw_get_cdclk_freq(dev_priv);
 }
 
 static void hsw_ddi_pll_enable(struct drm_i915_private *dev_priv,
 			       struct intel_shared_dpll *pll)
 {
-	I915_WRITE(WRPLL_CTL(pll->id), pll->hw_state.wrpll);
+	I915_WRITE(WRPLL_CTL(pll->id), pll->config.hw_state.wrpll);
 	POSTING_READ(WRPLL_CTL(pll->id));
 	udelay(20);
 }
@@ -1234,7 +1753,7 @@
 {
 	uint32_t val;
 
-	if (!intel_display_power_enabled(dev_priv, POWER_DOMAIN_PLLS))
+	if (!intel_display_power_is_enabled(dev_priv, POWER_DOMAIN_PLLS))
 		return false;
 
 	val = I915_READ(WRPLL_CTL(pll->id));
@@ -1248,10 +1767,8 @@
 	"WRPLL 2",
 };
 
-void intel_ddi_pll_init(struct drm_device *dev)
+static void hsw_shared_dplls_init(struct drm_i915_private *dev_priv)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	uint32_t val = I915_READ(LCPLL_CTL);
 	int i;
 
 	dev_priv->num_shared_dpll = 2;
@@ -1264,20 +1781,158 @@
 		dev_priv->shared_dplls[i].get_hw_state =
 			hsw_ddi_pll_get_hw_state;
 	}
+}
 
-	/* The LCPLL register should be turned on by the BIOS. For now let's
-	 * just check its state and print errors in case something is wrong.
-	 * Don't even try to turn it on.
-	 */
+static const char * const skl_ddi_pll_names[] = {
+	"DPLL 1",
+	"DPLL 2",
+	"DPLL 3",
+};
+
+struct skl_dpll_regs {
+	u32 ctl, cfgcr1, cfgcr2;
+};
+
+/* this array is indexed by the *shared* pll id */
+static const struct skl_dpll_regs skl_dpll_regs[3] = {
+	{
+		/* DPLL 1 */
+		.ctl = LCPLL2_CTL,
+		.cfgcr1 = DPLL1_CFGCR1,
+		.cfgcr2 = DPLL1_CFGCR2,
+	},
+	{
+		/* DPLL 2 */
+		.ctl = WRPLL_CTL1,
+		.cfgcr1 = DPLL2_CFGCR1,
+		.cfgcr2 = DPLL2_CFGCR2,
+	},
+	{
+		/* DPLL 3 */
+		.ctl = WRPLL_CTL2,
+		.cfgcr1 = DPLL3_CFGCR1,
+		.cfgcr2 = DPLL3_CFGCR2,
+	},
+};
+
+static void skl_ddi_pll_enable(struct drm_i915_private *dev_priv,
+			       struct intel_shared_dpll *pll)
+{
+	uint32_t val;
+	unsigned int dpll;
+	const struct skl_dpll_regs *regs = skl_dpll_regs;
+
+	/* DPLL0 is not part of the shared DPLLs, so pll->id is 0 for DPLL1 */
+	dpll = pll->id + 1;
+
+	val = I915_READ(DPLL_CTRL1);
+
+	val &= ~(DPLL_CTRL1_HDMI_MODE(dpll) | DPLL_CTRL1_SSC(dpll) |
+		 DPLL_CRTL1_LINK_RATE_MASK(dpll));
+	val |= pll->config.hw_state.ctrl1 << (dpll * 6);
+
+	I915_WRITE(DPLL_CTRL1, val);
+	POSTING_READ(DPLL_CTRL1);
+
+	I915_WRITE(regs[pll->id].cfgcr1, pll->config.hw_state.cfgcr1);
+	I915_WRITE(regs[pll->id].cfgcr2, pll->config.hw_state.cfgcr2);
+	POSTING_READ(regs[pll->id].cfgcr1);
+	POSTING_READ(regs[pll->id].cfgcr2);
+
+	/* the enable bit is always bit 31 */
+	I915_WRITE(regs[pll->id].ctl,
+		   I915_READ(regs[pll->id].ctl) | LCPLL_PLL_ENABLE);
+
+	if (wait_for(I915_READ(DPLL_STATUS) & DPLL_LOCK(dpll), 5))
+		DRM_ERROR("DPLL %d not locked\n", dpll);
+}
+
+static void skl_ddi_pll_disable(struct drm_i915_private *dev_priv,
+				struct intel_shared_dpll *pll)
+{
+	const struct skl_dpll_regs *regs = skl_dpll_regs;
+
+	/* the enable bit is always bit 31 */
+	I915_WRITE(regs[pll->id].ctl,
+		   I915_READ(regs[pll->id].ctl) & ~LCPLL_PLL_ENABLE);
+	POSTING_READ(regs[pll->id].ctl);
+}
+
+static bool skl_ddi_pll_get_hw_state(struct drm_i915_private *dev_priv,
+				     struct intel_shared_dpll *pll,
+				     struct intel_dpll_hw_state *hw_state)
+{
+	uint32_t val;
+	unsigned int dpll;
+	const struct skl_dpll_regs *regs = skl_dpll_regs;
+
+	if (!intel_display_power_is_enabled(dev_priv, POWER_DOMAIN_PLLS))
+		return false;
+
+	/* DPLL0 is not part of the shared DPLLs, so pll->id is 0 for DPLL1 */
+	dpll = pll->id + 1;
+
+	val = I915_READ(regs[pll->id].ctl);
+	if (!(val & LCPLL_PLL_ENABLE))
+		return false;
+
+	val = I915_READ(DPLL_CTRL1);
+	hw_state->ctrl1 = (val >> (dpll * 6)) & 0x3f;
+
+	/* avoid reading back stale values if HDMI mode is not enabled */
+	if (val & DPLL_CTRL1_HDMI_MODE(dpll)) {
+		hw_state->cfgcr1 = I915_READ(regs[pll->id].cfgcr1);
+		hw_state->cfgcr2 = I915_READ(regs[pll->id].cfgcr2);
+	}
+
+	return true;
+}
+
+static void skl_shared_dplls_init(struct drm_i915_private *dev_priv)
+{
+	int i;
+
+	dev_priv->num_shared_dpll = 3;
+
+	for (i = 0; i < dev_priv->num_shared_dpll; i++) {
+		dev_priv->shared_dplls[i].id = i;
+		dev_priv->shared_dplls[i].name = skl_ddi_pll_names[i];
+		dev_priv->shared_dplls[i].disable = skl_ddi_pll_disable;
+		dev_priv->shared_dplls[i].enable = skl_ddi_pll_enable;
+		dev_priv->shared_dplls[i].get_hw_state =
+			skl_ddi_pll_get_hw_state;
+	}
+}
+
+void intel_ddi_pll_init(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	uint32_t val = I915_READ(LCPLL_CTL);
+
+	if (IS_SKYLAKE(dev))
+		skl_shared_dplls_init(dev_priv);
+	else
+		hsw_shared_dplls_init(dev_priv);
 
 	DRM_DEBUG_KMS("CDCLK running at %dKHz\n",
 		      intel_ddi_get_cdclk_freq(dev_priv));
 
-	if (val & LCPLL_CD_SOURCE_FCLK)
-		DRM_ERROR("CDCLK source is not LCPLL\n");
+	if (IS_SKYLAKE(dev)) {
+		if (!(I915_READ(LCPLL1_CTL) & LCPLL_PLL_ENABLE))
+			DRM_ERROR("LCPLL1 is disabled\n");
+	} else {
+		/*
+		 * The LCPLL register should be turned on by the BIOS. For now
+		 * let's just check its state and print errors in case
+		 * something is wrong.  Don't even try to turn it on.
+		 */
+
+		if (val & LCPLL_CD_SOURCE_FCLK)
+			DRM_ERROR("CDCLK source is not LCPLL\n");
 
-	if (val & LCPLL_PLL_DISABLE)
-		DRM_ERROR("LCPLL is disabled\n");
+		if (val & LCPLL_PLL_DISABLE)
+			DRM_ERROR("LCPLL is disabled\n");
+	}
 }
 
 void intel_ddi_prepare_link_retrain(struct drm_encoder *encoder)
@@ -1373,6 +2028,7 @@
 	struct intel_crtc *intel_crtc = to_intel_crtc(encoder->base.crtc);
 	enum transcoder cpu_transcoder = intel_crtc->config.cpu_transcoder;
 	u32 temp, flags = 0;
+	struct drm_device *dev = dev_priv->dev;
 
 	temp = I915_READ(TRANS_DDI_FUNC_CTL(cpu_transcoder));
 	if (temp & TRANS_DDI_PHSYNC)
@@ -1406,6 +2062,7 @@
 	switch (temp & TRANS_DDI_MODE_SELECT_MASK) {
 	case TRANS_DDI_MODE_SELECT_HDMI:
 		pipe_config->has_hdmi_sink = true;
+		break;
 	case TRANS_DDI_MODE_SELECT_DVI:
 	case TRANS_DDI_MODE_SELECT_FDI:
 		break;
@@ -1418,9 +2075,17 @@
 		break;
 	}
 
-	if (intel_display_power_enabled(dev_priv, POWER_DOMAIN_AUDIO)) {
+	if (encoder->type == INTEL_OUTPUT_HDMI) {
+		struct intel_hdmi *intel_hdmi =
+			enc_to_intel_hdmi(&encoder->base);
+
+		if (intel_hdmi->infoframe_enabled(&encoder->base))
+			pipe_config->has_infoframe = true;
+	}
+
+	if (intel_display_power_is_enabled(dev_priv, POWER_DOMAIN_AUDIO)) {
 		temp = I915_READ(HSW_AUD_PIN_ELD_CP_VLD);
-		if (temp & (AUDIO_OUTPUT_ENABLE_A << (intel_crtc->pipe * 4)))
+		if (temp & AUDIO_OUTPUT_ENABLE(intel_crtc->pipe))
 			pipe_config->has_audio = true;
 	}
 
@@ -1444,7 +2109,10 @@
 		dev_priv->vbt.edp_bpp = pipe_config->pipe_bpp;
 	}
 
-	intel_ddi_clock_get(encoder, pipe_config);
+	if (INTEL_INFO(dev)->gen <= 8)
+		hsw_ddi_clock_get(encoder, pipe_config);
+	else
+		skl_ddi_clock_get(encoder, pipe_config);
 }
 
 static void intel_ddi_destroy(struct drm_encoder *encoder)
diff -urN a/drivers/gpu/drm/i915/intel_display.c b/drivers/gpu/drm/i915/intel_display.c
--- a/drivers/gpu/drm/i915/intel_display.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_display.c	2014-11-22 14:37:49.338700418 -0700
@@ -73,11 +73,6 @@
 	DRM_FORMAT_ARGB8888,
 };
 
-#define DIV_ROUND_CLOSEST_ULL(ll, d)	\
-({ unsigned long long _tmp = (ll)+(d)/2; do_div(_tmp, d); _tmp; })
-
-static void intel_increase_pllclock(struct drm_device *dev,
-				    enum pipe pipe);
 static void intel_crtc_update_cursor(struct drm_crtc *crtc, bool on);
 
 static void i9xx_crtc_clock_get(struct intel_crtc *crtc,
@@ -91,15 +86,18 @@
 				  struct intel_framebuffer *ifb,
 				  struct drm_mode_fb_cmd2 *mode_cmd,
 				  struct drm_i915_gem_object *obj);
-static void intel_dp_set_m_n(struct intel_crtc *crtc);
 static void i9xx_set_pipeconf(struct intel_crtc *intel_crtc);
 static void intel_set_pipe_timings(struct intel_crtc *intel_crtc);
 static void intel_cpu_transcoder_set_m_n(struct intel_crtc *crtc,
-					 struct intel_link_m_n *m_n);
+					 struct intel_link_m_n *m_n,
+					 struct intel_link_m_n *m2_n2);
 static void ironlake_set_pipeconf(struct drm_crtc *crtc);
 static void haswell_set_pipeconf(struct drm_crtc *crtc);
 static void intel_set_pipe_csc(struct drm_crtc *crtc);
-static void vlv_prepare_pll(struct intel_crtc *crtc);
+static void vlv_prepare_pll(struct intel_crtc *crtc,
+			    const struct intel_crtc_config *pipe_config);
+static void chv_prepare_pll(struct intel_crtc *crtc,
+			    const struct intel_crtc_config *pipe_config);
 
 static struct intel_encoder *intel_find_encoder(struct intel_connector *connector, int pipe)
 {
@@ -410,25 +408,43 @@
 /**
  * Returns whether any output on the specified pipe is of the specified type
  */
-static bool intel_pipe_has_type(struct drm_crtc *crtc, int type)
+bool intel_pipe_has_type(struct intel_crtc *crtc, enum intel_output_type type)
 {
-	struct drm_device *dev = crtc->dev;
+	struct drm_device *dev = crtc->base.dev;
 	struct intel_encoder *encoder;
 
-	for_each_encoder_on_crtc(dev, crtc, encoder)
+	for_each_encoder_on_crtc(dev, &crtc->base, encoder)
 		if (encoder->type == type)
 			return true;
 
 	return false;
 }
 
-static const intel_limit_t *intel_ironlake_limit(struct drm_crtc *crtc,
+/**
+ * Returns whether any output on the specified pipe will have the specified
+ * type after a staged modeset is complete, i.e., the same as
+ * intel_pipe_has_type() but looking at encoder->new_crtc instead of
+ * encoder->crtc.
+ */
+static bool intel_pipe_will_have_type(struct intel_crtc *crtc, int type)
+{
+	struct drm_device *dev = crtc->base.dev;
+	struct intel_encoder *encoder;
+
+	for_each_intel_encoder(dev, encoder)
+		if (encoder->new_crtc == crtc && encoder->type == type)
+			return true;
+
+	return false;
+}
+
+static const intel_limit_t *intel_ironlake_limit(struct intel_crtc *crtc,
 						int refclk)
 {
-	struct drm_device *dev = crtc->dev;
+	struct drm_device *dev = crtc->base.dev;
 	const intel_limit_t *limit;
 
-	if (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS)) {
+	if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS)) {
 		if (intel_is_dual_link_lvds(dev)) {
 			if (refclk == 100000)
 				limit = &intel_limits_ironlake_dual_lvds_100m;
@@ -446,20 +462,20 @@
 	return limit;
 }
 
-static const intel_limit_t *intel_g4x_limit(struct drm_crtc *crtc)
+static const intel_limit_t *intel_g4x_limit(struct intel_crtc *crtc)
 {
-	struct drm_device *dev = crtc->dev;
+	struct drm_device *dev = crtc->base.dev;
 	const intel_limit_t *limit;
 
-	if (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS)) {
+	if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS)) {
 		if (intel_is_dual_link_lvds(dev))
 			limit = &intel_limits_g4x_dual_channel_lvds;
 		else
 			limit = &intel_limits_g4x_single_channel_lvds;
-	} else if (intel_pipe_has_type(crtc, INTEL_OUTPUT_HDMI) ||
-		   intel_pipe_has_type(crtc, INTEL_OUTPUT_ANALOG)) {
+	} else if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_HDMI) ||
+		   intel_pipe_will_have_type(crtc, INTEL_OUTPUT_ANALOG)) {
 		limit = &intel_limits_g4x_hdmi;
-	} else if (intel_pipe_has_type(crtc, INTEL_OUTPUT_SDVO)) {
+	} else if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_SDVO)) {
 		limit = &intel_limits_g4x_sdvo;
 	} else /* The option is for other outputs */
 		limit = &intel_limits_i9xx_sdvo;
@@ -467,9 +483,9 @@
 	return limit;
 }
 
-static const intel_limit_t *intel_limit(struct drm_crtc *crtc, int refclk)
+static const intel_limit_t *intel_limit(struct intel_crtc *crtc, int refclk)
 {
-	struct drm_device *dev = crtc->dev;
+	struct drm_device *dev = crtc->base.dev;
 	const intel_limit_t *limit;
 
 	if (HAS_PCH_SPLIT(dev))
@@ -477,7 +493,7 @@
 	else if (IS_G4X(dev)) {
 		limit = intel_g4x_limit(crtc);
 	} else if (IS_PINEVIEW(dev)) {
-		if (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS))
+		if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS))
 			limit = &intel_limits_pineview_lvds;
 		else
 			limit = &intel_limits_pineview_sdvo;
@@ -486,14 +502,14 @@
 	} else if (IS_VALLEYVIEW(dev)) {
 		limit = &intel_limits_vlv;
 	} else if (!IS_GEN2(dev)) {
-		if (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS))
+		if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS))
 			limit = &intel_limits_i9xx_lvds;
 		else
 			limit = &intel_limits_i9xx_sdvo;
 	} else {
-		if (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS))
+		if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS))
 			limit = &intel_limits_i8xx_lvds;
-		else if (intel_pipe_has_type(crtc, INTEL_OUTPUT_DVO))
+		else if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_DVO))
 			limit = &intel_limits_i8xx_dvo;
 		else
 			limit = &intel_limits_i8xx_dac;
@@ -580,15 +596,15 @@
 }
 
 static bool
-i9xx_find_best_dpll(const intel_limit_t *limit, struct drm_crtc *crtc,
+i9xx_find_best_dpll(const intel_limit_t *limit, struct intel_crtc *crtc,
 		    int target, int refclk, intel_clock_t *match_clock,
 		    intel_clock_t *best_clock)
 {
-	struct drm_device *dev = crtc->dev;
+	struct drm_device *dev = crtc->base.dev;
 	intel_clock_t clock;
 	int err = target;
 
-	if (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS)) {
+	if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS)) {
 		/*
 		 * For LVDS just rely on its current settings for dual-channel.
 		 * We haven't figured out how to reliably set up different
@@ -641,15 +657,15 @@
 }
 
 static bool
-pnv_find_best_dpll(const intel_limit_t *limit, struct drm_crtc *crtc,
+pnv_find_best_dpll(const intel_limit_t *limit, struct intel_crtc *crtc,
 		   int target, int refclk, intel_clock_t *match_clock,
 		   intel_clock_t *best_clock)
 {
-	struct drm_device *dev = crtc->dev;
+	struct drm_device *dev = crtc->base.dev;
 	intel_clock_t clock;
 	int err = target;
 
-	if (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS)) {
+	if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS)) {
 		/*
 		 * For LVDS just rely on its current settings for dual-channel.
 		 * We haven't figured out how to reliably set up different
@@ -700,11 +716,11 @@
 }
 
 static bool
-g4x_find_best_dpll(const intel_limit_t *limit, struct drm_crtc *crtc,
+g4x_find_best_dpll(const intel_limit_t *limit, struct intel_crtc *crtc,
 		   int target, int refclk, intel_clock_t *match_clock,
 		   intel_clock_t *best_clock)
 {
-	struct drm_device *dev = crtc->dev;
+	struct drm_device *dev = crtc->base.dev;
 	intel_clock_t clock;
 	int max_n;
 	bool found;
@@ -712,7 +728,7 @@
 	int err_most = (target >> 8) + (target >> 9);
 	found = false;
 
-	if (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS)) {
+	if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS)) {
 		if (intel_is_dual_link_lvds(dev))
 			clock.p2 = limit->p2.p2_fast;
 		else
@@ -757,11 +773,11 @@
 }
 
 static bool
-vlv_find_best_dpll(const intel_limit_t *limit, struct drm_crtc *crtc,
+vlv_find_best_dpll(const intel_limit_t *limit, struct intel_crtc *crtc,
 		   int target, int refclk, intel_clock_t *match_clock,
 		   intel_clock_t *best_clock)
 {
-	struct drm_device *dev = crtc->dev;
+	struct drm_device *dev = crtc->base.dev;
 	intel_clock_t clock;
 	unsigned int bestppm = 1000000;
 	/* min update 19.2 MHz */
@@ -814,11 +830,11 @@
 }
 
 static bool
-chv_find_best_dpll(const intel_limit_t *limit, struct drm_crtc *crtc,
+chv_find_best_dpll(const intel_limit_t *limit, struct intel_crtc *crtc,
 		   int target, int refclk, intel_clock_t *match_clock,
 		   intel_clock_t *best_clock)
 {
-	struct drm_device *dev = crtc->dev;
+	struct drm_device *dev = crtc->base.dev;
 	intel_clock_t clock;
 	uint64_t m2;
 	int found = false;
@@ -891,58 +907,6 @@
 	return intel_crtc->config.cpu_transcoder;
 }
 
-static void g4x_wait_for_vblank(struct drm_device *dev, int pipe)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	u32 frame, frame_reg = PIPE_FRMCOUNT_GM45(pipe);
-
-	frame = I915_READ(frame_reg);
-
-	if (wait_for(I915_READ_NOTRACE(frame_reg) != frame, 50))
-		WARN(1, "vblank wait timed out\n");
-}
-
-/**
- * intel_wait_for_vblank - wait for vblank on a given pipe
- * @dev: drm device
- * @pipe: pipe to wait for
- *
- * Wait for vblank to occur on a given pipe.  Needed for various bits of
- * mode setting code.
- */
-void intel_wait_for_vblank(struct drm_device *dev, int pipe)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	int pipestat_reg = PIPESTAT(pipe);
-
-	if (IS_G4X(dev) || INTEL_INFO(dev)->gen >= 5) {
-		g4x_wait_for_vblank(dev, pipe);
-		return;
-	}
-
-	/* Clear existing vblank status. Note this will clear any other
-	 * sticky status fields as well.
-	 *
-	 * This races with i915_driver_irq_handler() with the result
-	 * that either function could miss a vblank event.  Here it is not
-	 * fatal, as we will either wait upon the next vblank interrupt or
-	 * timeout.  Generally speaking intel_wait_for_vblank() is only
-	 * called during modeset at which time the GPU should be idle and
-	 * should *not* be performing page flips and thus not waiting on
-	 * vblanks...
-	 * Currently, the result of us stealing a vblank from the irq
-	 * handler is that a single frame will be skipped during swapbuffers.
-	 */
-	I915_WRITE(pipestat_reg,
-		   I915_READ(pipestat_reg) | PIPE_VBLANK_INTERRUPT_STATUS);
-
-	/* Wait for vblank interrupt bit to set */
-	if (wait_for(I915_READ(pipestat_reg) &
-		     PIPE_VBLANK_INTERRUPT_STATUS,
-		     50))
-		DRM_DEBUG_KMS("vblank wait timed out\n");
-}
-
 static bool pipe_dsl_stopped(struct drm_device *dev, enum pipe pipe)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -964,8 +928,7 @@
 
 /*
  * intel_wait_for_pipe_off - wait for pipe to turn off
- * @dev: drm device
- * @pipe: pipe to wait for
+ * @crtc: crtc whose pipe to wait for
  *
  * After disabling a pipe, we can't wait for vblank in the usual way,
  * spinning on the vblank interrupt status bit, since we won't actually
@@ -979,11 +942,12 @@
  *   ends up stopping at the start of the next frame).
  *
  */
-void intel_wait_for_pipe_off(struct drm_device *dev, int pipe)
+static void intel_wait_for_pipe_off(struct intel_crtc *crtc)
 {
+	struct drm_device *dev = crtc->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	enum transcoder cpu_transcoder = intel_pipe_to_cpu_transcoder(dev_priv,
-								      pipe);
+	enum transcoder cpu_transcoder = crtc->config.cpu_transcoder;
+	enum pipe pipe = crtc->pipe;
 
 	if (INTEL_INFO(dev)->gen >= 4) {
 		int reg = PIPECONF(cpu_transcoder);
@@ -1189,30 +1153,43 @@
 	     state_string(state), state_string(cur_state));
 }
 
-static void assert_panel_unlocked(struct drm_i915_private *dev_priv,
-				  enum pipe pipe)
+void assert_panel_unlocked(struct drm_i915_private *dev_priv,
+			   enum pipe pipe)
 {
-	int pp_reg, lvds_reg;
+	struct drm_device *dev = dev_priv->dev;
+	int pp_reg;
 	u32 val;
 	enum pipe panel_pipe = PIPE_A;
 	bool locked = true;
 
-	if (HAS_PCH_SPLIT(dev_priv->dev)) {
+	if (WARN_ON(HAS_DDI(dev)))
+		return;
+
+	if (HAS_PCH_SPLIT(dev)) {
+		u32 port_sel;
+
 		pp_reg = PCH_PP_CONTROL;
-		lvds_reg = PCH_LVDS;
+		port_sel = I915_READ(PCH_PP_ON_DELAYS) & PANEL_PORT_SELECT_MASK;
+
+		if (port_sel == PANEL_PORT_SELECT_LVDS &&
+		    I915_READ(PCH_LVDS) & LVDS_PIPEB_SELECT)
+			panel_pipe = PIPE_B;
+		/* XXX: else fix for eDP */
+	} else if (IS_VALLEYVIEW(dev)) {
+		/* presumably write lock depends on pipe, not port select */
+		pp_reg = VLV_PIPE_PP_CONTROL(pipe);
+		panel_pipe = pipe;
 	} else {
 		pp_reg = PP_CONTROL;
-		lvds_reg = LVDS;
+		if (I915_READ(LVDS) & LVDS_PIPEB_SELECT)
+			panel_pipe = PIPE_B;
 	}
 
 	val = I915_READ(pp_reg);
 	if (!(val & PANEL_POWER_ON) ||
-	    ((val & PANEL_UNLOCK_REGS) == PANEL_UNLOCK_REGS))
+	    ((val & PANEL_UNLOCK_MASK) == PANEL_UNLOCK_REGS))
 		locked = false;
 
-	if (I915_READ(lvds_reg) & LVDS_PIPEB_SELECT)
-		panel_pipe = PIPE_B;
-
 	WARN(panel_pipe == pipe && locked,
 	     "panel assertion failure, pipe %c regs locked\n",
 	     pipe_name(pipe));
@@ -1236,7 +1213,7 @@
 #define assert_cursor_enabled(d, p) assert_cursor(d, p, true)
 #define assert_cursor_disabled(d, p) assert_cursor(d, p, false)
 
-void assert_pipe(struct drm_i915_private *dev_priv,
+bool assert_pipe(struct drm_i915_private *dev_priv,
 		 enum pipe pipe, bool state)
 {
 	int reg;
@@ -1245,11 +1222,12 @@
 	enum transcoder cpu_transcoder = intel_pipe_to_cpu_transcoder(dev_priv,
 								      pipe);
 
-	/* if we need the pipe A quirk it must be always on */
-	if (pipe == PIPE_A && dev_priv->quirks & QUIRK_PIPEA_FORCE)
+	/* if we need the pipe quirk it must be always on */
+	if ((pipe == PIPE_A && dev_priv->quirks & QUIRK_PIPEA_FORCE) ||
+	    (pipe == PIPE_B && dev_priv->quirks & QUIRK_PIPEB_FORCE))
 		state = true;
 
-	if (!intel_display_power_enabled(dev_priv,
+	if (!intel_display_power_is_enabled(dev_priv,
 				POWER_DOMAIN_TRANSCODER(cpu_transcoder))) {
 		cur_state = false;
 	} else {
@@ -1258,12 +1236,12 @@
 		cur_state = !!(val & PIPECONF_ENABLE);
 	}
 
-	WARN(cur_state != state,
-	     "pipe %c assertion failure (expected %s, current %s)\n",
-	     pipe_name(pipe), state_string(state), state_string(cur_state));
+	return !WARN(cur_state != state,
+		     "pipe %c assertion failure (expected %s, current %s)\n",
+		     pipe_name(pipe), state_string(state), state_string(cur_state));
 }
 
-static void assert_plane(struct drm_i915_private *dev_priv,
+static bool assert_plane(struct drm_i915_private *dev_priv,
 			 enum plane plane, bool state)
 {
 	int reg;
@@ -1273,9 +1251,9 @@
 	reg = DSPCNTR(plane);
 	val = I915_READ(reg);
 	cur_state = !!(val & DISPLAY_PLANE_ENABLE);
-	WARN(cur_state != state,
-	     "plane %c assertion failure (expected %s, current %s)\n",
-	     plane_name(plane), state_string(state), state_string(cur_state));
+	return !WARN(cur_state != state,
+		     "plane %c assertion failure (expected %s, current %s)\n",
+		     plane_name(plane), state_string(state), state_string(cur_state));
 }
 
 #define assert_plane_enabled(d, p) assert_plane(d, p, true)
@@ -1300,7 +1278,7 @@
 	}
 
 	/* Need to check both planes against the pipe */
-	for_each_pipe(i) {
+	for_each_pipe(dev_priv, i) {
 		reg = DSPCNTR(i);
 		val = I915_READ(reg);
 		cur_pipe = (val & DISPPLANE_SEL_PIPE_MASK) >>
@@ -1318,7 +1296,14 @@
 	int reg, sprite;
 	u32 val;
 
-	if (IS_VALLEYVIEW(dev)) {
+	if (INTEL_INFO(dev)->gen >= 9) {
+		for_each_sprite(pipe, sprite) {
+			val = I915_READ(PLANE_CTL(pipe, sprite));
+			WARN(val & PLANE_CTL_ENABLE,
+			     "plane %d assertion failure, should be off on pipe %c but is still active\n",
+			     sprite, pipe_name(pipe));
+		}
+	} else if (IS_VALLEYVIEW(dev)) {
 		for_each_sprite(pipe, sprite) {
 			reg = SPCNTR(pipe, sprite);
 			val = I915_READ(reg);
@@ -1341,6 +1326,12 @@
 	}
 }
 
+static void assert_vblank_disabled(struct drm_crtc *crtc)
+{
+	if (WARN_ON(drm_crtc_vblank_get(crtc) == 0))
+		drm_crtc_vblank_put(crtc);
+}
+
 static void ibx_assert_pch_refclk_enabled(struct drm_i915_private *dev_priv)
 {
 	u32 val;
@@ -1513,40 +1504,13 @@
 	}
 }
 
-static void intel_reset_dpio(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-
-	if (IS_CHERRYVIEW(dev)) {
-		enum dpio_phy phy;
-		u32 val;
-
-		for (phy = DPIO_PHY0; phy < I915_NUM_PHYS_VLV; phy++) {
-			/* Poll for phypwrgood signal */
-			if (wait_for(I915_READ(DISPLAY_PHY_STATUS) &
-						PHY_POWERGOOD(phy), 1))
-				DRM_ERROR("Display PHY %d is not power up\n", phy);
-
-			/*
-			 * Deassert common lane reset for PHY.
-			 *
-			 * This should only be done on init and resume from S3
-			 * with both PLLs disabled, or we risk losing DPIO and
-			 * PLL synchronization.
-			 */
-			val = I915_READ(DISPLAY_PHY_CONTROL);
-			I915_WRITE(DISPLAY_PHY_CONTROL,
-				PHY_COM_LANE_RESET_DEASSERT(phy, val));
-		}
-	}
-}
-
-static void vlv_enable_pll(struct intel_crtc *crtc)
+static void vlv_enable_pll(struct intel_crtc *crtc,
+			   const struct intel_crtc_config *pipe_config)
 {
 	struct drm_device *dev = crtc->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	int reg = DPLL(crtc->pipe);
-	u32 dpll = crtc->config.dpll_hw_state.dpll;
+	u32 dpll = pipe_config->dpll_hw_state.dpll;
 
 	assert_pipe_disabled(dev_priv, crtc->pipe);
 
@@ -1554,7 +1518,7 @@
 	BUG_ON(!IS_VALLEYVIEW(dev_priv->dev));
 
 	/* PLL is protected by panel, make sure we can write it */
-	if (IS_MOBILE(dev_priv->dev) && !IS_I830(dev_priv->dev))
+	if (IS_MOBILE(dev_priv->dev))
 		assert_panel_unlocked(dev_priv, crtc->pipe);
 
 	I915_WRITE(reg, dpll);
@@ -1564,7 +1528,7 @@
 	if (wait_for(((I915_READ(reg) & DPLL_LOCK_VLV) == DPLL_LOCK_VLV), 1))
 		DRM_ERROR("DPLL %d failed to lock\n", crtc->pipe);
 
-	I915_WRITE(DPLL_MD(crtc->pipe), crtc->config.dpll_hw_state.dpll_md);
+	I915_WRITE(DPLL_MD(crtc->pipe), pipe_config->dpll_hw_state.dpll_md);
 	POSTING_READ(DPLL_MD(crtc->pipe));
 
 	/* We do this three times for luck */
@@ -1579,7 +1543,8 @@
 	udelay(150); /* wait for warmup */
 }
 
-static void chv_enable_pll(struct intel_crtc *crtc)
+static void chv_enable_pll(struct intel_crtc *crtc,
+			   const struct intel_crtc_config *pipe_config)
 {
 	struct drm_device *dev = crtc->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -1604,19 +1569,31 @@
 	udelay(1);
 
 	/* Enable PLL */
-	I915_WRITE(DPLL(pipe), crtc->config.dpll_hw_state.dpll);
+	I915_WRITE(DPLL(pipe), pipe_config->dpll_hw_state.dpll);
 
 	/* Check PLL is locked */
 	if (wait_for(((I915_READ(DPLL(pipe)) & DPLL_LOCK_VLV) == DPLL_LOCK_VLV), 1))
 		DRM_ERROR("PLL %d failed to lock\n", pipe);
 
 	/* not sure when this should be written */
-	I915_WRITE(DPLL_MD(pipe), crtc->config.dpll_hw_state.dpll_md);
+	I915_WRITE(DPLL_MD(pipe), pipe_config->dpll_hw_state.dpll_md);
 	POSTING_READ(DPLL_MD(pipe));
 
 	mutex_unlock(&dev_priv->dpio_lock);
 }
 
+static int intel_num_dvo_pipes(struct drm_device *dev)
+{
+	struct intel_crtc *crtc;
+	int count = 0;
+
+	for_each_intel_crtc(dev, crtc)
+		count += crtc->active &&
+			intel_pipe_has_type(crtc, INTEL_OUTPUT_DVO);
+
+	return count;
+}
+
 static void i9xx_enable_pll(struct intel_crtc *crtc)
 {
 	struct drm_device *dev = crtc->base.dev;
@@ -1633,7 +1610,18 @@
 	if (IS_MOBILE(dev) && !IS_I830(dev))
 		assert_panel_unlocked(dev_priv, crtc->pipe);
 
-	I915_WRITE(reg, dpll);
+	/* Enable DVO 2x clock on both PLLs if necessary */
+	if (IS_I830(dev) && intel_num_dvo_pipes(dev) > 0) {
+		/*
+		 * It appears to be important that we don't enable this
+		 * for the current pipe before otherwise configuring the
+		 * PLL. No idea how this should be handled if multiple
+		 * DVO outputs are enabled simultaneosly.
+		 */
+		dpll |= DPLL_DVO_2X_MODE;
+		I915_WRITE(DPLL(!crtc->pipe),
+			   I915_READ(DPLL(!crtc->pipe)) | DPLL_DVO_2X_MODE);
+	}
 
 	/* Wait for the clocks to stabilize. */
 	POSTING_READ(reg);
@@ -1672,10 +1660,25 @@
  *
  * Note!  This is for pre-ILK only.
  */
-static void i9xx_disable_pll(struct drm_i915_private *dev_priv, enum pipe pipe)
+static void i9xx_disable_pll(struct intel_crtc *crtc)
 {
-	/* Don't disable pipe A or pipe A PLLs if needed */
-	if (pipe == PIPE_A && (dev_priv->quirks & QUIRK_PIPEA_FORCE))
+	struct drm_device *dev = crtc->base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	enum pipe pipe = crtc->pipe;
+
+	/* Disable DVO 2x clock on both PLLs if necessary */
+	if (IS_I830(dev) &&
+	    intel_pipe_has_type(crtc, INTEL_OUTPUT_DVO) &&
+	    intel_num_dvo_pipes(dev) == 1) {
+		I915_WRITE(DPLL(PIPE_B),
+			   I915_READ(DPLL(PIPE_B)) & ~DPLL_DVO_2X_MODE);
+		I915_WRITE(DPLL(PIPE_A),
+			   I915_READ(DPLL(PIPE_A)) & ~DPLL_DVO_2X_MODE);
+	}
+
+	/* Don't disable pipe or pipe PLLs if needed */
+	if ((pipe == PIPE_A && dev_priv->quirks & QUIRK_PIPEA_FORCE) ||
+	    (pipe == PIPE_B && dev_priv->quirks & QUIRK_PIPEB_FORCE))
 		return;
 
 	/* Make sure the pipe isn't still relying on us */
@@ -1712,7 +1715,7 @@
 	assert_pipe_disabled(dev_priv, pipe);
 
 	/* Set PLL en = 0 */
-	val = DPLL_SSC_REF_CLOCK_CHV;
+	val = DPLL_SSC_REF_CLOCK_CHV | DPLL_REFA_CLK_ENABLE_VLV;
 	if (pipe != PIPE_A)
 		val |= DPLL_INTEGRATED_CRI_CLK_VLV;
 	I915_WRITE(DPLL(pipe), val);
@@ -1776,7 +1779,7 @@
 	if (WARN_ON(pll == NULL))
 		return;
 
-	WARN_ON(!pll->refcount);
+	WARN_ON(!pll->config.crtc_mask);
 	if (pll->active == 0) {
 		DRM_DEBUG_DRIVER("setting up %s\n", pll->name);
 		WARN_ON(pll->on);
@@ -1803,10 +1806,10 @@
 	if (WARN_ON(pll == NULL))
 		return;
 
-	if (WARN_ON(pll->refcount == 0))
+	if (WARN_ON(pll->config.crtc_mask == 0))
 		return;
 
-	DRM_DEBUG_KMS("enable %s (active %d, on? %d)for crtc %d\n",
+	DRM_DEBUG_KMS("enable %s (active %d, on? %d) for crtc %d\n",
 		      pll->name, pll->active, pll->on,
 		      crtc->base.base.id);
 
@@ -1824,7 +1827,7 @@
 	pll->on = true;
 }
 
-void intel_disable_shared_dpll(struct intel_crtc *crtc)
+static void intel_disable_shared_dpll(struct intel_crtc *crtc)
 {
 	struct drm_device *dev = crtc->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -1835,7 +1838,7 @@
 	if (WARN_ON(pll == NULL))
 	       return;
 
-	if (WARN_ON(pll->refcount == 0))
+	if (WARN_ON(pll->config.crtc_mask == 0))
 		return;
 
 	DRM_DEBUG_KMS("disable %s (active %d, on? %d) for crtc %d\n",
@@ -1868,7 +1871,7 @@
 	uint32_t reg, val, pipeconf_val;
 
 	/* PCH only available on ILK+ */
-	BUG_ON(INTEL_INFO(dev)->gen < 5);
+	BUG_ON(!HAS_PCH_SPLIT(dev));
 
 	/* Make sure PCH DPLL is enabled */
 	assert_shared_dpll_enabled(dev_priv,
@@ -1903,7 +1906,7 @@
 	val &= ~TRANS_INTERLACE_MASK;
 	if ((pipeconf_val & PIPECONF_INTERLACE_MASK) == PIPECONF_INTERLACED_ILK)
 		if (HAS_PCH_IBX(dev_priv->dev) &&
-		    intel_pipe_has_type(crtc, INTEL_OUTPUT_SDVO))
+		    intel_pipe_has_type(intel_crtc, INTEL_OUTPUT_SDVO))
 			val |= TRANS_LEGACY_INTERLACED_ILK;
 		else
 			val |= TRANS_INTERLACED;
@@ -1921,7 +1924,7 @@
 	u32 val, pipeconf_val;
 
 	/* PCH only available on ILK+ */
-	BUG_ON(INTEL_INFO(dev_priv->dev)->gen < 5);
+	BUG_ON(!HAS_PCH_SPLIT(dev_priv->dev));
 
 	/* FDI must be feeding us bits for PCH ports */
 	assert_fdi_tx_enabled(dev_priv, (enum pipe) cpu_transcoder);
@@ -2026,7 +2029,7 @@
 	 * need the check.
 	 */
 	if (!HAS_PCH_SPLIT(dev_priv->dev))
-		if (intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_DSI))
+		if (intel_pipe_has_type(crtc, INTEL_OUTPUT_DSI))
 			assert_dsi_pll_enabled(dev_priv);
 		else
 			assert_pll_enabled(dev_priv, pipe);
@@ -2043,8 +2046,8 @@
 	reg = PIPECONF(cpu_transcoder);
 	val = I915_READ(reg);
 	if (val & PIPECONF_ENABLE) {
-		WARN_ON(!(pipe == PIPE_A &&
-			  dev_priv->quirks & QUIRK_PIPEA_FORCE));
+		WARN_ON(!((pipe == PIPE_A && dev_priv->quirks & QUIRK_PIPEA_FORCE) ||
+			  (pipe == PIPE_B && dev_priv->quirks & QUIRK_PIPEB_FORCE)));
 		return;
 	}
 
@@ -2054,21 +2057,19 @@
 
 /**
  * intel_disable_pipe - disable a pipe, asserting requirements
- * @dev_priv: i915 private structure
- * @pipe: pipe to disable
+ * @crtc: crtc whose pipes is to be disabled
  *
- * Disable @pipe, making sure that various hardware specific requirements
- * are met, if applicable, e.g. plane disabled, panel fitter off, etc.
- *
- * @pipe should be %PIPE_A or %PIPE_B.
+ * Disable the pipe of @crtc, making sure that various hardware
+ * specific requirements are met, if applicable, e.g. plane
+ * disabled, panel fitter off, etc.
  *
  * Will wait until the pipe has shut down before returning.
  */
-static void intel_disable_pipe(struct drm_i915_private *dev_priv,
-			       enum pipe pipe)
+static void intel_disable_pipe(struct intel_crtc *crtc)
 {
-	enum transcoder cpu_transcoder = intel_pipe_to_cpu_transcoder(dev_priv,
-								      pipe);
+	struct drm_i915_private *dev_priv = crtc->base.dev->dev_private;
+	enum transcoder cpu_transcoder = crtc->config.cpu_transcoder;
+	enum pipe pipe = crtc->pipe;
 	int reg;
 	u32 val;
 
@@ -2080,17 +2081,26 @@
 	assert_cursor_disabled(dev_priv, pipe);
 	assert_sprites_disabled(dev_priv, pipe);
 
-	/* Don't disable pipe A or pipe A PLLs if needed */
-	if (pipe == PIPE_A && (dev_priv->quirks & QUIRK_PIPEA_FORCE))
-		return;
-
 	reg = PIPECONF(cpu_transcoder);
 	val = I915_READ(reg);
 	if ((val & PIPECONF_ENABLE) == 0)
 		return;
 
-	I915_WRITE(reg, val & ~PIPECONF_ENABLE);
-	intel_wait_for_pipe_off(dev_priv->dev, pipe);
+	/*
+	 * Double wide has implications for planes
+	 * so best keep it disabled when not needed.
+	 */
+	if (crtc->config.double_wide)
+		val &= ~PIPECONF_DOUBLE_WIDE;
+
+	/* Don't disable pipe or pipe PLLs if needed */
+	if (!(pipe == PIPE_A && dev_priv->quirks & QUIRK_PIPEA_FORCE) &&
+	    !(pipe == PIPE_B && dev_priv->quirks & QUIRK_PIPEB_FORCE))
+		val &= ~PIPECONF_ENABLE;
+
+	I915_WRITE(reg, val);
+	if ((val & PIPECONF_ENABLE) == 0)
+		intel_wait_for_pipe_off(crtc);
 }
 
 /*
@@ -2109,35 +2119,28 @@
 
 /**
  * intel_enable_primary_hw_plane - enable the primary plane on a given pipe
- * @dev_priv: i915 private structure
- * @plane: plane to enable
- * @pipe: pipe being fed
+ * @plane:  plane to be enabled
+ * @crtc: crtc for the plane
  *
- * Enable @plane on @pipe, making sure that @pipe is running first.
+ * Enable @plane on @crtc, making sure that the pipe is running first.
  */
-static void intel_enable_primary_hw_plane(struct drm_i915_private *dev_priv,
-					  enum plane plane, enum pipe pipe)
+static void intel_enable_primary_hw_plane(struct drm_plane *plane,
+					  struct drm_crtc *crtc)
 {
-	struct drm_device *dev = dev_priv->dev;
-	struct intel_crtc *intel_crtc =
-		to_intel_crtc(dev_priv->pipe_to_crtc_mapping[pipe]);
-	int reg;
-	u32 val;
+	struct drm_device *dev = plane->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 
 	/* If the pipe isn't enabled, we can't pump pixels and may hang */
-	assert_pipe_enabled(dev_priv, pipe);
+	assert_pipe_enabled(dev_priv, intel_crtc->pipe);
 
 	if (intel_crtc->primary_enabled)
 		return;
 
 	intel_crtc->primary_enabled = true;
 
-	reg = DSPCNTR(plane);
-	val = I915_READ(reg);
-	WARN_ON(val & DISPLAY_PLANE_ENABLE);
-
-	I915_WRITE(reg, val | DISPLAY_PLANE_ENABLE);
-	intel_flush_primary_plane(dev_priv, plane);
+	dev_priv->display.update_primary_plane(crtc, plane->fb,
+					       crtc->x, crtc->y);
 
 	/*
 	 * BDW signals flip done immediately if the plane
@@ -2150,31 +2153,27 @@
 
 /**
  * intel_disable_primary_hw_plane - disable the primary hardware plane
- * @dev_priv: i915 private structure
- * @plane: plane to disable
- * @pipe: pipe consuming the data
+ * @plane: plane to be disabled
+ * @crtc: crtc for the plane
  *
- * Disable @plane; should be an independent operation.
+ * Disable @plane on @crtc, making sure that the pipe is running first.
  */
-static void intel_disable_primary_hw_plane(struct drm_i915_private *dev_priv,
-					   enum plane plane, enum pipe pipe)
+static void intel_disable_primary_hw_plane(struct drm_plane *plane,
+					   struct drm_crtc *crtc)
 {
-	struct intel_crtc *intel_crtc =
-		to_intel_crtc(dev_priv->pipe_to_crtc_mapping[pipe]);
-	int reg;
-	u32 val;
+	struct drm_device *dev = plane->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+
+	assert_pipe_enabled(dev_priv, intel_crtc->pipe);
 
 	if (!intel_crtc->primary_enabled)
 		return;
 
 	intel_crtc->primary_enabled = false;
 
-	reg = DSPCNTR(plane);
-	val = I915_READ(reg);
-	WARN_ON((val & DISPLAY_PLANE_ENABLE) == 0);
-
-	I915_WRITE(reg, val & ~DISPLAY_PLANE_ENABLE);
-	intel_flush_primary_plane(dev_priv, plane);
+	dev_priv->display.update_primary_plane(crtc, plane->fb,
+					       crtc->x, crtc->y);
 }
 
 static bool need_vtd_wa(struct drm_device *dev)
@@ -2195,11 +2194,13 @@
 }
 
 int
-intel_pin_and_fence_fb_obj(struct drm_device *dev,
-			   struct drm_i915_gem_object *obj,
-			   struct intel_engine_cs *pipelined)
+intel_pin_and_fence_fb_obj(struct drm_plane *plane,
+			   struct drm_framebuffer *fb,
+			   struct i915_gem_request *pipelined)
 {
+	struct drm_device *dev = fb->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_gem_object *obj = intel_fb_obj(fb);
 	u32 alignment;
 	int ret;
 
@@ -2207,7 +2208,9 @@
 
 	switch (obj->tiling_mode) {
 	case I915_TILING_NONE:
-		if (IS_BROADWATER(dev) || IS_CRESTLINE(dev))
+		if (INTEL_INFO(dev)->gen >= 9)
+			alignment = 256 * 1024;
+		else if (IS_BROADWATER(dev) || IS_CRESTLINE(dev))
 			alignment = 128 * 1024;
 		else if (INTEL_INFO(dev)->gen >= 4)
 			alignment = 4 * 1024;
@@ -2215,8 +2218,15 @@
 			alignment = 64 * 1024;
 		break;
 	case I915_TILING_X:
-		/* pin() will align the object as required by fence */
-		alignment = 0;
+		if (INTEL_INFO(dev)->gen >= 9) {
+			alignment = 256 * 1024;
+		} else {
+			/* Async page flipping requires X tiling and 
+			 * 32kB alignment, so just make all X tiled
+			 * frame buffers aligned for that.
+			 */
+			alignment = 32 * 1024;
+		}
 		break;
 	case I915_TILING_Y:
 		WARN(1, "Y tiled bo slipped through, driver bug!\n");
@@ -2376,6 +2386,7 @@
 				 struct intel_plane_config *plane_config)
 {
 	struct drm_device *dev = intel_crtc->base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_crtc *c;
 	struct intel_crtc *i;
 	struct drm_i915_gem_object *obj;
@@ -2407,6 +2418,9 @@
 			continue;
 
 		if (i915_gem_obj_ggtt_offset(obj) == plane_config->base) {
+			if (obj->tiling_mode != I915_TILING_NONE)
+				dev_priv->preserve_bios_swizzle = true;
+
 			drm_framebuffer_reference(c->primary->fb);
 			intel_crtc->base.primary->fb = c->primary->fb;
 			obj->frontbuffer_bits |= INTEL_FRONTBUFFER_PRIMARY(intel_crtc->pipe);
@@ -2422,16 +2436,52 @@
 	struct drm_device *dev = crtc->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
-	struct drm_i915_gem_object *obj = intel_fb_obj(fb);
+	struct drm_i915_gem_object *obj;
 	int plane = intel_crtc->plane;
 	unsigned long linear_offset;
 	u32 dspcntr;
-	u32 reg;
+	u32 reg = DSPCNTR(plane);
+	int pixel_size;
+
+	if (!intel_crtc->primary_enabled) {
+		I915_WRITE(reg, 0);
+		if (INTEL_INFO(dev)->gen >= 4)
+			I915_WRITE(DSPSURF(plane), 0);
+		else
+			I915_WRITE(DSPADDR(plane), 0);
+		POSTING_READ(reg);
+		return;
+	}
+
+	obj = intel_fb_obj(fb);
+	if (WARN_ON(obj == NULL))
+		return;
+
+	pixel_size = drm_format_plane_cpp(fb->pixel_format, 0);
+
+	dspcntr = DISPPLANE_GAMMA_ENABLE;
+
+	dspcntr |= DISPLAY_PLANE_ENABLE;
+
+	if (INTEL_INFO(dev)->gen < 4) {
+		if (intel_crtc->pipe == PIPE_B)
+			dspcntr |= DISPPLANE_SEL_PIPE_B;
+
+		/* pipesrc and dspsize control the size that is scaled from,
+		 * which should always be the user's requested size.
+		 */
+		I915_WRITE(DSPSIZE(plane),
+			   ((intel_crtc->config.pipe_src_h - 1) << 16) |
+			   (intel_crtc->config.pipe_src_w - 1));
+		I915_WRITE(DSPPOS(plane), 0);
+	} else if (IS_CHERRYVIEW(dev) && plane == PLANE_B) {
+		I915_WRITE(PRIMSIZE(plane),
+			   ((intel_crtc->config.pipe_src_h - 1) << 16) |
+			   (intel_crtc->config.pipe_src_w - 1));
+		I915_WRITE(PRIMPOS(plane), 0);
+		I915_WRITE(PRIMCNSTALPHA(plane), 0);
+	}
 
-	reg = DSPCNTR(plane);
-	dspcntr = I915_READ(reg);
-	/* Mask out pixel format bits in case we change it */
-	dspcntr &= ~DISPPLANE_PIXFORMAT_MASK;
 	switch (fb->pixel_format) {
 	case DRM_FORMAT_C8:
 		dspcntr |= DISPPLANE_8BPP;
@@ -2463,30 +2513,40 @@
 		BUG();
 	}
 
-	if (INTEL_INFO(dev)->gen >= 4) {
-		if (obj->tiling_mode != I915_TILING_NONE)
-			dspcntr |= DISPPLANE_TILED;
-		else
-			dspcntr &= ~DISPPLANE_TILED;
-	}
+	if (INTEL_INFO(dev)->gen >= 4 &&
+	    obj->tiling_mode != I915_TILING_NONE)
+		dspcntr |= DISPPLANE_TILED;
 
 	if (IS_G4X(dev))
 		dspcntr |= DISPPLANE_TRICKLE_FEED_DISABLE;
 
-	I915_WRITE(reg, dspcntr);
-
-	linear_offset = y * fb->pitches[0] + x * (fb->bits_per_pixel / 8);
+	linear_offset = y * fb->pitches[0] + x * pixel_size;
 
 	if (INTEL_INFO(dev)->gen >= 4) {
 		intel_crtc->dspaddr_offset =
 			intel_gen4_compute_page_offset(&x, &y, obj->tiling_mode,
-						       fb->bits_per_pixel / 8,
+						       pixel_size,
 						       fb->pitches[0]);
 		linear_offset -= intel_crtc->dspaddr_offset;
 	} else {
 		intel_crtc->dspaddr_offset = linear_offset;
 	}
 
+	if (to_intel_plane(crtc->primary)->rotation == BIT(DRM_ROTATE_180)) {
+		dspcntr |= DISPPLANE_ROTATE_180;
+
+		x += (intel_crtc->config.pipe_src_w - 1);
+		y += (intel_crtc->config.pipe_src_h - 1);
+
+		/* Finding the last pixel of the last line of the display
+		data and adding to linear_offset*/
+		linear_offset +=
+			(intel_crtc->config.pipe_src_h - 1) * fb->pitches[0] +
+			(intel_crtc->config.pipe_src_w - 1) * pixel_size;
+	}
+
+	I915_WRITE(reg, dspcntr);
+
 	DRM_DEBUG_KMS("Writing base %08lX %08lX %d %d %d\n",
 		      i915_gem_obj_ggtt_offset(obj), linear_offset, x, y,
 		      fb->pitches[0]);
@@ -2508,16 +2568,33 @@
 	struct drm_device *dev = crtc->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
-	struct drm_i915_gem_object *obj = intel_fb_obj(fb);
+	struct drm_i915_gem_object *obj;
 	int plane = intel_crtc->plane;
 	unsigned long linear_offset;
 	u32 dspcntr;
-	u32 reg;
+	u32 reg = DSPCNTR(plane);
+	int pixel_size;
+
+	if (!intel_crtc->primary_enabled) {
+		I915_WRITE(reg, 0);
+		I915_WRITE(DSPSURF(plane), 0);
+		POSTING_READ(reg);
+		return;
+	}
+
+	obj = intel_fb_obj(fb);
+	if (WARN_ON(obj == NULL))
+		return;
+
+	pixel_size = drm_format_plane_cpp(fb->pixel_format, 0);
+
+	dspcntr = DISPPLANE_GAMMA_ENABLE;
+
+	dspcntr |= DISPLAY_PLANE_ENABLE;
+
+	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
+		dspcntr |= DISPPLANE_PIPE_CSC_ENABLE;
 
-	reg = DSPCNTR(plane);
-	dspcntr = I915_READ(reg);
-	/* Mask out pixel format bits in case we change it */
-	dspcntr &= ~DISPPLANE_PIXFORMAT_MASK;
 	switch (fb->pixel_format) {
 	case DRM_FORMAT_C8:
 		dspcntr |= DISPPLANE_8BPP;
@@ -2547,22 +2624,32 @@
 
 	if (obj->tiling_mode != I915_TILING_NONE)
 		dspcntr |= DISPPLANE_TILED;
-	else
-		dspcntr &= ~DISPPLANE_TILED;
 
-	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
-		dspcntr &= ~DISPPLANE_TRICKLE_FEED_DISABLE;
-	else
+	if (!IS_HASWELL(dev) && !IS_BROADWELL(dev))
 		dspcntr |= DISPPLANE_TRICKLE_FEED_DISABLE;
 
-	I915_WRITE(reg, dspcntr);
-
-	linear_offset = y * fb->pitches[0] + x * (fb->bits_per_pixel / 8);
+	linear_offset = y * fb->pitches[0] + x * pixel_size;
 	intel_crtc->dspaddr_offset =
 		intel_gen4_compute_page_offset(&x, &y, obj->tiling_mode,
-					       fb->bits_per_pixel / 8,
+					       pixel_size,
 					       fb->pitches[0]);
 	linear_offset -= intel_crtc->dspaddr_offset;
+	if (to_intel_plane(crtc->primary)->rotation == BIT(DRM_ROTATE_180)) {
+		dspcntr |= DISPPLANE_ROTATE_180;
+
+		if (!IS_HASWELL(dev) && !IS_BROADWELL(dev)) {
+			x += (intel_crtc->config.pipe_src_w - 1);
+			y += (intel_crtc->config.pipe_src_h - 1);
+
+			/* Finding the last pixel of the last line of the display
+			data and adding to linear_offset*/
+			linear_offset +=
+				(intel_crtc->config.pipe_src_h - 1) * fb->pitches[0] +
+				(intel_crtc->config.pipe_src_w - 1) * pixel_size;
+		}
+	}
+
+	I915_WRITE(reg, dspcntr);
 
 	DRM_DEBUG_KMS("Writing base %08lX %08lX %d %d %d\n",
 		      i915_gem_obj_ggtt_offset(obj), linear_offset, x, y,
@@ -2579,34 +2666,119 @@
 	POSTING_READ(reg);
 }
 
-/* Assume fb object is pinned & idle & fenced and just update base pointers */
-static int
-intel_pipe_set_base_atomic(struct drm_crtc *crtc, struct drm_framebuffer *fb,
-			   int x, int y, enum mode_set_atomic state)
+static void skylake_update_primary_plane(struct drm_crtc *crtc,
+					 struct drm_framebuffer *fb,
+					 int x, int y)
 {
 	struct drm_device *dev = crtc->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	struct intel_framebuffer *intel_fb;
+	struct drm_i915_gem_object *obj;
+	int pipe = intel_crtc->pipe;
+	u32 plane_ctl, stride;
 
-	if (dev_priv->display.disable_fbc)
-		dev_priv->display.disable_fbc(dev);
-	intel_increase_pllclock(dev, to_intel_crtc(crtc)->pipe);
+	if (!intel_crtc->primary_enabled) {
+		I915_WRITE(PLANE_CTL(pipe, 0), 0);
+		I915_WRITE(PLANE_SURF(pipe, 0), 0);
+		POSTING_READ(PLANE_CTL(pipe, 0));
+		return;
+	}
 
-	dev_priv->display.update_primary_plane(crtc, fb, x, y);
+	plane_ctl = PLANE_CTL_ENABLE |
+		    PLANE_CTL_PIPE_GAMMA_ENABLE |
+		    PLANE_CTL_PIPE_CSC_ENABLE;
 
-	return 0;
-}
+	switch (fb->pixel_format) {
+	case DRM_FORMAT_RGB565:
+		plane_ctl |= PLANE_CTL_FORMAT_RGB_565;
+		break;
+	case DRM_FORMAT_XRGB8888:
+		plane_ctl |= PLANE_CTL_FORMAT_XRGB_8888;
+		break;
+	case DRM_FORMAT_XBGR8888:
+		plane_ctl |= PLANE_CTL_ORDER_RGBX;
+		plane_ctl |= PLANE_CTL_FORMAT_XRGB_8888;
+		break;
+	case DRM_FORMAT_XRGB2101010:
+		plane_ctl |= PLANE_CTL_FORMAT_XRGB_2101010;
+		break;
+	case DRM_FORMAT_XBGR2101010:
+		plane_ctl |= PLANE_CTL_ORDER_RGBX;
+		plane_ctl |= PLANE_CTL_FORMAT_XRGB_2101010;
+		break;
+	default:
+		BUG();
+	}
 
-void intel_display_handle_reset(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_crtc *crtc;
+	intel_fb = to_intel_framebuffer(fb);
+	obj = intel_fb->obj;
 
 	/*
-	 * Flips in the rings have been nuked by the reset,
-	 * so complete all pending flips so that user space
-	 * will get its events and not get stuck.
-	 *
-	 * Also update the base address of all primary
+	 * The stride is either expressed as a multiple of 64 bytes chunks for
+	 * linear buffers or in number of tiles for tiled buffers.
+	 */
+	switch (obj->tiling_mode) {
+	case I915_TILING_NONE:
+		stride = fb->pitches[0] >> 6;
+		break;
+	case I915_TILING_X:
+		plane_ctl |= PLANE_CTL_TILED_X;
+		stride = fb->pitches[0] >> 9;
+		break;
+	default:
+		BUG();
+	}
+
+	plane_ctl |= PLANE_CTL_PLANE_GAMMA_DISABLE;
+	if (to_intel_plane(crtc->primary)->rotation == BIT(DRM_ROTATE_180))
+		plane_ctl |= PLANE_CTL_ROTATE_180;
+
+	I915_WRITE(PLANE_CTL(pipe, 0), plane_ctl);
+
+	DRM_DEBUG_KMS("Writing base %08lX %d,%d,%d,%d pitch=%d\n",
+		      i915_gem_obj_ggtt_offset(obj),
+		      x, y, fb->width, fb->height,
+		      fb->pitches[0]);
+
+	I915_WRITE(PLANE_POS(pipe, 0), 0);
+	I915_WRITE(PLANE_OFFSET(pipe, 0), (y << 16) | x);
+	I915_WRITE(PLANE_SIZE(pipe, 0),
+		   (intel_crtc->config.pipe_src_h - 1) << 16 |
+		   (intel_crtc->config.pipe_src_w - 1));
+	I915_WRITE(PLANE_STRIDE(pipe, 0), stride);
+	I915_WRITE(PLANE_SURF(pipe, 0), i915_gem_obj_ggtt_offset(obj));
+
+	POSTING_READ(PLANE_SURF(pipe, 0));
+}
+
+/* Assume fb object is pinned & idle & fenced and just update base pointers */
+static int
+intel_pipe_set_base_atomic(struct drm_crtc *crtc, struct drm_framebuffer *fb,
+			   int x, int y, enum mode_set_atomic state)
+{
+	struct drm_device *dev = crtc->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	if (dev_priv->display.disable_fbc)
+		dev_priv->display.disable_fbc(dev);
+
+	dev_priv->display.update_primary_plane(crtc, fb, x, y);
+
+	return 0;
+}
+
+void intel_display_handle_reset(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_crtc *crtc;
+
+	/*
+	 * Flips in the rings have been nuked by the reset,
+	 * so complete all pending flips so that user space
+	 * will get its events and not get stuck.
+	 *
+	 * Also update the base address of all primary
 	 * planes to the the last fb to make sure we're
 	 * showing the correct fb after a reset.
 	 *
@@ -2669,20 +2841,57 @@
 	struct drm_device *dev = crtc->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
-	unsigned long flags;
 	bool pending;
 
-	if (i915_reset_in_progress(&dev_priv->gpu_error) ||
-	    intel_crtc->reset_counter != atomic_read(&dev_priv->gpu_error.reset_counter))
+	if (intel_crtc->reset_counter != atomic_read(&dev_priv->gpu_error.reset_counter))
 		return false;
 
-	spin_lock_irqsave(&dev->event_lock, flags);
+	spin_lock_irq(&dev->event_lock);
 	pending = to_intel_crtc(crtc)->unpin_work != NULL;
-	spin_unlock_irqrestore(&dev->event_lock, flags);
+	spin_unlock_irq(&dev->event_lock);
 
 	return pending;
 }
 
+static void intel_update_pipe_size(struct intel_crtc *crtc)
+{
+	struct drm_device *dev = crtc->base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	const struct drm_display_mode *adjusted_mode;
+
+	if (!i915_module.fastboot)
+		return;
+
+	/*
+	 * Update pipe size and adjust fitter if needed: the reason for this is
+	 * that in compute_mode_changes we check the native mode (not the pfit
+	 * mode) to see if we can flip rather than do a full mode set. In the
+	 * fastboot case, we'll flip, but if we don't update the pipesrc and
+	 * pfit state, we'll end up with a big fb scanned out into the wrong
+	 * sized surface.
+	 *
+	 * To fix this properly, we need to hoist the checks up into
+	 * compute_mode_changes (or above), check the actual pfit state and
+	 * whether the platform allows pfit disable with pipe active, and only
+	 * then update the pipesrc and pfit state, even on the flip path.
+	 */
+
+	adjusted_mode = &crtc->config.adjusted_mode;
+
+	I915_WRITE(PIPESRC(crtc->pipe),
+		   ((adjusted_mode->crtc_hdisplay - 1) << 16) |
+		   (adjusted_mode->crtc_vdisplay - 1));
+	if (!crtc->config.pch_pfit.enabled &&
+	    (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS) ||
+	     intel_pipe_has_type(crtc, INTEL_OUTPUT_EDP))) {
+		I915_WRITE(PF_CTL(crtc->pipe), 0);
+		I915_WRITE(PF_WIN_POS(crtc->pipe), 0);
+		I915_WRITE(PF_WIN_SZ(crtc->pipe), 0);
+	}
+	crtc->config.pipe_src_w = adjusted_mode->crtc_hdisplay;
+	crtc->config.pipe_src_h = adjusted_mode->crtc_vdisplay;
+}
+
 static int
 intel_pipe_set_base(struct drm_crtc *crtc, int x, int y,
 		    struct drm_framebuffer *fb)
@@ -2692,7 +2901,6 @@
 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 	enum pipe pipe = intel_crtc->pipe;
 	struct drm_framebuffer *old_fb = crtc->primary->fb;
-	struct drm_i915_gem_object *obj = intel_fb_obj(fb);
 	struct drm_i915_gem_object *old_obj = intel_fb_obj(old_fb);
 	int ret;
 
@@ -2715,9 +2923,9 @@
 	}
 
 	mutex_lock(&dev->struct_mutex);
-	ret = intel_pin_and_fence_fb_obj(dev, obj, NULL);
+	ret = intel_pin_and_fence_fb_obj(crtc->primary, fb, NULL);
 	if (ret == 0)
-		i915_gem_track_fb(old_obj, obj,
+		i915_gem_track_fb(old_obj, intel_fb_obj(fb),
 				  INTEL_FRONTBUFFER_PRIMARY(pipe));
 	mutex_unlock(&dev->struct_mutex);
 	if (ret != 0) {
@@ -2725,37 +2933,6 @@
 		return ret;
 	}
 
-	/*
-	 * Update pipe size and adjust fitter if needed: the reason for this is
-	 * that in compute_mode_changes we check the native mode (not the pfit
-	 * mode) to see if we can flip rather than do a full mode set. In the
-	 * fastboot case, we'll flip, but if we don't update the pipesrc and
-	 * pfit state, we'll end up with a big fb scanned out into the wrong
-	 * sized surface.
-	 *
-	 * To fix this properly, we need to hoist the checks up into
-	 * compute_mode_changes (or above), check the actual pfit state and
-	 * whether the platform allows pfit disable with pipe active, and only
-	 * then update the pipesrc and pfit state, even on the flip path.
-	 */
-	if (i915.fastboot) {
-		const struct drm_display_mode *adjusted_mode =
-			&intel_crtc->config.adjusted_mode;
-
-		I915_WRITE(PIPESRC(intel_crtc->pipe),
-			   ((adjusted_mode->crtc_hdisplay - 1) << 16) |
-			   (adjusted_mode->crtc_vdisplay - 1));
-		if (!intel_crtc->config.pch_pfit.enabled &&
-		    (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS) ||
-		     intel_pipe_has_type(crtc, INTEL_OUTPUT_EDP))) {
-			I915_WRITE(PF_CTL(intel_crtc->pipe), 0);
-			I915_WRITE(PF_WIN_POS(intel_crtc->pipe), 0);
-			I915_WRITE(PF_WIN_SZ(intel_crtc->pipe), 0);
-		}
-		intel_crtc->config.pipe_src_w = adjusted_mode->crtc_hdisplay;
-		intel_crtc->config.pipe_src_h = adjusted_mode->crtc_vdisplay;
-	}
-
 	dev_priv->display.update_primary_plane(crtc, fb, x, y);
 
 	if (intel_crtc->active)
@@ -3346,23 +3523,56 @@
 	return false;
 }
 
+static void page_flip_completed(struct intel_crtc *intel_crtc)
+{
+	struct drm_i915_private *dev_priv = to_i915(intel_crtc->base.dev);
+	struct intel_unpin_work *work = intel_crtc->unpin_work;
+
+	/* ensure that the unpin work is consistent wrt ->pending. */
+	smp_rmb();
+	intel_crtc->unpin_work = NULL;
+
+	if (work->event)
+		drm_send_vblank_event(intel_crtc->base.dev,
+				      intel_crtc->pipe,
+				      work->event);
+
+	drm_crtc_vblank_put(&intel_crtc->base);
+
+	wake_up_all(&dev_priv->pending_flip_queue);
+	queue_work(dev_priv->wq, &work->work);
+
+	trace_i915_flip_complete(intel_crtc->plane,
+				 work->pending_flip_obj);
+}
+
 void intel_crtc_wait_for_pending_flips(struct drm_crtc *crtc)
 {
 	struct drm_device *dev = crtc->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	if (crtc->primary->fb == NULL)
-		return;
+	/* Kick hangcheck before waiting upon GPU results */
+	i915_queue_hangcheck(dev);
 
 	WARN_ON(waitqueue_active(&dev_priv->pending_flip_queue));
+	if (WARN_ON(wait_event_timeout(dev_priv->pending_flip_queue,
+				       !intel_crtc_has_pending_flip(crtc),
+				       60*HZ) == 0)) {
+		struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 
-	WARN_ON(wait_event_timeout(dev_priv->pending_flip_queue,
-				   !intel_crtc_has_pending_flip(crtc),
-				   60*HZ) == 0);
+		spin_lock_irq(&dev->event_lock);
+		if (intel_crtc->unpin_work) {
+			WARN_ONCE(1, "Removing stuck page flip\n");
+			page_flip_completed(intel_crtc);
+		}
+		spin_unlock_irq(&dev->event_lock);
+	}
 
-	mutex_lock(&dev->struct_mutex);
-	intel_finish_fb(crtc->primary->fb);
-	mutex_unlock(&dev->struct_mutex);
+	if (crtc->primary->fb) {
+		mutex_lock(&dev->struct_mutex);
+		intel_finish_fb(crtc->primary->fb);
+		mutex_unlock(&dev->struct_mutex);
+	}
 }
 
 /* Program iCLKIP clock to the desired frequency */
@@ -3580,9 +3790,7 @@
 	intel_fdi_normal_train(crtc);
 
 	/* For PCH DP, enable TRANS_DP_CTL */
-	if (HAS_PCH_CPT(dev) &&
-	    (intel_pipe_has_type(crtc, INTEL_OUTPUT_DISPLAYPORT) ||
-	     intel_pipe_has_type(crtc, INTEL_OUTPUT_EDP))) {
+	if (HAS_PCH_CPT(dev) && intel_crtc->config.has_dp_encoder) {
 		u32 bpc = (I915_READ(PIPECONF(pipe)) & PIPECONF_BPC_MASK) >> 5;
 		reg = TRANS_DP_CTL(pipe);
 		temp = I915_READ(reg);
@@ -3642,12 +3850,13 @@
 	if (pll == NULL)
 		return;
 
-	if (pll->refcount == 0) {
-		WARN(1, "bad %s refcount\n", pll->name);
+	if (!(pll->config.crtc_mask & (1 << crtc->pipe))) {
+		WARN(1, "bad %s crtc mask\n", pll->name);
 		return;
 	}
 
-	if (--pll->refcount == 0) {
+	pll->config.crtc_mask &= ~(1 << crtc->pipe);
+	if (pll->config.crtc_mask == 0) {
 		WARN_ON(pll->on);
 		WARN_ON(pll->active);
 	}
@@ -3658,15 +3867,9 @@
 struct intel_shared_dpll *intel_get_shared_dpll(struct intel_crtc *crtc)
 {
 	struct drm_i915_private *dev_priv = crtc->base.dev->dev_private;
-	struct intel_shared_dpll *pll = intel_crtc_to_shared_dpll(crtc);
+	struct intel_shared_dpll *pll;
 	enum intel_dpll_id i;
 
-	if (pll) {
-		DRM_DEBUG_KMS("CRTC:%d dropping existing %s\n",
-			      crtc->base.base.id, pll->name);
-		intel_put_shared_dpll(crtc);
-	}
-
 	if (HAS_PCH_IBX(dev_priv->dev)) {
 		/* Ironlake PCH has a fixed PLL->PCH pipe mapping. */
 		i = (enum intel_dpll_id) crtc->pipe;
@@ -3675,7 +3878,7 @@
 		DRM_DEBUG_KMS("CRTC:%d using pre-allocated %s\n",
 			      crtc->base.base.id, pll->name);
 
-		WARN_ON(pll->refcount);
+		WARN_ON(pll->new_config->crtc_mask);
 
 		goto found;
 	}
@@ -3684,15 +3887,16 @@
 		pll = &dev_priv->shared_dplls[i];
 
 		/* Only want to check enabled timings first */
-		if (pll->refcount == 0)
+		if (pll->new_config->crtc_mask == 0)
 			continue;
 
-		if (memcmp(&crtc->config.dpll_hw_state, &pll->hw_state,
-			   sizeof(pll->hw_state)) == 0) {
-			DRM_DEBUG_KMS("CRTC:%d sharing existing %s (refcount %d, ative %d)\n",
-				      crtc->base.base.id,
-				      pll->name, pll->refcount, pll->active);
-
+		if (memcmp(&crtc->new_config->dpll_hw_state,
+			   &pll->new_config->hw_state,
+			   sizeof(pll->new_config->hw_state)) == 0) {
+			DRM_DEBUG_KMS("CRTC:%d sharing existing %s (crtc mask 0x%08x, ative %d)\n",
+				      crtc->base.base.id, pll->name,
+				      pll->new_config->crtc_mask,
+				      pll->active);
 			goto found;
 		}
 	}
@@ -3700,7 +3904,7 @@
 	/* Ok no matching timings, maybe there's a free one? */
 	for (i = 0; i < dev_priv->num_shared_dpll; i++) {
 		pll = &dev_priv->shared_dplls[i];
-		if (pll->refcount == 0) {
+		if (pll->new_config->crtc_mask == 0) {
 			DRM_DEBUG_KMS("CRTC:%d allocated %s\n",
 				      crtc->base.base.id, pll->name);
 			goto found;
@@ -3710,18 +3914,86 @@
 	return NULL;
 
 found:
-	if (pll->refcount == 0)
-		pll->hw_state = crtc->config.dpll_hw_state;
+	if (pll->new_config->crtc_mask == 0)
+		pll->new_config->hw_state = crtc->new_config->dpll_hw_state;
 
-	crtc->config.shared_dpll = i;
+	crtc->new_config->shared_dpll = i;
 	DRM_DEBUG_DRIVER("using %s for pipe %c\n", pll->name,
 			 pipe_name(crtc->pipe));
 
-	pll->refcount++;
+	pll->new_config->crtc_mask |= 1 << crtc->pipe;
 
 	return pll;
 }
 
+/**
+ * intel_shared_dpll_start_config - start a new PLL staged config
+ * @dev_priv: DRM device
+ * @clear_pipes: mask of pipes that will have their PLLs freed
+ *
+ * Starts a new PLL staged config, copying the current config but
+ * releasing the references of pipes specified in clear_pipes.
+ */
+static int intel_shared_dpll_start_config(struct drm_i915_private *dev_priv,
+					  unsigned clear_pipes)
+{
+	struct intel_shared_dpll *pll;
+	enum intel_dpll_id i;
+
+	for (i = 0; i < dev_priv->num_shared_dpll; i++) {
+		pll = &dev_priv->shared_dplls[i];
+
+		pll->new_config = kmemdup(&pll->config, sizeof pll->config,
+					  GFP_KERNEL);
+		if (!pll->new_config)
+			goto cleanup;
+
+		pll->new_config->crtc_mask &= ~clear_pipes;
+	}
+
+	return 0;
+
+cleanup:
+	while (--i >= 0) {
+		pll = &dev_priv->shared_dplls[i];
+		kfree(pll->new_config);
+		pll->new_config = NULL;
+	}
+
+	return -ENOMEM;
+}
+
+static void intel_shared_dpll_commit(struct drm_i915_private *dev_priv)
+{
+	struct intel_shared_dpll *pll;
+	enum intel_dpll_id i;
+
+	for (i = 0; i < dev_priv->num_shared_dpll; i++) {
+		pll = &dev_priv->shared_dplls[i];
+
+		WARN_ON(pll->new_config == &pll->config);
+
+		pll->config = *pll->new_config;
+		kfree(pll->new_config);
+		pll->new_config = NULL;
+	}
+}
+
+static void intel_shared_dpll_abort_config(struct drm_i915_private *dev_priv)
+{
+	struct intel_shared_dpll *pll;
+	enum intel_dpll_id i;
+
+	for (i = 0; i < dev_priv->num_shared_dpll; i++) {
+		pll = &dev_priv->shared_dplls[i];
+
+		WARN_ON(pll->new_config == &pll->config);
+
+		kfree(pll->new_config);
+		pll->new_config = NULL;
+	}
+}
+
 static void cpt_verify_modeset(struct drm_device *dev, int pipe)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -3736,6 +4008,19 @@
 	}
 }
 
+static void skylake_pfit_enable(struct intel_crtc *crtc)
+{
+	struct drm_device *dev = crtc->base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	int pipe = crtc->pipe;
+
+	if (crtc->config.pch_pfit.enabled) {
+		I915_WRITE(PS_CTL(pipe), PS_ENABLE);
+		I915_WRITE(PS_WIN_POS(pipe), crtc->config.pch_pfit.pos);
+		I915_WRITE(PS_WIN_SZ(pipe), crtc->config.pch_pfit.size);
+	}
+}
+
 static void ironlake_pfit_enable(struct intel_crtc *crtc)
 {
 	struct drm_device *dev = crtc->base.dev;
@@ -3859,7 +4144,7 @@
 		return;
 
 	if (!HAS_PCH_SPLIT(dev_priv->dev)) {
-		if (intel_pipe_has_type(crtc, INTEL_OUTPUT_DSI))
+		if (intel_pipe_has_type(intel_crtc, INTEL_OUTPUT_DSI))
 			assert_dsi_pll_enabled(dev_priv);
 		else
 			assert_pll_enabled(dev_priv, pipe);
@@ -3911,14 +4196,10 @@
 static void intel_crtc_enable_planes(struct drm_crtc *crtc)
 {
 	struct drm_device *dev = crtc->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 	int pipe = intel_crtc->pipe;
-	int plane = intel_crtc->plane;
 
-	drm_vblank_on(dev, pipe);
-
-	intel_enable_primary_hw_plane(dev_priv, plane, pipe);
+	intel_enable_primary_hw_plane(crtc->primary, crtc);
 	intel_enable_planes(crtc);
 	intel_crtc_update_cursor(crtc, true);
 	intel_crtc_dpms_overlay(intel_crtc, true);
@@ -3955,7 +4236,7 @@
 	intel_crtc_dpms_overlay(intel_crtc, false);
 	intel_crtc_update_cursor(crtc, false);
 	intel_disable_planes(crtc);
-	intel_disable_primary_hw_plane(dev_priv, plane, pipe);
+	intel_disable_primary_hw_plane(crtc->primary, crtc);
 
 	/*
 	 * FIXME: Once we grow proper nuclear flip support out of this we need
@@ -3963,8 +4244,6 @@
 	 * consider this a flip to a NULL plane.
 	 */
 	intel_frontbuffer_flip(dev, INTEL_FRONTBUFFER_ALL_MASK(pipe));
-
-	drm_vblank_off(dev, pipe);
 }
 
 static void ironlake_crtc_enable(struct drm_crtc *crtc)
@@ -3974,7 +4253,6 @@
 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 	struct intel_encoder *encoder;
 	int pipe = intel_crtc->pipe;
-	enum plane plane = intel_crtc->plane;
 
 	WARN_ON(!crtc->enabled);
 
@@ -3991,22 +4269,15 @@
 
 	if (intel_crtc->config.has_pch_encoder) {
 		intel_cpu_transcoder_set_m_n(intel_crtc,
-					     &intel_crtc->config.fdi_m_n);
+				     &intel_crtc->config.fdi_m_n, NULL);
 	}
 
 	ironlake_set_pipeconf(crtc);
 
-	/* Set up the display plane register */
-	I915_WRITE(DSPCNTR(plane), DISPPLANE_GAMMA_ENABLE);
-	POSTING_READ(DSPCNTR(plane));
-
-	dev_priv->display.update_primary_plane(crtc, crtc->primary->fb,
-					       crtc->x, crtc->y);
-
 	intel_crtc->active = true;
 
-	intel_set_cpu_fifo_underrun_reporting(dev, pipe, true);
-	intel_set_pch_fifo_underrun_reporting(dev, pipe, true);
+	intel_set_cpu_fifo_underrun_reporting(dev_priv, pipe, true);
+	intel_set_pch_fifo_underrun_reporting(dev_priv, pipe, true);
 
 	for_each_encoder_on_crtc(dev, crtc, encoder)
 		if (encoder->pre_enable)
@@ -4042,6 +4313,9 @@
 	if (HAS_PCH_CPT(dev))
 		cpt_verify_modeset(dev, intel_crtc->pipe);
 
+	assert_vblank_disabled(crtc);
+	drm_crtc_vblank_on(crtc);
+
 	intel_crtc_enable_planes(crtc);
 }
 
@@ -4087,7 +4361,6 @@
 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 	struct intel_encoder *encoder;
 	int pipe = intel_crtc->pipe;
-	enum plane plane = intel_crtc->plane;
 
 	WARN_ON(!crtc->enabled);
 
@@ -4102,37 +4375,39 @@
 
 	intel_set_pipe_timings(intel_crtc);
 
+	if (intel_crtc->config.cpu_transcoder != TRANSCODER_EDP) {
+		I915_WRITE(PIPE_MULT(intel_crtc->config.cpu_transcoder),
+			   intel_crtc->config.pixel_multiplier - 1);
+	}
+
 	if (intel_crtc->config.has_pch_encoder) {
 		intel_cpu_transcoder_set_m_n(intel_crtc,
-					     &intel_crtc->config.fdi_m_n);
+				     &intel_crtc->config.fdi_m_n, NULL);
 	}
 
 	haswell_set_pipeconf(crtc);
 
 	intel_set_pipe_csc(crtc);
 
-	/* Set up the display plane register */
-	I915_WRITE(DSPCNTR(plane), DISPPLANE_GAMMA_ENABLE | DISPPLANE_PIPE_CSC_ENABLE);
-	POSTING_READ(DSPCNTR(plane));
-
-	dev_priv->display.update_primary_plane(crtc, crtc->primary->fb,
-					       crtc->x, crtc->y);
-
 	intel_crtc->active = true;
 
-	intel_set_cpu_fifo_underrun_reporting(dev, pipe, true);
+	intel_set_cpu_fifo_underrun_reporting(dev_priv, pipe, true);
 	for_each_encoder_on_crtc(dev, crtc, encoder)
 		if (encoder->pre_enable)
 			encoder->pre_enable(encoder);
 
 	if (intel_crtc->config.has_pch_encoder) {
-		intel_set_pch_fifo_underrun_reporting(dev, TRANSCODER_A, true);
+		intel_set_pch_fifo_underrun_reporting(dev_priv, TRANSCODER_A,
+						      true);
 		dev_priv->display.fdi_link_train(crtc);
 	}
 
 	intel_ddi_enable_pipe_clock(intel_crtc);
 
-	ironlake_pfit_enable(intel_crtc);
+	if (IS_SKYLAKE(dev))
+		skylake_pfit_enable(intel_crtc);
+	else
+		ironlake_pfit_enable(intel_crtc);
 
 	/*
 	 * On ILK+ LUT must be loaded before the pipe is running but with
@@ -4157,12 +4432,30 @@
 		intel_opregion_notify_encoder(encoder, true);
 	}
 
+	assert_vblank_disabled(crtc);
+	drm_crtc_vblank_on(crtc);
+
 	/* If we change the relative order between pipe/planes enabling, we need
 	 * to change the workaround. */
 	haswell_mode_set_planes_workaround(intel_crtc);
 	intel_crtc_enable_planes(crtc);
 }
 
+static void skylake_pfit_disable(struct intel_crtc *crtc)
+{
+	struct drm_device *dev = crtc->base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	int pipe = crtc->pipe;
+
+	/* To avoid upsetting the power well on haswell only disable the pfit if
+	 * it's in use. The hw state code will make sure we get this right. */
+	if (crtc->config.pch_pfit.enabled) {
+		I915_WRITE(PS_CTL(pipe), 0);
+		I915_WRITE(PS_WIN_POS(pipe), 0);
+		I915_WRITE(PS_WIN_SZ(pipe), 0);
+	}
+}
+
 static void ironlake_pfit_disable(struct intel_crtc *crtc)
 {
 	struct drm_device *dev = crtc->base.dev;
@@ -4192,13 +4485,17 @@
 
 	intel_crtc_disable_planes(crtc);
 
+	drm_crtc_vblank_off(crtc);
+	assert_vblank_disabled(crtc);
+
 	for_each_encoder_on_crtc(dev, crtc, encoder)
 		encoder->disable(encoder);
 
 	if (intel_crtc->config.has_pch_encoder)
-		intel_set_pch_fifo_underrun_reporting(dev, pipe, false);
+		intel_set_pch_fifo_underrun_reporting(dev_priv, pipe, false);
+
+	intel_disable_pipe(intel_crtc);
 
-	intel_disable_pipe(dev_priv, pipe);
 	ironlake_pfit_disable(intel_crtc);
 
 	for_each_encoder_on_crtc(dev, crtc, encoder)
@@ -4209,7 +4506,7 @@
 		ironlake_fdi_disable(crtc);
 
 		ironlake_disable_pch_transcoder(dev_priv, pipe);
-		intel_set_pch_fifo_underrun_reporting(dev, pipe, true);
+		intel_set_pch_fifo_underrun_reporting(dev_priv, pipe, true);
 
 		if (HAS_PCH_CPT(dev)) {
 			/* disable TRANS_DP_CTL */
@@ -4246,7 +4543,6 @@
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 	struct intel_encoder *encoder;
-	int pipe = intel_crtc->pipe;
 	enum transcoder cpu_transcoder = intel_crtc->config.cpu_transcoder;
 
 	if (!intel_crtc->active)
@@ -4254,27 +4550,35 @@
 
 	intel_crtc_disable_planes(crtc);
 
+	drm_crtc_vblank_off(crtc);
+	assert_vblank_disabled(crtc);
+
 	for_each_encoder_on_crtc(dev, crtc, encoder) {
 		intel_opregion_notify_encoder(encoder, false);
 		encoder->disable(encoder);
 	}
 
 	if (intel_crtc->config.has_pch_encoder)
-		intel_set_pch_fifo_underrun_reporting(dev, TRANSCODER_A, false);
-	intel_disable_pipe(dev_priv, pipe);
+		intel_set_pch_fifo_underrun_reporting(dev_priv, TRANSCODER_A,
+						      false);
+	intel_disable_pipe(intel_crtc);
 
 	if (intel_crtc->config.dp_encoder_is_mst)
 		intel_ddi_set_vc_payload_alloc(crtc, false);
 
 	intel_ddi_disable_transcoder_func(dev_priv, cpu_transcoder);
 
-	ironlake_pfit_disable(intel_crtc);
+	if (IS_SKYLAKE(dev))
+		skylake_pfit_disable(intel_crtc);
+	else
+		ironlake_pfit_disable(intel_crtc);
 
 	intel_ddi_disable_pipe_clock(intel_crtc);
 
 	if (intel_crtc->config.has_pch_encoder) {
 		lpt_disable_pch_transcoder(dev_priv);
-		intel_set_pch_fifo_underrun_reporting(dev, TRANSCODER_A, true);
+		intel_set_pch_fifo_underrun_reporting(dev_priv, TRANSCODER_A,
+						      true);
 		intel_ddi_fdi_disable(crtc);
 	}
 
@@ -4395,20 +4699,6 @@
 	return mask;
 }
 
-void intel_display_set_init_power(struct drm_i915_private *dev_priv,
-				  bool enable)
-{
-	if (dev_priv->power_domains.init_power_on == enable)
-		return;
-
-	if (enable)
-		intel_display_power_get(dev_priv, POWER_DOMAIN_INIT);
-	else
-		intel_display_power_put(dev_priv, POWER_DOMAIN_INIT);
-
-	dev_priv->power_domains.init_power_on = enable;
-}
-
 static void modeset_update_crtc_power_domains(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -4431,6 +4721,9 @@
 			intel_display_power_get(dev_priv, domain);
 	}
 
+	if (dev_priv->display.modeset_global_resources)
+		dev_priv->display.modeset_global_resources(dev);
+
 	for_each_intel_crtc(dev, crtc) {
 		enum intel_display_power_domain domain;
 
@@ -4462,7 +4755,7 @@
 	struct drm_i915_private *dev_priv = dev->dev_private;
 
 	dev_priv->vlv_cdclk_freq = dev_priv->display.get_display_clock_speed(dev);
-	DRM_DEBUG_DRIVER("Current CD clock rate: %d kHz",
+	DRM_DEBUG_DRIVER("Current CD clock rate: %d kHz\n",
 			 dev_priv->vlv_cdclk_freq);
 
 	/*
@@ -4501,10 +4794,9 @@
 	mutex_unlock(&dev_priv->rps.hw_lock);
 
 	if (cdclk == 400000) {
-		u32 divider, vco;
+		u32 divider;
 
-		vco = valleyview_get_vco(dev_priv);
-		divider = DIV_ROUND_CLOSEST(vco << 1, cdclk) - 1;
+		divider = DIV_ROUND_CLOSEST(dev_priv->hpll_freq << 1, cdclk) - 1;
 
 		mutex_lock(&dev_priv->dpio_lock);
 		/* adjust cdclk divider */
@@ -4539,11 +4831,55 @@
 	vlv_update_cdclk(dev);
 }
 
+static void cherryview_set_cdclk(struct drm_device *dev, int cdclk)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	u32 val, cmd;
+
+	WARN_ON(dev_priv->display.get_display_clock_speed(dev) != dev_priv->vlv_cdclk_freq);
+
+	switch (cdclk) {
+	case 400000:
+		cmd = 3;
+		break;
+	case 333333:
+	case 320000:
+		cmd = 2;
+		break;
+	case 266667:
+		cmd = 1;
+		break;
+	case 200000:
+		cmd = 0;
+		break;
+	default:
+		WARN_ON(1);
+		return;
+	}
+
+	mutex_lock(&dev_priv->rps.hw_lock);
+	val = vlv_punit_read(dev_priv, PUNIT_REG_DSPFREQ);
+	val &= ~DSPFREQGUAR_MASK_CHV;
+	val |= (cmd << DSPFREQGUAR_SHIFT_CHV);
+	vlv_punit_write(dev_priv, PUNIT_REG_DSPFREQ, val);
+	if (wait_for((vlv_punit_read(dev_priv, PUNIT_REG_DSPFREQ) &
+		      DSPFREQSTAT_MASK_CHV) == (cmd << DSPFREQSTAT_SHIFT_CHV),
+		     50)) {
+		DRM_ERROR("timed out waiting for CDclk change\n");
+	}
+	mutex_unlock(&dev_priv->rps.hw_lock);
+
+	vlv_update_cdclk(dev);
+}
+
 static int valleyview_calc_cdclk(struct drm_i915_private *dev_priv,
 				 int max_pixclk)
 {
-	int vco = valleyview_get_vco(dev_priv);
-	int freq_320 = (vco <<  1) % 320000 != 0 ? 333333 : 320000;
+	int freq_320 = (dev_priv->hpll_freq <<  1) % 320000 != 0 ? 333333 : 320000;
+
+	/* FIXME: Punit isn't quite ready yet */
+	if (IS_CHERRYVIEW(dev_priv->dev))
+		return 400000;
 
 	/*
 	 * Really only a few cases to deal with, as only 4 CDclks are supported:
@@ -4607,59 +4943,54 @@
 	int max_pixclk = intel_mode_max_pixclk(dev_priv);
 	int req_cdclk = valleyview_calc_cdclk(dev_priv, max_pixclk);
 
-	if (req_cdclk != dev_priv->vlv_cdclk_freq)
-		valleyview_set_cdclk(dev, req_cdclk);
-	modeset_update_crtc_power_domains(dev);
+	if (req_cdclk != dev_priv->vlv_cdclk_freq) {
+		if (IS_CHERRYVIEW(dev))
+			cherryview_set_cdclk(dev, req_cdclk);
+		else
+			valleyview_set_cdclk(dev, req_cdclk);
+	}
 }
 
 static void valleyview_crtc_enable(struct drm_crtc *crtc)
 {
 	struct drm_device *dev = crtc->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = to_i915(dev);
 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 	struct intel_encoder *encoder;
 	int pipe = intel_crtc->pipe;
-	int plane = intel_crtc->plane;
 	bool is_dsi;
-	u32 dspcntr;
 
 	WARN_ON(!crtc->enabled);
 
 	if (intel_crtc->active)
 		return;
 
-	is_dsi = intel_pipe_has_type(crtc, INTEL_OUTPUT_DSI);
-
-	if (!is_dsi && !IS_CHERRYVIEW(dev))
-		vlv_prepare_pll(intel_crtc);
-
-	/* Set up the display plane register */
-	dspcntr = DISPPLANE_GAMMA_ENABLE;
+	is_dsi = intel_pipe_has_type(intel_crtc, INTEL_OUTPUT_DSI);
+
+	if (!is_dsi) {
+		if (IS_CHERRYVIEW(dev))
+			chv_prepare_pll(intel_crtc, &intel_crtc->config);
+		else
+			vlv_prepare_pll(intel_crtc, &intel_crtc->config);
+	}
 
 	if (intel_crtc->config.has_dp_encoder)
 		intel_dp_set_m_n(intel_crtc);
 
 	intel_set_pipe_timings(intel_crtc);
 
-	/* pipesrc and dspsize control the size that is scaled from,
-	 * which should always be the user's requested size.
-	 */
-	I915_WRITE(DSPSIZE(plane),
-		   ((intel_crtc->config.pipe_src_h - 1) << 16) |
-		   (intel_crtc->config.pipe_src_w - 1));
-	I915_WRITE(DSPPOS(plane), 0);
-
-	i9xx_set_pipeconf(intel_crtc);
+	if (IS_CHERRYVIEW(dev) && pipe == PIPE_B) {
+		struct drm_i915_private *dev_priv = dev->dev_private;
 
-	I915_WRITE(DSPCNTR(plane), dspcntr);
-	POSTING_READ(DSPCNTR(plane));
+		I915_WRITE(CHV_BLEND(pipe), CHV_BLEND_LEGACY);
+		I915_WRITE(CHV_CANVAS(pipe), 0);
+	}
 
-	dev_priv->display.update_primary_plane(crtc, crtc->primary->fb,
-					       crtc->x, crtc->y);
+	i9xx_set_pipeconf(intel_crtc);
 
 	intel_crtc->active = true;
 
-	intel_set_cpu_fifo_underrun_reporting(dev, pipe, true);
+	intel_set_cpu_fifo_underrun_reporting(dev_priv, pipe, true);
 
 	for_each_encoder_on_crtc(dev, crtc, encoder)
 		if (encoder->pre_pll_enable)
@@ -4667,9 +4998,9 @@
 
 	if (!is_dsi) {
 		if (IS_CHERRYVIEW(dev))
-			chv_enable_pll(intel_crtc);
+			chv_enable_pll(intel_crtc, &intel_crtc->config);
 		else
-			vlv_enable_pll(intel_crtc);
+			vlv_enable_pll(intel_crtc, &intel_crtc->config);
 	}
 
 	for_each_encoder_on_crtc(dev, crtc, encoder)
@@ -4686,10 +5017,13 @@
 	for_each_encoder_on_crtc(dev, crtc, encoder)
 		encoder->enable(encoder);
 
+	assert_vblank_disabled(crtc);
+	drm_crtc_vblank_on(crtc);
+
 	intel_crtc_enable_planes(crtc);
 
 	/* Underruns don't raise interrupts, so check manually. */
-	i9xx_check_fifo_underruns(dev);
+	i9xx_check_fifo_underruns(dev_priv);
 }
 
 static void i9xx_set_pll_dividers(struct intel_crtc *crtc)
@@ -4704,12 +5038,10 @@
 static void i9xx_crtc_enable(struct drm_crtc *crtc)
 {
 	struct drm_device *dev = crtc->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = to_i915(dev);
 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 	struct intel_encoder *encoder;
 	int pipe = intel_crtc->pipe;
-	int plane = intel_crtc->plane;
-	u32 dspcntr;
 
 	WARN_ON(!crtc->enabled);
 
@@ -4718,39 +5050,17 @@
 
 	i9xx_set_pll_dividers(intel_crtc);
 
-	/* Set up the display plane register */
-	dspcntr = DISPPLANE_GAMMA_ENABLE;
-
-	if (pipe == 0)
-		dspcntr &= ~DISPPLANE_SEL_PIPE_MASK;
-	else
-		dspcntr |= DISPPLANE_SEL_PIPE_B;
-
 	if (intel_crtc->config.has_dp_encoder)
 		intel_dp_set_m_n(intel_crtc);
 
 	intel_set_pipe_timings(intel_crtc);
 
-	/* pipesrc and dspsize control the size that is scaled from,
-	 * which should always be the user's requested size.
-	 */
-	I915_WRITE(DSPSIZE(plane),
-		   ((intel_crtc->config.pipe_src_h - 1) << 16) |
-		   (intel_crtc->config.pipe_src_w - 1));
-	I915_WRITE(DSPPOS(plane), 0);
-
 	i9xx_set_pipeconf(intel_crtc);
 
-	I915_WRITE(DSPCNTR(plane), dspcntr);
-	POSTING_READ(DSPCNTR(plane));
-
-	dev_priv->display.update_primary_plane(crtc, crtc->primary->fb,
-					       crtc->x, crtc->y);
-
 	intel_crtc->active = true;
 
 	if (!IS_GEN2(dev))
-		intel_set_cpu_fifo_underrun_reporting(dev, pipe, true);
+		intel_set_cpu_fifo_underrun_reporting(dev_priv, pipe, true);
 
 	for_each_encoder_on_crtc(dev, crtc, encoder)
 		if (encoder->pre_enable)
@@ -4768,6 +5078,9 @@
 	for_each_encoder_on_crtc(dev, crtc, encoder)
 		encoder->enable(encoder);
 
+	assert_vblank_disabled(crtc);
+	drm_crtc_vblank_on(crtc);
+
 	intel_crtc_enable_planes(crtc);
 
 	/*
@@ -4778,10 +5091,10 @@
 	 * but leave the pipe running.
 	 */
 	if (IS_GEN2(dev))
-		intel_set_cpu_fifo_underrun_reporting(dev, pipe, true);
+		intel_set_cpu_fifo_underrun_reporting(dev_priv, pipe, true);
 
 	/* Underruns don't raise interrupts, so check manually. */
-	i9xx_check_fifo_underruns(dev);
+	i9xx_check_fifo_underruns(dev_priv);
 }
 
 static void i9xx_pfit_disable(struct intel_crtc *crtc)
@@ -4817,7 +5130,7 @@
 	 * but leave the pipe running.
 	 */
 	if (IS_GEN2(dev))
-		intel_set_cpu_fifo_underrun_reporting(dev, pipe, false);
+		intel_set_cpu_fifo_underrun_reporting(dev_priv, pipe, false);
 
 	/*
 	 * Vblank time updates from the shadow to live plane control register
@@ -4831,9 +5144,6 @@
 	intel_set_memory_cxsr(dev_priv, false);
 	intel_crtc_disable_planes(crtc);
 
-	for_each_encoder_on_crtc(dev, crtc, encoder)
-		encoder->disable(encoder);
-
 	/*
 	 * On gen2 planes are double buffered but the pipe isn't, so we must
 	 * wait for planes to fully turn off before disabling the pipe.
@@ -4842,7 +5152,13 @@
 	 */
 	intel_wait_for_vblank(dev, pipe);
 
-	intel_disable_pipe(dev_priv, pipe);
+	drm_crtc_vblank_off(crtc);
+	assert_vblank_disabled(crtc);
+
+	for_each_encoder_on_crtc(dev, crtc, encoder)
+		encoder->disable(encoder);
+
+	intel_disable_pipe(intel_crtc);
 
 	i9xx_pfit_disable(intel_crtc);
 
@@ -4850,17 +5166,17 @@
 		if (encoder->post_disable)
 			encoder->post_disable(encoder);
 
-	if (!intel_pipe_has_type(crtc, INTEL_OUTPUT_DSI)) {
+	if (!intel_pipe_has_type(intel_crtc, INTEL_OUTPUT_DSI)) {
 		if (IS_CHERRYVIEW(dev))
 			chv_disable_pll(dev_priv, pipe);
 		else if (IS_VALLEYVIEW(dev))
 			vlv_disable_pll(dev_priv, pipe);
 		else
-			i9xx_disable_pll(dev_priv, pipe);
+			i9xx_disable_pll(intel_crtc);
 	}
 
 	if (!IS_GEN2(dev))
-		intel_set_cpu_fifo_underrun_reporting(dev, pipe, false);
+		intel_set_cpu_fifo_underrun_reporting(dev_priv, pipe, false);
 
 	intel_crtc->active = false;
 	intel_update_watermarks(crtc);
@@ -4874,36 +5190,6 @@
 {
 }
 
-static void intel_crtc_update_sarea(struct drm_crtc *crtc,
-				    bool enabled)
-{
-	struct drm_device *dev = crtc->dev;
-	struct drm_i915_master_private *master_priv;
-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
-	int pipe = intel_crtc->pipe;
-
-	if (!dev->primary->master)
-		return;
-
-	master_priv = dev->primary->master->driver_priv;
-	if (!master_priv->sarea_priv)
-		return;
-
-	switch (pipe) {
-	case 0:
-		master_priv->sarea_priv->pipeA_w = enabled ? crtc->mode.hdisplay : 0;
-		master_priv->sarea_priv->pipeA_h = enabled ? crtc->mode.vdisplay : 0;
-		break;
-	case 1:
-		master_priv->sarea_priv->pipeB_w = enabled ? crtc->mode.hdisplay : 0;
-		master_priv->sarea_priv->pipeB_h = enabled ? crtc->mode.vdisplay : 0;
-		break;
-	default:
-		DRM_ERROR("Can't update pipe %c in SAREA\n", pipe_name(pipe));
-		break;
-	}
-}
-
 /* Master function to enable/disable CRTC and corresponding power wells */
 void intel_crtc_control(struct drm_crtc *crtc, bool enable)
 {
@@ -4947,8 +5233,6 @@
 		enable |= intel_encoder->connectors_active;
 
 	intel_crtc_control(crtc, enable);
-
-	intel_crtc_update_sarea(crtc, enable);
 }
 
 static void intel_crtc_disable(struct drm_crtc *crtc)
@@ -4963,7 +5247,6 @@
 	WARN_ON(!crtc->enabled);
 
 	dev_priv->display.crtc_disable(crtc);
-	intel_crtc_update_sarea(crtc, false);
 	dev_priv->display.off(crtc);
 
 	if (crtc->primary->fb) {
@@ -5001,15 +5284,8 @@
  * state of the entire output pipe. */
 static void intel_encoder_dpms(struct intel_encoder *encoder, int mode)
 {
-	if (mode == DRM_MODE_DPMS_ON) {
-		encoder->connectors_active = true;
-
-		intel_crtc_update_dpms(encoder->base.crtc);
-	} else {
-		encoder->connectors_active = false;
-
-		intel_crtc_update_dpms(encoder->base.crtc);
-	}
+	encoder->connectors_active = mode == DRM_MODE_DPMS_ON;
+	intel_crtc_update_dpms(encoder->base.crtc);
 }
 
 /* Cross check the actual hw state with our own modeset state tracking (and it's
@@ -5193,7 +5469,7 @@
 static void hsw_compute_ips_config(struct intel_crtc *crtc,
 				   struct intel_crtc_config *pipe_config)
 {
-	pipe_config->ips_enabled = i915.enable_ips &&
+	pipe_config->ips_enabled = i915_module.enable_ips &&
 				   hsw_crtc_supports_ips(crtc) &&
 				   pipe_config->pipe_bpp <= 24;
 }
@@ -5202,11 +5478,11 @@
 				     struct intel_crtc_config *pipe_config)
 {
 	struct drm_device *dev = crtc->base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_display_mode *adjusted_mode = &pipe_config->adjusted_mode;
 
 	/* FIXME should check pixel clock limits on all platforms */
 	if (INTEL_INFO(dev)->gen < 4) {
-		struct drm_i915_private *dev_priv = dev->dev_private;
 		int clock_limit =
 			dev_priv->display.get_display_clock_speed(dev);
 
@@ -5233,7 +5509,7 @@
 	 * - LVDS dual channel mode
 	 * - Double wide pipe
 	 */
-	if ((intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_LVDS) &&
+	if ((intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS) &&
 	     intel_is_dual_link_lvds(dev)) || pipe_config->double_wide)
 		pipe_config->pipe_src_w &= ~1;
 
@@ -5255,13 +5531,6 @@
 	if (HAS_IPS(dev))
 		hsw_compute_ips_config(crtc, pipe_config);
 
-	/*
-	 * XXX: PCH/WRPLL clock sharing is done in ->mode_set, so make sure the
-	 * old clock survives for now.
-	 */
-	if (HAS_PCH_IBX(dev) || HAS_PCH_CPT(dev) || HAS_DDI(dev))
-		pipe_config->shared_dpll = crtc->config.shared_dpll;
-
 	if (pipe_config->has_pch_encoder)
 		return ironlake_fdi_compute_config(crtc, pipe_config);
 
@@ -5271,10 +5540,16 @@
 static int valleyview_get_display_clock_speed(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	int vco = valleyview_get_vco(dev_priv);
 	u32 val;
 	int divider;
 
+	/* FIXME: Punit isn't quite ready yet */
+	if (IS_CHERRYVIEW(dev))
+		return 400000;
+
+	if (dev_priv->hpll_freq == 0)
+		dev_priv->hpll_freq = valleyview_get_vco(dev_priv);
+
 	mutex_lock(&dev_priv->dpio_lock);
 	val = vlv_cck_read(dev_priv, CCK_DISPLAY_CLOCK_CONTROL);
 	mutex_unlock(&dev_priv->dpio_lock);
@@ -5285,7 +5560,7 @@
 	     (divider << DISPLAY_FREQUENCY_STATUS_SHIFT),
 	     "cdclk change in progress\n");
 
-	return DIV_ROUND_CLOSEST(vco << 1, divider + 1);
+	return DIV_ROUND_CLOSEST(dev_priv->hpll_freq << 1, divider + 1);
 }
 
 static int i945_get_display_clock_speed(struct drm_device *dev)
@@ -5411,21 +5686,21 @@
 
 static inline bool intel_panel_use_ssc(struct drm_i915_private *dev_priv)
 {
-	if (i915.panel_use_ssc >= 0)
-		return i915.panel_use_ssc != 0;
+	if (i915_module.panel_use_ssc >= 0)
+		return i915_module.panel_use_ssc != 0;
 	return dev_priv->vbt.lvds_use_ssc
 		&& !(dev_priv->quirks & QUIRK_LVDS_SSC_DISABLE);
 }
 
-static int i9xx_get_refclk(struct drm_crtc *crtc, int num_connectors)
+static int i9xx_get_refclk(struct intel_crtc *crtc, int num_connectors)
 {
-	struct drm_device *dev = crtc->dev;
+	struct drm_device *dev = crtc->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	int refclk;
 
 	if (IS_VALLEYVIEW(dev)) {
 		refclk = 100000;
-	} else if (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS) &&
+	} else if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS) &&
 	    intel_panel_use_ssc(dev_priv) && num_connectors < 2) {
 		refclk = dev_priv->vbt.lvds_ssc_freq;
 		DRM_DEBUG_KMS("using SSC reference clock of %d kHz\n", refclk);
@@ -5455,24 +5730,24 @@
 	u32 fp, fp2 = 0;
 
 	if (IS_PINEVIEW(dev)) {
-		fp = pnv_dpll_compute_fp(&crtc->config.dpll);
+		fp = pnv_dpll_compute_fp(&crtc->new_config->dpll);
 		if (reduced_clock)
 			fp2 = pnv_dpll_compute_fp(reduced_clock);
 	} else {
-		fp = i9xx_dpll_compute_fp(&crtc->config.dpll);
+		fp = i9xx_dpll_compute_fp(&crtc->new_config->dpll);
 		if (reduced_clock)
 			fp2 = i9xx_dpll_compute_fp(reduced_clock);
 	}
 
-	crtc->config.dpll_hw_state.fp0 = fp;
+	crtc->new_config->dpll_hw_state.fp0 = fp;
 
 	crtc->lowfreq_avail = false;
-	if (intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_LVDS) &&
-	    reduced_clock && i915.powersave) {
-		crtc->config.dpll_hw_state.fp1 = fp2;
+	if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS) &&
+	    reduced_clock && i915_module.powersave) {
+		crtc->new_config->dpll_hw_state.fp1 = fp2;
 		crtc->lowfreq_avail = true;
 	} else {
-		crtc->config.dpll_hw_state.fp1 = fp;
+		crtc->new_config->dpll_hw_state.fp1 = fp;
 	}
 }
 
@@ -5519,7 +5794,8 @@
 }
 
 static void intel_cpu_transcoder_set_m_n(struct intel_crtc *crtc,
-					 struct intel_link_m_n *m_n)
+					 struct intel_link_m_n *m_n,
+					 struct intel_link_m_n *m2_n2)
 {
 	struct drm_device *dev = crtc->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -5531,6 +5807,18 @@
 		I915_WRITE(PIPE_DATA_N1(transcoder), m_n->gmch_n);
 		I915_WRITE(PIPE_LINK_M1(transcoder), m_n->link_m);
 		I915_WRITE(PIPE_LINK_N1(transcoder), m_n->link_n);
+		/* M2_N2 registers to be set only for gen < 8 (M2_N2 available
+		 * for gen < 8) and if DRRS is supported (to make sure the
+		 * registers are not unnecessarily accessed).
+		 */
+		if (m2_n2 && INTEL_INFO(dev)->gen < 8 &&
+			crtc->config.has_drrs) {
+			I915_WRITE(PIPE_DATA_M2(transcoder),
+					TU_SIZE(m2_n2->tu) | m2_n2->gmch_m);
+			I915_WRITE(PIPE_DATA_N2(transcoder), m2_n2->gmch_n);
+			I915_WRITE(PIPE_LINK_M2(transcoder), m2_n2->link_m);
+			I915_WRITE(PIPE_LINK_N2(transcoder), m2_n2->link_n);
+		}
 	} else {
 		I915_WRITE(PIPE_DATA_M_G4X(pipe), TU_SIZE(m_n->tu) | m_n->gmch_m);
 		I915_WRITE(PIPE_DATA_N_G4X(pipe), m_n->gmch_n);
@@ -5539,15 +5827,17 @@
 	}
 }
 
-static void intel_dp_set_m_n(struct intel_crtc *crtc)
+void intel_dp_set_m_n(struct intel_crtc *crtc)
 {
 	if (crtc->config.has_pch_encoder)
 		intel_pch_transcoder_set_m_n(crtc, &crtc->config.dp_m_n);
 	else
-		intel_cpu_transcoder_set_m_n(crtc, &crtc->config.dp_m_n);
+		intel_cpu_transcoder_set_m_n(crtc, &crtc->config.dp_m_n,
+						   &crtc->config.dp_m2_n2);
 }
 
-static void vlv_update_pll(struct intel_crtc *crtc)
+static void vlv_update_pll(struct intel_crtc *crtc,
+			   struct intel_crtc_config *pipe_config)
 {
 	u32 dpll, dpll_md;
 
@@ -5562,14 +5852,15 @@
 	if (crtc->pipe == PIPE_B)
 		dpll |= DPLL_INTEGRATED_CRI_CLK_VLV;
 	dpll |= DPLL_VCO_ENABLE;
-	crtc->config.dpll_hw_state.dpll = dpll;
+	pipe_config->dpll_hw_state.dpll = dpll;
 
-	dpll_md = (crtc->config.pixel_multiplier - 1)
+	dpll_md = (pipe_config->pixel_multiplier - 1)
 		<< DPLL_MD_UDI_MULTIPLIER_SHIFT;
-	crtc->config.dpll_hw_state.dpll_md = dpll_md;
+	pipe_config->dpll_hw_state.dpll_md = dpll_md;
 }
 
-static void vlv_prepare_pll(struct intel_crtc *crtc)
+static void vlv_prepare_pll(struct intel_crtc *crtc,
+			    const struct intel_crtc_config *pipe_config)
 {
 	struct drm_device *dev = crtc->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -5580,11 +5871,11 @@
 
 	mutex_lock(&dev_priv->dpio_lock);
 
-	bestn = crtc->config.dpll.n;
-	bestm1 = crtc->config.dpll.m1;
-	bestm2 = crtc->config.dpll.m2;
-	bestp1 = crtc->config.dpll.p1;
-	bestp2 = crtc->config.dpll.p2;
+	bestn = pipe_config->dpll.n;
+	bestm1 = pipe_config->dpll.m1;
+	bestm2 = pipe_config->dpll.m2;
+	bestp1 = pipe_config->dpll.p1;
+	bestp2 = pipe_config->dpll.p2;
 
 	/* See eDP HDMI DPIO driver vbios notes doc */
 
@@ -5621,17 +5912,16 @@
 	vlv_dpio_write(dev_priv, pipe, VLV_PLL_DW3(pipe), mdiv);
 
 	/* Set HBR and RBR LPF coefficients */
-	if (crtc->config.port_clock == 162000 ||
-	    intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_ANALOG) ||
-	    intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_HDMI))
+	if (pipe_config->port_clock == 162000 ||
+	    intel_pipe_has_type(crtc, INTEL_OUTPUT_ANALOG) ||
+	    intel_pipe_has_type(crtc, INTEL_OUTPUT_HDMI))
 		vlv_dpio_write(dev_priv, pipe, VLV_PLL_DW10(pipe),
 				 0x009f0003);
 	else
 		vlv_dpio_write(dev_priv, pipe, VLV_PLL_DW10(pipe),
 				 0x00d0000f);
 
-	if (intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_EDP) ||
-	    intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_DISPLAYPORT)) {
+	if (crtc->config.has_dp_encoder) {
 		/* Use SSC source */
 		if (pipe == PIPE_A)
 			vlv_dpio_write(dev_priv, pipe, VLV_PLL_DW5(pipe),
@@ -5651,8 +5941,8 @@
 
 	coreclk = vlv_dpio_read(dev_priv, pipe, VLV_PLL_DW7(pipe));
 	coreclk = (coreclk & 0x0000ff00) | 0x01c00000;
-	if (intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_DISPLAYPORT) ||
-	    intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_EDP))
+	if (intel_pipe_has_type(crtc, INTEL_OUTPUT_DISPLAYPORT) ||
+	    intel_pipe_has_type(crtc, INTEL_OUTPUT_EDP))
 		coreclk |= 0x01000000;
 	vlv_dpio_write(dev_priv, pipe, VLV_PLL_DW7(pipe), coreclk);
 
@@ -5660,7 +5950,21 @@
 	mutex_unlock(&dev_priv->dpio_lock);
 }
 
-static void chv_update_pll(struct intel_crtc *crtc)
+static void chv_update_pll(struct intel_crtc *crtc,
+			   struct intel_crtc_config *pipe_config)
+{
+	pipe_config->dpll_hw_state.dpll = DPLL_SSC_REF_CLOCK_CHV |
+		DPLL_REFA_CLK_ENABLE_VLV | DPLL_VGA_MODE_DIS |
+		DPLL_VCO_ENABLE;
+	if (crtc->pipe != PIPE_A)
+		pipe_config->dpll_hw_state.dpll |= DPLL_INTEGRATED_CRI_CLK_VLV;
+
+	pipe_config->dpll_hw_state.dpll_md =
+		(pipe_config->pixel_multiplier - 1) << DPLL_MD_UDI_MULTIPLIER_SHIFT;
+}
+
+static void chv_prepare_pll(struct intel_crtc *crtc,
+			    const struct intel_crtc_config *pipe_config)
 {
 	struct drm_device *dev = crtc->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -5671,27 +5975,18 @@
 	u32 bestn, bestm1, bestm2, bestp1, bestp2, bestm2_frac;
 	int refclk;
 
-	crtc->config.dpll_hw_state.dpll = DPLL_SSC_REF_CLOCK_CHV |
-		DPLL_REFA_CLK_ENABLE_VLV | DPLL_VGA_MODE_DIS |
-		DPLL_VCO_ENABLE;
-	if (pipe != PIPE_A)
-		crtc->config.dpll_hw_state.dpll |= DPLL_INTEGRATED_CRI_CLK_VLV;
-
-	crtc->config.dpll_hw_state.dpll_md =
-		(crtc->config.pixel_multiplier - 1) << DPLL_MD_UDI_MULTIPLIER_SHIFT;
-
-	bestn = crtc->config.dpll.n;
-	bestm2_frac = crtc->config.dpll.m2 & 0x3fffff;
-	bestm1 = crtc->config.dpll.m1;
-	bestm2 = crtc->config.dpll.m2 >> 22;
-	bestp1 = crtc->config.dpll.p1;
-	bestp2 = crtc->config.dpll.p2;
+	bestn = pipe_config->dpll.n;
+	bestm2_frac = pipe_config->dpll.m2 & 0x3fffff;
+	bestm1 = pipe_config->dpll.m1;
+	bestm2 = pipe_config->dpll.m2 >> 22;
+	bestp1 = pipe_config->dpll.p1;
+	bestp2 = pipe_config->dpll.p2;
 
 	/*
 	 * Enable Refclk and SSC
 	 */
 	I915_WRITE(dpll_reg,
-		   crtc->config.dpll_hw_state.dpll & ~DPLL_VCO_ENABLE);
+		   pipe_config->dpll_hw_state.dpll & ~DPLL_VCO_ENABLE);
 
 	mutex_lock(&dev_priv->dpio_lock);
 
@@ -5719,7 +6014,7 @@
 		       (2 << DPIO_CHV_FEEDFWD_GAIN_SHIFT));
 
 	/* Loop filter */
-	refclk = i9xx_get_refclk(&crtc->base, 0);
+	refclk = i9xx_get_refclk(crtc, 0);
 	loopfilter = 5 << DPIO_CHV_PROP_COEFF_SHIFT |
 		2 << DPIO_CHV_GAIN_CTRL_SHIFT;
 	if (refclk == 100000)
@@ -5739,6 +6034,53 @@
 	mutex_unlock(&dev_priv->dpio_lock);
 }
 
+/**
+ * vlv_force_pll_on - forcibly enable just the PLL
+ * @dev_priv: i915 private structure
+ * @pipe: pipe PLL to enable
+ * @dpll: PLL configuration
+ *
+ * Enable the PLL for @pipe using the supplied @dpll config. To be used
+ * in cases where we need the PLL enabled even when @pipe is not going to
+ * be enabled.
+ */
+void vlv_force_pll_on(struct drm_device *dev, enum pipe pipe,
+		      const struct dpll *dpll)
+{
+	struct intel_crtc *crtc =
+		to_intel_crtc(intel_get_crtc_for_pipe(dev, pipe));
+	struct intel_crtc_config pipe_config = {
+		.pixel_multiplier = 1,
+		.dpll = *dpll,
+	};
+
+	if (IS_CHERRYVIEW(dev)) {
+		chv_update_pll(crtc, &pipe_config);
+		chv_prepare_pll(crtc, &pipe_config);
+		chv_enable_pll(crtc, &pipe_config);
+	} else {
+		vlv_update_pll(crtc, &pipe_config);
+		vlv_prepare_pll(crtc, &pipe_config);
+		vlv_enable_pll(crtc, &pipe_config);
+	}
+}
+
+/**
+ * vlv_force_pll_off - forcibly disable just the PLL
+ * @dev_priv: i915 private structure
+ * @pipe: pipe PLL to disable
+ *
+ * Disable the PLL for @pipe. To be used in cases where we need
+ * the PLL enabled even when @pipe is not going to be enabled.
+ */
+void vlv_force_pll_off(struct drm_device *dev, enum pipe pipe)
+{
+	if (IS_CHERRYVIEW(dev))
+		chv_disable_pll(to_i915(dev), pipe);
+	else
+		vlv_disable_pll(to_i915(dev), pipe);
+}
+
 static void i9xx_update_pll(struct intel_crtc *crtc,
 			    intel_clock_t *reduced_clock,
 			    int num_connectors)
@@ -5747,29 +6089,29 @@
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	u32 dpll;
 	bool is_sdvo;
-	struct dpll *clock = &crtc->config.dpll;
+	struct dpll *clock = &crtc->new_config->dpll;
 
 	i9xx_update_pll_dividers(crtc, reduced_clock);
 
-	is_sdvo = intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_SDVO) ||
-		intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_HDMI);
+	is_sdvo = intel_pipe_will_have_type(crtc, INTEL_OUTPUT_SDVO) ||
+		intel_pipe_will_have_type(crtc, INTEL_OUTPUT_HDMI);
 
 	dpll = DPLL_VGA_MODE_DIS;
 
-	if (intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_LVDS))
+	if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS))
 		dpll |= DPLLB_MODE_LVDS;
 	else
 		dpll |= DPLLB_MODE_DAC_SERIAL;
 
 	if (IS_I945G(dev) || IS_I945GM(dev) || IS_G33(dev)) {
-		dpll |= (crtc->config.pixel_multiplier - 1)
+		dpll |= (crtc->new_config->pixel_multiplier - 1)
 			<< SDVO_MULTIPLIER_SHIFT_HIRES;
 	}
 
 	if (is_sdvo)
 		dpll |= DPLL_SDVO_HIGH_SPEED;
 
-	if (intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_DISPLAYPORT))
+	if (crtc->new_config->has_dp_encoder)
 		dpll |= DPLL_SDVO_HIGH_SPEED;
 
 	/* compute bitmask from p1 value */
@@ -5797,21 +6139,21 @@
 	if (INTEL_INFO(dev)->gen >= 4)
 		dpll |= (6 << PLL_LOAD_PULSE_PHASE_SHIFT);
 
-	if (crtc->config.sdvo_tv_clock)
+	if (crtc->new_config->sdvo_tv_clock)
 		dpll |= PLL_REF_INPUT_TVCLKINBC;
-	else if (intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_LVDS) &&
+	else if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS) &&
 		 intel_panel_use_ssc(dev_priv) && num_connectors < 2)
 		dpll |= PLLB_REF_INPUT_SPREADSPECTRUMIN;
 	else
 		dpll |= PLL_REF_INPUT_DREFCLK;
 
 	dpll |= DPLL_VCO_ENABLE;
-	crtc->config.dpll_hw_state.dpll = dpll;
+	crtc->new_config->dpll_hw_state.dpll = dpll;
 
 	if (INTEL_INFO(dev)->gen >= 4) {
-		u32 dpll_md = (crtc->config.pixel_multiplier - 1)
+		u32 dpll_md = (crtc->new_config->pixel_multiplier - 1)
 			<< DPLL_MD_UDI_MULTIPLIER_SHIFT;
-		crtc->config.dpll_hw_state.dpll_md = dpll_md;
+		crtc->new_config->dpll_hw_state.dpll_md = dpll_md;
 	}
 }
 
@@ -5822,13 +6164,13 @@
 	struct drm_device *dev = crtc->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	u32 dpll;
-	struct dpll *clock = &crtc->config.dpll;
+	struct dpll *clock = &crtc->new_config->dpll;
 
 	i9xx_update_pll_dividers(crtc, reduced_clock);
 
 	dpll = DPLL_VGA_MODE_DIS;
 
-	if (intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_LVDS)) {
+	if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS)) {
 		dpll |= (1 << (clock->p1 - 1)) << DPLL_FPA01_P1_POST_DIV_SHIFT;
 	} else {
 		if (clock->p1 == 2)
@@ -5839,17 +6181,17 @@
 			dpll |= PLL_P2_DIVIDE_BY_4;
 	}
 
-	if (intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_DVO))
+	if (!IS_I830(dev) && intel_pipe_will_have_type(crtc, INTEL_OUTPUT_DVO))
 		dpll |= DPLL_DVO_2X_MODE;
 
-	if (intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_LVDS) &&
+	if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS) &&
 		 intel_panel_use_ssc(dev_priv) && num_connectors < 2)
 		dpll |= PLLB_REF_INPUT_SPREADSPECTRUMIN;
 	else
 		dpll |= PLL_REF_INPUT_DREFCLK;
 
 	dpll |= DPLL_VCO_ENABLE;
-	crtc->config.dpll_hw_state.dpll = dpll;
+	crtc->new_config->dpll_hw_state.dpll = dpll;
 }
 
 static void intel_set_pipe_timings(struct intel_crtc *intel_crtc)
@@ -5873,7 +6215,7 @@
 		crtc_vtotal -= 1;
 		crtc_vblank_end -= 1;
 
-		if (intel_pipe_has_type(&intel_crtc->base, INTEL_OUTPUT_SDVO))
+		if (intel_pipe_has_type(intel_crtc, INTEL_OUTPUT_SDVO))
 			vsyncshift = (adjusted_mode->crtc_htotal - 1) / 2;
 		else
 			vsyncshift = adjusted_mode->crtc_hsync_start -
@@ -5990,9 +6332,9 @@
 
 	pipeconf = 0;
 
-	if (dev_priv->quirks & QUIRK_PIPEA_FORCE &&
-	    I915_READ(PIPECONF(intel_crtc->pipe)) & PIPECONF_ENABLE)
-		pipeconf |= PIPECONF_ENABLE;
+	if ((intel_crtc->pipe == PIPE_A && dev_priv->quirks & QUIRK_PIPEA_FORCE) ||
+	    (intel_crtc->pipe == PIPE_B && dev_priv->quirks & QUIRK_PIPEB_FORCE))
+		pipeconf |= I915_READ(PIPECONF(intel_crtc->pipe)) & PIPECONF_ENABLE;
 
 	if (intel_crtc->config.double_wide)
 		pipeconf |= PIPECONF_DOUBLE_WIDE;
@@ -6031,7 +6373,7 @@
 
 	if (intel_crtc->config.adjusted_mode.flags & DRM_MODE_FLAG_INTERLACE) {
 		if (INTEL_INFO(dev)->gen < 4 ||
-		    intel_pipe_has_type(&intel_crtc->base, INTEL_OUTPUT_SDVO))
+		    intel_pipe_has_type(intel_crtc, INTEL_OUTPUT_SDVO))
 			pipeconf |= PIPECONF_INTERLACE_W_FIELD_INDICATION;
 		else
 			pipeconf |= PIPECONF_INTERLACE_W_SYNC_SHIFT;
@@ -6045,13 +6387,10 @@
 	POSTING_READ(PIPECONF(intel_crtc->pipe));
 }
 
-static int i9xx_crtc_mode_set(struct drm_crtc *crtc,
-			      int x, int y,
-			      struct drm_framebuffer *fb)
+static int i9xx_crtc_compute_clock(struct intel_crtc *crtc)
 {
-	struct drm_device *dev = crtc->dev;
+	struct drm_device *dev = crtc->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 	int refclk, num_connectors = 0;
 	intel_clock_t clock, reduced_clock;
 	bool ok, has_reduced_clock = false;
@@ -6059,7 +6398,10 @@
 	struct intel_encoder *encoder;
 	const intel_limit_t *limit;
 
-	for_each_encoder_on_crtc(dev, crtc, encoder) {
+	for_each_intel_encoder(dev, encoder) {
+		if (encoder->new_crtc != crtc)
+			continue;
+
 		switch (encoder->type) {
 		case INTEL_OUTPUT_LVDS:
 			is_lvds = true;
@@ -6067,6 +6409,8 @@
 		case INTEL_OUTPUT_DSI:
 			is_dsi = true;
 			break;
+		default:
+			break;
 		}
 
 		num_connectors++;
@@ -6075,7 +6419,7 @@
 	if (is_dsi)
 		return 0;
 
-	if (!intel_crtc->config.clock_set) {
+	if (!crtc->new_config->clock_set) {
 		refclk = i9xx_get_refclk(crtc, num_connectors);
 
 		/*
@@ -6086,7 +6430,7 @@
 		 */
 		limit = intel_limit(crtc, refclk);
 		ok = dev_priv->display.find_dpll(limit, crtc,
-						 intel_crtc->config.port_clock,
+						 crtc->new_config->port_clock,
 						 refclk, NULL, &clock);
 		if (!ok) {
 			DRM_ERROR("Couldn't find PLL settings for mode!\n");
@@ -6107,23 +6451,23 @@
 							    &reduced_clock);
 		}
 		/* Compat-code for transition, will disappear. */
-		intel_crtc->config.dpll.n = clock.n;
-		intel_crtc->config.dpll.m1 = clock.m1;
-		intel_crtc->config.dpll.m2 = clock.m2;
-		intel_crtc->config.dpll.p1 = clock.p1;
-		intel_crtc->config.dpll.p2 = clock.p2;
+		crtc->new_config->dpll.n = clock.n;
+		crtc->new_config->dpll.m1 = clock.m1;
+		crtc->new_config->dpll.m2 = clock.m2;
+		crtc->new_config->dpll.p1 = clock.p1;
+		crtc->new_config->dpll.p2 = clock.p2;
 	}
 
 	if (IS_GEN2(dev)) {
-		i8xx_update_pll(intel_crtc,
+		i8xx_update_pll(crtc,
 				has_reduced_clock ? &reduced_clock : NULL,
 				num_connectors);
 	} else if (IS_CHERRYVIEW(dev)) {
-		chv_update_pll(intel_crtc);
+		chv_update_pll(crtc, crtc->new_config);
 	} else if (IS_VALLEYVIEW(dev)) {
-		vlv_update_pll(intel_crtc);
+		vlv_update_pll(crtc, crtc->new_config);
 	} else {
-		i9xx_update_pll(intel_crtc,
+		i9xx_update_pll(crtc,
 				has_reduced_clock ? &reduced_clock : NULL,
 				num_connectors);
 	}
@@ -6235,7 +6579,7 @@
 	crtc->base.primary->fb->height = ((val >> 0) & 0xfff) + 1;
 
 	val = I915_READ(DSPSTRIDE(pipe));
-	crtc->base.primary->fb->pitches[0] = val & 0xffffff80;
+	crtc->base.primary->fb->pitches[0] = val & 0xffffffc0;
 
 	aligned_height = intel_align_height(dev, crtc->base.primary->fb->height,
 					    plane_config->tiled);
@@ -6289,8 +6633,8 @@
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	uint32_t tmp;
 
-	if (!intel_display_power_enabled(dev_priv,
-					 POWER_DOMAIN_PIPE(crtc->pipe)))
+	if (!intel_display_power_is_enabled(dev_priv,
+					    POWER_DOMAIN_PIPE(crtc->pipe)))
 		return false;
 
 	pipe_config->cpu_transcoder = (enum transcoder) crtc->pipe;
@@ -6345,6 +6689,14 @@
 	}
 	pipe_config->dpll_hw_state.dpll = I915_READ(DPLL(crtc->pipe));
 	if (!IS_VALLEYVIEW(dev)) {
+		/*
+		 * DPLL_DVO_2X_MODE must be enabled for both DPLLs
+		 * on 830. Filter it out here so that we don't
+		 * report errors due to that.
+		 */
+		if (IS_I830(dev))
+			pipe_config->dpll_hw_state.dpll &= ~DPLL_DVO_2X_MODE;
+
 		pipe_config->dpll_hw_state.fp0 = I915_READ(FP0(crtc->pipe));
 		pipe_config->dpll_hw_state.fp1 = I915_READ(FP1(crtc->pipe));
 	} else {
@@ -6367,7 +6719,6 @@
 static void ironlake_init_pch_refclk(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_mode_config *mode_config = &dev->mode_config;
 	struct intel_encoder *encoder;
 	u32 val, final;
 	bool has_lvds = false;
@@ -6377,8 +6728,7 @@
 	bool can_ssc = false;
 
 	/* We need to take the global config into account */
-	list_for_each_entry(encoder, &mode_config->encoder_list,
-			    base.head) {
+	for_each_intel_encoder(dev, encoder) {
 		switch (encoder->type) {
 		case INTEL_OUTPUT_LVDS:
 			has_panel = true;
@@ -6389,6 +6739,8 @@
 			if (enc_to_dig_port(&encoder->base)->port == PORT_A)
 				has_cpu_edp = true;
 			break;
+		default:
+			break;
 		}
 	}
 
@@ -6685,15 +7037,16 @@
 
 static void lpt_init_pch_refclk(struct drm_device *dev)
 {
-	struct drm_mode_config *mode_config = &dev->mode_config;
 	struct intel_encoder *encoder;
 	bool has_vga = false;
 
-	list_for_each_entry(encoder, &mode_config->encoder_list, base.head) {
+	for_each_intel_encoder(dev, encoder) {
 		switch (encoder->type) {
 		case INTEL_OUTPUT_ANALOG:
 			has_vga = true;
 			break;
+		default:
+			break;
 		}
 	}
 
@@ -6722,11 +7075,16 @@
 	int num_connectors = 0;
 	bool is_lvds = false;
 
-	for_each_encoder_on_crtc(dev, crtc, encoder) {
+	for_each_intel_encoder(dev, encoder) {
+		if (encoder->new_crtc != to_intel_crtc(crtc))
+			continue;
+
 		switch (encoder->type) {
 		case INTEL_OUTPUT_LVDS:
 			is_lvds = true;
 			break;
+		default:
+			break;
 		}
 		num_connectors++;
 	}
@@ -6871,7 +7229,7 @@
 	I915_WRITE(GAMMA_MODE(intel_crtc->pipe), GAMMA_MODE_MODE_8BIT);
 	POSTING_READ(GAMMA_MODE(intel_crtc->pipe));
 
-	if (IS_BROADWELL(dev)) {
+	if (IS_BROADWELL(dev) || INTEL_INFO(dev)->gen >= 9) {
 		val = 0;
 
 		switch (intel_crtc->config.pipe_bpp) {
@@ -6906,18 +7264,12 @@
 {
 	struct drm_device *dev = crtc->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_encoder *intel_encoder;
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 	int refclk;
 	const intel_limit_t *limit;
 	bool ret, is_lvds = false;
 
-	for_each_encoder_on_crtc(dev, crtc, intel_encoder) {
-		switch (intel_encoder->type) {
-		case INTEL_OUTPUT_LVDS:
-			is_lvds = true;
-			break;
-		}
-	}
+	is_lvds = intel_pipe_will_have_type(intel_crtc, INTEL_OUTPUT_LVDS);
 
 	refclk = ironlake_get_refclk(crtc);
 
@@ -6926,9 +7278,9 @@
 	 * refclk, or FALSE.  The returned values represent the clock equation:
 	 * reflck * (5 * (m1 + 2) + (m2 + 2)) / (n + 2) / p1 / p2.
 	 */
-	limit = intel_limit(crtc, refclk);
-	ret = dev_priv->display.find_dpll(limit, crtc,
-					  to_intel_crtc(crtc)->config.port_clock,
+	limit = intel_limit(intel_crtc, refclk);
+	ret = dev_priv->display.find_dpll(limit, intel_crtc,
+					  intel_crtc->new_config->port_clock,
 					  refclk, NULL, clock);
 	if (!ret)
 		return false;
@@ -6941,7 +7293,7 @@
 		 * downclock feature.
 		*/
 		*has_reduced_clock =
-			dev_priv->display.find_dpll(limit, crtc,
+			dev_priv->display.find_dpll(limit, intel_crtc,
 						    dev_priv->lvds_downclock,
 						    refclk, clock,
 						    reduced_clock);
@@ -6978,7 +7330,10 @@
 	int factor, num_connectors = 0;
 	bool is_lvds = false, is_sdvo = false;
 
-	for_each_encoder_on_crtc(dev, crtc, intel_encoder) {
+	for_each_intel_encoder(dev, intel_encoder) {
+		if (intel_encoder->new_crtc != to_intel_crtc(crtc))
+			continue;
+
 		switch (intel_encoder->type) {
 		case INTEL_OUTPUT_LVDS:
 			is_lvds = true;
@@ -6987,6 +7342,8 @@
 		case INTEL_OUTPUT_HDMI:
 			is_sdvo = true;
 			break;
+		default:
+			break;
 		}
 
 		num_connectors++;
@@ -6999,10 +7356,10 @@
 		     dev_priv->vbt.lvds_ssc_freq == 100000) ||
 		    (HAS_PCH_IBX(dev) && intel_is_dual_link_lvds(dev)))
 			factor = 25;
-	} else if (intel_crtc->config.sdvo_tv_clock)
+	} else if (intel_crtc->new_config->sdvo_tv_clock)
 		factor = 20;
 
-	if (ironlake_needs_fb_cb_tune(&intel_crtc->config.dpll, factor))
+	if (ironlake_needs_fb_cb_tune(&intel_crtc->new_config->dpll, factor))
 		*fp |= FP_CB_TUNE;
 
 	if (fp2 && (reduced_clock->m < factor * reduced_clock->n))
@@ -7015,20 +7372,20 @@
 	else
 		dpll |= DPLLB_MODE_DAC_SERIAL;
 
-	dpll |= (intel_crtc->config.pixel_multiplier - 1)
+	dpll |= (intel_crtc->new_config->pixel_multiplier - 1)
 		<< PLL_REF_SDVO_HDMI_MULTIPLIER_SHIFT;
 
 	if (is_sdvo)
 		dpll |= DPLL_SDVO_HIGH_SPEED;
-	if (intel_crtc->config.has_dp_encoder)
+	if (intel_crtc->new_config->has_dp_encoder)
 		dpll |= DPLL_SDVO_HIGH_SPEED;
 
 	/* compute bitmask from p1 value */
-	dpll |= (1 << (intel_crtc->config.dpll.p1 - 1)) << DPLL_FPA01_P1_POST_DIV_SHIFT;
+	dpll |= (1 << (intel_crtc->new_config->dpll.p1 - 1)) << DPLL_FPA01_P1_POST_DIV_SHIFT;
 	/* also FPA1 */
-	dpll |= (1 << (intel_crtc->config.dpll.p1 - 1)) << DPLL_FPA1_P1_POST_DIV_SHIFT;
+	dpll |= (1 << (intel_crtc->new_config->dpll.p1 - 1)) << DPLL_FPA1_P1_POST_DIV_SHIFT;
 
-	switch (intel_crtc->config.dpll.p2) {
+	switch (intel_crtc->new_config->dpll.p2) {
 	case 5:
 		dpll |= DPLL_DAC_SERIAL_P2_CLOCK_DIV_5;
 		break;
@@ -7051,78 +7408,64 @@
 	return dpll | DPLL_VCO_ENABLE;
 }
 
-static int ironlake_crtc_mode_set(struct drm_crtc *crtc,
-				  int x, int y,
-				  struct drm_framebuffer *fb)
+static int ironlake_crtc_compute_clock(struct intel_crtc *crtc)
 {
-	struct drm_device *dev = crtc->dev;
-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
-	int num_connectors = 0;
+	struct drm_device *dev = crtc->base.dev;
 	intel_clock_t clock, reduced_clock;
 	u32 dpll = 0, fp = 0, fp2 = 0;
 	bool ok, has_reduced_clock = false;
 	bool is_lvds = false;
-	struct intel_encoder *encoder;
 	struct intel_shared_dpll *pll;
 
-	for_each_encoder_on_crtc(dev, crtc, encoder) {
-		switch (encoder->type) {
-		case INTEL_OUTPUT_LVDS:
-			is_lvds = true;
-			break;
-		}
-
-		num_connectors++;
-	}
+	is_lvds = intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS);
 
 	WARN(!(HAS_PCH_IBX(dev) || HAS_PCH_CPT(dev)),
 	     "Unexpected PCH type %d\n", INTEL_PCH_TYPE(dev));
 
-	ok = ironlake_compute_clocks(crtc, &clock,
+	ok = ironlake_compute_clocks(&crtc->base, &clock,
 				     &has_reduced_clock, &reduced_clock);
-	if (!ok && !intel_crtc->config.clock_set) {
+	if (!ok && !crtc->new_config->clock_set) {
 		DRM_ERROR("Couldn't find PLL settings for mode!\n");
 		return -EINVAL;
 	}
 	/* Compat-code for transition, will disappear. */
-	if (!intel_crtc->config.clock_set) {
-		intel_crtc->config.dpll.n = clock.n;
-		intel_crtc->config.dpll.m1 = clock.m1;
-		intel_crtc->config.dpll.m2 = clock.m2;
-		intel_crtc->config.dpll.p1 = clock.p1;
-		intel_crtc->config.dpll.p2 = clock.p2;
+	if (!crtc->new_config->clock_set) {
+		crtc->new_config->dpll.n = clock.n;
+		crtc->new_config->dpll.m1 = clock.m1;
+		crtc->new_config->dpll.m2 = clock.m2;
+		crtc->new_config->dpll.p1 = clock.p1;
+		crtc->new_config->dpll.p2 = clock.p2;
 	}
 
 	/* CPU eDP is the only output that doesn't need a PCH PLL of its own. */
-	if (intel_crtc->config.has_pch_encoder) {
-		fp = i9xx_dpll_compute_fp(&intel_crtc->config.dpll);
+	if (crtc->new_config->has_pch_encoder) {
+		fp = i9xx_dpll_compute_fp(&crtc->new_config->dpll);
 		if (has_reduced_clock)
 			fp2 = i9xx_dpll_compute_fp(&reduced_clock);
 
-		dpll = ironlake_compute_dpll(intel_crtc,
+		dpll = ironlake_compute_dpll(crtc,
 					     &fp, &reduced_clock,
 					     has_reduced_clock ? &fp2 : NULL);
 
-		intel_crtc->config.dpll_hw_state.dpll = dpll;
-		intel_crtc->config.dpll_hw_state.fp0 = fp;
+		crtc->new_config->dpll_hw_state.dpll = dpll;
+		crtc->new_config->dpll_hw_state.fp0 = fp;
 		if (has_reduced_clock)
-			intel_crtc->config.dpll_hw_state.fp1 = fp2;
+			crtc->new_config->dpll_hw_state.fp1 = fp2;
 		else
-			intel_crtc->config.dpll_hw_state.fp1 = fp;
+			crtc->new_config->dpll_hw_state.fp1 = fp;
 
-		pll = intel_get_shared_dpll(intel_crtc);
+		pll = intel_get_shared_dpll(crtc);
 		if (pll == NULL) {
 			DRM_DEBUG_DRIVER("failed to find PLL for pipe %c\n",
-					 pipe_name(intel_crtc->pipe));
+					 pipe_name(crtc->pipe));
 			return -EINVAL;
 		}
-	} else
-		intel_put_shared_dpll(intel_crtc);
+	}
 
-	if (is_lvds && has_reduced_clock && i915.powersave)
-		intel_crtc->lowfreq_avail = true;
+	if (is_lvds && has_reduced_clock && i915_module.powersave)
+		crtc->lowfreq_avail = true;
 	else
-		intel_crtc->lowfreq_avail = false;
+		crtc->lowfreq_avail = false;
 
 	return 0;
 }
@@ -7145,7 +7488,8 @@
 
 static void intel_cpu_transcoder_get_m_n(struct intel_crtc *crtc,
 					 enum transcoder transcoder,
-					 struct intel_link_m_n *m_n)
+					 struct intel_link_m_n *m_n,
+					 struct intel_link_m_n *m2_n2)
 {
 	struct drm_device *dev = crtc->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -7159,6 +7503,20 @@
 		m_n->gmch_n = I915_READ(PIPE_DATA_N1(transcoder));
 		m_n->tu = ((I915_READ(PIPE_DATA_M1(transcoder))
 			    & TU_SIZE_MASK) >> TU_SIZE_SHIFT) + 1;
+		/* Read M2_N2 registers only for gen < 8 (M2_N2 available for
+		 * gen < 8) and if DRRS is supported (to make sure the
+		 * registers are not unnecessarily read).
+		 */
+		if (m2_n2 && INTEL_INFO(dev)->gen < 8 &&
+			crtc->config.has_drrs) {
+			m2_n2->link_m = I915_READ(PIPE_LINK_M2(transcoder));
+			m2_n2->link_n =	I915_READ(PIPE_LINK_N2(transcoder));
+			m2_n2->gmch_m =	I915_READ(PIPE_DATA_M2(transcoder))
+					& ~TU_SIZE_MASK;
+			m2_n2->gmch_n =	I915_READ(PIPE_DATA_N2(transcoder));
+			m2_n2->tu = ((I915_READ(PIPE_DATA_M2(transcoder))
+					& TU_SIZE_MASK) >> TU_SIZE_SHIFT) + 1;
+		}
 	} else {
 		m_n->link_m = I915_READ(PIPE_LINK_M_G4X(pipe));
 		m_n->link_n = I915_READ(PIPE_LINK_N_G4X(pipe));
@@ -7177,14 +7535,31 @@
 		intel_pch_transcoder_get_m_n(crtc, &pipe_config->dp_m_n);
 	else
 		intel_cpu_transcoder_get_m_n(crtc, pipe_config->cpu_transcoder,
-					     &pipe_config->dp_m_n);
+					     &pipe_config->dp_m_n,
+					     &pipe_config->dp_m2_n2);
 }
 
 static void ironlake_get_fdi_m_n_config(struct intel_crtc *crtc,
 					struct intel_crtc_config *pipe_config)
 {
 	intel_cpu_transcoder_get_m_n(crtc, pipe_config->cpu_transcoder,
-				     &pipe_config->fdi_m_n);
+				     &pipe_config->fdi_m_n, NULL);
+}
+
+static void skylake_get_pfit_config(struct intel_crtc *crtc,
+				    struct intel_crtc_config *pipe_config)
+{
+	struct drm_device *dev = crtc->base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	uint32_t tmp;
+
+	tmp = I915_READ(PS_CTL(crtc->pipe));
+
+	if (tmp & PS_ENABLE) {
+		pipe_config->pch_pfit.enabled = true;
+		pipe_config->pch_pfit.pos = I915_READ(PS_WIN_POS(crtc->pipe));
+		pipe_config->pch_pfit.size = I915_READ(PS_WIN_SZ(crtc->pipe));
+	}
 }
 
 static void ironlake_get_pfit_config(struct intel_crtc *crtc,
@@ -7255,7 +7630,7 @@
 	crtc->base.primary->fb->height = ((val >> 0) & 0xfff) + 1;
 
 	val = I915_READ(DSPSTRIDE(pipe));
-	crtc->base.primary->fb->pitches[0] = val & 0xffffff80;
+	crtc->base.primary->fb->pitches[0] = val & 0xffffffc0;
 
 	aligned_height = intel_align_height(dev, crtc->base.primary->fb->height,
 					    plane_config->tiled);
@@ -7278,8 +7653,8 @@
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	uint32_t tmp;
 
-	if (!intel_display_power_enabled(dev_priv,
-					 POWER_DOMAIN_PIPE(crtc->pipe)))
+	if (!intel_display_power_is_enabled(dev_priv,
+					    POWER_DOMAIN_PIPE(crtc->pipe)))
 		return false;
 
 	pipe_config->cpu_transcoder = (enum transcoder) crtc->pipe;
@@ -7472,7 +7847,6 @@
 static void hsw_restore_lcpll(struct drm_i915_private *dev_priv)
 {
 	uint32_t val;
-	unsigned long irqflags;
 
 	val = I915_READ(LCPLL_CTL);
 
@@ -7483,19 +7857,8 @@
 	/*
 	 * Make sure we're not on PC8 state before disabling PC8, otherwise
 	 * we'll hang the machine. To prevent PC8 state, just enable force_wake.
-	 *
-	 * The other problem is that hsw_restore_lcpll() is called as part of
-	 * the runtime PM resume sequence, so we can't just call
-	 * gen6_gt_force_wake_get() because that function calls
-	 * intel_runtime_pm_get(), and we can't change the runtime PM refcount
-	 * while we are on the resume sequence. So to solve this problem we have
-	 * to call special forcewake code that doesn't touch runtime PM and
-	 * doesn't enable the forcewake delayed work.
-	 */
-	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
-	if (dev_priv->uncore.forcewake_count++ == 0)
-		dev_priv->uncore.funcs.force_wake_get(dev_priv, FORCEWAKE_ALL);
-	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
+	 */
+	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
 
 	if (val & LCPLL_POWER_DOWN_ALLOW) {
 		val &= ~LCPLL_POWER_DOWN_ALLOW;
@@ -7525,11 +7888,7 @@
 			DRM_ERROR("Switching back to LCPLL failed\n");
 	}
 
-	/* See the big comment above. */
-	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
-	if (--dev_priv->uncore.forcewake_count == 0)
-		dev_priv->uncore.funcs.force_wake_put(dev_priv, FORCEWAKE_ALL);
-	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
+	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
 }
 
 /*
@@ -7591,28 +7950,52 @@
 	intel_prepare_ddi(dev);
 }
 
-static void snb_modeset_global_resources(struct drm_device *dev)
+static int haswell_crtc_compute_clock(struct intel_crtc *crtc)
 {
-	modeset_update_crtc_power_domains(dev);
-}
+	if (!intel_ddi_pll_select(crtc))
+		return -EINVAL;
 
-static void haswell_modeset_global_resources(struct drm_device *dev)
-{
-	modeset_update_crtc_power_domains(dev);
+	crtc->lowfreq_avail = false;
+
+	return 0;
 }
 
-static int haswell_crtc_mode_set(struct drm_crtc *crtc,
-				 int x, int y,
-				 struct drm_framebuffer *fb)
+static void skylake_get_ddi_pll(struct drm_i915_private *dev_priv,
+				enum port port,
+				struct intel_crtc_config *pipe_config)
 {
-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	u32 temp;
 
-	if (!intel_ddi_pll_select(intel_crtc))
-		return -EINVAL;
+	temp = I915_READ(DPLL_CTRL2) & DPLL_CTRL2_DDI_CLK_SEL_MASK(port);
+	pipe_config->ddi_pll_sel = temp >> (port * 3 + 1);
+
+	switch (pipe_config->ddi_pll_sel) {
+	case SKL_DPLL1:
+		pipe_config->shared_dpll = DPLL_ID_SKL_DPLL1;
+		break;
+	case SKL_DPLL2:
+		pipe_config->shared_dpll = DPLL_ID_SKL_DPLL2;
+		break;
+	case SKL_DPLL3:
+		pipe_config->shared_dpll = DPLL_ID_SKL_DPLL3;
+		break;
+	}
+}
 
-	intel_crtc->lowfreq_avail = false;
+static void haswell_get_ddi_pll(struct drm_i915_private *dev_priv,
+				enum port port,
+				struct intel_crtc_config *pipe_config)
+{
+	pipe_config->ddi_pll_sel = I915_READ(PORT_CLK_SEL(port));
 
-	return 0;
+	switch (pipe_config->ddi_pll_sel) {
+	case PORT_CLK_SEL_WRPLL1:
+		pipe_config->shared_dpll = DPLL_ID_WRPLL1;
+		break;
+	case PORT_CLK_SEL_WRPLL2:
+		pipe_config->shared_dpll = DPLL_ID_WRPLL2;
+		break;
+	}
 }
 
 static void haswell_get_ddi_port_state(struct intel_crtc *crtc,
@@ -7628,16 +8011,10 @@
 
 	port = (tmp & TRANS_DDI_PORT_MASK) >> TRANS_DDI_PORT_SHIFT;
 
-	pipe_config->ddi_pll_sel = I915_READ(PORT_CLK_SEL(port));
-
-	switch (pipe_config->ddi_pll_sel) {
-	case PORT_CLK_SEL_WRPLL1:
-		pipe_config->shared_dpll = DPLL_ID_WRPLL1;
-		break;
-	case PORT_CLK_SEL_WRPLL2:
-		pipe_config->shared_dpll = DPLL_ID_WRPLL2;
-		break;
-	}
+	if (IS_SKYLAKE(dev))
+		skylake_get_ddi_pll(dev_priv, port, pipe_config);
+	else
+		haswell_get_ddi_pll(dev_priv, port, pipe_config);
 
 	if (pipe_config->shared_dpll >= 0) {
 		pll = &dev_priv->shared_dplls[pipe_config->shared_dpll];
@@ -7651,7 +8028,8 @@
 	 * DDI E. So just check whether this pipe is wired to DDI E and whether
 	 * the PCH transcoder is on.
 	 */
-	if ((port == PORT_E) && I915_READ(LPT_TRANSCONF) & TRANS_ENABLE) {
+	if (INTEL_INFO(dev)->gen < 9 &&
+	    (port == PORT_E) && I915_READ(LPT_TRANSCONF) & TRANS_ENABLE) {
 		pipe_config->has_pch_encoder = true;
 
 		tmp = I915_READ(FDI_RX_CTL(PIPE_A));
@@ -7670,7 +8048,7 @@
 	enum intel_display_power_domain pfit_domain;
 	uint32_t tmp;
 
-	if (!intel_display_power_enabled(dev_priv,
+	if (!intel_display_power_is_enabled(dev_priv,
 					 POWER_DOMAIN_PIPE(crtc->pipe)))
 		return false;
 
@@ -7699,7 +8077,7 @@
 			pipe_config->cpu_transcoder = TRANSCODER_EDP;
 	}
 
-	if (!intel_display_power_enabled(dev_priv,
+	if (!intel_display_power_is_enabled(dev_priv,
 			POWER_DOMAIN_TRANSCODER(pipe_config->cpu_transcoder)))
 		return false;
 
@@ -7712,361 +8090,90 @@
 	intel_get_pipe_timings(crtc, pipe_config);
 
 	pfit_domain = POWER_DOMAIN_PIPE_PANEL_FITTER(crtc->pipe);
-	if (intel_display_power_enabled(dev_priv, pfit_domain))
-		ironlake_get_pfit_config(crtc, pipe_config);
+	if (intel_display_power_is_enabled(dev_priv, pfit_domain)) {
+		if (IS_SKYLAKE(dev))
+			skylake_get_pfit_config(crtc, pipe_config);
+		else
+			ironlake_get_pfit_config(crtc, pipe_config);
+	}
 
 	if (IS_HASWELL(dev))
 		pipe_config->ips_enabled = hsw_crtc_supports_ips(crtc) &&
 			(I915_READ(IPS_CTL) & IPS_ENABLE);
 
-	pipe_config->pixel_multiplier = 1;
+	if (pipe_config->cpu_transcoder != TRANSCODER_EDP) {
+		pipe_config->pixel_multiplier =
+			I915_READ(PIPE_MULT(pipe_config->cpu_transcoder)) + 1;
+	} else {
+		pipe_config->pixel_multiplier = 1;
+	}
 
 	return true;
 }
 
-static struct {
-	int clock;
-	u32 config;
-} hdmi_audio_clock[] = {
-	{ DIV_ROUND_UP(25200 * 1000, 1001), AUD_CONFIG_PIXEL_CLOCK_HDMI_25175 },
-	{ 25200, AUD_CONFIG_PIXEL_CLOCK_HDMI_25200 }, /* default per bspec */
-	{ 27000, AUD_CONFIG_PIXEL_CLOCK_HDMI_27000 },
-	{ 27000 * 1001 / 1000, AUD_CONFIG_PIXEL_CLOCK_HDMI_27027 },
-	{ 54000, AUD_CONFIG_PIXEL_CLOCK_HDMI_54000 },
-	{ 54000 * 1001 / 1000, AUD_CONFIG_PIXEL_CLOCK_HDMI_54054 },
-	{ DIV_ROUND_UP(74250 * 1000, 1001), AUD_CONFIG_PIXEL_CLOCK_HDMI_74176 },
-	{ 74250, AUD_CONFIG_PIXEL_CLOCK_HDMI_74250 },
-	{ DIV_ROUND_UP(148500 * 1000, 1001), AUD_CONFIG_PIXEL_CLOCK_HDMI_148352 },
-	{ 148500, AUD_CONFIG_PIXEL_CLOCK_HDMI_148500 },
-};
-
-/* get AUD_CONFIG_PIXEL_CLOCK_HDMI_* value for mode */
-static u32 audio_config_hdmi_pixel_clock(struct drm_display_mode *mode)
+static void i845_update_cursor(struct drm_crtc *crtc, u32 base)
 {
-	int i;
+	struct drm_device *dev = crtc->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	uint32_t cntl = 0, size = 0;
+
+	if (base) {
+		unsigned int width = intel_crtc->cursor_width;
+		unsigned int height = intel_crtc->cursor_height;
+		unsigned int stride = roundup_pow_of_two(width) * 4;
 
-	for (i = 0; i < ARRAY_SIZE(hdmi_audio_clock); i++) {
-		if (mode->clock == hdmi_audio_clock[i].clock)
+		switch (stride) {
+		default:
+			WARN_ONCE(1, "Invalid cursor width/stride, width=%u, stride=%u\n",
+				  width, stride);
+			stride = 256;
+			/* fallthrough */
+		case 256:
+		case 512:
+		case 1024:
+		case 2048:
 			break;
+		}
+
+		cntl |= CURSOR_ENABLE |
+			CURSOR_GAMMA_ENABLE |
+			CURSOR_FORMAT_ARGB |
+			CURSOR_STRIDE(stride);
+
+		size = (height << 12) | width;
+	}
+
+	if (intel_crtc->cursor_cntl != 0 &&
+	    (intel_crtc->cursor_base != base ||
+	     intel_crtc->cursor_size != size ||
+	     intel_crtc->cursor_cntl != cntl)) {
+		/* On these chipsets we can only modify the base/size/stride
+		 * whilst the cursor is disabled.
+		 */
+		I915_WRITE(_CURACNTR, 0);
+		POSTING_READ(_CURACNTR);
+		intel_crtc->cursor_cntl = 0;
 	}
 
-	if (i == ARRAY_SIZE(hdmi_audio_clock)) {
-		DRM_DEBUG_KMS("HDMI audio pixel clock setting for %d not found, falling back to defaults\n", mode->clock);
-		i = 1;
+	if (intel_crtc->cursor_base != base) {
+		I915_WRITE(_CURABASE, base);
+		intel_crtc->cursor_base = base;
 	}
 
-	DRM_DEBUG_KMS("Configuring HDMI audio for pixel clock %d (0x%08x)\n",
-		      hdmi_audio_clock[i].clock,
-		      hdmi_audio_clock[i].config);
+	if (intel_crtc->cursor_size != size) {
+		I915_WRITE(CURSIZE, size);
+		intel_crtc->cursor_size = size;
+	}
 
-	return hdmi_audio_clock[i].config;
+	if (intel_crtc->cursor_cntl != cntl) {
+		I915_WRITE(_CURACNTR, cntl);
+		POSTING_READ(_CURACNTR);
+		intel_crtc->cursor_cntl = cntl;
+	}
 }
 
-static bool intel_eld_uptodate(struct drm_connector *connector,
-			       int reg_eldv, uint32_t bits_eldv,
-			       int reg_elda, uint32_t bits_elda,
-			       int reg_edid)
-{
-	struct drm_i915_private *dev_priv = connector->dev->dev_private;
-	uint8_t *eld = connector->eld;
-	uint32_t i;
-
-	i = I915_READ(reg_eldv);
-	i &= bits_eldv;
-
-	if (!eld[0])
-		return !i;
-
-	if (!i)
-		return false;
-
-	i = I915_READ(reg_elda);
-	i &= ~bits_elda;
-	I915_WRITE(reg_elda, i);
-
-	for (i = 0; i < eld[2]; i++)
-		if (I915_READ(reg_edid) != *((uint32_t *)eld + i))
-			return false;
-
-	return true;
-}
-
-static void g4x_write_eld(struct drm_connector *connector,
-			  struct drm_crtc *crtc,
-			  struct drm_display_mode *mode)
-{
-	struct drm_i915_private *dev_priv = connector->dev->dev_private;
-	uint8_t *eld = connector->eld;
-	uint32_t eldv;
-	uint32_t len;
-	uint32_t i;
-
-	i = I915_READ(G4X_AUD_VID_DID);
-
-	if (i == INTEL_AUDIO_DEVBLC || i == INTEL_AUDIO_DEVCL)
-		eldv = G4X_ELDV_DEVCL_DEVBLC;
-	else
-		eldv = G4X_ELDV_DEVCTG;
-
-	if (intel_eld_uptodate(connector,
-			       G4X_AUD_CNTL_ST, eldv,
-			       G4X_AUD_CNTL_ST, G4X_ELD_ADDR,
-			       G4X_HDMIW_HDMIEDID))
-		return;
-
-	i = I915_READ(G4X_AUD_CNTL_ST);
-	i &= ~(eldv | G4X_ELD_ADDR);
-	len = (i >> 9) & 0x1f;		/* ELD buffer size */
-	I915_WRITE(G4X_AUD_CNTL_ST, i);
-
-	if (!eld[0])
-		return;
-
-	len = min_t(uint8_t, eld[2], len);
-	DRM_DEBUG_DRIVER("ELD size %d\n", len);
-	for (i = 0; i < len; i++)
-		I915_WRITE(G4X_HDMIW_HDMIEDID, *((uint32_t *)eld + i));
-
-	i = I915_READ(G4X_AUD_CNTL_ST);
-	i |= eldv;
-	I915_WRITE(G4X_AUD_CNTL_ST, i);
-}
-
-static void haswell_write_eld(struct drm_connector *connector,
-			      struct drm_crtc *crtc,
-			      struct drm_display_mode *mode)
-{
-	struct drm_i915_private *dev_priv = connector->dev->dev_private;
-	uint8_t *eld = connector->eld;
-	uint32_t eldv;
-	uint32_t i;
-	int len;
-	int pipe = to_intel_crtc(crtc)->pipe;
-	int tmp;
-
-	int hdmiw_hdmiedid = HSW_AUD_EDID_DATA(pipe);
-	int aud_cntl_st = HSW_AUD_DIP_ELD_CTRL(pipe);
-	int aud_config = HSW_AUD_CFG(pipe);
-	int aud_cntrl_st2 = HSW_AUD_PIN_ELD_CP_VLD;
-
-	/* Audio output enable */
-	DRM_DEBUG_DRIVER("HDMI audio: enable codec\n");
-	tmp = I915_READ(aud_cntrl_st2);
-	tmp |= (AUDIO_OUTPUT_ENABLE_A << (pipe * 4));
-	I915_WRITE(aud_cntrl_st2, tmp);
-	POSTING_READ(aud_cntrl_st2);
-
-	assert_pipe_disabled(dev_priv, to_intel_crtc(crtc)->pipe);
-
-	/* Set ELD valid state */
-	tmp = I915_READ(aud_cntrl_st2);
-	DRM_DEBUG_DRIVER("HDMI audio: pin eld vld status=0x%08x\n", tmp);
-	tmp |= (AUDIO_ELD_VALID_A << (pipe * 4));
-	I915_WRITE(aud_cntrl_st2, tmp);
-	tmp = I915_READ(aud_cntrl_st2);
-	DRM_DEBUG_DRIVER("HDMI audio: eld vld status=0x%08x\n", tmp);
-
-	/* Enable HDMI mode */
-	tmp = I915_READ(aud_config);
-	DRM_DEBUG_DRIVER("HDMI audio: audio conf: 0x%08x\n", tmp);
-	/* clear N_programing_enable and N_value_index */
-	tmp &= ~(AUD_CONFIG_N_VALUE_INDEX | AUD_CONFIG_N_PROG_ENABLE);
-	I915_WRITE(aud_config, tmp);
-
-	DRM_DEBUG_DRIVER("ELD on pipe %c\n", pipe_name(pipe));
-
-	eldv = AUDIO_ELD_VALID_A << (pipe * 4);
-
-	if (intel_pipe_has_type(crtc, INTEL_OUTPUT_DISPLAYPORT)) {
-		DRM_DEBUG_DRIVER("ELD: DisplayPort detected\n");
-		eld[5] |= (1 << 2);	/* Conn_Type, 0x1 = DisplayPort */
-		I915_WRITE(aud_config, AUD_CONFIG_N_VALUE_INDEX); /* 0x1 = DP */
-	} else {
-		I915_WRITE(aud_config, audio_config_hdmi_pixel_clock(mode));
-	}
-
-	if (intel_eld_uptodate(connector,
-			       aud_cntrl_st2, eldv,
-			       aud_cntl_st, IBX_ELD_ADDRESS,
-			       hdmiw_hdmiedid))
-		return;
-
-	i = I915_READ(aud_cntrl_st2);
-	i &= ~eldv;
-	I915_WRITE(aud_cntrl_st2, i);
-
-	if (!eld[0])
-		return;
-
-	i = I915_READ(aud_cntl_st);
-	i &= ~IBX_ELD_ADDRESS;
-	I915_WRITE(aud_cntl_st, i);
-	i = (i >> 29) & DIP_PORT_SEL_MASK;		/* DIP_Port_Select, 0x1 = PortB */
-	DRM_DEBUG_DRIVER("port num:%d\n", i);
-
-	len = min_t(uint8_t, eld[2], 21);	/* 84 bytes of hw ELD buffer */
-	DRM_DEBUG_DRIVER("ELD size %d\n", len);
-	for (i = 0; i < len; i++)
-		I915_WRITE(hdmiw_hdmiedid, *((uint32_t *)eld + i));
-
-	i = I915_READ(aud_cntrl_st2);
-	i |= eldv;
-	I915_WRITE(aud_cntrl_st2, i);
-
-}
-
-static void ironlake_write_eld(struct drm_connector *connector,
-			       struct drm_crtc *crtc,
-			       struct drm_display_mode *mode)
-{
-	struct drm_i915_private *dev_priv = connector->dev->dev_private;
-	uint8_t *eld = connector->eld;
-	uint32_t eldv;
-	uint32_t i;
-	int len;
-	int hdmiw_hdmiedid;
-	int aud_config;
-	int aud_cntl_st;
-	int aud_cntrl_st2;
-	int pipe = to_intel_crtc(crtc)->pipe;
-
-	if (HAS_PCH_IBX(connector->dev)) {
-		hdmiw_hdmiedid = IBX_HDMIW_HDMIEDID(pipe);
-		aud_config = IBX_AUD_CFG(pipe);
-		aud_cntl_st = IBX_AUD_CNTL_ST(pipe);
-		aud_cntrl_st2 = IBX_AUD_CNTL_ST2;
-	} else if (IS_VALLEYVIEW(connector->dev)) {
-		hdmiw_hdmiedid = VLV_HDMIW_HDMIEDID(pipe);
-		aud_config = VLV_AUD_CFG(pipe);
-		aud_cntl_st = VLV_AUD_CNTL_ST(pipe);
-		aud_cntrl_st2 = VLV_AUD_CNTL_ST2;
-	} else {
-		hdmiw_hdmiedid = CPT_HDMIW_HDMIEDID(pipe);
-		aud_config = CPT_AUD_CFG(pipe);
-		aud_cntl_st = CPT_AUD_CNTL_ST(pipe);
-		aud_cntrl_st2 = CPT_AUD_CNTRL_ST2;
-	}
-
-	DRM_DEBUG_DRIVER("ELD on pipe %c\n", pipe_name(pipe));
-
-	if (IS_VALLEYVIEW(connector->dev))  {
-		struct intel_encoder *intel_encoder;
-		struct intel_digital_port *intel_dig_port;
-
-		intel_encoder = intel_attached_encoder(connector);
-		intel_dig_port = enc_to_dig_port(&intel_encoder->base);
-		i = intel_dig_port->port;
-	} else {
-		i = I915_READ(aud_cntl_st);
-		i = (i >> 29) & DIP_PORT_SEL_MASK;
-		/* DIP_Port_Select, 0x1 = PortB */
-	}
-
-	if (!i) {
-		DRM_DEBUG_DRIVER("Audio directed to unknown port\n");
-		/* operate blindly on all ports */
-		eldv = IBX_ELD_VALIDB;
-		eldv |= IBX_ELD_VALIDB << 4;
-		eldv |= IBX_ELD_VALIDB << 8;
-	} else {
-		DRM_DEBUG_DRIVER("ELD on port %c\n", port_name(i));
-		eldv = IBX_ELD_VALIDB << ((i - 1) * 4);
-	}
-
-	if (intel_pipe_has_type(crtc, INTEL_OUTPUT_DISPLAYPORT)) {
-		DRM_DEBUG_DRIVER("ELD: DisplayPort detected\n");
-		eld[5] |= (1 << 2);	/* Conn_Type, 0x1 = DisplayPort */
-		I915_WRITE(aud_config, AUD_CONFIG_N_VALUE_INDEX); /* 0x1 = DP */
-	} else {
-		I915_WRITE(aud_config, audio_config_hdmi_pixel_clock(mode));
-	}
-
-	if (intel_eld_uptodate(connector,
-			       aud_cntrl_st2, eldv,
-			       aud_cntl_st, IBX_ELD_ADDRESS,
-			       hdmiw_hdmiedid))
-		return;
-
-	i = I915_READ(aud_cntrl_st2);
-	i &= ~eldv;
-	I915_WRITE(aud_cntrl_st2, i);
-
-	if (!eld[0])
-		return;
-
-	i = I915_READ(aud_cntl_st);
-	i &= ~IBX_ELD_ADDRESS;
-	I915_WRITE(aud_cntl_st, i);
-
-	len = min_t(uint8_t, eld[2], 21);	/* 84 bytes of hw ELD buffer */
-	DRM_DEBUG_DRIVER("ELD size %d\n", len);
-	for (i = 0; i < len; i++)
-		I915_WRITE(hdmiw_hdmiedid, *((uint32_t *)eld + i));
-
-	i = I915_READ(aud_cntrl_st2);
-	i |= eldv;
-	I915_WRITE(aud_cntrl_st2, i);
-}
-
-void intel_write_eld(struct drm_encoder *encoder,
-		     struct drm_display_mode *mode)
-{
-	struct drm_crtc *crtc = encoder->crtc;
-	struct drm_connector *connector;
-	struct drm_device *dev = encoder->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-
-	connector = drm_select_eld(encoder, mode);
-	if (!connector)
-		return;
-
-	DRM_DEBUG_DRIVER("ELD on [CONNECTOR:%d:%s], [ENCODER:%d:%s]\n",
-			 connector->base.id,
-			 connector->name,
-			 connector->encoder->base.id,
-			 connector->encoder->name);
-
-	connector->eld[6] = drm_av_sync_delay(connector, mode) / 2;
-
-	if (dev_priv->display.write_eld)
-		dev_priv->display.write_eld(connector, crtc, mode);
-}
-
-static void i845_update_cursor(struct drm_crtc *crtc, u32 base)
-{
-	struct drm_device *dev = crtc->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
-	uint32_t cntl;
-
-	if (base != intel_crtc->cursor_base) {
-		/* On these chipsets we can only modify the base whilst
-		 * the cursor is disabled.
-		 */
-		if (intel_crtc->cursor_cntl) {
-			I915_WRITE(_CURACNTR, 0);
-			POSTING_READ(_CURACNTR);
-			intel_crtc->cursor_cntl = 0;
-		}
-
-		I915_WRITE(_CURABASE, base);
-		POSTING_READ(_CURABASE);
-	}
-
-	/* XXX width must be 64, stride 256 => 0x00 << 28 */
-	cntl = 0;
-	if (base)
-		cntl = (CURSOR_ENABLE |
-			CURSOR_GAMMA_ENABLE |
-			CURSOR_FORMAT_ARGB);
-	if (intel_crtc->cursor_cntl != cntl) {
-		I915_WRITE(_CURACNTR, cntl);
-		POSTING_READ(_CURACNTR);
-		intel_crtc->cursor_cntl = cntl;
-	}
-}
-
-static void i9xx_update_cursor(struct drm_crtc *crtc, u32 base)
+static void i9xx_update_cursor(struct drm_crtc *crtc, u32 base)
 {
 	struct drm_device *dev = crtc->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -8092,46 +8199,13 @@
 				return;
 		}
 		cntl |= pipe << 28; /* Connect to correct pipe */
-	}
-	if (intel_crtc->cursor_cntl != cntl) {
-		I915_WRITE(CURCNTR(pipe), cntl);
-		POSTING_READ(CURCNTR(pipe));
-		intel_crtc->cursor_cntl = cntl;
-	}
 
-	/* and commit changes on next vblank */
-	I915_WRITE(CURBASE(pipe), base);
-	POSTING_READ(CURBASE(pipe));
-}
-
-static void ivb_update_cursor(struct drm_crtc *crtc, u32 base)
-{
-	struct drm_device *dev = crtc->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
-	int pipe = intel_crtc->pipe;
-	uint32_t cntl;
-
-	cntl = 0;
-	if (base) {
-		cntl = MCURSOR_GAMMA_ENABLE;
-		switch (intel_crtc->cursor_width) {
-			case 64:
-				cntl |= CURSOR_MODE_64_ARGB_AX;
-				break;
-			case 128:
-				cntl |= CURSOR_MODE_128_ARGB_AX;
-				break;
-			case 256:
-				cntl |= CURSOR_MODE_256_ARGB_AX;
-				break;
-			default:
-				WARN_ON(1);
-				return;
-		}
+		if (IS_HASWELL(dev) || IS_BROADWELL(dev))
+			cntl |= CURSOR_PIPE_CSC_ENABLE;
 	}
-	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
-		cntl |= CURSOR_PIPE_CSC_ENABLE;
+
+	if (to_intel_plane(crtc->cursor)->rotation == BIT(DRM_ROTATE_180))
+		cntl |= CURSOR_ROTATE_180;
 
 	if (intel_crtc->cursor_cntl != cntl) {
 		I915_WRITE(CURCNTR(pipe), cntl);
@@ -8142,6 +8216,8 @@
 	/* and commit changes on next vblank */
 	I915_WRITE(CURBASE(pipe), base);
 	POSTING_READ(CURBASE(pipe));
+
+	intel_crtc->cursor_base = base;
 }
 
 /* If no-part of the cursor is visible on the framebuffer, then the GPU may hang... */
@@ -8188,28 +8264,62 @@
 
 	I915_WRITE(CURPOS(pipe), pos);
 
-	if (IS_IVYBRIDGE(dev) || IS_HASWELL(dev) || IS_BROADWELL(dev))
-		ivb_update_cursor(crtc, base);
-	else if (IS_845G(dev) || IS_I865G(dev))
+	/* ILK+ do this automagically */
+	if (HAS_GMCH_DISPLAY(dev) &&
+		to_intel_plane(crtc->cursor)->rotation == BIT(DRM_ROTATE_180)) {
+		base += (intel_crtc->cursor_height *
+			intel_crtc->cursor_width - 1) * 4;
+	}
+
+	if (IS_845G(dev) || IS_I865G(dev))
 		i845_update_cursor(crtc, base);
 	else
 		i9xx_update_cursor(crtc, base);
-	intel_crtc->cursor_base = base;
 }
 
-/*
- * intel_crtc_cursor_set_obj - Set cursor to specified GEM object
- *
- * Note that the object's reference will be consumed if the update fails.  If
- * the update succeeds, the reference of the old object (if any) will be
- * consumed.
- */
+static bool cursor_size_ok(struct drm_device *dev,
+			   uint32_t width, uint32_t height)
+{
+	if (width == 0 || height == 0)
+		return false;
+
+	/*
+	 * 845g/865g are special in that they are only limited by
+	 * the width of their cursors, the height is arbitrary up to
+	 * the precision of the register. Everything else requires
+	 * square cursors, limited to a few power-of-two sizes.
+	 */
+	if (IS_845G(dev) || IS_I865G(dev)) {
+		if ((width & 63) != 0)
+			return false;
+
+		if (width > (IS_845G(dev) ? 64 : 512))
+			return false;
+
+		if (height > 1023)
+			return false;
+	} else {
+		switch (width | height) {
+		case 256:
+		case 128:
+			if (IS_GEN2(dev))
+				return false;
+		case 64:
+			break;
+		default:
+			return false;
+		}
+	}
+
+	return true;
+}
+
 static int intel_crtc_cursor_set_obj(struct drm_crtc *crtc,
 				     struct drm_i915_gem_object *obj,
 				     uint32_t width, uint32_t height)
 {
 	struct drm_device *dev = crtc->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = to_i915(dev);
 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 	enum pipe pipe = intel_crtc->pipe;
 	unsigned old_width;
@@ -8220,36 +8330,15 @@
 	if (!obj) {
 		DRM_DEBUG_KMS("cursor off\n");
 		addr = 0;
-		obj = NULL;
 		mutex_lock(&dev->struct_mutex);
 		goto finish;
 	}
 
-	/* Check for which cursor types we support */
-	if (!((width == 64 && height == 64) ||
-			(width == 128 && height == 128 && !IS_GEN2(dev)) ||
-			(width == 256 && height == 256 && !IS_GEN2(dev)))) {
-		DRM_DEBUG("Cursor dimension not supported\n");
-		return -EINVAL;
-	}
-
-	if (obj->base.size < width * height * 4) {
-		DRM_DEBUG_KMS("buffer is too small\n");
-		ret = -ENOMEM;
-		goto fail;
-	}
-
 	/* we only need to pin inside GTT if cursor is non-phy */
 	mutex_lock(&dev->struct_mutex);
 	if (!INTEL_INFO(dev)->cursor_needs_physical) {
 		unsigned alignment;
 
-		if (obj->tiling_mode) {
-			DRM_DEBUG_KMS("cursor cannot be tiled\n");
-			ret = -EINVAL;
-			goto fail_locked;
-		}
-
 		/*
 		 * Global gtt pte registers are special registers which actually
 		 * forward writes to a chunk of system memory. Which means that
@@ -8295,9 +8384,6 @@
 		addr = obj->phys_handle->busaddr;
 	}
 
-	if (IS_GEN2(dev))
-		I915_WRITE(CURSIZE, (height << 12) | width);
-
  finish:
 	if (intel_crtc->cursor_bo) {
 		if (!INTEL_INFO(dev)->cursor_needs_physical)
@@ -8319,17 +8405,15 @@
 		if (old_width != width)
 			intel_update_watermarks(crtc);
 		intel_crtc_update_cursor(crtc, intel_crtc->cursor_bo != NULL);
-	}
 
-	intel_frontbuffer_flip(dev, INTEL_FRONTBUFFER_CURSOR(pipe));
+		intel_frontbuffer_flip(dev, INTEL_FRONTBUFFER_CURSOR(pipe));
+	}
 
 	return 0;
 fail_unpin:
 	i915_gem_object_unpin_from_display_plane(obj);
 fail_locked:
 	mutex_unlock(&dev->struct_mutex);
-fail:
-	drm_gem_object_unreference_unlocked(&obj->base);
 	return ret;
 }
 
@@ -8364,7 +8448,7 @@
 
 	intel_fb = kzalloc(sizeof(*intel_fb), GFP_KERNEL);
 	if (!intel_fb) {
-		drm_gem_object_unreference_unlocked(&obj->base);
+		drm_gem_object_unreference(&obj->base);
 		return ERR_PTR(-ENOMEM);
 	}
 
@@ -8374,7 +8458,7 @@
 
 	return &intel_fb->base;
 err:
-	drm_gem_object_unreference_unlocked(&obj->base);
+	drm_gem_object_unreference(&obj->base);
 	kfree(intel_fb);
 
 	return ERR_PTR(ret);
@@ -8507,6 +8591,9 @@
 		ret = drm_modeset_lock(&crtc->mutex, ctx);
 		if (ret)
 			goto fail_unlock;
+		ret = drm_modeset_lock(&crtc->primary->mutex, ctx);
+		if (ret)
+			goto fail_unlock;
 
 		old->dpms_mode = connector->dpms;
 		old->load_detect_temp = false;
@@ -8544,6 +8631,9 @@
 	ret = drm_modeset_lock(&crtc->mutex, ctx);
 	if (ret)
 		goto fail_unlock;
+	ret = drm_modeset_lock(&crtc->primary->mutex, ctx);
+	if (ret)
+		goto fail_unlock;
 	intel_encoder->new_crtc = to_intel_crtc(crtc);
 	to_intel_connector(connector)->new_encoder = intel_encoder;
 
@@ -8826,35 +8916,6 @@
 	return mode;
 }
 
-static void intel_increase_pllclock(struct drm_device *dev,
-				    enum pipe pipe)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	int dpll_reg = DPLL(pipe);
-	int dpll;
-
-	if (!HAS_GMCH_DISPLAY(dev))
-		return;
-
-	if (!dev_priv->lvds_downclock_avail)
-		return;
-
-	dpll = I915_READ(dpll_reg);
-	if (!HAS_PIPE_CXSR(dev) && (dpll & DISPLAY_RATE_SELECT_FPA1)) {
-		DRM_DEBUG_DRIVER("upclocking LVDS\n");
-
-		assert_panel_unlocked(dev_priv, pipe);
-
-		dpll &= ~DISPLAY_RATE_SELECT_FPA1;
-		I915_WRITE(dpll_reg, dpll);
-		intel_wait_for_vblank(dev, pipe);
-
-		dpll = I915_READ(dpll_reg);
-		if (dpll & DISPLAY_RATE_SELECT_FPA1)
-			DRM_DEBUG_DRIVER("failed to upclock LVDS!\n");
-	}
-}
-
 static void intel_decrease_pllclock(struct drm_crtc *crtc)
 {
 	struct drm_device *dev = crtc->dev;
@@ -8900,6 +8961,8 @@
 
 	intel_runtime_pm_get(dev_priv);
 	i915_update_gfx_val(dev_priv);
+	if (INTEL_INFO(dev)->gen >= 6)
+		gen6_rps_busy(dev_priv);
 	dev_priv->mm.busy = true;
 }
 
@@ -8913,7 +8976,7 @@
 
 	dev_priv->mm.busy = false;
 
-	if (!i915.powersave)
+	if (!i915_module.powersave)
 		goto out;
 
 	for_each_crtc(dev, crtc) {
@@ -8930,190 +8993,16 @@
 	intel_runtime_pm_put(dev_priv);
 }
 
-
-/**
- * intel_mark_fb_busy - mark given planes as busy
- * @dev: DRM device
- * @frontbuffer_bits: bits for the affected planes
- * @ring: optional ring for asynchronous commands
- *
- * This function gets called every time the screen contents change. It can be
- * used to keep e.g. the update rate at the nominal refresh rate with DRRS.
- */
-static void intel_mark_fb_busy(struct drm_device *dev,
-			       unsigned frontbuffer_bits,
-			       struct intel_engine_cs *ring)
-{
-	enum pipe pipe;
-
-	if (!i915.powersave)
-		return;
-
-	for_each_pipe(pipe) {
-		if (!(frontbuffer_bits & INTEL_FRONTBUFFER_ALL_MASK(pipe)))
-			continue;
-
-		intel_increase_pllclock(dev, pipe);
-		if (ring && intel_fbc_enabled(dev))
-			ring->fbc_dirty = true;
-	}
-}
-
-/**
- * intel_fb_obj_invalidate - invalidate frontbuffer object
- * @obj: GEM object to invalidate
- * @ring: set for asynchronous rendering
- *
- * This function gets called every time rendering on the given object starts and
- * frontbuffer caching (fbc, low refresh rate for DRRS, panel self refresh) must
- * be invalidated. If @ring is non-NULL any subsequent invalidation will be delayed
- * until the rendering completes or a flip on this frontbuffer plane is
- * scheduled.
- */
-void intel_fb_obj_invalidate(struct drm_i915_gem_object *obj,
-			     struct intel_engine_cs *ring)
-{
-	struct drm_device *dev = obj->base.dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-
-	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
-
-	if (!obj->frontbuffer_bits)
-		return;
-
-	if (ring) {
-		mutex_lock(&dev_priv->fb_tracking.lock);
-		dev_priv->fb_tracking.busy_bits
-			|= obj->frontbuffer_bits;
-		dev_priv->fb_tracking.flip_bits
-			&= ~obj->frontbuffer_bits;
-		mutex_unlock(&dev_priv->fb_tracking.lock);
-	}
-
-	intel_mark_fb_busy(dev, obj->frontbuffer_bits, ring);
-
-	intel_edp_psr_invalidate(dev, obj->frontbuffer_bits);
-}
-
-/**
- * intel_frontbuffer_flush - flush frontbuffer
- * @dev: DRM device
- * @frontbuffer_bits: frontbuffer plane tracking bits
- *
- * This function gets called every time rendering on the given planes has
- * completed and frontbuffer caching can be started again. Flushes will get
- * delayed if they're blocked by some oustanding asynchronous rendering.
- *
- * Can be called without any locks held.
- */
-void intel_frontbuffer_flush(struct drm_device *dev,
-			     unsigned frontbuffer_bits)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-
-	/* Delay flushing when rings are still busy.*/
-	mutex_lock(&dev_priv->fb_tracking.lock);
-	frontbuffer_bits &= ~dev_priv->fb_tracking.busy_bits;
-	mutex_unlock(&dev_priv->fb_tracking.lock);
-
-	intel_mark_fb_busy(dev, frontbuffer_bits, NULL);
-
-	intel_edp_psr_flush(dev, frontbuffer_bits);
-}
-
-/**
- * intel_fb_obj_flush - flush frontbuffer object
- * @obj: GEM object to flush
- * @retire: set when retiring asynchronous rendering
- *
- * This function gets called every time rendering on the given object has
- * completed and frontbuffer caching can be started again. If @retire is true
- * then any delayed flushes will be unblocked.
- */
-void intel_fb_obj_flush(struct drm_i915_gem_object *obj,
-			bool retire)
-{
-	struct drm_device *dev = obj->base.dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	unsigned frontbuffer_bits;
-
-	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
-
-	if (!obj->frontbuffer_bits)
-		return;
-
-	frontbuffer_bits = obj->frontbuffer_bits;
-
-	if (retire) {
-		mutex_lock(&dev_priv->fb_tracking.lock);
-		/* Filter out new bits since rendering started. */
-		frontbuffer_bits &= dev_priv->fb_tracking.busy_bits;
-
-		dev_priv->fb_tracking.busy_bits &= ~frontbuffer_bits;
-		mutex_unlock(&dev_priv->fb_tracking.lock);
-	}
-
-	intel_frontbuffer_flush(dev, frontbuffer_bits);
-}
-
-/**
- * intel_frontbuffer_flip_prepare - prepare asnychronous frontbuffer flip
- * @dev: DRM device
- * @frontbuffer_bits: frontbuffer plane tracking bits
- *
- * This function gets called after scheduling a flip on @obj. The actual
- * frontbuffer flushing will be delayed until completion is signalled with
- * intel_frontbuffer_flip_complete. If an invalidate happens in between this
- * flush will be cancelled.
- *
- * Can be called without any locks held.
- */
-void intel_frontbuffer_flip_prepare(struct drm_device *dev,
-				    unsigned frontbuffer_bits)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-
-	mutex_lock(&dev_priv->fb_tracking.lock);
-	dev_priv->fb_tracking.flip_bits
-		|= frontbuffer_bits;
-	mutex_unlock(&dev_priv->fb_tracking.lock);
-}
-
-/**
- * intel_frontbuffer_flip_complete - complete asynchronous frontbuffer flush
- * @dev: DRM device
- * @frontbuffer_bits: frontbuffer plane tracking bits
- *
- * This function gets called after the flip has been latched and will complete
- * on the next vblank. It will execute the fush if it hasn't been cancalled yet.
- *
- * Can be called without any locks held.
- */
-void intel_frontbuffer_flip_complete(struct drm_device *dev,
-				     unsigned frontbuffer_bits)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-
-	mutex_lock(&dev_priv->fb_tracking.lock);
-	/* Mask any cancelled flips. */
-	frontbuffer_bits &= dev_priv->fb_tracking.flip_bits;
-	dev_priv->fb_tracking.flip_bits &= ~frontbuffer_bits;
-	mutex_unlock(&dev_priv->fb_tracking.lock);
-
-	intel_frontbuffer_flush(dev, frontbuffer_bits);
-}
-
 static void intel_crtc_destroy(struct drm_crtc *crtc)
 {
 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 	struct drm_device *dev = crtc->dev;
 	struct intel_unpin_work *work;
-	unsigned long flags;
 
-	spin_lock_irqsave(&dev->event_lock, flags);
+	spin_lock_irq(&dev->event_lock);
 	work = intel_crtc->unpin_work;
 	intel_crtc->unpin_work = NULL;
-	spin_unlock_irqrestore(&dev->event_lock, flags);
+	spin_unlock_irq(&dev->event_lock);
 
 	if (work) {
 		cancel_work_sync(&work->work);
@@ -9136,6 +9025,7 @@
 	intel_unpin_fb_obj(work->old_fb_obj);
 	drm_gem_object_unreference(&work->pending_flip_obj->base);
 	drm_gem_object_unreference(&work->old_fb_obj->base);
+	i915_request_put(work->flip_queued_request);
 
 	intel_update_fbc(dev);
 	mutex_unlock(&dev->struct_mutex);
@@ -9151,7 +9041,6 @@
 static void do_intel_finish_page_flip(struct drm_device *dev,
 				      struct drm_crtc *crtc)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 	struct intel_unpin_work *work;
 	unsigned long flags;
@@ -9160,6 +9049,10 @@
 	if (intel_crtc == NULL)
 		return;
 
+	/*
+	 * This is called both by irq handlers and the reset code (to complete
+	 * lost pageflips) so needs the full irqsave spinlocks.
+	 */
 	spin_lock_irqsave(&dev->event_lock, flags);
 	work = intel_crtc->unpin_work;
 
@@ -9171,23 +9064,9 @@
 		return;
 	}
 
-	/* and that the unpin work is consistent wrt ->pending. */
-	smp_rmb();
-
-	intel_crtc->unpin_work = NULL;
-
-	if (work->event)
-		drm_send_vblank_event(dev, intel_crtc->pipe, work->event);
-
-	drm_crtc_vblank_put(crtc);
+	page_flip_completed(intel_crtc);
 
 	spin_unlock_irqrestore(&dev->event_lock, flags);
-
-	wake_up_all(&dev_priv->pending_flip_queue);
-
-	queue_work(dev_priv->wq, &work->work);
-
-	trace_i915_flip_complete(intel_crtc->plane, work->pending_flip_obj);
 }
 
 void intel_finish_page_flip(struct drm_device *dev, int pipe)
@@ -9255,7 +9134,12 @@
 		to_intel_crtc(dev_priv->plane_to_crtc_mapping[plane]);
 	unsigned long flags;
 
-	/* NB: An MMIO update of the plane base pointer will also
+
+	/*
+	 * This is called both by irq handlers and the reset code (to complete
+	 * lost pageflips) so needs the full irqsave spinlocks.
+	 *
+	 * NB: An MMIO update of the plane base pointer will also
 	 * generate a page-flip completion irq, i.e. every modeset
 	 * is also accompanied by a spurious intel_prepare_page_flip().
 	 */
@@ -9274,97 +9158,86 @@
 	smp_wmb();
 }
 
-static int intel_gen2_queue_flip(struct drm_device *dev,
-				 struct drm_crtc *crtc,
+static int intel_gen2_queue_flip(struct i915_gem_request *rq,
+				 struct intel_crtc *crtc,
 				 struct drm_framebuffer *fb,
 				 struct drm_i915_gem_object *obj,
-				 struct intel_engine_cs *ring,
 				 uint32_t flags)
 {
-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	struct intel_ringbuffer *ring;
 	u32 flip_mask;
-	int ret;
 
-	ret = intel_ring_begin(ring, 6);
-	if (ret)
-		return ret;
+	ring = intel_ring_begin(rq, 5);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
 	/* Can't queue multiple flips, so wait for the previous
 	 * one to finish before executing the next.
 	 */
-	if (intel_crtc->plane)
+	if (crtc->plane)
 		flip_mask = MI_WAIT_FOR_PLANE_B_FLIP;
 	else
 		flip_mask = MI_WAIT_FOR_PLANE_A_FLIP;
 	intel_ring_emit(ring, MI_WAIT_FOR_EVENT | flip_mask);
-	intel_ring_emit(ring, MI_NOOP);
 	intel_ring_emit(ring, MI_DISPLAY_FLIP |
-			MI_DISPLAY_FLIP_PLANE(intel_crtc->plane));
+			MI_DISPLAY_FLIP_PLANE(crtc->plane));
 	intel_ring_emit(ring, fb->pitches[0]);
-	intel_ring_emit(ring, intel_crtc->unpin_work->gtt_offset);
+	intel_ring_emit(ring, crtc->unpin_work->gtt_offset);
 	intel_ring_emit(ring, 0); /* aux display base address, unused */
+	intel_ring_advance(ring);
 
-	intel_mark_page_flip_active(intel_crtc);
-	__intel_ring_advance(ring);
 	return 0;
 }
 
-static int intel_gen3_queue_flip(struct drm_device *dev,
-				 struct drm_crtc *crtc,
+static int intel_gen3_queue_flip(struct i915_gem_request *rq,
+				 struct intel_crtc *crtc,
 				 struct drm_framebuffer *fb,
 				 struct drm_i915_gem_object *obj,
-				 struct intel_engine_cs *ring,
 				 uint32_t flags)
 {
-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	struct intel_ringbuffer *ring;
 	u32 flip_mask;
-	int ret;
 
-	ret = intel_ring_begin(ring, 6);
-	if (ret)
-		return ret;
+	ring = intel_ring_begin(rq, 4);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
-	if (intel_crtc->plane)
+	if (crtc->plane)
 		flip_mask = MI_WAIT_FOR_PLANE_B_FLIP;
 	else
 		flip_mask = MI_WAIT_FOR_PLANE_A_FLIP;
 	intel_ring_emit(ring, MI_WAIT_FOR_EVENT | flip_mask);
-	intel_ring_emit(ring, MI_NOOP);
 	intel_ring_emit(ring, MI_DISPLAY_FLIP_I915 |
-			MI_DISPLAY_FLIP_PLANE(intel_crtc->plane));
+			MI_DISPLAY_FLIP_PLANE(crtc->plane));
 	intel_ring_emit(ring, fb->pitches[0]);
-	intel_ring_emit(ring, intel_crtc->unpin_work->gtt_offset);
-	intel_ring_emit(ring, MI_NOOP);
+	intel_ring_emit(ring, crtc->unpin_work->gtt_offset);
+	intel_ring_advance(ring);
 
-	intel_mark_page_flip_active(intel_crtc);
-	__intel_ring_advance(ring);
 	return 0;
 }
 
-static int intel_gen4_queue_flip(struct drm_device *dev,
-				 struct drm_crtc *crtc,
+static int intel_gen4_queue_flip(struct i915_gem_request *rq,
+				 struct intel_crtc *crtc,
 				 struct drm_framebuffer *fb,
 				 struct drm_i915_gem_object *obj,
-				 struct intel_engine_cs *ring,
 				 uint32_t flags)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	struct drm_i915_private *dev_priv = rq->i915;
+	struct intel_ringbuffer *ring;
 	uint32_t pf, pipesrc;
-	int ret;
 
-	ret = intel_ring_begin(ring, 4);
-	if (ret)
-		return ret;
+	ring = intel_ring_begin(rq, 4);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
 	/* i965+ uses the linear or tiled offsets from the
 	 * Display Registers (which do not change across a page-flip)
 	 * so we need only reprogram the base address.
 	 */
 	intel_ring_emit(ring, MI_DISPLAY_FLIP |
-			MI_DISPLAY_FLIP_PLANE(intel_crtc->plane));
+			MI_DISPLAY_FLIP_PLANE(crtc->plane));
 	intel_ring_emit(ring, fb->pitches[0]);
-	intel_ring_emit(ring, intel_crtc->unpin_work->gtt_offset |
+	intel_ring_emit(ring, crtc->unpin_work->gtt_offset |
 			obj->tiling_mode);
 
 	/* XXX Enabling the panel-fitter across page-flip is so far
@@ -9372,62 +9245,53 @@
 	 * pf = I915_READ(pipe == 0 ? PFA_CTL_1 : PFB_CTL_1) & PF_ENABLE;
 	 */
 	pf = 0;
-	pipesrc = I915_READ(PIPESRC(intel_crtc->pipe)) & 0x0fff0fff;
+	pipesrc = I915_READ(PIPESRC(crtc->pipe)) & 0x0fff0fff;
 	intel_ring_emit(ring, pf | pipesrc);
+	intel_ring_advance(ring);
 
-	intel_mark_page_flip_active(intel_crtc);
-	__intel_ring_advance(ring);
 	return 0;
 }
 
-static int intel_gen6_queue_flip(struct drm_device *dev,
-				 struct drm_crtc *crtc,
+static int intel_gen6_queue_flip(struct i915_gem_request *rq,
+				 struct intel_crtc *crtc,
 				 struct drm_framebuffer *fb,
 				 struct drm_i915_gem_object *obj,
-				 struct intel_engine_cs *ring,
 				 uint32_t flags)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
-	uint32_t pf, pipesrc;
-	int ret;
+	struct intel_ringbuffer *ring;
+	u32 cmd, base;
 
-	ret = intel_ring_begin(ring, 4);
-	if (ret)
-		return ret;
+	ring = intel_ring_begin(rq, 3);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
-	intel_ring_emit(ring, MI_DISPLAY_FLIP |
-			MI_DISPLAY_FLIP_PLANE(intel_crtc->plane));
-	intel_ring_emit(ring, fb->pitches[0] | obj->tiling_mode);
-	intel_ring_emit(ring, intel_crtc->unpin_work->gtt_offset);
+	cmd = MI_DISPLAY_FLIP_I915 | MI_DISPLAY_FLIP_PLANE(crtc->plane);
+	base = crtc->unpin_work->gtt_offset;
+	if (flags & DRM_MODE_PAGE_FLIP_ASYNC) {
+		cmd |= MI_DISPLAY_FLIP_ASYNC;
+		base |= MI_DISPLAY_FLIP_TYPE_ASYNC;
+	}
 
-	/* Contrary to the suggestions in the documentation,
-	 * "Enable Panel Fitter" does not seem to be required when page
-	 * flipping with a non-native mode, and worse causes a normal
-	 * modeset to fail.
-	 * pf = I915_READ(PF_CTL(intel_crtc->pipe)) & PF_ENABLE;
-	 */
-	pf = 0;
-	pipesrc = I915_READ(PIPESRC(intel_crtc->pipe)) & 0x0fff0fff;
-	intel_ring_emit(ring, pf | pipesrc);
+	intel_ring_emit(ring, cmd);
+	intel_ring_emit(ring, fb->pitches[0] | obj->tiling_mode);
+	intel_ring_emit(ring, base);
+	intel_ring_advance(ring);
 
-	intel_mark_page_flip_active(intel_crtc);
-	__intel_ring_advance(ring);
 	return 0;
 }
 
-static int intel_gen7_queue_flip(struct drm_device *dev,
-				 struct drm_crtc *crtc,
+static int intel_gen7_queue_flip(struct i915_gem_request *rq,
+				 struct intel_crtc *crtc,
 				 struct drm_framebuffer *fb,
 				 struct drm_i915_gem_object *obj,
-				 struct intel_engine_cs *ring,
 				 uint32_t flags)
 {
-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	struct intel_ringbuffer *ring;
 	uint32_t plane_bit = 0;
+	u32 cmd, base;
 	int len, ret;
 
-	switch (intel_crtc->plane) {
+	switch (crtc->plane) {
 	case PLANE_A:
 		plane_bit = MI_DISPLAY_FLIP_IVB_PLANE_A;
 		break;
@@ -9442,16 +9306,16 @@
 		return -ENODEV;
 	}
 
-	len = 4;
-	if (ring->id == RCS) {
+	len = 3;
+	if (rq->engine->id == RCS) {
 		len += 6;
 		/*
 		 * On Gen 8, SRM is now taking an extra dword to accommodate
 		 * 48bits addresses, and we need a NOOP for the batch size to
 		 * stay even.
 		 */
-		if (IS_GEN8(dev))
-			len += 2;
+		if (IS_GEN8(rq->i915))
+			len += 1;
 	}
 
 	/*
@@ -9464,13 +9328,13 @@
 	 * then do the cacheline alignment, and finally emit the
 	 * MI_DISPLAY_FLIP.
 	 */
-	ret = intel_ring_cacheline_align(ring);
+	ret = intel_ring_cacheline_align(rq);
 	if (ret)
 		return ret;
 
-	ret = intel_ring_begin(ring, len);
-	if (ret)
-		return ret;
+	ring = intel_ring_begin(rq, len);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
 	/* Unmask the flip-done completion message. Note that the bspec says that
 	 * we should do this for both the BCS and RCS, and that we must not unmask
@@ -9481,38 +9345,102 @@
 	 * for the RCS also doesn't appear to drop events. Setting the DERRMR
 	 * to zero does lead to lockups within MI_DISPLAY_FLIP.
 	 */
-	if (ring->id == RCS) {
+	if (rq->engine->id == RCS) {
 		intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
 		intel_ring_emit(ring, DERRMR);
 		intel_ring_emit(ring, ~(DERRMR_PIPEA_PRI_FLIP_DONE |
 					DERRMR_PIPEB_PRI_FLIP_DONE |
 					DERRMR_PIPEC_PRI_FLIP_DONE));
-		if (IS_GEN8(dev))
+		if (IS_GEN8(rq->i915))
 			intel_ring_emit(ring, MI_STORE_REGISTER_MEM_GEN8(1) |
 					      MI_SRM_LRM_GLOBAL_GTT);
 		else
 			intel_ring_emit(ring, MI_STORE_REGISTER_MEM(1) |
 					      MI_SRM_LRM_GLOBAL_GTT);
 		intel_ring_emit(ring, DERRMR);
-		intel_ring_emit(ring, ring->scratch.gtt_offset + 256);
-		if (IS_GEN8(dev)) {
+		intel_ring_emit(ring, rq->engine->scratch.gtt_offset + 256);
+		if (IS_GEN8(rq->i915))
 			intel_ring_emit(ring, 0);
-			intel_ring_emit(ring, MI_NOOP);
-		}
 	}
 
-	intel_ring_emit(ring, MI_DISPLAY_FLIP_I915 | plane_bit);
-	intel_ring_emit(ring, (fb->pitches[0] | obj->tiling_mode));
-	intel_ring_emit(ring, intel_crtc->unpin_work->gtt_offset);
-	intel_ring_emit(ring, (MI_NOOP));
+	cmd = MI_DISPLAY_FLIP_I915 | plane_bit;
+	base = crtc->unpin_work->gtt_offset;
+	if (flags & DRM_MODE_PAGE_FLIP_ASYNC) {
+		cmd |= MI_DISPLAY_FLIP_ASYNC;
+		base |= MI_DISPLAY_FLIP_TYPE_ASYNC;
+	}
+
+	intel_ring_emit(ring, cmd);
+	intel_ring_emit(ring, fb->pitches[0] | obj->tiling_mode);
+	intel_ring_emit(ring, base);
+	intel_ring_advance(ring);
+
+	return 0;
+}
+
+static int intel_gen9_queue_flip(struct i915_gem_request *rq,
+				 struct intel_crtc *crtc,
+				 struct drm_framebuffer *fb,
+				 struct drm_i915_gem_object *obj,
+				 uint32_t flags)
+{
+	struct intel_ringbuffer *ring;
+	uint32_t plane = 0, stride;
+
+	switch (crtc->pipe) {
+	case PIPE_A:
+		plane = MI_DISPLAY_FLIP_SKL_PLANE_1_A;
+		break;
+	case PIPE_B:
+		plane = MI_DISPLAY_FLIP_SKL_PLANE_1_B;
+		break;
+	case PIPE_C:
+		plane = MI_DISPLAY_FLIP_SKL_PLANE_1_C;
+		break;
+	default:
+		WARN_ONCE(1, "unknown plane in flip command\n");
+		return -ENODEV;
+	}
+
+	switch (obj->tiling_mode) {
+	case I915_TILING_NONE:
+		stride = fb->pitches[0] >> 6;
+		break;
+	case I915_TILING_X:
+		stride = fb->pitches[0] >> 9;
+		break;
+	default:
+		WARN_ONCE(1, "unknown tiling in flip command\n");
+		return -ENODEV;
+	}
+
+	ring = intel_ring_begin(rq, 10);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
+
+	intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
+	intel_ring_emit(ring, DERRMR);
+	intel_ring_emit(ring, ~(DERRMR_PIPEA_PRI_FLIP_DONE |
+				DERRMR_PIPEB_PRI_FLIP_DONE |
+				DERRMR_PIPEC_PRI_FLIP_DONE));
+	intel_ring_emit(ring, MI_STORE_REGISTER_MEM_GEN8(1) |
+			      MI_SRM_LRM_GLOBAL_GTT);
+	intel_ring_emit(ring, DERRMR);
+	intel_ring_emit(ring, rq->engine->scratch.gtt_offset + 256);
+	intel_ring_emit(ring, 0);
+
+	intel_ring_emit(ring, MI_DISPLAY_FLIP_I915 | plane);
+	intel_ring_emit(ring, stride << 6 | obj->tiling_mode);
+	intel_ring_emit(ring, crtc->unpin_work->gtt_offset);
+	intel_ring_advance(ring);
 
-	intel_mark_page_flip_active(intel_crtc);
-	__intel_ring_advance(ring);
 	return 0;
 }
 
-static bool use_mmio_flip(struct intel_engine_cs *ring,
-			  struct drm_i915_gem_object *obj)
+static bool use_mmio_flip(struct intel_crtc *intel_crtc,
+			  struct intel_engine_cs *engine,
+			  struct drm_i915_gem_object *obj,
+			  unsigned flags)
 {
 	/*
 	 * This is not being used for older platforms, because
@@ -9522,18 +9450,25 @@
 	 * So using MMIO flips there would disrupt this mechanism.
 	 */
 
-	if (ring == NULL)
+	if (engine == NULL)
 		return true;
 
-	if (INTEL_INFO(ring->dev)->gen < 5)
+	if (INTEL_INFO(engine->i915)->gen < 5)
 		return false;
 
-	if (i915.use_mmio_flip < 0)
+	if (__i915_reset_in_progress(intel_crtc->reset_counter) |
+	    __i915_terminally_wedged(intel_crtc->reset_counter))
+		return true;
+
+	if (flags & DRM_MODE_PAGE_FLIP_ASYNC)
+		return false; /* XXX undecided */
+
+	if (i915_module.use_mmio_flip < 0)
 		return false;
-	else if (i915.use_mmio_flip > 0)
+	else if (i915_module.use_mmio_flip > 0)
 		return true;
 	else
-		return ring != obj->ring;
+		return engine != i915_request_engine(obj->last_write.request);
 }
 
 static void intel_do_mmio_flip(struct intel_crtc *intel_crtc)
@@ -9543,126 +9478,161 @@
 	struct intel_framebuffer *intel_fb =
 		to_intel_framebuffer(intel_crtc->base.primary->fb);
 	struct drm_i915_gem_object *obj = intel_fb->obj;
+	bool atomic_update;
+	u32 start_vbl_count;
 	u32 dspcntr;
 	u32 reg;
 
 	intel_mark_page_flip_active(intel_crtc);
 
+	atomic_update = intel_pipe_update_start(intel_crtc, &start_vbl_count);
+
 	reg = DSPCNTR(intel_crtc->plane);
 	dspcntr = I915_READ(reg);
 
-	if (INTEL_INFO(dev)->gen >= 4) {
-		if (obj->tiling_mode != I915_TILING_NONE)
-			dspcntr |= DISPPLANE_TILED;
-		else
-			dspcntr &= ~DISPPLANE_TILED;
-	}
+	if (obj->tiling_mode != I915_TILING_NONE)
+		dspcntr |= DISPPLANE_TILED;
+	else
+		dspcntr &= ~DISPPLANE_TILED;
+
 	I915_WRITE(reg, dspcntr);
 
 	I915_WRITE(DSPSURF(intel_crtc->plane),
 		   intel_crtc->unpin_work->gtt_offset);
 	POSTING_READ(DSPSURF(intel_crtc->plane));
+
+	if (atomic_update)
+		intel_pipe_update_end(intel_crtc, start_vbl_count);
 }
 
-static int intel_postpone_flip(struct drm_i915_gem_object *obj)
+struct flip_work {
+	struct work_struct work;
+	struct i915_gem_request *rq;
+	struct intel_crtc *crtc;
+};
+
+static void intel_mmio_flip_work(struct work_struct *work)
 {
-	struct intel_engine_cs *ring;
-	int ret;
+	struct flip_work *flip = container_of(work, struct flip_work, work);
+
+	if (WARN_ON(__i915_request_wait(flip->rq, false, NULL, NULL)))
+		/* should never happen, but still prevent a lockup */
+		page_flip_completed(flip->crtc);
+	else
+		intel_do_mmio_flip(flip->crtc);
 
-	lockdep_assert_held(&obj->base.dev->struct_mutex);
+	i915_request_put__unlocked(flip->rq);
+	kfree(flip);
+}
 
-	if (!obj->last_write_seqno)
-		return 0;
+static int intel_queue_mmio_flip(struct intel_crtc *crtc,
+				 struct i915_gem_request *rq)
+{
+	struct flip_work *flip;
 
-	ring = obj->ring;
+	if (WARN_ON(crtc->mmio_flip))
+		return -EBUSY;
 
-	if (i915_seqno_passed(ring->get_seqno(ring, true),
-			      obj->last_write_seqno))
+	if (rq == NULL) {
+		intel_do_mmio_flip(crtc);
 		return 0;
+	}
 
-	ret = i915_gem_check_olr(ring, obj->last_write_seqno);
-	if (ret)
+	if (i915_request_complete(rq)) {
+		intel_do_mmio_flip(crtc);
+		return 0;
+	}
+
+	flip = kmalloc(sizeof(*flip), GFP_KERNEL);
+	if (flip == NULL)
+		return -ENOMEM;
+
+	INIT_WORK(&flip->work, intel_mmio_flip_work);
+	flip->crtc = crtc;
+	flip->rq = i915_request_get_breadcrumb(rq);
+	if (IS_ERR(flip->rq)) {
+		int ret = PTR_ERR(flip->rq);
+		kfree(flip);
 		return ret;
+	}
 
-	if (WARN_ON(!ring->irq_get(ring)))
-		return 0;
+	schedule_work(&flip->work);
+	return 0;
+}
 
-	return 1;
+static int intel_default_queue_flip(struct i915_gem_request *rq,
+				    struct intel_crtc *crtc,
+				    struct drm_framebuffer *fb,
+				    struct drm_i915_gem_object *obj,
+				    uint32_t flags)
+{
+	return -ENODEV;
 }
 
-void intel_notify_mmio_flip(struct intel_engine_cs *ring)
+static bool __intel_pageflip_stall_check(struct drm_device *dev,
+					 struct drm_crtc *crtc)
 {
-	struct drm_i915_private *dev_priv = to_i915(ring->dev);
-	struct intel_crtc *intel_crtc;
-	unsigned long irq_flags;
-	u32 seqno;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	struct intel_unpin_work *work = intel_crtc->unpin_work;
+	u32 addr;
 
-	seqno = ring->get_seqno(ring, false);
+	if (atomic_read(&work->pending) >= INTEL_FLIP_COMPLETE)
+		return true;
 
-	spin_lock_irqsave(&dev_priv->mmio_flip_lock, irq_flags);
-	for_each_intel_crtc(ring->dev, intel_crtc) {
-		struct intel_mmio_flip *mmio_flip;
+	if (!work->enable_stall_check)
+		return false;
 
-		mmio_flip = &intel_crtc->mmio_flip;
-		if (mmio_flip->seqno == 0)
-			continue;
+	if (intel_crtc->reset_counter != atomic_read(&dev_priv->gpu_error.reset_counter))
+		return true;
 
-		if (ring->id != mmio_flip->ring_id)
-			continue;
+	if (work->flip_ready_vblank == 0) {
+		if (work->flip_queued_request &&
+		    !i915_request_complete(work->flip_queued_request))
+			return false;
 
-		if (i915_seqno_passed(seqno, mmio_flip->seqno)) {
-			intel_do_mmio_flip(intel_crtc);
-			mmio_flip->seqno = 0;
-			ring->irq_put(ring);
-		}
+		work->flip_ready_vblank = drm_vblank_count(dev, intel_crtc->pipe);
 	}
-	spin_unlock_irqrestore(&dev_priv->mmio_flip_lock, irq_flags);
+
+	if (drm_vblank_count(dev, intel_crtc->pipe) - work->flip_ready_vblank < 3)
+		return false;
+
+	/* Potential stall - if we see that the flip has happened,
+	 * assume a missed interrupt. */
+	if (INTEL_INFO(dev)->gen >= 4)
+		addr = I915_HI_DISPBASE(I915_READ(DSPSURF(intel_crtc->plane)));
+	else
+		addr = I915_READ(DSPADDR(intel_crtc->plane));
+
+	/* There is a potential issue here with a false positive after a flip
+	 * to the same address. We could address this by checking for a
+	 * non-incrementing frame counter.
+	 */
+	return addr == work->gtt_offset;
 }
 
-static int intel_queue_mmio_flip(struct drm_device *dev,
-				 struct drm_crtc *crtc,
-				 struct drm_framebuffer *fb,
-				 struct drm_i915_gem_object *obj,
-				 struct intel_engine_cs *ring,
-				 uint32_t flags)
+void intel_check_page_flip(struct drm_device *dev, int pipe)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_crtc *crtc = dev_priv->pipe_to_crtc_mapping[pipe];
 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
-	unsigned long irq_flags;
-	int ret;
-
-	if (WARN_ON(intel_crtc->mmio_flip.seqno))
-		return -EBUSY;
-
-	ret = intel_postpone_flip(obj);
-	if (ret < 0)
-		return ret;
-	if (ret == 0) {
-		intel_do_mmio_flip(intel_crtc);
-		return 0;
-	}
 
-	spin_lock_irqsave(&dev_priv->mmio_flip_lock, irq_flags);
-	intel_crtc->mmio_flip.seqno = obj->last_write_seqno;
-	intel_crtc->mmio_flip.ring_id = obj->ring->id;
-	spin_unlock_irqrestore(&dev_priv->mmio_flip_lock, irq_flags);
+	WARN_ON(!in_irq());
 
-	/*
-	 * Double check to catch cases where irq fired before
-	 * mmio flip data was ready
-	 */
-	intel_notify_mmio_flip(obj->ring);
-	return 0;
-}
+	if (crtc == NULL)
+		return;
 
-static int intel_default_queue_flip(struct drm_device *dev,
-				    struct drm_crtc *crtc,
-				    struct drm_framebuffer *fb,
-				    struct drm_i915_gem_object *obj,
-				    struct intel_engine_cs *ring,
-				    uint32_t flags)
-{
-	return -ENODEV;
+	spin_lock(&dev->event_lock);
+	if (intel_crtc->unpin_work && __intel_pageflip_stall_check(dev, crtc)) {
+		WARN_ONCE(1, "Kicking stuck page flip: queued at %d, now %d\n",
+			 intel_crtc->unpin_work->flip_queued_vblank, drm_vblank_count(dev, pipe));
+		page_flip_completed(intel_crtc);
+	}
+	if (intel_crtc->unpin_work != NULL &&
+	    intel_crtc->unpin_work->rcs_active &&
+	    drm_vblank_count(dev, pipe) - intel_crtc->unpin_work->flip_queued_vblank > 1)
+		intel_queue_rps_boost_for_request(dev, intel_crtc->unpin_work->flip_queued_request);
+	spin_unlock(&dev->event_lock);
 }
 
 static int intel_crtc_page_flip(struct drm_crtc *crtc,
@@ -9677,8 +9647,8 @@
 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 	enum pipe pipe = intel_crtc->pipe;
 	struct intel_unpin_work *work;
-	struct intel_engine_cs *ring;
-	unsigned long flags;
+	struct intel_engine_cs *engine;
+	struct i915_gem_request *rq;
 	int ret;
 
 	/*
@@ -9702,8 +9672,13 @@
 	     fb->pitches[0] != crtc->primary->fb->pitches[0]))
 		return -EINVAL;
 
-	if (i915_terminally_wedged(&dev_priv->gpu_error))
-		goto out_hang;
+	if (page_flip_flags & DRM_MODE_PAGE_FLIP_ASYNC) {
+		if (obj->tiling_mode != I915_TILING_X)
+			return -EINVAL;
+
+		if (to_intel_framebuffer(old_fb)->obj->tiling_mode != I915_TILING_X)
+			return -EINVAL;
+	}
 
 	work = kzalloc(sizeof(*work), GFP_KERNEL);
 	if (work == NULL)
@@ -9719,17 +9694,29 @@
 		goto free_work;
 
 	/* We borrow the event spin lock for protecting unpin_work */
-	spin_lock_irqsave(&dev->event_lock, flags);
+	spin_lock_irq(&dev->event_lock);
 	if (intel_crtc->unpin_work) {
-		spin_unlock_irqrestore(&dev->event_lock, flags);
-		kfree(work);
-		drm_crtc_vblank_put(crtc);
-
-		DRM_DEBUG_DRIVER("flip queue: crtc already busy\n");
-		return -EBUSY;
+		/* Before declaring the flip queue wedged, check if
+		 * the hardware completed the operation behind our backs.
+		 */
+		if (intel_crtc->unpin_work->async ||
+		    __intel_pageflip_stall_check(dev, crtc)) {
+			DRM_DEBUG_DRIVER("flip queue: previous flip completed, continuing\n");
+			page_flip_completed(intel_crtc);
+		} else {
+			DRM_DEBUG_DRIVER("flip queue: crtc already busy: flip queud at %d, ready at %d, now %d\n",
+					 intel_crtc->unpin_work->flip_queued_vblank,
+					 intel_crtc->unpin_work->flip_ready_vblank,
+					 drm_vblank_count(dev, intel_crtc->pipe));
+			spin_unlock_irq(&dev->event_lock);
+
+			drm_crtc_vblank_put(crtc);
+			kfree(work);
+			return -EBUSY;
+		}
 	}
 	intel_crtc->unpin_work = work;
-	spin_unlock_irqrestore(&dev->event_lock, flags);
+	spin_unlock_irq(&dev->event_lock);
 
 	if (atomic_read(&intel_crtc->unpin_work_count) >= 2)
 		flush_workqueue(dev_priv->wq);
@@ -9745,45 +9732,84 @@
 	crtc->primary->fb = fb;
 
 	work->pending_flip_obj = obj;
-
-	work->enable_stall_check = true;
+	work->rcs_active = RCS_ENGINE(dev_priv)->last_request != NULL;
+	work->async = page_flip_flags & DRM_MODE_PAGE_FLIP_ASYNC;
 
 	atomic_inc(&intel_crtc->unpin_work_count);
 	intel_crtc->reset_counter = atomic_read(&dev_priv->gpu_error.reset_counter);
+	if (__i915_reset_in_progress(intel_crtc->reset_counter) |
+	    __i915_terminally_wedged(intel_crtc->reset_counter)) {
+		ret = -EIO;
+		goto cleanup_pending;
+	}
 
 	if (INTEL_INFO(dev)->gen >= 5 || IS_G4X(dev))
 		work->flip_count = I915_READ(PIPE_FLIPCOUNT_GM45(pipe)) + 1;
 
 	if (IS_VALLEYVIEW(dev)) {
-		ring = &dev_priv->ring[BCS];
+		engine = &dev_priv->engine[BCS];
 		if (obj->tiling_mode != work->old_fb_obj->tiling_mode)
 			/* vlv: DISPLAY_FLIP fails to change tiling */
-			ring = NULL;
+			engine = NULL;
 	} else if (IS_IVYBRIDGE(dev)) {
-		ring = &dev_priv->ring[BCS];
+		engine = &dev_priv->engine[BCS];
 	} else if (INTEL_INFO(dev)->gen >= 7) {
-		ring = obj->ring;
-		if (ring == NULL || ring->id != RCS)
-			ring = &dev_priv->ring[BCS];
+		engine = i915_request_engine(obj->last_write.request);
+		if (engine == NULL || engine->id != RCS)
+			engine = &dev_priv->engine[BCS];
 	} else {
-		ring = &dev_priv->ring[RCS];
+		engine = &dev_priv->engine[RCS];
 	}
 
-	ret = intel_pin_and_fence_fb_obj(dev, obj, ring);
-	if (ret)
-		goto cleanup_pending;
+	if (use_mmio_flip(intel_crtc, engine, obj, page_flip_flags)) {
+		rq = i915_request_get(obj->last_write.request);
 
-	work->gtt_offset =
-		i915_gem_obj_ggtt_offset(obj) + intel_crtc->dspaddr_offset;
+		ret = intel_pin_and_fence_fb_obj(crtc->primary, fb, rq);
+		if (ret)
+			goto cleanup_rq;
 
-	if (use_mmio_flip(ring, obj))
-		ret = intel_queue_mmio_flip(dev, crtc, fb, obj, ring,
-					    page_flip_flags);
-	else
-		ret = dev_priv->display.queue_flip(dev, crtc, fb, obj, ring,
-				page_flip_flags);
-	if (ret)
-		goto cleanup_unpin;
+		work->gtt_offset =
+			i915_gem_obj_ggtt_offset(obj) + intel_crtc->dspaddr_offset;
+
+		ret = intel_queue_mmio_flip(intel_crtc, rq);
+		if (ret)
+			goto cleanup_unpin;
+	} else {
+		struct intel_context *ctx = engine->default_context;
+		if (obj->last_write.request)
+			ctx = obj->last_write.request->ctx;
+		rq = i915_request_create(ctx, engine);
+		if (IS_ERR(rq)) {
+			ret = PTR_ERR(rq);
+			goto cleanup_pending;
+		}
+
+		ret = intel_pin_and_fence_fb_obj(crtc->primary, fb, rq);
+		if (ret)
+			goto cleanup_rq;
+
+		work->gtt_offset =
+			i915_gem_obj_ggtt_offset(obj) + intel_crtc->dspaddr_offset;
+
+		ret = i915_request_emit_flush(rq, I915_FLUSH_CACHES);
+		if (ret)
+			goto cleanup_unpin;
+
+		ret = dev_priv->display.queue_flip(rq, intel_crtc, fb, obj,
+						   page_flip_flags);
+		if (ret)
+			goto cleanup_unpin;
+
+		intel_mark_page_flip_active(intel_crtc);
+
+		ret = i915_request_commit(rq);
+		if (ret)
+			goto cleanup_unpin;
+	}
+
+	work->flip_queued_request = rq;
+	work->flip_queued_vblank = drm_vblank_count(dev, intel_crtc->pipe);
+	work->enable_stall_check = true;
 
 	i915_gem_track_fb(work->old_fb_obj, obj,
 			  INTEL_FRONTBUFFER_PRIMARY(pipe));
@@ -9798,6 +9824,8 @@
 
 cleanup_unpin:
 	intel_unpin_fb_obj(obj);
+cleanup_rq:
+	i915_request_put(rq);
 cleanup_pending:
 	atomic_dec(&intel_crtc->unpin_work_count);
 	crtc->primary->fb = old_fb;
@@ -9806,20 +9834,22 @@
 	mutex_unlock(&dev->struct_mutex);
 
 cleanup:
-	spin_lock_irqsave(&dev->event_lock, flags);
+	spin_lock_irq(&dev->event_lock);
 	intel_crtc->unpin_work = NULL;
-	spin_unlock_irqrestore(&dev->event_lock, flags);
+	spin_unlock_irq(&dev->event_lock);
 
 	drm_crtc_vblank_put(crtc);
 free_work:
 	kfree(work);
 
 	if (ret == -EIO) {
-out_hang:
 		intel_crtc_wait_for_pending_flips(crtc);
 		ret = intel_pipe_set_base(crtc, crtc->x, crtc->y, fb);
-		if (ret == 0 && event)
+		if (ret == 0 && event) {
+			spin_lock_irq(&dev->event_lock);
 			drm_send_vblank_event(dev, pipe, event);
+			spin_unlock_irq(&dev->event_lock);
+		}
 	}
 	return ret;
 }
@@ -9847,8 +9877,7 @@
 			to_intel_encoder(connector->base.encoder);
 	}
 
-	list_for_each_entry(encoder, &dev->mode_config.encoder_list,
-			    base.head) {
+	for_each_intel_encoder(dev, encoder) {
 		encoder->new_crtc =
 			to_intel_crtc(encoder->base.crtc);
 	}
@@ -9879,8 +9908,7 @@
 		connector->base.encoder = &connector->new_encoder->base;
 	}
 
-	list_for_each_entry(encoder, &dev->mode_config.encoder_list,
-			    base.head) {
+	for_each_intel_encoder(dev, encoder) {
 		encoder->base.crtc = &encoder->new_crtc->base;
 	}
 
@@ -10007,6 +10035,15 @@
 		      pipe_config->dp_m_n.gmch_m, pipe_config->dp_m_n.gmch_n,
 		      pipe_config->dp_m_n.link_m, pipe_config->dp_m_n.link_n,
 		      pipe_config->dp_m_n.tu);
+
+	DRM_DEBUG_KMS("dp: %i, gmch_m2: %u, gmch_n2: %u, link_m2: %u, link_n2: %u, tu2: %u\n",
+		      pipe_config->has_dp_encoder,
+		      pipe_config->dp_m2_n2.gmch_m,
+		      pipe_config->dp_m2_n2.gmch_n,
+		      pipe_config->dp_m2_n2.link_m,
+		      pipe_config->dp_m2_n2.link_n,
+		      pipe_config->dp_m2_n2.tu);
+
 	DRM_DEBUG_KMS("requested mode:\n");
 	drm_mode_debug_printmodeline(&pipe_config->requested_mode);
 	DRM_DEBUG_KMS("adjusted mode:\n");
@@ -10041,8 +10078,7 @@
 	struct drm_device *dev = crtc->base.dev;
 	struct intel_encoder *source_encoder;
 
-	list_for_each_entry(source_encoder,
-			    &dev->mode_config.encoder_list, base.head) {
+	for_each_intel_encoder(dev, source_encoder) {
 		if (source_encoder->new_crtc != crtc)
 			continue;
 
@@ -10058,8 +10094,7 @@
 	struct drm_device *dev = crtc->base.dev;
 	struct intel_encoder *encoder;
 
-	list_for_each_entry(encoder,
-			    &dev->mode_config.encoder_list, base.head) {
+	for_each_intel_encoder(dev, encoder) {
 		if (encoder->new_crtc != crtc)
 			continue;
 
@@ -10143,8 +10178,7 @@
 	 * adjust it according to limitations or connector properties, and also
 	 * a chance to reject the mode entirely.
 	 */
-	list_for_each_entry(encoder, &dev->mode_config.encoder_list,
-			    base.head) {
+	for_each_intel_encoder(dev, encoder) {
 
 		if (&encoder->new_crtc->base != crtc)
 			continue;
@@ -10222,8 +10256,7 @@
 				1 << connector->new_encoder->new_crtc->pipe;
 	}
 
-	list_for_each_entry(encoder, &dev->mode_config.encoder_list,
-			    base.head) {
+	for_each_intel_encoder(dev, encoder) {
 		if (encoder->base.crtc == &encoder->new_crtc->base)
 			continue;
 
@@ -10293,12 +10326,14 @@
 static void
 intel_modeset_update_state(struct drm_device *dev, unsigned prepare_pipes)
 {
+	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_encoder *intel_encoder;
 	struct intel_crtc *intel_crtc;
 	struct drm_connector *connector;
 
-	list_for_each_entry(intel_encoder, &dev->mode_config.encoder_list,
-			    base.head) {
+	intel_shared_dpll_commit(dev_priv);
+
+	for_each_intel_encoder(dev, intel_encoder) {
 		if (!intel_encoder->base.crtc)
 			continue;
 
@@ -10387,6 +10422,22 @@
 		return false; \
 	}
 
+/* This is required for BDW+ where there is only one set of registers for
+ * switching between high and low RR.
+ * This macro can be used whenever a comparison has to be made between one
+ * hw state and multiple sw state variables.
+ */
+#define PIPE_CONF_CHECK_I_ALT(name, alt_name) \
+	if ((current_config->name != pipe_config->name) && \
+		(current_config->alt_name != pipe_config->name)) { \
+			DRM_ERROR("mismatch in " #name " " \
+				  "(expected %i or %i, found %i)\n", \
+				  current_config->name, \
+				  current_config->alt_name, \
+				  pipe_config->name); \
+			return false; \
+	}
+
 #define PIPE_CONF_CHECK_FLAGS(name, mask)	\
 	if ((current_config->name ^ pipe_config->name) & (mask)) { \
 		DRM_ERROR("mismatch in " #name "(" #mask ") "	   \
@@ -10419,11 +10470,28 @@
 	PIPE_CONF_CHECK_I(fdi_m_n.tu);
 
 	PIPE_CONF_CHECK_I(has_dp_encoder);
-	PIPE_CONF_CHECK_I(dp_m_n.gmch_m);
-	PIPE_CONF_CHECK_I(dp_m_n.gmch_n);
-	PIPE_CONF_CHECK_I(dp_m_n.link_m);
-	PIPE_CONF_CHECK_I(dp_m_n.link_n);
-	PIPE_CONF_CHECK_I(dp_m_n.tu);
+
+	if (INTEL_INFO(dev)->gen < 8) {
+		PIPE_CONF_CHECK_I(dp_m_n.gmch_m);
+		PIPE_CONF_CHECK_I(dp_m_n.gmch_n);
+		PIPE_CONF_CHECK_I(dp_m_n.link_m);
+		PIPE_CONF_CHECK_I(dp_m_n.link_n);
+		PIPE_CONF_CHECK_I(dp_m_n.tu);
+
+		if (current_config->has_drrs) {
+			PIPE_CONF_CHECK_I(dp_m2_n2.gmch_m);
+			PIPE_CONF_CHECK_I(dp_m2_n2.gmch_n);
+			PIPE_CONF_CHECK_I(dp_m2_n2.link_m);
+			PIPE_CONF_CHECK_I(dp_m2_n2.link_n);
+			PIPE_CONF_CHECK_I(dp_m2_n2.tu);
+		}
+	} else {
+		PIPE_CONF_CHECK_I_ALT(dp_m_n.gmch_m, dp_m2_n2.gmch_m);
+		PIPE_CONF_CHECK_I_ALT(dp_m_n.gmch_n, dp_m2_n2.gmch_n);
+		PIPE_CONF_CHECK_I_ALT(dp_m_n.link_m, dp_m2_n2.link_m);
+		PIPE_CONF_CHECK_I_ALT(dp_m_n.link_n, dp_m2_n2.link_n);
+		PIPE_CONF_CHECK_I_ALT(dp_m_n.tu, dp_m2_n2.tu);
+	}
 
 	PIPE_CONF_CHECK_I(adjusted_mode.crtc_hdisplay);
 	PIPE_CONF_CHECK_I(adjusted_mode.crtc_htotal);
@@ -10444,6 +10512,7 @@
 	if ((INTEL_INFO(dev)->gen < 8 && !IS_HASWELL(dev)) ||
 	    IS_VALLEYVIEW(dev))
 		PIPE_CONF_CHECK_I(limited_color_range);
+	PIPE_CONF_CHECK_I(has_infoframe);
 
 	PIPE_CONF_CHECK_I(has_audio);
 
@@ -10500,6 +10569,9 @@
 	PIPE_CONF_CHECK_X(dpll_hw_state.fp0);
 	PIPE_CONF_CHECK_X(dpll_hw_state.fp1);
 	PIPE_CONF_CHECK_X(dpll_hw_state.wrpll);
+	PIPE_CONF_CHECK_X(dpll_hw_state.ctrl1);
+	PIPE_CONF_CHECK_X(dpll_hw_state.cfgcr1);
+	PIPE_CONF_CHECK_X(dpll_hw_state.cfgcr2);
 
 	if (IS_G4X(dev) || INTEL_INFO(dev)->gen >= 5)
 		PIPE_CONF_CHECK_I(pipe_bpp);
@@ -10509,6 +10581,7 @@
 
 #undef PIPE_CONF_CHECK_X
 #undef PIPE_CONF_CHECK_I
+#undef PIPE_CONF_CHECK_I_ALT
 #undef PIPE_CONF_CHECK_FLAGS
 #undef PIPE_CONF_CHECK_CLOCK_FUZZY
 #undef PIPE_CONF_QUIRK
@@ -10516,6 +10589,56 @@
 	return true;
 }
 
+static void check_wm_state(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct skl_ddb_allocation hw_ddb, *sw_ddb;
+	struct intel_crtc *intel_crtc;
+	int plane;
+
+	if (INTEL_INFO(dev)->gen < 9)
+		return;
+
+	skl_ddb_get_hw_state(dev_priv, &hw_ddb);
+	sw_ddb = &dev_priv->wm.skl_hw.ddb;
+
+	for_each_intel_crtc(dev, intel_crtc) {
+		struct skl_ddb_entry *hw_entry, *sw_entry;
+		const enum pipe pipe = intel_crtc->pipe;
+
+		if (!intel_crtc->active)
+			continue;
+
+		/* planes */
+		for_each_plane(pipe, plane) {
+			hw_entry = &hw_ddb.plane[pipe][plane];
+			sw_entry = &sw_ddb->plane[pipe][plane];
+
+			if (skl_ddb_entry_equal(hw_entry, sw_entry))
+				continue;
+
+			DRM_ERROR("mismatch in DDB state pipe %c plane %d "
+				  "(expected (%u,%u), found (%u,%u))\n",
+				  pipe_name(pipe), plane + 1,
+				  sw_entry->start, sw_entry->end,
+				  hw_entry->start, hw_entry->end);
+		}
+
+		/* cursor */
+		hw_entry = &hw_ddb.cursor[pipe];
+		sw_entry = &sw_ddb->cursor[pipe];
+
+		if (skl_ddb_entry_equal(hw_entry, sw_entry))
+			continue;
+
+		DRM_ERROR("mismatch in DDB state pipe %c cursor "
+			  "(expected (%u,%u), found (%u,%u))\n",
+			  pipe_name(pipe),
+			  sw_entry->start, sw_entry->end,
+			  hw_entry->start, hw_entry->end);
+	}
+}
+
 static void
 check_connector_state(struct drm_device *dev)
 {
@@ -10538,8 +10661,7 @@
 	struct intel_encoder *encoder;
 	struct intel_connector *connector;
 
-	list_for_each_entry(encoder, &dev->mode_config.encoder_list,
-			    base.head) {
+	for_each_intel_encoder(dev, encoder) {
 		bool enabled = false;
 		bool active = false;
 		enum pipe pipe, tracked_pipe;
@@ -10618,8 +10740,7 @@
 		WARN(crtc->active && !crtc->base.enabled,
 		     "active crtc, but not enabled in sw tracking\n");
 
-		list_for_each_entry(encoder, &dev->mode_config.encoder_list,
-				    base.head) {
+		for_each_intel_encoder(dev, encoder) {
 			if (encoder->base.crtc != &crtc->base)
 				continue;
 			enabled = true;
@@ -10637,12 +10758,12 @@
 		active = dev_priv->display.get_pipe_config(crtc,
 							   &pipe_config);
 
-		/* hw state is inconsistent with the pipe A quirk */
-		if (crtc->pipe == PIPE_A && dev_priv->quirks & QUIRK_PIPEA_FORCE)
+		/* hw state is inconsistent with the pipe quirk */
+		if ((crtc->pipe == PIPE_A && dev_priv->quirks & QUIRK_PIPEA_FORCE) ||
+		    (crtc->pipe == PIPE_B && dev_priv->quirks & QUIRK_PIPEB_FORCE))
 			active = crtc->active;
 
-		list_for_each_entry(encoder, &dev->mode_config.encoder_list,
-				    base.head) {
+		for_each_intel_encoder(dev, encoder) {
 			enum pipe pipe;
 			if (encoder->base.crtc != &crtc->base)
 				continue;
@@ -10684,9 +10805,9 @@
 
 		active = pll->get_hw_state(dev_priv, pll, &dpll_hw_state);
 
-		WARN(pll->active > pll->refcount,
+		WARN(pll->active > hweight32(pll->config.crtc_mask),
 		     "more active pll users than references: %i vs %i\n",
-		     pll->active, pll->refcount);
+		     pll->active, hweight32(pll->config.crtc_mask));
 		WARN(pll->active && !pll->on,
 		     "pll in active use but not on in sw tracking\n");
 		WARN(pll->on && !pll->active,
@@ -10704,11 +10825,11 @@
 		WARN(pll->active != active_crtcs,
 		     "pll active crtcs mismatch (expected %i, found %i)\n",
 		     pll->active, active_crtcs);
-		WARN(pll->refcount != enabled_crtcs,
+		WARN(hweight32(pll->config.crtc_mask) != enabled_crtcs,
 		     "pll enabled crtcs mismatch (expected %i, found %i)\n",
-		     pll->refcount, enabled_crtcs);
+		     hweight32(pll->config.crtc_mask), enabled_crtcs);
 
-		WARN(pll->on && memcmp(&pll->hw_state, &dpll_hw_state,
+		WARN(pll->on && memcmp(&pll->config.hw_state, &dpll_hw_state,
 				       sizeof(dpll_hw_state)),
 		     "pll hw state mismatch\n");
 	}
@@ -10717,6 +10838,7 @@
 void
 intel_modeset_check_state(struct drm_device *dev)
 {
+	check_wm_state(dev);
 	check_connector_state(dev);
 	check_encoder_state(dev);
 	check_crtc_state(dev);
@@ -10767,51 +10889,66 @@
 
 		crtc->scanline_offset = vtotal - 1;
 	} else if (HAS_DDI(dev) &&
-		   intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_HDMI)) {
+		   intel_pipe_has_type(crtc, INTEL_OUTPUT_HDMI)) {
 		crtc->scanline_offset = 2;
 	} else
 		crtc->scanline_offset = 1;
 }
 
+static struct intel_crtc_config *
+intel_modeset_compute_config(struct drm_crtc *crtc,
+			     struct drm_display_mode *mode,
+			     struct drm_framebuffer *fb,
+			     unsigned *modeset_pipes,
+			     unsigned *prepare_pipes,
+			     unsigned *disable_pipes)
+{
+	struct intel_crtc_config *pipe_config = NULL;
+
+	intel_modeset_affected_pipes(crtc, modeset_pipes,
+				     prepare_pipes, disable_pipes);
+
+	if ((*modeset_pipes) == 0)
+		goto out;
+
+	/*
+	 * Note this needs changes when we start tracking multiple modes
+	 * and crtcs.  At that point we'll need to compute the whole config
+	 * (i.e. one pipe_config for each crtc) rather than just the one
+	 * for this crtc.
+	 */
+	pipe_config = intel_modeset_pipe_config(crtc, fb, mode);
+	if (IS_ERR(pipe_config)) {
+		goto out;
+	}
+	intel_dump_pipe_config(to_intel_crtc(crtc), pipe_config,
+			       "[modeset]");
+	to_intel_crtc(crtc)->new_config = pipe_config;
+
+out:
+	return pipe_config;
+}
+
 static int __intel_set_mode(struct drm_crtc *crtc,
 			    struct drm_display_mode *mode,
-			    int x, int y, struct drm_framebuffer *fb)
+			    int x, int y, struct drm_framebuffer *fb,
+			    struct intel_crtc_config *pipe_config,
+			    unsigned modeset_pipes,
+			    unsigned prepare_pipes,
+			    unsigned disable_pipes)
 {
 	struct drm_device *dev = crtc->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_display_mode *saved_mode;
-	struct intel_crtc_config *pipe_config = NULL;
 	struct intel_crtc *intel_crtc;
-	unsigned disable_pipes, prepare_pipes, modeset_pipes;
 	int ret = 0;
 
 	saved_mode = kmalloc(sizeof(*saved_mode), GFP_KERNEL);
 	if (!saved_mode)
 		return -ENOMEM;
 
-	intel_modeset_affected_pipes(crtc, &modeset_pipes,
-				     &prepare_pipes, &disable_pipes);
-
 	*saved_mode = crtc->mode;
 
-	/* Hack: Because we don't (yet) support global modeset on multiple
-	 * crtcs, we don't keep track of the new mode for more than one crtc.
-	 * Hence simply check whether any bit is set in modeset_pipes in all the
-	 * pieces of code that are not yet converted to deal with mutliple crtcs
-	 * changing their mode at the same time. */
-	if (modeset_pipes) {
-		pipe_config = intel_modeset_pipe_config(crtc, fb, mode);
-		if (IS_ERR(pipe_config)) {
-			ret = PTR_ERR(pipe_config);
-			pipe_config = NULL;
-
-			goto out;
-		}
-		intel_dump_pipe_config(to_intel_crtc(crtc), pipe_config,
-				       "[modeset]");
-		to_intel_crtc(crtc)->new_config = pipe_config;
-	}
-
 	/*
 	 * See if the config requires any additional preparation, e.g.
 	 * to adjust global state with pipes off.  We need to do this
@@ -10826,6 +10963,22 @@
 		prepare_pipes &= ~disable_pipes;
 	}
 
+	if (dev_priv->display.crtc_compute_clock) {
+		unsigned clear_pipes = modeset_pipes | disable_pipes;
+
+		ret = intel_shared_dpll_start_config(dev_priv, clear_pipes);
+		if (ret)
+			goto done;
+
+		for_each_intel_crtc_masked(dev, modeset_pipes, intel_crtc) {
+			ret = dev_priv->display.crtc_compute_clock(intel_crtc);
+			if (ret) {
+				intel_shared_dpll_abort_config(dev_priv);
+				goto done;
+			}
+		}
+	}
+
 	for_each_intel_crtc_masked(dev, disable_pipes, intel_crtc)
 		intel_crtc_disable(&intel_crtc->base);
 
@@ -10836,6 +10989,10 @@
 
 	/* crtc->mode is already used by the ->mode_set callbacks, hence we need
 	 * to set it here already despite that we pass it down the callchain.
+	 *
+	 * Note we'll need to fix this up when we start tracking multiple
+	 * pipes; here we assume a single modeset_pipe and only track the
+	 * single crtc and mode.
 	 */
 	if (modeset_pipes) {
 		crtc->mode = *mode;
@@ -10857,27 +11014,37 @@
 	 * update the the output configuration. */
 	intel_modeset_update_state(dev, prepare_pipes);
 
-	if (dev_priv->display.modeset_global_resources)
-		dev_priv->display.modeset_global_resources(dev);
+	modeset_update_crtc_power_domains(dev);
 
 	/* Set up the DPLL and any encoders state that needs to adjust or depend
 	 * on the DPLL.
 	 */
 	for_each_intel_crtc_masked(dev, modeset_pipes, intel_crtc) {
-		struct drm_framebuffer *old_fb = crtc->primary->fb;
-		struct drm_i915_gem_object *old_obj = intel_fb_obj(old_fb);
+		struct drm_i915_gem_object *old_obj = intel_fb_obj(crtc->primary->fb);
 		struct drm_i915_gem_object *obj = intel_fb_obj(fb);
 
+		if (!assert_pipe_disabled(dev_priv, intel_crtc->pipe) ||
+		    !assert_plane_disabled(dev_priv, intel_crtc->plane)) {
+			ret = -EIO;
+			goto done;
+		}
+
+		/* The display engine is disabled. We can safely remove the
+		 * current object pointed to by hardware registers as before
+		 * we enable the pipe again, we will always update those
+		 * registers to point to the currently pinned object. Even
+		 * if we fail, though the hardware points to a stale address,
+		 * that address is never read.
+		 */
+
 		mutex_lock(&dev->struct_mutex);
-		ret = intel_pin_and_fence_fb_obj(dev,
-						 obj,
-						 NULL);
+		ret = intel_pin_and_fence_fb_obj(crtc->primary, fb, NULL);
 		if (ret != 0) {
 			DRM_ERROR("pin & fence failed\n");
 			mutex_unlock(&dev->struct_mutex);
 			goto done;
 		}
-		if (old_fb)
+		if (old_obj)
 			intel_unpin_fb_obj(old_obj);
 		i915_gem_track_fb(old_obj, obj,
 				  INTEL_FRONTBUFFER_PRIMARY(intel_crtc->pipe));
@@ -10886,11 +11053,6 @@
 		crtc->primary->fb = fb;
 		crtc->x = x;
 		crtc->y = y;
-
-		ret = dev_priv->display.crtc_mode_set(&intel_crtc->base,
-						      x, y, fb);
-		if (ret)
-			goto done;
 	}
 
 	/* Now enable the clocks, plane, pipe, and connectors that we set up. */
@@ -10905,19 +11067,23 @@
 	if (ret && crtc->enabled)
 		crtc->mode = *saved_mode;
 
-out:
 	kfree(pipe_config);
 	kfree(saved_mode);
 	return ret;
 }
 
-static int intel_set_mode(struct drm_crtc *crtc,
-			  struct drm_display_mode *mode,
-			  int x, int y, struct drm_framebuffer *fb)
+static int intel_set_mode_pipes(struct drm_crtc *crtc,
+				struct drm_display_mode *mode,
+				int x, int y, struct drm_framebuffer *fb,
+				struct intel_crtc_config *pipe_config,
+				unsigned modeset_pipes,
+				unsigned prepare_pipes,
+				unsigned disable_pipes)
 {
 	int ret;
 
-	ret = __intel_set_mode(crtc, mode, x, y, fb);
+	ret = __intel_set_mode(crtc, mode, x, y, fb, pipe_config, modeset_pipes,
+			       prepare_pipes, disable_pipes);
 
 	if (ret == 0)
 		intel_modeset_check_state(crtc->dev);
@@ -10925,6 +11091,26 @@
 	return ret;
 }
 
+static int intel_set_mode(struct drm_crtc *crtc,
+			  struct drm_display_mode *mode,
+			  int x, int y, struct drm_framebuffer *fb)
+{
+	struct intel_crtc_config *pipe_config;
+	unsigned modeset_pipes, prepare_pipes, disable_pipes;
+
+	pipe_config = intel_modeset_compute_config(crtc, mode, fb,
+						   &modeset_pipes,
+						   &prepare_pipes,
+						   &disable_pipes);
+
+	if (IS_ERR(pipe_config))
+		return PTR_ERR(pipe_config);
+
+	return intel_set_mode_pipes(crtc, mode, x, y, fb, pipe_config,
+				    modeset_pipes, prepare_pipes,
+				    disable_pipes);
+}
+
 void intel_crtc_restore_mode(struct drm_crtc *crtc)
 {
 	intel_set_mode(crtc, &crtc->mode, crtc->x, crtc->y, crtc->primary->fb);
@@ -11010,7 +11196,7 @@
 	}
 
 	count = 0;
-	list_for_each_entry(encoder, &dev->mode_config.encoder_list, base.head) {
+	for_each_intel_encoder(dev, encoder) {
 		encoder->new_crtc =
 			to_intel_crtc(config->save_encoder_crtcs[count++]);
 	}
@@ -11169,8 +11355,7 @@
 	}
 
 	/* Check for any encoders that needs to be disabled. */
-	list_for_each_entry(encoder, &dev->mode_config.encoder_list,
-			    base.head) {
+	for_each_intel_encoder(dev, encoder) {
 		int num_connectors = 0;
 		list_for_each_entry(connector,
 				    &dev->mode_config.connector_list,
@@ -11203,9 +11388,7 @@
 	for_each_intel_crtc(dev, crtc) {
 		crtc->new_enabled = false;
 
-		list_for_each_entry(encoder,
-				    &dev->mode_config.encoder_list,
-				    base.head) {
+		for_each_intel_encoder(dev, encoder) {
 			if (encoder->new_crtc == crtc) {
 				crtc->new_enabled = true;
 				break;
@@ -11242,7 +11425,7 @@
 			connector->new_encoder = NULL;
 	}
 
-	list_for_each_entry(encoder, &dev->mode_config.encoder_list, base.head) {
+	for_each_intel_encoder(dev, encoder) {
 		if (encoder->new_crtc == crtc)
 			encoder->new_crtc = NULL;
 	}
@@ -11256,6 +11439,8 @@
 	struct drm_device *dev;
 	struct drm_mode_set save_set;
 	struct intel_set_config *config;
+	struct intel_crtc_config *pipe_config;
+	unsigned modeset_pipes, prepare_pipes, disable_pipes;
 	int ret;
 
 	BUG_ON(!set);
@@ -11301,11 +11486,37 @@
 	if (ret)
 		goto fail;
 
+	pipe_config = intel_modeset_compute_config(set->crtc, set->mode,
+						   set->fb,
+						   &modeset_pipes,
+						   &prepare_pipes,
+						   &disable_pipes);
+	if (IS_ERR(pipe_config)) {
+		ret = PTR_ERR(pipe_config);
+		goto fail;
+	} else if (pipe_config) {
+		if (to_intel_crtc(set->crtc)->new_config->has_audio !=
+		    to_intel_crtc(set->crtc)->config.has_audio)
+			config->mode_changed = true;
+
+		/* Force mode sets for any infoframe stuff */
+		if (to_intel_crtc(set->crtc)->new_config->has_infoframe ||
+		    to_intel_crtc(set->crtc)->config.has_infoframe)
+			config->mode_changed = true;
+	}
+
+	/* set_mode will free it in the mode_changed case */
+	if (!config->mode_changed)
+		kfree(pipe_config);
+
+	intel_update_pipe_size(to_intel_crtc(set->crtc));
+
 	if (config->mode_changed) {
-		ret = intel_set_mode(set->crtc, set->mode,
-				     set->x, set->y, set->fb);
+		ret = intel_set_mode_pipes(set->crtc, set->mode,
+					   set->x, set->y, set->fb, pipe_config,
+					   modeset_pipes, prepare_pipes,
+					   disable_pipes);
 	} else if (config->fb_changed) {
-		struct drm_i915_private *dev_priv = dev->dev_private;
 		struct intel_crtc *intel_crtc = to_intel_crtc(set->crtc);
 
 		intel_crtc_wait_for_pending_flips(set->crtc);
@@ -11319,8 +11530,7 @@
 		 */
 		if (!intel_crtc->primary_enabled && ret == 0) {
 			WARN_ON(!intel_crtc->active);
-			intel_enable_primary_hw_plane(dev_priv, intel_crtc->plane,
-						      intel_crtc->pipe);
+			intel_enable_primary_hw_plane(set->crtc->primary, set->crtc);
 		}
 
 		/*
@@ -11331,7 +11541,7 @@
 		 * flipping, so increasing its cost here shouldn't be a big
 		 * deal).
 		 */
-		if (i915.fastboot && ret == 0)
+		if (i915_module.fastboot && ret == 0)
 			intel_modeset_check_state(set->crtc->dev);
 	}
 
@@ -11375,7 +11585,7 @@
 {
 	uint32_t val;
 
-	if (!intel_display_power_enabled(dev_priv, POWER_DOMAIN_PLLS))
+	if (!intel_display_power_is_enabled(dev_priv, POWER_DOMAIN_PLLS))
 		return false;
 
 	val = I915_READ(PCH_DPLL(pll->id));
@@ -11389,8 +11599,8 @@
 static void ibx_pch_dpll_mode_set(struct drm_i915_private *dev_priv,
 				  struct intel_shared_dpll *pll)
 {
-	I915_WRITE(PCH_FP0(pll->id), pll->hw_state.fp0);
-	I915_WRITE(PCH_FP1(pll->id), pll->hw_state.fp1);
+	I915_WRITE(PCH_FP0(pll->id), pll->config.hw_state.fp0);
+	I915_WRITE(PCH_FP1(pll->id), pll->config.hw_state.fp1);
 }
 
 static void ibx_pch_dpll_enable(struct drm_i915_private *dev_priv,
@@ -11399,7 +11609,7 @@
 	/* PCH refclock must be enabled first */
 	ibx_assert_pch_refclk_enabled(dev_priv);
 
-	I915_WRITE(PCH_DPLL(pll->id), pll->hw_state.dpll);
+	I915_WRITE(PCH_DPLL(pll->id), pll->config.hw_state.dpll);
 
 	/* Wait for the clocks to stabilize. */
 	POSTING_READ(PCH_DPLL(pll->id));
@@ -11410,7 +11620,7 @@
 	 *
 	 * So write it again.
 	 */
-	I915_WRITE(PCH_DPLL(pll->id), pll->hw_state.dpll);
+	I915_WRITE(PCH_DPLL(pll->id), pll->config.hw_state.dpll);
 	POSTING_READ(PCH_DPLL(pll->id));
 	udelay(200);
 }
@@ -11473,8 +11683,6 @@
 intel_primary_plane_disable(struct drm_plane *plane)
 {
 	struct drm_device *dev = plane->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_plane *intel_plane = to_intel_plane(plane);
 	struct intel_crtc *intel_crtc;
 
 	if (!plane->fb)
@@ -11497,8 +11705,8 @@
 		goto disable_unpin;
 
 	intel_crtc_wait_for_pending_flips(plane->crtc);
-	intel_disable_primary_hw_plane(dev_priv, intel_plane->plane,
-				       intel_plane->pipe);
+	intel_disable_primary_hw_plane(plane, plane->crtc);
+
 disable_unpin:
 	mutex_lock(&dev->struct_mutex);
 	i915_gem_track_fb(intel_fb_obj(plane->fb), NULL,
@@ -11511,123 +11719,195 @@
 }
 
 static int
-intel_primary_plane_setplane(struct drm_plane *plane, struct drm_crtc *crtc,
-			     struct drm_framebuffer *fb, int crtc_x, int crtc_y,
-			     unsigned int crtc_w, unsigned int crtc_h,
-			     uint32_t src_x, uint32_t src_y,
-			     uint32_t src_w, uint32_t src_h)
+intel_check_primary_plane(struct drm_plane *plane,
+			  struct intel_plane_state *state)
+{
+	struct drm_crtc *crtc = state->crtc;
+	struct drm_framebuffer *fb = state->fb;
+	struct drm_rect *dest = &state->dst;
+	struct drm_rect *src = &state->src;
+	const struct drm_rect *clip = &state->clip;
+
+	return drm_plane_helper_check_update(plane, crtc, fb,
+					     src, dest, clip,
+					     DRM_PLANE_HELPER_NO_SCALING,
+					     DRM_PLANE_HELPER_NO_SCALING,
+					     false, true, &state->visible);
+}
+
+static int
+intel_prepare_primary_plane(struct drm_plane *plane,
+			    struct intel_plane_state *state)
 {
+	struct drm_crtc *crtc = state->crtc;
+	struct drm_framebuffer *fb = state->fb;
 	struct drm_device *dev = crtc->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
-	struct intel_plane *intel_plane = to_intel_plane(plane);
+	enum pipe pipe = intel_crtc->pipe;
 	struct drm_i915_gem_object *obj = intel_fb_obj(fb);
 	struct drm_i915_gem_object *old_obj = intel_fb_obj(plane->fb);
-	struct drm_rect dest = {
-		/* integer pixels */
-		.x1 = crtc_x,
-		.y1 = crtc_y,
-		.x2 = crtc_x + crtc_w,
-		.y2 = crtc_y + crtc_h,
-	};
-	struct drm_rect src = {
-		/* 16.16 fixed point */
-		.x1 = src_x,
-		.y1 = src_y,
-		.x2 = src_x + src_w,
-		.y2 = src_y + src_h,
-	};
-	const struct drm_rect clip = {
-		/* integer pixels */
-		.x2 = intel_crtc->active ? intel_crtc->config.pipe_src_w : 0,
-		.y2 = intel_crtc->active ? intel_crtc->config.pipe_src_h : 0,
-	};
-	bool visible;
 	int ret;
 
-	ret = drm_plane_helper_check_update(plane, crtc, fb,
-					    &src, &dest, &clip,
-					    DRM_PLANE_HELPER_NO_SCALING,
-					    DRM_PLANE_HELPER_NO_SCALING,
-					    false, true, &visible);
+	intel_crtc_wait_for_pending_flips(crtc);
 
-	if (ret)
-		return ret;
+	if (intel_crtc_has_pending_flip(crtc)) {
+		DRM_ERROR("pipe is still busy with an old pageflip\n");
+		return -EBUSY;
+	}
 
-	/*
-	 * If the CRTC isn't enabled, we're just pinning the framebuffer,
-	 * updating the fb pointer, and returning without touching the
-	 * hardware.  This allows us to later do a drmModeSetCrtc with fb=-1 to
-	 * turn on the display with all planes setup as desired.
-	 */
-	if (!crtc->enabled) {
+	if (old_obj != obj) {
 		mutex_lock(&dev->struct_mutex);
-
-		/*
-		 * If we already called setplane while the crtc was disabled,
-		 * we may have an fb pinned; unpin it.
-		 */
-		if (plane->fb)
-			intel_unpin_fb_obj(old_obj);
-
-		i915_gem_track_fb(old_obj, obj,
-				  INTEL_FRONTBUFFER_PRIMARY(intel_crtc->pipe));
-
-		/* Pin and return without programming hardware */
-		ret = intel_pin_and_fence_fb_obj(dev, obj, NULL);
+		ret = intel_pin_and_fence_fb_obj(plane, fb, NULL);
+		if (ret == 0)
+			i915_gem_track_fb(old_obj, obj,
+					  INTEL_FRONTBUFFER_PRIMARY(pipe));
 		mutex_unlock(&dev->struct_mutex);
-
-		return ret;
+		if (ret != 0) {
+			DRM_DEBUG_KMS("pin & fence failed\n");
+			return ret;
+		}
 	}
 
-	intel_crtc_wait_for_pending_flips(crtc);
+	return 0;
+}
 
-	/*
-	 * If clipping results in a non-visible primary plane, we'll disable
-	 * the primary plane.  Note that this is a bit different than what
-	 * happens if userspace explicitly disables the plane by passing fb=0
-	 * because plane->fb still gets set and pinned.
-	 */
-	if (!visible) {
-		mutex_lock(&dev->struct_mutex);
+static void
+intel_commit_primary_plane(struct drm_plane *plane,
+			   struct intel_plane_state *state)
+{
+	struct drm_crtc *crtc = state->crtc;
+	struct drm_framebuffer *fb = state->fb;
+	struct drm_device *dev = crtc->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	enum pipe pipe = intel_crtc->pipe;
+	struct drm_framebuffer *old_fb = plane->fb;
+	struct drm_i915_gem_object *obj = intel_fb_obj(fb);
+	struct drm_i915_gem_object *old_obj = intel_fb_obj(plane->fb);
+	struct intel_plane *intel_plane = to_intel_plane(plane);
+	struct drm_rect *src = &state->src;
 
+	crtc->primary->fb = fb;
+	crtc->x = src->x1 >> 16;
+	crtc->y = src->y1 >> 16;
+
+	intel_plane->crtc_x = state->orig_dst.x1;
+	intel_plane->crtc_y = state->orig_dst.y1;
+	intel_plane->crtc_w = drm_rect_width(&state->orig_dst);
+	intel_plane->crtc_h = drm_rect_height(&state->orig_dst);
+	intel_plane->src_x = state->orig_src.x1;
+	intel_plane->src_y = state->orig_src.y1;
+	intel_plane->src_w = drm_rect_width(&state->orig_src);
+	intel_plane->src_h = drm_rect_height(&state->orig_src);
+	intel_plane->obj = obj;
+
+	if (intel_crtc->active) {
 		/*
-		 * Try to pin the new fb first so that we can bail out if we
-		 * fail.
+		 * FBC does not work on some platforms for rotated
+		 * planes, so disable it when rotation is not 0 and
+		 * update it when rotation is set back to 0.
+		 *
+		 * FIXME: This is redundant with the fbc update done in
+		 * the primary plane enable function except that that
+		 * one is done too late. We eventually need to unify
+		 * this.
 		 */
-		if (plane->fb != fb) {
-			ret = intel_pin_and_fence_fb_obj(dev, obj, NULL);
-			if (ret) {
-				mutex_unlock(&dev->struct_mutex);
-				return ret;
-			}
+		if (intel_crtc->primary_enabled &&
+		    INTEL_INFO(dev)->gen <= 4 && !IS_G4X(dev) &&
+		    dev_priv->fbc.plane == intel_crtc->plane &&
+		    intel_plane->rotation != BIT(DRM_ROTATE_0)) {
+			intel_disable_fbc(dev);
 		}
 
-		i915_gem_track_fb(old_obj, obj,
-				  INTEL_FRONTBUFFER_PRIMARY(intel_crtc->pipe));
+		if (state->visible) {
+			bool was_enabled = intel_crtc->primary_enabled;
+
+			/* FIXME: kill this fastboot hack */
+			intel_update_pipe_size(intel_crtc);
 
-		if (intel_crtc->primary_enabled)
-			intel_disable_primary_hw_plane(dev_priv,
-						       intel_plane->plane,
-						       intel_plane->pipe);
+			intel_crtc->primary_enabled = true;
 
+			dev_priv->display.update_primary_plane(crtc, plane->fb,
+					crtc->x, crtc->y);
+
+			/*
+			 * BDW signals flip done immediately if the plane
+			 * is disabled, even if the plane enable is already
+			 * armed to occur at the next vblank :(
+			 */
+			if (IS_BROADWELL(dev) && !was_enabled)
+				intel_wait_for_vblank(dev, intel_crtc->pipe);
+		} else {
+			/*
+			 * If clipping results in a non-visible primary plane,
+			 * we'll disable the primary plane.  Note that this is
+			 * a bit different than what happens if userspace
+			 * explicitly disables the plane by passing fb=0
+			 * because plane->fb still gets set and pinned.
+			 */
+			intel_disable_primary_hw_plane(plane, crtc);
+		}
 
-		if (plane->fb != fb)
-			if (plane->fb)
-				intel_unpin_fb_obj(old_obj);
+		intel_frontbuffer_flip(dev, INTEL_FRONTBUFFER_PRIMARY(pipe));
 
+		mutex_lock(&dev->struct_mutex);
+		intel_update_fbc(dev);
 		mutex_unlock(&dev->struct_mutex);
+	}
 
-		return 0;
+	if (old_fb && old_fb != fb) {
+		if (intel_crtc->active)
+			intel_wait_for_vblank(dev, intel_crtc->pipe);
+
+		mutex_lock(&dev->struct_mutex);
+		intel_unpin_fb_obj(old_obj);
+		mutex_unlock(&dev->struct_mutex);
 	}
+}
+
+static int
+intel_primary_plane_setplane(struct drm_plane *plane, struct drm_crtc *crtc,
+			     struct drm_framebuffer *fb, int crtc_x, int crtc_y,
+			     unsigned int crtc_w, unsigned int crtc_h,
+			     uint32_t src_x, uint32_t src_y,
+			     uint32_t src_w, uint32_t src_h)
+{
+	struct intel_plane_state state;
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	int ret;
+
+	state.crtc = crtc;
+	state.fb = fb;
+
+	/* sample coordinates in 16.16 fixed point */
+	state.src.x1 = src_x;
+	state.src.x2 = src_x + src_w;
+	state.src.y1 = src_y;
+	state.src.y2 = src_y + src_h;
+
+	/* integer pixels */
+	state.dst.x1 = crtc_x;
+	state.dst.x2 = crtc_x + crtc_w;
+	state.dst.y1 = crtc_y;
+	state.dst.y2 = crtc_y + crtc_h;
+
+	state.clip.x1 = 0;
+	state.clip.y1 = 0;
+	state.clip.x2 = intel_crtc->active ? intel_crtc->config.pipe_src_w : 0;
+	state.clip.y2 = intel_crtc->active ? intel_crtc->config.pipe_src_h : 0;
+
+	state.orig_src = state.src;
+	state.orig_dst = state.dst;
 
-	ret = intel_pipe_set_base(crtc, src.x1, src.y1, fb);
+	ret = intel_check_primary_plane(plane, &state);
 	if (ret)
 		return ret;
 
-	if (!intel_crtc->primary_enabled)
-		intel_enable_primary_hw_plane(dev_priv, intel_crtc->plane,
-					      intel_crtc->pipe);
+	ret = intel_prepare_primary_plane(plane, &state);
+	if (ret)
+		return ret;
+
+	intel_commit_primary_plane(plane, &state);
 
 	return 0;
 }
@@ -11644,6 +11924,7 @@
 	.update_plane = intel_primary_plane_setplane,
 	.disable_plane = intel_primary_plane_disable,
 	.destroy = intel_plane_destroy,
+	.set_property = intel_plane_set_property
 };
 
 static struct drm_plane *intel_primary_plane_create(struct drm_device *dev,
@@ -11661,6 +11942,7 @@
 	primary->max_downscale = 1;
 	primary->pipe = pipe;
 	primary->plane = pipe;
+	primary->rotation = BIT(DRM_ROTATE_0);
 	if (HAS_FBC(dev) && INTEL_INFO(dev)->gen < 4)
 		primary->plane = !pipe;
 
@@ -11676,6 +11958,19 @@
 				 &intel_primary_plane_funcs,
 				 intel_primary_formats, num_formats,
 				 DRM_PLANE_TYPE_PRIMARY);
+
+	if (INTEL_INFO(dev)->gen >= 4) {
+		if (!dev->mode_config.rotation_property)
+			dev->mode_config.rotation_property =
+				drm_mode_create_rotation_property(dev,
+							BIT(DRM_ROTATE_0) |
+							BIT(DRM_ROTATE_180));
+		if (dev->mode_config.rotation_property)
+			drm_object_attach_property(&primary->base.base,
+				dev->mode_config.rotation_property,
+				primary->rotation);
+	}
+
 	return &primary->base;
 }
 
@@ -11691,58 +11986,146 @@
 }
 
 static int
-intel_cursor_plane_update(struct drm_plane *plane, struct drm_crtc *crtc,
-			  struct drm_framebuffer *fb, int crtc_x, int crtc_y,
-			  unsigned int crtc_w, unsigned int crtc_h,
-			  uint32_t src_x, uint32_t src_y,
-			  uint32_t src_w, uint32_t src_h)
+intel_check_cursor_plane(struct drm_plane *plane,
+			 struct intel_plane_state *state)
 {
-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
-	struct intel_framebuffer *intel_fb = to_intel_framebuffer(fb);
-	struct drm_i915_gem_object *obj = intel_fb->obj;
-	struct drm_rect dest = {
-		/* integer pixels */
-		.x1 = crtc_x,
-		.y1 = crtc_y,
-		.x2 = crtc_x + crtc_w,
-		.y2 = crtc_y + crtc_h,
-	};
-	struct drm_rect src = {
-		/* 16.16 fixed point */
-		.x1 = src_x,
-		.y1 = src_y,
-		.x2 = src_x + src_w,
-		.y2 = src_y + src_h,
-	};
-	const struct drm_rect clip = {
-		/* integer pixels */
-		.x2 = intel_crtc->active ? intel_crtc->config.pipe_src_w : 0,
-		.y2 = intel_crtc->active ? intel_crtc->config.pipe_src_h : 0,
-	};
-	bool visible;
+	struct drm_crtc *crtc = state->crtc;
+	struct drm_device *dev = crtc->dev;
+	struct drm_framebuffer *fb = state->fb;
+	struct drm_rect *dest = &state->dst;
+	struct drm_rect *src = &state->src;
+	const struct drm_rect *clip = &state->clip;
+	struct drm_i915_gem_object *obj = intel_fb_obj(fb);
+	int crtc_w, crtc_h;
+	unsigned stride;
 	int ret;
 
 	ret = drm_plane_helper_check_update(plane, crtc, fb,
-					    &src, &dest, &clip,
+					    src, dest, clip,
 					    DRM_PLANE_HELPER_NO_SCALING,
 					    DRM_PLANE_HELPER_NO_SCALING,
-					    true, true, &visible);
+					    true, true, &state->visible);
 	if (ret)
 		return ret;
 
-	crtc->cursor_x = crtc_x;
-	crtc->cursor_y = crtc_y;
+
+	/* if we want to turn off the cursor ignore width and height */
+	if (!obj)
+		return 0;
+
+	/* Check for which cursor types we support */
+	crtc_w = drm_rect_width(&state->orig_dst);
+	crtc_h = drm_rect_height(&state->orig_dst);
+	if (!cursor_size_ok(dev, crtc_w, crtc_h)) {
+		DRM_DEBUG("Cursor dimension not supported\n");
+		return -EINVAL;
+	}
+
+	stride = roundup_pow_of_two(crtc_w) * 4;
+	if (obj->base.size < stride * crtc_h) {
+		DRM_DEBUG_KMS("buffer is too small\n");
+		return -ENOMEM;
+	}
+
+	if (fb == crtc->cursor->fb)
+		return 0;
+
+	/* we only need to pin inside GTT if cursor is non-phy */
+	mutex_lock(&dev->struct_mutex);
+	if (!INTEL_INFO(dev)->cursor_needs_physical && obj->tiling_mode) {
+		DRM_DEBUG_KMS("cursor cannot be tiled\n");
+		ret = -EINVAL;
+	}
+	mutex_unlock(&dev->struct_mutex);
+
+	return ret;
+}
+
+static int
+intel_commit_cursor_plane(struct drm_plane *plane,
+			  struct intel_plane_state *state)
+{
+	struct drm_crtc *crtc = state->crtc;
+	struct drm_framebuffer *fb = state->fb;
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	struct intel_plane *intel_plane = to_intel_plane(plane);
+	struct intel_framebuffer *intel_fb = to_intel_framebuffer(fb);
+	struct drm_i915_gem_object *obj = intel_fb->obj;
+	int crtc_w, crtc_h;
+
+	crtc->cursor_x = state->orig_dst.x1;
+	crtc->cursor_y = state->orig_dst.y1;
+
+	intel_plane->crtc_x = state->orig_dst.x1;
+	intel_plane->crtc_y = state->orig_dst.y1;
+	intel_plane->crtc_w = drm_rect_width(&state->orig_dst);
+	intel_plane->crtc_h = drm_rect_height(&state->orig_dst);
+	intel_plane->src_x = state->orig_src.x1;
+	intel_plane->src_y = state->orig_src.y1;
+	intel_plane->src_w = drm_rect_width(&state->orig_src);
+	intel_plane->src_h = drm_rect_height(&state->orig_src);
+	intel_plane->obj = obj;
+
 	if (fb != crtc->cursor->fb) {
+		crtc_w = drm_rect_width(&state->orig_dst);
+		crtc_h = drm_rect_height(&state->orig_dst);
 		return intel_crtc_cursor_set_obj(crtc, obj, crtc_w, crtc_h);
 	} else {
-		intel_crtc_update_cursor(crtc, visible);
+		intel_crtc_update_cursor(crtc, state->visible);
+
+		intel_frontbuffer_flip(crtc->dev,
+				       INTEL_FRONTBUFFER_CURSOR(intel_crtc->pipe));
+
 		return 0;
 	}
 }
+
+static int
+intel_cursor_plane_update(struct drm_plane *plane, struct drm_crtc *crtc,
+			  struct drm_framebuffer *fb, int crtc_x, int crtc_y,
+			  unsigned int crtc_w, unsigned int crtc_h,
+			  uint32_t src_x, uint32_t src_y,
+			  uint32_t src_w, uint32_t src_h)
+{
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	struct intel_plane_state state;
+	int ret;
+
+	state.crtc = crtc;
+	state.fb = fb;
+
+	/* sample coordinates in 16.16 fixed point */
+	state.src.x1 = src_x;
+	state.src.x2 = src_x + src_w;
+	state.src.y1 = src_y;
+	state.src.y2 = src_y + src_h;
+
+	/* integer pixels */
+	state.dst.x1 = crtc_x;
+	state.dst.x2 = crtc_x + crtc_w;
+	state.dst.y1 = crtc_y;
+	state.dst.y2 = crtc_y + crtc_h;
+
+	state.clip.x1 = 0;
+	state.clip.y1 = 0;
+	state.clip.x2 = intel_crtc->active ? intel_crtc->config.pipe_src_w : 0;
+	state.clip.y2 = intel_crtc->active ? intel_crtc->config.pipe_src_h : 0;
+
+	state.orig_src = state.src;
+	state.orig_dst = state.dst;
+
+	ret = intel_check_cursor_plane(plane, &state);
+	if (ret)
+		return ret;
+
+	return intel_commit_cursor_plane(plane, &state);
+}
+
 static const struct drm_plane_funcs intel_cursor_plane_funcs = {
 	.update_plane = intel_cursor_plane_update,
 	.disable_plane = intel_cursor_plane_disable,
 	.destroy = intel_plane_destroy,
+	.set_property = intel_plane_set_property,
 };
 
 static struct drm_plane *intel_cursor_plane_create(struct drm_device *dev,
@@ -11758,12 +12141,26 @@
 	cursor->max_downscale = 1;
 	cursor->pipe = pipe;
 	cursor->plane = pipe;
+	cursor->rotation = BIT(DRM_ROTATE_0);
 
 	drm_universal_plane_init(dev, &cursor->base, 0,
 				 &intel_cursor_plane_funcs,
 				 intel_cursor_formats,
 				 ARRAY_SIZE(intel_cursor_formats),
 				 DRM_PLANE_TYPE_CURSOR);
+
+	if (INTEL_INFO(dev)->gen >= 4) {
+		if (!dev->mode_config.rotation_property)
+			dev->mode_config.rotation_property =
+				drm_mode_create_rotation_property(dev,
+							BIT(DRM_ROTATE_0) |
+							BIT(DRM_ROTATE_180));
+		if (dev->mode_config.rotation_property)
+			drm_object_attach_property(&cursor->base.base,
+				dev->mode_config.rotation_property,
+				cursor->rotation);
+	}
+
 	return &cursor->base;
 }
 
@@ -11812,8 +12209,7 @@
 
 	intel_crtc->cursor_base = ~0;
 	intel_crtc->cursor_cntl = ~0;
-
-	init_waitqueue_head(&intel_crtc->vbl_wait);
+	intel_crtc->cursor_size = ~0;
 
 	BUG_ON(pipe >= ARRAY_SIZE(dev_priv->plane_to_crtc_mapping) ||
 	       dev_priv->plane_to_crtc_mapping[intel_crtc->plane] != NULL);
@@ -11840,7 +12236,7 @@
 
 	WARN_ON(!drm_modeset_is_locked(&dev->mode_config.connection_mutex));
 
-	if (!encoder)
+	if (!encoder || WARN_ON(!encoder->crtc))
 		return INVALID_PIPE;
 
 	return to_intel_crtc(encoder->crtc)->pipe;
@@ -11876,8 +12272,7 @@
 	int index_mask = 0;
 	int entry = 0;
 
-	list_for_each_entry(source_encoder,
-			    &dev->mode_config.encoder_list, base.head) {
+	for_each_intel_encoder(dev, source_encoder) {
 		if (encoders_cloneable(encoder, source_encoder))
 			index_mask |= (1 << entry);
 
@@ -11929,7 +12324,10 @@
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	if (IS_ULT(dev))
+	if (INTEL_INFO(dev)->gen >= 9)
+		return false;
+
+	if (IS_HSW_ULT(dev) || IS_BDW_ULT(dev))
 		return false;
 
 	if (IS_CHERRYVIEW(dev))
@@ -11999,27 +12397,36 @@
 		if (I915_READ(PCH_DP_D) & DP_DETECTED)
 			intel_dp_init(dev, PCH_DP_D, PORT_D);
 	} else if (IS_VALLEYVIEW(dev)) {
-		if (I915_READ(VLV_DISPLAY_BASE + GEN4_HDMIB) & SDVO_DETECTED) {
+		/*
+		 * The DP_DETECTED bit is the latched state of the DDC
+		 * SDA pin at boot. However since eDP doesn't require DDC
+		 * (no way to plug in a DP->HDMI dongle) the DDC pins for
+		 * eDP ports may have been muxed to an alternate function.
+		 * Thus we can't rely on the DP_DETECTED bit alone to detect
+		 * eDP ports. Consult the VBT as well as DP_DETECTED to
+		 * detect eDP ports.
+		 */
+		if (I915_READ(VLV_DISPLAY_BASE + GEN4_HDMIB) & SDVO_DETECTED)
 			intel_hdmi_init(dev, VLV_DISPLAY_BASE + GEN4_HDMIB,
 					PORT_B);
-			if (I915_READ(VLV_DISPLAY_BASE + DP_B) & DP_DETECTED)
-				intel_dp_init(dev, VLV_DISPLAY_BASE + DP_B, PORT_B);
-		}
+		if (I915_READ(VLV_DISPLAY_BASE + DP_B) & DP_DETECTED ||
+		    intel_dp_is_edp(dev, PORT_B))
+			intel_dp_init(dev, VLV_DISPLAY_BASE + DP_B, PORT_B);
 
-		if (I915_READ(VLV_DISPLAY_BASE + GEN4_HDMIC) & SDVO_DETECTED) {
+		if (I915_READ(VLV_DISPLAY_BASE + GEN4_HDMIC) & SDVO_DETECTED)
 			intel_hdmi_init(dev, VLV_DISPLAY_BASE + GEN4_HDMIC,
 					PORT_C);
-			if (I915_READ(VLV_DISPLAY_BASE + DP_C) & DP_DETECTED)
-				intel_dp_init(dev, VLV_DISPLAY_BASE + DP_C, PORT_C);
-		}
+		if (I915_READ(VLV_DISPLAY_BASE + DP_C) & DP_DETECTED ||
+		    intel_dp_is_edp(dev, PORT_C))
+			intel_dp_init(dev, VLV_DISPLAY_BASE + DP_C, PORT_C);
 
 		if (IS_CHERRYVIEW(dev)) {
-			if (I915_READ(VLV_DISPLAY_BASE + CHV_HDMID) & SDVO_DETECTED) {
+			if (I915_READ(VLV_DISPLAY_BASE + CHV_HDMID) & SDVO_DETECTED)
 				intel_hdmi_init(dev, VLV_DISPLAY_BASE + CHV_HDMID,
 						PORT_D);
-				if (I915_READ(VLV_DISPLAY_BASE + DP_D) & DP_DETECTED)
-					intel_dp_init(dev, VLV_DISPLAY_BASE + DP_D, PORT_D);
-			}
+			/* eDP not supported on port D, so don't check VBT */
+			if (I915_READ(VLV_DISPLAY_BASE + DP_D) & DP_DETECTED)
+				intel_dp_init(dev, VLV_DISPLAY_BASE + DP_D, PORT_D);
 		}
 
 		intel_dsi_init(dev);
@@ -12064,9 +12471,9 @@
 	if (SUPPORTS_TV(dev))
 		intel_tv_init(dev);
 
-	intel_edp_psr_init(dev);
+	intel_psr_init(dev);
 
-	list_for_each_entry(encoder, &dev->mode_config.encoder_list, base.head) {
+	for_each_intel_encoder(dev, encoder) {
 		encoder->base.possible_crtcs = encoder->crtc_mask;
 		encoder->base.possible_clones =
 			intel_encoder_clones(encoder);
@@ -12268,16 +12675,22 @@
 	if (HAS_DDI(dev)) {
 		dev_priv->display.get_pipe_config = haswell_get_pipe_config;
 		dev_priv->display.get_plane_config = ironlake_get_plane_config;
-		dev_priv->display.crtc_mode_set = haswell_crtc_mode_set;
+		dev_priv->display.crtc_compute_clock =
+			haswell_crtc_compute_clock;
 		dev_priv->display.crtc_enable = haswell_crtc_enable;
 		dev_priv->display.crtc_disable = haswell_crtc_disable;
 		dev_priv->display.off = ironlake_crtc_off;
-		dev_priv->display.update_primary_plane =
-			ironlake_update_primary_plane;
+		if (INTEL_INFO(dev)->gen >= 9)
+			dev_priv->display.update_primary_plane =
+				skylake_update_primary_plane;
+		else
+			dev_priv->display.update_primary_plane =
+				ironlake_update_primary_plane;
 	} else if (HAS_PCH_SPLIT(dev)) {
 		dev_priv->display.get_pipe_config = ironlake_get_pipe_config;
 		dev_priv->display.get_plane_config = ironlake_get_plane_config;
-		dev_priv->display.crtc_mode_set = ironlake_crtc_mode_set;
+		dev_priv->display.crtc_compute_clock =
+			ironlake_crtc_compute_clock;
 		dev_priv->display.crtc_enable = ironlake_crtc_enable;
 		dev_priv->display.crtc_disable = ironlake_crtc_disable;
 		dev_priv->display.off = ironlake_crtc_off;
@@ -12286,7 +12699,7 @@
 	} else if (IS_VALLEYVIEW(dev)) {
 		dev_priv->display.get_pipe_config = i9xx_get_pipe_config;
 		dev_priv->display.get_plane_config = i9xx_get_plane_config;
-		dev_priv->display.crtc_mode_set = i9xx_crtc_mode_set;
+		dev_priv->display.crtc_compute_clock = i9xx_crtc_compute_clock;
 		dev_priv->display.crtc_enable = valleyview_crtc_enable;
 		dev_priv->display.crtc_disable = i9xx_crtc_disable;
 		dev_priv->display.off = i9xx_crtc_off;
@@ -12295,7 +12708,7 @@
 	} else {
 		dev_priv->display.get_pipe_config = i9xx_get_pipe_config;
 		dev_priv->display.get_plane_config = i9xx_get_plane_config;
-		dev_priv->display.crtc_mode_set = i9xx_crtc_mode_set;
+		dev_priv->display.crtc_compute_clock = i9xx_crtc_compute_clock;
 		dev_priv->display.crtc_enable = i9xx_crtc_enable;
 		dev_priv->display.crtc_disable = i9xx_crtc_disable;
 		dev_priv->display.off = i9xx_crtc_off;
@@ -12332,33 +12745,20 @@
 		dev_priv->display.get_display_clock_speed =
 			i830_get_display_clock_speed;
 
-	if (HAS_PCH_SPLIT(dev)) {
-		if (IS_GEN5(dev)) {
-			dev_priv->display.fdi_link_train = ironlake_fdi_link_train;
-			dev_priv->display.write_eld = ironlake_write_eld;
-		} else if (IS_GEN6(dev)) {
-			dev_priv->display.fdi_link_train = gen6_fdi_link_train;
-			dev_priv->display.write_eld = ironlake_write_eld;
-			dev_priv->display.modeset_global_resources =
-				snb_modeset_global_resources;
-		} else if (IS_IVYBRIDGE(dev)) {
-			/* FIXME: detect B0+ stepping and use auto training */
-			dev_priv->display.fdi_link_train = ivb_manual_fdi_link_train;
-			dev_priv->display.write_eld = ironlake_write_eld;
-			dev_priv->display.modeset_global_resources =
-				ivb_modeset_global_resources;
-		} else if (IS_HASWELL(dev) || IS_GEN8(dev)) {
-			dev_priv->display.fdi_link_train = hsw_fdi_link_train;
-			dev_priv->display.write_eld = haswell_write_eld;
-			dev_priv->display.modeset_global_resources =
-				haswell_modeset_global_resources;
-		}
-	} else if (IS_G4X(dev)) {
-		dev_priv->display.write_eld = g4x_write_eld;
+	if (IS_GEN5(dev)) {
+		dev_priv->display.fdi_link_train = ironlake_fdi_link_train;
+	} else if (IS_GEN6(dev)) {
+		dev_priv->display.fdi_link_train = gen6_fdi_link_train;
+	} else if (IS_IVYBRIDGE(dev)) {
+		/* FIXME: detect B0+ stepping and use auto training */
+		dev_priv->display.fdi_link_train = ivb_manual_fdi_link_train;
+		dev_priv->display.modeset_global_resources =
+			ivb_modeset_global_resources;
+	} else if (IS_HASWELL(dev) || IS_BROADWELL(dev)) {
+		dev_priv->display.fdi_link_train = hsw_fdi_link_train;
 	} else if (IS_VALLEYVIEW(dev)) {
 		dev_priv->display.modeset_global_resources =
 			valleyview_modeset_global_resources;
-		dev_priv->display.write_eld = ironlake_write_eld;
 	}
 
 	/* Default just returns -ENODEV to indicate unsupported */
@@ -12385,9 +12785,14 @@
 	case 8: /* FIXME(BDW): Check that the gen8 RCS flip works. */
 		dev_priv->display.queue_flip = intel_gen7_queue_flip;
 		break;
+	case 9:
+		dev_priv->display.queue_flip = intel_gen9_queue_flip;
+		break;
 	}
 
 	intel_panel_init_backlight_funcs(dev);
+
+	mutex_init(&dev_priv->pps_mutex);
 }
 
 /*
@@ -12403,6 +12808,14 @@
 	DRM_INFO("applying pipe a force quirk\n");
 }
 
+static void quirk_pipeb_force(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	dev_priv->quirks |= QUIRK_PIPEB_FORCE;
+	DRM_INFO("applying pipe b force quirk\n");
+}
+
 /*
  * Some machines (Lenovo U160) do not work with SSC on LVDS for some reason
  */
@@ -12477,6 +12890,12 @@
 	/* ThinkPad T60 needs pipe A force quirk (bug #16494) */
 	{ 0x2782, 0x17aa, 0x201a, quirk_pipea_force },
 
+	/* 830 needs to leave pipe A & dpll A up */
+	{ 0x3577, PCI_ANY_ID, PCI_ANY_ID, quirk_pipea_force },
+
+	/* 830 needs to leave pipe B & dpll B up */
+	{ 0x3577, PCI_ANY_ID, PCI_ANY_ID, quirk_pipeb_force },
+
 	/* Lenovo U160 cannot use SSC on LVDS */
 	{ 0x0046, 0x17aa, 0x3920, quirk_ssc_force_disable },
 
@@ -12553,7 +12972,11 @@
 	vga_put(dev->pdev, VGA_RSRC_LEGACY_IO);
 	udelay(300);
 
-	I915_WRITE(vga_reg, VGA_DISP_DISABLE);
+	/*
+	 * Fujitsu-Siemens Lifebook S6010 (830) has problems resuming
+	 * from S3 without preserving (some of?) the other bits.
+	 */
+	I915_WRITE(vga_reg, dev_priv->bios_vgacntr | VGA_DISP_DISABLE);
 	POSTING_READ(vga_reg);
 }
 
@@ -12566,16 +12989,9 @@
 
 	intel_init_clock_gating(dev);
 
-	intel_reset_dpio(dev);
-
 	intel_enable_gt_powersave(dev);
 }
 
-void intel_modeset_suspend_hw(struct drm_device *dev)
-{
-	intel_suspend_hw(dev);
-}
-
 void intel_modeset_init(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -12601,6 +13017,7 @@
 		return;
 
 	intel_init_display(dev);
+	intel_init_audio(dev);
 
 	if (IS_GEN2(dev)) {
 		dev->mode_config.max_width = 2048;
@@ -12613,7 +13030,10 @@
 		dev->mode_config.max_height = 8192;
 	}
 
-	if (IS_GEN2(dev)) {
+	if (IS_845G(dev) || IS_I865G(dev)) {
+		dev->mode_config.cursor_width = IS_845G(dev) ? 64 : 512;
+		dev->mode_config.cursor_height = 1023;
+	} else if (IS_GEN2(dev)) {
 		dev->mode_config.cursor_width = GEN2_CURSOR_WIDTH;
 		dev->mode_config.cursor_height = GEN2_CURSOR_HEIGHT;
 	} else {
@@ -12621,13 +13041,16 @@
 		dev->mode_config.cursor_height = MAX_CURSOR_HEIGHT;
 	}
 
+	if (INTEL_INFO(dev)->gen >= 6)
+		dev->mode_config.async_page_flip = true;
+
 	dev->mode_config.fb_base = dev_priv->gtt.mappable_base;
 
 	DRM_DEBUG_KMS("%d display pipe%s available.\n",
 		      INTEL_INFO(dev)->num_pipes,
 		      INTEL_INFO(dev)->num_pipes > 1 ? "s" : "");
 
-	for_each_pipe(pipe) {
+	for_each_pipe(dev_priv, pipe) {
 		intel_crtc_init(dev, pipe);
 		for_each_sprite(pipe, sprite) {
 			ret = intel_plane_init(dev, pipe, sprite);
@@ -12638,10 +13061,11 @@
 	}
 
 	intel_init_dpio(dev);
-	intel_reset_dpio(dev);
 
 	intel_shared_dpll_init(dev);
 
+	/* save the BIOS value before clobbering it */
+	dev_priv->bios_vgacntr = I915_READ(i915_vgacntrl_reg(dev));
 	/* Just disable it once at startup */
 	i915_disable_vga(dev);
 	intel_setup_outputs(dev);
@@ -12733,9 +13157,10 @@
 	I915_WRITE(reg, I915_READ(reg) & ~PIPECONF_FRAME_START_DELAY_MASK);
 
 	/* restore vblank interrupts to correct state */
-	if (crtc->active)
+	if (crtc->active) {
+		update_scanline_offset(crtc);
 		drm_vblank_on(dev, crtc->pipe);
-	else
+	} else
 		drm_vblank_off(dev, crtc->pipe);
 
 	/* We need to sanitize the plane -> pipe mapping first because this will
@@ -12791,6 +13216,7 @@
 	/* Adjust the state of the output pipe according to whether we
 	 * have active connectors/encoders. */
 	intel_crtc_update_dpms(&crtc->base);
+	intel_crtc_update_cursor(&crtc->base, crtc->active && crtc->cursor_bo);
 
 	if (crtc->active != crtc->base.enabled) {
 		struct intel_encoder *encoder;
@@ -12818,7 +13244,7 @@
 		}
 	}
 
-	if (crtc->active || IS_VALLEYVIEW(dev) || INTEL_INFO(dev)->gen < 5) {
+	if (crtc->active || HAS_GMCH_DISPLAY(dev)) {
 		/*
 		 * We start out with underrun reporting disabled to avoid races.
 		 * For correct bookkeeping mark this on active crtcs.
@@ -12834,8 +13260,6 @@
 		 */
 		crtc->cpu_fifo_underrun_disabled = true;
 		crtc->pch_fifo_underrun_disabled = true;
-
-		update_scanline_offset(crtc);
 	}
 }
 
@@ -12908,7 +13332,7 @@
 	 * level, just check if the power well is enabled instead of trying to
 	 * follow the "don't touch the power well if we don't need it" policy
 	 * the rest of the driver uses. */
-	if (!intel_display_power_enabled(dev_priv, POWER_DOMAIN_VGA))
+	if (!intel_display_power_is_enabled(dev_priv, POWER_DOMAIN_VGA))
 		return;
 
 	i915_redisable_vga_power_on(dev);
@@ -12952,23 +13376,25 @@
 	for (i = 0; i < dev_priv->num_shared_dpll; i++) {
 		struct intel_shared_dpll *pll = &dev_priv->shared_dplls[i];
 
-		pll->on = pll->get_hw_state(dev_priv, pll, &pll->hw_state);
+		pll->on = pll->get_hw_state(dev_priv, pll,
+					    &pll->config.hw_state);
 		pll->active = 0;
+		pll->config.crtc_mask = 0;
 		for_each_intel_crtc(dev, crtc) {
-			if (crtc->active && intel_crtc_to_shared_dpll(crtc) == pll)
+			if (crtc->active && intel_crtc_to_shared_dpll(crtc) == pll) {
 				pll->active++;
+				pll->config.crtc_mask |= 1 << crtc->pipe;
+			}
 		}
-		pll->refcount = pll->active;
 
-		DRM_DEBUG_KMS("%s hw state readout: refcount %i, on %i\n",
-			      pll->name, pll->refcount, pll->on);
+		DRM_DEBUG_KMS("%s hw state readout: crtc_mask 0x%08x, on %i\n",
+			      pll->name, pll->config.crtc_mask, pll->on);
 
-		if (pll->refcount)
+		if (pll->config.crtc_mask)
 			intel_display_power_get(dev_priv, POWER_DOMAIN_PLLS);
 	}
 
-	list_for_each_entry(encoder, &dev->mode_config.encoder_list,
-			    base.head) {
+	for_each_intel_encoder(dev, encoder) {
 		pipe = 0;
 
 		if (encoder->get_hw_state(encoder, &pipe)) {
@@ -13023,7 +13449,7 @@
 	 * checking everywhere.
 	 */
 	for_each_intel_crtc(dev, crtc) {
-		if (crtc->active && i915.fastboot) {
+		if (crtc->active && i915_module.fastboot) {
 			intel_mode_from_pipe_config(&crtc->base.mode, &crtc->config);
 			DRM_DEBUG_KMS("[CRTC:%d] found active mode: ",
 				      crtc->base.base.id);
@@ -13032,12 +13458,11 @@
 	}
 
 	/* HW state is read out, now we need to sanitize this mess. */
-	list_for_each_entry(encoder, &dev->mode_config.encoder_list,
-			    base.head) {
+	for_each_intel_encoder(dev, encoder) {
 		intel_sanitize_encoder(encoder);
 	}
 
-	for_each_pipe(pipe) {
+	for_each_pipe(dev_priv, pipe) {
 		crtc = to_intel_crtc(dev_priv->pipe_to_crtc_mapping[pipe]);
 		intel_sanitize_crtc(crtc);
 		intel_dump_pipe_config(crtc, &crtc->config, "[setup_hw_state]");
@@ -13055,7 +13480,9 @@
 		pll->on = false;
 	}
 
-	if (HAS_PCH_SPLIT(dev))
+	if (IS_GEN9(dev))
+		skl_wm_get_hw_state(dev);
+	else if (HAS_PCH_SPLIT(dev))
 		ilk_wm_get_hw_state(dev);
 
 	if (force_restore) {
@@ -13065,12 +13492,12 @@
 		 * We need to use raw interfaces for restoring state to avoid
 		 * checking (bogus) intermediate states.
 		 */
-		for_each_pipe(pipe) {
+		for_each_pipe(dev_priv, pipe) {
 			struct drm_crtc *crtc =
 				dev_priv->pipe_to_crtc_mapping[pipe];
 
-			__intel_set_mode(crtc, &crtc->mode, crtc->x, crtc->y,
-					 crtc->primary->fb);
+			intel_set_mode(crtc, &crtc->mode, crtc->x, crtc->y,
+				       crtc->primary->fb);
 		}
 	} else {
 		intel_modeset_update_staged_output_state(dev);
@@ -13081,6 +13508,7 @@
 
 void intel_modeset_gem_init(struct drm_device *dev)
 {
+	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_crtc *c;
 	struct drm_i915_gem_object *obj;
 
@@ -13088,6 +13516,16 @@
 	intel_init_gt_powersave(dev);
 	mutex_unlock(&dev->struct_mutex);
 
+	/*
+	 * There may be no VBT; and if the BIOS enabled SSC we can
+	 * just keep using it to avoid unnecessary flicker.  Whereas if the
+	 * BIOS isn't using it, don't assume it will work even if the VBT
+	 * indicates as much.
+	 */
+	if (HAS_PCH_IBX(dev) || HAS_PCH_CPT(dev))
+		dev_priv->vbt.lvds_use_ssc = !!(I915_READ(PCH_DREF_CONTROL) &
+						DREF_SSC1_ENABLE);
+
 	intel_modeset_init_hw(dev);
 
 	intel_setup_overlay(dev);
@@ -13103,7 +13541,9 @@
 		if (obj == NULL)
 			continue;
 
-		if (intel_pin_and_fence_fb_obj(dev, obj, NULL)) {
+		if (intel_pin_and_fence_fb_obj(c->primary,
+					       c->primary->fb,
+					       NULL)) {
 			DRM_ERROR("failed to pin boot fb on pipe %d\n",
 				  to_intel_crtc(c)->pipe);
 			drm_framebuffer_unreference(c->primary->fb);
@@ -13111,6 +13551,8 @@
 		}
 	}
 	mutex_unlock(&dev->struct_mutex);
+
+	intel_backlight_register(dev);
 }
 
 void intel_connector_unregister(struct intel_connector *intel_connector)
@@ -13126,14 +13568,16 @@
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_connector *connector;
 
+	intel_disable_gt_powersave(dev);
+
+	intel_backlight_unregister(dev);
+
 	/*
 	 * Interrupts and polling as the first thing to avoid creating havoc.
-	 * Too much stuff here (turning of rps, connectors, ...) would
+	 * Too much stuff here (turning of connectors, ...) would
 	 * experience fancy races otherwise.
 	 */
-	drm_irq_uninstall(dev);
-	intel_hpd_cancel_work(dev_priv);
-	dev_priv->pm._irqs_disabled = true;
+	intel_irq_uninstall(dev_priv);
 
 	/*
 	 * Due to the hpd irq storm handling the hotplug work can re-arm the
@@ -13147,8 +13591,6 @@
 
 	intel_disable_fbc(dev);
 
-	intel_disable_gt_powersave(dev);
-
 	ironlake_teardown_rc6(dev);
 
 	mutex_unlock(&dev->struct_mutex);
@@ -13286,10 +13728,10 @@
 	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
 		error->power_well_driver = I915_READ(HSW_PWR_WELL_DRIVER);
 
-	for_each_pipe(i) {
+	for_each_pipe(dev_priv, i) {
 		error->pipe[i].power_domain_on =
-			intel_display_power_enabled_unlocked(dev_priv,
-							   POWER_DOMAIN_PIPE(i));
+			__intel_display_power_is_enabled(dev_priv,
+							 POWER_DOMAIN_PIPE(i));
 		if (!error->pipe[i].power_domain_on)
 			continue;
 
@@ -13324,7 +13766,7 @@
 		enum transcoder cpu_transcoder = transcoders[i];
 
 		error->transcoder[i].power_domain_on =
-			intel_display_power_enabled_unlocked(dev_priv,
+			__intel_display_power_is_enabled(dev_priv,
 				POWER_DOMAIN_TRANSCODER(cpu_transcoder));
 		if (!error->transcoder[i].power_domain_on)
 			continue;
@@ -13350,6 +13792,7 @@
 				struct drm_device *dev,
 				struct intel_display_error_state *error)
 {
+	struct drm_i915_private *dev_priv = dev->dev_private;
 	int i;
 
 	if (!error)
@@ -13359,7 +13802,7 @@
 	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
 		err_printf(m, "PWR_WELL_CTL2: %08x\n",
 			   error->power_well_driver);
-	for_each_pipe(i) {
+	for_each_pipe(dev_priv, i) {
 		err_printf(m, "Pipe [%d]:\n", i);
 		err_printf(m, "  Power: %s\n",
 			   error->pipe[i].power_domain_on ? "on" : "off");
@@ -13400,3 +13843,24 @@
 		err_printf(m, "  VSYNC: %08x\n", error->transcoder[i].vsync);
 	}
 }
+
+void intel_modeset_preclose(struct drm_device *dev, struct drm_file *file)
+{
+	struct intel_crtc *crtc;
+
+	for_each_intel_crtc(dev, crtc) {
+		struct intel_unpin_work *work;
+
+		spin_lock_irq(&dev->event_lock);
+
+		work = crtc->unpin_work;
+
+		if (work && work->event &&
+		    work->event->base.file_priv == file) {
+			kfree(work->event);
+			work->event = NULL;
+		}
+
+		spin_unlock_irq(&dev->event_lock);
+	}
+}
diff -urN a/drivers/gpu/drm/i915/intel_dp.c b/drivers/gpu/drm/i915/intel_dp.c
--- a/drivers/gpu/drm/i915/intel_dp.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_dp.c	2014-11-22 14:37:49.338700418 -0700
@@ -111,8 +111,11 @@
 }
 
 static void intel_dp_link_down(struct intel_dp *intel_dp);
-static bool _edp_panel_vdd_on(struct intel_dp *intel_dp);
+static bool edp_panel_vdd_on(struct intel_dp *intel_dp);
 static void edp_panel_vdd_off(struct intel_dp *intel_dp, bool sync);
+static void vlv_init_panel_power_sequencer(struct intel_dp *intel_dp);
+static void vlv_steal_power_sequencer(struct drm_device *dev,
+				      enum pipe pipe);
 
 int
 intel_dp_max_link_bw(struct intel_dp *intel_dp)
@@ -212,6 +215,10 @@
 	max_rate = intel_dp_max_data_rate(max_link_clock, max_lanes);
 	mode_rate = intel_dp_link_required(target_clock, 18);
 
+	DRM_DEBUG_KMS("mode %s: maximum link clock %d with %d lanes,"
+		      " giving a maximum dot clock of %d but mode requires %d\n",
+		     mode->name,  max_link_clock, max_lanes, max_rate, mode_rate);
+
 	if (mode_rate > max_rate)
 		return MODE_CLOCK_HIGH;
 
@@ -224,8 +231,7 @@
 	return MODE_OK;
 }
 
-static uint32_t
-pack_aux(uint8_t *src, int src_bytes)
+uint32_t intel_dp_pack_aux(const uint8_t *src, int src_bytes)
 {
 	int	i;
 	uint32_t v = 0;
@@ -237,8 +243,7 @@
 	return v;
 }
 
-static void
-unpack_aux(uint32_t src, uint8_t *dst, int dst_bytes)
+void intel_dp_unpack_aux(uint32_t src, uint8_t *dst, int dst_bytes)
 {
 	int i;
 	if (dst_bytes > 4)
@@ -283,39 +288,275 @@
 
 static void
 intel_dp_init_panel_power_sequencer(struct drm_device *dev,
-				    struct intel_dp *intel_dp,
-				    struct edp_power_seq *out);
+				    struct intel_dp *intel_dp);
 static void
 intel_dp_init_panel_power_sequencer_registers(struct drm_device *dev,
-					      struct intel_dp *intel_dp,
-					      struct edp_power_seq *out);
+					      struct intel_dp *intel_dp);
+
+static void pps_lock(struct intel_dp *intel_dp)
+{
+	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+	struct intel_encoder *encoder = &intel_dig_port->base;
+	struct drm_device *dev = encoder->base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	enum intel_display_power_domain power_domain;
+
+	/*
+	 * See vlv_power_sequencer_reset() why we need
+	 * a power domain reference here.
+	 */
+	power_domain = intel_display_port_power_domain(encoder);
+	intel_display_power_get(dev_priv, power_domain);
+
+	mutex_lock(&dev_priv->pps_mutex);
+}
+
+static void pps_unlock(struct intel_dp *intel_dp)
+{
+	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+	struct intel_encoder *encoder = &intel_dig_port->base;
+	struct drm_device *dev = encoder->base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	enum intel_display_power_domain power_domain;
+
+	mutex_unlock(&dev_priv->pps_mutex);
+
+	power_domain = intel_display_port_power_domain(encoder);
+	intel_display_power_put(dev_priv, power_domain);
+}
+
+static void
+vlv_power_sequencer_kick(struct intel_dp *intel_dp)
+{
+	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+	struct drm_device *dev = intel_dig_port->base.base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	enum pipe pipe = intel_dp->pps_pipe;
+	bool pll_enabled;
+	uint32_t DP;
+
+	if (WARN(I915_READ(intel_dp->output_reg) & DP_PORT_EN,
+		 "skipping pipe %c power seqeuncer kick due to port %c being active\n",
+		 pipe_name(pipe), port_name(intel_dig_port->port)))
+		return;
+
+	DRM_DEBUG_KMS("kicking pipe %c power sequencer for port %c\n",
+		      pipe_name(pipe), port_name(intel_dig_port->port));
+
+	/* Preserve the BIOS-computed detected bit. This is
+	 * supposed to be read-only.
+	 */
+	DP = I915_READ(intel_dp->output_reg) & DP_DETECTED;
+	DP |= DP_VOLTAGE_0_4 | DP_PRE_EMPHASIS_0;
+	DP |= DP_PORT_WIDTH(1);
+	DP |= DP_LINK_TRAIN_PAT_1;
+
+	if (IS_CHERRYVIEW(dev))
+		DP |= DP_PIPE_SELECT_CHV(pipe);
+	else if (pipe == PIPE_B)
+		DP |= DP_PIPEB_SELECT;
+
+	pll_enabled = I915_READ(DPLL(pipe)) & DPLL_VCO_ENABLE;
+
+	/*
+	 * The DPLL for the pipe must be enabled for this to work.
+	 * So enable temporarily it if it's not already enabled.
+	 */
+	if (!pll_enabled)
+		vlv_force_pll_on(dev, pipe, IS_CHERRYVIEW(dev) ?
+				 &chv_dpll[0].dpll : &vlv_dpll[0].dpll);
+
+	/*
+	 * Similar magic as in intel_dp_enable_port().
+	 * We _must_ do this port enable + disable trick
+	 * to make this power seqeuencer lock onto the port.
+	 * Otherwise even VDD force bit won't work.
+	 */
+	I915_WRITE(intel_dp->output_reg, DP);
+	POSTING_READ(intel_dp->output_reg);
+
+	I915_WRITE(intel_dp->output_reg, DP | DP_PORT_EN);
+	POSTING_READ(intel_dp->output_reg);
+
+	I915_WRITE(intel_dp->output_reg, DP & ~DP_PORT_EN);
+	POSTING_READ(intel_dp->output_reg);
+
+	if (!pll_enabled)
+		vlv_force_pll_off(dev, pipe);
+}
 
 static enum pipe
 vlv_power_sequencer_pipe(struct intel_dp *intel_dp)
 {
 	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
-	struct drm_crtc *crtc = intel_dig_port->base.base.crtc;
 	struct drm_device *dev = intel_dig_port->base.base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	enum port port = intel_dig_port->port;
+	struct intel_encoder *encoder;
+	unsigned int pipes = (1 << PIPE_A) | (1 << PIPE_B);
 	enum pipe pipe;
 
-	/* modeset should have pipe */
-	if (crtc)
-		return to_intel_crtc(crtc)->pipe;
+	lockdep_assert_held(&dev_priv->pps_mutex);
+
+	/* We should never land here with regular DP ports */
+	WARN_ON(!is_edp(intel_dp));
+
+	if (intel_dp->pps_pipe != INVALID_PIPE)
+		return intel_dp->pps_pipe;
+
+	/*
+	 * We don't have power sequencer currently.
+	 * Pick one that's not used by other ports.
+	 */
+	list_for_each_entry(encoder, &dev->mode_config.encoder_list,
+			    base.head) {
+		struct intel_dp *tmp;
+
+		if (encoder->type != INTEL_OUTPUT_EDP)
+			continue;
+
+		tmp = enc_to_intel_dp(&encoder->base);
+
+		if (tmp->pps_pipe != INVALID_PIPE)
+			pipes &= ~(1 << tmp->pps_pipe);
+	}
+
+	/*
+	 * Didn't find one. This should not happen since there
+	 * are two power sequencers and up to two eDP ports.
+	 */
+	if (WARN_ON(pipes == 0))
+		pipe = PIPE_A;
+	else
+		pipe = ffs(pipes) - 1;
+
+	vlv_steal_power_sequencer(dev, pipe);
+	intel_dp->pps_pipe = pipe;
+
+	DRM_DEBUG_KMS("picked pipe %c power sequencer for port %c\n",
+		      pipe_name(intel_dp->pps_pipe),
+		      port_name(intel_dig_port->port));
+
+	/* init power sequencer on this pipe and port */
+	intel_dp_init_panel_power_sequencer(dev, intel_dp);
+	intel_dp_init_panel_power_sequencer_registers(dev, intel_dp);
+
+	/*
+	 * Even vdd force doesn't work until we've made
+	 * the power sequencer lock in on the port.
+	 */
+	vlv_power_sequencer_kick(intel_dp);
+
+	return intel_dp->pps_pipe;
+}
+
+typedef bool (*vlv_pipe_check)(struct drm_i915_private *dev_priv,
+			       enum pipe pipe);
+
+static bool vlv_pipe_has_pp_on(struct drm_i915_private *dev_priv,
+			       enum pipe pipe)
+{
+	return I915_READ(VLV_PIPE_PP_STATUS(pipe)) & PP_ON;
+}
+
+static bool vlv_pipe_has_vdd_on(struct drm_i915_private *dev_priv,
+				enum pipe pipe)
+{
+	return I915_READ(VLV_PIPE_PP_CONTROL(pipe)) & EDP_FORCE_VDD;
+}
+
+static bool vlv_pipe_any(struct drm_i915_private *dev_priv,
+			 enum pipe pipe)
+{
+	return true;
+}
+
+static enum pipe
+vlv_initial_pps_pipe(struct drm_i915_private *dev_priv,
+		     enum port port,
+		     vlv_pipe_check pipe_check)
+{
+	enum pipe pipe;
 
-	/* init time, try to find a pipe with this port selected */
 	for (pipe = PIPE_A; pipe <= PIPE_B; pipe++) {
 		u32 port_sel = I915_READ(VLV_PIPE_PP_ON_DELAYS(pipe)) &
 			PANEL_PORT_SELECT_MASK;
-		if (port_sel == PANEL_PORT_SELECT_DPB_VLV && port == PORT_B)
-			return pipe;
-		if (port_sel == PANEL_PORT_SELECT_DPC_VLV && port == PORT_C)
-			return pipe;
+
+		if (port_sel != PANEL_PORT_SELECT_VLV(port))
+			continue;
+
+		if (!pipe_check(dev_priv, pipe))
+			continue;
+
+		return pipe;
+	}
+
+	return INVALID_PIPE;
+}
+
+static void
+vlv_initial_power_sequencer_setup(struct intel_dp *intel_dp)
+{
+	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+	struct drm_device *dev = intel_dig_port->base.base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	enum port port = intel_dig_port->port;
+
+	lockdep_assert_held(&dev_priv->pps_mutex);
+
+	/* try to find a pipe with this port selected */
+	/* first pick one where the panel is on */
+	intel_dp->pps_pipe = vlv_initial_pps_pipe(dev_priv, port,
+						  vlv_pipe_has_pp_on);
+	/* didn't find one? pick one where vdd is on */
+	if (intel_dp->pps_pipe == INVALID_PIPE)
+		intel_dp->pps_pipe = vlv_initial_pps_pipe(dev_priv, port,
+							  vlv_pipe_has_vdd_on);
+	/* didn't find one? pick one with just the correct port */
+	if (intel_dp->pps_pipe == INVALID_PIPE)
+		intel_dp->pps_pipe = vlv_initial_pps_pipe(dev_priv, port,
+							  vlv_pipe_any);
+
+	/* didn't find one? just let vlv_power_sequencer_pipe() pick one when needed */
+	if (intel_dp->pps_pipe == INVALID_PIPE) {
+		DRM_DEBUG_KMS("no initial power sequencer for port %c\n",
+			      port_name(port));
+		return;
 	}
 
-	/* shrug */
-	return PIPE_A;
+	DRM_DEBUG_KMS("initial power sequencer for port %c: pipe %c\n",
+		      port_name(port), pipe_name(intel_dp->pps_pipe));
+
+	intel_dp_init_panel_power_sequencer(dev, intel_dp);
+	intel_dp_init_panel_power_sequencer_registers(dev, intel_dp);
+}
+
+void vlv_power_sequencer_reset(struct drm_i915_private *dev_priv)
+{
+	struct drm_device *dev = dev_priv->dev;
+	struct intel_encoder *encoder;
+
+	if (WARN_ON(!IS_VALLEYVIEW(dev)))
+		return;
+
+	/*
+	 * We can't grab pps_mutex here due to deadlock with power_domain
+	 * mutex when power_domain functions are called while holding pps_mutex.
+	 * That also means that in order to use pps_pipe the code needs to
+	 * hold both a power domain reference and pps_mutex, and the power domain
+	 * reference get/put must be done while _not_ holding pps_mutex.
+	 * pps_{lock,unlock}() do these steps in the correct order, so one
+	 * should use them always.
+	 */
+
+	list_for_each_entry(encoder, &dev->mode_config.encoder_list, base.head) {
+		struct intel_dp *intel_dp;
+
+		if (encoder->type != INTEL_OUTPUT_EDP)
+			continue;
+
+		intel_dp = enc_to_intel_dp(&encoder->base);
+		intel_dp->pps_pipe = INVALID_PIPE;
+	}
 }
 
 static u32 _pp_ctrl_reg(struct intel_dp *intel_dp)
@@ -349,12 +590,15 @@
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	u32 pp_div;
 	u32 pp_ctrl_reg, pp_div_reg;
-	enum pipe pipe = vlv_power_sequencer_pipe(intel_dp);
 
 	if (!is_edp(intel_dp) || code != SYS_RESTART)
 		return 0;
 
+	pps_lock(intel_dp);
+
 	if (IS_VALLEYVIEW(dev)) {
+		enum pipe pipe = vlv_power_sequencer_pipe(intel_dp);
+
 		pp_ctrl_reg = VLV_PIPE_PP_CONTROL(pipe);
 		pp_div_reg  = VLV_PIPE_PP_DIVISOR(pipe);
 		pp_div = I915_READ(pp_div_reg);
@@ -366,6 +610,8 @@
 		msleep(intel_dp->panel_power_cycle_delay);
 	}
 
+	pps_unlock(intel_dp);
+
 	return 0;
 }
 
@@ -374,6 +620,12 @@
 	struct drm_device *dev = intel_dp_to_dev(intel_dp);
 	struct drm_i915_private *dev_priv = dev->dev_private;
 
+	lockdep_assert_held(&dev_priv->pps_mutex);
+
+	if (IS_VALLEYVIEW(dev) &&
+	    intel_dp->pps_pipe == INVALID_PIPE)
+		return false;
+
 	return (I915_READ(_pp_stat_reg(intel_dp)) & PP_ON) != 0;
 }
 
@@ -381,13 +633,14 @@
 {
 	struct drm_device *dev = intel_dp_to_dev(intel_dp);
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
-	struct intel_encoder *intel_encoder = &intel_dig_port->base;
-	enum intel_display_power_domain power_domain;
 
-	power_domain = intel_display_port_power_domain(intel_encoder);
-	return intel_display_power_enabled(dev_priv, power_domain) &&
-	       (I915_READ(_pp_ctrl_reg(intel_dp)) & EDP_FORCE_VDD) != 0;
+	lockdep_assert_held(&dev_priv->pps_mutex);
+
+	if (IS_VALLEYVIEW(dev) &&
+	    intel_dp->pps_pipe == INVALID_PIPE)
+		return false;
+
+	return I915_READ(_pp_ctrl_reg(intel_dp)) & EDP_FORCE_VDD;
 }
 
 static void
@@ -488,6 +741,16 @@
 	return index ? 0 : 100;
 }
 
+static uint32_t skl_get_aux_clock_divider(struct intel_dp *intel_dp, int index)
+{
+	/*
+	 * SKL doesn't need us to program the AUX clock divider (Hardware will
+	 * derive the clock from CDCLK automatically). We still implement the
+	 * get_aux_clock_divider vfunc to plug-in into the existing code.
+	 */
+	return index ? 0 : 1;
+}
+
 static uint32_t i9xx_get_aux_send_ctl(struct intel_dp *intel_dp,
 				      bool has_aux_irq,
 				      int send_bytes,
@@ -518,9 +781,24 @@
 	       (aux_clock_divider << DP_AUX_CH_CTL_BIT_CLOCK_2X_SHIFT);
 }
 
+static uint32_t skl_get_aux_send_ctl(struct intel_dp *intel_dp,
+				      bool has_aux_irq,
+				      int send_bytes,
+				      uint32_t unused)
+{
+	return DP_AUX_CH_CTL_SEND_BUSY |
+	       DP_AUX_CH_CTL_DONE |
+	       (has_aux_irq ? DP_AUX_CH_CTL_INTERRUPT : 0) |
+	       DP_AUX_CH_CTL_TIME_OUT_ERROR |
+	       DP_AUX_CH_CTL_TIME_OUT_1600us |
+	       DP_AUX_CH_CTL_RECEIVE_ERROR |
+	       (send_bytes << DP_AUX_CH_CTL_MESSAGE_SIZE_SHIFT) |
+	       DP_AUX_CH_CTL_SYNC_PULSE_SKL(32);
+}
+
 static int
 intel_dp_aux_ch(struct intel_dp *intel_dp,
-		uint8_t *send, int send_bytes,
+		const uint8_t *send, int send_bytes,
 		uint8_t *recv, int recv_size)
 {
 	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
@@ -535,7 +813,15 @@
 	bool has_aux_irq = HAS_AUX_IRQ(dev);
 	bool vdd;
 
-	vdd = _edp_panel_vdd_on(intel_dp);
+	pps_lock(intel_dp);
+
+	/*
+	 * We will be called with VDD already enabled for dpcd/edid/oui reads.
+	 * In such cases we want to leave VDD enabled and it's up to upper layers
+	 * to turn it off. But for eg. i2c-dev access we need to turn it on/off
+	 * ourselves.
+	 */
+	vdd = edp_panel_vdd_on(intel_dp);
 
 	/* dp aux is extremely sensitive to irq latency, hence request the
 	 * lowest possible wakeup latency and so prevent the cpu from going into
@@ -579,7 +865,8 @@
 			/* Load the send data into the aux channel data registers */
 			for (i = 0; i < send_bytes; i += 4)
 				I915_WRITE(ch_data + i,
-					   pack_aux(send + i, send_bytes - i));
+					   intel_dp_pack_aux(send + i,
+							     send_bytes - i));
 
 			/* Send the command and wait for it to complete */
 			I915_WRITE(ch_ctl, send_ctl);
@@ -633,8 +920,8 @@
 		recv_bytes = recv_size;
 
 	for (i = 0; i < recv_bytes; i += 4)
-		unpack_aux(I915_READ(ch_data + i),
-			   recv + i, recv_bytes - i);
+		intel_dp_unpack_aux(I915_READ(ch_data + i),
+				    recv + i, recv_bytes - i);
 
 	ret = recv_bytes;
 out:
@@ -644,6 +931,8 @@
 	if (vdd)
 		edp_panel_vdd_off(intel_dp, false);
 
+	pps_unlock(intel_dp);
+
 	return ret;
 }
 
@@ -742,7 +1031,16 @@
 		BUG();
 	}
 
-	if (!HAS_DDI(dev))
+	/*
+	 * The AUX_CTL register is usually DP_CTL + 0x10.
+	 *
+	 * On Haswell and Broadwell though:
+	 *   - Both port A DDI_BUF_CTL and DDI_AUX_CTL are on the CPU
+	 *   - Port B/C/D AUX channels are on the PCH, DDI_BUF_CTL on the CPU
+	 *
+	 * Skylake moves AUX_CTL back next to DDI_BUF_CTL, on the CPU.
+	 */
+	if (!IS_HASWELL(dev) && !IS_BROADWELL(dev))
 		intel_dp->aux_ch_ctl_reg = intel_dp->output_reg + 0x10;
 
 	intel_dp->aux.name = name;
@@ -780,6 +1078,33 @@
 }
 
 static void
+skl_edp_set_pll_config(struct intel_crtc_config *pipe_config, int link_bw)
+{
+	u32 ctrl1;
+
+	pipe_config->ddi_pll_sel = SKL_DPLL0;
+	pipe_config->dpll_hw_state.cfgcr1 = 0;
+	pipe_config->dpll_hw_state.cfgcr2 = 0;
+
+	ctrl1 = DPLL_CTRL1_OVERRIDE(SKL_DPLL0);
+	switch (link_bw) {
+	case DP_LINK_BW_1_62:
+		ctrl1 |= DPLL_CRTL1_LINK_RATE(DPLL_CRTL1_LINK_RATE_810,
+					      SKL_DPLL0);
+		break;
+	case DP_LINK_BW_2_7:
+		ctrl1 |= DPLL_CRTL1_LINK_RATE(DPLL_CRTL1_LINK_RATE_1350,
+					      SKL_DPLL0);
+		break;
+	case DP_LINK_BW_5_4:
+		ctrl1 |= DPLL_CRTL1_LINK_RATE(DPLL_CRTL1_LINK_RATE_2700,
+					      SKL_DPLL0);
+		break;
+	}
+	pipe_config->dpll_hw_state.ctrl1 = ctrl1;
+}
+
+static void
 hsw_dp_set_ddi_pll_sel(struct intel_crtc_config *pipe_config, int link_bw)
 {
 	switch (link_bw) {
@@ -828,20 +1153,6 @@
 	}
 }
 
-static void
-intel_dp_set_m2_n2(struct intel_crtc *crtc, struct intel_link_m_n *m_n)
-{
-	struct drm_device *dev = crtc->base.dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	enum transcoder transcoder = crtc->config.cpu_transcoder;
-
-	I915_WRITE(PIPE_DATA_M2(transcoder),
-		TU_SIZE(m_n->tu) | m_n->gmch_m);
-	I915_WRITE(PIPE_DATA_N2(transcoder), m_n->gmch_n);
-	I915_WRITE(PIPE_LINK_M2(transcoder), m_n->link_m);
-	I915_WRITE(PIPE_LINK_N2(transcoder), m_n->link_n);
-}
-
 bool
 intel_dp_compute_config(struct intel_encoder *encoder,
 			struct intel_crtc_config *pipe_config)
@@ -867,6 +1178,7 @@
 		pipe_config->has_pch_encoder = true;
 
 	pipe_config->has_dp_encoder = true;
+	pipe_config->has_drrs = false;
 	pipe_config->has_audio = intel_dp->has_audio;
 
 	if (is_edp(intel_dp) && intel_connector->panel.fixed_mode) {
@@ -898,23 +1210,15 @@
 			bpp = dev_priv->vbt.edp_bpp;
 		}
 
-		if (IS_BROADWELL(dev)) {
-			/* Yes, it's an ugly hack. */
-			min_lane_count = max_lane_count;
-			DRM_DEBUG_KMS("forcing lane count to max (%u) on BDW\n",
-				      min_lane_count);
-		} else if (dev_priv->vbt.edp_lanes) {
-			min_lane_count = min(dev_priv->vbt.edp_lanes,
-					     max_lane_count);
-			DRM_DEBUG_KMS("using min %u lanes per VBT\n",
-				      min_lane_count);
-		}
-
-		if (dev_priv->vbt.edp_rate) {
-			min_clock = min(dev_priv->vbt.edp_rate >> 3, max_clock);
-			DRM_DEBUG_KMS("using min %02x link bw per VBT\n",
-				      bws[min_clock]);
-		}
+		/*
+		 * Use the maximum clock and number of lanes the eDP panel
+		 * advertizes being capable of. The panels are generally
+		 * designed to support only a single clock and lane
+		 * configuration, and typically these values correspond to the
+		 * native resolution of the panel.
+		 */
+		min_lane_count = max_lane_count;
+		min_clock = max_clock;
 	}
 
 	for (; bpp >= 6*3; bpp -= 2*3) {
@@ -970,13 +1274,16 @@
 
 	if (intel_connector->panel.downclock_mode != NULL &&
 		intel_dp->drrs_state.type == SEAMLESS_DRRS_SUPPORT) {
+			pipe_config->has_drrs = true;
 			intel_link_compute_m_n(bpp, lane_count,
 				intel_connector->panel.downclock_mode->clock,
 				pipe_config->port_clock,
 				&pipe_config->dp_m2_n2);
 	}
 
-	if (HAS_DDI(dev))
+	if (IS_SKYLAKE(dev) && is_edp(intel_dp))
+		skl_edp_set_pll_config(pipe_config, intel_dp->link_bw);
+	else if (IS_HASWELL(dev) || IS_BROADWELL(dev))
 		hsw_dp_set_ddi_pll_sel(pipe_config, intel_dp->link_bw);
 	else
 		intel_dp_set_clock(encoder, pipe_config, intel_dp->link_bw);
@@ -1049,12 +1356,8 @@
 	intel_dp->DP |= DP_VOLTAGE_0_4 | DP_PRE_EMPHASIS_0;
 	intel_dp->DP |= DP_PORT_WIDTH(intel_dp->lane_count);
 
-	if (crtc->config.has_audio) {
-		DRM_DEBUG_DRIVER("Enabling DP audio on pipe %c\n",
-				 pipe_name(crtc->pipe));
+	if (crtc->config.has_audio)
 		intel_dp->DP |= DP_AUDIO_OUTPUT_ENABLE;
-		intel_write_eld(&encoder->base, adjusted_mode);
-	}
 
 	/* Split out the IBX/CPU vs CPT settings */
 
@@ -1110,6 +1413,8 @@
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	u32 pp_stat_reg, pp_ctrl_reg;
 
+	lockdep_assert_held(&dev_priv->pps_mutex);
+
 	pp_stat_reg = _pp_stat_reg(intel_dp);
 	pp_ctrl_reg = _pp_ctrl_reg(intel_dp);
 
@@ -1173,13 +1478,20 @@
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	u32 control;
 
+	lockdep_assert_held(&dev_priv->pps_mutex);
+
 	control = I915_READ(_pp_ctrl_reg(intel_dp));
 	control &= ~PANEL_UNLOCK_MASK;
 	control |= PANEL_UNLOCK_REGS;
 	return control;
 }
 
-static bool _edp_panel_vdd_on(struct intel_dp *intel_dp)
+/*
+ * Must be paired with edp_panel_vdd_off().
+ * Must hold pps_mutex around the whole on/off sequence.
+ * Can be nested with intel_edp_panel_vdd_{on,off}() calls.
+ */
+static bool edp_panel_vdd_on(struct intel_dp *intel_dp)
 {
 	struct drm_device *dev = intel_dp_to_dev(intel_dp);
 	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
@@ -1190,6 +1502,8 @@
 	u32 pp_stat_reg, pp_ctrl_reg;
 	bool need_to_disable = !intel_dp->want_panel_vdd;
 
+	lockdep_assert_held(&dev_priv->pps_mutex);
+
 	if (!is_edp(intel_dp))
 		return false;
 
@@ -1201,7 +1515,8 @@
 	power_domain = intel_display_port_power_domain(intel_encoder);
 	intel_display_power_get(dev_priv, power_domain);
 
-	DRM_DEBUG_KMS("Turning eDP VDD on\n");
+	DRM_DEBUG_KMS("Turning eDP port %c VDD on\n",
+		      port_name(intel_dig_port->port));
 
 	if (!edp_have_panel_power(intel_dp))
 		wait_panel_power_cycle(intel_dp);
@@ -1220,69 +1535,86 @@
 	 * If the panel wasn't on, delay before accessing aux channel
 	 */
 	if (!edp_have_panel_power(intel_dp)) {
-		DRM_DEBUG_KMS("eDP was not running\n");
+		DRM_DEBUG_KMS("eDP port %c panel power wasn't enabled\n",
+			      port_name(intel_dig_port->port));
 		msleep(intel_dp->panel_power_up_delay);
 	}
 
 	return need_to_disable;
 }
 
+/*
+ * Must be paired with intel_edp_panel_vdd_off() or
+ * intel_edp_panel_off().
+ * Nested calls to these functions are not allowed since
+ * we drop the lock. Caller must use some higher level
+ * locking to prevent nested calls from other threads.
+ */
 void intel_edp_panel_vdd_on(struct intel_dp *intel_dp)
 {
-	if (is_edp(intel_dp)) {
-		bool vdd = _edp_panel_vdd_on(intel_dp);
+	bool vdd;
 
-		WARN(!vdd, "eDP VDD already requested on\n");
-	}
+	if (!is_edp(intel_dp))
+		return;
+
+	pps_lock(intel_dp);
+	vdd = edp_panel_vdd_on(intel_dp);
+	pps_unlock(intel_dp);
+
+	WARN(!vdd, "eDP port %c VDD already requested on\n",
+	     port_name(dp_to_dig_port(intel_dp)->port));
 }
 
 static void edp_panel_vdd_off_sync(struct intel_dp *intel_dp)
 {
 	struct drm_device *dev = intel_dp_to_dev(intel_dp);
 	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_digital_port *intel_dig_port =
+		dp_to_dig_port(intel_dp);
+	struct intel_encoder *intel_encoder = &intel_dig_port->base;
+	enum intel_display_power_domain power_domain;
 	u32 pp;
 	u32 pp_stat_reg, pp_ctrl_reg;
 
-	WARN_ON(!drm_modeset_is_locked(&dev->mode_config.connection_mutex));
+	lockdep_assert_held(&dev_priv->pps_mutex);
 
-	if (!intel_dp->want_panel_vdd && edp_have_panel_vdd(intel_dp)) {
-		struct intel_digital_port *intel_dig_port =
-						dp_to_dig_port(intel_dp);
-		struct intel_encoder *intel_encoder = &intel_dig_port->base;
-		enum intel_display_power_domain power_domain;
+	WARN_ON(intel_dp->want_panel_vdd);
+
+	if (!edp_have_panel_vdd(intel_dp))
+		return;
 
-		DRM_DEBUG_KMS("Turning eDP VDD off\n");
+	DRM_DEBUG_KMS("Turning eDP port %c VDD off\n",
+		      port_name(intel_dig_port->port));
 
-		pp = ironlake_get_pp_control(intel_dp);
-		pp &= ~EDP_FORCE_VDD;
+	pp = ironlake_get_pp_control(intel_dp);
+	pp &= ~EDP_FORCE_VDD;
 
-		pp_ctrl_reg = _pp_ctrl_reg(intel_dp);
-		pp_stat_reg = _pp_stat_reg(intel_dp);
+	pp_ctrl_reg = _pp_ctrl_reg(intel_dp);
+	pp_stat_reg = _pp_stat_reg(intel_dp);
 
-		I915_WRITE(pp_ctrl_reg, pp);
-		POSTING_READ(pp_ctrl_reg);
+	I915_WRITE(pp_ctrl_reg, pp);
+	POSTING_READ(pp_ctrl_reg);
 
-		/* Make sure sequencer is idle before allowing subsequent activity */
-		DRM_DEBUG_KMS("PP_STATUS: 0x%08x PP_CONTROL: 0x%08x\n",
-		I915_READ(pp_stat_reg), I915_READ(pp_ctrl_reg));
+	/* Make sure sequencer is idle before allowing subsequent activity */
+	DRM_DEBUG_KMS("PP_STATUS: 0x%08x PP_CONTROL: 0x%08x\n",
+	I915_READ(pp_stat_reg), I915_READ(pp_ctrl_reg));
 
-		if ((pp & POWER_TARGET_ON) == 0)
-			intel_dp->last_power_cycle = jiffies;
+	if ((pp & POWER_TARGET_ON) == 0)
+		intel_dp->last_power_cycle = jiffies;
 
-		power_domain = intel_display_port_power_domain(intel_encoder);
-		intel_display_power_put(dev_priv, power_domain);
-	}
+	power_domain = intel_display_port_power_domain(intel_encoder);
+	intel_display_power_put(dev_priv, power_domain);
 }
 
 static void edp_panel_vdd_work(struct work_struct *__work)
 {
 	struct intel_dp *intel_dp = container_of(to_delayed_work(__work),
 						 struct intel_dp, panel_vdd_work);
-	struct drm_device *dev = intel_dp_to_dev(intel_dp);
 
-	drm_modeset_lock(&dev->mode_config.connection_mutex, NULL);
-	edp_panel_vdd_off_sync(intel_dp);
-	drm_modeset_unlock(&dev->mode_config.connection_mutex);
+	pps_lock(intel_dp);
+	if (!intel_dp->want_panel_vdd)
+		edp_panel_vdd_off_sync(intel_dp);
+	pps_unlock(intel_dp);
 }
 
 static void edp_panel_vdd_schedule_off(struct intel_dp *intel_dp)
@@ -1298,12 +1630,23 @@
 	schedule_delayed_work(&intel_dp->panel_vdd_work, delay);
 }
 
+/*
+ * Must be paired with edp_panel_vdd_on().
+ * Must hold pps_mutex around the whole on/off sequence.
+ * Can be nested with intel_edp_panel_vdd_{on,off}() calls.
+ */
 static void edp_panel_vdd_off(struct intel_dp *intel_dp, bool sync)
 {
+	struct drm_i915_private *dev_priv =
+		intel_dp_to_dev(intel_dp)->dev_private;
+
+	lockdep_assert_held(&dev_priv->pps_mutex);
+
 	if (!is_edp(intel_dp))
 		return;
 
-	WARN(!intel_dp->want_panel_vdd, "eDP VDD not forced on");
+	WARN(!intel_dp->want_panel_vdd, "eDP port %c VDD not forced on",
+	     port_name(dp_to_dig_port(intel_dp)->port));
 
 	intel_dp->want_panel_vdd = false;
 
@@ -1313,22 +1656,25 @@
 		edp_panel_vdd_schedule_off(intel_dp);
 }
 
-void intel_edp_panel_on(struct intel_dp *intel_dp)
+static void edp_panel_on(struct intel_dp *intel_dp)
 {
 	struct drm_device *dev = intel_dp_to_dev(intel_dp);
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	u32 pp;
 	u32 pp_ctrl_reg;
 
+	lockdep_assert_held(&dev_priv->pps_mutex);
+
 	if (!is_edp(intel_dp))
 		return;
 
-	DRM_DEBUG_KMS("Turn eDP power on\n");
+	DRM_DEBUG_KMS("Turn eDP port %c panel power on\n",
+		      port_name(dp_to_dig_port(intel_dp)->port));
 
-	if (edp_have_panel_power(intel_dp)) {
-		DRM_DEBUG_KMS("eDP power already on\n");
+	if (WARN(edp_have_panel_power(intel_dp),
+		 "eDP port %c panel power already on\n",
+		 port_name(dp_to_dig_port(intel_dp)->port)))
 		return;
-	}
 
 	wait_panel_power_cycle(intel_dp);
 
@@ -1358,7 +1704,18 @@
 	}
 }
 
-void intel_edp_panel_off(struct intel_dp *intel_dp)
+void intel_edp_panel_on(struct intel_dp *intel_dp)
+{
+	if (!is_edp(intel_dp))
+		return;
+
+	pps_lock(intel_dp);
+	edp_panel_on(intel_dp);
+	pps_unlock(intel_dp);
+}
+
+
+static void edp_panel_off(struct intel_dp *intel_dp)
 {
 	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
 	struct intel_encoder *intel_encoder = &intel_dig_port->base;
@@ -1368,12 +1725,16 @@
 	u32 pp;
 	u32 pp_ctrl_reg;
 
+	lockdep_assert_held(&dev_priv->pps_mutex);
+
 	if (!is_edp(intel_dp))
 		return;
 
-	DRM_DEBUG_KMS("Turn eDP power off\n");
+	DRM_DEBUG_KMS("Turn eDP port %c panel power off\n",
+		      port_name(dp_to_dig_port(intel_dp)->port));
 
-	WARN(!intel_dp->want_panel_vdd, "Need VDD to turn off panel\n");
+	WARN(!intel_dp->want_panel_vdd, "Need eDP port %c VDD to turn off panel\n",
+	     port_name(dp_to_dig_port(intel_dp)->port));
 
 	pp = ironlake_get_pp_control(intel_dp);
 	/* We need to switch off panel power _and_ force vdd, for otherwise some
@@ -1396,20 +1757,24 @@
 	intel_display_power_put(dev_priv, power_domain);
 }
 
-void intel_edp_backlight_on(struct intel_dp *intel_dp)
+void intel_edp_panel_off(struct intel_dp *intel_dp)
 {
-	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
-	struct drm_device *dev = intel_dig_port->base.base.dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	u32 pp;
-	u32 pp_ctrl_reg;
-
 	if (!is_edp(intel_dp))
 		return;
 
-	DRM_DEBUG_KMS("\n");
+	pps_lock(intel_dp);
+	edp_panel_off(intel_dp);
+	pps_unlock(intel_dp);
+}
 
-	intel_panel_enable_backlight(intel_dp->attached_connector);
+/* Enable backlight in the panel power control. */
+static void _intel_edp_backlight_on(struct intel_dp *intel_dp)
+{
+	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+	struct drm_device *dev = intel_dig_port->base.base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	u32 pp;
+	u32 pp_ctrl_reg;
 
 	/*
 	 * If we enable the backlight right away following a panel power
@@ -1418,6 +1783,9 @@
 	 * allowing it to appear.
 	 */
 	wait_backlight_on(intel_dp);
+
+	pps_lock(intel_dp);
+
 	pp = ironlake_get_pp_control(intel_dp);
 	pp |= EDP_BLC_ENABLE;
 
@@ -1425,9 +1793,24 @@
 
 	I915_WRITE(pp_ctrl_reg, pp);
 	POSTING_READ(pp_ctrl_reg);
+
+	pps_unlock(intel_dp);
 }
 
-void intel_edp_backlight_off(struct intel_dp *intel_dp)
+/* Enable backlight PWM and backlight PP control. */
+void intel_edp_backlight_on(struct intel_dp *intel_dp)
+{
+	if (!is_edp(intel_dp))
+		return;
+
+	DRM_DEBUG_KMS("\n");
+
+	intel_panel_enable_backlight(intel_dp->attached_connector);
+	_intel_edp_backlight_on(intel_dp);
+}
+
+/* Disable backlight in the panel power control. */
+static void _intel_edp_backlight_off(struct intel_dp *intel_dp)
 {
 	struct drm_device *dev = intel_dp_to_dev(intel_dp);
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -1437,7 +1820,8 @@
 	if (!is_edp(intel_dp))
 		return;
 
-	DRM_DEBUG_KMS("\n");
+	pps_lock(intel_dp);
+
 	pp = ironlake_get_pp_control(intel_dp);
 	pp &= ~EDP_BLC_ENABLE;
 
@@ -1445,13 +1829,51 @@
 
 	I915_WRITE(pp_ctrl_reg, pp);
 	POSTING_READ(pp_ctrl_reg);
-	intel_dp->last_backlight_off = jiffies;
 
+	pps_unlock(intel_dp);
+
+	intel_dp->last_backlight_off = jiffies;
 	edp_wait_backlight_off(intel_dp);
+}
+
+/* Disable backlight PP control and backlight PWM. */
+void intel_edp_backlight_off(struct intel_dp *intel_dp)
+{
+	if (!is_edp(intel_dp))
+		return;
+
+	DRM_DEBUG_KMS("\n");
 
+	_intel_edp_backlight_off(intel_dp);
 	intel_panel_disable_backlight(intel_dp->attached_connector);
 }
 
+/*
+ * Hook for controlling the panel power control backlight through the bl_power
+ * sysfs attribute. Take care to handle multiple calls.
+ */
+static void intel_edp_backlight_power(struct intel_connector *connector,
+				      bool enable)
+{
+	struct intel_dp *intel_dp = intel_attached_dp(&connector->base);
+	bool is_enabled;
+
+	pps_lock(intel_dp);
+	is_enabled = ironlake_get_pp_control(intel_dp) & EDP_BLC_ENABLE;
+	pps_unlock(intel_dp);
+
+	if (is_enabled == enable)
+		return;
+
+	DRM_DEBUG_KMS("panel power control backlight %s\n",
+		      enable ? "enable" : "disable");
+
+	if (enable)
+		_intel_edp_backlight_on(intel_dp);
+	else
+		_intel_edp_backlight_off(intel_dp);
+}
+
 static void ironlake_edp_pll_on(struct intel_dp *intel_dp)
 {
 	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
@@ -1515,8 +1937,6 @@
 	if (mode != DRM_MODE_DPMS_ON) {
 		ret = drm_dp_dpcd_writeb(&intel_dp->aux, DP_SET_POWER,
 					 DP_SET_POWER_D3);
-		if (ret != 1)
-			DRM_DEBUG_DRIVER("failed to write sink power state\n");
 	} else {
 		/*
 		 * When turning on, we need to retry for 1ms to give the sink
@@ -1530,6 +1950,10 @@
 			msleep(1);
 		}
 	}
+
+	if (ret != 1)
+		DRM_DEBUG_KMS("failed to %s sink power state\n",
+			      mode == DRM_MODE_DPMS_ON ? "enable" : "disable");
 }
 
 static bool intel_dp_get_hw_state(struct intel_encoder *encoder,
@@ -1543,7 +1967,7 @@
 	u32 tmp;
 
 	power_domain = intel_display_port_power_domain(encoder);
-	if (!intel_display_power_enabled(dev_priv, power_domain))
+	if (!intel_display_power_is_enabled(dev_priv, power_domain))
 		return false;
 
 	tmp = I915_READ(intel_dp->output_reg);
@@ -1576,7 +2000,7 @@
 			return true;
 		}
 
-		for_each_pipe(i) {
+		for_each_pipe(dev_priv, i) {
 			trans_dp = I915_READ(TRANS_DP_CTL(i));
 			if ((trans_dp & TRANS_DP_PORT_SEL_MASK) == trans_sel) {
 				*pipe = i;
@@ -1675,369 +2099,14 @@
 	}
 }
 
-static bool is_edp_psr(struct intel_dp *intel_dp)
-{
-	return intel_dp->psr_dpcd[0] & DP_PSR_IS_SUPPORTED;
-}
-
-static bool intel_edp_is_psr_enabled(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-
-	if (!HAS_PSR(dev))
-		return false;
-
-	return I915_READ(EDP_PSR_CTL(dev)) & EDP_PSR_ENABLE;
-}
-
-static void intel_edp_psr_write_vsc(struct intel_dp *intel_dp,
-				    struct edp_vsc_psr *vsc_psr)
-{
-	struct intel_digital_port *dig_port = dp_to_dig_port(intel_dp);
-	struct drm_device *dev = dig_port->base.base.dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_crtc *crtc = to_intel_crtc(dig_port->base.base.crtc);
-	u32 ctl_reg = HSW_TVIDEO_DIP_CTL(crtc->config.cpu_transcoder);
-	u32 data_reg = HSW_TVIDEO_DIP_VSC_DATA(crtc->config.cpu_transcoder);
-	uint32_t *data = (uint32_t *) vsc_psr;
-	unsigned int i;
-
-	/* As per BSPec (Pipe Video Data Island Packet), we need to disable
-	   the video DIP being updated before program video DIP data buffer
-	   registers for DIP being updated. */
-	I915_WRITE(ctl_reg, 0);
-	POSTING_READ(ctl_reg);
-
-	for (i = 0; i < VIDEO_DIP_VSC_DATA_SIZE; i += 4) {
-		if (i < sizeof(struct edp_vsc_psr))
-			I915_WRITE(data_reg + i, *data++);
-		else
-			I915_WRITE(data_reg + i, 0);
-	}
-
-	I915_WRITE(ctl_reg, VIDEO_DIP_ENABLE_VSC_HSW);
-	POSTING_READ(ctl_reg);
-}
-
-static void intel_edp_psr_setup(struct intel_dp *intel_dp)
-{
-	struct drm_device *dev = intel_dp_to_dev(intel_dp);
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct edp_vsc_psr psr_vsc;
-
-	/* Prepare VSC packet as per EDP 1.3 spec, Table 3.10 */
-	memset(&psr_vsc, 0, sizeof(psr_vsc));
-	psr_vsc.sdp_header.HB0 = 0;
-	psr_vsc.sdp_header.HB1 = 0x7;
-	psr_vsc.sdp_header.HB2 = 0x2;
-	psr_vsc.sdp_header.HB3 = 0x8;
-	intel_edp_psr_write_vsc(intel_dp, &psr_vsc);
-
-	/* Avoid continuous PSR exit by masking memup and hpd */
-	I915_WRITE(EDP_PSR_DEBUG_CTL(dev), EDP_PSR_DEBUG_MASK_MEMUP |
-		   EDP_PSR_DEBUG_MASK_HPD | EDP_PSR_DEBUG_MASK_LPSP);
-}
-
-static void intel_edp_psr_enable_sink(struct intel_dp *intel_dp)
-{
-	struct intel_digital_port *dig_port = dp_to_dig_port(intel_dp);
-	struct drm_device *dev = dig_port->base.base.dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	uint32_t aux_clock_divider;
-	int precharge = 0x3;
-	int msg_size = 5;       /* Header(4) + Message(1) */
-	bool only_standby = false;
-
-	aux_clock_divider = intel_dp->get_aux_clock_divider(intel_dp, 0);
-
-	if (IS_BROADWELL(dev) && dig_port->port != PORT_A)
-		only_standby = true;
-
-	/* Enable PSR in sink */
-	if (intel_dp->psr_dpcd[1] & DP_PSR_NO_TRAIN_ON_EXIT || only_standby)
-		drm_dp_dpcd_writeb(&intel_dp->aux, DP_PSR_EN_CFG,
-				   DP_PSR_ENABLE & ~DP_PSR_MAIN_LINK_ACTIVE);
-	else
-		drm_dp_dpcd_writeb(&intel_dp->aux, DP_PSR_EN_CFG,
-				   DP_PSR_ENABLE | DP_PSR_MAIN_LINK_ACTIVE);
-
-	/* Setup AUX registers */
-	I915_WRITE(EDP_PSR_AUX_DATA1(dev), EDP_PSR_DPCD_COMMAND);
-	I915_WRITE(EDP_PSR_AUX_DATA2(dev), EDP_PSR_DPCD_NORMAL_OPERATION);
-	I915_WRITE(EDP_PSR_AUX_CTL(dev),
-		   DP_AUX_CH_CTL_TIME_OUT_400us |
-		   (msg_size << DP_AUX_CH_CTL_MESSAGE_SIZE_SHIFT) |
-		   (precharge << DP_AUX_CH_CTL_PRECHARGE_2US_SHIFT) |
-		   (aux_clock_divider << DP_AUX_CH_CTL_BIT_CLOCK_2X_SHIFT));
-}
-
-static void intel_edp_psr_enable_source(struct intel_dp *intel_dp)
-{
-	struct intel_digital_port *dig_port = dp_to_dig_port(intel_dp);
-	struct drm_device *dev = dig_port->base.base.dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	uint32_t max_sleep_time = 0x1f;
-	uint32_t idle_frames = 1;
-	uint32_t val = 0x0;
-	const uint32_t link_entry_time = EDP_PSR_MIN_LINK_ENTRY_TIME_8_LINES;
-	bool only_standby = false;
-
-	if (IS_BROADWELL(dev) && dig_port->port != PORT_A)
-		only_standby = true;
-
-	if (intel_dp->psr_dpcd[1] & DP_PSR_NO_TRAIN_ON_EXIT || only_standby) {
-		val |= EDP_PSR_LINK_STANDBY;
-		val |= EDP_PSR_TP2_TP3_TIME_0us;
-		val |= EDP_PSR_TP1_TIME_0us;
-		val |= EDP_PSR_SKIP_AUX_EXIT;
-		val |= IS_BROADWELL(dev) ? BDW_PSR_SINGLE_FRAME : 0;
-	} else
-		val |= EDP_PSR_LINK_DISABLE;
-
-	I915_WRITE(EDP_PSR_CTL(dev), val |
-		   (IS_BROADWELL(dev) ? 0 : link_entry_time) |
-		   max_sleep_time << EDP_PSR_MAX_SLEEP_TIME_SHIFT |
-		   idle_frames << EDP_PSR_IDLE_FRAME_SHIFT |
-		   EDP_PSR_ENABLE);
-}
-
-static bool intel_edp_psr_match_conditions(struct intel_dp *intel_dp)
-{
-	struct intel_digital_port *dig_port = dp_to_dig_port(intel_dp);
-	struct drm_device *dev = dig_port->base.base.dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_crtc *crtc = dig_port->base.base.crtc;
-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
-
-	lockdep_assert_held(&dev_priv->psr.lock);
-	WARN_ON(!drm_modeset_is_locked(&dev->mode_config.connection_mutex));
-	WARN_ON(!drm_modeset_is_locked(&crtc->mutex));
-
-	dev_priv->psr.source_ok = false;
-
-	if (IS_HASWELL(dev) && dig_port->port != PORT_A) {
-		DRM_DEBUG_KMS("HSW ties PSR to DDI A (eDP)\n");
-		return false;
-	}
-
-	if (!i915.enable_psr) {
-		DRM_DEBUG_KMS("PSR disable by flag\n");
-		return false;
-	}
-
-	/* Below limitations aren't valid for Broadwell */
-	if (IS_BROADWELL(dev))
-		goto out;
-
-	if (I915_READ(HSW_STEREO_3D_CTL(intel_crtc->config.cpu_transcoder)) &
-	    S3D_ENABLE) {
-		DRM_DEBUG_KMS("PSR condition failed: Stereo 3D is Enabled\n");
-		return false;
-	}
-
-	if (intel_crtc->config.adjusted_mode.flags & DRM_MODE_FLAG_INTERLACE) {
-		DRM_DEBUG_KMS("PSR condition failed: Interlaced is Enabled\n");
-		return false;
-	}
-
- out:
-	dev_priv->psr.source_ok = true;
-	return true;
-}
-
-static void intel_edp_psr_do_enable(struct intel_dp *intel_dp)
-{
-	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
-	struct drm_device *dev = intel_dig_port->base.base.dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-
-	WARN_ON(I915_READ(EDP_PSR_CTL(dev)) & EDP_PSR_ENABLE);
-	WARN_ON(dev_priv->psr.active);
-	lockdep_assert_held(&dev_priv->psr.lock);
-
-	/* Enable PSR on the panel */
-	intel_edp_psr_enable_sink(intel_dp);
-
-	/* Enable PSR on the host */
-	intel_edp_psr_enable_source(intel_dp);
-
-	dev_priv->psr.active = true;
-}
-
-void intel_edp_psr_enable(struct intel_dp *intel_dp)
-{
-	struct drm_device *dev = intel_dp_to_dev(intel_dp);
-	struct drm_i915_private *dev_priv = dev->dev_private;
-
-	if (!HAS_PSR(dev)) {
-		DRM_DEBUG_KMS("PSR not supported on this platform\n");
-		return;
-	}
-
-	if (!is_edp_psr(intel_dp)) {
-		DRM_DEBUG_KMS("PSR not supported by this panel\n");
-		return;
-	}
-
-	mutex_lock(&dev_priv->psr.lock);
-	if (dev_priv->psr.enabled) {
-		DRM_DEBUG_KMS("PSR already in use\n");
-		mutex_unlock(&dev_priv->psr.lock);
-		return;
-	}
-
-	dev_priv->psr.busy_frontbuffer_bits = 0;
-
-	/* Setup PSR once */
-	intel_edp_psr_setup(intel_dp);
-
-	if (intel_edp_psr_match_conditions(intel_dp))
-		dev_priv->psr.enabled = intel_dp;
-	mutex_unlock(&dev_priv->psr.lock);
-}
-
-void intel_edp_psr_disable(struct intel_dp *intel_dp)
-{
-	struct drm_device *dev = intel_dp_to_dev(intel_dp);
-	struct drm_i915_private *dev_priv = dev->dev_private;
-
-	mutex_lock(&dev_priv->psr.lock);
-	if (!dev_priv->psr.enabled) {
-		mutex_unlock(&dev_priv->psr.lock);
-		return;
-	}
-
-	if (dev_priv->psr.active) {
-		I915_WRITE(EDP_PSR_CTL(dev),
-			   I915_READ(EDP_PSR_CTL(dev)) & ~EDP_PSR_ENABLE);
-
-		/* Wait till PSR is idle */
-		if (_wait_for((I915_READ(EDP_PSR_STATUS_CTL(dev)) &
-			       EDP_PSR_STATUS_STATE_MASK) == 0, 2000, 10))
-			DRM_ERROR("Timed out waiting for PSR Idle State\n");
-
-		dev_priv->psr.active = false;
-	} else {
-		WARN_ON(I915_READ(EDP_PSR_CTL(dev)) & EDP_PSR_ENABLE);
-	}
-
-	dev_priv->psr.enabled = NULL;
-	mutex_unlock(&dev_priv->psr.lock);
-
-	cancel_delayed_work_sync(&dev_priv->psr.work);
-}
-
-static void intel_edp_psr_work(struct work_struct *work)
-{
-	struct drm_i915_private *dev_priv =
-		container_of(work, typeof(*dev_priv), psr.work.work);
-	struct intel_dp *intel_dp = dev_priv->psr.enabled;
-
-	mutex_lock(&dev_priv->psr.lock);
-	intel_dp = dev_priv->psr.enabled;
-
-	if (!intel_dp)
-		goto unlock;
-
-	/*
-	 * The delayed work can race with an invalidate hence we need to
-	 * recheck. Since psr_flush first clears this and then reschedules we
-	 * won't ever miss a flush when bailing out here.
-	 */
-	if (dev_priv->psr.busy_frontbuffer_bits)
-		goto unlock;
-
-	intel_edp_psr_do_enable(intel_dp);
-unlock:
-	mutex_unlock(&dev_priv->psr.lock);
-}
-
-static void intel_edp_psr_do_exit(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-
-	if (dev_priv->psr.active) {
-		u32 val = I915_READ(EDP_PSR_CTL(dev));
-
-		WARN_ON(!(val & EDP_PSR_ENABLE));
-
-		I915_WRITE(EDP_PSR_CTL(dev), val & ~EDP_PSR_ENABLE);
-
-		dev_priv->psr.active = false;
-	}
-
-}
-
-void intel_edp_psr_invalidate(struct drm_device *dev,
-			      unsigned frontbuffer_bits)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_crtc *crtc;
-	enum pipe pipe;
-
-	mutex_lock(&dev_priv->psr.lock);
-	if (!dev_priv->psr.enabled) {
-		mutex_unlock(&dev_priv->psr.lock);
-		return;
-	}
-
-	crtc = dp_to_dig_port(dev_priv->psr.enabled)->base.base.crtc;
-	pipe = to_intel_crtc(crtc)->pipe;
-
-	intel_edp_psr_do_exit(dev);
-
-	frontbuffer_bits &= INTEL_FRONTBUFFER_ALL_MASK(pipe);
-
-	dev_priv->psr.busy_frontbuffer_bits |= frontbuffer_bits;
-	mutex_unlock(&dev_priv->psr.lock);
-}
-
-void intel_edp_psr_flush(struct drm_device *dev,
-			 unsigned frontbuffer_bits)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_crtc *crtc;
-	enum pipe pipe;
-
-	mutex_lock(&dev_priv->psr.lock);
-	if (!dev_priv->psr.enabled) {
-		mutex_unlock(&dev_priv->psr.lock);
-		return;
-	}
-
-	crtc = dp_to_dig_port(dev_priv->psr.enabled)->base.base.crtc;
-	pipe = to_intel_crtc(crtc)->pipe;
-	dev_priv->psr.busy_frontbuffer_bits &= ~frontbuffer_bits;
-
-	/*
-	 * On Haswell sprite plane updates don't result in a psr invalidating
-	 * signal in the hardware. Which means we need to manually fake this in
-	 * software for all flushes, not just when we've seen a preceding
-	 * invalidation through frontbuffer rendering.
-	 */
-	if (IS_HASWELL(dev) &&
-	    (frontbuffer_bits & INTEL_FRONTBUFFER_SPRITE(pipe)))
-		intel_edp_psr_do_exit(dev);
-
-	if (!dev_priv->psr.active && !dev_priv->psr.busy_frontbuffer_bits)
-		schedule_delayed_work(&dev_priv->psr.work,
-				      msecs_to_jiffies(100));
-	mutex_unlock(&dev_priv->psr.lock);
-}
-
-void intel_edp_psr_init(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-
-	INIT_DELAYED_WORK(&dev_priv->psr.work, intel_edp_psr_work);
-	mutex_init(&dev_priv->psr.lock);
-}
-
 static void intel_disable_dp(struct intel_encoder *encoder)
 {
 	struct intel_dp *intel_dp = enc_to_intel_dp(&encoder->base);
-	enum port port = dp_to_dig_port(intel_dp)->port;
 	struct drm_device *dev = encoder->base.dev;
+	struct intel_crtc *crtc = to_intel_crtc(encoder->base.crtc);
+
+	if (crtc->config.has_audio)
+		intel_audio_codec_disable(encoder);
 
 	/* Make sure the panel is off before trying to change the mode. But also
 	 * ensure that we have vdd while we switch off the panel. */
@@ -2046,21 +2115,19 @@
 	intel_dp_sink_dpms(intel_dp, DRM_MODE_DPMS_OFF);
 	intel_edp_panel_off(intel_dp);
 
-	/* cpu edp my only be disable _after_ the cpu pipe/plane is disabled. */
-	if (!(port == PORT_A || IS_VALLEYVIEW(dev)))
+	/* disable the port before the pipe on g4x */
+	if (INTEL_INFO(dev)->gen < 5)
 		intel_dp_link_down(intel_dp);
 }
 
-static void g4x_post_disable_dp(struct intel_encoder *encoder)
+static void ilk_post_disable_dp(struct intel_encoder *encoder)
 {
 	struct intel_dp *intel_dp = enc_to_intel_dp(&encoder->base);
 	enum port port = dp_to_dig_port(intel_dp)->port;
 
-	if (port != PORT_A)
-		return;
-
 	intel_dp_link_down(intel_dp);
-	ironlake_edp_pll_off(intel_dp);
+	if (port == PORT_A)
+		ironlake_edp_pll_off(intel_dp);
 }
 
 static void vlv_post_disable_dp(struct intel_encoder *encoder)
@@ -2106,23 +2173,150 @@
 	mutex_unlock(&dev_priv->dpio_lock);
 }
 
+static void
+_intel_dp_set_link_train(struct intel_dp *intel_dp,
+			 uint32_t *DP,
+			 uint8_t dp_train_pat)
+{
+	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+	struct drm_device *dev = intel_dig_port->base.base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	enum port port = intel_dig_port->port;
+
+	if (HAS_DDI(dev)) {
+		uint32_t temp = I915_READ(DP_TP_CTL(port));
+
+		if (dp_train_pat & DP_LINK_SCRAMBLING_DISABLE)
+			temp |= DP_TP_CTL_SCRAMBLE_DISABLE;
+		else
+			temp &= ~DP_TP_CTL_SCRAMBLE_DISABLE;
+
+		temp &= ~DP_TP_CTL_LINK_TRAIN_MASK;
+		switch (dp_train_pat & DP_TRAINING_PATTERN_MASK) {
+		case DP_TRAINING_PATTERN_DISABLE:
+			temp |= DP_TP_CTL_LINK_TRAIN_NORMAL;
+
+			break;
+		case DP_TRAINING_PATTERN_1:
+			temp |= DP_TP_CTL_LINK_TRAIN_PAT1;
+			break;
+		case DP_TRAINING_PATTERN_2:
+			temp |= DP_TP_CTL_LINK_TRAIN_PAT2;
+			break;
+		case DP_TRAINING_PATTERN_3:
+			temp |= DP_TP_CTL_LINK_TRAIN_PAT3;
+			break;
+		}
+		I915_WRITE(DP_TP_CTL(port), temp);
+
+	} else if (HAS_PCH_CPT(dev) && (IS_GEN7(dev) || port != PORT_A)) {
+		*DP &= ~DP_LINK_TRAIN_MASK_CPT;
+
+		switch (dp_train_pat & DP_TRAINING_PATTERN_MASK) {
+		case DP_TRAINING_PATTERN_DISABLE:
+			*DP |= DP_LINK_TRAIN_OFF_CPT;
+			break;
+		case DP_TRAINING_PATTERN_1:
+			*DP |= DP_LINK_TRAIN_PAT_1_CPT;
+			break;
+		case DP_TRAINING_PATTERN_2:
+			*DP |= DP_LINK_TRAIN_PAT_2_CPT;
+			break;
+		case DP_TRAINING_PATTERN_3:
+			DRM_ERROR("DP training pattern 3 not supported\n");
+			*DP |= DP_LINK_TRAIN_PAT_2_CPT;
+			break;
+		}
+
+	} else {
+		if (IS_CHERRYVIEW(dev))
+			*DP &= ~DP_LINK_TRAIN_MASK_CHV;
+		else
+			*DP &= ~DP_LINK_TRAIN_MASK;
+
+		switch (dp_train_pat & DP_TRAINING_PATTERN_MASK) {
+		case DP_TRAINING_PATTERN_DISABLE:
+			*DP |= DP_LINK_TRAIN_OFF;
+			break;
+		case DP_TRAINING_PATTERN_1:
+			*DP |= DP_LINK_TRAIN_PAT_1;
+			break;
+		case DP_TRAINING_PATTERN_2:
+			*DP |= DP_LINK_TRAIN_PAT_2;
+			break;
+		case DP_TRAINING_PATTERN_3:
+			if (IS_CHERRYVIEW(dev)) {
+				*DP |= DP_LINK_TRAIN_PAT_3_CHV;
+			} else {
+				DRM_ERROR("DP training pattern 3 not supported\n");
+				*DP |= DP_LINK_TRAIN_PAT_2;
+			}
+			break;
+		}
+	}
+}
+
+static void intel_dp_enable_port(struct intel_dp *intel_dp)
+{
+	struct drm_device *dev = intel_dp_to_dev(intel_dp);
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	/* enable with pattern 1 (as per spec) */
+	_intel_dp_set_link_train(intel_dp, &intel_dp->DP,
+				 DP_TRAINING_PATTERN_1);
+
+	I915_WRITE(intel_dp->output_reg, intel_dp->DP);
+	POSTING_READ(intel_dp->output_reg);
+
+	/*
+	 * Magic for VLV/CHV. We _must_ first set up the register
+	 * without actually enabling the port, and then do another
+	 * write to enable the port. Otherwise link training will
+	 * fail when the power sequencer is freshly used for this port.
+	 */
+	intel_dp->DP |= DP_PORT_EN;
+
+	I915_WRITE(intel_dp->output_reg, intel_dp->DP);
+	POSTING_READ(intel_dp->output_reg);
+}
+
 static void intel_enable_dp(struct intel_encoder *encoder)
 {
 	struct intel_dp *intel_dp = enc_to_intel_dp(&encoder->base);
 	struct drm_device *dev = encoder->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *crtc = to_intel_crtc(encoder->base.crtc);
 	uint32_t dp_reg = I915_READ(intel_dp->output_reg);
 
 	if (WARN_ON(dp_reg & DP_PORT_EN))
 		return;
 
-	intel_edp_panel_vdd_on(intel_dp);
+	pps_lock(intel_dp);
+
+	if (IS_VALLEYVIEW(dev))
+		vlv_init_panel_power_sequencer(intel_dp);
+
+	intel_dp_enable_port(intel_dp);
+
+	edp_panel_vdd_on(intel_dp);
+	edp_panel_on(intel_dp);
+	edp_panel_vdd_off(intel_dp, true);
+
+	pps_unlock(intel_dp);
+
+	if (IS_VALLEYVIEW(dev))
+		vlv_wait_port_ready(dev_priv, dp_to_dig_port(intel_dp));
+
 	intel_dp_sink_dpms(intel_dp, DRM_MODE_DPMS_ON);
 	intel_dp_start_link_train(intel_dp);
-	intel_edp_panel_on(intel_dp);
-	edp_panel_vdd_off(intel_dp, true);
 	intel_dp_complete_link_train(intel_dp);
 	intel_dp_stop_link_train(intel_dp);
+
+	if (crtc->config.has_audio) {
+		DRM_DEBUG_DRIVER("Enabling DP audio on pipe %c\n",
+				 pipe_name(crtc->pipe));
+		intel_audio_codec_enable(encoder);
+	}
 }
 
 static void g4x_enable_dp(struct intel_encoder *encoder)
@@ -2154,6 +2348,110 @@
 	}
 }
 
+static void vlv_detach_power_sequencer(struct intel_dp *intel_dp)
+{
+	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+	struct drm_i915_private *dev_priv = intel_dig_port->base.base.dev->dev_private;
+	enum pipe pipe = intel_dp->pps_pipe;
+	int pp_on_reg = VLV_PIPE_PP_ON_DELAYS(pipe);
+
+	edp_panel_vdd_off_sync(intel_dp);
+
+	/*
+	 * VLV seems to get confused when multiple power seqeuencers
+	 * have the same port selected (even if only one has power/vdd
+	 * enabled). The failure manifests as vlv_wait_port_ready() failing
+	 * CHV on the other hand doesn't seem to mind having the same port
+	 * selected in multiple power seqeuencers, but let's clear the
+	 * port select always when logically disconnecting a power sequencer
+	 * from a port.
+	 */
+	DRM_DEBUG_KMS("detaching pipe %c power sequencer from port %c\n",
+		      pipe_name(pipe), port_name(intel_dig_port->port));
+	I915_WRITE(pp_on_reg, 0);
+	POSTING_READ(pp_on_reg);
+
+	intel_dp->pps_pipe = INVALID_PIPE;
+}
+
+static void vlv_steal_power_sequencer(struct drm_device *dev,
+				      enum pipe pipe)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_encoder *encoder;
+
+	lockdep_assert_held(&dev_priv->pps_mutex);
+
+	if (WARN_ON(pipe != PIPE_A && pipe != PIPE_B))
+		return;
+
+	list_for_each_entry(encoder, &dev->mode_config.encoder_list,
+			    base.head) {
+		struct intel_dp *intel_dp;
+		enum port port;
+
+		if (encoder->type != INTEL_OUTPUT_EDP)
+			continue;
+
+		intel_dp = enc_to_intel_dp(&encoder->base);
+		port = dp_to_dig_port(intel_dp)->port;
+
+		if (intel_dp->pps_pipe != pipe)
+			continue;
+
+		DRM_DEBUG_KMS("stealing pipe %c power sequencer from port %c\n",
+			      pipe_name(pipe), port_name(port));
+
+		WARN(encoder->connectors_active,
+		     "stealing pipe %c power sequencer from active eDP port %c\n",
+		     pipe_name(pipe), port_name(port));
+
+		/* make sure vdd is off before we steal it */
+		vlv_detach_power_sequencer(intel_dp);
+	}
+}
+
+static void vlv_init_panel_power_sequencer(struct intel_dp *intel_dp)
+{
+	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+	struct intel_encoder *encoder = &intel_dig_port->base;
+	struct drm_device *dev = encoder->base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *crtc = to_intel_crtc(encoder->base.crtc);
+
+	lockdep_assert_held(&dev_priv->pps_mutex);
+
+	if (!is_edp(intel_dp))
+		return;
+
+	if (intel_dp->pps_pipe == crtc->pipe)
+		return;
+
+	/*
+	 * If another power sequencer was being used on this
+	 * port previously make sure to turn off vdd there while
+	 * we still have control of it.
+	 */
+	if (intel_dp->pps_pipe != INVALID_PIPE)
+		vlv_detach_power_sequencer(intel_dp);
+
+	/*
+	 * We may be stealing the power
+	 * sequencer from another port.
+	 */
+	vlv_steal_power_sequencer(dev, crtc->pipe);
+
+	/* now it's all ours */
+	intel_dp->pps_pipe = crtc->pipe;
+
+	DRM_DEBUG_KMS("initializing pipe %c power sequencer for port %c\n",
+		      pipe_name(intel_dp->pps_pipe), port_name(intel_dig_port->port));
+
+	/* init power sequencer on this pipe and port */
+	intel_dp_init_panel_power_sequencer(dev, intel_dp);
+	intel_dp_init_panel_power_sequencer_registers(dev, intel_dp);
+}
+
 static void vlv_pre_enable_dp(struct intel_encoder *encoder)
 {
 	struct intel_dp *intel_dp = enc_to_intel_dp(&encoder->base);
@@ -2163,7 +2461,6 @@
 	struct intel_crtc *intel_crtc = to_intel_crtc(encoder->base.crtc);
 	enum dpio_channel port = vlv_dport_to_channel(dport);
 	int pipe = intel_crtc->pipe;
-	struct edp_power_seq power_seq;
 	u32 val;
 
 	mutex_lock(&dev_priv->dpio_lock);
@@ -2181,16 +2478,7 @@
 
 	mutex_unlock(&dev_priv->dpio_lock);
 
-	if (is_edp(intel_dp)) {
-		/* init power sequencer on this pipe and port */
-		intel_dp_init_panel_power_sequencer(dev, intel_dp, &power_seq);
-		intel_dp_init_panel_power_sequencer_registers(dev, intel_dp,
-							      &power_seq);
-	}
-
 	intel_enable_dp(encoder);
-
-	vlv_wait_port_ready(dev_priv, dport);
 }
 
 static void vlv_dp_pre_pll_enable(struct intel_encoder *encoder)
@@ -2229,7 +2517,6 @@
 	struct intel_digital_port *dport = dp_to_dig_port(intel_dp);
 	struct drm_device *dev = encoder->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct edp_power_seq power_seq;
 	struct intel_crtc *intel_crtc =
 		to_intel_crtc(encoder->base.crtc);
 	enum dpio_channel ch = vlv_dport_to_channel(dport);
@@ -2239,6 +2526,15 @@
 
 	mutex_lock(&dev_priv->dpio_lock);
 
+	/* allow hardware to manage TX FIFO reset source */
+	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS01_DW11(ch));
+	val &= ~DPIO_LANEDESKEW_STRAP_OVRD;
+	vlv_dpio_write(dev_priv, pipe, VLV_PCS01_DW11(ch), val);
+
+	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS23_DW11(ch));
+	val &= ~DPIO_LANEDESKEW_STRAP_OVRD;
+	vlv_dpio_write(dev_priv, pipe, VLV_PCS23_DW11(ch), val);
+
 	/* Deassert soft data lane reset*/
 	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS01_DW1(ch));
 	val |= CHV_PCS_REQ_SOFTRESET_EN;
@@ -2274,16 +2570,7 @@
 
 	mutex_unlock(&dev_priv->dpio_lock);
 
-	if (is_edp(intel_dp)) {
-		/* init power sequencer on this pipe and port */
-		intel_dp_init_panel_power_sequencer(dev, intel_dp, &power_seq);
-		intel_dp_init_panel_power_sequencer_registers(dev, intel_dp,
-							      &power_seq);
-	}
-
 	intel_enable_dp(encoder);
-
-	vlv_wait_port_ready(dev_priv, dport);
 }
 
 static void chv_dp_pre_pll_enable(struct intel_encoder *encoder)
@@ -2297,6 +2584,8 @@
 	enum pipe pipe = intel_crtc->pipe;
 	u32 val;
 
+	intel_dp_prepare(encoder);
+
 	mutex_lock(&dev_priv->dpio_lock);
 
 	/* program left/right clock distribution */
@@ -2401,14 +2690,16 @@
 	struct drm_device *dev = intel_dp_to_dev(intel_dp);
 	enum port port = dp_to_dig_port(intel_dp)->port;
 
-	if (IS_VALLEYVIEW(dev))
-		return DP_TRAIN_VOLTAGE_SWING_1200;
+	if (INTEL_INFO(dev)->gen >= 9)
+		return DP_TRAIN_VOLTAGE_SWING_LEVEL_2;
+	else if (IS_VALLEYVIEW(dev))
+		return DP_TRAIN_VOLTAGE_SWING_LEVEL_3;
 	else if (IS_GEN7(dev) && port == PORT_A)
-		return DP_TRAIN_VOLTAGE_SWING_800;
+		return DP_TRAIN_VOLTAGE_SWING_LEVEL_2;
 	else if (HAS_PCH_CPT(dev) && port != PORT_A)
-		return DP_TRAIN_VOLTAGE_SWING_1200;
+		return DP_TRAIN_VOLTAGE_SWING_LEVEL_3;
 	else
-		return DP_TRAIN_VOLTAGE_SWING_800;
+		return DP_TRAIN_VOLTAGE_SWING_LEVEL_2;
 }
 
 static uint8_t
@@ -2417,51 +2708,62 @@
 	struct drm_device *dev = intel_dp_to_dev(intel_dp);
 	enum port port = dp_to_dig_port(intel_dp)->port;
 
-	if (IS_HASWELL(dev) || IS_BROADWELL(dev)) {
+	if (INTEL_INFO(dev)->gen >= 9) {
 		switch (voltage_swing & DP_TRAIN_VOLTAGE_SWING_MASK) {
-		case DP_TRAIN_VOLTAGE_SWING_400:
-			return DP_TRAIN_PRE_EMPHASIS_9_5;
-		case DP_TRAIN_VOLTAGE_SWING_600:
-			return DP_TRAIN_PRE_EMPHASIS_6;
-		case DP_TRAIN_VOLTAGE_SWING_800:
-			return DP_TRAIN_PRE_EMPHASIS_3_5;
-		case DP_TRAIN_VOLTAGE_SWING_1200:
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
+			return DP_TRAIN_PRE_EMPH_LEVEL_3;
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_1:
+			return DP_TRAIN_PRE_EMPH_LEVEL_2;
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_2:
+			return DP_TRAIN_PRE_EMPH_LEVEL_1;
 		default:
-			return DP_TRAIN_PRE_EMPHASIS_0;
+			return DP_TRAIN_PRE_EMPH_LEVEL_0;
+		}
+	} else if (IS_HASWELL(dev) || IS_BROADWELL(dev)) {
+		switch (voltage_swing & DP_TRAIN_VOLTAGE_SWING_MASK) {
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
+			return DP_TRAIN_PRE_EMPH_LEVEL_3;
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_1:
+			return DP_TRAIN_PRE_EMPH_LEVEL_2;
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_2:
+			return DP_TRAIN_PRE_EMPH_LEVEL_1;
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_3:
+		default:
+			return DP_TRAIN_PRE_EMPH_LEVEL_0;
 		}
 	} else if (IS_VALLEYVIEW(dev)) {
 		switch (voltage_swing & DP_TRAIN_VOLTAGE_SWING_MASK) {
-		case DP_TRAIN_VOLTAGE_SWING_400:
-			return DP_TRAIN_PRE_EMPHASIS_9_5;
-		case DP_TRAIN_VOLTAGE_SWING_600:
-			return DP_TRAIN_PRE_EMPHASIS_6;
-		case DP_TRAIN_VOLTAGE_SWING_800:
-			return DP_TRAIN_PRE_EMPHASIS_3_5;
-		case DP_TRAIN_VOLTAGE_SWING_1200:
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
+			return DP_TRAIN_PRE_EMPH_LEVEL_3;
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_1:
+			return DP_TRAIN_PRE_EMPH_LEVEL_2;
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_2:
+			return DP_TRAIN_PRE_EMPH_LEVEL_1;
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_3:
 		default:
-			return DP_TRAIN_PRE_EMPHASIS_0;
+			return DP_TRAIN_PRE_EMPH_LEVEL_0;
 		}
 	} else if (IS_GEN7(dev) && port == PORT_A) {
 		switch (voltage_swing & DP_TRAIN_VOLTAGE_SWING_MASK) {
-		case DP_TRAIN_VOLTAGE_SWING_400:
-			return DP_TRAIN_PRE_EMPHASIS_6;
-		case DP_TRAIN_VOLTAGE_SWING_600:
-		case DP_TRAIN_VOLTAGE_SWING_800:
-			return DP_TRAIN_PRE_EMPHASIS_3_5;
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
+			return DP_TRAIN_PRE_EMPH_LEVEL_2;
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_1:
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_2:
+			return DP_TRAIN_PRE_EMPH_LEVEL_1;
 		default:
-			return DP_TRAIN_PRE_EMPHASIS_0;
+			return DP_TRAIN_PRE_EMPH_LEVEL_0;
 		}
 	} else {
 		switch (voltage_swing & DP_TRAIN_VOLTAGE_SWING_MASK) {
-		case DP_TRAIN_VOLTAGE_SWING_400:
-			return DP_TRAIN_PRE_EMPHASIS_6;
-		case DP_TRAIN_VOLTAGE_SWING_600:
-			return DP_TRAIN_PRE_EMPHASIS_6;
-		case DP_TRAIN_VOLTAGE_SWING_800:
-			return DP_TRAIN_PRE_EMPHASIS_3_5;
-		case DP_TRAIN_VOLTAGE_SWING_1200:
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
+			return DP_TRAIN_PRE_EMPH_LEVEL_2;
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_1:
+			return DP_TRAIN_PRE_EMPH_LEVEL_2;
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_2:
+			return DP_TRAIN_PRE_EMPH_LEVEL_1;
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_3:
 		default:
-			return DP_TRAIN_PRE_EMPHASIS_0;
+			return DP_TRAIN_PRE_EMPH_LEVEL_0;
 		}
 	}
 }
@@ -2480,22 +2782,22 @@
 	int pipe = intel_crtc->pipe;
 
 	switch (train_set & DP_TRAIN_PRE_EMPHASIS_MASK) {
-	case DP_TRAIN_PRE_EMPHASIS_0:
+	case DP_TRAIN_PRE_EMPH_LEVEL_0:
 		preemph_reg_value = 0x0004000;
 		switch (train_set & DP_TRAIN_VOLTAGE_SWING_MASK) {
-		case DP_TRAIN_VOLTAGE_SWING_400:
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
 			demph_reg_value = 0x2B405555;
 			uniqtranscale_reg_value = 0x552AB83A;
 			break;
-		case DP_TRAIN_VOLTAGE_SWING_600:
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_1:
 			demph_reg_value = 0x2B404040;
 			uniqtranscale_reg_value = 0x5548B83A;
 			break;
-		case DP_TRAIN_VOLTAGE_SWING_800:
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_2:
 			demph_reg_value = 0x2B245555;
 			uniqtranscale_reg_value = 0x5560B83A;
 			break;
-		case DP_TRAIN_VOLTAGE_SWING_1200:
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_3:
 			demph_reg_value = 0x2B405555;
 			uniqtranscale_reg_value = 0x5598DA3A;
 			break;
@@ -2503,18 +2805,18 @@
 			return 0;
 		}
 		break;
-	case DP_TRAIN_PRE_EMPHASIS_3_5:
+	case DP_TRAIN_PRE_EMPH_LEVEL_1:
 		preemph_reg_value = 0x0002000;
 		switch (train_set & DP_TRAIN_VOLTAGE_SWING_MASK) {
-		case DP_TRAIN_VOLTAGE_SWING_400:
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
 			demph_reg_value = 0x2B404040;
 			uniqtranscale_reg_value = 0x5552B83A;
 			break;
-		case DP_TRAIN_VOLTAGE_SWING_600:
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_1:
 			demph_reg_value = 0x2B404848;
 			uniqtranscale_reg_value = 0x5580B83A;
 			break;
-		case DP_TRAIN_VOLTAGE_SWING_800:
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_2:
 			demph_reg_value = 0x2B404040;
 			uniqtranscale_reg_value = 0x55ADDA3A;
 			break;
@@ -2522,14 +2824,14 @@
 			return 0;
 		}
 		break;
-	case DP_TRAIN_PRE_EMPHASIS_6:
+	case DP_TRAIN_PRE_EMPH_LEVEL_2:
 		preemph_reg_value = 0x0000000;
 		switch (train_set & DP_TRAIN_VOLTAGE_SWING_MASK) {
-		case DP_TRAIN_VOLTAGE_SWING_400:
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
 			demph_reg_value = 0x2B305555;
 			uniqtranscale_reg_value = 0x5570B83A;
 			break;
-		case DP_TRAIN_VOLTAGE_SWING_600:
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_1:
 			demph_reg_value = 0x2B2B4040;
 			uniqtranscale_reg_value = 0x55ADDA3A;
 			break;
@@ -2537,10 +2839,10 @@
 			return 0;
 		}
 		break;
-	case DP_TRAIN_PRE_EMPHASIS_9_5:
+	case DP_TRAIN_PRE_EMPH_LEVEL_3:
 		preemph_reg_value = 0x0006000;
 		switch (train_set & DP_TRAIN_VOLTAGE_SWING_MASK) {
-		case DP_TRAIN_VOLTAGE_SWING_400:
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
 			demph_reg_value = 0x1B405555;
 			uniqtranscale_reg_value = 0x55ADDA3A;
 			break;
@@ -2579,21 +2881,21 @@
 	int i;
 
 	switch (train_set & DP_TRAIN_PRE_EMPHASIS_MASK) {
-	case DP_TRAIN_PRE_EMPHASIS_0:
+	case DP_TRAIN_PRE_EMPH_LEVEL_0:
 		switch (train_set & DP_TRAIN_VOLTAGE_SWING_MASK) {
-		case DP_TRAIN_VOLTAGE_SWING_400:
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
 			deemph_reg_value = 128;
 			margin_reg_value = 52;
 			break;
-		case DP_TRAIN_VOLTAGE_SWING_600:
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_1:
 			deemph_reg_value = 128;
 			margin_reg_value = 77;
 			break;
-		case DP_TRAIN_VOLTAGE_SWING_800:
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_2:
 			deemph_reg_value = 128;
 			margin_reg_value = 102;
 			break;
-		case DP_TRAIN_VOLTAGE_SWING_1200:
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_3:
 			deemph_reg_value = 128;
 			margin_reg_value = 154;
 			/* FIXME extra to set for 1200 */
@@ -2602,17 +2904,17 @@
 			return 0;
 		}
 		break;
-	case DP_TRAIN_PRE_EMPHASIS_3_5:
+	case DP_TRAIN_PRE_EMPH_LEVEL_1:
 		switch (train_set & DP_TRAIN_VOLTAGE_SWING_MASK) {
-		case DP_TRAIN_VOLTAGE_SWING_400:
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
 			deemph_reg_value = 85;
 			margin_reg_value = 78;
 			break;
-		case DP_TRAIN_VOLTAGE_SWING_600:
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_1:
 			deemph_reg_value = 85;
 			margin_reg_value = 116;
 			break;
-		case DP_TRAIN_VOLTAGE_SWING_800:
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_2:
 			deemph_reg_value = 85;
 			margin_reg_value = 154;
 			break;
@@ -2620,13 +2922,13 @@
 			return 0;
 		}
 		break;
-	case DP_TRAIN_PRE_EMPHASIS_6:
+	case DP_TRAIN_PRE_EMPH_LEVEL_2:
 		switch (train_set & DP_TRAIN_VOLTAGE_SWING_MASK) {
-		case DP_TRAIN_VOLTAGE_SWING_400:
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
 			deemph_reg_value = 64;
 			margin_reg_value = 104;
 			break;
-		case DP_TRAIN_VOLTAGE_SWING_600:
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_1:
 			deemph_reg_value = 64;
 			margin_reg_value = 154;
 			break;
@@ -2634,9 +2936,9 @@
 			return 0;
 		}
 		break;
-	case DP_TRAIN_PRE_EMPHASIS_9_5:
+	case DP_TRAIN_PRE_EMPH_LEVEL_3:
 		switch (train_set & DP_TRAIN_VOLTAGE_SWING_MASK) {
-		case DP_TRAIN_VOLTAGE_SWING_400:
+		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
 			deemph_reg_value = 43;
 			margin_reg_value = 154;
 			break;
@@ -2653,12 +2955,26 @@
 	/* Clear calc init */
 	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS01_DW10(ch));
 	val &= ~(DPIO_PCS_SWING_CALC_TX0_TX2 | DPIO_PCS_SWING_CALC_TX1_TX3);
+	val &= ~(DPIO_PCS_TX1DEEMP_MASK | DPIO_PCS_TX2DEEMP_MASK);
+	val |= DPIO_PCS_TX1DEEMP_9P5 | DPIO_PCS_TX2DEEMP_9P5;
 	vlv_dpio_write(dev_priv, pipe, VLV_PCS01_DW10(ch), val);
 
 	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS23_DW10(ch));
 	val &= ~(DPIO_PCS_SWING_CALC_TX0_TX2 | DPIO_PCS_SWING_CALC_TX1_TX3);
+	val &= ~(DPIO_PCS_TX1DEEMP_MASK | DPIO_PCS_TX2DEEMP_MASK);
+	val |= DPIO_PCS_TX1DEEMP_9P5 | DPIO_PCS_TX2DEEMP_9P5;
 	vlv_dpio_write(dev_priv, pipe, VLV_PCS23_DW10(ch), val);
 
+	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS01_DW9(ch));
+	val &= ~(DPIO_PCS_TX1MARGIN_MASK | DPIO_PCS_TX2MARGIN_MASK);
+	val |= DPIO_PCS_TX1MARGIN_000 | DPIO_PCS_TX2MARGIN_000;
+	vlv_dpio_write(dev_priv, pipe, VLV_PCS01_DW9(ch), val);
+
+	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS23_DW9(ch));
+	val &= ~(DPIO_PCS_TX1MARGIN_MASK | DPIO_PCS_TX2MARGIN_MASK);
+	val |= DPIO_PCS_TX1MARGIN_000 | DPIO_PCS_TX2MARGIN_000;
+	vlv_dpio_write(dev_priv, pipe, VLV_PCS23_DW9(ch), val);
+
 	/* Program swing deemph */
 	for (i = 0; i < 4; i++) {
 		val = vlv_dpio_read(dev_priv, pipe, CHV_TX_DW4(ch, i));
@@ -2670,8 +2986,8 @@
 	/* Program swing margin */
 	for (i = 0; i < 4; i++) {
 		val = vlv_dpio_read(dev_priv, pipe, CHV_TX_DW2(ch, i));
-		val &= ~DPIO_SWING_MARGIN_MASK;
-		val |= margin_reg_value << DPIO_SWING_MARGIN_SHIFT;
+		val &= ~DPIO_SWING_MARGIN000_MASK;
+		val |= margin_reg_value << DPIO_SWING_MARGIN000_SHIFT;
 		vlv_dpio_write(dev_priv, pipe, CHV_TX_DW2(ch, i), val);
 	}
 
@@ -2683,9 +2999,9 @@
 	}
 
 	if (((train_set & DP_TRAIN_PRE_EMPHASIS_MASK)
-			== DP_TRAIN_PRE_EMPHASIS_0) &&
+			== DP_TRAIN_PRE_EMPH_LEVEL_0) &&
 		((train_set & DP_TRAIN_VOLTAGE_SWING_MASK)
-			== DP_TRAIN_VOLTAGE_SWING_1200)) {
+			== DP_TRAIN_VOLTAGE_SWING_LEVEL_3)) {
 
 		/*
 		 * The document said it needs to set bit 27 for ch0 and bit 26
@@ -2764,32 +3080,32 @@
 	uint32_t	signal_levels = 0;
 
 	switch (train_set & DP_TRAIN_VOLTAGE_SWING_MASK) {
-	case DP_TRAIN_VOLTAGE_SWING_400:
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
 	default:
 		signal_levels |= DP_VOLTAGE_0_4;
 		break;
-	case DP_TRAIN_VOLTAGE_SWING_600:
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_1:
 		signal_levels |= DP_VOLTAGE_0_6;
 		break;
-	case DP_TRAIN_VOLTAGE_SWING_800:
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_2:
 		signal_levels |= DP_VOLTAGE_0_8;
 		break;
-	case DP_TRAIN_VOLTAGE_SWING_1200:
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_3:
 		signal_levels |= DP_VOLTAGE_1_2;
 		break;
 	}
 	switch (train_set & DP_TRAIN_PRE_EMPHASIS_MASK) {
-	case DP_TRAIN_PRE_EMPHASIS_0:
+	case DP_TRAIN_PRE_EMPH_LEVEL_0:
 	default:
 		signal_levels |= DP_PRE_EMPHASIS_0;
 		break;
-	case DP_TRAIN_PRE_EMPHASIS_3_5:
+	case DP_TRAIN_PRE_EMPH_LEVEL_1:
 		signal_levels |= DP_PRE_EMPHASIS_3_5;
 		break;
-	case DP_TRAIN_PRE_EMPHASIS_6:
+	case DP_TRAIN_PRE_EMPH_LEVEL_2:
 		signal_levels |= DP_PRE_EMPHASIS_6;
 		break;
-	case DP_TRAIN_PRE_EMPHASIS_9_5:
+	case DP_TRAIN_PRE_EMPH_LEVEL_3:
 		signal_levels |= DP_PRE_EMPHASIS_9_5;
 		break;
 	}
@@ -2803,19 +3119,19 @@
 	int signal_levels = train_set & (DP_TRAIN_VOLTAGE_SWING_MASK |
 					 DP_TRAIN_PRE_EMPHASIS_MASK);
 	switch (signal_levels) {
-	case DP_TRAIN_VOLTAGE_SWING_400 | DP_TRAIN_PRE_EMPHASIS_0:
-	case DP_TRAIN_VOLTAGE_SWING_600 | DP_TRAIN_PRE_EMPHASIS_0:
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_0 | DP_TRAIN_PRE_EMPH_LEVEL_0:
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_1 | DP_TRAIN_PRE_EMPH_LEVEL_0:
 		return EDP_LINK_TRAIN_400_600MV_0DB_SNB_B;
-	case DP_TRAIN_VOLTAGE_SWING_400 | DP_TRAIN_PRE_EMPHASIS_3_5:
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_0 | DP_TRAIN_PRE_EMPH_LEVEL_1:
 		return EDP_LINK_TRAIN_400MV_3_5DB_SNB_B;
-	case DP_TRAIN_VOLTAGE_SWING_400 | DP_TRAIN_PRE_EMPHASIS_6:
-	case DP_TRAIN_VOLTAGE_SWING_600 | DP_TRAIN_PRE_EMPHASIS_6:
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_0 | DP_TRAIN_PRE_EMPH_LEVEL_2:
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_1 | DP_TRAIN_PRE_EMPH_LEVEL_2:
 		return EDP_LINK_TRAIN_400_600MV_6DB_SNB_B;
-	case DP_TRAIN_VOLTAGE_SWING_600 | DP_TRAIN_PRE_EMPHASIS_3_5:
-	case DP_TRAIN_VOLTAGE_SWING_800 | DP_TRAIN_PRE_EMPHASIS_3_5:
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_1 | DP_TRAIN_PRE_EMPH_LEVEL_1:
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_2 | DP_TRAIN_PRE_EMPH_LEVEL_1:
 		return EDP_LINK_TRAIN_600_800MV_3_5DB_SNB_B;
-	case DP_TRAIN_VOLTAGE_SWING_800 | DP_TRAIN_PRE_EMPHASIS_0:
-	case DP_TRAIN_VOLTAGE_SWING_1200 | DP_TRAIN_PRE_EMPHASIS_0:
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_2 | DP_TRAIN_PRE_EMPH_LEVEL_0:
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_3 | DP_TRAIN_PRE_EMPH_LEVEL_0:
 		return EDP_LINK_TRAIN_800_1200MV_0DB_SNB_B;
 	default:
 		DRM_DEBUG_KMS("Unsupported voltage swing/pre-emphasis level:"
@@ -2831,21 +3147,21 @@
 	int signal_levels = train_set & (DP_TRAIN_VOLTAGE_SWING_MASK |
 					 DP_TRAIN_PRE_EMPHASIS_MASK);
 	switch (signal_levels) {
-	case DP_TRAIN_VOLTAGE_SWING_400 | DP_TRAIN_PRE_EMPHASIS_0:
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_0 | DP_TRAIN_PRE_EMPH_LEVEL_0:
 		return EDP_LINK_TRAIN_400MV_0DB_IVB;
-	case DP_TRAIN_VOLTAGE_SWING_400 | DP_TRAIN_PRE_EMPHASIS_3_5:
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_0 | DP_TRAIN_PRE_EMPH_LEVEL_1:
 		return EDP_LINK_TRAIN_400MV_3_5DB_IVB;
-	case DP_TRAIN_VOLTAGE_SWING_400 | DP_TRAIN_PRE_EMPHASIS_6:
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_0 | DP_TRAIN_PRE_EMPH_LEVEL_2:
 		return EDP_LINK_TRAIN_400MV_6DB_IVB;
 
-	case DP_TRAIN_VOLTAGE_SWING_600 | DP_TRAIN_PRE_EMPHASIS_0:
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_1 | DP_TRAIN_PRE_EMPH_LEVEL_0:
 		return EDP_LINK_TRAIN_600MV_0DB_IVB;
-	case DP_TRAIN_VOLTAGE_SWING_600 | DP_TRAIN_PRE_EMPHASIS_3_5:
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_1 | DP_TRAIN_PRE_EMPH_LEVEL_1:
 		return EDP_LINK_TRAIN_600MV_3_5DB_IVB;
 
-	case DP_TRAIN_VOLTAGE_SWING_800 | DP_TRAIN_PRE_EMPHASIS_0:
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_2 | DP_TRAIN_PRE_EMPH_LEVEL_0:
 		return EDP_LINK_TRAIN_800MV_0DB_IVB;
-	case DP_TRAIN_VOLTAGE_SWING_800 | DP_TRAIN_PRE_EMPHASIS_3_5:
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_2 | DP_TRAIN_PRE_EMPH_LEVEL_1:
 		return EDP_LINK_TRAIN_800MV_3_5DB_IVB;
 
 	default:
@@ -2862,30 +3178,30 @@
 	int signal_levels = train_set & (DP_TRAIN_VOLTAGE_SWING_MASK |
 					 DP_TRAIN_PRE_EMPHASIS_MASK);
 	switch (signal_levels) {
-	case DP_TRAIN_VOLTAGE_SWING_400 | DP_TRAIN_PRE_EMPHASIS_0:
-		return DDI_BUF_EMP_400MV_0DB_HSW;
-	case DP_TRAIN_VOLTAGE_SWING_400 | DP_TRAIN_PRE_EMPHASIS_3_5:
-		return DDI_BUF_EMP_400MV_3_5DB_HSW;
-	case DP_TRAIN_VOLTAGE_SWING_400 | DP_TRAIN_PRE_EMPHASIS_6:
-		return DDI_BUF_EMP_400MV_6DB_HSW;
-	case DP_TRAIN_VOLTAGE_SWING_400 | DP_TRAIN_PRE_EMPHASIS_9_5:
-		return DDI_BUF_EMP_400MV_9_5DB_HSW;
-
-	case DP_TRAIN_VOLTAGE_SWING_600 | DP_TRAIN_PRE_EMPHASIS_0:
-		return DDI_BUF_EMP_600MV_0DB_HSW;
-	case DP_TRAIN_VOLTAGE_SWING_600 | DP_TRAIN_PRE_EMPHASIS_3_5:
-		return DDI_BUF_EMP_600MV_3_5DB_HSW;
-	case DP_TRAIN_VOLTAGE_SWING_600 | DP_TRAIN_PRE_EMPHASIS_6:
-		return DDI_BUF_EMP_600MV_6DB_HSW;
-
-	case DP_TRAIN_VOLTAGE_SWING_800 | DP_TRAIN_PRE_EMPHASIS_0:
-		return DDI_BUF_EMP_800MV_0DB_HSW;
-	case DP_TRAIN_VOLTAGE_SWING_800 | DP_TRAIN_PRE_EMPHASIS_3_5:
-		return DDI_BUF_EMP_800MV_3_5DB_HSW;
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_0 | DP_TRAIN_PRE_EMPH_LEVEL_0:
+		return DDI_BUF_TRANS_SELECT(0);
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_0 | DP_TRAIN_PRE_EMPH_LEVEL_1:
+		return DDI_BUF_TRANS_SELECT(1);
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_0 | DP_TRAIN_PRE_EMPH_LEVEL_2:
+		return DDI_BUF_TRANS_SELECT(2);
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_0 | DP_TRAIN_PRE_EMPH_LEVEL_3:
+		return DDI_BUF_TRANS_SELECT(3);
+
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_1 | DP_TRAIN_PRE_EMPH_LEVEL_0:
+		return DDI_BUF_TRANS_SELECT(4);
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_1 | DP_TRAIN_PRE_EMPH_LEVEL_1:
+		return DDI_BUF_TRANS_SELECT(5);
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_1 | DP_TRAIN_PRE_EMPH_LEVEL_2:
+		return DDI_BUF_TRANS_SELECT(6);
+
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_2 | DP_TRAIN_PRE_EMPH_LEVEL_0:
+		return DDI_BUF_TRANS_SELECT(7);
+	case DP_TRAIN_VOLTAGE_SWING_LEVEL_2 | DP_TRAIN_PRE_EMPH_LEVEL_1:
+		return DDI_BUF_TRANS_SELECT(8);
 	default:
 		DRM_DEBUG_KMS("Unsupported voltage swing/pre-emphasis level:"
 			      "0x%x\n", signal_levels);
-		return DDI_BUF_EMP_400MV_0DB_HSW;
+		return DDI_BUF_TRANS_SELECT(0);
 	}
 }
 
@@ -2899,7 +3215,7 @@
 	uint32_t signal_levels, mask;
 	uint8_t train_set = intel_dp->train_set[0];
 
-	if (IS_HASWELL(dev) || IS_BROADWELL(dev)) {
+	if (IS_HASWELL(dev) || IS_BROADWELL(dev) || INTEL_INFO(dev)->gen >= 9) {
 		signal_levels = intel_hsw_signal_levels(train_set);
 		mask = DDI_BUF_EMP_MASK;
 	} else if (IS_CHERRYVIEW(dev)) {
@@ -2922,84 +3238,20 @@
 	DRM_DEBUG_KMS("Using signal levels %08x\n", signal_levels);
 
 	*DP = (*DP & ~mask) | signal_levels;
-}
-
-static bool
-intel_dp_set_link_train(struct intel_dp *intel_dp,
-			uint32_t *DP,
-			uint8_t dp_train_pat)
-{
-	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
-	struct drm_device *dev = intel_dig_port->base.base.dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	enum port port = intel_dig_port->port;
-	uint8_t buf[sizeof(intel_dp->train_set) + 1];
-	int ret, len;
-
-	if (HAS_DDI(dev)) {
-		uint32_t temp = I915_READ(DP_TP_CTL(port));
-
-		if (dp_train_pat & DP_LINK_SCRAMBLING_DISABLE)
-			temp |= DP_TP_CTL_SCRAMBLE_DISABLE;
-		else
-			temp &= ~DP_TP_CTL_SCRAMBLE_DISABLE;
-
-		temp &= ~DP_TP_CTL_LINK_TRAIN_MASK;
-		switch (dp_train_pat & DP_TRAINING_PATTERN_MASK) {
-		case DP_TRAINING_PATTERN_DISABLE:
-			temp |= DP_TP_CTL_LINK_TRAIN_NORMAL;
-
-			break;
-		case DP_TRAINING_PATTERN_1:
-			temp |= DP_TP_CTL_LINK_TRAIN_PAT1;
-			break;
-		case DP_TRAINING_PATTERN_2:
-			temp |= DP_TP_CTL_LINK_TRAIN_PAT2;
-			break;
-		case DP_TRAINING_PATTERN_3:
-			temp |= DP_TP_CTL_LINK_TRAIN_PAT3;
-			break;
-		}
-		I915_WRITE(DP_TP_CTL(port), temp);
-
-	} else if (HAS_PCH_CPT(dev) && (IS_GEN7(dev) || port != PORT_A)) {
-		*DP &= ~DP_LINK_TRAIN_MASK_CPT;
-
-		switch (dp_train_pat & DP_TRAINING_PATTERN_MASK) {
-		case DP_TRAINING_PATTERN_DISABLE:
-			*DP |= DP_LINK_TRAIN_OFF_CPT;
-			break;
-		case DP_TRAINING_PATTERN_1:
-			*DP |= DP_LINK_TRAIN_PAT_1_CPT;
-			break;
-		case DP_TRAINING_PATTERN_2:
-			*DP |= DP_LINK_TRAIN_PAT_2_CPT;
-			break;
-		case DP_TRAINING_PATTERN_3:
-			DRM_ERROR("DP training pattern 3 not supported\n");
-			*DP |= DP_LINK_TRAIN_PAT_2_CPT;
-			break;
-		}
-
-	} else {
-		*DP &= ~DP_LINK_TRAIN_MASK;
-
-		switch (dp_train_pat & DP_TRAINING_PATTERN_MASK) {
-		case DP_TRAINING_PATTERN_DISABLE:
-			*DP |= DP_LINK_TRAIN_OFF;
-			break;
-		case DP_TRAINING_PATTERN_1:
-			*DP |= DP_LINK_TRAIN_PAT_1;
-			break;
-		case DP_TRAINING_PATTERN_2:
-			*DP |= DP_LINK_TRAIN_PAT_2;
-			break;
-		case DP_TRAINING_PATTERN_3:
-			DRM_ERROR("DP training pattern 3 not supported\n");
-			*DP |= DP_LINK_TRAIN_PAT_2;
-			break;
-		}
-	}
+}
+
+static bool
+intel_dp_set_link_train(struct intel_dp *intel_dp,
+			uint32_t *DP,
+			uint8_t dp_train_pat)
+{
+	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+	struct drm_device *dev = intel_dig_port->base.base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	uint8_t buf[sizeof(intel_dp->train_set) + 1];
+	int ret, len;
+
+	_intel_dp_set_link_train(intel_dp, DP, dp_train_pat);
 
 	I915_WRITE(intel_dp->output_reg, *DP);
 	POSTING_READ(intel_dp->output_reg);
@@ -3227,7 +3479,6 @@
 
 		/* Try 5 times, then try clock recovery if that fails */
 		if (tries > 5) {
-			intel_dp_link_down(intel_dp);
 			intel_dp_start_link_train(intel_dp);
 			intel_dp_set_link_train(intel_dp, &DP,
 						training_pattern |
@@ -3283,7 +3534,10 @@
 		DP &= ~DP_LINK_TRAIN_MASK_CPT;
 		I915_WRITE(intel_dp->output_reg, DP | DP_LINK_TRAIN_PAT_IDLE_CPT);
 	} else {
-		DP &= ~DP_LINK_TRAIN_MASK;
+		if (IS_CHERRYVIEW(dev))
+			DP &= ~DP_LINK_TRAIN_MASK_CHV;
+		else
+			DP &= ~DP_LINK_TRAIN_MASK;
 		I915_WRITE(intel_dp->output_reg, DP | DP_LINK_TRAIN_PAT_IDLE);
 	}
 	POSTING_READ(intel_dp->output_reg);
@@ -3329,15 +3583,11 @@
 	struct drm_device *dev = dig_port->base.base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	char dpcd_hex_dump[sizeof(intel_dp->dpcd) * 3];
-
 	if (intel_dp_dpcd_read_wake(&intel_dp->aux, 0x000, intel_dp->dpcd,
 				    sizeof(intel_dp->dpcd)) < 0)
 		return false; /* aux transfer failed */
 
-	hex_dump_to_buffer(intel_dp->dpcd, sizeof(intel_dp->dpcd),
-			   32, 1, dpcd_hex_dump, sizeof(dpcd_hex_dump), false);
-	DRM_DEBUG_KMS("DPCD: %s\n", dpcd_hex_dump);
+	DRM_DEBUG_KMS("DPCD: %*ph\n", (int) sizeof(intel_dp->dpcd), intel_dp->dpcd);
 
 	if (intel_dp->dpcd[DP_DPCD_REV] == 0)
 		return false; /* DPCD not present */
@@ -3357,9 +3607,9 @@
 	/* Training Pattern 3 support, both source and sink */
 	if (intel_dp->dpcd[DP_DPCD_REV] >= 0x12 &&
 	    intel_dp->dpcd[DP_MAX_LANE_COUNT] & DP_TPS3_SUPPORTED &&
-	    (IS_HASWELL(dev) || INTEL_INFO(dev)->gen >= 8)) {
+	    (IS_HASWELL(dev_priv) || INTEL_INFO(dev_priv)->gen >= 8)) {
 		intel_dp->use_tps3 = true;
-		DRM_DEBUG_KMS("Displayport TPS3 supported");
+		DRM_DEBUG_KMS("Displayport TPS3 supported\n");
 	} else
 		intel_dp->use_tps3 = false;
 
@@ -3386,8 +3636,6 @@
 	if (!(intel_dp->dpcd[DP_DOWN_STREAM_PORT_COUNT] & DP_OUI_SUPPORT))
 		return;
 
-	intel_edp_panel_vdd_on(intel_dp);
-
 	if (intel_dp_dpcd_read_wake(&intel_dp->aux, DP_SINK_OUI, buf, 3) == 3)
 		DRM_DEBUG_KMS("Sink OUI: %02hx%02hx%02hx\n",
 			      buf[0], buf[1], buf[2]);
@@ -3395,8 +3643,6 @@
 	if (intel_dp_dpcd_read_wake(&intel_dp->aux, DP_BRANCH_OUI, buf, 3) == 3)
 		DRM_DEBUG_KMS("Branch OUI: %02hx%02hx%02hx\n",
 			      buf[0], buf[1], buf[2]);
-
-	edp_panel_vdd_off(intel_dp, false);
 }
 
 static bool
@@ -3410,7 +3656,6 @@
 	if (intel_dp->dpcd[DP_DPCD_REV] < 0x12)
 		return false;
 
-	_edp_panel_vdd_on(intel_dp);
 	if (intel_dp_dpcd_read_wake(&intel_dp->aux, DP_MSTM_CAP, buf, 1)) {
 		if (buf[0] & DP_MST_CAP) {
 			DRM_DEBUG_KMS("Sink is MST capable\n");
@@ -3420,7 +3665,6 @@
 			intel_dp->is_mst = false;
 		}
 	}
-	edp_panel_vdd_off(intel_dp, false);
 
 	drm_dp_mst_topology_mgr_set_mst(&intel_dp->mst_mgr, intel_dp->is_mst);
 	return intel_dp->is_mst;
@@ -3432,26 +3676,48 @@
 	struct drm_device *dev = intel_dig_port->base.base.dev;
 	struct intel_crtc *intel_crtc =
 		to_intel_crtc(intel_dig_port->base.base.crtc);
-	u8 buf[1];
+	u8 buf;
+	int test_crc_count;
+	int attempts = 6;
 
-	if (drm_dp_dpcd_readb(&intel_dp->aux, DP_TEST_SINK_MISC, buf) < 0)
-		return -EAGAIN;
+	if (drm_dp_dpcd_readb(&intel_dp->aux, DP_TEST_SINK_MISC, &buf) < 0)
+		return -EIO;
 
-	if (!(buf[0] & DP_TEST_CRC_SUPPORTED))
+	if (!(buf & DP_TEST_CRC_SUPPORTED))
 		return -ENOTTY;
 
+	if (drm_dp_dpcd_readb(&intel_dp->aux, DP_TEST_SINK, &buf) < 0)
+		return -EIO;
+
 	if (drm_dp_dpcd_writeb(&intel_dp->aux, DP_TEST_SINK,
-			       DP_TEST_SINK_START) < 0)
-		return -EAGAIN;
+				buf | DP_TEST_SINK_START) < 0)
+		return -EIO;
 
-	/* Wait 2 vblanks to be sure we will have the correct CRC value */
-	intel_wait_for_vblank(dev, intel_crtc->pipe);
-	intel_wait_for_vblank(dev, intel_crtc->pipe);
+	if (drm_dp_dpcd_readb(&intel_dp->aux, DP_TEST_SINK_MISC, &buf) < 0)
+		return -EIO;
+	test_crc_count = buf & DP_TEST_COUNT_MASK;
+
+	do {
+		if (drm_dp_dpcd_readb(&intel_dp->aux,
+				      DP_TEST_SINK_MISC, &buf) < 0)
+			return -EIO;
+		intel_wait_for_vblank(dev, intel_crtc->pipe);
+	} while (--attempts && (buf & DP_TEST_COUNT_MASK) == test_crc_count);
+
+	if (attempts == 0) {
+		DRM_DEBUG_KMS("Panel is unable to calculate CRC after 6 vblanks\n");
+		return -ETIMEDOUT;
+	}
 
 	if (drm_dp_dpcd_read(&intel_dp->aux, DP_TEST_CRC_R_CR, crc, 6) < 0)
-		return -EAGAIN;
+		return -EIO;
+
+	if (drm_dp_dpcd_readb(&intel_dp->aux, DP_TEST_SINK, &buf) < 0)
+		return -EIO;
+	if (drm_dp_dpcd_writeb(&intel_dp->aux, DP_TEST_SINK,
+			       buf & ~DP_TEST_SINK_START) < 0)
+		return -EIO;
 
-	drm_dp_dpcd_writeb(&intel_dp->aux, DP_TEST_SINK, 0);
 	return 0;
 }
 
@@ -3652,20 +3918,24 @@
 }
 
 static enum drm_connector_status
+edp_detect(struct intel_dp *intel_dp)
+{
+	struct drm_device *dev = intel_dp_to_dev(intel_dp);
+	enum drm_connector_status status;
+
+	status = intel_panel_detect(dev);
+	if (status == connector_status_unknown)
+		status = connector_status_connected;
+
+	return status;
+}
+
+static enum drm_connector_status
 ironlake_dp_detect(struct intel_dp *intel_dp)
 {
 	struct drm_device *dev = intel_dp_to_dev(intel_dp);
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
-	enum drm_connector_status status;
-
-	/* Can't disconnect eDP, but you can close the lid... */
-	if (is_edp(intel_dp)) {
-		status = intel_panel_detect(dev);
-		if (status == connector_status_unknown)
-			status = connector_status_connected;
-		return status;
-	}
 
 	if (!ibx_digital_port_connected(dev_priv, intel_dig_port))
 		return connector_status_disconnected;
@@ -3721,16 +3991,6 @@
 	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
 	int ret;
 
-	/* Can't disconnect eDP, but you can close the lid... */
-	if (is_edp(intel_dp)) {
-		enum drm_connector_status status;
-
-		status = intel_panel_detect(dev);
-		if (status == connector_status_unknown)
-			status = connector_status_connected;
-		return status;
-	}
-
 	ret = g4x_digital_port_connected(dev, intel_dig_port);
 	if (ret == -EINVAL)
 		return connector_status_unknown;
@@ -3741,9 +4001,9 @@
 }
 
 static struct edid *
-intel_dp_get_edid(struct drm_connector *connector, struct i2c_adapter *adapter)
+intel_dp_get_edid(struct intel_dp *intel_dp)
 {
-	struct intel_connector *intel_connector = to_intel_connector(connector);
+	struct intel_connector *intel_connector = intel_dp->attached_connector;
 
 	/* use cached edid if we have one */
 	if (intel_connector->edid) {
@@ -3752,27 +4012,71 @@
 			return NULL;
 
 		return drm_edid_duplicate(intel_connector->edid);
-	}
+	} else
+		return drm_get_edid(&intel_connector->base,
+				    &intel_dp->aux.ddc);
+}
+
+static bool
+intel_dp_update_audio(struct intel_dp *intel_dp)
+{
+	struct intel_connector *intel_connector = intel_dp->attached_connector;
+	struct edid *edid = intel_connector->detect_edid ;
+	bool has_audio;
+
+	if (intel_dp->force_audio != HDMI_AUDIO_AUTO)
+		has_audio = intel_dp->force_audio == HDMI_AUDIO_ON;
+	else
+		has_audio = drm_detect_monitor_audio(edid);
+
+	if (has_audio == intel_dp->has_audio)
+		return false;
 
-	return drm_get_edid(connector, adapter);
+	intel_dp->has_audio = has_audio;
+	return true;
 }
 
-static int
-intel_dp_get_edid_modes(struct drm_connector *connector, struct i2c_adapter *adapter)
+static void
+intel_dp_set_edid(struct intel_dp *intel_dp)
 {
-	struct intel_connector *intel_connector = to_intel_connector(connector);
+	struct intel_connector *intel_connector = intel_dp->attached_connector;
+	struct edid *edid;
 
-	/* use cached edid if we have one */
-	if (intel_connector->edid) {
-		/* invalid edid */
-		if (IS_ERR(intel_connector->edid))
-			return 0;
+	edid = intel_dp_get_edid(intel_dp);
+	intel_connector->detect_edid = edid;
 
-		return intel_connector_update_modes(connector,
-						    intel_connector->edid);
-	}
+	intel_dp_update_audio(intel_dp);
+}
+
+static void
+intel_dp_unset_edid(struct intel_dp *intel_dp)
+{
+	struct intel_connector *intel_connector = intel_dp->attached_connector;
+
+	kfree(intel_connector->detect_edid);
+	intel_connector->detect_edid = NULL;
+
+	intel_dp->has_audio = false;
+}
+
+static enum intel_display_power_domain
+intel_dp_power_get(struct intel_dp *dp)
+{
+	struct intel_encoder *encoder = &dp_to_dig_port(dp)->base;
+	enum intel_display_power_domain power_domain;
 
-	return intel_ddc_get_modes(connector, adapter);
+	power_domain = intel_display_port_power_domain(encoder);
+	intel_display_power_get(to_i915(encoder->base.dev), power_domain);
+
+	return power_domain;
+}
+
+static void
+intel_dp_power_put(struct intel_dp *dp,
+		   enum intel_display_power_domain power_domain)
+{
+	struct intel_encoder *encoder = &dp_to_dig_port(dp)->base;
+	intel_display_power_put(to_i915(encoder->base.dev), power_domain);
 }
 
 static enum drm_connector_status
@@ -3782,33 +4086,30 @@
 	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
 	struct intel_encoder *intel_encoder = &intel_dig_port->base;
 	struct drm_device *dev = connector->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
 	enum drm_connector_status status;
 	enum intel_display_power_domain power_domain;
-	struct edid *edid = NULL;
 	bool ret;
 
-	power_domain = intel_display_port_power_domain(intel_encoder);
-	intel_display_power_get(dev_priv, power_domain);
-
 	DRM_DEBUG_KMS("[CONNECTOR:%d:%s]\n",
 		      connector->base.id, connector->name);
+	intel_dp_unset_edid(intel_dp);
 
 	if (intel_dp->is_mst) {
 		/* MST devices are disconnected from a monitor POV */
 		if (intel_encoder->type != INTEL_OUTPUT_EDP)
 			intel_encoder->type = INTEL_OUTPUT_DISPLAYPORT;
-		status = connector_status_disconnected;
-		goto out;
+		return connector_status_disconnected;
 	}
 
-	intel_dp->has_audio = false;
+	power_domain = intel_dp_power_get(intel_dp);
 
-	if (HAS_PCH_SPLIT(dev))
+	/* Can't disconnect eDP, but you can close the lid... */
+	if (is_edp(intel_dp))
+		status = edp_detect(intel_dp);
+	else if (HAS_PCH_SPLIT(dev))
 		status = ironlake_dp_detect(intel_dp);
 	else
 		status = g4x_dp_detect(intel_dp);
-
 	if (status != connector_status_connected)
 		goto out;
 
@@ -3824,84 +4125,67 @@
 		goto out;
 	}
 
-	if (intel_dp->force_audio != HDMI_AUDIO_AUTO) {
-		intel_dp->has_audio = (intel_dp->force_audio == HDMI_AUDIO_ON);
-	} else {
-		edid = intel_dp_get_edid(connector, &intel_dp->aux.ddc);
-		if (edid) {
-			intel_dp->has_audio = drm_detect_monitor_audio(edid);
-			kfree(edid);
-		}
-	}
+	intel_dp_set_edid(intel_dp);
 
 	if (intel_encoder->type != INTEL_OUTPUT_EDP)
 		intel_encoder->type = INTEL_OUTPUT_DISPLAYPORT;
 	status = connector_status_connected;
 
 out:
-	intel_display_power_put(dev_priv, power_domain);
+	intel_dp_power_put(intel_dp, power_domain);
 	return status;
 }
 
-static int intel_dp_get_modes(struct drm_connector *connector)
+static void
+intel_dp_force(struct drm_connector *connector)
 {
 	struct intel_dp *intel_dp = intel_attached_dp(connector);
-	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
-	struct intel_encoder *intel_encoder = &intel_dig_port->base;
-	struct intel_connector *intel_connector = to_intel_connector(connector);
-	struct drm_device *dev = connector->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_encoder *intel_encoder = &dp_to_dig_port(intel_dp)->base;
 	enum intel_display_power_domain power_domain;
-	int ret;
 
-	/* We should parse the EDID data and find out if it has an audio sink
-	 */
+	DRM_DEBUG_KMS("[CONNECTOR:%d:%s]\n",
+		      connector->base.id, connector->name);
+	intel_dp_unset_edid(intel_dp);
 
-	power_domain = intel_display_port_power_domain(intel_encoder);
-	intel_display_power_get(dev_priv, power_domain);
+	if (connector->status != connector_status_connected)
+		return;
 
-	ret = intel_dp_get_edid_modes(connector, &intel_dp->aux.ddc);
-	intel_display_power_put(dev_priv, power_domain);
-	if (ret)
-		return ret;
+	power_domain = intel_dp_power_get(intel_dp);
 
-	/* if eDP has no EDID, fall back to fixed mode */
-	if (is_edp(intel_dp) && intel_connector->panel.fixed_mode) {
-		struct drm_display_mode *mode;
-		mode = drm_mode_duplicate(dev,
-					  intel_connector->panel.fixed_mode);
-		if (mode) {
-			drm_mode_probed_add(connector, mode);
-			return 1;
-		}
-	}
-	return 0;
+	intel_dp_set_edid(intel_dp);
+
+	intel_dp_power_put(intel_dp, power_domain);
+
+	if (intel_encoder->type != INTEL_OUTPUT_EDP)
+		intel_encoder->type = INTEL_OUTPUT_DISPLAYPORT;
 }
 
-static bool
-intel_dp_detect_audio(struct drm_connector *connector)
+static int intel_dp_get_modes(struct drm_connector *connector)
 {
-	struct intel_dp *intel_dp = intel_attached_dp(connector);
-	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
-	struct intel_encoder *intel_encoder = &intel_dig_port->base;
-	struct drm_device *dev = connector->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	enum intel_display_power_domain power_domain;
+	struct intel_connector *intel_connector = to_intel_connector(connector);
 	struct edid *edid;
-	bool has_audio = false;
 
-	power_domain = intel_display_port_power_domain(intel_encoder);
-	intel_display_power_get(dev_priv, power_domain);
-
-	edid = intel_dp_get_edid(connector, &intel_dp->aux.ddc);
+	edid = intel_connector->detect_edid;
 	if (edid) {
-		has_audio = drm_detect_monitor_audio(edid);
-		kfree(edid);
+		int ret = intel_connector_update_modes(connector, edid);
+		if (ret)
+			return ret;
 	}
 
-	intel_display_power_put(dev_priv, power_domain);
+	/* if eDP has no EDID, fall back to fixed mode */
+	if (is_edp(intel_attached_dp(connector)) &&
+	    intel_connector->panel.fixed_mode) {
+		struct drm_display_mode *mode;
 
-	return has_audio;
+		mode = drm_mode_duplicate(connector->dev,
+					  intel_connector->panel.fixed_mode);
+		if (mode) {
+			drm_mode_probed_add(connector, mode);
+			return 1;
+		}
+	}
+
+	return 0;
 }
 
 static int
@@ -3921,22 +4205,14 @@
 
 	if (property == dev_priv->force_audio_property) {
 		int i = val;
-		bool has_audio;
 
 		if (i == intel_dp->force_audio)
 			return 0;
 
 		intel_dp->force_audio = i;
-
-		if (i == HDMI_AUDIO_AUTO)
-			has_audio = intel_dp_detect_audio(connector);
-		else
-			has_audio = (i == HDMI_AUDIO_ON);
-
-		if (has_audio == intel_dp->has_audio)
+		if (!intel_dp_update_audio(intel_dp))
 			return 0;
 
-		intel_dp->has_audio = has_audio;
 		goto done;
 	}
 
@@ -3997,6 +4273,8 @@
 {
 	struct intel_connector *intel_connector = to_intel_connector(connector);
 
+	kfree(intel_connector->detect_edid);
+
 	if (!IS_ERR_OR_NULL(intel_connector->edid))
 		kfree(intel_connector->edid);
 
@@ -4013,16 +4291,20 @@
 {
 	struct intel_digital_port *intel_dig_port = enc_to_dig_port(encoder);
 	struct intel_dp *intel_dp = &intel_dig_port->dp;
-	struct drm_device *dev = intel_dp_to_dev(intel_dp);
 
 	drm_dp_aux_unregister(&intel_dp->aux);
 	intel_dp_mst_encoder_cleanup(intel_dig_port);
 	drm_encoder_cleanup(encoder);
 	if (is_edp(intel_dp)) {
 		cancel_delayed_work_sync(&intel_dp->panel_vdd_work);
-		drm_modeset_lock(&dev->mode_config.connection_mutex, NULL);
+		/*
+		 * vdd might still be enabled do to the delayed vdd off.
+		 * Make sure vdd is actually turned off here.
+		 */
+		pps_lock(intel_dp);
 		edp_panel_vdd_off_sync(intel_dp);
-		drm_modeset_unlock(&dev->mode_config.connection_mutex);
+		pps_unlock(intel_dp);
+
 		if (intel_dp->edp_notifier.notifier_call) {
 			unregister_reboot_notifier(&intel_dp->edp_notifier);
 			intel_dp->edp_notifier.notifier_call = NULL;
@@ -4038,17 +4320,67 @@
 	if (!is_edp(intel_dp))
 		return;
 
+	/*
+	 * vdd might still be enabled do to the delayed vdd off.
+	 * Make sure vdd is actually turned off here.
+	 */
+	pps_lock(intel_dp);
 	edp_panel_vdd_off_sync(intel_dp);
+	pps_unlock(intel_dp);
+}
+
+static void intel_edp_panel_vdd_sanitize(struct intel_dp *intel_dp)
+{
+	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+	struct drm_device *dev = intel_dig_port->base.base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	enum intel_display_power_domain power_domain;
+
+	lockdep_assert_held(&dev_priv->pps_mutex);
+
+	if (!edp_have_panel_vdd(intel_dp))
+		return;
+
+	/*
+	 * The VDD bit needs a power domain reference, so if the bit is
+	 * already enabled when we boot or resume, grab this reference and
+	 * schedule a vdd off, so we don't hold on to the reference
+	 * indefinitely.
+	 */
+	DRM_DEBUG_KMS("VDD left on by BIOS, adjusting state tracking\n");
+	power_domain = intel_display_port_power_domain(&intel_dig_port->base);
+	intel_display_power_get(dev_priv, power_domain);
+
+	edp_panel_vdd_schedule_off(intel_dp);
 }
 
 static void intel_dp_encoder_reset(struct drm_encoder *encoder)
 {
-	intel_edp_panel_vdd_sanitize(to_intel_encoder(encoder));
+	struct intel_dp *intel_dp;
+
+	if (to_intel_encoder(encoder)->type != INTEL_OUTPUT_EDP)
+		return;
+
+	intel_dp = enc_to_intel_dp(encoder);
+
+	pps_lock(intel_dp);
+
+	/*
+	 * Read out the current power sequencer assignment,
+	 * in case the BIOS did something with it.
+	 */
+	if (IS_VALLEYVIEW(encoder->dev))
+		vlv_initial_power_sequencer_setup(intel_dp);
+
+	intel_edp_panel_vdd_sanitize(intel_dp);
+
+	pps_unlock(intel_dp);
 }
 
 static const struct drm_connector_funcs intel_dp_connector_funcs = {
 	.dpms = intel_connector_dpms,
 	.detect = intel_dp_detect,
+	.force = intel_dp_force,
 	.fill_modes = drm_helper_probe_single_connector_modes,
 	.set_property = intel_dp_set_property,
 	.destroy = intel_dp_connector_destroy,
@@ -4084,7 +4416,20 @@
 	if (intel_dig_port->base.type != INTEL_OUTPUT_EDP)
 		intel_dig_port->base.type = INTEL_OUTPUT_DISPLAYPORT;
 
-	DRM_DEBUG_KMS("got hpd irq on port %d - %s\n", intel_dig_port->port,
+	if (long_hpd && intel_dig_port->base.type == INTEL_OUTPUT_EDP) {
+		/*
+		 * vdd off can generate a long pulse on eDP which
+		 * would require vdd on to handle it, and thus we
+		 * would end up in an endless cycle of
+		 * "vdd off -> long hpd -> vdd on -> detect -> vdd off -> ..."
+		 */
+		DRM_DEBUG_KMS("ignoring long hpd on eDP port %c\n",
+			      port_name(intel_dig_port->port));
+		return false;
+	}
+
+	DRM_DEBUG_KMS("got hpd irq on port %c - %s\n",
+		      port_name(intel_dig_port->port),
 		      long_hpd ? "long" : "short");
 
 	power_domain = intel_display_port_power_domain(intel_encoder);
@@ -4216,14 +4561,20 @@
 
 static void
 intel_dp_init_panel_power_sequencer(struct drm_device *dev,
-				    struct intel_dp *intel_dp,
-				    struct edp_power_seq *out)
+				    struct intel_dp *intel_dp)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct edp_power_seq cur, vbt, spec, final;
+	struct edp_power_seq cur, vbt, spec,
+		*final = &intel_dp->pps_delays;
 	u32 pp_on, pp_off, pp_div, pp;
 	int pp_ctrl_reg, pp_on_reg, pp_off_reg, pp_div_reg;
 
+	lockdep_assert_held(&dev_priv->pps_mutex);
+
+	/* already initialized? */
+	if (final->t11_t12 != 0)
+		return;
+
 	if (HAS_PCH_SPLIT(dev)) {
 		pp_ctrl_reg = PCH_PP_CONTROL;
 		pp_on_reg = PCH_PP_ON_DELAYS;
@@ -4285,7 +4636,7 @@
 
 	/* Use the max of the register settings and vbt. If both are
 	 * unset, fall back to the spec limits. */
-#define assign_final(field)	final.field = (max(cur.field, vbt.field) == 0 ? \
+#define assign_final(field)	final->field = (max(cur.field, vbt.field) == 0 ? \
 				       spec.field : \
 				       max(cur.field, vbt.field))
 	assign_final(t1_t3);
@@ -4295,7 +4646,7 @@
 	assign_final(t11_t12);
 #undef assign_final
 
-#define get_delay(field)	(DIV_ROUND_UP(final.field, 10))
+#define get_delay(field)	(DIV_ROUND_UP(final->field, 10))
 	intel_dp->panel_power_up_delay = get_delay(t1_t3);
 	intel_dp->backlight_on_delay = get_delay(t8);
 	intel_dp->backlight_off_delay = get_delay(t9);
@@ -4309,20 +4660,20 @@
 
 	DRM_DEBUG_KMS("backlight on delay %d, off delay %d\n",
 		      intel_dp->backlight_on_delay, intel_dp->backlight_off_delay);
-
-	if (out)
-		*out = final;
 }
 
 static void
 intel_dp_init_panel_power_sequencer_registers(struct drm_device *dev,
-					      struct intel_dp *intel_dp,
-					      struct edp_power_seq *seq)
+					      struct intel_dp *intel_dp)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	u32 pp_on, pp_off, pp_div, port_sel = 0;
 	int div = HAS_PCH_SPLIT(dev) ? intel_pch_rawclk(dev) : intel_hrawclk(dev);
 	int pp_on_reg, pp_off_reg, pp_div_reg;
+	enum port port = dp_to_dig_port(intel_dp)->port;
+	const struct edp_power_seq *seq = &intel_dp->pps_delays;
+
+	lockdep_assert_held(&dev_priv->pps_mutex);
 
 	if (HAS_PCH_SPLIT(dev)) {
 		pp_on_reg = PCH_PP_ON_DELAYS;
@@ -4357,12 +4708,9 @@
 	/* Haswell doesn't have any port selection bits for the panel
 	 * power sequencer any more. */
 	if (IS_VALLEYVIEW(dev)) {
-		if (dp_to_dig_port(intel_dp)->port == PORT_B)
-			port_sel = PANEL_PORT_SELECT_DPB_VLV;
-		else
-			port_sel = PANEL_PORT_SELECT_DPC_VLV;
+		port_sel = PANEL_PORT_SELECT_VLV(port);
 	} else if (HAS_PCH_IBX(dev) || HAS_PCH_CPT(dev)) {
-		if (dp_to_dig_port(intel_dp)->port == PORT_A)
+		if (port == PORT_A)
 			port_sel = PANEL_PORT_SELECT_DPA;
 		else
 			port_sel = PANEL_PORT_SELECT_DPD;
@@ -4406,7 +4754,7 @@
 	 * hard to tell without seeing the user of this function of this code.
 	 * Check locking and ordering once that lands.
 	 */
-	if (INTEL_INFO(dev)->gen < 8 && intel_edp_is_psr_enabled(dev)) {
+	if (INTEL_INFO(dev)->gen < 8 && intel_psr_is_enabled(dev)) {
 		DRM_DEBUG_KMS("DRRS is disabled as PSR is enabled\n");
 		return;
 	}
@@ -4446,7 +4794,7 @@
 		val = I915_READ(reg);
 		if (index > DRRS_HIGH_RR) {
 			val |= PIPECONF_EDP_RR_MODE_SWITCH;
-			intel_dp_set_m2_n2(intel_crtc, &config->dp_m2_n2);
+			intel_dp_set_m_n(intel_crtc);
 		} else {
 			val &= ~PIPECONF_EDP_RR_MODE_SWITCH;
 		}
@@ -4486,7 +4834,7 @@
 	}
 
 	if (dev_priv->vbt.drrs_type != SEAMLESS_DRRS_SUPPORT) {
-		DRM_INFO("VBT doesn't support DRRS\n");
+		DRM_DEBUG_KMS("VBT doesn't support DRRS\n");
 		return NULL;
 	}
 
@@ -4494,7 +4842,7 @@
 					(dev, fixed_mode, connector);
 
 	if (!downclock_mode) {
-		DRM_INFO("DRRS not supported\n");
+		DRM_DEBUG_KMS("DRRS not supported\n");
 		return NULL;
 	}
 
@@ -4505,39 +4853,12 @@
 	intel_dp->drrs_state.type = dev_priv->vbt.drrs_type;
 
 	intel_dp->drrs_state.refresh_rate_type = DRRS_HIGH_RR;
-	DRM_INFO("seamless DRRS supported for eDP panel.\n");
+	DRM_DEBUG_KMS("seamless DRRS supported for eDP panel.\n");
 	return downclock_mode;
 }
 
-void intel_edp_panel_vdd_sanitize(struct intel_encoder *intel_encoder)
-{
-	struct drm_device *dev = intel_encoder->base.dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_dp *intel_dp;
-	enum intel_display_power_domain power_domain;
-
-	if (intel_encoder->type != INTEL_OUTPUT_EDP)
-		return;
-
-	intel_dp = enc_to_intel_dp(&intel_encoder->base);
-	if (!edp_have_panel_vdd(intel_dp))
-		return;
-	/*
-	 * The VDD bit needs a power domain reference, so if the bit is
-	 * already enabled when we boot or resume, grab this reference and
-	 * schedule a vdd off, so we don't hold on to the reference
-	 * indefinitely.
-	 */
-	DRM_DEBUG_KMS("VDD left on by BIOS, adjusting state tracking\n");
-	power_domain = intel_display_port_power_domain(intel_encoder);
-	intel_display_power_get(dev_priv, power_domain);
-
-	edp_panel_vdd_schedule_off(intel_dp);
-}
-
 static bool intel_edp_init_connector(struct intel_dp *intel_dp,
-				     struct intel_connector *intel_connector,
-				     struct edp_power_seq *power_seq)
+				     struct intel_connector *intel_connector)
 {
 	struct drm_connector *connector = &intel_connector->base;
 	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
@@ -4549,18 +4870,19 @@
 	bool has_dpcd;
 	struct drm_display_mode *scan;
 	struct edid *edid;
+	enum pipe pipe = INVALID_PIPE;
 
 	intel_dp->drrs_state.type = DRRS_NOT_SUPPORTED;
 
 	if (!is_edp(intel_dp))
 		return true;
 
-	intel_edp_panel_vdd_sanitize(intel_encoder);
+	pps_lock(intel_dp);
+	intel_edp_panel_vdd_sanitize(intel_dp);
+	pps_unlock(intel_dp);
 
 	/* Cache DPCD and EDID for edp. */
-	intel_edp_panel_vdd_on(intel_dp);
 	has_dpcd = intel_dp_get_dpcd(intel_dp);
-	edp_panel_vdd_off(intel_dp, false);
 
 	if (has_dpcd) {
 		if (intel_dp->dpcd[DP_DPCD_REV] >= 0x11)
@@ -4574,7 +4896,9 @@
 	}
 
 	/* We now know it's not a ghost, init power sequence regs. */
-	intel_dp_init_panel_power_sequencer_registers(dev, intel_dp, power_seq);
+	pps_lock(intel_dp);
+	intel_dp_init_panel_power_sequencer_registers(dev, intel_dp);
+	pps_unlock(intel_dp);
 
 	mutex_lock(&dev->mode_config.mutex);
 	edid = drm_get_edid(connector, &intel_dp->aux.ddc);
@@ -4615,10 +4939,30 @@
 	if (IS_VALLEYVIEW(dev)) {
 		intel_dp->edp_notifier.notifier_call = edp_notify_handler;
 		register_reboot_notifier(&intel_dp->edp_notifier);
+
+		/*
+		 * Figure out the current pipe for the initial backlight setup.
+		 * If the current pipe isn't valid, try the PPS pipe, and if that
+		 * fails just assume pipe A.
+		 */
+		if (IS_CHERRYVIEW(dev))
+			pipe = DP_PORT_TO_PIPE_CHV(intel_dp->DP);
+		else
+			pipe = PORT_TO_PIPE(intel_dp->DP);
+
+		if (pipe != PIPE_A && pipe != PIPE_B)
+			pipe = intel_dp->pps_pipe;
+
+		if (pipe != PIPE_A && pipe != PIPE_B)
+			pipe = PIPE_A;
+
+		DRM_DEBUG_KMS("using pipe %c for initial backlight setup\n",
+			      pipe_name(pipe));
 	}
 
 	intel_panel_init(&intel_connector->panel, fixed_mode, downclock_mode);
-	intel_panel_setup_backlight(connector);
+	intel_connector->panel.backlight_power = intel_edp_backlight_power;
+	intel_panel_setup_backlight(connector, pipe);
 
 	return true;
 }
@@ -4633,11 +4977,14 @@
 	struct drm_device *dev = intel_encoder->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	enum port port = intel_dig_port->port;
-	struct edp_power_seq power_seq = { 0 };
 	int type;
 
+	intel_dp->pps_pipe = INVALID_PIPE;
+
 	/* intel_dp vfuncs */
-	if (IS_VALLEYVIEW(dev))
+	if (INTEL_INFO(dev)->gen >= 9)
+		intel_dp->get_aux_clock_divider = skl_get_aux_clock_divider;
+	else if (IS_VALLEYVIEW(dev))
 		intel_dp->get_aux_clock_divider = vlv_get_aux_clock_divider;
 	else if (IS_HASWELL(dev) || IS_BROADWELL(dev))
 		intel_dp->get_aux_clock_divider = hsw_get_aux_clock_divider;
@@ -4646,7 +4993,10 @@
 	else
 		intel_dp->get_aux_clock_divider = i9xx_get_aux_clock_divider;
 
-	intel_dp->get_aux_send_ctl = i9xx_get_aux_send_ctl;
+	if (INTEL_INFO(dev)->gen >= 9)
+		intel_dp->get_aux_send_ctl = skl_get_aux_send_ctl;
+	else
+		intel_dp->get_aux_send_ctl = i9xx_get_aux_send_ctl;
 
 	/* Preserve the current hw state. */
 	intel_dp->DP = I915_READ(intel_dp->output_reg);
@@ -4665,6 +5015,11 @@
 	if (type == DRM_MODE_CONNECTOR_eDP)
 		intel_encoder->type = INTEL_OUTPUT_EDP;
 
+	/* eDP only on port B and/or C on vlv/chv */
+	if (WARN_ON(IS_VALLEYVIEW(dev) && is_edp(intel_dp) &&
+		    port != PORT_B && port != PORT_C))
+		return false;
+
 	DRM_DEBUG_KMS("Adding %s connector on port %c\n",
 			type == DRM_MODE_CONNECTOR_eDP ? "eDP" : "DP",
 			port_name(port));
@@ -4706,8 +5061,13 @@
 	}
 
 	if (is_edp(intel_dp)) {
+		pps_lock(intel_dp);
 		intel_dp_init_panel_power_timestamps(intel_dp);
-		intel_dp_init_panel_power_sequencer(dev, intel_dp, &power_seq);
+		if (IS_VALLEYVIEW(dev))
+			vlv_initial_power_sequencer_setup(intel_dp);
+		else
+			intel_dp_init_panel_power_sequencer(dev, intel_dp);
+		pps_unlock(intel_dp);
 	}
 
 	intel_dp_aux_init(intel_dp, intel_connector);
@@ -4715,17 +5075,22 @@
 	/* init MST on ports that can support it */
 	if (IS_HASWELL(dev) || IS_BROADWELL(dev)) {
 		if (port == PORT_B || port == PORT_C || port == PORT_D) {
-			intel_dp_mst_encoder_init(intel_dig_port, intel_connector->base.base.id);
+			intel_dp_mst_encoder_init(intel_dig_port,
+						  intel_connector->base.base.id);
 		}
 	}
 
-	if (!intel_edp_init_connector(intel_dp, intel_connector, &power_seq)) {
+	if (!intel_edp_init_connector(intel_dp, intel_connector)) {
 		drm_dp_aux_unregister(&intel_dp->aux);
 		if (is_edp(intel_dp)) {
 			cancel_delayed_work_sync(&intel_dp->panel_vdd_work);
-			drm_modeset_lock(&dev->mode_config.connection_mutex, NULL);
+			/*
+			 * vdd might still be enabled do to the delayed vdd off.
+			 * Make sure vdd is actually turned off here.
+			 */
+			pps_lock(intel_dp);
 			edp_panel_vdd_off_sync(intel_dp);
-			drm_modeset_unlock(&dev->mode_config.connection_mutex);
+			pps_unlock(intel_dp);
 		}
 		drm_connector_unregister(connector);
 		drm_connector_cleanup(connector);
@@ -4789,7 +5154,8 @@
 	} else {
 		intel_encoder->pre_enable = g4x_pre_enable_dp;
 		intel_encoder->enable = g4x_enable_dp;
-		intel_encoder->post_disable = g4x_post_disable_dp;
+		if (INTEL_INFO(dev)->gen >= 5)
+			intel_encoder->post_disable = ilk_post_disable_dp;
 	}
 
 	intel_dig_port->port = port;
diff -urN a/drivers/gpu/drm/i915/intel_dp_mst.c b/drivers/gpu/drm/i915/intel_dp_mst.c
--- a/drivers/gpu/drm/i915/intel_dp_mst.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_dp_mst.c	2014-11-22 14:37:49.338700418 -0700
@@ -278,7 +278,7 @@
 }
 
 static enum drm_connector_status
-intel_mst_port_dp_detect(struct drm_connector *connector)
+intel_dp_mst_detect(struct drm_connector *connector, bool force)
 {
 	struct intel_connector *intel_connector = to_intel_connector(connector);
 	struct intel_dp *intel_dp = intel_connector->mst_port;
@@ -286,14 +286,6 @@
 	return drm_dp_mst_detect_port(&intel_dp->mst_mgr, intel_connector->port);
 }
 
-static enum drm_connector_status
-intel_dp_mst_detect(struct drm_connector *connector, bool force)
-{
-	enum drm_connector_status status;
-	status = intel_mst_port_dp_detect(connector);
-	return status;
-}
-
 static int
 intel_dp_mst_set_property(struct drm_connector *connector,
 			  struct drm_property *property,
@@ -393,7 +385,7 @@
 #endif
 }
 
-static struct drm_connector *intel_dp_add_mst_connector(struct drm_dp_mst_topology_mgr *mgr, struct drm_dp_mst_port *port, char *pathprop)
+static struct drm_connector *intel_dp_add_mst_connector(struct drm_dp_mst_topology_mgr *mgr, struct drm_dp_mst_port *port, const char *pathprop)
 {
 	struct intel_dp *intel_dp = container_of(mgr, struct intel_dp, mst_mgr);
 	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
@@ -436,6 +428,7 @@
 {
 	struct intel_connector *intel_connector = to_intel_connector(connector);
 	struct drm_device *dev = connector->dev;
+
 	/* need to nuke the connector */
 	mutex_lock(&dev->mode_config.mutex);
 	intel_connector_dpms(connector, DRM_MODE_DPMS_OFF);
diff -urN a/drivers/gpu/drm/i915/intel_drv.h b/drivers/gpu/drm/i915/intel_drv.h
--- a/drivers/gpu/drm/i915/intel_drv.h	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_drv.h	2014-11-22 14:37:49.342700417 -0700
@@ -25,6 +25,7 @@
 #ifndef __INTEL_DRV_H__
 #define __INTEL_DRV_H__
 
+#include <linux/async.h>
 #include <linux/i2c.h>
 #include <linux/hdmi.h>
 #include <drm/i915_drm.h>
@@ -33,6 +34,10 @@
 #include <drm/drm_crtc_helper.h>
 #include <drm/drm_fb_helper.h>
 #include <drm/drm_dp_mst_helper.h>
+#include <drm/drm_rect.h>
+
+#define DIV_ROUND_CLOSEST_ULL(ll, d)	\
+({ unsigned long long _tmp = (ll)+(d)/2; do_div(_tmp, d); _tmp; })
 
 /**
  * _wait_for - magic (register) wait macro
@@ -89,18 +94,20 @@
 
 /* these are outputs from the chip - integrated only
    external chips are via DVO or SDVO output */
-#define INTEL_OUTPUT_UNUSED 0
-#define INTEL_OUTPUT_ANALOG 1
-#define INTEL_OUTPUT_DVO 2
-#define INTEL_OUTPUT_SDVO 3
-#define INTEL_OUTPUT_LVDS 4
-#define INTEL_OUTPUT_TVOUT 5
-#define INTEL_OUTPUT_HDMI 6
-#define INTEL_OUTPUT_DISPLAYPORT 7
-#define INTEL_OUTPUT_EDP 8
-#define INTEL_OUTPUT_DSI 9
-#define INTEL_OUTPUT_UNKNOWN 10
-#define INTEL_OUTPUT_DP_MST 11
+enum intel_output_type {
+	INTEL_OUTPUT_UNUSED = 0,
+	INTEL_OUTPUT_ANALOG = 1,
+	INTEL_OUTPUT_DVO = 2,
+	INTEL_OUTPUT_SDVO = 3,
+	INTEL_OUTPUT_LVDS = 4,
+	INTEL_OUTPUT_TVOUT = 5,
+	INTEL_OUTPUT_HDMI = 6,
+	INTEL_OUTPUT_DISPLAYPORT = 7,
+	INTEL_OUTPUT_EDP = 8,
+	INTEL_OUTPUT_DSI = 9,
+	INTEL_OUTPUT_UNKNOWN = 10,
+	INTEL_OUTPUT_DP_MST = 11,
+};
 
 #define INTEL_DVO_CHIP_NONE 0
 #define INTEL_DVO_CHIP_LVDS 1
@@ -131,7 +138,7 @@
 	 */
 	struct intel_crtc *new_crtc;
 
-	int type;
+	enum intel_output_type type;
 	unsigned int cloneable;
 	bool connectors_active;
 	void (*hot_plug)(struct intel_encoder *);
@@ -179,6 +186,8 @@
 		bool active_low_pwm;
 		struct backlight_device *device;
 	} backlight;
+
+	void (*backlight_power)(struct intel_connector *, bool enable);
 };
 
 struct intel_connector {
@@ -211,6 +220,7 @@
 
 	/* Cached EDID for eDP and LVDS. May hold ERR_PTR for invalid EDID. */
 	struct edid *edid;
+	struct edid *detect_edid;
 
 	/* since POLL and HPD connectors may use the same HPD line keep the native
 	   state of connector->polled in case hotplug storm detection changes it */
@@ -233,6 +243,17 @@
 	int	p;
 } intel_clock_t;
 
+struct intel_plane_state {
+	struct drm_crtc *crtc;
+	struct drm_framebuffer *fb;
+	struct drm_rect src;
+	struct drm_rect dst;
+	struct drm_rect clip;
+	struct drm_rect orig_src;
+	struct drm_rect orig_dst;
+	bool visible;
+};
+
 struct intel_plane_config {
 	bool tiled;
 	int size;
@@ -271,6 +292,9 @@
 	 * between pch encoders and cpu encoders. */
 	bool has_pch_encoder;
 
+	/* Are we sending infoframes on the attached port */
+	bool has_infoframe;
+
 	/* CPU Transcoder for the pipe. Currently this can only differ from the
 	 * pipe on Haswell (where we have a special eDP transcoder). */
 	enum transcoder cpu_transcoder;
@@ -319,7 +343,10 @@
 	/* Selected dpll when shared or DPLL_ID_PRIVATE. */
 	enum intel_dpll_id shared_dpll;
 
-	/* PORT_CLK_SEL for DDI ports. */
+	/*
+	 * - PORT_CLK_SEL for DDI ports on HSW/BDW.
+	 * - enum skl_dpll on SKL
+	 */
 	uint32_t ddi_pll_sel;
 
 	/* Actual register state of the dpll, for shared dpll cross-checking. */
@@ -330,6 +357,7 @@
 
 	/* m2_n2 for eDP downclock */
 	struct intel_link_m_n dp_m2_n2;
+	bool has_drrs;
 
 	/*
 	 * Frequence the dpll for the port should run at. Differs from the
@@ -377,9 +405,10 @@
 	bool sprites_scaled;
 };
 
-struct intel_mmio_flip {
-	u32 seqno;
-	u32 ring_id;
+struct skl_pipe_wm {
+	struct skl_wm_level wm[8];
+	struct skl_wm_level trans_wm;
+	uint32_t linetime;
 };
 
 struct intel_crtc {
@@ -410,6 +439,7 @@
 	uint32_t cursor_addr;
 	int16_t cursor_width, cursor_height;
 	uint32_t cursor_cntl;
+	uint32_t cursor_size;
 	uint32_t cursor_base;
 
 	struct intel_plane_config plane_config;
@@ -428,12 +458,12 @@
 	struct {
 		/* watermarks currently being used  */
 		struct intel_pipe_wm active;
+		/* SKL wm values currently in use */
+		struct skl_pipe_wm skl_active;
 	} wm;
 
-	wait_queue_head_t vbl_wait;
-
 	int scanline_offset;
-	struct intel_mmio_flip mmio_flip;
+	struct i915_gem_request *mmio_flip;
 };
 
 struct intel_plane_wm_parameters {
@@ -455,6 +485,7 @@
 	unsigned int crtc_w, crtc_h;
 	uint32_t src_x, src_y;
 	uint32_t src_w, src_h;
+	unsigned int rotation;
 
 	/* Since we need to change the watermarks before/after
 	 * enabling/disabling the planes, we need to store the parameters here
@@ -521,6 +552,7 @@
 	void (*set_infoframes)(struct drm_encoder *encoder,
 			       bool enable,
 			       struct drm_display_mode *adjusted_mode);
+	bool (*infoframe_enabled)(struct drm_encoder *encoder);
 };
 
 struct intel_dp_mst_encoder;
@@ -565,6 +597,13 @@
 
 	struct notifier_block edp_notifier;
 
+	/*
+	 * Pipe whose power sequencer is currently locked into
+	 * this port. Only relevant on VLV/CHV.
+	 */
+	enum pipe pps_pipe;
+	struct edp_power_seq pps_delays;
+
 	bool use_tps3;
 	bool can_mst; /* this port supports mst */
 	bool is_mst;
@@ -663,7 +702,12 @@
 #define INTEL_FLIP_COMPLETE	2
 	u32 flip_count;
 	u32 gtt_offset;
-	bool enable_stall_check;
+	struct i915_gem_request *flip_queued_request;
+	int flip_queued_vblank;
+	int flip_ready_vblank;
+	bool enable_stall_check:1;
+	bool rcs_active:1;
+	bool async:1;
 };
 
 struct intel_set_config {
@@ -716,32 +760,47 @@
 	return container_of(intel_hdmi, struct intel_digital_port, hdmi);
 }
 
+/*
+ * Returns the number of planes for this pipe, ie the number of sprites + 1
+ * (primary plane). This doesn't count the cursor plane then.
+ */
+static inline unsigned int intel_num_planes(struct intel_crtc *crtc)
+{
+	return INTEL_INFO(crtc->base.dev)->num_sprites[crtc->pipe] + 1;
+}
 
-/* i915_irq.c */
-bool intel_set_cpu_fifo_underrun_reporting(struct drm_device *dev,
+/* intel_fifo_underrun.c */
+bool intel_set_cpu_fifo_underrun_reporting(struct drm_i915_private *dev_priv,
 					   enum pipe pipe, bool enable);
-bool intel_set_pch_fifo_underrun_reporting(struct drm_device *dev,
+bool intel_set_pch_fifo_underrun_reporting(struct drm_i915_private *dev_priv,
 					   enum transcoder pch_transcoder,
 					   bool enable);
+void intel_cpu_fifo_underrun_irq_handler(struct drm_i915_private *dev_priv,
+					 enum pipe pipe);
+void intel_pch_fifo_underrun_irq_handler(struct drm_i915_private *dev_priv,
+					 enum transcoder pch_transcoder);
+void i9xx_check_fifo_underruns(struct drm_i915_private *dev_priv);
+
+/* i915_irq.c */
 void gen5_enable_gt_irq(struct drm_i915_private *dev_priv, uint32_t mask);
 void gen5_disable_gt_irq(struct drm_i915_private *dev_priv, uint32_t mask);
 void gen6_enable_pm_irq(struct drm_i915_private *dev_priv, uint32_t mask);
 void gen6_disable_pm_irq(struct drm_i915_private *dev_priv, uint32_t mask);
-void gen8_enable_pm_irq(struct drm_i915_private *dev_priv, uint32_t mask);
-void gen8_disable_pm_irq(struct drm_i915_private *dev_priv, uint32_t mask);
-void intel_runtime_pm_disable_interrupts(struct drm_device *dev);
-void intel_runtime_pm_restore_interrupts(struct drm_device *dev);
+void gen6_reset_rps_interrupts(struct drm_device *dev);
+void gen6_enable_rps_interrupts(struct drm_device *dev);
+void gen6_disable_rps_interrupts(struct drm_device *dev);
+void intel_runtime_pm_disable_interrupts(struct drm_i915_private *dev_priv);
+void intel_runtime_pm_enable_interrupts(struct drm_i915_private *dev_priv);
 static inline bool intel_irqs_enabled(struct drm_i915_private *dev_priv)
 {
 	/*
 	 * We only use drm_irq_uninstall() at unload and VT switch, so
 	 * this is the only thing we need to check.
 	 */
-	return !dev_priv->pm._irqs_disabled;
+	return dev_priv->pm.irqs_enabled;
 }
 
 int intel_get_crtc_scanline(struct intel_crtc *crtc);
-void i9xx_check_fifo_underruns(struct drm_device *dev);
 void gen8_irq_power_well_post_enable(struct drm_i915_private *dev_priv);
 
 /* intel_crt.c */
@@ -774,13 +833,9 @@
 			 struct intel_crtc_config *pipe_config);
 void intel_ddi_set_vc_payload_alloc(struct drm_crtc *crtc, bool state);
 
-/* intel_display.c */
-const char *intel_output_name(int output);
-bool intel_has_pending_fb_unpin(struct drm_device *dev);
-int intel_pch_rawclk(struct drm_device *dev);
-void intel_mark_busy(struct drm_device *dev);
+/* intel_frontbuffer.c */
 void intel_fb_obj_invalidate(struct drm_i915_gem_object *obj,
-			     struct intel_engine_cs *ring);
+			     struct i915_gem_request *rq);
 void intel_frontbuffer_flip_prepare(struct drm_device *dev,
 				    unsigned frontbuffer_bits);
 void intel_frontbuffer_flip_complete(struct drm_device *dev,
@@ -788,7 +843,7 @@
 void intel_frontbuffer_flush(struct drm_device *dev,
 			     unsigned frontbuffer_bits);
 /**
- * intel_frontbuffer_flip - prepare frontbuffer flip
+ * intel_frontbuffer_flip - synchronous frontbuffer flip
  * @dev: DRM device
  * @frontbuffer_bits: frontbuffer plane tracking bits
  *
@@ -806,6 +861,18 @@
 }
 
 void intel_fb_obj_flush(struct drm_i915_gem_object *obj, bool retire);
+
+
+/* intel_audio.c */
+void intel_init_audio(struct drm_device *dev);
+void intel_audio_codec_enable(struct intel_encoder *encoder);
+void intel_audio_codec_disable(struct intel_encoder *encoder);
+
+/* intel_display.c */
+const char *intel_output_name(int output);
+bool intel_has_pending_fb_unpin(struct drm_device *dev);
+int intel_pch_rawclk(struct drm_device *dev);
+void intel_mark_busy(struct drm_device *dev);
 void intel_mark_idle(struct drm_device *dev);
 void intel_crtc_restore_mode(struct drm_crtc *crtc);
 void intel_crtc_control(struct drm_crtc *crtc, bool enable);
@@ -826,8 +893,12 @@
 				struct drm_file *file_priv);
 enum transcoder intel_pipe_to_cpu_transcoder(struct drm_i915_private *dev_priv,
 					     enum pipe pipe);
-void intel_wait_for_vblank(struct drm_device *dev, int pipe);
-void intel_wait_for_pipe_off(struct drm_device *dev, int pipe);
+bool intel_pipe_has_type(struct intel_crtc *crtc, enum intel_output_type type);
+static inline void
+intel_wait_for_vblank(struct drm_device *dev, int pipe)
+{
+	drm_wait_one_vblank(dev, pipe);
+}
 int ironlake_get_lanes_required(int target_clock, int link_bw, int bpp);
 void vlv_wait_port_ready(struct drm_i915_private *dev_priv,
 			 struct intel_digital_port *dport);
@@ -837,9 +908,9 @@
 				struct drm_modeset_acquire_ctx *ctx);
 void intel_release_load_detect_pipe(struct drm_connector *connector,
 				    struct intel_load_detect_pipe *old);
-int intel_pin_and_fence_fb_obj(struct drm_device *dev,
-			       struct drm_i915_gem_object *obj,
-			       struct intel_engine_cs *pipelined);
+int intel_pin_and_fence_fb_obj(struct drm_plane *plane,
+			       struct drm_framebuffer *fb,
+			       struct i915_gem_request *pipelined);
 void intel_unpin_fb_obj(struct drm_i915_gem_object *obj);
 struct drm_framebuffer *
 __intel_framebuffer_create(struct drm_device *dev,
@@ -848,6 +919,7 @@
 void intel_prepare_page_flip(struct drm_device *dev, int plane);
 void intel_finish_page_flip(struct drm_device *dev, int pipe);
 void intel_finish_page_flip_plane(struct drm_device *dev, int plane);
+void intel_check_page_flip(struct drm_device *dev, int pipe);
 
 /* shared dpll functions */
 struct intel_shared_dpll *intel_crtc_to_shared_dpll(struct intel_crtc *crtc);
@@ -859,7 +931,13 @@
 struct intel_shared_dpll *intel_get_shared_dpll(struct intel_crtc *crtc);
 void intel_put_shared_dpll(struct intel_crtc *crtc);
 
+void vlv_force_pll_on(struct drm_device *dev, enum pipe pipe,
+		      const struct dpll *dpll);
+void vlv_force_pll_off(struct drm_device *dev, enum pipe pipe);
+
 /* modesetting asserts */
+void assert_panel_unlocked(struct drm_i915_private *dev_priv,
+			   enum pipe pipe);
 void assert_pll(struct drm_i915_private *dev_priv,
 		enum pipe pipe, bool state);
 #define assert_pll_enabled(d, p) assert_pll(d, p, true)
@@ -868,11 +946,9 @@
 		       enum pipe pipe, bool state);
 #define assert_fdi_rx_pll_enabled(d, p) assert_fdi_rx_pll(d, p, true)
 #define assert_fdi_rx_pll_disabled(d, p) assert_fdi_rx_pll(d, p, false)
-void assert_pipe(struct drm_i915_private *dev_priv, enum pipe pipe, bool state);
+bool assert_pipe(struct drm_i915_private *dev_priv, enum pipe pipe, bool state);
 #define assert_pipe_enabled(d, p) assert_pipe(d, p, true)
 #define assert_pipe_disabled(d, p) assert_pipe(d, p, false)
-void intel_write_eld(struct drm_encoder *encoder,
-		     struct drm_display_mode *mode);
 unsigned long intel_gen4_compute_page_offset(int *x, int *y,
 					     unsigned int tiling_mode,
 					     unsigned int bpp,
@@ -882,6 +958,7 @@
 void hsw_disable_pc8(struct drm_i915_private *dev_priv);
 void intel_dp_get_m_n(struct intel_crtc *crtc,
 		      struct intel_crtc_config *pipe_config);
+void intel_dp_set_m_n(struct intel_crtc *crtc);
 int intel_dotclock_calculate(int link_freq, const struct intel_link_m_n *m_n);
 void
 ironlake_check_encoder_dotclock(const struct intel_crtc_config *pipe_config,
@@ -889,14 +966,13 @@
 bool intel_crtc_active(struct drm_crtc *crtc);
 void hsw_enable_ips(struct intel_crtc *crtc);
 void hsw_disable_ips(struct intel_crtc *crtc);
-void intel_display_set_init_power(struct drm_i915_private *dev, bool enable);
 enum intel_display_power_domain
 intel_display_port_power_domain(struct intel_encoder *intel_encoder);
 void intel_mode_from_pipe_config(struct drm_display_mode *mode,
 				 struct intel_crtc_config *pipe_config);
 int intel_format_to_fourcc(int format);
 void intel_crtc_wait_for_pending_flips(struct drm_crtc *crtc);
-
+void intel_modeset_preclose(struct drm_device *dev, struct drm_file *file);
 
 /* intel_dp.c */
 void intel_dp_init(struct drm_device *dev, int output_reg, enum port port);
@@ -917,24 +993,18 @@
 void intel_edp_backlight_on(struct intel_dp *intel_dp);
 void intel_edp_backlight_off(struct intel_dp *intel_dp);
 void intel_edp_panel_vdd_on(struct intel_dp *intel_dp);
-void intel_edp_panel_vdd_sanitize(struct intel_encoder *intel_encoder);
 void intel_edp_panel_on(struct intel_dp *intel_dp);
 void intel_edp_panel_off(struct intel_dp *intel_dp);
-void intel_edp_psr_enable(struct intel_dp *intel_dp);
-void intel_edp_psr_disable(struct intel_dp *intel_dp);
 void intel_dp_set_drrs_state(struct drm_device *dev, int refresh_rate);
-void intel_edp_psr_invalidate(struct drm_device *dev,
-			      unsigned frontbuffer_bits);
-void intel_edp_psr_flush(struct drm_device *dev,
-			 unsigned frontbuffer_bits);
-void intel_edp_psr_init(struct drm_device *dev);
-
-int intel_dp_handle_hpd_irq(struct intel_digital_port *digport, bool long_hpd);
 void intel_dp_add_properties(struct intel_dp *intel_dp, struct drm_connector *connector);
 void intel_dp_mst_suspend(struct drm_device *dev);
 void intel_dp_mst_resume(struct drm_device *dev);
 int intel_dp_max_link_bw(struct intel_dp *intel_dp);
 void intel_dp_hot_plug(struct intel_encoder *intel_encoder);
+void vlv_power_sequencer_reset(struct drm_i915_private *dev_priv);
+uint32_t intel_dp_pack_aux(const uint8_t *src, int src_bytes);
+void intel_dp_unpack_aux(uint32_t src, uint8_t *dst, int dst_bytes);
+
 /* intel_dp_mst.c */
 int intel_dp_mst_encoder_init(struct intel_digital_port *intel_dig_port, int conn_id);
 void intel_dp_mst_encoder_cleanup(struct intel_digital_port *intel_dig_port);
@@ -949,9 +1019,9 @@
 /* legacy fbdev emulation in intel_fbdev.c */
 #ifdef CONFIG_DRM_I915_FBDEV
 extern int intel_fbdev_init(struct drm_device *dev);
-extern void intel_fbdev_initial_config(struct drm_device *dev);
+extern void intel_fbdev_initial_config(void *data, async_cookie_t cookie);
 extern void intel_fbdev_fini(struct drm_device *dev);
-extern void intel_fbdev_set_suspend(struct drm_device *dev, int state);
+extern void intel_fbdev_set_suspend(struct drm_device *dev, int state, bool synchronous);
 extern void intel_fbdev_output_poll_changed(struct drm_device *dev);
 extern void intel_fbdev_restore_mode(struct drm_device *dev);
 #else
@@ -960,7 +1030,7 @@
 	return 0;
 }
 
-static inline void intel_fbdev_initial_config(struct drm_device *dev)
+static inline void intel_fbdev_initial_config(void *data, async_cookie_t cookie)
 {
 }
 
@@ -968,7 +1038,7 @@
 {
 }
 
-static inline void intel_fbdev_set_suspend(struct drm_device *dev, int state)
+static inline void intel_fbdev_set_suspend(struct drm_device *dev, int state, bool synchronous)
 {
 }
 
@@ -1024,7 +1094,7 @@
 			      int fitting_mode);
 void intel_panel_set_backlight_acpi(struct intel_connector *connector,
 				    u32 level, u32 max);
-int intel_panel_setup_backlight(struct drm_connector *connector);
+int intel_panel_setup_backlight(struct drm_connector *connector, enum pipe pipe);
 void intel_panel_enable_backlight(struct intel_connector *connector);
 void intel_panel_disable_backlight(struct intel_connector *connector);
 void intel_panel_destroy_backlight(struct drm_connector *connector);
@@ -1034,6 +1104,41 @@
 				struct drm_device *dev,
 				struct drm_display_mode *fixed_mode,
 				struct drm_connector *connector);
+void intel_backlight_register(struct drm_device *dev);
+void intel_backlight_unregister(struct drm_device *dev);
+
+
+/* intel_psr.c */
+bool intel_psr_is_enabled(struct drm_device *dev);
+void intel_psr_enable(struct intel_dp *intel_dp);
+void intel_psr_disable(struct intel_dp *intel_dp);
+void intel_psr_invalidate(struct drm_device *dev,
+			      unsigned frontbuffer_bits);
+void intel_psr_flush(struct drm_device *dev,
+			 unsigned frontbuffer_bits);
+void intel_psr_init(struct drm_device *dev);
+
+/* intel_runtime_pm.c */
+int intel_power_domains_init(struct drm_i915_private *);
+void intel_power_domains_fini(struct drm_i915_private *);
+void intel_power_domains_init_hw(struct drm_i915_private *dev_priv);
+void intel_runtime_pm_enable(struct drm_i915_private *dev_priv);
+
+bool intel_display_power_is_enabled(struct drm_i915_private *dev_priv,
+				    enum intel_display_power_domain domain);
+bool __intel_display_power_is_enabled(struct drm_i915_private *dev_priv,
+				      enum intel_display_power_domain domain);
+void intel_display_power_get(struct drm_i915_private *dev_priv,
+			     enum intel_display_power_domain domain);
+void intel_display_power_put(struct drm_i915_private *dev_priv,
+			     enum intel_display_power_domain domain);
+void intel_aux_display_runtime_get(struct drm_i915_private *dev_priv);
+void intel_aux_display_runtime_put(struct drm_i915_private *dev_priv);
+void intel_runtime_pm_get(struct drm_i915_private *dev_priv);
+void intel_runtime_pm_get_noresume(struct drm_i915_private *dev_priv);
+void intel_runtime_pm_put(struct drm_i915_private *dev_priv);
+
+void intel_display_set_init_power(struct drm_i915_private *dev, bool enable);
 
 /* intel_pm.c */
 void intel_init_clock_gating(struct drm_device *dev);
@@ -1052,17 +1157,6 @@
 void intel_update_fbc(struct drm_device *dev);
 void intel_gpu_ips_init(struct drm_i915_private *dev_priv);
 void intel_gpu_ips_teardown(void);
-int intel_power_domains_init(struct drm_i915_private *);
-void intel_power_domains_remove(struct drm_i915_private *);
-bool intel_display_power_enabled(struct drm_i915_private *dev_priv,
-				 enum intel_display_power_domain domain);
-bool intel_display_power_enabled_unlocked(struct drm_i915_private *dev_priv,
-					  enum intel_display_power_domain domain);
-void intel_display_power_get(struct drm_i915_private *dev_priv,
-			     enum intel_display_power_domain domain);
-void intel_display_power_put(struct drm_i915_private *dev_priv,
-			     enum intel_display_power_domain domain);
-void intel_power_domains_init_hw(struct drm_i915_private *dev_priv);
 void intel_init_gt_powersave(struct drm_device *dev);
 void intel_cleanup_gt_powersave(struct drm_device *dev);
 void intel_enable_gt_powersave(struct drm_device *dev);
@@ -1071,16 +1165,17 @@
 void intel_reset_gt_powersave(struct drm_device *dev);
 void ironlake_teardown_rc6(struct drm_device *dev);
 void gen6_update_ring_freq(struct drm_device *dev);
+void gen6_rps_busy(struct drm_i915_private *dev_priv);
+void gen6_rps_reset_ei(struct drm_i915_private *dev_priv);
 void gen6_rps_idle(struct drm_i915_private *dev_priv);
-void gen6_rps_boost(struct drm_i915_private *dev_priv);
-void intel_aux_display_runtime_get(struct drm_i915_private *dev_priv);
-void intel_aux_display_runtime_put(struct drm_i915_private *dev_priv);
-void intel_runtime_pm_get(struct drm_i915_private *dev_priv);
-void intel_runtime_pm_get_noresume(struct drm_i915_private *dev_priv);
-void intel_runtime_pm_put(struct drm_i915_private *dev_priv);
-void intel_init_runtime_pm(struct drm_i915_private *dev_priv);
-void intel_fini_runtime_pm(struct drm_i915_private *dev_priv);
+void gen6_rps_boost(struct drm_i915_private *dev_priv,
+		    struct drm_i915_file_private *file_priv);
+void intel_queue_rps_boost_for_request(struct drm_device *dev,
+				       struct i915_gem_request *rq);
 void ilk_wm_get_hw_state(struct drm_device *dev);
+void skl_wm_get_hw_state(struct drm_device *dev);
+void skl_ddb_get_hw_state(struct drm_i915_private *dev_priv,
+			  struct skl_ddb_allocation *ddb /* out */);
 
 
 /* intel_sdvo.c */
@@ -1091,13 +1186,18 @@
 int intel_plane_init(struct drm_device *dev, enum pipe pipe, int plane);
 void intel_flush_primary_plane(struct drm_i915_private *dev_priv,
 			       enum plane plane);
-void intel_plane_restore(struct drm_plane *plane);
+int intel_plane_set_property(struct drm_plane *plane,
+			     struct drm_property *prop,
+			     uint64_t val);
+int intel_plane_restore(struct drm_plane *plane);
 void intel_plane_disable(struct drm_plane *plane);
 int intel_sprite_set_colorkey(struct drm_device *dev, void *data,
 			      struct drm_file *file_priv);
 int intel_sprite_get_colorkey(struct drm_device *dev, void *data,
 			      struct drm_file *file_priv);
-
+bool intel_pipe_update_start(struct intel_crtc *crtc,
+			     uint32_t *start_vbl_count);
+void intel_pipe_update_end(struct intel_crtc *crtc, u32 start_vbl_count);
 
 /* intel_tv.c */
 void intel_tv_init(struct drm_device *dev);
diff -urN a/drivers/gpu/drm/i915/intel_dsi.c b/drivers/gpu/drm/i915/intel_dsi.c
--- a/drivers/gpu/drm/i915/intel_dsi.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_dsi.c	2014-11-22 14:37:49.342700417 -0700
@@ -184,7 +184,7 @@
 
 	/* update the hw state for DPLL */
 	intel_crtc->config.dpll_hw_state.dpll = DPLL_INTEGRATED_CLOCK_VLV |
-						DPLL_REFA_CLK_ENABLE_VLV;
+		DPLL_REFA_CLK_ENABLE_VLV;
 
 	tmp = I915_READ(DSPCLK_GATE_D);
 	tmp |= DPOUNIT_CLOCK_GATE_DISABLE;
@@ -259,8 +259,8 @@
 	temp = I915_READ(MIPI_CTRL(pipe));
 	temp &= ~ESCAPE_CLOCK_DIVIDER_MASK;
 	I915_WRITE(MIPI_CTRL(pipe), temp |
-			intel_dsi->escape_clk_div <<
-			ESCAPE_CLOCK_DIVIDER_SHIFT);
+		   intel_dsi->escape_clk_div <<
+		   ESCAPE_CLOCK_DIVIDER_SHIFT);
 
 	I915_WRITE(MIPI_EOT_DISABLE(pipe), CLOCKSTOP);
 
@@ -297,7 +297,7 @@
 	usleep_range(2000, 2500);
 
 	if (wait_for(((I915_READ(MIPI_PORT_CTRL(pipe)) & AFE_LATCHOUT)
-					== 0x00000), 30))
+		      == 0x00000), 30))
 		DRM_ERROR("DSI LP not going Low\n");
 
 	val = I915_READ(MIPI_PORT_CTRL(pipe));
@@ -344,7 +344,7 @@
 	DRM_DEBUG_KMS("\n");
 
 	power_domain = intel_display_port_power_domain(encoder);
-	if (!intel_display_power_enabled(dev_priv, power_domain))
+	if (!intel_display_power_is_enabled(dev_priv, power_domain))
 		return false;
 
 	/* XXX: this only works for one DSI output */
@@ -423,9 +423,11 @@
 }
 
 /* return pixels in terms of txbyteclkhs */
-static u16 txbyteclkhs(u16 pixels, int bpp, int lane_count)
+static u16 txbyteclkhs(u16 pixels, int bpp, int lane_count,
+		       u16 burst_mode_ratio)
 {
-	return DIV_ROUND_UP(DIV_ROUND_UP(pixels * bpp, 8), lane_count);
+	return DIV_ROUND_UP(DIV_ROUND_UP(pixels * bpp * burst_mode_ratio,
+					 8 * 100), lane_count);
 }
 
 static void set_dsi_timings(struct drm_encoder *encoder,
@@ -451,10 +453,12 @@
 	vbp = mode->vtotal - mode->vsync_end;
 
 	/* horizontal values are in terms of high speed byte clock */
-	hactive = txbyteclkhs(hactive, bpp, lane_count);
-	hfp = txbyteclkhs(hfp, bpp, lane_count);
-	hsync = txbyteclkhs(hsync, bpp, lane_count);
-	hbp = txbyteclkhs(hbp, bpp, lane_count);
+	hactive = txbyteclkhs(hactive, bpp, lane_count,
+			      intel_dsi->burst_mode_ratio);
+	hfp = txbyteclkhs(hfp, bpp, lane_count, intel_dsi->burst_mode_ratio);
+	hsync = txbyteclkhs(hsync, bpp, lane_count,
+			    intel_dsi->burst_mode_ratio);
+	hbp = txbyteclkhs(hbp, bpp, lane_count, intel_dsi->burst_mode_ratio);
 
 	I915_WRITE(MIPI_HACTIVE_AREA_COUNT(pipe), hactive);
 	I915_WRITE(MIPI_HFP_COUNT(pipe), hfp);
@@ -541,12 +545,14 @@
 	    intel_dsi->video_mode_format == VIDEO_MODE_BURST) {
 		I915_WRITE(MIPI_HS_TX_TIMEOUT(pipe),
 			   txbyteclkhs(adjusted_mode->htotal, bpp,
-				       intel_dsi->lane_count) + 1);
+				       intel_dsi->lane_count,
+				       intel_dsi->burst_mode_ratio) + 1);
 	} else {
 		I915_WRITE(MIPI_HS_TX_TIMEOUT(pipe),
 			   txbyteclkhs(adjusted_mode->vtotal *
 				       adjusted_mode->htotal,
-				       bpp, intel_dsi->lane_count) + 1);
+				       bpp, intel_dsi->lane_count,
+				       intel_dsi->burst_mode_ratio) + 1);
 	}
 	I915_WRITE(MIPI_LP_RX_TIMEOUT(pipe), intel_dsi->lp_rx_timeout);
 	I915_WRITE(MIPI_TURN_AROUND_TIMEOUT(pipe), intel_dsi->turn_arnd_val);
@@ -576,7 +582,7 @@
 	 * XXX: write MIPI_STOP_STATE_STALL?
 	 */
 	I915_WRITE(MIPI_HIGH_LOW_SWITCH_COUNT(pipe),
-						intel_dsi->hs_to_lp_count);
+		   intel_dsi->hs_to_lp_count);
 
 	/* XXX: low power clock equivalence in terms of byte clock. the number
 	 * of byte clocks occupied in one low power clock. based on txbyteclkhs
@@ -601,10 +607,10 @@
 		 * 64 like 1366 x 768. Enable RANDOM resolution support for such
 		 * panels by default */
 		I915_WRITE(MIPI_VIDEO_MODE_FORMAT(pipe),
-				intel_dsi->video_frmt_cfg_bits |
-				intel_dsi->video_mode_format |
-				IP_TG_CONFIG |
-				RANDOM_DPI_DISPLAY_RESOLUTION);
+			   intel_dsi->video_frmt_cfg_bits |
+			   intel_dsi->video_mode_format |
+			   IP_TG_CONFIG |
+			   RANDOM_DPI_DISPLAY_RESOLUTION);
 }
 
 static void intel_dsi_pre_pll_enable(struct intel_encoder *encoder)
diff -urN a/drivers/gpu/drm/i915/intel_dsi_cmd.c b/drivers/gpu/drm/i915/intel_dsi_cmd.c
--- a/drivers/gpu/drm/i915/intel_dsi_cmd.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_dsi_cmd.c	2014-11-22 14:37:49.342700417 -0700
@@ -430,7 +430,7 @@
 	u32 mask;
 
 	mask = LP_CTRL_FIFO_EMPTY | HS_CTRL_FIFO_EMPTY |
-					LP_DATA_FIFO_EMPTY | HS_DATA_FIFO_EMPTY;
+		LP_DATA_FIFO_EMPTY | HS_DATA_FIFO_EMPTY;
 
 	if (wait_for((I915_READ(MIPI_GEN_FIFO_STAT(pipe)) & mask) == mask, 100))
 		DRM_ERROR("DPI FIFOs are not empty\n");
diff -urN a/drivers/gpu/drm/i915/intel_dsi.h b/drivers/gpu/drm/i915/intel_dsi.h
--- a/drivers/gpu/drm/i915/intel_dsi.h	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_dsi.h	2014-11-22 14:37:49.342700417 -0700
@@ -116,6 +116,8 @@
 	u16 clk_hs_to_lp_count;
 
 	u16 init_count;
+	u32 pclk;
+	u16 burst_mode_ratio;
 
 	/* all delays in ms */
 	u16 backlight_off_delay;
diff -urN a/drivers/gpu/drm/i915/intel_dsi_panel_vbt.c b/drivers/gpu/drm/i915/intel_dsi_panel_vbt.c
--- a/drivers/gpu/drm/i915/intel_dsi_panel_vbt.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_dsi_panel_vbt.c	2014-11-22 14:37:49.342700417 -0700
@@ -271,6 +271,8 @@
 	u32 ths_prepare_ns, tclk_trail_ns;
 	u32 tclk_prepare_clkzero, ths_prepare_hszero;
 	u32 lp_to_hs_switch, hs_to_lp_switch;
+	u32 pclk, computed_ddr;
+	u16 burst_mode_ratio;
 
 	DRM_DEBUG_KMS("\n");
 
@@ -284,8 +286,6 @@
 	else if (intel_dsi->pixel_format == VID_MODE_FORMAT_RGB565)
 		bits_per_pixel = 16;
 
-	bitrate = (mode->clock * bits_per_pixel) / intel_dsi->lane_count;
-
 	intel_dsi->operation_mode = mipi_config->is_cmd_mode;
 	intel_dsi->video_mode_format = mipi_config->video_transfer_mode;
 	intel_dsi->escape_clk_div = mipi_config->byte_clk_sel;
@@ -297,6 +297,40 @@
 	intel_dsi->video_frmt_cfg_bits =
 		mipi_config->bta_enabled ? DISABLE_VIDEO_BTA : 0;
 
+	pclk = mode->clock;
+
+	/* Burst Mode Ratio
+	 * Target ddr frequency from VBT / non burst ddr freq
+	 * multiply by 100 to preserve remainder
+	 */
+	if (intel_dsi->video_mode_format == VIDEO_MODE_BURST) {
+		if (mipi_config->target_burst_mode_freq) {
+			computed_ddr =
+				(pclk * bits_per_pixel) / intel_dsi->lane_count;
+
+			if (mipi_config->target_burst_mode_freq <
+								computed_ddr) {
+				DRM_ERROR("Burst mode freq is less than computed\n");
+				return false;
+			}
+
+			burst_mode_ratio = DIV_ROUND_UP(
+				mipi_config->target_burst_mode_freq * 100,
+				computed_ddr);
+
+			pclk = DIV_ROUND_UP(pclk * burst_mode_ratio, 100);
+		} else {
+			DRM_ERROR("Burst mode target is not set\n");
+			return false;
+		}
+	} else
+		burst_mode_ratio = 100;
+
+	intel_dsi->burst_mode_ratio = burst_mode_ratio;
+	intel_dsi->pclk = pclk;
+
+	bitrate = (pclk * bits_per_pixel) / intel_dsi->lane_count;
+
 	switch (intel_dsi->escape_clk_div) {
 	case 0:
 		tlpx_ns = 50;
diff -urN a/drivers/gpu/drm/i915/intel_dsi_pll.c b/drivers/gpu/drm/i915/intel_dsi_pll.c
--- a/drivers/gpu/drm/i915/intel_dsi_pll.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_dsi_pll.c	2014-11-22 14:37:49.342700417 -0700
@@ -134,8 +134,7 @@
 #else
 
 /* Get DSI clock from pixel clock */
-static u32 dsi_clk_from_pclk(const struct drm_display_mode *mode,
-			  int pixel_format, int lane_count)
+static u32 dsi_clk_from_pclk(u32 pclk, int pixel_format, int lane_count)
 {
 	u32 dsi_clk_khz;
 	u32 bpp;
@@ -156,7 +155,7 @@
 
 	/* DSI data rate = pixel clock * bits per pixel / lane count
 	   pixel clock is converted from KHz to Hz */
-	dsi_clk_khz = DIV_ROUND_CLOSEST(mode->clock * bpp, lane_count);
+	dsi_clk_khz = DIV_ROUND_CLOSEST(pclk * bpp, lane_count);
 
 	return dsi_clk_khz;
 }
@@ -191,7 +190,7 @@
 	for (m = 62; m <= 92; m++) {
 		for (p = 2; p <= 6; p++) {
 			/* Find the optimal m and p divisors
-			with minimal error +/- the required clock */
+			   with minimal error +/- the required clock */
 			calc_dsi_clk = (m * ref_clk) / p;
 			if (calc_dsi_clk == target_dsi_clk) {
 				calc_m = m;
@@ -228,15 +227,13 @@
 static void vlv_configure_dsi_pll(struct intel_encoder *encoder)
 {
 	struct drm_i915_private *dev_priv = encoder->base.dev->dev_private;
-	struct intel_crtc *intel_crtc = to_intel_crtc(encoder->base.crtc);
-	const struct drm_display_mode *mode = &intel_crtc->config.adjusted_mode;
 	struct intel_dsi *intel_dsi = enc_to_intel_dsi(&encoder->base);
 	int ret;
 	struct dsi_mnp dsi_mnp;
 	u32 dsi_clk;
 
-	dsi_clk = dsi_clk_from_pclk(mode, intel_dsi->pixel_format,
-						intel_dsi->lane_count);
+	dsi_clk = dsi_clk_from_pclk(intel_dsi->pclk, intel_dsi->pixel_format,
+				    intel_dsi->lane_count);
 
 	ret = dsi_calc_mnp(dsi_clk, &dsi_mnp);
 	if (ret) {
@@ -318,8 +315,8 @@
 	}
 
 	WARN(bpp != pipe_bpp,
-		"bpp match assertion failure (expected %d, current %d)\n",
-		bpp, pipe_bpp);
+	     "bpp match assertion failure (expected %d, current %d)\n",
+	     bpp, pipe_bpp);
 }
 
 u32 vlv_get_dsi_pclk(struct intel_encoder *encoder, int pipe_bpp)
diff -urN a/drivers/gpu/drm/i915/intel_dvo.c b/drivers/gpu/drm/i915/intel_dvo.c
--- a/drivers/gpu/drm/i915/intel_dvo.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_dvo.c	2014-11-22 14:37:49.342700417 -0700
@@ -85,7 +85,7 @@
 	{
 	        .type = INTEL_DVO_CHIP_TMDS,
 		.name = "ns2501",
-		.dvo_reg = DVOC,
+		.dvo_reg = DVOB,
 		.slave_addr = NS2501_ADDR,
 		.dev_ops = &ns2501_ops,
        }
@@ -185,12 +185,13 @@
 	u32 dvo_reg = intel_dvo->dev.dvo_reg;
 	u32 temp = I915_READ(dvo_reg);
 
-	I915_WRITE(dvo_reg, temp | DVO_ENABLE);
-	I915_READ(dvo_reg);
 	intel_dvo->dev.dev_ops->mode_set(&intel_dvo->dev,
 					 &crtc->config.requested_mode,
 					 &crtc->config.adjusted_mode);
 
+	I915_WRITE(dvo_reg, temp | DVO_ENABLE);
+	I915_READ(dvo_reg);
+
 	intel_dvo->dev.dev_ops->dpms(&intel_dvo->dev, true);
 }
 
@@ -226,10 +227,6 @@
 
 		intel_crtc_update_dpms(crtc);
 
-		intel_dvo->dev.dev_ops->mode_set(&intel_dvo->dev,
-						 &config->requested_mode,
-						 &config->adjusted_mode);
-
 		intel_dvo->dev.dev_ops->dpms(&intel_dvo->dev, true);
 	} else {
 		intel_dvo->dev.dev_ops->dpms(&intel_dvo->dev, false);
diff -urN a/drivers/gpu/drm/i915/intel_fbdev.c b/drivers/gpu/drm/i915/intel_fbdev.c
--- a/drivers/gpu/drm/i915/intel_fbdev.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_fbdev.c	2014-11-22 14:37:49.342700417 -0700
@@ -24,8 +24,10 @@
  *     David Airlie
  */
 
+#include <linux/async.h>
 #include <linux/module.h>
 #include <linux/kernel.h>
+#include <linux/console.h>
 #include <linux/errno.h>
 #include <linux/string.h>
 #include <linux/mm.h>
@@ -117,25 +119,25 @@
 		goto out;
 	}
 
-	/* Flush everything out, we'll be doing GTT only from now on */
-	ret = intel_pin_and_fence_fb_obj(dev, obj, NULL);
-	if (ret) {
-		DRM_ERROR("failed to pin obj: %d\n", ret);
-		goto out_unref;
-	}
-
 	fb = __intel_framebuffer_create(dev, &mode_cmd, obj);
 	if (IS_ERR(fb)) {
 		ret = PTR_ERR(fb);
-		goto out_unpin;
+		goto out_unref;
+	}
+
+	/* Flush everything out, we'll be doing GTT only from now on */
+	ret = intel_pin_and_fence_fb_obj(NULL, fb, NULL);
+	if (ret) {
+		DRM_ERROR("failed to pin obj: %d\n", ret);
+		goto out_fb;
 	}
 
 	ifbdev->fb = to_intel_framebuffer(fb);
 
 	return 0;
 
-out_unpin:
-	i915_gem_object_ggtt_unpin(obj);
+out_fb:
+	drm_framebuffer_remove(fb);
 out_unref:
 	drm_gem_object_unreference(&obj->base);
 out:
@@ -331,24 +333,6 @@
 	int num_connectors_enabled = 0;
 	int num_connectors_detected = 0;
 
-	/*
-	 * If the user specified any force options, just bail here
-	 * and use that config.
-	 */
-	for (i = 0; i < fb_helper->connector_count; i++) {
-		struct drm_fb_helper_connector *fb_conn;
-		struct drm_connector *connector;
-
-		fb_conn = fb_helper->connector_info[i];
-		connector = fb_conn->connector;
-
-		if (!enabled[i])
-			continue;
-
-		if (connector->force != DRM_FORCE_UNSPECIFIED)
-			return false;
-	}
-
 	save_enabled = kcalloc(dev->mode_config.num_connector, sizeof(bool),
 			       GFP_KERNEL);
 	if (!save_enabled)
@@ -374,8 +358,18 @@
 			continue;
 		}
 
+		if (connector->force == DRM_FORCE_OFF) {
+			DRM_DEBUG_KMS("connector %s is disabled by user, skipping\n",
+				      connector->name);
+			enabled[i] = false;
+			continue;
+		}
+
 		encoder = connector->encoder;
 		if (!encoder || WARN_ON(!encoder->crtc)) {
+			if (connector->force > DRM_FORCE_OFF)
+				goto bail;
+
 			DRM_DEBUG_KMS("connector %s has no encoder or crtc, skipping\n",
 				      connector->name);
 			enabled[i] = false;
@@ -394,8 +388,7 @@
 		for (j = 0; j < fb_helper->connector_count; j++) {
 			if (crtcs[j] == new_crtc) {
 				DRM_DEBUG_KMS("fallback: cloned configuration\n");
-				fallback = true;
-				goto out;
+				goto bail;
 			}
 		}
 
@@ -466,8 +459,8 @@
 		fallback = true;
 	}
 
-out:
 	if (fallback) {
+bail:
 		DRM_DEBUG_KMS("Not using firmware configuration\n");
 		memcpy(enabled, save_enabled, dev->mode_config.num_connector);
 		kfree(save_enabled);
@@ -523,7 +516,7 @@
 	struct intel_plane_config *plane_config = NULL;
 	unsigned int max_size = 0;
 
-	if (!i915.fastboot)
+	if (!i915_module.fastboot)
 		return false;
 
 	/* Find the largest fb */
@@ -636,6 +629,15 @@
 	return false;
 }
 
+static void intel_fbdev_suspend_worker(struct work_struct *work)
+{
+	intel_fbdev_set_suspend(container_of(work,
+					     struct drm_i915_private,
+					     fbdev_suspend_work)->dev,
+				FBINFO_STATE_RUNNING,
+				true);
+}
+
 int intel_fbdev_init(struct drm_device *dev)
 {
 	struct intel_fbdev *ifbdev;
@@ -662,14 +664,16 @@
 	}
 
 	dev_priv->fbdev = ifbdev;
+	INIT_WORK(&dev_priv->fbdev_suspend_work, intel_fbdev_suspend_worker);
+
 	drm_fb_helper_single_add_all_connectors(&ifbdev->helper);
 
 	return 0;
 }
 
-void intel_fbdev_initial_config(struct drm_device *dev)
+void intel_fbdev_initial_config(void *data, async_cookie_t cookie)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = data;
 	struct intel_fbdev *ifbdev = dev_priv->fbdev;
 
 	/* Due to peculiar init order wrt to hpd handling this is separate. */
@@ -682,12 +686,15 @@
 	if (!dev_priv->fbdev)
 		return;
 
+	flush_work(&dev_priv->fbdev_suspend_work);
+
+	async_synchronize_full();
 	intel_fbdev_destroy(dev, dev_priv->fbdev);
 	kfree(dev_priv->fbdev);
 	dev_priv->fbdev = NULL;
 }
 
-void intel_fbdev_set_suspend(struct drm_device *dev, int state)
+void intel_fbdev_set_suspend(struct drm_device *dev, int state, bool synchronous)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_fbdev *ifbdev = dev_priv->fbdev;
@@ -698,6 +705,33 @@
 
 	info = ifbdev->helper.fbdev;
 
+	if (synchronous) {
+		/* Flush any pending work to turn the console on, and then
+		 * wait to turn it off. It must be synchronous as we are
+		 * about to suspend or unload the driver.
+		 *
+		 * Note that from within the work-handler, we cannot flush
+		 * ourselves, so only flush outstanding work upon suspend!
+		 */
+		if (state != FBINFO_STATE_RUNNING)
+			flush_work(&dev_priv->fbdev_suspend_work);
+		console_lock();
+	} else {
+		/*
+		 * The console lock can be pretty contented on resume due
+		 * to all the printk activity.  Try to keep it out of the hot
+		 * path of resume if possible.
+		 */
+		WARN_ON(state != FBINFO_STATE_RUNNING);
+		if (!console_trylock()) {
+			/* Don't block our own workqueue as this can
+			 * be run in parallel with other i915.ko tasks.
+			 */
+			schedule_work(&dev_priv->fbdev_suspend_work);
+			return;
+		}
+	}
+
 	/* On resume from hibernation: If the object is shmemfs backed, it has
 	 * been restored from swap. If the object is stolen however, it will be
 	 * full of whatever garbage was left in there.
@@ -706,6 +740,7 @@
 		memset_io(info->screen_base, 0, info->screen_size);
 
 	fb_set_suspend(info, state);
+	console_unlock();
 }
 
 void intel_fbdev_output_poll_changed(struct drm_device *dev)
diff -urN a/drivers/gpu/drm/i915/intel_fifo_underrun.c b/drivers/gpu/drm/i915/intel_fifo_underrun.c
--- a/drivers/gpu/drm/i915/intel_fifo_underrun.c	1969-12-31 17:00:00.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_fifo_underrun.c	2014-11-22 14:37:49.342700417 -0700
@@ -0,0 +1,381 @@
+/*
+ * Copyright  2014 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ *
+ * Authors:
+ *    Daniel Vetter <daniel.vetter@ffwll.ch>
+ *
+ */
+
+#include "i915_drv.h"
+#include "intel_drv.h"
+
+/**
+ * DOC: fifo underrun handling
+ *
+ * The i915 driver checks for display fifo underruns using the interrupt signals
+ * provided by the hardware. This is enabled by default and fairly useful to
+ * debug display issues, especially watermark settings.
+ *
+ * If an underrun is detected this is logged into dmesg. To avoid flooding logs
+ * and occupying the cpu underrun interrupts are disabled after the first
+ * occurrence until the next modeset on a given pipe.
+ *
+ * Note that underrun detection on gmch platforms is a bit more ugly since there
+ * is no interrupt (despite that the signalling bit is in the PIPESTAT pipe
+ * interrupt register). Also on some other platforms underrun interrupts are
+ * shared, which means that if we detect an underrun we need to disable underrun
+ * reporting on all pipes.
+ *
+ * The code also supports underrun detection on the PCH transcoder.
+ */
+
+static bool ivb_can_enable_err_int(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *crtc;
+	enum pipe pipe;
+
+	assert_spin_locked(&dev_priv->irq_lock);
+
+	for_each_pipe(dev_priv, pipe) {
+		crtc = to_intel_crtc(dev_priv->pipe_to_crtc_mapping[pipe]);
+
+		if (crtc->cpu_fifo_underrun_disabled)
+			return false;
+	}
+
+	return true;
+}
+
+static bool cpt_can_enable_serr_int(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	enum pipe pipe;
+	struct intel_crtc *crtc;
+
+	assert_spin_locked(&dev_priv->irq_lock);
+
+	for_each_pipe(dev_priv, pipe) {
+		crtc = to_intel_crtc(dev_priv->pipe_to_crtc_mapping[pipe]);
+
+		if (crtc->pch_fifo_underrun_disabled)
+			return false;
+	}
+
+	return true;
+}
+
+/**
+ * i9xx_check_fifo_underruns - check for fifo underruns
+ * @dev_priv: i915 device instance
+ *
+ * This function checks for fifo underruns on GMCH platforms. This needs to be
+ * done manually on modeset to make sure that we catch all underruns since they
+ * do not generate an interrupt by themselves on these platforms.
+ */
+void i9xx_check_fifo_underruns(struct drm_i915_private *dev_priv)
+{
+	struct intel_crtc *crtc;
+
+	spin_lock_irq(&dev_priv->irq_lock);
+
+	for_each_intel_crtc(dev_priv->dev, crtc) {
+		u32 reg = PIPESTAT(crtc->pipe);
+		u32 pipestat;
+
+		if (crtc->cpu_fifo_underrun_disabled)
+			continue;
+
+		pipestat = I915_READ(reg) & 0xffff0000;
+		if ((pipestat & PIPE_FIFO_UNDERRUN_STATUS) == 0)
+			continue;
+
+		I915_WRITE(reg, pipestat | PIPE_FIFO_UNDERRUN_STATUS);
+		POSTING_READ(reg);
+
+		DRM_ERROR("pipe %c underrun\n", pipe_name(crtc->pipe));
+	}
+
+	spin_unlock_irq(&dev_priv->irq_lock);
+}
+
+static void i9xx_set_fifo_underrun_reporting(struct drm_device *dev,
+					     enum pipe pipe,
+					     bool enable, bool old)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	u32 reg = PIPESTAT(pipe);
+	u32 pipestat = I915_READ(reg) & 0xffff0000;
+
+	assert_spin_locked(&dev_priv->irq_lock);
+
+	if (enable) {
+		I915_WRITE(reg, pipestat | PIPE_FIFO_UNDERRUN_STATUS);
+		POSTING_READ(reg);
+	} else {
+		if (old && pipestat & PIPE_FIFO_UNDERRUN_STATUS)
+			DRM_ERROR("pipe %c underrun\n", pipe_name(pipe));
+	}
+}
+
+static void ironlake_set_fifo_underrun_reporting(struct drm_device *dev,
+						 enum pipe pipe, bool enable)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	uint32_t bit = (pipe == PIPE_A) ? DE_PIPEA_FIFO_UNDERRUN :
+					  DE_PIPEB_FIFO_UNDERRUN;
+
+	if (enable)
+		ironlake_enable_display_irq(dev_priv, bit);
+	else
+		ironlake_disable_display_irq(dev_priv, bit);
+}
+
+static void ivybridge_set_fifo_underrun_reporting(struct drm_device *dev,
+						  enum pipe pipe,
+						  bool enable, bool old)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	if (enable) {
+		I915_WRITE(GEN7_ERR_INT, ERR_INT_FIFO_UNDERRUN(pipe));
+
+		if (!ivb_can_enable_err_int(dev))
+			return;
+
+		ironlake_enable_display_irq(dev_priv, DE_ERR_INT_IVB);
+	} else {
+		ironlake_disable_display_irq(dev_priv, DE_ERR_INT_IVB);
+
+		if (old &&
+		    I915_READ(GEN7_ERR_INT) & ERR_INT_FIFO_UNDERRUN(pipe)) {
+			DRM_ERROR("uncleared fifo underrun on pipe %c\n",
+				  pipe_name(pipe));
+		}
+	}
+}
+
+static void broadwell_set_fifo_underrun_reporting(struct drm_device *dev,
+						  enum pipe pipe, bool enable)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	assert_spin_locked(&dev_priv->irq_lock);
+
+	if (enable)
+		dev_priv->de_irq_mask[pipe] &= ~GEN8_PIPE_FIFO_UNDERRUN;
+	else
+		dev_priv->de_irq_mask[pipe] |= GEN8_PIPE_FIFO_UNDERRUN;
+	I915_WRITE(GEN8_DE_PIPE_IMR(pipe), dev_priv->de_irq_mask[pipe]);
+	POSTING_READ(GEN8_DE_PIPE_IMR(pipe));
+}
+
+static void ibx_set_fifo_underrun_reporting(struct drm_device *dev,
+					    enum transcoder pch_transcoder,
+					    bool enable)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	uint32_t bit = (pch_transcoder == TRANSCODER_A) ?
+		       SDE_TRANSA_FIFO_UNDER : SDE_TRANSB_FIFO_UNDER;
+
+	if (enable)
+		ibx_enable_display_interrupt(dev_priv, bit);
+	else
+		ibx_disable_display_interrupt(dev_priv, bit);
+}
+
+static void cpt_set_fifo_underrun_reporting(struct drm_device *dev,
+					    enum transcoder pch_transcoder,
+					    bool enable, bool old)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	if (enable) {
+		I915_WRITE(SERR_INT,
+			   SERR_INT_TRANS_FIFO_UNDERRUN(pch_transcoder));
+
+		if (!cpt_can_enable_serr_int(dev))
+			return;
+
+		ibx_enable_display_interrupt(dev_priv, SDE_ERROR_CPT);
+	} else {
+		ibx_disable_display_interrupt(dev_priv, SDE_ERROR_CPT);
+
+		if (old && I915_READ(SERR_INT) &
+		    SERR_INT_TRANS_FIFO_UNDERRUN(pch_transcoder)) {
+			DRM_ERROR("uncleared pch fifo underrun on pch transcoder %c\n",
+				  transcoder_name(pch_transcoder));
+		}
+	}
+}
+
+static bool __intel_set_cpu_fifo_underrun_reporting(struct drm_device *dev,
+						    enum pipe pipe, bool enable)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_crtc *crtc = dev_priv->pipe_to_crtc_mapping[pipe];
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	bool old;
+
+	assert_spin_locked(&dev_priv->irq_lock);
+
+	old = !intel_crtc->cpu_fifo_underrun_disabled;
+	intel_crtc->cpu_fifo_underrun_disabled = !enable;
+
+	if (HAS_GMCH_DISPLAY(dev))
+		i9xx_set_fifo_underrun_reporting(dev, pipe, enable, old);
+	else if (IS_GEN5(dev) || IS_GEN6(dev))
+		ironlake_set_fifo_underrun_reporting(dev, pipe, enable);
+	else if (IS_GEN7(dev))
+		ivybridge_set_fifo_underrun_reporting(dev, pipe, enable, old);
+	else if (IS_GEN8(dev) || IS_GEN9(dev))
+		broadwell_set_fifo_underrun_reporting(dev, pipe, enable);
+
+	return old;
+}
+
+/**
+ * intel_set_cpu_fifo_underrun_reporting - set cpu fifo underrrun reporting state
+ * @dev_priv: i915 device instance
+ * @pipe: (CPU) pipe to set state for
+ * @enable: whether underruns should be reported or not
+ *
+ * This function sets the fifo underrun state for @pipe. It is used in the
+ * modeset code to avoid false positives since on many platforms underruns are
+ * expected when disabling or enabling the pipe.
+ *
+ * Notice that on some platforms disabling underrun reports for one pipe
+ * disables for all due to shared interrupts. Actual reporting is still per-pipe
+ * though.
+ *
+ * Returns the previous state of underrun reporting.
+ */
+bool intel_set_cpu_fifo_underrun_reporting(struct drm_i915_private *dev_priv,
+					   enum pipe pipe, bool enable)
+{
+	unsigned long flags;
+	bool ret;
+
+	spin_lock_irqsave(&dev_priv->irq_lock, flags);
+	ret = __intel_set_cpu_fifo_underrun_reporting(dev_priv->dev, pipe,
+						      enable);
+	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
+
+	return ret;
+}
+
+static bool
+__cpu_fifo_underrun_reporting_enabled(struct drm_i915_private *dev_priv,
+				      enum pipe pipe)
+{
+	struct drm_crtc *crtc = dev_priv->pipe_to_crtc_mapping[pipe];
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+
+	return !intel_crtc->cpu_fifo_underrun_disabled;
+}
+
+/**
+ * intel_set_pch_fifo_underrun_reporting - set PCH fifo underrun reporting state
+ * @dev_priv: i915 device instance
+ * @pch_transcoder: the PCH transcoder (same as pipe on IVB and older)
+ * @enable: whether underruns should be reported or not
+ *
+ * This function makes us disable or enable PCH fifo underruns for a specific
+ * PCH transcoder. Notice that on some PCHs (e.g. CPT/PPT), disabling FIFO
+ * underrun reporting for one transcoder may also disable all the other PCH
+ * error interruts for the other transcoders, due to the fact that there's just
+ * one interrupt mask/enable bit for all the transcoders.
+ *
+ * Returns the previous state of underrun reporting.
+ */
+bool intel_set_pch_fifo_underrun_reporting(struct drm_i915_private *dev_priv,
+					   enum transcoder pch_transcoder,
+					   bool enable)
+{
+	struct drm_crtc *crtc = dev_priv->pipe_to_crtc_mapping[pch_transcoder];
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	unsigned long flags;
+	bool old;
+
+	/*
+	 * NOTE: Pre-LPT has a fixed cpu pipe -> pch transcoder mapping, but LPT
+	 * has only one pch transcoder A that all pipes can use. To avoid racy
+	 * pch transcoder -> pipe lookups from interrupt code simply store the
+	 * underrun statistics in crtc A. Since we never expose this anywhere
+	 * nor use it outside of the fifo underrun code here using the "wrong"
+	 * crtc on LPT won't cause issues.
+	 */
+
+	spin_lock_irqsave(&dev_priv->irq_lock, flags);
+
+	old = !intel_crtc->pch_fifo_underrun_disabled;
+	intel_crtc->pch_fifo_underrun_disabled = !enable;
+
+	if (HAS_PCH_IBX(dev_priv->dev))
+		ibx_set_fifo_underrun_reporting(dev_priv->dev, pch_transcoder,
+						enable);
+	else
+		cpt_set_fifo_underrun_reporting(dev_priv->dev, pch_transcoder,
+						enable, old);
+
+	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
+	return old;
+}
+
+/**
+ * intel_pch_fifo_underrun_irq_handler - handle PCH fifo underrun interrupt
+ * @dev_priv: i915 device instance
+ * @pipe: (CPU) pipe to set state for
+ *
+ * This handles a CPU fifo underrun interrupt, generating an underrun warning
+ * into dmesg if underrun reporting is enabled and then disables the underrun
+ * interrupt to avoid an irq storm.
+ */
+void intel_cpu_fifo_underrun_irq_handler(struct drm_i915_private *dev_priv,
+					 enum pipe pipe)
+{
+	/* GMCH can't disable fifo underruns, filter them. */
+	if (HAS_GMCH_DISPLAY(dev_priv->dev) &&
+	    !__cpu_fifo_underrun_reporting_enabled(dev_priv, pipe))
+		return;
+
+	if (intel_set_cpu_fifo_underrun_reporting(dev_priv, pipe, false))
+		DRM_ERROR("CPU pipe %c FIFO underrun\n",
+			  pipe_name(pipe));
+}
+
+/**
+ * intel_pch_fifo_underrun_irq_handler - handle PCH fifo underrun interrupt
+ * @dev_priv: i915 device instance
+ * @pch_transcoder: the PCH transcoder (same as pipe on IVB and older)
+ *
+ * This handles a PCH fifo underrun interrupt, generating an underrun warning
+ * into dmesg if underrun reporting is enabled and then disables the underrun
+ * interrupt to avoid an irq storm.
+ */
+void intel_pch_fifo_underrun_irq_handler(struct drm_i915_private *dev_priv,
+					 enum transcoder pch_transcoder)
+{
+	if (intel_set_pch_fifo_underrun_reporting(dev_priv, pch_transcoder,
+						  false))
+		DRM_ERROR("PCH transcoder %c FIFO underrun\n",
+			  transcoder_name(pch_transcoder));
+}
diff -urN a/drivers/gpu/drm/i915/intel_frontbuffer.c b/drivers/gpu/drm/i915/intel_frontbuffer.c
--- a/drivers/gpu/drm/i915/intel_frontbuffer.c	1969-12-31 17:00:00.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_frontbuffer.c	2014-11-22 14:37:49.342700417 -0700
@@ -0,0 +1,279 @@
+/*
+ * Copyright  2014 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ *
+ * Authors:
+ *	Daniel Vetter <daniel.vetter@ffwll.ch>
+ */
+
+/**
+ * DOC: frontbuffer tracking
+ *
+ * Many features require us to track changes to the currently active
+ * frontbuffer, especially rendering targeted at the frontbuffer.
+ *
+ * To be able to do so GEM tracks frontbuffers using a bitmask for all possible
+ * frontbuffer slots through i915_gem_track_fb(). The function in this file are
+ * then called when the contents of the frontbuffer are invalidated, when
+ * frontbuffer rendering has stopped again to flush out all the changes and when
+ * the frontbuffer is exchanged with a flip. Subsystems interested in
+ * frontbuffer changes (e.g. PSR, FBC, DRRS) should directly put their callbacks
+ * into the relevant places and filter for the frontbuffer slots that they are
+ * interested int.
+ *
+ * On a high level there are two types of powersaving features. The first one
+ * work like a special cache (FBC and PSR) and are interested when they should
+ * stop caching and when to restart caching. This is done by placing callbacks
+ * into the invalidate and the flush functions: At invalidate the caching must
+ * be stopped and at flush time it can be restarted. And maybe they need to know
+ * when the frontbuffer changes (e.g. when the hw doesn't initiate an invalidate
+ * and flush on its own) which can be achieved with placing callbacks into the
+ * flip functions.
+ *
+ * The other type of display power saving feature only cares about busyness
+ * (e.g. DRRS). In that case all three (invalidate, flush and flip) indicate
+ * busyness. There is no direct way to detect idleness. Instead an idle timer
+ * work delayed work should be started from the flush and flip functions and
+ * cancelled as soon as busyness is detected.
+ *
+ * Note that there's also an older frontbuffer activity tracking scheme which
+ * just tracks general activity. This is done by the various mark_busy and
+ * mark_idle functions. For display power management features using these
+ * functions is deprecated and should be avoided.
+ */
+
+#include <drm/drmP.h>
+
+#include "intel_drv.h"
+#include "i915_drv.h"
+
+static void intel_increase_pllclock(struct drm_device *dev,
+				    enum pipe pipe)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	int dpll_reg = DPLL(pipe);
+	int dpll;
+
+	if (!HAS_GMCH_DISPLAY(dev))
+		return;
+
+	if (!dev_priv->lvds_downclock_avail)
+		return;
+
+	dpll = I915_READ(dpll_reg);
+	if (!HAS_PIPE_CXSR(dev) && (dpll & DISPLAY_RATE_SELECT_FPA1)) {
+		DRM_DEBUG_DRIVER("upclocking LVDS\n");
+
+		assert_panel_unlocked(dev_priv, pipe);
+
+		dpll &= ~DISPLAY_RATE_SELECT_FPA1;
+		I915_WRITE(dpll_reg, dpll);
+		intel_wait_for_vblank(dev, pipe);
+
+		dpll = I915_READ(dpll_reg);
+		if (dpll & DISPLAY_RATE_SELECT_FPA1)
+			DRM_DEBUG_DRIVER("failed to upclock LVDS!\n");
+	}
+}
+
+/**
+ * intel_mark_fb_busy - mark given planes as busy
+ * @dev: DRM device
+ * @frontbuffer_bits: bits for the affected planes
+ * @rq: optional request for asynchronous commands
+ *
+ * This function gets called every time the screen contents change. It can be
+ * used to keep e.g. the update rate at the nominal refresh rate with DRRS.
+ */
+static void intel_mark_fb_busy(struct drm_device *dev,
+			       unsigned frontbuffer_bits,
+			       struct i915_gem_request *rq)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	enum pipe pipe;
+
+	if (!i915_module.powersave)
+		return;
+
+	for_each_pipe(dev_priv, pipe) {
+		if (!(frontbuffer_bits & INTEL_FRONTBUFFER_ALL_MASK(pipe)))
+			continue;
+
+		intel_increase_pllclock(dev, pipe);
+		if (rq && intel_fbc_enabled(dev))
+			rq->pending_flush |= I915_KICK_FBC;
+	}
+}
+
+/**
+ * intel_fb_obj_invalidate - invalidate frontbuffer object
+ * @obj: GEM object to invalidate
+ * @rq: set for asynchronous rendering
+ *
+ * This function gets called every time rendering on the given object starts and
+ * frontbuffer caching (fbc, low refresh rate for DRRS, panel self refresh) must
+ * be invalidated. If @ring is non-NULL any subsequent invalidation will be delayed
+ * until the rendering completes or a flip on this frontbuffer plane is
+ * scheduled.
+ */
+void intel_fb_obj_invalidate(struct drm_i915_gem_object *obj,
+			     struct i915_gem_request *rq)
+{
+	struct drm_device *dev = obj->base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
+
+	if (!obj->frontbuffer_bits)
+		return;
+
+	if (rq) {
+		mutex_lock(&dev_priv->fb_tracking.lock);
+		dev_priv->fb_tracking.busy_bits
+			|= obj->frontbuffer_bits;
+		dev_priv->fb_tracking.flip_bits
+			&= ~obj->frontbuffer_bits;
+		mutex_unlock(&dev_priv->fb_tracking.lock);
+	}
+
+	intel_mark_fb_busy(dev, obj->frontbuffer_bits, rq);
+
+	intel_psr_invalidate(dev, obj->frontbuffer_bits);
+}
+
+/**
+ * intel_frontbuffer_flush - flush frontbuffer
+ * @dev: DRM device
+ * @frontbuffer_bits: frontbuffer plane tracking bits
+ *
+ * This function gets called every time rendering on the given planes has
+ * completed and frontbuffer caching can be started again. Flushes will get
+ * delayed if they're blocked by some outstanding asynchronous rendering.
+ *
+ * Can be called without any locks held.
+ */
+void intel_frontbuffer_flush(struct drm_device *dev,
+			     unsigned frontbuffer_bits)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	/* Delay flushing when rings are still busy.*/
+	mutex_lock(&dev_priv->fb_tracking.lock);
+	frontbuffer_bits &= ~dev_priv->fb_tracking.busy_bits;
+	mutex_unlock(&dev_priv->fb_tracking.lock);
+
+	intel_mark_fb_busy(dev, frontbuffer_bits, NULL);
+
+	intel_psr_flush(dev, frontbuffer_bits);
+
+	/*
+	 * FIXME: Unconditional fbc flushing here is a rather gross hack and
+	 * needs to be reworked into a proper frontbuffer tracking scheme like
+	 * psr employs.
+	 */
+	if (dev_priv->fbc.need_sw_cache_clean) {
+		dev_priv->fbc.need_sw_cache_clean = false;
+		bdw_fbc_sw_flush(dev, FBC_REND_CACHE_CLEAN);
+	}
+}
+
+/**
+ * intel_fb_obj_flush - flush frontbuffer object
+ * @obj: GEM object to flush
+ * @retire: set when retiring asynchronous rendering
+ *
+ * This function gets called every time rendering on the given object has
+ * completed and frontbuffer caching can be started again. If @retire is true
+ * then any delayed flushes will be unblocked.
+ */
+void intel_fb_obj_flush(struct drm_i915_gem_object *obj,
+			bool retire)
+{
+	struct drm_device *dev = obj->base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	unsigned frontbuffer_bits;
+
+	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
+
+	if (!obj->frontbuffer_bits)
+		return;
+
+	frontbuffer_bits = obj->frontbuffer_bits;
+
+	if (retire) {
+		mutex_lock(&dev_priv->fb_tracking.lock);
+		/* Filter out new bits since rendering started. */
+		frontbuffer_bits &= dev_priv->fb_tracking.busy_bits;
+
+		dev_priv->fb_tracking.busy_bits &= ~frontbuffer_bits;
+		mutex_unlock(&dev_priv->fb_tracking.lock);
+	}
+
+	intel_frontbuffer_flush(dev, frontbuffer_bits);
+}
+
+/**
+ * intel_frontbuffer_flip_prepare - prepare asynchronous frontbuffer flip
+ * @dev: DRM device
+ * @frontbuffer_bits: frontbuffer plane tracking bits
+ *
+ * This function gets called after scheduling a flip on @obj. The actual
+ * frontbuffer flushing will be delayed until completion is signalled with
+ * intel_frontbuffer_flip_complete. If an invalidate happens in between this
+ * flush will be cancelled.
+ *
+ * Can be called without any locks held.
+ */
+void intel_frontbuffer_flip_prepare(struct drm_device *dev,
+				    unsigned frontbuffer_bits)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	mutex_lock(&dev_priv->fb_tracking.lock);
+	dev_priv->fb_tracking.flip_bits |= frontbuffer_bits;
+	/* Remove stale busy bits due to the old buffer. */
+	dev_priv->fb_tracking.busy_bits &= ~frontbuffer_bits;
+	mutex_unlock(&dev_priv->fb_tracking.lock);
+}
+
+/**
+ * intel_frontbuffer_flip_complete - complete asynchronous frontbuffer flip
+ * @dev: DRM device
+ * @frontbuffer_bits: frontbuffer plane tracking bits
+ *
+ * This function gets called after the flip has been latched and will complete
+ * on the next vblank. It will execute the flush if it hasn't been cancelled yet.
+ *
+ * Can be called without any locks held.
+ */
+void intel_frontbuffer_flip_complete(struct drm_device *dev,
+				     unsigned frontbuffer_bits)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	mutex_lock(&dev_priv->fb_tracking.lock);
+	/* Mask any cancelled flips. */
+	frontbuffer_bits &= dev_priv->fb_tracking.flip_bits;
+	dev_priv->fb_tracking.flip_bits &= ~frontbuffer_bits;
+	mutex_unlock(&dev_priv->fb_tracking.lock);
+
+	intel_frontbuffer_flush(dev, frontbuffer_bits);
+}
diff -urN a/drivers/gpu/drm/i915/intel_hdmi.c b/drivers/gpu/drm/i915/intel_hdmi.c
--- a/drivers/gpu/drm/i915/intel_hdmi.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_hdmi.c	2014-11-22 14:37:49.342700417 -0700
@@ -166,6 +166,15 @@
 	POSTING_READ(VIDEO_DIP_CTL);
 }
 
+static bool g4x_infoframe_enabled(struct drm_encoder *encoder)
+{
+	struct drm_device *dev = encoder->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	u32 val = I915_READ(VIDEO_DIP_CTL);
+
+	return val & VIDEO_DIP_ENABLE;
+}
+
 static void ibx_write_infoframe(struct drm_encoder *encoder,
 				enum hdmi_infoframe_type type,
 				const void *frame, ssize_t len)
@@ -204,6 +213,17 @@
 	POSTING_READ(reg);
 }
 
+static bool ibx_infoframe_enabled(struct drm_encoder *encoder)
+{
+	struct drm_device *dev = encoder->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = to_intel_crtc(encoder->crtc);
+	int reg = TVIDEO_DIP_CTL(intel_crtc->pipe);
+	u32 val = I915_READ(reg);
+
+	return val & VIDEO_DIP_ENABLE;
+}
+
 static void cpt_write_infoframe(struct drm_encoder *encoder,
 				enum hdmi_infoframe_type type,
 				const void *frame, ssize_t len)
@@ -245,6 +265,17 @@
 	POSTING_READ(reg);
 }
 
+static bool cpt_infoframe_enabled(struct drm_encoder *encoder)
+{
+	struct drm_device *dev = encoder->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = to_intel_crtc(encoder->crtc);
+	int reg = TVIDEO_DIP_CTL(intel_crtc->pipe);
+	u32 val = I915_READ(reg);
+
+	return val & VIDEO_DIP_ENABLE;
+}
+
 static void vlv_write_infoframe(struct drm_encoder *encoder,
 				enum hdmi_infoframe_type type,
 				const void *frame, ssize_t len)
@@ -283,6 +314,17 @@
 	POSTING_READ(reg);
 }
 
+static bool vlv_infoframe_enabled(struct drm_encoder *encoder)
+{
+	struct drm_device *dev = encoder->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = to_intel_crtc(encoder->crtc);
+	int reg = VLV_TVIDEO_DIP_CTL(intel_crtc->pipe);
+	u32 val = I915_READ(reg);
+
+	return val & VIDEO_DIP_ENABLE;
+}
+
 static void hsw_write_infoframe(struct drm_encoder *encoder,
 				enum hdmi_infoframe_type type,
 				const void *frame, ssize_t len)
@@ -320,6 +362,18 @@
 	POSTING_READ(ctl_reg);
 }
 
+static bool hsw_infoframe_enabled(struct drm_encoder *encoder)
+{
+	struct drm_device *dev = encoder->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = to_intel_crtc(encoder->crtc);
+	u32 ctl_reg = HSW_TVIDEO_DIP_CTL(intel_crtc->config.cpu_transcoder);
+	u32 val = I915_READ(ctl_reg);
+
+	return val & (VIDEO_DIP_ENABLE_AVI_HSW | VIDEO_DIP_ENABLE_SPD_HSW |
+		      VIDEO_DIP_ENABLE_VS_HSW);
+}
+
 /*
  * The data we write to the DIP data buffer registers is 1 byte bigger than the
  * HDMI infoframe size because of an ECC/reserved byte at position 3 (starting
@@ -661,14 +715,6 @@
 	if (crtc->config.has_hdmi_sink)
 		hdmi_val |= HDMI_MODE_SELECT_HDMI;
 
-	if (crtc->config.has_audio) {
-		WARN_ON(!crtc->config.has_hdmi_sink);
-		DRM_DEBUG_DRIVER("Enabling HDMI audio on pipe %c\n",
-				 pipe_name(crtc->pipe));
-		hdmi_val |= SDVO_AUDIO_ENABLE;
-		intel_write_eld(&encoder->base, adjusted_mode);
-	}
-
 	if (HAS_PCH_CPT(dev))
 		hdmi_val |= SDVO_PIPE_SEL_CPT(crtc->pipe);
 	else if (IS_CHERRYVIEW(dev))
@@ -690,7 +736,7 @@
 	u32 tmp;
 
 	power_domain = intel_display_port_power_domain(encoder);
-	if (!intel_display_power_enabled(dev_priv, power_domain))
+	if (!intel_display_power_is_enabled(dev_priv, power_domain))
 		return false;
 
 	tmp = I915_READ(intel_hdmi->hdmi_reg);
@@ -732,6 +778,9 @@
 	if (tmp & HDMI_MODE_SELECT_HDMI)
 		pipe_config->has_hdmi_sink = true;
 
+	if (intel_hdmi->infoframe_enabled(&encoder->base))
+		pipe_config->has_infoframe = true;
+
 	if (tmp & SDVO_AUDIO_ENABLE)
 		pipe_config->has_audio = true;
 
@@ -791,6 +840,13 @@
 		I915_WRITE(intel_hdmi->hdmi_reg, temp);
 		POSTING_READ(intel_hdmi->hdmi_reg);
 	}
+
+	if (intel_crtc->config.has_audio) {
+		WARN_ON(!intel_crtc->config.has_hdmi_sink);
+		DRM_DEBUG_DRIVER("Enabling HDMI audio on pipe %c\n",
+				 pipe_name(intel_crtc->pipe));
+		intel_audio_codec_enable(encoder);
+	}
 }
 
 static void vlv_enable_hdmi(struct intel_encoder *encoder)
@@ -802,9 +858,13 @@
 	struct drm_device *dev = encoder->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_hdmi *intel_hdmi = enc_to_intel_hdmi(&encoder->base);
+	struct intel_crtc *crtc = to_intel_crtc(encoder->base.crtc);
 	u32 temp;
 	u32 enable_bits = SDVO_ENABLE | SDVO_AUDIO_ENABLE;
 
+	if (crtc->config.has_audio)
+		intel_audio_codec_disable(encoder);
+
 	temp = I915_READ(intel_hdmi->hdmi_reg);
 
 	/* HW workaround for IBX, we need to move the port to transcoder A
@@ -869,10 +929,15 @@
 intel_hdmi_mode_valid(struct drm_connector *connector,
 		      struct drm_display_mode *mode)
 {
-	if (mode->clock > hdmi_portclock_limit(intel_attached_hdmi(connector),
-					       true))
+	int clock = mode->clock;
+
+	if (mode->flags & DRM_MODE_FLAG_DBLCLK)
+		clock *= 2;
+
+	if (clock > hdmi_portclock_limit(intel_attached_hdmi(connector),
+					 true))
 		return MODE_CLOCK_HIGH;
-	if (mode->clock < 20000)
+	if (clock < 20000)
 		return MODE_CLOCK_LOW;
 
 	if (mode->flags & DRM_MODE_FLAG_DBLSCAN)
@@ -890,7 +955,7 @@
 	if (HAS_GMCH_DISPLAY(dev))
 		return false;
 
-	list_for_each_entry(encoder, &dev->mode_config.encoder_list, base.head) {
+	for_each_intel_encoder(dev, encoder) {
 		if (encoder->new_crtc != crtc)
 			continue;
 
@@ -917,6 +982,9 @@
 
 	pipe_config->has_hdmi_sink = intel_hdmi->has_hdmi_sink;
 
+	if (pipe_config->has_hdmi_sink)
+		pipe_config->has_infoframe = true;
+
 	if (intel_hdmi->color_range_auto) {
 		/* See CEA-861-E - 5.1 Default Encoding Parameters */
 		if (pipe_config->has_hdmi_sink &&
@@ -926,6 +994,10 @@
 			intel_hdmi->color_range = 0;
 	}
 
+	if (adjusted_mode->flags & DRM_MODE_FLAG_DBLCLK) {
+		pipe_config->pixel_multiplier = 2;
+	}
+
 	if (intel_hdmi->color_range)
 		pipe_config->limited_color_range = true;
 
@@ -967,106 +1039,121 @@
 	return true;
 }
 
-static enum drm_connector_status
-intel_hdmi_detect(struct drm_connector *connector, bool force)
+static void
+intel_hdmi_unset_edid(struct drm_connector *connector)
 {
-	struct drm_device *dev = connector->dev;
 	struct intel_hdmi *intel_hdmi = intel_attached_hdmi(connector);
-	struct intel_digital_port *intel_dig_port =
-		hdmi_to_dig_port(intel_hdmi);
-	struct intel_encoder *intel_encoder = &intel_dig_port->base;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct edid *edid;
-	enum intel_display_power_domain power_domain;
-	enum drm_connector_status status = connector_status_disconnected;
 
-	DRM_DEBUG_KMS("[CONNECTOR:%d:%s]\n",
-		      connector->base.id, connector->name);
+	intel_hdmi->has_hdmi_sink = false;
+	intel_hdmi->has_audio = false;
+	intel_hdmi->rgb_quant_range_selectable = false;
+
+	kfree(to_intel_connector(connector)->detect_edid);
+	to_intel_connector(connector)->detect_edid = NULL;
+}
+
+static bool
+intel_hdmi_update_audio(struct drm_connector *connector)
+{
+	struct intel_hdmi *intel_hdmi = intel_attached_hdmi(connector);
+	struct edid *edid = to_intel_connector(connector)->detect_edid;
+	bool has_audio, has_sink;
+	bool changed = false;
+
+	if (intel_hdmi->force_audio == HDMI_AUDIO_AUTO)
+		has_audio = drm_detect_monitor_audio(edid);
+	else
+		has_audio = intel_hdmi->force_audio == HDMI_AUDIO_ON;
+	changed |= intel_hdmi->has_audio != has_audio;
+	intel_hdmi->has_audio = has_audio;
+
+	has_sink = false;
+	if (intel_hdmi->force_audio != HDMI_AUDIO_OFF_DVI)
+		has_sink = drm_detect_hdmi_monitor(edid);
+	changed |= intel_hdmi->has_hdmi_sink != has_sink;
+	intel_hdmi->has_hdmi_sink = has_sink;
+
+	return changed;
+}
+
+static bool
+intel_hdmi_set_edid(struct drm_connector *connector)
+{
+	struct drm_i915_private *dev_priv = to_i915(connector->dev);
+	struct intel_hdmi *intel_hdmi = intel_attached_hdmi(connector);
+	struct intel_encoder *intel_encoder =
+		&hdmi_to_dig_port(intel_hdmi)->base;
+	enum intel_display_power_domain power_domain;
+	struct edid *edid;
+	bool connected = false;
 
 	power_domain = intel_display_port_power_domain(intel_encoder);
 	intel_display_power_get(dev_priv, power_domain);
 
-	intel_hdmi->has_hdmi_sink = false;
-	intel_hdmi->has_audio = false;
-	intel_hdmi->rgb_quant_range_selectable = false;
 	edid = drm_get_edid(connector,
 			    intel_gmbus_get_adapter(dev_priv,
 						    intel_hdmi->ddc_bus));
 
-	if (edid) {
-		if (edid->input & DRM_EDID_INPUT_DIGITAL) {
-			status = connector_status_connected;
-			if (intel_hdmi->force_audio != HDMI_AUDIO_OFF_DVI)
-				intel_hdmi->has_hdmi_sink =
-						drm_detect_hdmi_monitor(edid);
-			intel_hdmi->has_audio = drm_detect_monitor_audio(edid);
-			intel_hdmi->rgb_quant_range_selectable =
-				drm_rgb_quant_range_selectable(edid);
-		}
-		kfree(edid);
-	}
+	intel_display_power_put(dev_priv, power_domain);
 
-	if (status == connector_status_connected) {
-		if (intel_hdmi->force_audio != HDMI_AUDIO_AUTO)
-			intel_hdmi->has_audio =
-				(intel_hdmi->force_audio == HDMI_AUDIO_ON);
-		intel_encoder->type = INTEL_OUTPUT_HDMI;
+	to_intel_connector(connector)->detect_edid = edid;
+	if (edid && edid->input & DRM_EDID_INPUT_DIGITAL) {
+		intel_hdmi->rgb_quant_range_selectable =
+			drm_rgb_quant_range_selectable(edid);
+		intel_hdmi_update_audio(connector);
+		connected = true;
 	}
 
-	intel_display_power_put(dev_priv, power_domain);
-
-	return status;
+	return connected;
 }
 
-static int intel_hdmi_get_modes(struct drm_connector *connector)
+static enum drm_connector_status
+intel_hdmi_detect(struct drm_connector *connector, bool force)
 {
-	struct intel_encoder *intel_encoder = intel_attached_encoder(connector);
-	struct intel_hdmi *intel_hdmi = enc_to_intel_hdmi(&intel_encoder->base);
-	struct drm_i915_private *dev_priv = connector->dev->dev_private;
-	enum intel_display_power_domain power_domain;
-	int ret;
+	enum drm_connector_status status;
 
-	/* We should parse the EDID data and find out if it's an HDMI sink so
-	 * we can send audio to it.
-	 */
+	DRM_DEBUG_KMS("[CONNECTOR:%d:%s]\n",
+		      connector->base.id, connector->name);
 
-	power_domain = intel_display_port_power_domain(intel_encoder);
-	intel_display_power_get(dev_priv, power_domain);
+	intel_hdmi_unset_edid(connector);
 
-	ret = intel_ddc_get_modes(connector,
-				   intel_gmbus_get_adapter(dev_priv,
-							   intel_hdmi->ddc_bus));
+	if (intel_hdmi_set_edid(connector)) {
+		struct intel_hdmi *intel_hdmi = intel_attached_hdmi(connector);
 
-	intel_display_power_put(dev_priv, power_domain);
+		hdmi_to_dig_port(intel_hdmi)->base.type = INTEL_OUTPUT_HDMI;
+		status = connector_status_connected;
+	} else
+		status = connector_status_disconnected;
 
-	return ret;
+	return status;
 }
 
-static bool
-intel_hdmi_detect_audio(struct drm_connector *connector)
+static void
+intel_hdmi_force(struct drm_connector *connector)
 {
-	struct intel_encoder *intel_encoder = intel_attached_encoder(connector);
-	struct intel_hdmi *intel_hdmi = enc_to_intel_hdmi(&intel_encoder->base);
-	struct drm_i915_private *dev_priv = connector->dev->dev_private;
-	enum intel_display_power_domain power_domain;
-	struct edid *edid;
-	bool has_audio = false;
+	struct intel_hdmi *intel_hdmi = intel_attached_hdmi(connector);
 
-	power_domain = intel_display_port_power_domain(intel_encoder);
-	intel_display_power_get(dev_priv, power_domain);
+	DRM_DEBUG_KMS("[CONNECTOR:%d:%s]\n",
+		      connector->base.id, connector->name);
 
-	edid = drm_get_edid(connector,
-			    intel_gmbus_get_adapter(dev_priv,
-						    intel_hdmi->ddc_bus));
-	if (edid) {
-		if (edid->input & DRM_EDID_INPUT_DIGITAL)
-			has_audio = drm_detect_monitor_audio(edid);
-		kfree(edid);
-	}
+	intel_hdmi_unset_edid(connector);
 
-	intel_display_power_put(dev_priv, power_domain);
+	if (connector->status != connector_status_connected)
+		return;
+
+	intel_hdmi_set_edid(connector);
+	hdmi_to_dig_port(intel_hdmi)->base.type = INTEL_OUTPUT_HDMI;
+}
+
+static int intel_hdmi_get_modes(struct drm_connector *connector)
+{
+	struct edid *edid;
+
+	edid = to_intel_connector(connector)->detect_edid;
+	if (edid == NULL)
+		return 0;
 
-	return has_audio;
+	return intel_connector_update_modes(connector, edid);
 }
 
 static int
@@ -1086,22 +1173,14 @@
 
 	if (property == dev_priv->force_audio_property) {
 		enum hdmi_force_audio i = val;
-		bool has_audio;
 
 		if (i == intel_hdmi->force_audio)
 			return 0;
 
 		intel_hdmi->force_audio = i;
+		if (!intel_hdmi_update_audio(connector))
+			return 0;
 
-		if (i == HDMI_AUDIO_AUTO)
-			has_audio = intel_hdmi_detect_audio(connector);
-		else
-			has_audio = (i == HDMI_AUDIO_ON);
-
-		if (i == HDMI_AUDIO_OFF_DVI)
-			intel_hdmi->has_hdmi_sink = 0;
-
-		intel_hdmi->has_audio = has_audio;
 		goto done;
 	}
 
@@ -1265,6 +1344,8 @@
 	enum pipe pipe = intel_crtc->pipe;
 	u32 val;
 
+	intel_hdmi_prepare(encoder);
+
 	mutex_lock(&dev_priv->dpio_lock);
 
 	/* program left/right clock distribution */
@@ -1381,6 +1462,15 @@
 
 	mutex_lock(&dev_priv->dpio_lock);
 
+	/* allow hardware to manage TX FIFO reset source */
+	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS01_DW11(ch));
+	val &= ~DPIO_LANEDESKEW_STRAP_OVRD;
+	vlv_dpio_write(dev_priv, pipe, VLV_PCS01_DW11(ch), val);
+
+	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS23_DW11(ch));
+	val &= ~DPIO_LANEDESKEW_STRAP_OVRD;
+	vlv_dpio_write(dev_priv, pipe, VLV_PCS23_DW11(ch), val);
+
 	/* Deassert soft data lane reset*/
 	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS01_DW1(ch));
 	val |= CHV_PCS_REQ_SOFTRESET_EN;
@@ -1417,12 +1507,26 @@
 	/* Clear calc init */
 	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS01_DW10(ch));
 	val &= ~(DPIO_PCS_SWING_CALC_TX0_TX2 | DPIO_PCS_SWING_CALC_TX1_TX3);
+	val &= ~(DPIO_PCS_TX1DEEMP_MASK | DPIO_PCS_TX2DEEMP_MASK);
+	val |= DPIO_PCS_TX1DEEMP_9P5 | DPIO_PCS_TX2DEEMP_9P5;
 	vlv_dpio_write(dev_priv, pipe, VLV_PCS01_DW10(ch), val);
 
 	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS23_DW10(ch));
 	val &= ~(DPIO_PCS_SWING_CALC_TX0_TX2 | DPIO_PCS_SWING_CALC_TX1_TX3);
+	val &= ~(DPIO_PCS_TX1DEEMP_MASK | DPIO_PCS_TX2DEEMP_MASK);
+	val |= DPIO_PCS_TX1DEEMP_9P5 | DPIO_PCS_TX2DEEMP_9P5;
 	vlv_dpio_write(dev_priv, pipe, VLV_PCS23_DW10(ch), val);
 
+	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS01_DW9(ch));
+	val &= ~(DPIO_PCS_TX1MARGIN_MASK | DPIO_PCS_TX2MARGIN_MASK);
+	val |= DPIO_PCS_TX1MARGIN_000 | DPIO_PCS_TX2MARGIN_000;
+	vlv_dpio_write(dev_priv, pipe, VLV_PCS01_DW9(ch), val);
+
+	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS23_DW9(ch));
+	val &= ~(DPIO_PCS_TX1MARGIN_MASK | DPIO_PCS_TX2MARGIN_MASK);
+	val |= DPIO_PCS_TX1MARGIN_000 | DPIO_PCS_TX2MARGIN_000;
+	vlv_dpio_write(dev_priv, pipe, VLV_PCS23_DW9(ch), val);
+
 	/* FIXME: Program the support xxx V-dB */
 	/* Use 800mV-0dB */
 	for (i = 0; i < 4; i++) {
@@ -1434,8 +1538,8 @@
 
 	for (i = 0; i < 4; i++) {
 		val = vlv_dpio_read(dev_priv, pipe, CHV_TX_DW2(ch, i));
-		val &= ~DPIO_SWING_MARGIN_MASK;
-		val |= 102 << DPIO_SWING_MARGIN_SHIFT;
+		val &= ~DPIO_SWING_MARGIN000_MASK;
+		val |= 102 << DPIO_SWING_MARGIN000_SHIFT;
 		vlv_dpio_write(dev_priv, pipe, CHV_TX_DW2(ch, i), val);
 	}
 
@@ -1482,6 +1586,7 @@
 
 static void intel_hdmi_destroy(struct drm_connector *connector)
 {
+	kfree(to_intel_connector(connector)->detect_edid);
 	drm_connector_cleanup(connector);
 	kfree(connector);
 }
@@ -1489,6 +1594,7 @@
 static const struct drm_connector_funcs intel_hdmi_connector_funcs = {
 	.dpms = intel_connector_dpms,
 	.detect = intel_hdmi_detect,
+	.force = intel_hdmi_force,
 	.fill_modes = drm_helper_probe_single_connector_modes,
 	.set_property = intel_hdmi_set_property,
 	.destroy = intel_hdmi_destroy,
@@ -1567,18 +1673,23 @@
 	if (IS_VALLEYVIEW(dev)) {
 		intel_hdmi->write_infoframe = vlv_write_infoframe;
 		intel_hdmi->set_infoframes = vlv_set_infoframes;
+		intel_hdmi->infoframe_enabled = vlv_infoframe_enabled;
 	} else if (IS_G4X(dev)) {
 		intel_hdmi->write_infoframe = g4x_write_infoframe;
 		intel_hdmi->set_infoframes = g4x_set_infoframes;
+		intel_hdmi->infoframe_enabled = g4x_infoframe_enabled;
 	} else if (HAS_DDI(dev)) {
 		intel_hdmi->write_infoframe = hsw_write_infoframe;
 		intel_hdmi->set_infoframes = hsw_set_infoframes;
+		intel_hdmi->infoframe_enabled = hsw_infoframe_enabled;
 	} else if (HAS_PCH_IBX(dev)) {
 		intel_hdmi->write_infoframe = ibx_write_infoframe;
 		intel_hdmi->set_infoframes = ibx_set_infoframes;
+		intel_hdmi->infoframe_enabled = ibx_infoframe_enabled;
 	} else {
 		intel_hdmi->write_infoframe = cpt_write_infoframe;
 		intel_hdmi->set_infoframes = cpt_set_infoframes;
+		intel_hdmi->infoframe_enabled = cpt_infoframe_enabled;
 	}
 
 	if (HAS_DDI(dev))
diff -urN a/drivers/gpu/drm/i915/intel_lrc.c b/drivers/gpu/drm/i915/intel_lrc.c
--- a/drivers/gpu/drm/i915/intel_lrc.c	1969-12-31 17:00:00.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_lrc.c	2014-11-22 14:37:49.342700417 -0700
@@ -0,0 +1,788 @@
+/*
+ * Copyright  2014 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ *
+ * Authors:
+ *    Ben Widawsky <ben@bwidawsk.net>
+ *    Michel Thierry <michel.thierry@intel.com>
+ *    Thomas Daniel <thomas.daniel@intel.com>
+ *    Oscar Mateo <oscar.mateo@intel.com>
+ *
+ */
+
+/**
+ * DOC: Logical Rings, Logical Ring Contexts and Execlists
+ *
+ * Motivation:
+ * GEN8 brings an expansion of the HW contexts: "Logical Ring Contexts".
+ * These expanded contexts enable a number of new abilities, especially
+ * "Execlists" (also implemented in this file).
+ *
+ * One of the main differences with the legacy HW contexts is that logical
+ * ring contexts incorporate many more things to the context's state, like
+ * PDPs or ringbuffer control registers:
+ *
+ * The reason why PDPs are included in the context is straightforward: as
+ * PPGTTs (per-process GTTs) are actually per-context, having the PDPs
+ * contained there mean you don't need to do a ppgtt->switch_mm yourself,
+ * instead, the GPU will do it for you on the context switch.
+ *
+ * But, what about the ringbuffer control registers (head, tail, etc..)?
+ * shouldn't we just need a set of those per engine command streamer? This is
+ * where the name "Logical Rings" starts to make sense: by virtualizing the
+ * rings, the engine cs shifts to a new "ring buffer" with every context
+ * switch. When you want to submit a workload to the GPU you: A) choose your
+ * context, B) find its appropriate virtualized ring, C) write commands to it
+ * and then, finally, D) tell the GPU to switch to that context.
+ *
+ * Instead of the legacy MI_SET_CONTEXT, the way you tell the GPU to switch
+ * to a contexts is via a context execution list, ergo "Execlists".
+ *
+ * LRC implementation:
+ * Regarding the creation of contexts, we have:
+ *
+ * - One global default context.
+ * - One local default context for each opened fd.
+ * - One local extra context for each context create ioctl call.
+ *
+ * Now that ringbuffers belong per-context (and not per-engine, like before)
+ * and that contexts are uniquely tied to a given engine (and not reusable,
+ * like before) we need:
+ *
+ * - One ringbuffer per-engine inside each context.
+ * - One backing object per-engine inside each context.
+ *
+ * The global default context starts its life with these new objects fully
+ * allocated and populated. The local default context for each opened fd is
+ * more complex, because we don't know at creation time which engine is going
+ * to use them. To handle this, we have implemented a deferred creation of LR
+ * contexts:
+ *
+ * The local context starts its life as a hollow or blank holder, that only
+ * gets populated for a given engine once we receive an execbuffer. If later
+ * on we receive another execbuffer ioctl for the same context but a different
+ * engine, we allocate/populate a new ringbuffer and context backing object and
+ * so on.
+ *
+ * Finally, regarding local contexts created using the ioctl call: as they are
+ * only allowed with the render ring, we can allocate & populate them right
+ * away (no need to defer anything, at least for now).
+ *
+ * Execlists implementation:
+ * Execlists are the new method by which, on gen8+ hardware, workloads are
+ * submitted for execution (as opposed to the legacy, ringbuffer-based, method).
+ * This method works as follows:
+ *
+ * When a request is committed, its commands (the BB start and any leading or
+ * trailing commands, like the seqno breadcrumbs) are placed in the ringbuffer
+ * for the appropriate context. The tail pointer in the hardware context is not
+ * updated at this time, but instead, kept by the driver in the ringbuffer
+ * structure. A structure representing this request is added to a request queue
+ * for the appropriate engine: this structure contains a copy of the context's
+ * tail after the request was written to the ring buffer and a pointer to the
+ * context itself.
+ *
+ * If the engine's request queue was empty before the request was added, the
+ * queue is processed immediately. Otherwise the queue will be processed during
+ * a context switch interrupt. In any case, elements on the queue will get sent
+ * (in pairs) to the GPU's ExecLists Submit Port (ELSP, for short) with a
+ * globally unique 20-bits submission ID.
+ *
+ * When execution of a request completes, the GPU updates the context status
+ * buffer with a context complete event and generates a context switch interrupt.
+ * During the interrupt handling, the driver examines the events in the buffer:
+ * for each context complete event, if the announced ID matches that on the head
+ * of the request queue, then that request is retired and removed from the queue.
+ *
+ * After processing, if any requests were retired and the queue is not empty
+ * then a new execution list can be submitted. The two requests at the front of
+ * the queue are next to be submitted but since a context may not occur twice in
+ * an execution list, if subsequent requests have the same ID as the first then
+ * the two requests must be combined. This is done simply by discarding requests
+ * at the head of the queue until either only one requests is left (in which case
+ * we use a NULL second context) or the first two requests have unique IDs.
+ *
+ * By always executing the first two requests in the queue the driver ensures
+ * that the GPU is kept as busy as possible. In the case where a single context
+ * completes but a second context is still executing, the request for this second
+ * context will be at the head of the queue when we remove the first one. This
+ * request will then be resubmitted along with a new request for a different context,
+ * which will cause the hardware to continue executing the second request and queue
+ * the new request (the GPU detects the condition of a context getting preempted
+ * with the same context and optimizes the context switch flow by not doing
+ * preemption, but just sampling the new tail pointer).
+ *
+ */
+
+#include <drm/drmP.h>
+#include <drm/i915_drm.h>
+#include "i915_drv.h"
+
+#define GEN9_LR_CONTEXT_RENDER_SIZE (22 * PAGE_SIZE)
+#define GEN8_LR_CONTEXT_RENDER_SIZE (20 * PAGE_SIZE)
+#define GEN8_LR_CONTEXT_OTHER_SIZE (2 * PAGE_SIZE)
+
+#define RING_EXECLIST_QFULL		(1 << 0x2)
+#define RING_EXECLIST1_VALID		(1 << 0x3)
+#define RING_EXECLIST0_VALID		(1 << 0x4)
+#define RING_EXECLIST_ACTIVE_STATUS	(3 << 0xE)
+#define RING_EXECLIST1_ACTIVE		(1 << 0x11)
+#define RING_EXECLIST0_ACTIVE		(1 << 0x12)
+
+#define GEN8_CTX_STATUS_IDLE_ACTIVE	(1 << 0)
+#define GEN8_CTX_STATUS_PREEMPTED	(1 << 1)
+#define GEN8_CTX_STATUS_ELEMENT_SWITCH	(1 << 2)
+#define GEN8_CTX_STATUS_ACTIVE_IDLE	(1 << 3)
+#define GEN8_CTX_STATUS_COMPLETE	(1 << 4)
+#define GEN8_CTX_STATUS_LITE_RESTORE	(1 << 15)
+
+#define CTX_LRI_HEADER_0		0x01
+#define CTX_CONTEXT_CONTROL		0x02
+#define CTX_RING_HEAD			0x04
+#define CTX_RING_TAIL			0x06
+#define CTX_RING_BUFFER_START		0x08
+#define CTX_RING_BUFFER_CONTROL		0x0a
+#define CTX_BB_HEAD_U			0x0c
+#define CTX_BB_HEAD_L			0x0e
+#define CTX_BB_STATE			0x10
+#define CTX_SECOND_BB_HEAD_U		0x12
+#define CTX_SECOND_BB_HEAD_L		0x14
+#define CTX_SECOND_BB_STATE		0x16
+#define CTX_BB_PER_CTX_PTR		0x18
+#define CTX_RCS_INDIRECT_CTX		0x1a
+#define CTX_RCS_INDIRECT_CTX_OFFSET	0x1c
+#define CTX_LRI_HEADER_1		0x21
+#define CTX_CTX_TIMESTAMP		0x22
+#define CTX_PDP3_UDW			0x24
+#define CTX_PDP3_LDW			0x26
+#define CTX_PDP2_UDW			0x28
+#define CTX_PDP2_LDW			0x2a
+#define CTX_PDP1_UDW			0x2c
+#define CTX_PDP1_LDW			0x2e
+#define CTX_PDP0_UDW			0x30
+#define CTX_PDP0_LDW			0x32
+#define CTX_LRI_HEADER_2		0x41
+#define CTX_R_PWR_CLK_STATE		0x42
+#define CTX_GPGPU_CSR_BASE_ADDRESS	0x44
+
+#define GEN8_CTX_VALID (1<<0)
+#define GEN8_CTX_FORCE_PD_RESTORE (1<<1)
+#define GEN8_CTX_FORCE_RESTORE (1<<2)
+#define GEN8_CTX_L3LLC_COHERENT (1<<5)
+#define GEN8_CTX_PRIVILEGE (1<<8)
+enum {
+	ADVANCED_CONTEXT = 0,
+	LEGACY_CONTEXT,
+	ADVANCED_AD_CONTEXT,
+	LEGACY_64B_CONTEXT
+};
+#define GEN8_CTX_MODE_SHIFT 3
+enum {
+	FAULT_AND_HANG = 0,
+	FAULT_AND_HALT, /* Debug only */
+	FAULT_AND_STREAM,
+	FAULT_AND_CONTINUE /* Unsupported */
+};
+#define GEN8_CTX_ID_SHIFT 32
+
+static u32 execlists_ctx_descriptor(struct drm_i915_gem_object *ctx)
+{
+	u32 desc;
+
+	desc = GEN8_CTX_VALID;
+	desc |= LEGACY_CONTEXT << GEN8_CTX_MODE_SHIFT;
+	desc |= GEN8_CTX_L3LLC_COHERENT;
+	desc |= GEN8_CTX_PRIVILEGE;
+	desc |= i915_gem_obj_ggtt_offset(ctx);
+
+	/* TODO: WaDisableLiteRestore when we start using semaphore
+	 * signalling between Command Streamers */
+	/* desc |= GEN8_CTX_FORCE_RESTORE; */
+
+	return desc;
+}
+
+static u32 *ctx_get_regs(struct drm_i915_gem_object *obj)
+{
+	int ret;
+
+	/* The second page of the context object contains some fields which
+	 * must be set up prior to the first execution.
+	 */
+
+	ret = i915_gem_object_get_pages(obj);
+	if (ret)
+		return ERR_PTR(ret);
+
+	ret = i915_gem_object_set_to_cpu_domain(obj, true);
+	if (ret)
+		return ERR_PTR(ret);
+
+	return kmap_atomic(i915_gem_object_get_page(obj, 1));
+}
+
+static u32 execlists_ctx_write_tail(const struct i915_gem_request *rq)
+{
+	struct drm_i915_gem_object *obj = rq->ctx->ring[rq->engine->id].state;
+	u32 *regs;
+
+	regs = kmap_atomic(i915_gem_object_get_page(obj, 1));
+	regs[CTX_RING_TAIL+1] = rq->tail;
+	kunmap_atomic(regs);
+
+	return execlists_ctx_descriptor(obj);
+}
+
+static void execlists_submit_pair(struct intel_engine_cs *engine,
+				  const struct i915_gem_request *rq[2])
+{
+	struct drm_i915_private *dev_priv = engine->i915;
+	u32 desc[4];
+
+	desc[2] = execlists_ctx_write_tail(rq[0]);
+	desc[3] = rq[0]->tag;
+
+	if (rq[1]) {
+		desc[0] = execlists_ctx_write_tail(rq[1]);
+		desc[1] = rq[1]->tag;
+	} else
+		desc[1] = desc[0] = 0;
+
+	gen6_gt_force_wake_get(dev_priv, engine->power_domains);
+	/* XXX: You must always write both descriptors in the order below. */
+	I915_WRITE(RING_ELSP(engine), desc[1]);
+	I915_WRITE(RING_ELSP(engine), desc[0]);
+	I915_WRITE(RING_ELSP(engine), desc[3]);
+	/* The context is automatically loaded after the following */
+	I915_WRITE(RING_ELSP(engine), desc[2]);
+
+	/* ELSP is a wo register, so use another nearby reg for posting instead */
+	POSTING_READ(RING_EXECLIST_STATUS(engine));
+	gen6_gt_force_wake_put(dev_priv, engine->power_domains);
+}
+
+static u16 next_tag(struct intel_engine_cs *engine)
+{
+	/* status tags are limited to 20b, so we use a u16 for convenience */
+	if (++engine->next_tag == 0)
+		++engine->next_tag;
+	WARN_ON((s16)(engine->next_tag - engine->tag) < 0);
+	return engine->next_tag;
+}
+
+static void execlists_submit(struct intel_engine_cs *engine)
+{
+	const struct i915_gem_request *rq[2] = {};
+	int i = 0;
+
+	assert_spin_locked(&engine->irqlock);
+
+	/* Try to submit requests in pairs */
+	while (!list_empty(&engine->pending)) {
+		struct i915_gem_request *next;
+
+		next = list_first_entry(&engine->pending,
+					typeof(*next),
+					engine_link);
+
+		if (rq[i] == NULL) {
+new_slot:
+			next->tag = next_tag(engine);
+			rq[i] = next;
+		} else if (rq[i]->ctx == next->ctx) {
+			/* Same ctx: ignore first request, as second request
+			 * will update tail past first request's workload */
+			next->tag = rq[i]->tag;
+			rq[i] = next;
+		} else {
+			if (++i == ARRAY_SIZE(rq))
+				break;
+
+			goto new_slot;
+		}
+
+		/* Move to requests is staged via the submitted list
+		 * so that we can keep the main request list out of
+		 * the spinlock coverage.
+		 */
+		list_move_tail(&next->engine_link, &engine->submitted);
+	}
+
+	if (rq[0] == NULL)
+		return;
+
+	execlists_submit_pair(engine, rq);
+
+	engine->execlists_submitted++;
+	if (rq[1])
+		engine->execlists_submitted++;
+}
+
+/**
+ * intel_execlists_handle_ctx_events() - handle Context Switch interrupts
+ * @ring: Engine Command Streamer to handle.
+ *
+ * Check the unread Context Status Buffers and manage the submission of new
+ * contexts to the ELSP accordingly.
+ */
+void intel_execlists_irq_handler(struct intel_engine_cs *engine)
+{
+	struct drm_i915_private *dev_priv = engine->i915;
+	unsigned long flags;
+	u8 read_pointer;
+	u8 write_pointer;
+
+	read_pointer = engine->next_context_status_buffer;
+	write_pointer = I915_READ(RING_CONTEXT_STATUS_PTR(engine)) & 0x07;
+	if (read_pointer > write_pointer)
+		write_pointer += 6;
+
+	spin_lock_irqsave(&engine->irqlock, flags);
+
+	while (read_pointer++ < write_pointer) {
+		u32 reg = (RING_CONTEXT_STATUS_BUF(engine) +
+			   (read_pointer % 6) * 8);
+		u32 status = I915_READ(reg);
+
+		if (status & GEN8_CTX_STATUS_PREEMPTED) {
+			if (status & GEN8_CTX_STATUS_LITE_RESTORE)
+				WARN_ONCE(1, "Lite Restored request removed from queue\n");
+			else
+				WARN_ONCE(1, "Preemption without Lite Restore\n");
+		}
+
+		if (status & (GEN8_CTX_STATUS_ACTIVE_IDLE | GEN8_CTX_STATUS_ELEMENT_SWITCH)) {
+			engine->tag = I915_READ(reg + 4);
+			engine->execlists_submitted--;
+		}
+	}
+
+	if (engine->execlists_submitted < 2)
+		execlists_submit(engine);
+
+	spin_unlock_irqrestore(&engine->irqlock, flags);
+
+	engine->next_context_status_buffer = write_pointer % 6;
+	I915_WRITE(RING_CONTEXT_STATUS_PTR(engine),
+		   ((u32)engine->next_context_status_buffer & 0x07) << 8);
+}
+
+static u32 get_lr_context_size(const struct intel_engine_cs *engine)
+{
+	if (engine->id == RCS) {
+		if (INTEL_INFO(engine->i915)->gen >= 9)
+			return ALIGN(GEN9_LR_CONTEXT_RENDER_SIZE,4096);
+		else
+			return ALIGN(GEN8_LR_CONTEXT_RENDER_SIZE, 4096);
+	} else
+		return ALIGN(GEN8_LR_CONTEXT_OTHER_SIZE, 4096);
+}
+
+static int
+populate_lr_context(struct intel_context *ctx,
+		    struct intel_engine_cs *engine,
+		    struct drm_i915_gem_object *state,
+		    struct intel_ringbuffer *ring)
+{
+	struct i915_hw_ppgtt *ppgtt;
+	u32 *regs;
+
+	/* The second page of the context object contains some fields which must
+	 * be set up prior to the first execution. */
+	regs = ctx_get_regs(state);
+	if (IS_ERR(regs))
+		return PTR_ERR(regs);
+
+	/* A context is actually a big batch buffer with several MI_LOAD_REGISTER_IMM
+	 * commands followed by (reg, value) pairs. The values we are setting here are
+	 * only for the first context restore: on a subsequent save, the GPU will
+	 * recreate this batchbuffer with new values (including all the missing
+	 * MI_LOAD_REGISTER_IMM commands that we are not initializing here). */
+	if (engine->id == RCS)
+		regs[CTX_LRI_HEADER_0] = MI_LOAD_REGISTER_IMM(14);
+	else
+		regs[CTX_LRI_HEADER_0] = MI_LOAD_REGISTER_IMM(11);
+	regs[CTX_LRI_HEADER_0] |= MI_LRI_FORCE_POSTED;
+
+	regs[CTX_CONTEXT_CONTROL] = RING_CONTEXT_CONTROL(engine);
+	regs[CTX_CONTEXT_CONTROL+1] =
+			_MASKED_BIT_ENABLE((1<<3) | MI_RESTORE_INHIBIT);
+
+	regs[CTX_RING_HEAD] = RING_HEAD(engine->mmio_base);
+	regs[CTX_RING_HEAD+1] = 0;
+	regs[CTX_RING_TAIL] = RING_TAIL(engine->mmio_base);
+	regs[CTX_RING_TAIL+1] = 0;
+	regs[CTX_RING_BUFFER_START] = RING_START(engine->mmio_base);
+	regs[CTX_RING_BUFFER_CONTROL] = RING_CTL(engine->mmio_base);
+	regs[CTX_RING_BUFFER_CONTROL+1] =
+			((ring->size - PAGE_SIZE) & RING_NR_PAGES) | RING_VALID;
+
+	regs[CTX_BB_HEAD_U] = engine->mmio_base + 0x168;
+	regs[CTX_BB_HEAD_U+1] = 0;
+	regs[CTX_BB_HEAD_L] = engine->mmio_base + 0x140;
+	regs[CTX_BB_HEAD_L+1] = 0;
+	regs[CTX_BB_STATE] = engine->mmio_base + 0x110;
+	regs[CTX_BB_STATE+1] = (1<<5);
+
+	regs[CTX_SECOND_BB_HEAD_U] = engine->mmio_base + 0x11c;
+	regs[CTX_SECOND_BB_HEAD_U+1] = 0;
+	regs[CTX_SECOND_BB_HEAD_L] = engine->mmio_base + 0x114;
+	regs[CTX_SECOND_BB_HEAD_L+1] = 0;
+	regs[CTX_SECOND_BB_STATE] = engine->mmio_base + 0x118;
+	regs[CTX_SECOND_BB_STATE+1] = 0;
+
+	if (engine->id == RCS) {
+		/* TODO: according to BSpec, the register state context
+		 * for CHV does not have these. OTOH, these registers do
+		 * exist in CHV. I'm waiting for a clarification */
+		regs[CTX_BB_PER_CTX_PTR] = engine->mmio_base + 0x1c0;
+		regs[CTX_BB_PER_CTX_PTR+1] = 0;
+		regs[CTX_RCS_INDIRECT_CTX] = engine->mmio_base + 0x1c4;
+		regs[CTX_RCS_INDIRECT_CTX+1] = 0;
+		regs[CTX_RCS_INDIRECT_CTX_OFFSET] = engine->mmio_base + 0x1c8;
+		regs[CTX_RCS_INDIRECT_CTX_OFFSET+1] = 0;
+	}
+
+	regs[CTX_LRI_HEADER_1] = MI_LOAD_REGISTER_IMM(9);
+	regs[CTX_LRI_HEADER_1] |= MI_LRI_FORCE_POSTED;
+	regs[CTX_CTX_TIMESTAMP] = engine->mmio_base + 0x3a8;
+	regs[CTX_CTX_TIMESTAMP+1] = 0;
+
+	regs[CTX_PDP3_UDW] = GEN8_RING_PDP_UDW(engine, 3);
+	regs[CTX_PDP3_LDW] = GEN8_RING_PDP_LDW(engine, 3);
+	regs[CTX_PDP2_UDW] = GEN8_RING_PDP_UDW(engine, 2);
+	regs[CTX_PDP2_LDW] = GEN8_RING_PDP_LDW(engine, 2);
+	regs[CTX_PDP1_UDW] = GEN8_RING_PDP_UDW(engine, 1);
+	regs[CTX_PDP1_LDW] = GEN8_RING_PDP_LDW(engine, 1);
+	regs[CTX_PDP0_UDW] = GEN8_RING_PDP_UDW(engine, 0);
+	regs[CTX_PDP0_LDW] = GEN8_RING_PDP_LDW(engine, 0);
+
+	ppgtt = ctx->ppgtt ?: engine->i915->mm.aliasing_ppgtt;
+	regs[CTX_PDP3_UDW+1] = upper_32_bits(ppgtt->pd_dma_addr[3]);
+	regs[CTX_PDP3_LDW+1] = lower_32_bits(ppgtt->pd_dma_addr[3]);
+	regs[CTX_PDP2_UDW+1] = upper_32_bits(ppgtt->pd_dma_addr[2]);
+	regs[CTX_PDP2_LDW+1] = lower_32_bits(ppgtt->pd_dma_addr[2]);
+	regs[CTX_PDP1_UDW+1] = upper_32_bits(ppgtt->pd_dma_addr[1]);
+	regs[CTX_PDP1_LDW+1] = lower_32_bits(ppgtt->pd_dma_addr[1]);
+	regs[CTX_PDP0_UDW+1] = upper_32_bits(ppgtt->pd_dma_addr[0]);
+	regs[CTX_PDP0_LDW+1] = lower_32_bits(ppgtt->pd_dma_addr[0]);
+
+	if (engine->id == RCS) {
+		regs[CTX_LRI_HEADER_2] = MI_LOAD_REGISTER_IMM(1);
+		regs[CTX_R_PWR_CLK_STATE] = 0x20c8;
+		regs[CTX_R_PWR_CLK_STATE+1] = 0;
+	}
+
+	kunmap_atomic(regs);
+
+	return 0;
+}
+
+static struct intel_ringbuffer *
+execlists_get_ring(struct intel_engine_cs *engine,
+		   struct intel_context *ctx)
+{
+	struct drm_i915_gem_object *state;
+	struct intel_ringbuffer *ring;
+	int ret;
+
+	if (ctx->ring[engine->id].ring)
+		return ctx->ring[engine->id].ring;
+
+	ring = intel_engine_alloc_ring(engine, ctx, 32 * PAGE_SIZE);
+	if (IS_ERR(ring)) {
+		ret = PTR_ERR(ring);
+		goto err;
+	}
+
+	state = i915_gem_alloc_object(engine->i915->dev,
+				      get_lr_context_size(engine));
+	if (IS_ERR(state)) {
+		ret = PTR_ERR(state);
+		goto err_ring;
+	}
+
+	ret = populate_lr_context(ctx, engine, state, ring);
+	if (ret)
+		goto err_ctx;
+
+	/* The status page is offset 0 from the context object in LRCs. */
+	if (ctx == engine->default_context) {
+		ret = i915_gem_object_ggtt_pin(state, 0, 0);
+		if (ret)
+			goto err_ctx;
+
+		engine->status_page.obj = state;
+		drm_gem_object_reference(&state->base);
+
+		engine->status_page.gfx_addr = i915_gem_obj_ggtt_offset(state);
+		engine->status_page.page_addr = kmap(i915_gem_object_get_page(state, 0));
+	}
+
+	ctx->ring[engine->id].state = state;
+	ctx->ring[engine->id].ring = ring;
+	return ring;
+
+err_ctx:
+	drm_gem_object_unreference(&state->base);
+err_ring:
+	intel_ring_free(ring);
+err:
+	DRM_DEBUG_DRIVER("Failed to allocate ring %s %s: %d\n",
+			 engine->name,
+			 ctx == engine->default_context ? "(default)" : "",
+			 ret);
+	return ERR_PTR(ret);
+}
+
+static struct intel_ringbuffer *
+execlists_pin_context(struct intel_engine_cs *engine,
+		      struct intel_context *ctx)
+{
+	struct intel_engine_context *hw = &ctx->ring[engine->id];
+	struct intel_ringbuffer *ring;
+	u32 ggtt_offset;
+	int ret;
+
+	ring = execlists_get_ring(engine, ctx);
+	if (IS_ERR(ring))
+		return ring;
+
+	ret = i915_gem_object_ggtt_pin(hw->state, 0, 0);
+	if (ret)
+		goto err;
+
+	if (ctx->ppgtt && ctx->ppgtt->state) {
+		ret = i915_gem_object_ggtt_pin(ctx->ppgtt->state, 0, 0);
+		if (ret)
+			goto err_unpin_ctx;
+	}
+
+	ret = i915_gem_object_ggtt_pin(ring->obj, 0, 0);
+	if (ret)
+		goto err_unpin_mm;
+
+	ggtt_offset = i915_gem_obj_ggtt_offset(ring->obj);
+	if (ring->ggtt_offset != ggtt_offset) {
+		u32 *regs = ctx_get_regs(hw->state);
+		if (IS_ERR(regs)) {
+			ret = PTR_ERR(regs);
+			goto err_unpin_ring;
+		}
+
+		regs[CTX_RING_BUFFER_START+1] = ggtt_offset;
+		kunmap_atomic(regs);
+
+		ring->ggtt_offset = ggtt_offset;
+	}
+	return ring;
+
+err_unpin_ring:
+	i915_gem_object_ggtt_unpin(ring->obj);
+err_unpin_mm:
+	if (ctx->ppgtt && ctx->ppgtt->state)
+		i915_gem_object_ggtt_unpin(ctx->ppgtt->state);
+err_unpin_ctx:
+	i915_gem_object_ggtt_unpin(hw->state);
+err:
+	return ERR_PTR(ret);
+}
+
+static void
+rq_add_ggtt(struct i915_gem_request *rq, struct drm_i915_gem_object *obj)
+{
+	obj->base.pending_read_domains = I915_GEM_DOMAIN_INSTRUCTION;
+	/* obj is kept alive until the next request by its active ref */
+	drm_gem_object_reference(&obj->base);
+	i915_request_add_vma(rq, i915_gem_obj_get_ggtt(obj), 0);
+}
+
+static void execlists_add_context(struct i915_gem_request *rq,
+				  struct intel_context *ctx)
+{
+	rq_add_ggtt(rq, ctx->ring[rq->engine->id].ring->obj);
+	if (ctx->ppgtt && ctx->ppgtt->state)
+		rq_add_ggtt(rq, ctx->ppgtt->state);
+	rq_add_ggtt(rq, ctx->ring[rq->engine->id].state);
+}
+
+static void
+execlists_unpin_context(struct intel_engine_cs *engine,
+			struct intel_context *ctx)
+{
+	i915_gem_object_ggtt_unpin(ctx->ring[engine->id].ring->obj);
+	if (ctx->ppgtt && ctx->ppgtt->state)
+		i915_gem_object_ggtt_unpin(ctx->ppgtt->state);
+	i915_gem_object_ggtt_unpin(ctx->ring[engine->id].state);
+}
+
+static void execlists_free_context(struct intel_engine_cs *engine,
+				   struct intel_context *ctx)
+{
+	if (ctx->ring[engine->id].ring)
+		intel_ring_free(ctx->ring[engine->id].ring);
+	if (ctx->ring[engine->id].state)
+		drm_gem_object_unreference(&ctx->ring[engine->id].state->base);
+}
+
+static int execlists_add_request(struct i915_gem_request *rq)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&rq->engine->irqlock, flags);
+
+	list_add_tail(&rq->engine_link, &rq->engine->pending);
+	if (rq->engine->execlists_submitted < 2)
+		execlists_submit(rq->engine);
+
+	spin_unlock_irqrestore(&rq->engine->irqlock, flags);
+
+	return 0;
+}
+
+static bool execlists_rq_is_complete(struct i915_gem_request *rq)
+{
+	return (s16)(rq->engine->tag - rq->tag) >= 0;
+}
+
+static int execlists_suspend(struct intel_engine_cs *engine)
+{
+	struct drm_i915_private *dev_priv = engine->i915;
+	unsigned long flags;
+
+	/* disable submitting more requests until resume */
+	spin_lock_irqsave(&engine->irqlock, flags);
+	engine->execlists_submitted = ~0;
+	spin_unlock_irqrestore(&engine->irqlock, flags);
+
+	I915_WRITE(RING_MODE_GEN7(engine),
+		   _MASKED_BIT_ENABLE(GFX_REPLAY_MODE) |
+		   _MASKED_BIT_DISABLE(GFX_RUN_LIST_ENABLE));
+	POSTING_READ(RING_MODE_GEN7(engine));
+	DRM_DEBUG_DRIVER("Execlists disabled for %s\n", engine->name);
+
+	return 0;
+}
+
+static int execlists_resume(struct intel_engine_cs *engine)
+{
+	struct drm_i915_private *dev_priv = engine->i915;
+	unsigned long flags;
+
+	I915_WRITE(RING_HWS_PGA(engine->mmio_base),
+		   engine->status_page.gfx_addr);
+
+	/* XXX */
+	I915_WRITE(RING_HWSTAM(engine->mmio_base), 0xffffffff);
+	I915_WRITE(INSTPM, _MASKED_BIT_ENABLE(INSTPM_FORCE_ORDERING));
+
+	/* We need to disable the AsyncFlip performance optimisations in order
+	 * to use MI_WAIT_FOR_EVENT within the CS. It should already be
+	 * programmed to '1' on all products.
+	 *
+	 * WaDisableAsyncFlipPerfMode:bdw
+	 */
+	I915_WRITE(MI_MODE, _MASKED_BIT_ENABLE(ASYNC_FLIP_PERF_DISABLE));
+
+	I915_WRITE(RING_MODE_GEN7(engine),
+		   _MASKED_BIT_DISABLE(GFX_REPLAY_MODE) |
+		   _MASKED_BIT_ENABLE(GFX_RUN_LIST_ENABLE));
+	POSTING_READ(RING_MODE_GEN7(engine));
+	DRM_DEBUG_DRIVER("Execlists enabled for %s\n", engine->name);
+
+	spin_lock_irqsave(&engine->irqlock, flags);
+	engine->execlists_submitted = 0;
+	execlists_submit(engine);
+	spin_unlock_irqrestore(&engine->irqlock, flags);
+
+	return 0;
+}
+
+static void execlists_retire(struct intel_engine_cs *engine,
+			     u32 seqno)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&engine->irqlock, flags);
+	list_splice_tail_init(&engine->submitted, &engine->requests);
+	spin_unlock_irqrestore(&engine->irqlock, flags);
+}
+
+static void execlists_reset(struct intel_engine_cs *engine)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&engine->irqlock, flags);
+	list_splice_tail_init(&engine->pending, &engine->submitted);
+	list_splice_tail_init(&engine->submitted, &engine->requests);
+	spin_unlock_irqrestore(&engine->irqlock, flags);
+}
+
+static bool enable_execlists(struct drm_i915_private *dev_priv)
+{
+	if (!HAS_LOGICAL_RING_CONTEXTS(dev_priv) ||
+	    !USES_PPGTT(dev_priv))
+		return false;
+
+	return i915_module.enable_execlists;
+}
+
+static const int gen8_irq_shift[] = {
+	[RCS] = GEN8_RCS_IRQ_SHIFT,
+	[VCS] = GEN8_VCS1_IRQ_SHIFT,
+	[BCS] = GEN8_BCS_IRQ_SHIFT,
+	[VECS] = GEN8_VECS_IRQ_SHIFT,
+	[VCS2] = GEN8_VCS2_IRQ_SHIFT,
+};
+
+int intel_engine_enable_execlists(struct intel_engine_cs *engine)
+{
+	if (!enable_execlists(engine->i915))
+		return 0;
+
+	if (WARN_ON(!IS_GEN8(engine->i915)))
+		return 0;
+
+	engine->irq_keep_mask |=
+		GT_CONTEXT_SWITCH_INTERRUPT << gen8_irq_shift[engine->id];
+
+	engine->pin_context = execlists_pin_context;
+	engine->add_context = execlists_add_context;
+	engine->unpin_context = execlists_unpin_context;
+	engine->free_context = execlists_free_context;
+
+	engine->add_request = execlists_add_request;
+	engine->is_complete = execlists_rq_is_complete;
+
+	/* Disable semaphores until further notice */
+	engine->semaphore.wait = NULL;
+
+	engine->suspend = execlists_suspend;
+	engine->resume = execlists_resume;
+	engine->reset = execlists_reset;
+	engine->retire = execlists_retire;
+
+	/* start suspended */
+	engine->execlists_enabled = true;
+	engine->execlists_submitted = ~0;
+
+	return 0;
+}
diff -urN a/drivers/gpu/drm/i915/intel_lrc.h b/drivers/gpu/drm/i915/intel_lrc.h
--- a/drivers/gpu/drm/i915/intel_lrc.h	1969-12-31 17:00:00.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_lrc.h	2014-11-22 14:37:49.342700417 -0700
@@ -0,0 +1,38 @@
+/*
+ * Copyright  2014 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+#ifndef _INTEL_LRC_H_
+#define _INTEL_LRC_H_
+
+/* Execlists regs */
+#define RING_ELSP(ring)			((ring)->mmio_base+0x230)
+#define RING_EXECLIST_STATUS(ring)	((ring)->mmio_base+0x234)
+#define RING_CONTEXT_CONTROL(ring)	((ring)->mmio_base+0x244)
+#define RING_CONTEXT_STATUS_BUF(ring)	((ring)->mmio_base+0x370)
+#define RING_CONTEXT_STATUS_PTR(ring)	((ring)->mmio_base+0x3a0)
+
+/* Execlists */
+int intel_engine_enable_execlists(struct intel_engine_cs *engine);
+void intel_execlists_irq_handler(struct intel_engine_cs *engine);
+
+#endif /* _INTEL_LRC_H_ */
diff -urN a/drivers/gpu/drm/i915/intel_lvds.c b/drivers/gpu/drm/i915/intel_lvds.c
--- a/drivers/gpu/drm/i915/intel_lvds.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_lvds.c	2014-11-22 14:37:49.342700417 -0700
@@ -76,7 +76,7 @@
 	u32 tmp;
 
 	power_domain = intel_display_port_power_domain(encoder);
-	if (!intel_display_power_enabled(dev_priv, power_domain))
+	if (!intel_display_power_is_enabled(dev_priv, power_domain))
 		return false;
 
 	tmp = I915_READ(lvds_encoder->reg);
@@ -823,8 +823,7 @@
 	struct intel_encoder *encoder;
 	struct intel_lvds_encoder *lvds_encoder;
 
-	list_for_each_entry(encoder, &dev->mode_config.encoder_list,
-			    base.head) {
+	for_each_intel_encoder(dev, encoder) {
 		if (encoder->type == INTEL_OUTPUT_LVDS) {
 			lvds_encoder = to_lvds_encoder(&encoder->base);
 
@@ -842,8 +841,8 @@
 	struct drm_i915_private *dev_priv = dev->dev_private;
 
 	/* use the module option value if specified */
-	if (i915.lvds_channel_mode > 0)
-		return i915.lvds_channel_mode == 2;
+	if (i915_module.lvds_channel_mode > 0)
+		return i915_module.lvds_channel_mode == 2;
 
 	if (dmi_check_system(intel_dual_link_lvds))
 		return true;
@@ -976,10 +975,10 @@
 	}
 
 	/* create the scaling mode property */
-	drm_mode_create_scaling_mode_property(dev);
-	drm_object_attach_property(&connector->base,
-				      dev->mode_config.scaling_mode_property,
-				      DRM_MODE_SCALE_ASPECT);
+	if (drm_mode_create_scaling_mode_property(dev) == 0)
+		drm_object_attach_property(&connector->base,
+					   dev->mode_config.scaling_mode_property,
+					   DRM_MODE_SCALE_ASPECT);
 	intel_connector->panel.fitting_mode = DRM_MODE_SCALE_ASPECT;
 	/*
 	 * LVDS discovery:
@@ -1032,7 +1031,7 @@
 					intel_find_panel_downclock(dev,
 					fixed_mode, connector);
 				if (downclock_mode != NULL &&
-					i915.lvds_downclock) {
+					i915_module.lvds_downclock) {
 					/* We found the downclock for LVDS. */
 					dev_priv->lvds_downclock_avail = true;
 					dev_priv->lvds_downclock =
@@ -1117,7 +1116,7 @@
 	drm_connector_register(connector);
 
 	intel_panel_init(&intel_connector->panel, fixed_mode, downclock_mode);
-	intel_panel_setup_backlight(connector);
+	intel_panel_setup_backlight(connector, INVALID_PIPE);
 
 	return;
 
diff -urN a/drivers/gpu/drm/i915/intel_overlay.c b/drivers/gpu/drm/i915/intel_overlay.c
--- a/drivers/gpu/drm/i915/intel_overlay.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_overlay.c	2014-11-22 14:37:49.342700417 -0700
@@ -175,14 +175,15 @@
 	int active;
 	int pfit_active;
 	u32 pfit_vscale_ratio; /* shifted-point number, (1<<12) == 1.0 */
-	u32 color_key;
+	u32 color_key:24;
+	u32 color_key_enabled:1;
 	u32 brightness, contrast, saturation;
 	u32 old_xscale, old_yscale;
 	/* register access */
 	u32 flip_addr;
 	struct drm_i915_gem_object *reg_bo;
 	/* flip handling */
-	uint32_t last_flip_req;
+	struct i915_gem_request *flip_request;
 	void (*flip_tail)(struct intel_overlay *);
 };
 
@@ -208,53 +209,85 @@
 		io_mapping_unmap(regs);
 }
 
-static int intel_overlay_do_wait_request(struct intel_overlay *overlay,
-					 void (*tail)(struct intel_overlay *))
+/* recover from an interruption due to a signal
+ * We have to be careful not to repeat work forever an make forward progess. */
+static int intel_overlay_recover_from_interrupt(struct intel_overlay *overlay)
 {
-	struct drm_device *dev = overlay->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring = &dev_priv->ring[RCS];
 	int ret;
 
-	BUG_ON(overlay->last_flip_req);
-	ret = i915_add_request(ring, &overlay->last_flip_req);
-	if (ret)
-		return ret;
+	if (overlay->flip_request == NULL)
+		return 0;
 
-	overlay->flip_tail = tail;
-	ret = i915_wait_seqno(ring, overlay->last_flip_req);
+	ret = i915_request_wait(overlay->flip_request);
 	if (ret)
 		return ret;
-	i915_gem_retire_requests(dev);
 
-	overlay->last_flip_req = 0;
+	i915_request_put(overlay->flip_request);
+	overlay->flip_request = NULL;
+
+	i915_gem_retire_requests(overlay->dev);
+
+	if (overlay->flip_tail)
+		overlay->flip_tail(overlay);
+
 	return 0;
 }
 
+static int intel_overlay_add_request(struct intel_overlay *overlay,
+				     struct i915_gem_request *rq,
+				     void (*tail)(struct intel_overlay *))
+{
+	BUG_ON(overlay->flip_request);
+	overlay->flip_request = rq;
+	overlay->flip_tail = tail;
+
+	return i915_request_commit(rq);
+}
+
+static int intel_overlay_do_wait_request(struct intel_overlay *overlay,
+					 struct i915_gem_request *rq,
+					 void (*tail)(struct intel_overlay *))
+{
+	intel_overlay_add_request(overlay, rq, tail);
+	return intel_overlay_recover_from_interrupt(overlay);
+}
+
+static struct i915_gem_request *
+intel_overlay_alloc_request(struct intel_overlay *overlay)
+{
+	struct intel_engine_cs *rcs = RCS_ENGINE(overlay->dev);
+	return i915_request_create(rcs->default_context, rcs);
+}
+
 /* overlay needs to be disable in OCMD reg */
 static int intel_overlay_on(struct intel_overlay *overlay)
 {
 	struct drm_device *dev = overlay->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring = &dev_priv->ring[RCS];
-	int ret;
+	struct i915_gem_request *rq;
+	struct intel_ringbuffer *ring;
 
 	BUG_ON(overlay->active);
 	overlay->active = 1;
 
 	WARN_ON(IS_I830(dev) && !(dev_priv->quirks & QUIRK_PIPEA_FORCE));
 
-	ret = intel_ring_begin(ring, 4);
-	if (ret)
-		return ret;
+	rq = intel_overlay_alloc_request(overlay);
+	if (IS_ERR(rq))
+		return PTR_ERR(rq);
+
+	ring = intel_ring_begin(rq, 3);
+	if (IS_ERR(ring)) {
+		i915_request_put(rq);
+		return PTR_ERR(ring);
+	}
 
 	intel_ring_emit(ring, MI_OVERLAY_FLIP | MI_OVERLAY_ON);
 	intel_ring_emit(ring, overlay->flip_addr | OFC_UPDATE);
 	intel_ring_emit(ring, MI_WAIT_FOR_EVENT | MI_WAIT_FOR_OVERLAY_FLIP);
-	intel_ring_emit(ring, MI_NOOP);
 	intel_ring_advance(ring);
 
-	return intel_overlay_do_wait_request(overlay, NULL);
+	return intel_overlay_do_wait_request(overlay, rq, NULL);
 }
 
 /* overlay needs to be enabled in OCMD reg */
@@ -263,10 +296,10 @@
 {
 	struct drm_device *dev = overlay->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring = &dev_priv->ring[RCS];
 	u32 flip_addr = overlay->flip_addr;
+	struct i915_gem_request *rq;
+	struct intel_ringbuffer *ring;
 	u32 tmp;
-	int ret;
 
 	BUG_ON(!overlay->active);
 
@@ -278,21 +311,30 @@
 	if (tmp & (1 << 17))
 		DRM_DEBUG("overlay underrun, DOVSTA: %x\n", tmp);
 
-	ret = intel_ring_begin(ring, 2);
-	if (ret)
-		return ret;
+	rq = intel_overlay_alloc_request(overlay);
+	if (IS_ERR(rq))
+		return PTR_ERR(rq);
+
+	ring = intel_ring_begin(rq, 2);
+	if (IS_ERR(ring)) {
+		i915_request_put(rq);
+		return PTR_ERR(ring);
+	}
 
 	intel_ring_emit(ring, MI_OVERLAY_FLIP | MI_OVERLAY_CONTINUE);
 	intel_ring_emit(ring, flip_addr);
 	intel_ring_advance(ring);
 
-	return i915_add_request(ring, &overlay->last_flip_req);
+	return intel_overlay_add_request(overlay, rq, NULL);
 }
 
 static void intel_overlay_release_old_vid_tail(struct intel_overlay *overlay)
 {
 	struct drm_i915_gem_object *obj = overlay->old_vid_bo;
 
+	i915_gem_track_fb(obj, NULL,
+			  INTEL_FRONTBUFFER_OVERLAY(overlay->crtc->pipe));
+
 	i915_gem_object_ggtt_unpin(obj);
 	drm_gem_object_unreference(&obj->base);
 
@@ -319,10 +361,10 @@
 static int intel_overlay_off(struct intel_overlay *overlay)
 {
 	struct drm_device *dev = overlay->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring = &dev_priv->ring[RCS];
 	u32 flip_addr = overlay->flip_addr;
-	int ret;
+	struct i915_gem_request *rq;
+	struct intel_ringbuffer *ring;
+	int len;
 
 	BUG_ON(!overlay->active);
 
@@ -332,53 +374,36 @@
 	 * of the hw. Do it in both cases */
 	flip_addr |= OFC_UPDATE;
 
-	ret = intel_ring_begin(ring, 6);
-	if (ret)
-		return ret;
+	rq = intel_overlay_alloc_request(overlay);
+	if (IS_ERR(rq))
+		return PTR_ERR(rq);
+
+	len = 3;
+	if (!IS_I830(dev))
+		len += 3;
+
+	ring = intel_ring_begin(rq, len);
+	if (IS_ERR(ring)) {
+		i915_request_put(rq);
+		return PTR_ERR(ring);
+	}
 
 	/* wait for overlay to go idle */
 	intel_ring_emit(ring, MI_OVERLAY_FLIP | MI_OVERLAY_CONTINUE);
 	intel_ring_emit(ring, flip_addr);
 	intel_ring_emit(ring, MI_WAIT_FOR_EVENT | MI_WAIT_FOR_OVERLAY_FLIP);
-	/* turn overlay off */
-	if (IS_I830(dev)) {
-		/* Workaround: Don't disable the overlay fully, since otherwise
-		 * it dies on the next OVERLAY_ON cmd. */
-		intel_ring_emit(ring, MI_NOOP);
-		intel_ring_emit(ring, MI_NOOP);
-		intel_ring_emit(ring, MI_NOOP);
-	} else {
+	/* turn overlay off
+	 * Workaround for i830: Don't disable the overlay fully, since
+	 * otherwise it dies on the next OVERLAY_ON cmd.
+	 */
+	if (!IS_I830(dev)) {
 		intel_ring_emit(ring, MI_OVERLAY_FLIP | MI_OVERLAY_OFF);
 		intel_ring_emit(ring, flip_addr);
 		intel_ring_emit(ring, MI_WAIT_FOR_EVENT | MI_WAIT_FOR_OVERLAY_FLIP);
 	}
 	intel_ring_advance(ring);
 
-	return intel_overlay_do_wait_request(overlay, intel_overlay_off_tail);
-}
-
-/* recover from an interruption due to a signal
- * We have to be careful not to repeat work forever an make forward progess. */
-static int intel_overlay_recover_from_interrupt(struct intel_overlay *overlay)
-{
-	struct drm_device *dev = overlay->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring = &dev_priv->ring[RCS];
-	int ret;
-
-	if (overlay->last_flip_req == 0)
-		return 0;
-
-	ret = i915_wait_seqno(ring, overlay->last_flip_req);
-	if (ret)
-		return ret;
-	i915_gem_retire_requests(dev);
-
-	if (overlay->flip_tail)
-		overlay->flip_tail(overlay);
-
-	overlay->last_flip_req = 0;
-	return 0;
+	return intel_overlay_do_wait_request(overlay, rq, intel_overlay_off_tail);
 }
 
 /* Wait for pending overlay flip and release old frame.
@@ -387,10 +412,8 @@
  */
 static int intel_overlay_release_old_vid(struct intel_overlay *overlay)
 {
-	struct drm_device *dev = overlay->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring = &dev_priv->ring[RCS];
-	int ret;
+	struct drm_i915_private *dev_priv = to_i915(overlay->dev);
+	int ret = 0;
 
 	/* Only wait if there is actually an old frame to release to
 	 * guarantee forward progress.
@@ -399,27 +422,29 @@
 		return 0;
 
 	if (I915_READ(ISR) & I915_OVERLAY_PLANE_FLIP_PENDING_INTERRUPT) {
+		struct i915_gem_request *rq;
+		struct intel_ringbuffer *ring;
+
+		rq = intel_overlay_alloc_request(overlay);
+		if (IS_ERR(rq))
+			return PTR_ERR(rq);
+
 		/* synchronous slowpath */
-		ret = intel_ring_begin(ring, 2);
-		if (ret)
-			return ret;
+		ring = intel_ring_begin(rq, 1);
+		if (IS_ERR(ring)) {
+			i915_request_put(rq);
+			return PTR_ERR(ring);
+		}
 
 		intel_ring_emit(ring, MI_WAIT_FOR_EVENT | MI_WAIT_FOR_OVERLAY_FLIP);
-		intel_ring_emit(ring, MI_NOOP);
 		intel_ring_advance(ring);
 
-		ret = intel_overlay_do_wait_request(overlay,
+		ret = intel_overlay_do_wait_request(overlay, rq,
 						    intel_overlay_release_old_vid_tail);
-		if (ret)
-			return ret;
-	}
-
-	intel_overlay_release_old_vid_tail(overlay);
+	} else
+		intel_overlay_release_old_vid_tail(overlay);
 
-
-	i915_gem_track_fb(overlay->old_vid_bo, NULL,
-			  INTEL_FRONTBUFFER_OVERLAY(overlay->crtc->pipe));
-	return 0;
+	return ret;
 }
 
 struct put_image_params {
@@ -609,31 +634,36 @@
 			    struct overlay_registers __iomem *regs)
 {
 	u32 key = overlay->color_key;
+	u32 flags;
+
+	flags = 0;
+	if (overlay->color_key_enabled)
+		flags |= DST_KEY_ENABLE;
 
 	switch (overlay->crtc->base.primary->fb->bits_per_pixel) {
 	case 8:
-		iowrite32(0, &regs->DCLRKV);
-		iowrite32(CLK_RGB8I_MASK | DST_KEY_ENABLE, &regs->DCLRKM);
+		key = 0;
+		flags |= CLK_RGB8I_MASK;
 		break;
 
 	case 16:
 		if (overlay->crtc->base.primary->fb->depth == 15) {
-			iowrite32(RGB15_TO_COLORKEY(key), &regs->DCLRKV);
-			iowrite32(CLK_RGB15_MASK | DST_KEY_ENABLE,
-				  &regs->DCLRKM);
+			key = RGB15_TO_COLORKEY(key);
+			flags |= CLK_RGB15_MASK;
 		} else {
-			iowrite32(RGB16_TO_COLORKEY(key), &regs->DCLRKV);
-			iowrite32(CLK_RGB16_MASK | DST_KEY_ENABLE,
-				  &regs->DCLRKM);
+			key = RGB16_TO_COLORKEY(key);
+			flags |= CLK_RGB16_MASK;
 		}
 		break;
 
 	case 24:
 	case 32:
-		iowrite32(key, &regs->DCLRKV);
-		iowrite32(CLK_RGB24_MASK | DST_KEY_ENABLE, &regs->DCLRKM);
+		flags |= CLK_RGB24_MASK;
 		break;
 	}
+
+	iowrite32(key, &regs->DCLRKV);
+	iowrite32(flags, &regs->DCLRKM);
 }
 
 static u32 overlay_cmd_reg(struct put_image_params *params)
@@ -821,12 +851,7 @@
 	iowrite32(0, &regs->OCMD);
 	intel_overlay_unmap_regs(overlay, regs);
 
-	ret = intel_overlay_off(overlay);
-	if (ret != 0)
-		return ret;
-
-	intel_overlay_off_tail(overlay);
-	return 0;
+	return intel_overlay_off(overlay);
 }
 
 static int check_overlay_possible_on_crtc(struct intel_overlay *overlay,
@@ -1310,6 +1335,7 @@
 			I915_WRITE(OGAMC5, attrs->gamma5);
 		}
 	}
+	overlay->color_key_enabled = (attrs->flags & I915_OVERLAY_DISABLE_DEST_COLORKEY) == 0;
 
 	ret = 0;
 out_unlock:
@@ -1357,7 +1383,7 @@
 		}
 		overlay->flip_addr = reg_bo->phys_handle->busaddr;
 	} else {
-		ret = i915_gem_obj_ggtt_pin(reg_bo, PAGE_SIZE, PIN_MAPPABLE);
+		ret = i915_gem_object_ggtt_pin(reg_bo, PAGE_SIZE, PIN_MAPPABLE);
 		if (ret) {
 			DRM_ERROR("failed to pin overlay register bo\n");
 			goto out_free_bo;
@@ -1373,6 +1399,7 @@
 
 	/* init all values */
 	overlay->color_key = 0x0101fe;
+	overlay->color_key_enabled = true;
 	overlay->brightness = -19;
 	overlay->contrast = 75;
 	overlay->saturation = 146;
diff -urN a/drivers/gpu/drm/i915/intel_panel.c b/drivers/gpu/drm/i915/intel_panel.c
--- a/drivers/gpu/drm/i915/intel_panel.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_panel.c	2014-11-22 14:37:49.342700417 -0700
@@ -382,13 +382,13 @@
 	struct drm_i915_private *dev_priv = dev->dev_private;
 
 	/* Assume that the BIOS does not lie through the OpRegion... */
-	if (!i915.panel_ignore_lid && dev_priv->opregion.lid_state) {
+	if (!i915_module.panel_ignore_lid && dev_priv->opregion.lid_state) {
 		return ioread32(dev_priv->opregion.lid_state) & 0x1 ?
 			connector_status_connected :
 			connector_status_disconnected;
 	}
 
-	switch (i915.panel_ignore_lid) {
+	switch (i915_module.panel_ignore_lid) {
 	case -2:
 		return connector_status_connected;
 	case -1:
@@ -398,9 +398,6 @@
 	}
 }
 
-#define DIV_ROUND_CLOSEST_ULL(ll, d)	\
-({ unsigned long long _tmp = (ll)+(d)/2; do_div(_tmp, d); _tmp; })
-
 /**
  * scale - scale values from one range to another
  *
@@ -472,10 +469,10 @@
 
 	WARN_ON(panel->backlight.max == 0);
 
-	if (i915.invert_brightness < 0)
+	if (i915_module.invert_brightness < 0)
 		return val;
 
-	if (i915.invert_brightness > 0 ||
+	if (i915_module.invert_brightness > 0 ||
 	    dev_priv->quirks & QUIRK_INVERT_BRIGHTNESS) {
 		return panel->backlight.max - val;
 	}
@@ -524,6 +521,9 @@
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 
+	if (WARN_ON(pipe != PIPE_A && pipe != PIPE_B))
+		return 0;
+
 	return I915_READ(VLV_BLC_PWM_CTL(pipe)) & BACKLIGHT_DUTY_CYCLE_MASK;
 }
 
@@ -539,15 +539,17 @@
 {
 	struct drm_device *dev = connector->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	u32 val;
-	unsigned long flags;
+	struct intel_panel *panel = &connector->panel;
+	u32 val = 0;
 
-	spin_lock_irqsave(&dev_priv->backlight_lock, flags);
+	mutex_lock(&dev_priv->backlight_lock);
 
-	val = dev_priv->display.get_backlight(connector);
-	val = intel_panel_compute_brightness(connector, val);
+	if (panel->backlight.enabled) {
+		val = dev_priv->display.get_backlight(connector);
+		val = intel_panel_compute_brightness(connector, val);
+	}
 
-	spin_unlock_irqrestore(&dev_priv->backlight_lock, flags);
+	mutex_unlock(&dev_priv->backlight_lock);
 
 	DRM_DEBUG_DRIVER("get backlight PWM = %d\n", val);
 	return val;
@@ -606,6 +608,9 @@
 	enum pipe pipe = intel_get_pipe_from_connector(connector);
 	u32 tmp;
 
+	if (WARN_ON(pipe != PIPE_A && pipe != PIPE_B))
+		return;
+
 	tmp = I915_READ(VLV_BLC_PWM_CTL(pipe)) & ~BACKLIGHT_DUTY_CYCLE_MASK;
 	I915_WRITE(VLV_BLC_PWM_CTL(pipe), tmp | level);
 }
@@ -629,24 +634,25 @@
 	struct drm_device *dev = connector->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_panel *panel = &connector->panel;
-	enum pipe pipe = intel_get_pipe_from_connector(connector);
 	u32 hw_level;
-	unsigned long flags;
 
-	if (!panel->backlight.present || pipe == INVALID_PIPE)
+	if (!panel->backlight.present)
 		return;
 
-	spin_lock_irqsave(&dev_priv->backlight_lock, flags);
+	mutex_lock(&dev_priv->backlight_lock);
 
 	WARN_ON(panel->backlight.max == 0);
 
 	hw_level = scale_user_to_hw(connector, user_level, user_max);
 	panel->backlight.level = hw_level;
 
+	DRM_DEBUG_KMS("user level = %d/%d, backlight = %d\n",
+		      user_level, user_max, hw_level);
+
 	if (panel->backlight.enabled)
 		intel_panel_actually_set_backlight(connector, hw_level);
 
-	spin_unlock_irqrestore(&dev_priv->backlight_lock, flags);
+	mutex_unlock(&dev_priv->backlight_lock);
 }
 
 /* set backlight brightness to level in range [0..max], assuming hw min is
@@ -660,28 +666,40 @@
 	struct intel_panel *panel = &connector->panel;
 	enum pipe pipe = intel_get_pipe_from_connector(connector);
 	u32 hw_level;
-	unsigned long flags;
 
+	/*
+	 * INVALID_PIPE may occur during driver init because
+	 * connection_mutex isn't held across the entire backlight
+	 * setup + modeset readout, and the BIOS can issue the
+	 * requests at any time.
+	 */
 	if (!panel->backlight.present || pipe == INVALID_PIPE)
 		return;
 
-	spin_lock_irqsave(&dev_priv->backlight_lock, flags);
+	mutex_lock(&dev_priv->backlight_lock);
 
 	WARN_ON(panel->backlight.max == 0);
 
 	hw_level = clamp_user_to_hw(connector, user_level, user_max);
 	panel->backlight.level = hw_level;
 
-	if (panel->backlight.device)
+	DRM_DEBUG_KMS("user level = %d/%d, backlight = %d\n",
+		      user_level, user_max, hw_level);
+
+	if (panel->backlight.device) {
 		panel->backlight.device->props.brightness =
 			scale_hw_to_user(connector,
 					 panel->backlight.level,
 					 panel->backlight.device->props.max_brightness);
+		DRM_DEBUG_KMS("brightness = %d/%d\n",
+			      panel->backlight.device->props.brightness,
+			      panel->backlight.device->props.max_brightness);
+	}
 
 	if (panel->backlight.enabled)
 		intel_panel_actually_set_backlight(connector, hw_level);
 
-	spin_unlock_irqrestore(&dev_priv->backlight_lock, flags);
+	mutex_unlock(&dev_priv->backlight_lock);
 }
 
 static void pch_disable_backlight(struct intel_connector *connector)
@@ -723,6 +741,9 @@
 	enum pipe pipe = intel_get_pipe_from_connector(connector);
 	u32 tmp;
 
+	if (WARN_ON(pipe != PIPE_A && pipe != PIPE_B))
+		return;
+
 	intel_panel_actually_set_backlight(connector, 0);
 
 	tmp = I915_READ(VLV_BLC_PWM_CTL2(pipe));
@@ -734,10 +755,8 @@
 	struct drm_device *dev = connector->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_panel *panel = &connector->panel;
-	enum pipe pipe = intel_get_pipe_from_connector(connector);
-	unsigned long flags;
 
-	if (!panel->backlight.present || pipe == INVALID_PIPE)
+	if (!panel->backlight.present)
 		return;
 
 	/*
@@ -751,12 +770,14 @@
 		return;
 	}
 
-	spin_lock_irqsave(&dev_priv->backlight_lock, flags);
+	mutex_lock(&dev_priv->backlight_lock);
 
+	if (panel->backlight.device)
+		panel->backlight.device->props.power = FB_BLANK_POWERDOWN;
 	panel->backlight.enabled = false;
 	dev_priv->display.disable_backlight(connector);
 
-	spin_unlock_irqrestore(&dev_priv->backlight_lock, flags);
+	mutex_unlock(&dev_priv->backlight_lock);
 }
 
 static void bdw_enable_backlight(struct intel_connector *connector)
@@ -780,8 +801,9 @@
 	if (panel->backlight.active_low_pwm)
 		pch_ctl1 |= BLM_PCH_POLARITY;
 
-	/* BDW always uses the pch pwm controls. */
-	pch_ctl1 |= BLM_PCH_OVERRIDE_ENABLE;
+	/* After LPT, override is the default. */
+	if (HAS_PCH_LPT(dev_priv))
+		pch_ctl1 |= BLM_PCH_OVERRIDE_ENABLE;
 
 	I915_WRITE(BLC_PWM_PCH_CTL1, pch_ctl1);
 	POSTING_READ(BLC_PWM_PCH_CTL1);
@@ -910,6 +932,9 @@
 	enum pipe pipe = intel_get_pipe_from_connector(connector);
 	u32 ctl, ctl2;
 
+	if (WARN_ON(pipe != PIPE_A && pipe != PIPE_B))
+		return;
+
 	ctl2 = I915_READ(VLV_BLC_PWM_CTL2(pipe));
 	if (ctl2 & BLM_PWM_ENABLE) {
 		DRM_DEBUG_KMS("backlight already enabled\n");
@@ -937,36 +962,44 @@
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_panel *panel = &connector->panel;
 	enum pipe pipe = intel_get_pipe_from_connector(connector);
-	unsigned long flags;
 
-	if (!panel->backlight.present || pipe == INVALID_PIPE)
+	if (!panel->backlight.present)
 		return;
 
 	DRM_DEBUG_KMS("pipe %c\n", pipe_name(pipe));
 
-	spin_lock_irqsave(&dev_priv->backlight_lock, flags);
+	mutex_lock(&dev_priv->backlight_lock);
 
 	WARN_ON(panel->backlight.max == 0);
 
 	if (panel->backlight.level == 0) {
+		DRM_DEBUG_KMS("overriding backlight level of 0, setting max %d\n",
+			      panel->backlight.max);
 		panel->backlight.level = panel->backlight.max;
-		if (panel->backlight.device)
+		if (panel->backlight.device) {
 			panel->backlight.device->props.brightness =
 				scale_hw_to_user(connector,
 						 panel->backlight.level,
 						 panel->backlight.device->props.max_brightness);
+			DRM_DEBUG_KMS("brightness = %d/%d\n",
+				      panel->backlight.device->props.brightness,
+				      panel->backlight.device->props.max_brightness);
+		}
 	}
 
 	dev_priv->display.enable_backlight(connector);
 	panel->backlight.enabled = true;
+	if (panel->backlight.device)
+		panel->backlight.device->props.power = FB_BLANK_UNBLANK;
 
-	spin_unlock_irqrestore(&dev_priv->backlight_lock, flags);
+	mutex_unlock(&dev_priv->backlight_lock);
 }
 
 #if IS_ENABLED(CONFIG_BACKLIGHT_CLASS_DEVICE)
 static int intel_backlight_device_update_status(struct backlight_device *bd)
 {
 	struct intel_connector *connector = bl_get_data(bd);
+	struct intel_panel *panel = &connector->panel;
 	struct drm_device *dev = connector->base.dev;
 
 	drm_modeset_lock(&dev->mode_config.connection_mutex, NULL);
@@ -974,6 +1007,23 @@
 		      bd->props.brightness, bd->props.max_brightness);
 	intel_panel_set_backlight(connector, bd->props.brightness,
 				  bd->props.max_brightness);
+
+	/*
+	 * Allow flipping bl_power as a sub-state of enabled. Sadly the
+	 * backlight class device does not make it easy to to differentiate
+	 * between callbacks for brightness and bl_power, so our backlight_power
+	 * callback needs to take this into account.
+	 */
+	if (panel->backlight.enabled) {
+		if (panel->backlight_power) {
+			bool enable = bd->props.power == FB_BLANK_UNBLANK &&
+				bd->props.brightness != 0;
+			panel->backlight_power(connector, enable);
+		}
+	} else {
+		bd->props.power = FB_BLANK_POWERDOWN;
+	}
+
 	drm_modeset_unlock(&dev->mode_config.connection_mutex);
 	return 0;
 }
@@ -1011,6 +1061,9 @@
 	if (WARN_ON(panel->backlight.device))
 		return -ENODEV;
 
+	if (!panel->backlight.present)
+		return 0;
+
 	WARN_ON(panel->backlight.max == 0);
 
 	memset(&props, 0, sizeof(props));
@@ -1025,6 +1078,11 @@
 					    panel->backlight.level,
 					    props.max_brightness);
 
+	if (panel->backlight.enabled)
+		props.power = FB_BLANK_UNBLANK;
+	else
+		props.power = FB_BLANK_POWERDOWN;
+
 	/*
 	 * Note: using the same name independent of the connector prevents
 	 * registration of multiple backlight devices in the driver.
@@ -1041,6 +1099,10 @@
 		panel->backlight.device = NULL;
 		return -ENODEV;
 	}
+
+	DRM_DEBUG_KMS("Connector %s backlight sysfs interface registered\n",
+		      connector->base.name);
+
 	return 0;
 }
 
@@ -1095,7 +1157,7 @@
 	return scale(min, 0, 255, 0, panel->backlight.max);
 }
 
-static int bdw_setup_backlight(struct intel_connector *connector)
+static int bdw_setup_backlight(struct intel_connector *connector, enum pipe unused)
 {
 	struct drm_device *dev = connector->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -1121,7 +1183,7 @@
 	return 0;
 }
 
-static int pch_setup_backlight(struct intel_connector *connector)
+static int pch_setup_backlight(struct intel_connector *connector, enum pipe unused)
 {
 	struct drm_device *dev = connector->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -1148,7 +1210,7 @@
 	return 0;
 }
 
-static int i9xx_setup_backlight(struct intel_connector *connector)
+static int i9xx_setup_backlight(struct intel_connector *connector, enum pipe unused)
 {
 	struct drm_device *dev = connector->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -1180,7 +1242,7 @@
 	return 0;
 }
 
-static int i965_setup_backlight(struct intel_connector *connector)
+static int i965_setup_backlight(struct intel_connector *connector, enum pipe unused)
 {
 	struct drm_device *dev = connector->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -1210,37 +1272,40 @@
 	return 0;
 }
 
-static int vlv_setup_backlight(struct intel_connector *connector)
+static int vlv_setup_backlight(struct intel_connector *connector, enum pipe pipe)
 {
 	struct drm_device *dev = connector->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_panel *panel = &connector->panel;
-	enum pipe pipe;
+	enum pipe p;
 	u32 ctl, ctl2, val;
 
-	for_each_pipe(pipe) {
-		u32 cur_val = I915_READ(VLV_BLC_PWM_CTL(pipe));
+	for_each_pipe(dev_priv, p) {
+		u32 cur_val = I915_READ(VLV_BLC_PWM_CTL(p));
 
 		/* Skip if the modulation freq is already set */
 		if (cur_val & ~BACKLIGHT_DUTY_CYCLE_MASK)
 			continue;
 
 		cur_val &= BACKLIGHT_DUTY_CYCLE_MASK;
-		I915_WRITE(VLV_BLC_PWM_CTL(pipe), (0xf42 << 16) |
+		I915_WRITE(VLV_BLC_PWM_CTL(p), (0xf42 << 16) |
 			   cur_val);
 	}
 
-	ctl2 = I915_READ(VLV_BLC_PWM_CTL2(PIPE_A));
+	if (WARN_ON(pipe != PIPE_A && pipe != PIPE_B))
+		return -ENODEV;
+
+	ctl2 = I915_READ(VLV_BLC_PWM_CTL2(pipe));
 	panel->backlight.active_low_pwm = ctl2 & BLM_POLARITY_I965;
 
-	ctl = I915_READ(VLV_BLC_PWM_CTL(PIPE_A));
+	ctl = I915_READ(VLV_BLC_PWM_CTL(pipe));
 	panel->backlight.max = ctl >> 16;
 	if (!panel->backlight.max)
 		return -ENODEV;
 
 	panel->backlight.min = get_backlight_min_vbt(connector);
 
-	val = _vlv_get_backlight(dev, PIPE_A);
+	val = _vlv_get_backlight(dev, pipe);
 	panel->backlight.level = intel_panel_compute_brightness(connector, val);
 
 	panel->backlight.enabled = (ctl2 & BLM_PWM_ENABLE) &&
@@ -1249,13 +1314,12 @@
 	return 0;
 }
 
-int intel_panel_setup_backlight(struct drm_connector *connector)
+int intel_panel_setup_backlight(struct drm_connector *connector, enum pipe pipe)
 {
 	struct drm_device *dev = connector->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_connector *intel_connector = to_intel_connector(connector);
 	struct intel_panel *panel = &intel_connector->panel;
-	unsigned long flags;
 	int ret;
 
 	if (!dev_priv->vbt.backlight.present) {
@@ -1268,9 +1332,9 @@
 	}
 
 	/* set level and max in panel struct */
-	spin_lock_irqsave(&dev_priv->backlight_lock, flags);
-	ret = dev_priv->display.setup_backlight(intel_connector);
-	spin_unlock_irqrestore(&dev_priv->backlight_lock, flags);
+	mutex_lock(&dev_priv->backlight_lock);
+	ret = dev_priv->display.setup_backlight(intel_connector, pipe);
+	mutex_unlock(&dev_priv->backlight_lock);
 
 	if (ret) {
 		DRM_DEBUG_KMS("failed to setup backlight for connector %s\n",
@@ -1278,15 +1342,12 @@
 		return ret;
 	}
 
-	intel_backlight_device_register(intel_connector);
-
 	panel->backlight.present = true;
 
-	DRM_DEBUG_KMS("backlight initialized, %s, brightness %u/%u, "
-		      "sysfs interface %sregistered\n",
+	DRM_DEBUG_KMS("Connector %s backlight initialized, %s, brightness %u/%u\n",
+		      connector->name,
 		      panel->backlight.enabled ? "enabled" : "disabled",
-		      panel->backlight.level, panel->backlight.max,
-		      panel->backlight.device ? "" : "not ");
+		      panel->backlight.level, panel->backlight.max);
 
 	return 0;
 }
@@ -1297,7 +1358,6 @@
 	struct intel_panel *panel = &intel_connector->panel;
 
 	panel->backlight.present = false;
-	intel_backlight_device_unregister(intel_connector);
 }
 
 /* Set up chip specific backlight functions */
@@ -1305,7 +1365,7 @@
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	if (IS_BROADWELL(dev)) {
+	if (IS_BROADWELL(dev) || (INTEL_INFO(dev)->gen >= 9)) {
 		dev_priv->display.setup_backlight = bdw_setup_backlight;
 		dev_priv->display.enable_backlight = bdw_enable_backlight;
 		dev_priv->display.disable_backlight = pch_disable_backlight;
@@ -1360,3 +1420,19 @@
 		drm_mode_destroy(intel_connector->base.dev,
 				panel->downclock_mode);
 }
+
+void intel_backlight_register(struct drm_device *dev)
+{
+	struct intel_connector *connector;
+
+	list_for_each_entry(connector, &dev->mode_config.connector_list, base.head)
+		intel_backlight_device_register(connector);
+}
+
+void intel_backlight_unregister(struct drm_device *dev)
+{
+	struct intel_connector *connector;
+
+	list_for_each_entry(connector, &dev->mode_config.connector_list, base.head)
+		intel_backlight_device_unregister(connector);
+}
diff -urN a/drivers/gpu/drm/i915/intel_pm.c b/drivers/gpu/drm/i915/intel_pm.c
--- a/drivers/gpu/drm/i915/intel_pm.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_pm.c	2014-11-22 14:37:49.342700417 -0700
@@ -30,9 +30,6 @@
 #include "intel_drv.h"
 #include "../../../platform/x86/intel_ips.h"
 #include <linux/module.h>
-#include <linux/vgaarb.h>
-#include <drm/i915_powerwell.h>
-#include <linux/pm_runtime.h>
 
 /**
  * RC6 is a special power stage which allows the GPU to enter an very
@@ -66,11 +63,37 @@
  * i915.i915_enable_fbc parameter
  */
 
+static void gen9_init_clock_gating(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	/*
+	 * WaDisableSDEUnitClockGating:skl
+	 * This seems to be a pre-production w/a.
+	 */
+	I915_WRITE(GEN8_UCGCTL6, I915_READ(GEN8_UCGCTL6) |
+		   GEN8_SDEUNIT_CLOCK_GATE_DISABLE);
+
+	/*
+	 * WaDisableDgMirrorFixInHalfSliceChicken5:skl
+	 * This is a pre-production w/a.
+	 */
+	I915_WRITE(GEN9_HALF_SLICE_CHICKEN5,
+		   I915_READ(GEN9_HALF_SLICE_CHICKEN5) &
+		   ~GEN9_DG_MIRROR_FIX_ENABLE);
+
+	/* Wa4x4STCOptimizationDisable:skl */
+	I915_WRITE(CACHE_MODE_1,
+		   _MASKED_BIT_ENABLE(GEN8_4x4_STC_OPTIMIZATION_DISABLE));
+}
+
 static void i8xx_disable_fbc(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	u32 fbc_ctl;
 
+	dev_priv->fbc.enabled = false;
+
 	/* Disable compression */
 	fbc_ctl = I915_READ(FBC_CONTROL);
 	if ((fbc_ctl & FBC_CTL_EN) == 0)
@@ -99,6 +122,8 @@
 	int i;
 	u32 fbc_ctl;
 
+	dev_priv->fbc.enabled = true;
+
 	cfb_pitch = dev_priv->fbc.size / FBC_LL_SIZE;
 	if (fb->pitches[0] < cfb_pitch)
 		cfb_pitch = fb->pitches[0];
@@ -153,6 +178,8 @@
 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 	u32 dpfc_ctl;
 
+	dev_priv->fbc.enabled = true;
+
 	dpfc_ctl = DPFC_CTL_PLANE(intel_crtc->plane) | DPFC_SR_EN;
 	if (drm_format_plane_cpp(fb->pixel_format, 0) == 2)
 		dpfc_ctl |= DPFC_CTL_LIMIT_2X;
@@ -173,6 +200,8 @@
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	u32 dpfc_ctl;
 
+	dev_priv->fbc.enabled = false;
+
 	/* Disable compression */
 	dpfc_ctl = I915_READ(DPFC_CONTROL);
 	if (dpfc_ctl & DPFC_CTL_EN) {
@@ -224,6 +253,8 @@
 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 	u32 dpfc_ctl;
 
+	dev_priv->fbc.enabled = true;
+
 	dpfc_ctl = DPFC_CTL_PLANE(intel_crtc->plane);
 	if (drm_format_plane_cpp(fb->pixel_format, 0) == 2)
 		dev_priv->fbc.threshold++;
@@ -264,6 +295,8 @@
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	u32 dpfc_ctl;
 
+	dev_priv->fbc.enabled = false;
+
 	/* Disable compression */
 	dpfc_ctl = I915_READ(ILK_DPFC_CONTROL);
 	if (dpfc_ctl & DPFC_CTL_EN) {
@@ -290,6 +323,8 @@
 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 	u32 dpfc_ctl;
 
+	dev_priv->fbc.enabled = true;
+
 	dpfc_ctl = IVB_DPFC_CTL_PLANE(intel_crtc->plane);
 	if (drm_format_plane_cpp(fb->pixel_format, 0) == 2)
 		dev_priv->fbc.threshold++;
@@ -309,6 +344,9 @@
 
 	dpfc_ctl |= IVB_DPFC_CTL_FENCE_EN;
 
+	if (dev_priv->fbc.false_color)
+		dpfc_ctl |= FBC_CTL_FALSE_COLOR;
+
 	I915_WRITE(ILK_DPFC_CONTROL, dpfc_ctl | DPFC_CTL_EN);
 
 	if (IS_IVYBRIDGE(dev)) {
@@ -336,10 +374,20 @@
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	if (!dev_priv->display.fbc_enabled)
-		return false;
+	return dev_priv->fbc.enabled;
+}
+
+void bdw_fbc_sw_flush(struct drm_device *dev, u32 value)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	if (!IS_GEN8(dev))
+		return;
+
+	if (!intel_fbc_enabled(dev))
+		return;
 
-	return dev_priv->display.fbc_enabled(dev);
+	I915_WRITE(MSG_FBC_REND_STATE, value);
 }
 
 static void intel_fbc_work_fn(struct work_struct *__work)
@@ -490,7 +538,7 @@
 		return;
 	}
 
-	if (!i915.powersave) {
+	if (!i915_module.powersave) {
 		if (set_no_fbc_reason(dev_priv, FBC_MODULE_PARAM))
 			DRM_DEBUG_KMS("fbc disabled per module param\n");
 		return;
@@ -528,12 +576,12 @@
 	obj = intel_fb_obj(fb);
 	adjusted_mode = &intel_crtc->config.adjusted_mode;
 
-	if (i915.enable_fbc < 0) {
+	if (i915_module.enable_fbc < 0) {
 		if (set_no_fbc_reason(dev_priv, FBC_CHIP_DEFAULT))
 			DRM_DEBUG_KMS("disabled per chip default\n");
 		goto out_disable;
 	}
-	if (!i915.enable_fbc) {
+	if (!i915_module.enable_fbc) {
 		if (set_no_fbc_reason(dev_priv, FBC_MODULE_PARAM))
 			DRM_DEBUG_KMS("fbc disabled per module param\n");
 		goto out_disable;
@@ -578,6 +626,12 @@
 			DRM_DEBUG_KMS("framebuffer not tiled or fenced, disabling compression\n");
 		goto out_disable;
 	}
+	if (INTEL_INFO(dev)->gen <= 4 && !IS_G4X(dev) &&
+	    to_intel_plane(crtc->primary)->rotation != BIT(DRM_ROTATE_0)) {
+		if (set_no_fbc_reason(dev_priv, FBC_UNSUPPORTED_MODE))
+			DRM_DEBUG_KMS("Rotation unsupported, disabling\n");
+		goto out_disable;
+	}
 
 	/* If the kernel debugger is active, always disable compression */
 	if (in_dbg_master())
@@ -853,7 +907,7 @@
  * A value of 5us seems to be a good balance; safe for very low end
  * platforms but not overly aggressive on lower latency configs.
  */
-static const int latency_ns = 5000;
+static const int pessimal_latency_ns = 5000;
 
 static int i9xx_get_fifo_size(struct drm_device *dev, int plane)
 {
@@ -982,13 +1036,20 @@
 	.guard_size = 2,
 	.cacheline_size = I915_FIFO_LINE_SIZE,
 };
-static const struct intel_watermark_params i830_wm_info = {
+static const struct intel_watermark_params i830_a_wm_info = {
 	.fifo_size = I855GM_FIFO_SIZE,
 	.max_wm = I915_MAX_WM,
 	.default_wm = 1,
 	.guard_size = 2,
 	.cacheline_size = I830_FIFO_LINE_SIZE,
 };
+static const struct intel_watermark_params i830_bc_wm_info = {
+	.fifo_size = I855GM_FIFO_SIZE,
+	.max_wm = I915_MAX_WM/2,
+	.default_wm = 1,
+	.guard_size = 2,
+	.cacheline_size = I830_FIFO_LINE_SIZE,
+};
 static const struct intel_watermark_params i845_wm_info = {
 	.fifo_size = I830_FIFO_SIZE,
 	.max_wm = I915_MAX_WM,
@@ -1044,6 +1105,17 @@
 		wm_size = wm->max_wm;
 	if (wm_size <= 0)
 		wm_size = wm->default_wm;
+
+	/*
+	 * Bspec seems to indicate that the value shouldn't be lower than
+	 * 'burst size + 1'. Certainly 830 is quite unhappy with low values.
+	 * Lets go for 8 which is the burst size since certain platforms
+	 * already use a hardcoded 8 (which is what the spec says should be
+	 * done).
+	 */
+	if (wm_size <= 8)
+		wm_size = 8;
+
 	return wm_size;
 }
 
@@ -1268,33 +1340,32 @@
 			      display, cursor);
 }
 
-static bool vlv_compute_drain_latency(struct drm_device *dev,
-				     int plane,
-				     int *plane_prec_mult,
-				     int *plane_dl,
-				     int *cursor_prec_mult,
-				     int *cursor_dl)
+static bool vlv_compute_drain_latency(struct drm_crtc *crtc,
+				      int pixel_size,
+				      int *prec_mult,
+				      int *drain_latency)
 {
-	struct drm_crtc *crtc;
-	int clock, pixel_size;
+	struct drm_device *dev = crtc->dev;
 	int entries;
+	int clock = to_intel_crtc(crtc)->config.adjusted_mode.crtc_clock;
 
-	crtc = intel_get_crtc_for_plane(dev, plane);
-	if (!intel_crtc_active(crtc))
+	if (WARN(clock == 0, "Pixel clock is zero!\n"))
 		return false;
 
-	clock = to_intel_crtc(crtc)->config.adjusted_mode.crtc_clock;
-	pixel_size = crtc->primary->fb->bits_per_pixel / 8;	/* BPP */
+	if (WARN(pixel_size == 0, "Pixel size is zero!\n"))
+		return false;
+
+	entries = DIV_ROUND_UP(clock, 1000) * pixel_size;
+	if (IS_CHERRYVIEW(dev))
+		*prec_mult = (entries > 128) ? DRAIN_LATENCY_PRECISION_32 :
+					       DRAIN_LATENCY_PRECISION_16;
+	else
+		*prec_mult = (entries > 128) ? DRAIN_LATENCY_PRECISION_64 :
+					       DRAIN_LATENCY_PRECISION_32;
+	*drain_latency = (64 * (*prec_mult) * 4) / entries;
 
-	entries = (clock / 1000) * pixel_size;
-	*plane_prec_mult = (entries > 128) ?
-		DRAIN_LATENCY_PRECISION_64 : DRAIN_LATENCY_PRECISION_32;
-	*plane_dl = (64 * (*plane_prec_mult) * 4) / entries;
-
-	entries = (clock / 1000) * 4;	/* BPP is always 4 for cursor */
-	*cursor_prec_mult = (entries > 128) ?
-		DRAIN_LATENCY_PRECISION_64 : DRAIN_LATENCY_PRECISION_32;
-	*cursor_dl = (64 * (*cursor_prec_mult) * 4) / entries;
+	if (*drain_latency > DRAIN_LATENCY_MASK)
+		*drain_latency = DRAIN_LATENCY_MASK;
 
 	return true;
 }
@@ -1307,39 +1378,51 @@
  * latency value.
  */
 
-static void vlv_update_drain_latency(struct drm_device *dev)
+static void vlv_update_drain_latency(struct drm_crtc *crtc)
 {
+	struct drm_device *dev = crtc->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	int planea_prec, planea_dl, planeb_prec, planeb_dl;
-	int cursora_prec, cursora_dl, cursorb_prec, cursorb_dl;
-	int plane_prec_mult, cursor_prec_mult; /* Precision multiplier is
-							either 16 or 32 */
-
-	/* For plane A, Cursor A */
-	if (vlv_compute_drain_latency(dev, 0, &plane_prec_mult, &planea_dl,
-				      &cursor_prec_mult, &cursora_dl)) {
-		cursora_prec = (cursor_prec_mult == DRAIN_LATENCY_PRECISION_32) ?
-			DDL_CURSORA_PRECISION_32 : DDL_CURSORA_PRECISION_64;
-		planea_prec = (plane_prec_mult == DRAIN_LATENCY_PRECISION_32) ?
-			DDL_PLANEA_PRECISION_32 : DDL_PLANEA_PRECISION_64;
-
-		I915_WRITE(VLV_DDL1, cursora_prec |
-				(cursora_dl << DDL_CURSORA_SHIFT) |
-				planea_prec | planea_dl);
-	}
-
-	/* For plane B, Cursor B */
-	if (vlv_compute_drain_latency(dev, 1, &plane_prec_mult, &planeb_dl,
-				      &cursor_prec_mult, &cursorb_dl)) {
-		cursorb_prec = (cursor_prec_mult == DRAIN_LATENCY_PRECISION_32) ?
-			DDL_CURSORB_PRECISION_32 : DDL_CURSORB_PRECISION_64;
-		planeb_prec = (plane_prec_mult == DRAIN_LATENCY_PRECISION_32) ?
-			DDL_PLANEB_PRECISION_32 : DDL_PLANEB_PRECISION_64;
-
-		I915_WRITE(VLV_DDL2, cursorb_prec |
-				(cursorb_dl << DDL_CURSORB_SHIFT) |
-				planeb_prec | planeb_dl);
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	int pixel_size;
+	int drain_latency;
+	enum pipe pipe = intel_crtc->pipe;
+	int plane_prec, prec_mult, plane_dl;
+	const int high_precision = IS_CHERRYVIEW(dev) ?
+		DRAIN_LATENCY_PRECISION_32 : DRAIN_LATENCY_PRECISION_64;
+
+	plane_dl = I915_READ(VLV_DDL(pipe)) & ~(DDL_PLANE_PRECISION_HIGH |
+		   DRAIN_LATENCY_MASK | DDL_CURSOR_PRECISION_HIGH |
+		   (DRAIN_LATENCY_MASK << DDL_CURSOR_SHIFT));
+
+	if (!intel_crtc_active(crtc)) {
+		I915_WRITE(VLV_DDL(pipe), plane_dl);
+		return;
+	}
+
+	/* Primary plane Drain Latency */
+	pixel_size = crtc->primary->fb->bits_per_pixel / 8;	/* BPP */
+	if (vlv_compute_drain_latency(crtc, pixel_size, &prec_mult, &drain_latency)) {
+		plane_prec = (prec_mult == high_precision) ?
+					   DDL_PLANE_PRECISION_HIGH :
+					   DDL_PLANE_PRECISION_LOW;
+		plane_dl |= plane_prec | drain_latency;
+	}
+
+	/* Cursor Drain Latency
+	 * BPP is always 4 for cursor
+	 */
+	pixel_size = 4;
+
+	/* Program cursor DL only if it is enabled */
+	if (intel_crtc->cursor_base &&
+	    vlv_compute_drain_latency(crtc, pixel_size, &prec_mult, &drain_latency)) {
+		plane_prec = (prec_mult == high_precision) ?
+					   DDL_CURSOR_PRECISION_HIGH :
+					   DDL_CURSOR_PRECISION_LOW;
+		plane_dl |= plane_prec | (drain_latency << DDL_CURSOR_SHIFT);
 	}
+
+	I915_WRITE(VLV_DDL(pipe), plane_dl);
 }
 
 #define single_plane_enabled(mask) is_power_of_2(mask)
@@ -1355,20 +1438,92 @@
 	unsigned int enabled = 0;
 	bool cxsr_enabled;
 
-	vlv_update_drain_latency(dev);
+	vlv_update_drain_latency(crtc);
+
+	if (g4x_compute_wm0(dev, PIPE_A,
+			    &valleyview_wm_info, pessimal_latency_ns,
+			    &valleyview_cursor_wm_info, pessimal_latency_ns,
+			    &planea_wm, &cursora_wm))
+		enabled |= 1 << PIPE_A;
+
+	if (g4x_compute_wm0(dev, PIPE_B,
+			    &valleyview_wm_info, pessimal_latency_ns,
+			    &valleyview_cursor_wm_info, pessimal_latency_ns,
+			    &planeb_wm, &cursorb_wm))
+		enabled |= 1 << PIPE_B;
+
+	if (single_plane_enabled(enabled) &&
+	    g4x_compute_srwm(dev, ffs(enabled) - 1,
+			     sr_latency_ns,
+			     &valleyview_wm_info,
+			     &valleyview_cursor_wm_info,
+			     &plane_sr, &ignore_cursor_sr) &&
+	    g4x_compute_srwm(dev, ffs(enabled) - 1,
+			     2*sr_latency_ns,
+			     &valleyview_wm_info,
+			     &valleyview_cursor_wm_info,
+			     &ignore_plane_sr, &cursor_sr)) {
+		cxsr_enabled = true;
+	} else {
+		cxsr_enabled = false;
+		intel_set_memory_cxsr(dev_priv, false);
+		plane_sr = cursor_sr = 0;
+	}
+
+	DRM_DEBUG_KMS("Setting FIFO watermarks - A: plane=%d, cursor=%d, "
+		      "B: plane=%d, cursor=%d, SR: plane=%d, cursor=%d\n",
+		      planea_wm, cursora_wm,
+		      planeb_wm, cursorb_wm,
+		      plane_sr, cursor_sr);
+
+	I915_WRITE(DSPFW1,
+		   (plane_sr << DSPFW_SR_SHIFT) |
+		   (cursorb_wm << DSPFW_CURSORB_SHIFT) |
+		   (planeb_wm << DSPFW_PLANEB_SHIFT) |
+		   (planea_wm << DSPFW_PLANEA_SHIFT));
+	I915_WRITE(DSPFW2,
+		   (I915_READ(DSPFW2) & ~DSPFW_CURSORA_MASK) |
+		   (cursora_wm << DSPFW_CURSORA_SHIFT));
+	I915_WRITE(DSPFW3,
+		   (I915_READ(DSPFW3) & ~DSPFW_CURSOR_SR_MASK) |
+		   (cursor_sr << DSPFW_CURSOR_SR_SHIFT));
+
+	if (cxsr_enabled)
+		intel_set_memory_cxsr(dev_priv, true);
+}
+
+static void cherryview_update_wm(struct drm_crtc *crtc)
+{
+	struct drm_device *dev = crtc->dev;
+	static const int sr_latency_ns = 12000;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	int planea_wm, planeb_wm, planec_wm;
+	int cursora_wm, cursorb_wm, cursorc_wm;
+	int plane_sr, cursor_sr;
+	int ignore_plane_sr, ignore_cursor_sr;
+	unsigned int enabled = 0;
+	bool cxsr_enabled;
+
+	vlv_update_drain_latency(crtc);
 
 	if (g4x_compute_wm0(dev, PIPE_A,
-			    &valleyview_wm_info, latency_ns,
-			    &valleyview_cursor_wm_info, latency_ns,
+			    &valleyview_wm_info, pessimal_latency_ns,
+			    &valleyview_cursor_wm_info, pessimal_latency_ns,
 			    &planea_wm, &cursora_wm))
 		enabled |= 1 << PIPE_A;
 
 	if (g4x_compute_wm0(dev, PIPE_B,
-			    &valleyview_wm_info, latency_ns,
-			    &valleyview_cursor_wm_info, latency_ns,
+			    &valleyview_wm_info, pessimal_latency_ns,
+			    &valleyview_cursor_wm_info, pessimal_latency_ns,
 			    &planeb_wm, &cursorb_wm))
 		enabled |= 1 << PIPE_B;
 
+	if (g4x_compute_wm0(dev, PIPE_C,
+			    &valleyview_wm_info, pessimal_latency_ns,
+			    &valleyview_cursor_wm_info, pessimal_latency_ns,
+			    &planec_wm, &cursorc_wm))
+		enabled |= 1 << PIPE_C;
+
 	if (single_plane_enabled(enabled) &&
 	    g4x_compute_srwm(dev, ffs(enabled) - 1,
 			     sr_latency_ns,
@@ -1387,27 +1542,68 @@
 		plane_sr = cursor_sr = 0;
 	}
 
-	DRM_DEBUG_KMS("Setting FIFO watermarks - A: plane=%d, cursor=%d, B: plane=%d, cursor=%d, SR: plane=%d, cursor=%d\n",
+	DRM_DEBUG_KMS("Setting FIFO watermarks - A: plane=%d, cursor=%d, "
+		      "B: plane=%d, cursor=%d, C: plane=%d, cursor=%d, "
+		      "SR: plane=%d, cursor=%d\n",
 		      planea_wm, cursora_wm,
 		      planeb_wm, cursorb_wm,
+		      planec_wm, cursorc_wm,
 		      plane_sr, cursor_sr);
 
 	I915_WRITE(DSPFW1,
 		   (plane_sr << DSPFW_SR_SHIFT) |
 		   (cursorb_wm << DSPFW_CURSORB_SHIFT) |
 		   (planeb_wm << DSPFW_PLANEB_SHIFT) |
-		   planea_wm);
+		   (planea_wm << DSPFW_PLANEA_SHIFT));
 	I915_WRITE(DSPFW2,
 		   (I915_READ(DSPFW2) & ~DSPFW_CURSORA_MASK) |
 		   (cursora_wm << DSPFW_CURSORA_SHIFT));
 	I915_WRITE(DSPFW3,
 		   (I915_READ(DSPFW3) & ~DSPFW_CURSOR_SR_MASK) |
 		   (cursor_sr << DSPFW_CURSOR_SR_SHIFT));
+	I915_WRITE(DSPFW9_CHV,
+		   (I915_READ(DSPFW9_CHV) & ~(DSPFW_PLANEC_MASK |
+					      DSPFW_CURSORC_MASK)) |
+		   (planec_wm << DSPFW_PLANEC_SHIFT) |
+		   (cursorc_wm << DSPFW_CURSORC_SHIFT));
 
 	if (cxsr_enabled)
 		intel_set_memory_cxsr(dev_priv, true);
 }
 
+static void valleyview_update_sprite_wm(struct drm_plane *plane,
+					struct drm_crtc *crtc,
+					uint32_t sprite_width,
+					uint32_t sprite_height,
+					int pixel_size,
+					bool enabled, bool scaled)
+{
+	struct drm_device *dev = crtc->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	int pipe = to_intel_plane(plane)->pipe;
+	int sprite = to_intel_plane(plane)->plane;
+	int drain_latency;
+	int plane_prec;
+	int sprite_dl;
+	int prec_mult;
+	const int high_precision = IS_CHERRYVIEW(dev) ?
+		DRAIN_LATENCY_PRECISION_32 : DRAIN_LATENCY_PRECISION_64;
+
+	sprite_dl = I915_READ(VLV_DDL(pipe)) & ~(DDL_SPRITE_PRECISION_HIGH(sprite) |
+		    (DRAIN_LATENCY_MASK << DDL_SPRITE_SHIFT(sprite)));
+
+	if (enabled && vlv_compute_drain_latency(crtc, pixel_size, &prec_mult,
+						 &drain_latency)) {
+		plane_prec = (prec_mult == high_precision) ?
+					   DDL_SPRITE_PRECISION_HIGH(sprite) :
+					   DDL_SPRITE_PRECISION_LOW(sprite);
+		sprite_dl |= plane_prec |
+			     (drain_latency << DDL_SPRITE_SHIFT(sprite));
+	}
+
+	I915_WRITE(VLV_DDL(pipe), sprite_dl);
+}
+
 static void g4x_update_wm(struct drm_crtc *crtc)
 {
 	struct drm_device *dev = crtc->dev;
@@ -1419,14 +1615,14 @@
 	bool cxsr_enabled;
 
 	if (g4x_compute_wm0(dev, PIPE_A,
-			    &g4x_wm_info, latency_ns,
-			    &g4x_cursor_wm_info, latency_ns,
+			    &g4x_wm_info, pessimal_latency_ns,
+			    &g4x_cursor_wm_info, pessimal_latency_ns,
 			    &planea_wm, &cursora_wm))
 		enabled |= 1 << PIPE_A;
 
 	if (g4x_compute_wm0(dev, PIPE_B,
-			    &g4x_wm_info, latency_ns,
-			    &g4x_cursor_wm_info, latency_ns,
+			    &g4x_wm_info, pessimal_latency_ns,
+			    &g4x_cursor_wm_info, pessimal_latency_ns,
 			    &planeb_wm, &cursorb_wm))
 		enabled |= 1 << PIPE_B;
 
@@ -1443,7 +1639,8 @@
 		plane_sr = cursor_sr = 0;
 	}
 
-	DRM_DEBUG_KMS("Setting FIFO watermarks - A: plane=%d, cursor=%d, B: plane=%d, cursor=%d, SR: plane=%d, cursor=%d\n",
+	DRM_DEBUG_KMS("Setting FIFO watermarks - A: plane=%d, cursor=%d, "
+		      "B: plane=%d, cursor=%d, SR: plane=%d, cursor=%d\n",
 		      planea_wm, cursora_wm,
 		      planeb_wm, cursorb_wm,
 		      plane_sr, cursor_sr);
@@ -1452,7 +1649,7 @@
 		   (plane_sr << DSPFW_SR_SHIFT) |
 		   (cursorb_wm << DSPFW_CURSORB_SHIFT) |
 		   (planeb_wm << DSPFW_PLANEB_SHIFT) |
-		   planea_wm);
+		   (planea_wm << DSPFW_PLANEA_SHIFT));
 	I915_WRITE(DSPFW2,
 		   (I915_READ(DSPFW2) & ~DSPFW_CURSORA_MASK) |
 		   (cursora_wm << DSPFW_CURSORA_SHIFT));
@@ -1526,8 +1723,11 @@
 
 	/* 965 has limitations... */
 	I915_WRITE(DSPFW1, (srwm << DSPFW_SR_SHIFT) |
-		   (8 << 16) | (8 << 8) | (8 << 0));
-	I915_WRITE(DSPFW2, (8 << 8) | (8 << 0));
+		   (8 << DSPFW_CURSORB_SHIFT) |
+		   (8 << DSPFW_PLANEB_SHIFT) |
+		   (8 << DSPFW_PLANEA_SHIFT));
+	I915_WRITE(DSPFW2, (8 << DSPFW_CURSORA_SHIFT) |
+		   (8 << DSPFW_PLANEC_SHIFT_OLD));
 	/* update cursor SR watermark */
 	I915_WRITE(DSPFW3, (cursor_sr << DSPFW_CURSOR_SR_SHIFT));
 
@@ -1552,7 +1752,7 @@
 	else if (!IS_GEN2(dev))
 		wm_info = &i915_wm_info;
 	else
-		wm_info = &i830_wm_info;
+		wm_info = &i830_a_wm_info;
 
 	fifo_size = dev_priv->display.get_fifo_size(dev, 0);
 	crtc = intel_get_crtc_for_plane(dev, 0);
@@ -1565,10 +1765,16 @@
 		adjusted_mode = &to_intel_crtc(crtc)->config.adjusted_mode;
 		planea_wm = intel_calculate_wm(adjusted_mode->crtc_clock,
 					       wm_info, fifo_size, cpp,
-					       latency_ns);
+					       pessimal_latency_ns);
 		enabled = crtc;
-	} else
+	} else {
 		planea_wm = fifo_size - wm_info->guard_size;
+		if (planea_wm > (long)wm_info->max_wm)
+			planea_wm = wm_info->max_wm;
+	}
+
+	if (IS_GEN2(dev))
+		wm_info = &i830_bc_wm_info;
 
 	fifo_size = dev_priv->display.get_fifo_size(dev, 1);
 	crtc = intel_get_crtc_for_plane(dev, 1);
@@ -1581,13 +1787,16 @@
 		adjusted_mode = &to_intel_crtc(crtc)->config.adjusted_mode;
 		planeb_wm = intel_calculate_wm(adjusted_mode->crtc_clock,
 					       wm_info, fifo_size, cpp,
-					       latency_ns);
+					       pessimal_latency_ns);
 		if (enabled == NULL)
 			enabled = crtc;
 		else
 			enabled = NULL;
-	} else
+	} else {
 		planeb_wm = fifo_size - wm_info->guard_size;
+		if (planeb_wm > (long)wm_info->max_wm)
+			planeb_wm = wm_info->max_wm;
+	}
 
 	DRM_DEBUG_KMS("FIFO watermarks - A: %d, B: %d\n", planea_wm, planeb_wm);
 
@@ -1674,7 +1883,7 @@
 	planea_wm = intel_calculate_wm(adjusted_mode->crtc_clock,
 				       &i845_wm_info,
 				       dev_priv->display.get_fifo_size(dev, 0),
-				       4, latency_ns);
+				       4, pessimal_latency_ns);
 	fwater_lo = I915_READ(FW_BLC) & ~0xfff;
 	fwater_lo |= (3<<8) | planea_wm;
 
@@ -1751,6 +1960,14 @@
 	return DIV_ROUND_UP(pri_val * 64, horiz_pixels * bytes_per_pixel) + 2;
 }
 
+struct skl_pipe_wm_parameters {
+	bool active;
+	uint32_t pipe_htotal;
+	uint32_t pixel_rate; /* in KHz */
+	struct intel_plane_wm_parameters plane[I915_MAX_PLANES];
+	struct intel_plane_wm_parameters cursor;
+};
+
 struct ilk_pipe_wm_parameters {
 	bool active;
 	uint32_t pipe_htotal;
@@ -2062,11 +2279,82 @@
 	       PIPE_WM_LINETIME_TIME(linetime);
 }
 
-static void intel_read_wm_latency(struct drm_device *dev, uint16_t wm[5])
+static void intel_read_wm_latency(struct drm_device *dev, uint16_t wm[8])
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	if (IS_HASWELL(dev) || IS_BROADWELL(dev)) {
+	if (IS_GEN9(dev)) {
+		uint32_t val;
+		int ret, i;
+		int level, max_level = ilk_wm_max_level(dev);
+
+		/* read the first set of memory latencies[0:3] */
+		val = 0; /* data0 to be programmed to 0 for first set */
+		mutex_lock(&dev_priv->rps.hw_lock);
+		ret = sandybridge_pcode_read(dev_priv,
+					     GEN9_PCODE_READ_MEM_LATENCY,
+					     &val);
+		mutex_unlock(&dev_priv->rps.hw_lock);
+
+		if (ret) {
+			DRM_ERROR("SKL Mailbox read error = %d\n", ret);
+			return;
+		}
+
+		wm[0] = val & GEN9_MEM_LATENCY_LEVEL_MASK;
+		wm[1] = (val >> GEN9_MEM_LATENCY_LEVEL_1_5_SHIFT) &
+				GEN9_MEM_LATENCY_LEVEL_MASK;
+		wm[2] = (val >> GEN9_MEM_LATENCY_LEVEL_2_6_SHIFT) &
+				GEN9_MEM_LATENCY_LEVEL_MASK;
+		wm[3] = (val >> GEN9_MEM_LATENCY_LEVEL_3_7_SHIFT) &
+				GEN9_MEM_LATENCY_LEVEL_MASK;
+
+		/* read the second set of memory latencies[4:7] */
+		val = 1; /* data0 to be programmed to 1 for second set */
+		mutex_lock(&dev_priv->rps.hw_lock);
+		ret = sandybridge_pcode_read(dev_priv,
+					     GEN9_PCODE_READ_MEM_LATENCY,
+					     &val);
+		mutex_unlock(&dev_priv->rps.hw_lock);
+		if (ret) {
+			DRM_ERROR("SKL Mailbox read error = %d\n", ret);
+			return;
+		}
+
+		wm[4] = val & GEN9_MEM_LATENCY_LEVEL_MASK;
+		wm[5] = (val >> GEN9_MEM_LATENCY_LEVEL_1_5_SHIFT) &
+				GEN9_MEM_LATENCY_LEVEL_MASK;
+		wm[6] = (val >> GEN9_MEM_LATENCY_LEVEL_2_6_SHIFT) &
+				GEN9_MEM_LATENCY_LEVEL_MASK;
+		wm[7] = (val >> GEN9_MEM_LATENCY_LEVEL_3_7_SHIFT) &
+				GEN9_MEM_LATENCY_LEVEL_MASK;
+
+		/*
+		 * punit doesn't take into account the read latency so we need
+		 * to add 2us to the various latency levels we retrieve from
+		 * the punit.
+		 *   - W0 is a bit special in that it's the only level that
+		 *   can't be disabled if we want to have display working, so
+		 *   we always add 2us there.
+		 *   - For levels >=1, punit returns 0us latency when they are
+		 *   disabled, so we respect that and don't add 2us then
+		 *
+		 * Additionally, if a level n (n > 1) has a 0us latency, all
+		 * levels m (m >= n) need to be disabled. We make sure to
+		 * sanitize the values out of the punit to satisfy this
+		 * requirement.
+		 */
+		wm[0] += 2;
+		for (level = 1; level <= max_level; level++)
+			if (wm[level] != 0)
+				wm[level] += 2;
+			else {
+				for (i = level + 1; i <= max_level; i++)
+					wm[i] = 0;
+
+				break;
+			}
+	} else if (IS_HASWELL(dev) || IS_BROADWELL(dev)) {
 		uint64_t sskpd = I915_READ64(MCH_SSKPD);
 
 		wm[0] = (sskpd >> 56) & 0xFF;
@@ -2114,7 +2402,9 @@
 int ilk_wm_max_level(const struct drm_device *dev)
 {
 	/* how many WM levels are we expecting */
-	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
+	if (IS_GEN9(dev))
+		return 7;
+	else if (IS_HASWELL(dev) || IS_BROADWELL(dev))
 		return 4;
 	else if (INTEL_INFO(dev)->gen >= 6)
 		return 3;
@@ -2124,7 +2414,7 @@
 
 static void intel_print_wm_latency(struct drm_device *dev,
 				   const char *name,
-				   const uint16_t wm[5])
+				   const uint16_t wm[8])
 {
 	int level, max_level = ilk_wm_max_level(dev);
 
@@ -2137,8 +2427,13 @@
 			continue;
 		}
 
-		/* WM1+ latency values in 0.5us units */
-		if (level > 0)
+		/*
+		 * - latencies are in us on gen9.
+		 * - before then, WM1+ latency values are in 0.5us units
+		 */
+		if (IS_GEN9(dev))
+			latency *= 10;
+		else if (level > 0)
 			latency *= 5;
 
 		DRM_DEBUG_KMS("%s WM%d latency %u (%u.%u usec)\n",
@@ -2206,6 +2501,14 @@
 		snb_wm_latency_quirk(dev);
 }
 
+static void skl_setup_wm_latency(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	intel_read_wm_latency(dev, dev_priv->wm.skl_latency);
+	intel_print_wm_latency(dev, "Gen9 Plane", dev_priv->wm.skl_latency);
+}
+
 static void ilk_compute_wm_parameters(struct drm_crtc *crtc,
 				      struct ilk_pipe_wm_parameters *p)
 {
@@ -2527,7 +2830,7 @@
 #define WM_DIRTY_FBC (1 << 24)
 #define WM_DIRTY_DDB (1 << 25)
 
-static unsigned int ilk_compute_wm_dirty(struct drm_device *dev,
+static unsigned int ilk_compute_wm_dirty(struct drm_i915_private *dev_priv,
 					 const struct ilk_wm_values *old,
 					 const struct ilk_wm_values *new)
 {
@@ -2535,7 +2838,7 @@
 	enum pipe pipe;
 	int wm_lp;
 
-	for_each_pipe(pipe) {
+	for_each_pipe(dev_priv, pipe) {
 		if (old->wm_linetime[pipe] != new->wm_linetime[pipe]) {
 			dirty |= WM_DIRTY_LINETIME(pipe);
 			/* Must disable LP1+ watermarks too */
@@ -2621,7 +2924,7 @@
 	unsigned int dirty;
 	uint32_t val;
 
-	dirty = ilk_compute_wm_dirty(dev, previous, results);
+	dirty = ilk_compute_wm_dirty(dev_priv, previous, results);
 	if (!dirty)
 		return;
 
@@ -2696,1339 +2999,1664 @@
 	return _ilk_disable_lp_wm(dev_priv, WM_DIRTY_LP_ALL);
 }
 
-static void ilk_update_wm(struct drm_crtc *crtc)
-{
-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
-	struct drm_device *dev = crtc->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct ilk_wm_maximums max;
-	struct ilk_pipe_wm_parameters params = {};
-	struct ilk_wm_values results = {};
-	enum intel_ddb_partitioning partitioning;
-	struct intel_pipe_wm pipe_wm = {};
-	struct intel_pipe_wm lp_wm_1_2 = {}, lp_wm_5_6 = {}, *best_lp_wm;
-	struct intel_wm_config config = {};
+/*
+ * On gen9, we need to allocate Display Data Buffer (DDB) portions to the
+ * different active planes.
+ */
 
-	ilk_compute_wm_parameters(crtc, &params);
+#define SKL_DDB_SIZE		896	/* in blocks */
 
-	intel_compute_pipe_wm(crtc, &params, &pipe_wm);
+static void
+skl_ddb_get_pipe_allocation_limits(struct drm_device *dev,
+				   struct drm_crtc *for_crtc,
+				   const struct intel_wm_config *config,
+				   const struct skl_pipe_wm_parameters *params,
+				   struct skl_ddb_entry *alloc /* out */)
+{
+	struct drm_crtc *crtc;
+	unsigned int pipe_size, ddb_size;
+	int nth_active_pipe;
 
-	if (!memcmp(&intel_crtc->wm.active, &pipe_wm, sizeof(pipe_wm)))
+	if (!params->active) {
+		alloc->start = 0;
+		alloc->end = 0;
 		return;
+	}
 
-	intel_crtc->wm.active = pipe_wm;
+	ddb_size = SKL_DDB_SIZE;
 
-	ilk_compute_wm_config(dev, &config);
+	ddb_size -= 4; /* 4 blocks for bypass path allocation */
 
-	ilk_compute_wm_maximums(dev, 1, &config, INTEL_DDB_PART_1_2, &max);
-	ilk_wm_merge(dev, &config, &max, &lp_wm_1_2);
+	nth_active_pipe = 0;
+	for_each_crtc(dev, crtc) {
+		if (!intel_crtc_active(crtc))
+			continue;
 
-	/* 5/6 split only in single pipe config on IVB+ */
-	if (INTEL_INFO(dev)->gen >= 7 &&
-	    config.num_pipes_active == 1 && config.sprites_enabled) {
-		ilk_compute_wm_maximums(dev, 1, &config, INTEL_DDB_PART_5_6, &max);
-		ilk_wm_merge(dev, &config, &max, &lp_wm_5_6);
+		if (crtc == for_crtc)
+			break;
 
-		best_lp_wm = ilk_find_best_result(dev, &lp_wm_1_2, &lp_wm_5_6);
-	} else {
-		best_lp_wm = &lp_wm_1_2;
+		nth_active_pipe++;
 	}
 
-	partitioning = (best_lp_wm == &lp_wm_1_2) ?
-		       INTEL_DDB_PART_1_2 : INTEL_DDB_PART_5_6;
+	pipe_size = ddb_size / config->num_pipes_active;
+	alloc->start = nth_active_pipe * ddb_size / config->num_pipes_active;
+	alloc->end = alloc->start + pipe_size;
+}
 
-	ilk_compute_wm_results(dev, best_lp_wm, partitioning, &results);
+static unsigned int skl_cursor_allocation(const struct intel_wm_config *config)
+{
+	if (config->num_pipes_active == 1)
+		return 32;
 
-	ilk_write_wm_values(dev_priv, &results);
+	return 8;
 }
 
-static void
-ilk_update_sprite_wm(struct drm_plane *plane,
-		     struct drm_crtc *crtc,
-		     uint32_t sprite_width, uint32_t sprite_height,
-		     int pixel_size, bool enabled, bool scaled)
+static void skl_ddb_entry_init_from_hw(struct skl_ddb_entry *entry, u32 reg)
 {
-	struct drm_device *dev = plane->dev;
-	struct intel_plane *intel_plane = to_intel_plane(plane);
+	entry->start = reg & 0x3ff;
+	entry->end = (reg >> 16) & 0x3ff;
+	if (entry->end)
+		entry->end += 1;
+}
 
-	intel_plane->wm.enabled = enabled;
-	intel_plane->wm.scaled = scaled;
-	intel_plane->wm.horiz_pixels = sprite_width;
-	intel_plane->wm.vert_pixels = sprite_width;
-	intel_plane->wm.bytes_per_pixel = pixel_size;
+void skl_ddb_get_hw_state(struct drm_i915_private *dev_priv,
+			  struct skl_ddb_allocation *ddb /* out */)
+{
+	struct drm_device *dev = dev_priv->dev;
+	enum pipe pipe;
+	int plane;
+	u32 val;
 
-	/*
-	 * IVB workaround: must disable low power watermarks for at least
-	 * one frame before enabling scaling.  LP watermarks can be re-enabled
-	 * when scaling is disabled.
-	 *
-	 * WaCxSRDisabledForSpriteScaling:ivb
-	 */
-	if (IS_IVYBRIDGE(dev) && scaled && ilk_disable_lp_wm(dev))
-		intel_wait_for_vblank(dev, intel_plane->pipe);
+	for_each_pipe(dev_priv, pipe) {
+		for_each_plane(pipe, plane) {
+			val = I915_READ(PLANE_BUF_CFG(pipe, plane));
+			skl_ddb_entry_init_from_hw(&ddb->plane[pipe][plane],
+						   val);
+		}
 
-	ilk_update_wm(crtc);
+		val = I915_READ(CUR_BUF_CFG(pipe));
+		skl_ddb_entry_init_from_hw(&ddb->cursor[pipe], val);
+	}
 }
 
-static void ilk_pipe_wm_get_hw_state(struct drm_crtc *crtc)
+static unsigned int
+skl_plane_relative_data_rate(const struct intel_plane_wm_parameters *p)
+{
+	return p->horiz_pixels * p->vert_pixels * p->bytes_per_pixel;
+}
+
+/*
+ * We don't overflow 32 bits. Worst case is 3 planes enabled, each fetching
+ * a 8192x4096@32bpp framebuffer:
+ *   3 * 4096 * 8192  * 4 < 2^32
+ */
+static unsigned int
+skl_get_total_relative_data_rate(struct intel_crtc *intel_crtc,
+				 const struct skl_pipe_wm_parameters *params)
+{
+	unsigned int total_data_rate = 0;
+	int plane;
+
+	for (plane = 0; plane < intel_num_planes(intel_crtc); plane++) {
+		const struct intel_plane_wm_parameters *p;
+
+		p = &params->plane[plane];
+		if (!p->enabled)
+			continue;
+
+		total_data_rate += skl_plane_relative_data_rate(p);
+	}
+
+	return total_data_rate;
+}
+
+static void
+skl_allocate_pipe_ddb(struct drm_crtc *crtc,
+		      const struct intel_wm_config *config,
+		      const struct skl_pipe_wm_parameters *params,
+		      struct skl_ddb_allocation *ddb /* out */)
 {
 	struct drm_device *dev = crtc->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct ilk_wm_values *hw = &dev_priv->wm.hw;
 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
-	struct intel_pipe_wm *active = &intel_crtc->wm.active;
 	enum pipe pipe = intel_crtc->pipe;
-	static const unsigned int wm0_pipe_reg[] = {
-		[PIPE_A] = WM0_PIPEA_ILK,
-		[PIPE_B] = WM0_PIPEB_ILK,
-		[PIPE_C] = WM0_PIPEC_IVB,
-	};
+	struct skl_ddb_entry *alloc = &ddb->pipe[pipe];
+	uint16_t alloc_size, start, cursor_blocks;
+	unsigned int total_data_rate;
+	int plane;
 
-	hw->wm_pipe[pipe] = I915_READ(wm0_pipe_reg[pipe]);
-	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
-		hw->wm_linetime[pipe] = I915_READ(PIPE_WM_LINETIME(pipe));
+	skl_ddb_get_pipe_allocation_limits(dev, crtc, config, params, alloc);
+	alloc_size = skl_ddb_entry_size(alloc);
+	if (alloc_size == 0) {
+		memset(ddb->plane[pipe], 0, sizeof(ddb->plane[pipe]));
+		memset(&ddb->cursor[pipe], 0, sizeof(ddb->cursor[pipe]));
+		return;
+	}
 
-	active->pipe_enabled = intel_crtc_active(crtc);
+	cursor_blocks = skl_cursor_allocation(config);
+	ddb->cursor[pipe].start = alloc->end - cursor_blocks;
+	ddb->cursor[pipe].end = alloc->end;
 
-	if (active->pipe_enabled) {
-		u32 tmp = hw->wm_pipe[pipe];
+	alloc_size -= cursor_blocks;
+	alloc->end -= cursor_blocks;
 
-		/*
-		 * For active pipes LP0 watermark is marked as
-		 * enabled, and LP1+ watermaks as disabled since
-		 * we can't really reverse compute them in case
-		 * multiple pipes are active.
-		 */
-		active->wm[0].enable = true;
-		active->wm[0].pri_val = (tmp & WM0_PIPE_PLANE_MASK) >> WM0_PIPE_PLANE_SHIFT;
-		active->wm[0].spr_val = (tmp & WM0_PIPE_SPRITE_MASK) >> WM0_PIPE_SPRITE_SHIFT;
-		active->wm[0].cur_val = tmp & WM0_PIPE_CURSOR_MASK;
-		active->linetime = hw->wm_linetime[pipe];
-	} else {
-		int level, max_level = ilk_wm_max_level(dev);
+	/*
+	 * Each active plane get a portion of the remaining space, in
+	 * proportion to the amount of data they need to fetch from memory.
+	 *
+	 * FIXME: we may not allocate every single block here.
+	 */
+	total_data_rate = skl_get_total_relative_data_rate(intel_crtc, params);
+
+	start = alloc->start;
+	for (plane = 0; plane < intel_num_planes(intel_crtc); plane++) {
+		const struct intel_plane_wm_parameters *p;
+		unsigned int data_rate;
+		uint16_t plane_blocks;
+
+		p = &params->plane[plane];
+		if (!p->enabled)
+			continue;
+
+		data_rate = skl_plane_relative_data_rate(p);
 
 		/*
-		 * For inactive pipes, all watermark levels
-		 * should be marked as enabled but zeroed,
-		 * which is what we'd compute them to.
+		 * promote the expression to 64 bits to avoid overflowing, the
+		 * result is < available as data_rate / total_data_rate < 1
 		 */
-		for (level = 0; level <= max_level; level++)
-			active->wm[level].enable = true;
+		plane_blocks = div_u64((uint64_t)alloc_size * data_rate,
+				       total_data_rate);
+
+		ddb->plane[pipe][plane].start = start;
+		ddb->plane[pipe][plane].end = start + plane_blocks;
+
+		start += plane_blocks;
 	}
+
 }
 
-void ilk_wm_get_hw_state(struct drm_device *dev)
+static uint32_t skl_pipe_pixel_rate(const struct intel_crtc_config *config)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct ilk_wm_values *hw = &dev_priv->wm.hw;
-	struct drm_crtc *crtc;
-
-	for_each_crtc(dev, crtc)
-		ilk_pipe_wm_get_hw_state(crtc);
+	/* TODO: Take into account the scalers once we support them */
+	return config->adjusted_mode.crtc_clock;
+}
 
-	hw->wm_lp[0] = I915_READ(WM1_LP_ILK);
-	hw->wm_lp[1] = I915_READ(WM2_LP_ILK);
-	hw->wm_lp[2] = I915_READ(WM3_LP_ILK);
+/*
+ * The max latency should be 257 (max the punit can code is 255 and we add 2us
+ * for the read latency) and bytes_per_pixel should always be <= 8, so that
+ * should allow pixel_rate up to ~2 GHz which seems sufficient since max
+ * 2xcdclk is 1350 MHz and the pixel rate should never exceed that.
+*/
+static uint32_t skl_wm_method1(uint32_t pixel_rate, uint8_t bytes_per_pixel,
+			       uint32_t latency)
+{
+	uint32_t wm_intermediate_val, ret;
 
-	hw->wm_lp_spr[0] = I915_READ(WM1S_LP_ILK);
-	if (INTEL_INFO(dev)->gen >= 7) {
-		hw->wm_lp_spr[1] = I915_READ(WM2S_LP_IVB);
-		hw->wm_lp_spr[2] = I915_READ(WM3S_LP_IVB);
-	}
+	if (latency == 0)
+		return UINT_MAX;
 
-	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
-		hw->partitioning = (I915_READ(WM_MISC) & WM_MISC_DATA_PARTITION_5_6) ?
-			INTEL_DDB_PART_5_6 : INTEL_DDB_PART_1_2;
-	else if (IS_IVYBRIDGE(dev))
-		hw->partitioning = (I915_READ(DISP_ARB_CTL2) & DISP_DATA_PARTITION_5_6) ?
-			INTEL_DDB_PART_5_6 : INTEL_DDB_PART_1_2;
+	wm_intermediate_val = latency * pixel_rate * bytes_per_pixel;
+	ret = DIV_ROUND_UP(wm_intermediate_val, 1000);
 
-	hw->enable_fbc_wm =
-		!(I915_READ(DISP_ARB_CTL) & DISP_FBC_WM_DIS);
+	return ret;
 }
 
-/**
- * intel_update_watermarks - update FIFO watermark values based on current modes
- *
- * Calculate watermark values for the various WM regs based on current mode
- * and plane configuration.
- *
- * There are several cases to deal with here:
- *   - normal (i.e. non-self-refresh)
- *   - self-refresh (SR) mode
- *   - lines are large relative to FIFO size (buffer can hold up to 2)
- *   - lines are small relative to FIFO size (buffer can hold more than 2
- *     lines), so need to account for TLB latency
- *
- *   The normal calculation is:
- *     watermark = dotclock * bytes per pixel * latency
- *   where latency is platform & configuration dependent (we assume pessimal
- *   values here).
- *
- *   The SR calculation is:
- *     watermark = (trunc(latency/line time)+1) * surface width *
- *       bytes per pixel
- *   where
- *     line time = htotal / dotclock
- *     surface width = hdisplay for normal plane and 64 for cursor
- *   and latency is assumed to be high, as above.
- *
- * The final value programmed to the register should always be rounded up,
- * and include an extra 2 entries to account for clock crossings.
- *
- * We don't use the sprite, so we can ignore that.  And on Crestline we have
- * to set the non-SR watermarks to 8.
- */
-void intel_update_watermarks(struct drm_crtc *crtc)
+static uint32_t skl_wm_method2(uint32_t pixel_rate, uint32_t pipe_htotal,
+			       uint32_t horiz_pixels, uint8_t bytes_per_pixel,
+			       uint32_t latency)
 {
-	struct drm_i915_private *dev_priv = crtc->dev->dev_private;
+	uint32_t ret, plane_bytes_per_line, wm_intermediate_val;
 
-	if (dev_priv->display.update_wm)
-		dev_priv->display.update_wm(crtc);
+	if (latency == 0)
+		return UINT_MAX;
+
+	plane_bytes_per_line = horiz_pixels * bytes_per_pixel;
+	wm_intermediate_val = latency * pixel_rate;
+	ret = DIV_ROUND_UP(wm_intermediate_val, pipe_htotal * 1000) *
+				plane_bytes_per_line;
+
+	return ret;
 }
 
-void intel_update_sprite_watermarks(struct drm_plane *plane,
-				    struct drm_crtc *crtc,
-				    uint32_t sprite_width,
-				    uint32_t sprite_height,
-				    int pixel_size,
-				    bool enabled, bool scaled)
+static bool skl_ddb_allocation_changed(const struct skl_ddb_allocation *new_ddb,
+				       const struct intel_crtc *intel_crtc)
 {
-	struct drm_i915_private *dev_priv = plane->dev->dev_private;
+	struct drm_device *dev = intel_crtc->base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	const struct skl_ddb_allocation *cur_ddb = &dev_priv->wm.skl_hw.ddb;
+	enum pipe pipe = intel_crtc->pipe;
 
-	if (dev_priv->display.update_sprite_wm)
-		dev_priv->display.update_sprite_wm(plane, crtc,
-						   sprite_width, sprite_height,
-						   pixel_size, enabled, scaled);
+	if (memcmp(new_ddb->plane[pipe], cur_ddb->plane[pipe],
+		   sizeof(new_ddb->plane[pipe])))
+		return true;
+
+	if (memcmp(&new_ddb->cursor[pipe], &cur_ddb->cursor[pipe],
+		    sizeof(new_ddb->cursor[pipe])))
+		return true;
+
+	return false;
 }
 
-static struct drm_i915_gem_object *
-intel_alloc_context_page(struct drm_device *dev)
+static void skl_compute_wm_global_parameters(struct drm_device *dev,
+					     struct intel_wm_config *config)
 {
-	struct drm_i915_gem_object *ctx;
-	int ret;
+	struct drm_crtc *crtc;
+	struct drm_plane *plane;
 
-	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
+	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head)
+		config->num_pipes_active += intel_crtc_active(crtc);
 
-	ctx = i915_gem_alloc_object(dev, 4096);
-	if (!ctx) {
-		DRM_DEBUG("failed to alloc power context, RC6 disabled\n");
-		return NULL;
-	}
+	/* FIXME: I don't think we need those two global parameters on SKL */
+	list_for_each_entry(plane, &dev->mode_config.plane_list, head) {
+		struct intel_plane *intel_plane = to_intel_plane(plane);
 
-	ret = i915_gem_obj_ggtt_pin(ctx, 4096, 0);
-	if (ret) {
-		DRM_ERROR("failed to pin power context: %d\n", ret);
-		goto err_unref;
+		config->sprites_enabled |= intel_plane->wm.enabled;
+		config->sprites_scaled |= intel_plane->wm.scaled;
 	}
+}
 
-	ret = i915_gem_object_set_to_gtt_domain(ctx, 1);
-	if (ret) {
-		DRM_ERROR("failed to set-domain on power context: %d\n", ret);
-		goto err_unpin;
+static void skl_compute_wm_pipe_parameters(struct drm_crtc *crtc,
+					   struct skl_pipe_wm_parameters *p)
+{
+	struct drm_device *dev = crtc->dev;
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	enum pipe pipe = intel_crtc->pipe;
+	struct drm_plane *plane;
+	int i = 1; /* Index for sprite planes start */
+
+	p->active = intel_crtc_active(crtc);
+	if (p->active) {
+		p->pipe_htotal = intel_crtc->config.adjusted_mode.crtc_htotal;
+		p->pixel_rate = skl_pipe_pixel_rate(&intel_crtc->config);
+
+		/*
+		 * For now, assume primary and cursor planes are always enabled.
+		 */
+		p->plane[0].enabled = true;
+		p->plane[0].bytes_per_pixel =
+			crtc->primary->fb->bits_per_pixel / 8;
+		p->plane[0].horiz_pixels = intel_crtc->config.pipe_src_w;
+		p->plane[0].vert_pixels = intel_crtc->config.pipe_src_h;
+
+		p->cursor.enabled = true;
+		p->cursor.bytes_per_pixel = 4;
+		p->cursor.horiz_pixels = intel_crtc->cursor_width ?
+					 intel_crtc->cursor_width : 64;
 	}
 
-	return ctx;
+	list_for_each_entry(plane, &dev->mode_config.plane_list, head) {
+		struct intel_plane *intel_plane = to_intel_plane(plane);
 
-err_unpin:
-	i915_gem_object_ggtt_unpin(ctx);
-err_unref:
-	drm_gem_object_unreference(&ctx->base);
-	return NULL;
+		if (intel_plane->pipe == pipe)
+			p->plane[i++] = intel_plane->wm;
+	}
 }
 
-/**
- * Lock protecting IPS related data structures
- */
-DEFINE_SPINLOCK(mchdev_lock);
+static bool skl_compute_plane_wm(struct skl_pipe_wm_parameters *p,
+				 struct intel_plane_wm_parameters *p_params,
+				 uint16_t ddb_allocation,
+				 uint32_t mem_value,
+				 uint16_t *out_blocks, /* out */
+				 uint8_t *out_lines /* out */)
+{
+	uint32_t method1, method2, plane_bytes_per_line, res_blocks, res_lines;
+	uint32_t result_bytes;
 
-/* Global for IPS driver to get at the current i915 device. Protected by
- * mchdev_lock. */
-static struct drm_i915_private *i915_mch_dev;
+	if (mem_value == 0 || !p->active || !p_params->enabled)
+		return false;
 
-bool ironlake_set_drps(struct drm_device *dev, u8 val)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	u16 rgvswctl;
+	method1 = skl_wm_method1(p->pixel_rate,
+				 p_params->bytes_per_pixel,
+				 mem_value);
+	method2 = skl_wm_method2(p->pixel_rate,
+				 p->pipe_htotal,
+				 p_params->horiz_pixels,
+				 p_params->bytes_per_pixel,
+				 mem_value);
 
-	assert_spin_locked(&mchdev_lock);
+	plane_bytes_per_line = p_params->horiz_pixels *
+					p_params->bytes_per_pixel;
 
-	rgvswctl = I915_READ16(MEMSWCTL);
-	if (rgvswctl & MEMCTL_CMD_STS) {
-		DRM_DEBUG("gpu busy, RCS change rejected\n");
-		return false; /* still busy with another command */
-	}
+	/* For now xtile and linear */
+	if (((ddb_allocation * 512) / plane_bytes_per_line) >= 1)
+		result_bytes = min(method1, method2);
+	else
+		result_bytes = method1;
 
-	rgvswctl = (MEMCTL_CMD_CHFREQ << MEMCTL_CMD_SHIFT) |
-		(val << MEMCTL_FREQ_SHIFT) | MEMCTL_SFCAVM;
-	I915_WRITE16(MEMSWCTL, rgvswctl);
-	POSTING_READ16(MEMSWCTL);
+	res_blocks = DIV_ROUND_UP(result_bytes, 512) + 1;
+	res_lines = DIV_ROUND_UP(result_bytes, plane_bytes_per_line);
 
-	rgvswctl |= MEMCTL_CMD_STS;
-	I915_WRITE16(MEMSWCTL, rgvswctl);
+	if (res_blocks > ddb_allocation || res_lines > 31)
+		return false;
+
+	*out_blocks = res_blocks;
+	*out_lines = res_lines;
 
 	return true;
 }
 
-static void ironlake_enable_drps(struct drm_device *dev)
+static void skl_compute_wm_level(const struct drm_i915_private *dev_priv,
+				 struct skl_ddb_allocation *ddb,
+				 struct skl_pipe_wm_parameters *p,
+				 enum pipe pipe,
+				 int level,
+				 int num_planes,
+				 struct skl_wm_level *result)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	u32 rgvmodectl = I915_READ(MEMMODECTL);
-	u8 fmax, fmin, fstart, vstart;
-
-	spin_lock_irq(&mchdev_lock);
+	uint16_t latency = dev_priv->wm.skl_latency[level];
+	uint16_t ddb_blocks;
+	int i;
 
-	/* Enable temp reporting */
-	I915_WRITE16(PMMISC, I915_READ(PMMISC) | MCPPCE_EN);
-	I915_WRITE16(TSC1, I915_READ(TSC1) | TSE);
+	for (i = 0; i < num_planes; i++) {
+		ddb_blocks = skl_ddb_entry_size(&ddb->plane[pipe][i]);
 
-	/* 100ms RC evaluation intervals */
-	I915_WRITE(RCUPEI, 100000);
-	I915_WRITE(RCDNEI, 100000);
+		result->plane_en[i] = skl_compute_plane_wm(p, &p->plane[i],
+						ddb_blocks,
+						latency,
+						&result->plane_res_b[i],
+						&result->plane_res_l[i]);
+	}
 
-	/* Set max/min thresholds to 90ms and 80ms respectively */
-	I915_WRITE(RCBMAXAVG, 90000);
-	I915_WRITE(RCBMINAVG, 80000);
+	ddb_blocks = skl_ddb_entry_size(&ddb->cursor[pipe]);
+	result->cursor_en = skl_compute_plane_wm(p, &p->cursor, ddb_blocks,
+						 latency, &result->cursor_res_b,
+						 &result->cursor_res_l);
+}
 
-	I915_WRITE(MEMIHYST, 1);
+static uint32_t
+skl_compute_linetime_wm(struct drm_crtc *crtc, struct skl_pipe_wm_parameters *p)
+{
+	if (!intel_crtc_active(crtc))
+		return 0;
 
-	/* Set up min, max, and cur for interrupt handling */
-	fmax = (rgvmodectl & MEMMODE_FMAX_MASK) >> MEMMODE_FMAX_SHIFT;
-	fmin = (rgvmodectl & MEMMODE_FMIN_MASK);
-	fstart = (rgvmodectl & MEMMODE_FSTART_MASK) >>
-		MEMMODE_FSTART_SHIFT;
+	return DIV_ROUND_UP(8 * p->pipe_htotal * 1000, p->pixel_rate);
 
-	vstart = (I915_READ(PXVFREQ_BASE + (fstart * 4)) & PXVFREQ_PX_MASK) >>
-		PXVFREQ_PX_SHIFT;
+}
 
-	dev_priv->ips.fmax = fmax; /* IPS callback will increase this */
-	dev_priv->ips.fstart = fstart;
+static void skl_compute_transition_wm(struct drm_crtc *crtc,
+				      struct skl_pipe_wm_parameters *params,
+				      struct skl_wm_level *trans_wm /* out */)
+{
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	int i;
 
-	dev_priv->ips.max_delay = fstart;
-	dev_priv->ips.min_delay = fmin;
-	dev_priv->ips.cur_delay = fstart;
+	if (!params->active)
+		return;
 
-	DRM_DEBUG_DRIVER("fmax: %d, fmin: %d, fstart: %d\n",
-			 fmax, fmin, fstart);
+	/* Until we know more, just disable transition WMs */
+	for (i = 0; i < intel_num_planes(intel_crtc); i++)
+		trans_wm->plane_en[i] = false;
+	trans_wm->cursor_en = false;
+}
 
-	I915_WRITE(MEMINTREN, MEMINT_CX_SUPR_EN | MEMINT_EVAL_CHG_EN);
+static void skl_compute_pipe_wm(struct drm_crtc *crtc,
+				struct skl_ddb_allocation *ddb,
+				struct skl_pipe_wm_parameters *params,
+				struct skl_pipe_wm *pipe_wm)
+{
+	struct drm_device *dev = crtc->dev;
+	const struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	int level, max_level = ilk_wm_max_level(dev);
 
-	/*
-	 * Interrupts will be enabled in ironlake_irq_postinstall
-	 */
+	for (level = 0; level <= max_level; level++) {
+		skl_compute_wm_level(dev_priv, ddb, params, intel_crtc->pipe,
+				     level, intel_num_planes(intel_crtc),
+				     &pipe_wm->wm[level]);
+	}
+	pipe_wm->linetime = skl_compute_linetime_wm(crtc, params);
 
-	I915_WRITE(VIDSTART, vstart);
-	POSTING_READ(VIDSTART);
+	skl_compute_transition_wm(crtc, params, &pipe_wm->trans_wm);
+}
 
-	rgvmodectl |= MEMMODE_SWMODE_EN;
-	I915_WRITE(MEMMODECTL, rgvmodectl);
+static void skl_compute_wm_results(struct drm_device *dev,
+				   struct skl_pipe_wm_parameters *p,
+				   struct skl_pipe_wm *p_wm,
+				   struct skl_wm_values *r,
+				   struct intel_crtc *intel_crtc)
+{
+	int level, max_level = ilk_wm_max_level(dev);
+	enum pipe pipe = intel_crtc->pipe;
+	uint32_t temp;
+	int i;
 
-	if (wait_for_atomic((I915_READ(MEMSWCTL) & MEMCTL_CMD_STS) == 0, 10))
-		DRM_ERROR("stuck trying to change perf mode\n");
-	mdelay(1);
+	for (level = 0; level <= max_level; level++) {
+		for (i = 0; i < intel_num_planes(intel_crtc); i++) {
+			temp = 0;
 
-	ironlake_set_drps(dev, fstart);
+			temp |= p_wm->wm[level].plane_res_l[i] <<
+					PLANE_WM_LINES_SHIFT;
+			temp |= p_wm->wm[level].plane_res_b[i];
+			if (p_wm->wm[level].plane_en[i])
+				temp |= PLANE_WM_EN;
 
-	dev_priv->ips.last_count1 = I915_READ(0x112e4) + I915_READ(0x112e8) +
-		I915_READ(0x112e0);
-	dev_priv->ips.last_time1 = jiffies_to_msecs(jiffies);
-	dev_priv->ips.last_count2 = I915_READ(0x112f4);
-	dev_priv->ips.last_time2 = ktime_get_raw_ns();
+			r->plane[pipe][i][level] = temp;
+		}
 
-	spin_unlock_irq(&mchdev_lock);
-}
+		temp = 0;
 
-static void ironlake_disable_drps(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	u16 rgvswctl;
+		temp |= p_wm->wm[level].cursor_res_l << PLANE_WM_LINES_SHIFT;
+		temp |= p_wm->wm[level].cursor_res_b;
 
-	spin_lock_irq(&mchdev_lock);
+		if (p_wm->wm[level].cursor_en)
+			temp |= PLANE_WM_EN;
 
-	rgvswctl = I915_READ16(MEMSWCTL);
+		r->cursor[pipe][level] = temp;
 
-	/* Ack interrupts, disable EFC interrupt */
-	I915_WRITE(MEMINTREN, I915_READ(MEMINTREN) & ~MEMINT_EVAL_CHG_EN);
-	I915_WRITE(MEMINTRSTS, MEMINT_EVAL_CHG);
-	I915_WRITE(DEIER, I915_READ(DEIER) & ~DE_PCU_EVENT);
-	I915_WRITE(DEIIR, DE_PCU_EVENT);
-	I915_WRITE(DEIMR, I915_READ(DEIMR) | DE_PCU_EVENT);
+	}
 
-	/* Go back to the starting frequency */
-	ironlake_set_drps(dev, dev_priv->ips.fstart);
-	mdelay(1);
-	rgvswctl |= MEMCTL_CMD_STS;
-	I915_WRITE(MEMSWCTL, rgvswctl);
-	mdelay(1);
+	/* transition WMs */
+	for (i = 0; i < intel_num_planes(intel_crtc); i++) {
+		temp = 0;
+		temp |= p_wm->trans_wm.plane_res_l[i] << PLANE_WM_LINES_SHIFT;
+		temp |= p_wm->trans_wm.plane_res_b[i];
+		if (p_wm->trans_wm.plane_en[i])
+			temp |= PLANE_WM_EN;
 
-	spin_unlock_irq(&mchdev_lock);
-}
+		r->plane_trans[pipe][i] = temp;
+	}
 
-/* There's a funny hw issue where the hw returns all 0 when reading from
- * GEN6_RP_INTERRUPT_LIMITS. Hence we always need to compute the desired value
- * ourselves, instead of doing a rmw cycle (which might result in us clearing
- * all limits and the gpu stuck at whatever frequency it is at atm).
- */
-static u32 gen6_rps_limits(struct drm_i915_private *dev_priv, u8 val)
-{
-	u32 limits;
+	temp = 0;
+	temp |= p_wm->trans_wm.cursor_res_l << PLANE_WM_LINES_SHIFT;
+	temp |= p_wm->trans_wm.cursor_res_b;
+	if (p_wm->trans_wm.cursor_en)
+		temp |= PLANE_WM_EN;
 
-	/* Only set the down limit when we've reached the lowest level to avoid
-	 * getting more interrupts, otherwise leave this clear. This prevents a
-	 * race in the hw when coming out of rc6: There's a tiny window where
-	 * the hw runs at the minimal clock before selecting the desired
-	 * frequency, if the down threshold expires in that window we will not
-	 * receive a down interrupt. */
-	limits = dev_priv->rps.max_freq_softlimit << 24;
-	if (val <= dev_priv->rps.min_freq_softlimit)
-		limits |= dev_priv->rps.min_freq_softlimit << 16;
+	r->cursor_trans[pipe] = temp;
 
-	return limits;
+	r->wm_linetime[pipe] = p_wm->linetime;
 }
 
-static void gen6_set_rps_thresholds(struct drm_i915_private *dev_priv, u8 val)
+static void skl_ddb_entry_write(struct drm_i915_private *dev_priv, uint32_t reg,
+				const struct skl_ddb_entry *entry)
 {
-	int new_power;
+	if (entry->end)
+		I915_WRITE(reg, (entry->end - 1) << 16 | entry->start);
+	else
+		I915_WRITE(reg, 0);
+}
 
-	new_power = dev_priv->rps.power;
-	switch (dev_priv->rps.power) {
-	case LOW_POWER:
-		if (val > dev_priv->rps.efficient_freq + 1 && val > dev_priv->rps.cur_freq)
-			new_power = BETWEEN;
-		break;
+static void skl_write_wm_values(struct drm_i915_private *dev_priv,
+				const struct skl_wm_values *new)
+{
+	struct drm_device *dev = dev_priv->dev;
+	struct intel_crtc *crtc;
 
-	case BETWEEN:
-		if (val <= dev_priv->rps.efficient_freq && val < dev_priv->rps.cur_freq)
-			new_power = LOW_POWER;
-		else if (val >= dev_priv->rps.rp0_freq && val > dev_priv->rps.cur_freq)
-			new_power = HIGH_POWER;
-		break;
+	list_for_each_entry(crtc, &dev->mode_config.crtc_list, base.head) {
+		int i, level, max_level = ilk_wm_max_level(dev);
+		enum pipe pipe = crtc->pipe;
 
-	case HIGH_POWER:
-		if (val < (dev_priv->rps.rp1_freq + dev_priv->rps.rp0_freq) >> 1 && val < dev_priv->rps.cur_freq)
-			new_power = BETWEEN;
-		break;
-	}
-	/* Max/min bins are special */
-	if (val == dev_priv->rps.min_freq_softlimit)
-		new_power = LOW_POWER;
-	if (val == dev_priv->rps.max_freq_softlimit)
-		new_power = HIGH_POWER;
-	if (new_power == dev_priv->rps.power)
-		return;
+		if (!new->dirty[pipe])
+			continue;
 
-	/* Note the units here are not exactly 1us, but 1280ns. */
-	switch (new_power) {
-	case LOW_POWER:
-		/* Upclock if more than 95% busy over 16ms */
-		I915_WRITE(GEN6_RP_UP_EI, 12500);
-		I915_WRITE(GEN6_RP_UP_THRESHOLD, 11800);
+		I915_WRITE(PIPE_WM_LINETIME(pipe), new->wm_linetime[pipe]);
 
-		/* Downclock if less than 85% busy over 32ms */
-		I915_WRITE(GEN6_RP_DOWN_EI, 25000);
-		I915_WRITE(GEN6_RP_DOWN_THRESHOLD, 21250);
+		for (level = 0; level <= max_level; level++) {
+			for (i = 0; i < intel_num_planes(crtc); i++)
+				I915_WRITE(PLANE_WM(pipe, i, level),
+					   new->plane[pipe][i][level]);
+			I915_WRITE(CUR_WM(pipe, level),
+				   new->cursor[pipe][level]);
+		}
+		for (i = 0; i < intel_num_planes(crtc); i++)
+			I915_WRITE(PLANE_WM_TRANS(pipe, i),
+				   new->plane_trans[pipe][i]);
+		I915_WRITE(CUR_WM_TRANS(pipe), new->cursor_trans[pipe]);
+
+		for (i = 0; i < intel_num_planes(crtc); i++)
+			skl_ddb_entry_write(dev_priv,
+					    PLANE_BUF_CFG(pipe, i),
+					    &new->ddb.plane[pipe][i]);
 
-		I915_WRITE(GEN6_RP_CONTROL,
-			   GEN6_RP_MEDIA_TURBO |
-			   GEN6_RP_MEDIA_HW_NORMAL_MODE |
-			   GEN6_RP_MEDIA_IS_GFX |
-			   GEN6_RP_ENABLE |
-			   GEN6_RP_UP_BUSY_AVG |
-			   GEN6_RP_DOWN_IDLE_AVG);
-		break;
+		skl_ddb_entry_write(dev_priv, CUR_BUF_CFG(pipe),
+				    &new->ddb.cursor[pipe]);
+	}
+}
 
-	case BETWEEN:
-		/* Upclock if more than 90% busy over 13ms */
-		I915_WRITE(GEN6_RP_UP_EI, 10250);
-		I915_WRITE(GEN6_RP_UP_THRESHOLD, 9225);
+/*
+ * When setting up a new DDB allocation arrangement, we need to correctly
+ * sequence the times at which the new allocations for the pipes are taken into
+ * account or we'll have pipes fetching from space previously allocated to
+ * another pipe.
+ *
+ * Roughly the sequence looks like:
+ *  1. re-allocate the pipe(s) with the allocation being reduced and not
+ *     overlapping with a previous light-up pipe (another way to put it is:
+ *     pipes with their new allocation strickly included into their old ones).
+ *  2. re-allocate the other pipes that get their allocation reduced
+ *  3. allocate the pipes having their allocation increased
+ *
+ * Steps 1. and 2. are here to take care of the following case:
+ * - Initially DDB looks like this:
+ *     |   B    |   C    |
+ * - enable pipe A.
+ * - pipe B has a reduced DDB allocation that overlaps with the old pipe C
+ *   allocation
+ *     |  A  |  B  |  C  |
+ *
+ * We need to sequence the re-allocation: C, B, A (and not B, C, A).
+ */
 
-		/* Downclock if less than 75% busy over 32ms */
-		I915_WRITE(GEN6_RP_DOWN_EI, 25000);
-		I915_WRITE(GEN6_RP_DOWN_THRESHOLD, 18750);
+static void
+skl_wm_flush_pipe(struct drm_i915_private *dev_priv, enum pipe pipe, int pass)
+{
+	struct drm_device *dev = dev_priv->dev;
+	int plane;
 
-		I915_WRITE(GEN6_RP_CONTROL,
-			   GEN6_RP_MEDIA_TURBO |
-			   GEN6_RP_MEDIA_HW_NORMAL_MODE |
-			   GEN6_RP_MEDIA_IS_GFX |
-			   GEN6_RP_ENABLE |
-			   GEN6_RP_UP_BUSY_AVG |
-			   GEN6_RP_DOWN_IDLE_AVG);
-		break;
+	DRM_DEBUG_KMS("flush pipe %c (pass %d)\n", pipe_name(pipe), pass);
 
-	case HIGH_POWER:
-		/* Upclock if more than 85% busy over 10ms */
-		I915_WRITE(GEN6_RP_UP_EI, 8000);
-		I915_WRITE(GEN6_RP_UP_THRESHOLD, 6800);
+	for_each_plane(pipe, plane) {
+		I915_WRITE(PLANE_SURF(pipe, plane),
+			   I915_READ(PLANE_SURF(pipe, plane)));
+	}
+	I915_WRITE(CURBASE(pipe), I915_READ(CURBASE(pipe)));
+}
 
-		/* Downclock if less than 60% busy over 32ms */
-		I915_WRITE(GEN6_RP_DOWN_EI, 25000);
-		I915_WRITE(GEN6_RP_DOWN_THRESHOLD, 15000);
+static bool
+skl_ddb_allocation_included(const struct skl_ddb_allocation *old,
+			    const struct skl_ddb_allocation *new,
+			    enum pipe pipe)
+{
+	uint16_t old_size, new_size;
 
-		I915_WRITE(GEN6_RP_CONTROL,
-			   GEN6_RP_MEDIA_TURBO |
-			   GEN6_RP_MEDIA_HW_NORMAL_MODE |
-			   GEN6_RP_MEDIA_IS_GFX |
-			   GEN6_RP_ENABLE |
-			   GEN6_RP_UP_BUSY_AVG |
-			   GEN6_RP_DOWN_IDLE_AVG);
-		break;
-	}
+	old_size = skl_ddb_entry_size(&old->pipe[pipe]);
+	new_size = skl_ddb_entry_size(&new->pipe[pipe]);
 
-	dev_priv->rps.power = new_power;
-	dev_priv->rps.last_adj = 0;
+	return old_size != new_size &&
+	       new->pipe[pipe].start >= old->pipe[pipe].start &&
+	       new->pipe[pipe].end <= old->pipe[pipe].end;
 }
 
-static u32 gen6_rps_pm_mask(struct drm_i915_private *dev_priv, u8 val)
+static void skl_flush_wm_values(struct drm_i915_private *dev_priv,
+				struct skl_wm_values *new_values)
 {
-	u32 mask = 0;
-
-	if (val > dev_priv->rps.min_freq_softlimit)
-		mask |= GEN6_PM_RP_DOWN_THRESHOLD | GEN6_PM_RP_DOWN_TIMEOUT;
-	if (val < dev_priv->rps.max_freq_softlimit)
-		mask |= GEN6_PM_RP_UP_THRESHOLD;
+	struct drm_device *dev = dev_priv->dev;
+	struct skl_ddb_allocation *cur_ddb, *new_ddb;
+	bool reallocated[I915_MAX_PIPES] = {false, false, false};
+	struct intel_crtc *crtc;
+	enum pipe pipe;
 
-	mask |= dev_priv->pm_rps_events & (GEN6_PM_RP_DOWN_EI_EXPIRED | GEN6_PM_RP_UP_EI_EXPIRED);
-	mask &= dev_priv->pm_rps_events;
+	new_ddb = &new_values->ddb;
+	cur_ddb = &dev_priv->wm.skl_hw.ddb;
 
-	/* IVB and SNB hard hangs on looping batchbuffer
-	 * if GEN6_PM_UP_EI_EXPIRED is masked.
+	/*
+	 * First pass: flush the pipes with the new allocation contained into
+	 * the old space.
+	 *
+	 * We'll wait for the vblank on those pipes to ensure we can safely
+	 * re-allocate the freed space without this pipe fetching from it.
 	 */
-	if (INTEL_INFO(dev_priv->dev)->gen <= 7 && !IS_HASWELL(dev_priv->dev))
-		mask |= GEN6_PM_RP_UP_EI_EXPIRED;
+	for_each_intel_crtc(dev, crtc) {
+		if (!crtc->active)
+			continue;
 
-	if (IS_GEN8(dev_priv->dev))
-		mask |= GEN8_PMINTR_REDIRECT_TO_NON_DISP;
+		pipe = crtc->pipe;
 
-	return ~mask;
-}
+		if (!skl_ddb_allocation_included(cur_ddb, new_ddb, pipe))
+			continue;
 
-/* gen6_set_rps is called to update the frequency request, but should also be
- * called when the range (min_delay and max_delay) is modified so that we can
- * update the GEN6_RP_INTERRUPT_LIMITS register accordingly. */
-void gen6_set_rps(struct drm_device *dev, u8 val)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
+		skl_wm_flush_pipe(dev_priv, pipe, 1);
+		intel_wait_for_vblank(dev, pipe);
 
-	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
-	WARN_ON(val > dev_priv->rps.max_freq_softlimit);
-	WARN_ON(val < dev_priv->rps.min_freq_softlimit);
+		reallocated[pipe] = true;
+	}
 
-	/* min/max delay may still have been modified so be sure to
-	 * write the limits value.
+
+	/*
+	 * Second pass: flush the pipes that are having their allocation
+	 * reduced, but overlapping with a previous allocation.
+	 *
+	 * Here as well we need to wait for the vblank to make sure the freed
+	 * space is not used anymore.
 	 */
-	if (val != dev_priv->rps.cur_freq) {
-		gen6_set_rps_thresholds(dev_priv, val);
+	for_each_intel_crtc(dev, crtc) {
+		if (!crtc->active)
+			continue;
 
-		if (IS_HASWELL(dev) || IS_BROADWELL(dev))
-			I915_WRITE(GEN6_RPNSWREQ,
-				   HSW_FREQUENCY(val));
-		else
-			I915_WRITE(GEN6_RPNSWREQ,
-				   GEN6_FREQUENCY(val) |
-				   GEN6_OFFSET(0) |
-				   GEN6_AGGRESSIVE_TURBO);
+		pipe = crtc->pipe;
+
+		if (reallocated[pipe])
+			continue;
+
+		if (skl_ddb_entry_size(&new_ddb->pipe[pipe]) <
+		    skl_ddb_entry_size(&cur_ddb->pipe[pipe])) {
+			skl_wm_flush_pipe(dev_priv, pipe, 2);
+			intel_wait_for_vblank(dev, pipe);
+		}
+
+		reallocated[pipe] = true;
 	}
 
-	/* Make sure we continue to get interrupts
-	 * until we hit the minimum or maximum frequencies.
+	/*
+	 * Third pass: flush the pipes that got more space allocated.
+	 *
+	 * We don't need to actively wait for the update here, next vblank
+	 * will just get more DDB space with the correct WM values.
 	 */
-	I915_WRITE(GEN6_RP_INTERRUPT_LIMITS, gen6_rps_limits(dev_priv, val));
-	I915_WRITE(GEN6_PMINTRMSK, gen6_rps_pm_mask(dev_priv, val));
+	for_each_intel_crtc(dev, crtc) {
+		if (!crtc->active)
+			continue;
 
-	POSTING_READ(GEN6_RPNSWREQ);
+		pipe = crtc->pipe;
 
-	dev_priv->rps.cur_freq = val;
-	trace_intel_gpu_freq_change(val * 50);
+		/*
+		 * At this point, only the pipes more space than before are
+		 * left to re-allocate.
+		 */
+		if (reallocated[pipe])
+			continue;
+
+		skl_wm_flush_pipe(dev_priv, pipe, 3);
+	}
 }
 
-/* vlv_set_rps_idle: Set the frequency to Rpn if Gfx clocks are down
- *
- * * If Gfx is Idle, then
- * 1. Mask Turbo interrupts
- * 2. Bring up Gfx clock
- * 3. Change the freq to Rpn and wait till P-Unit updates freq
- * 4. Clear the Force GFX CLK ON bit so that Gfx can down
- * 5. Unmask Turbo interrupts
-*/
-static void vlv_set_rps_idle(struct drm_i915_private *dev_priv)
+static bool skl_update_pipe_wm(struct drm_crtc *crtc,
+			       struct skl_pipe_wm_parameters *params,
+			       struct intel_wm_config *config,
+			       struct skl_ddb_allocation *ddb, /* out */
+			       struct skl_pipe_wm *pipe_wm /* out */)
 {
-	struct drm_device *dev = dev_priv->dev;
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 
-	/* Latest VLV doesn't need to force the gfx clock */
-	if (dev->pdev->revision >= 0xd) {
-		valleyview_set_rps(dev_priv->dev, dev_priv->rps.min_freq_softlimit);
-		return;
-	}
+	skl_compute_wm_pipe_parameters(crtc, params);
+	skl_allocate_pipe_ddb(crtc, config, params, ddb);
+	skl_compute_pipe_wm(crtc, ddb, params, pipe_wm);
+
+	if (!memcmp(&intel_crtc->wm.skl_active, pipe_wm, sizeof(*pipe_wm)))
+		return false;
+
+	intel_crtc->wm.skl_active = *pipe_wm;
+	return true;
+}
+
+static void skl_update_other_pipe_wm(struct drm_device *dev,
+				     struct drm_crtc *crtc,
+				     struct intel_wm_config *config,
+				     struct skl_wm_values *r)
+{
+	struct intel_crtc *intel_crtc;
+	struct intel_crtc *this_crtc = to_intel_crtc(crtc);
 
 	/*
-	 * When we are idle.  Drop to min voltage state.
+	 * If the WM update hasn't changed the allocation for this_crtc (the
+	 * crtc we are currently computing the new WM values for), other
+	 * enabled crtcs will keep the same allocation and we don't need to
+	 * recompute anything for them.
 	 */
-
-	if (dev_priv->rps.cur_freq <= dev_priv->rps.min_freq_softlimit)
+	if (!skl_ddb_allocation_changed(&r->ddb, this_crtc))
 		return;
 
-	/* Mask turbo interrupt so that they will not come in between */
-	I915_WRITE(GEN6_PMINTRMSK, 0xffffffff);
-
-	vlv_force_gfx_clock(dev_priv, true);
+	/*
+	 * Otherwise, because of this_crtc being freshly enabled/disabled, the
+	 * other active pipes need new DDB allocation and WM values.
+	 */
+	list_for_each_entry(intel_crtc, &dev->mode_config.crtc_list,
+				base.head) {
+		struct skl_pipe_wm_parameters params = {};
+		struct skl_pipe_wm pipe_wm = {};
+		bool wm_changed;
 
-	dev_priv->rps.cur_freq = dev_priv->rps.min_freq_softlimit;
+		if (this_crtc->pipe == intel_crtc->pipe)
+			continue;
 
-	vlv_punit_write(dev_priv, PUNIT_REG_GPU_FREQ_REQ,
-					dev_priv->rps.min_freq_softlimit);
+		if (!intel_crtc->active)
+			continue;
 
-	if (wait_for(((vlv_punit_read(dev_priv, PUNIT_REG_GPU_FREQ_STS))
-				& GENFREQSTATUS) == 0, 5))
-		DRM_ERROR("timed out waiting for Punit\n");
+		wm_changed = skl_update_pipe_wm(&intel_crtc->base,
+						&params, config,
+						&r->ddb, &pipe_wm);
 
-	vlv_force_gfx_clock(dev_priv, false);
+		/*
+		 * If we end up re-computing the other pipe WM values, it's
+		 * because it was really needed, so we expect the WM values to
+		 * be different.
+		 */
+		WARN_ON(!wm_changed);
 
-	I915_WRITE(GEN6_PMINTRMSK,
-		   gen6_rps_pm_mask(dev_priv, dev_priv->rps.cur_freq));
+		skl_compute_wm_results(dev, &params, &pipe_wm, r, intel_crtc);
+		r->dirty[intel_crtc->pipe] = true;
+	}
 }
 
-void gen6_rps_idle(struct drm_i915_private *dev_priv)
+static void skl_update_wm(struct drm_crtc *crtc)
 {
-	struct drm_device *dev = dev_priv->dev;
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	struct drm_device *dev = crtc->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct skl_pipe_wm_parameters params = {};
+	struct skl_wm_values *results = &dev_priv->wm.skl_results;
+	struct skl_pipe_wm pipe_wm = {};
+	struct intel_wm_config config = {};
 
-	mutex_lock(&dev_priv->rps.hw_lock);
-	if (dev_priv->rps.enabled) {
-		if (IS_CHERRYVIEW(dev))
-			valleyview_set_rps(dev_priv->dev, dev_priv->rps.min_freq_softlimit);
-		else if (IS_VALLEYVIEW(dev))
-			vlv_set_rps_idle(dev_priv);
-		else
-			gen6_set_rps(dev_priv->dev, dev_priv->rps.min_freq_softlimit);
-		dev_priv->rps.last_adj = 0;
-	}
-	mutex_unlock(&dev_priv->rps.hw_lock);
-}
+	memset(results, 0, sizeof(*results));
 
-void gen6_rps_boost(struct drm_i915_private *dev_priv)
-{
-	struct drm_device *dev = dev_priv->dev;
+	skl_compute_wm_global_parameters(dev, &config);
 
-	mutex_lock(&dev_priv->rps.hw_lock);
-	if (dev_priv->rps.enabled) {
-		if (IS_VALLEYVIEW(dev))
-			valleyview_set_rps(dev_priv->dev, dev_priv->rps.max_freq_softlimit);
-		else
-			gen6_set_rps(dev_priv->dev, dev_priv->rps.max_freq_softlimit);
-		dev_priv->rps.last_adj = 0;
-	}
-	mutex_unlock(&dev_priv->rps.hw_lock);
-}
+	if (!skl_update_pipe_wm(crtc, &params, &config,
+				&results->ddb, &pipe_wm))
+		return;
 
-void valleyview_set_rps(struct drm_device *dev, u8 val)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	skl_compute_wm_results(dev, &params, &pipe_wm, results, intel_crtc);
+	results->dirty[intel_crtc->pipe] = true;
 
-	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
-	WARN_ON(val > dev_priv->rps.max_freq_softlimit);
-	WARN_ON(val < dev_priv->rps.min_freq_softlimit);
+	skl_update_other_pipe_wm(dev, crtc, &config, results);
+	skl_write_wm_values(dev_priv, results);
+	skl_flush_wm_values(dev_priv, results);
 
-	DRM_DEBUG_DRIVER("GPU freq request from %d MHz (%u) to %d MHz (%u)\n",
-			 vlv_gpu_freq(dev_priv, dev_priv->rps.cur_freq),
-			 dev_priv->rps.cur_freq,
-			 vlv_gpu_freq(dev_priv, val), val);
+	/* store the new configuration */
+	dev_priv->wm.skl_hw = *results;
+}
 
-	if (val != dev_priv->rps.cur_freq)
-		vlv_punit_write(dev_priv, PUNIT_REG_GPU_FREQ_REQ, val);
+static void
+skl_update_sprite_wm(struct drm_plane *plane, struct drm_crtc *crtc,
+		     uint32_t sprite_width, uint32_t sprite_height,
+		     int pixel_size, bool enabled, bool scaled)
+{
+	struct intel_plane *intel_plane = to_intel_plane(plane);
 
-	I915_WRITE(GEN6_PMINTRMSK, gen6_rps_pm_mask(dev_priv, val));
+	intel_plane->wm.enabled = enabled;
+	intel_plane->wm.scaled = scaled;
+	intel_plane->wm.horiz_pixels = sprite_width;
+	intel_plane->wm.vert_pixels = sprite_height;
+	intel_plane->wm.bytes_per_pixel = pixel_size;
 
-	dev_priv->rps.cur_freq = val;
-	trace_intel_gpu_freq_change(vlv_gpu_freq(dev_priv, val));
+	skl_update_wm(crtc);
 }
 
-static void gen8_disable_rps_interrupts(struct drm_device *dev)
+static void ilk_update_wm(struct drm_crtc *crtc)
 {
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	struct drm_device *dev = crtc->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct ilk_wm_maximums max;
+	struct ilk_pipe_wm_parameters params = {};
+	struct ilk_wm_values results = {};
+	enum intel_ddb_partitioning partitioning;
+	struct intel_pipe_wm pipe_wm = {};
+	struct intel_pipe_wm lp_wm_1_2 = {}, lp_wm_5_6 = {}, *best_lp_wm;
+	struct intel_wm_config config = {};
 
-	I915_WRITE(GEN6_PMINTRMSK, ~GEN8_PMINTR_REDIRECT_TO_NON_DISP);
-	I915_WRITE(GEN8_GT_IER(2), I915_READ(GEN8_GT_IER(2)) &
-				   ~dev_priv->pm_rps_events);
-	/* Complete PM interrupt masking here doesn't race with the rps work
-	 * item again unmasking PM interrupts because that is using a different
-	 * register (GEN8_GT_IMR(2)) to mask PM interrupts. The only risk is in
-	 * leaving stale bits in GEN8_GT_IIR(2) and GEN8_GT_IMR(2) which
-	 * gen8_enable_rps will clean up. */
-
-	spin_lock_irq(&dev_priv->irq_lock);
-	dev_priv->rps.pm_iir = 0;
-	spin_unlock_irq(&dev_priv->irq_lock);
+	ilk_compute_wm_parameters(crtc, &params);
 
-	I915_WRITE(GEN8_GT_IIR(2), dev_priv->pm_rps_events);
-}
+	intel_compute_pipe_wm(crtc, &params, &pipe_wm);
 
-static void gen6_disable_rps_interrupts(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	if (!memcmp(&intel_crtc->wm.active, &pipe_wm, sizeof(pipe_wm)))
+		return;
 
-	I915_WRITE(GEN6_PMINTRMSK, 0xffffffff);
-	I915_WRITE(GEN6_PMIER, I915_READ(GEN6_PMIER) &
-				~dev_priv->pm_rps_events);
-	/* Complete PM interrupt masking here doesn't race with the rps work
-	 * item again unmasking PM interrupts because that is using a different
-	 * register (PMIMR) to mask PM interrupts. The only risk is in leaving
-	 * stale bits in PMIIR and PMIMR which gen6_enable_rps will clean up. */
-
-	spin_lock_irq(&dev_priv->irq_lock);
-	dev_priv->rps.pm_iir = 0;
-	spin_unlock_irq(&dev_priv->irq_lock);
+	intel_crtc->wm.active = pipe_wm;
 
-	I915_WRITE(GEN6_PMIIR, dev_priv->pm_rps_events);
-}
+	ilk_compute_wm_config(dev, &config);
 
-static void gen6_disable_rps(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	ilk_compute_wm_maximums(dev, 1, &config, INTEL_DDB_PART_1_2, &max);
+	ilk_wm_merge(dev, &config, &max, &lp_wm_1_2);
 
-	I915_WRITE(GEN6_RC_CONTROL, 0);
-	I915_WRITE(GEN6_RPNSWREQ, 1 << 31);
+	/* 5/6 split only in single pipe config on IVB+ */
+	if (INTEL_INFO(dev)->gen >= 7 &&
+	    config.num_pipes_active == 1 && config.sprites_enabled) {
+		ilk_compute_wm_maximums(dev, 1, &config, INTEL_DDB_PART_5_6, &max);
+		ilk_wm_merge(dev, &config, &max, &lp_wm_5_6);
 
-	if (IS_BROADWELL(dev))
-		gen8_disable_rps_interrupts(dev);
-	else
-		gen6_disable_rps_interrupts(dev);
-}
+		best_lp_wm = ilk_find_best_result(dev, &lp_wm_1_2, &lp_wm_5_6);
+	} else {
+		best_lp_wm = &lp_wm_1_2;
+	}
 
-static void cherryview_disable_rps(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	partitioning = (best_lp_wm == &lp_wm_1_2) ?
+		       INTEL_DDB_PART_1_2 : INTEL_DDB_PART_5_6;
 
-	I915_WRITE(GEN6_RC_CONTROL, 0);
+	ilk_compute_wm_results(dev, best_lp_wm, partitioning, &results);
 
-	gen8_disable_rps_interrupts(dev);
+	ilk_write_wm_values(dev_priv, &results);
 }
 
-static void valleyview_disable_rps(struct drm_device *dev)
+static void
+ilk_update_sprite_wm(struct drm_plane *plane,
+		     struct drm_crtc *crtc,
+		     uint32_t sprite_width, uint32_t sprite_height,
+		     int pixel_size, bool enabled, bool scaled)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_device *dev = plane->dev;
+	struct intel_plane *intel_plane = to_intel_plane(plane);
 
-	I915_WRITE(GEN6_RC_CONTROL, 0);
+	intel_plane->wm.enabled = enabled;
+	intel_plane->wm.scaled = scaled;
+	intel_plane->wm.horiz_pixels = sprite_width;
+	intel_plane->wm.vert_pixels = sprite_width;
+	intel_plane->wm.bytes_per_pixel = pixel_size;
+
+	/*
+	 * IVB workaround: must disable low power watermarks for at least
+	 * one frame before enabling scaling.  LP watermarks can be re-enabled
+	 * when scaling is disabled.
+	 *
+	 * WaCxSRDisabledForSpriteScaling:ivb
+	 */
+	if (IS_IVYBRIDGE(dev) && scaled && ilk_disable_lp_wm(dev))
+		intel_wait_for_vblank(dev, intel_plane->pipe);
 
-	gen6_disable_rps_interrupts(dev);
+	ilk_update_wm(crtc);
 }
 
-static void intel_print_rc6_info(struct drm_device *dev, u32 mode)
-{
-	if (IS_VALLEYVIEW(dev)) {
-		if (mode & (GEN7_RC_CTL_TO_MODE | GEN6_RC_CTL_EI_MODE(1)))
-			mode = GEN6_RC_CTL_RC6_ENABLE;
-		else
-			mode = 0;
+static void skl_pipe_wm_active_state(uint32_t val,
+				     struct skl_pipe_wm *active,
+				     bool is_transwm,
+				     bool is_cursor,
+				     int i,
+				     int level)
+{
+	bool is_enabled = (val & PLANE_WM_EN) != 0;
+
+	if (!is_transwm) {
+		if (!is_cursor) {
+			active->wm[level].plane_en[i] = is_enabled;
+			active->wm[level].plane_res_b[i] =
+					val & PLANE_WM_BLOCKS_MASK;
+			active->wm[level].plane_res_l[i] =
+					(val >> PLANE_WM_LINES_SHIFT) &
+						PLANE_WM_LINES_MASK;
+		} else {
+			active->wm[level].cursor_en = is_enabled;
+			active->wm[level].cursor_res_b =
+					val & PLANE_WM_BLOCKS_MASK;
+			active->wm[level].cursor_res_l =
+					(val >> PLANE_WM_LINES_SHIFT) &
+						PLANE_WM_LINES_MASK;
+		}
+	} else {
+		if (!is_cursor) {
+			active->trans_wm.plane_en[i] = is_enabled;
+			active->trans_wm.plane_res_b[i] =
+					val & PLANE_WM_BLOCKS_MASK;
+			active->trans_wm.plane_res_l[i] =
+					(val >> PLANE_WM_LINES_SHIFT) &
+						PLANE_WM_LINES_MASK;
+		} else {
+			active->trans_wm.cursor_en = is_enabled;
+			active->trans_wm.cursor_res_b =
+					val & PLANE_WM_BLOCKS_MASK;
+			active->trans_wm.cursor_res_l =
+					(val >> PLANE_WM_LINES_SHIFT) &
+						PLANE_WM_LINES_MASK;
+		}
 	}
-	DRM_DEBUG_KMS("Enabling RC6 states: RC6 %s, RC6p %s, RC6pp %s\n",
-		      (mode & GEN6_RC_CTL_RC6_ENABLE) ? "on" : "off",
-		      (mode & GEN6_RC_CTL_RC6p_ENABLE) ? "on" : "off",
-		      (mode & GEN6_RC_CTL_RC6pp_ENABLE) ? "on" : "off");
 }
 
-static int sanitize_rc6_option(const struct drm_device *dev, int enable_rc6)
+static void skl_pipe_wm_get_hw_state(struct drm_crtc *crtc)
 {
-	/* No RC6 before Ironlake */
-	if (INTEL_INFO(dev)->gen < 5)
-		return 0;
+	struct drm_device *dev = crtc->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct skl_wm_values *hw = &dev_priv->wm.skl_hw;
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	struct skl_pipe_wm *active = &intel_crtc->wm.skl_active;
+	enum pipe pipe = intel_crtc->pipe;
+	int level, i, max_level;
+	uint32_t temp;
 
-	/* RC6 is only on Ironlake mobile not on desktop */
-	if (INTEL_INFO(dev)->gen == 5 && !IS_IRONLAKE_M(dev))
-		return 0;
+	max_level = ilk_wm_max_level(dev);
 
-	/* Respect the kernel parameter if it is set */
-	if (enable_rc6 >= 0) {
-		int mask;
+	hw->wm_linetime[pipe] = I915_READ(PIPE_WM_LINETIME(pipe));
 
-		if (INTEL_INFO(dev)->gen == 6 || IS_IVYBRIDGE(dev))
-			mask = INTEL_RC6_ENABLE | INTEL_RC6p_ENABLE |
-			       INTEL_RC6pp_ENABLE;
-		else
-			mask = INTEL_RC6_ENABLE;
+	for (level = 0; level <= max_level; level++) {
+		for (i = 0; i < intel_num_planes(intel_crtc); i++)
+			hw->plane[pipe][i][level] =
+					I915_READ(PLANE_WM(pipe, i, level));
+		hw->cursor[pipe][level] = I915_READ(CUR_WM(pipe, level));
+	}
 
-		if ((enable_rc6 & mask) != enable_rc6)
-			DRM_DEBUG_KMS("Adjusting RC6 mask to %d (requested %d, valid %d)\n",
-				      enable_rc6 & mask, enable_rc6, mask);
+	for (i = 0; i < intel_num_planes(intel_crtc); i++)
+		hw->plane_trans[pipe][i] = I915_READ(PLANE_WM_TRANS(pipe, i));
+	hw->cursor_trans[pipe] = I915_READ(CUR_WM_TRANS(pipe));
 
-		return enable_rc6 & mask;
-	}
+	if (!intel_crtc_active(crtc))
+		return;
 
-	/* Disable RC6 on Ironlake */
-	if (INTEL_INFO(dev)->gen == 5)
-		return 0;
+	hw->dirty[pipe] = true;
 
-	if (IS_IVYBRIDGE(dev))
-		return (INTEL_RC6_ENABLE | INTEL_RC6p_ENABLE);
+	active->linetime = hw->wm_linetime[pipe];
 
-	return INTEL_RC6_ENABLE;
-}
+	for (level = 0; level <= max_level; level++) {
+		for (i = 0; i < intel_num_planes(intel_crtc); i++) {
+			temp = hw->plane[pipe][i][level];
+			skl_pipe_wm_active_state(temp, active, false,
+						false, i, level);
+		}
+		temp = hw->cursor[pipe][level];
+		skl_pipe_wm_active_state(temp, active, false, true, i, level);
+	}
 
-int intel_enable_rc6(const struct drm_device *dev)
-{
-	return i915.enable_rc6;
+	for (i = 0; i < intel_num_planes(intel_crtc); i++) {
+		temp = hw->plane_trans[pipe][i];
+		skl_pipe_wm_active_state(temp, active, true, false, i, 0);
+	}
+
+	temp = hw->cursor_trans[pipe];
+	skl_pipe_wm_active_state(temp, active, true, true, i, 0);
 }
 
-static void gen8_enable_rps_interrupts(struct drm_device *dev)
+void skl_wm_get_hw_state(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct skl_ddb_allocation *ddb = &dev_priv->wm.skl_hw.ddb;
+	struct drm_crtc *crtc;
 
-	spin_lock_irq(&dev_priv->irq_lock);
-	WARN_ON(dev_priv->rps.pm_iir);
-	gen8_enable_pm_irq(dev_priv, dev_priv->pm_rps_events);
-	I915_WRITE(GEN8_GT_IIR(2), dev_priv->pm_rps_events);
-	spin_unlock_irq(&dev_priv->irq_lock);
+	skl_ddb_get_hw_state(dev_priv, ddb);
+	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head)
+		skl_pipe_wm_get_hw_state(crtc);
 }
 
-static void gen6_enable_rps_interrupts(struct drm_device *dev)
+static void ilk_pipe_wm_get_hw_state(struct drm_crtc *crtc)
 {
+	struct drm_device *dev = crtc->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct ilk_wm_values *hw = &dev_priv->wm.hw;
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	struct intel_pipe_wm *active = &intel_crtc->wm.active;
+	enum pipe pipe = intel_crtc->pipe;
+	static const unsigned int wm0_pipe_reg[] = {
+		[PIPE_A] = WM0_PIPEA_ILK,
+		[PIPE_B] = WM0_PIPEB_ILK,
+		[PIPE_C] = WM0_PIPEC_IVB,
+	};
 
-	spin_lock_irq(&dev_priv->irq_lock);
-	WARN_ON(dev_priv->rps.pm_iir);
-	gen6_enable_pm_irq(dev_priv, dev_priv->pm_rps_events);
-	I915_WRITE(GEN6_PMIIR, dev_priv->pm_rps_events);
-	spin_unlock_irq(&dev_priv->irq_lock);
-}
+	hw->wm_pipe[pipe] = I915_READ(wm0_pipe_reg[pipe]);
+	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
+		hw->wm_linetime[pipe] = I915_READ(PIPE_WM_LINETIME(pipe));
 
-static void parse_rp_state_cap(struct drm_i915_private *dev_priv, u32 rp_state_cap)
-{
-	/* All of these values are in units of 50MHz */
-	dev_priv->rps.cur_freq		= 0;
-	/* static values from HW: RP0 < RPe < RP1 < RPn (min_freq) */
-	dev_priv->rps.rp1_freq		= (rp_state_cap >>  8) & 0xff;
-	dev_priv->rps.rp0_freq		= (rp_state_cap >>  0) & 0xff;
-	dev_priv->rps.min_freq		= (rp_state_cap >> 16) & 0xff;
-	/* XXX: only BYT has a special efficient freq */
-	dev_priv->rps.efficient_freq	= dev_priv->rps.rp1_freq;
-	/* hw_max = RP0 until we check for overclocking */
-	dev_priv->rps.max_freq		= dev_priv->rps.rp0_freq;
+	active->pipe_enabled = intel_crtc_active(crtc);
 
-	/* Preserve min/max settings in case of re-init */
-	if (dev_priv->rps.max_freq_softlimit == 0)
-		dev_priv->rps.max_freq_softlimit = dev_priv->rps.max_freq;
+	if (active->pipe_enabled) {
+		u32 tmp = hw->wm_pipe[pipe];
 
-	if (dev_priv->rps.min_freq_softlimit == 0)
-		dev_priv->rps.min_freq_softlimit = dev_priv->rps.min_freq;
+		/*
+		 * For active pipes LP0 watermark is marked as
+		 * enabled, and LP1+ watermaks as disabled since
+		 * we can't really reverse compute them in case
+		 * multiple pipes are active.
+		 */
+		active->wm[0].enable = true;
+		active->wm[0].pri_val = (tmp & WM0_PIPE_PLANE_MASK) >> WM0_PIPE_PLANE_SHIFT;
+		active->wm[0].spr_val = (tmp & WM0_PIPE_SPRITE_MASK) >> WM0_PIPE_SPRITE_SHIFT;
+		active->wm[0].cur_val = tmp & WM0_PIPE_CURSOR_MASK;
+		active->linetime = hw->wm_linetime[pipe];
+	} else {
+		int level, max_level = ilk_wm_max_level(dev);
+
+		/*
+		 * For inactive pipes, all watermark levels
+		 * should be marked as enabled but zeroed,
+		 * which is what we'd compute them to.
+		 */
+		for (level = 0; level <= max_level; level++)
+			active->wm[level].enable = true;
+	}
 }
 
-static void gen8_enable_rps(struct drm_device *dev)
+void ilk_wm_get_hw_state(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
-	uint32_t rc6_mask = 0, rp_state_cap;
-	int unused;
-
-	/* 1a: Software RC state - RC0 */
-	I915_WRITE(GEN6_RC_STATE, 0);
+	struct ilk_wm_values *hw = &dev_priv->wm.hw;
+	struct drm_crtc *crtc;
 
-	/* 1c & 1d: Get forcewake during program sequence. Although the driver
-	 * hasn't enabled a state yet where we need forcewake, BIOS may have.*/
-	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
+	for_each_crtc(dev, crtc)
+		ilk_pipe_wm_get_hw_state(crtc);
 
-	/* 2a: Disable RC states. */
-	I915_WRITE(GEN6_RC_CONTROL, 0);
+	hw->wm_lp[0] = I915_READ(WM1_LP_ILK);
+	hw->wm_lp[1] = I915_READ(WM2_LP_ILK);
+	hw->wm_lp[2] = I915_READ(WM3_LP_ILK);
 
-	rp_state_cap = I915_READ(GEN6_RP_STATE_CAP);
-	parse_rp_state_cap(dev_priv, rp_state_cap);
+	hw->wm_lp_spr[0] = I915_READ(WM1S_LP_ILK);
+	if (INTEL_INFO(dev)->gen >= 7) {
+		hw->wm_lp_spr[1] = I915_READ(WM2S_LP_IVB);
+		hw->wm_lp_spr[2] = I915_READ(WM3S_LP_IVB);
+	}
 
-	/* 2b: Program RC6 thresholds.*/
-	I915_WRITE(GEN6_RC6_WAKE_RATE_LIMIT, 40 << 16);
-	I915_WRITE(GEN6_RC_EVALUATION_INTERVAL, 125000); /* 12500 * 1280ns */
-	I915_WRITE(GEN6_RC_IDLE_HYSTERSIS, 25); /* 25 * 1280ns */
-	for_each_ring(ring, dev_priv, unused)
-		I915_WRITE(RING_MAX_IDLE(ring->mmio_base), 10);
-	I915_WRITE(GEN6_RC_SLEEP, 0);
-	if (IS_BROADWELL(dev))
-		I915_WRITE(GEN6_RC6_THRESHOLD, 625); /* 800us/1.28 for TO */
-	else
-		I915_WRITE(GEN6_RC6_THRESHOLD, 50000); /* 50/125ms per EI */
+	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
+		hw->partitioning = (I915_READ(WM_MISC) & WM_MISC_DATA_PARTITION_5_6) ?
+			INTEL_DDB_PART_5_6 : INTEL_DDB_PART_1_2;
+	else if (IS_IVYBRIDGE(dev))
+		hw->partitioning = (I915_READ(DISP_ARB_CTL2) & DISP_DATA_PARTITION_5_6) ?
+			INTEL_DDB_PART_5_6 : INTEL_DDB_PART_1_2;
 
-	/* 3: Enable RC6 */
-	if (intel_enable_rc6(dev) & INTEL_RC6_ENABLE)
-		rc6_mask = GEN6_RC_CTL_RC6_ENABLE;
-	intel_print_rc6_info(dev, rc6_mask);
-	if (IS_BROADWELL(dev))
-		I915_WRITE(GEN6_RC_CONTROL, GEN6_RC_CTL_HW_ENABLE |
-				GEN7_RC_CTL_TO_MODE |
-				rc6_mask);
-	else
-		I915_WRITE(GEN6_RC_CONTROL, GEN6_RC_CTL_HW_ENABLE |
-				GEN6_RC_CTL_EI_MODE(1) |
-				rc6_mask);
+	hw->enable_fbc_wm =
+		!(I915_READ(DISP_ARB_CTL) & DISP_FBC_WM_DIS);
+}
 
-	/* 4 Program defaults and thresholds for RPS*/
-	I915_WRITE(GEN6_RPNSWREQ,
-		   HSW_FREQUENCY(dev_priv->rps.rp1_freq));
-	I915_WRITE(GEN6_RC_VIDEO_FREQ,
-		   HSW_FREQUENCY(dev_priv->rps.rp1_freq));
-	/* NB: Docs say 1s, and 1000000 - which aren't equivalent */
-	I915_WRITE(GEN6_RP_DOWN_TIMEOUT, 100000000 / 128); /* 1 second timeout */
+/**
+ * intel_update_watermarks - update FIFO watermark values based on current modes
+ *
+ * Calculate watermark values for the various WM regs based on current mode
+ * and plane configuration.
+ *
+ * There are several cases to deal with here:
+ *   - normal (i.e. non-self-refresh)
+ *   - self-refresh (SR) mode
+ *   - lines are large relative to FIFO size (buffer can hold up to 2)
+ *   - lines are small relative to FIFO size (buffer can hold more than 2
+ *     lines), so need to account for TLB latency
+ *
+ *   The normal calculation is:
+ *     watermark = dotclock * bytes per pixel * latency
+ *   where latency is platform & configuration dependent (we assume pessimal
+ *   values here).
+ *
+ *   The SR calculation is:
+ *     watermark = (trunc(latency/line time)+1) * surface width *
+ *       bytes per pixel
+ *   where
+ *     line time = htotal / dotclock
+ *     surface width = hdisplay for normal plane and 64 for cursor
+ *   and latency is assumed to be high, as above.
+ *
+ * The final value programmed to the register should always be rounded up,
+ * and include an extra 2 entries to account for clock crossings.
+ *
+ * We don't use the sprite, so we can ignore that.  And on Crestline we have
+ * to set the non-SR watermarks to 8.
+ */
+void intel_update_watermarks(struct drm_crtc *crtc)
+{
+	struct drm_i915_private *dev_priv = crtc->dev->dev_private;
 
-	/* Docs recommend 900MHz, and 300 MHz respectively */
-	I915_WRITE(GEN6_RP_INTERRUPT_LIMITS,
-		   dev_priv->rps.max_freq_softlimit << 24 |
-		   dev_priv->rps.min_freq_softlimit << 16);
+	if (dev_priv->display.update_wm)
+		dev_priv->display.update_wm(crtc);
+}
 
-	I915_WRITE(GEN6_RP_UP_THRESHOLD, 7600000 / 128); /* 76ms busyness per EI, 90% */
-	I915_WRITE(GEN6_RP_DOWN_THRESHOLD, 31300000 / 128); /* 313ms busyness per EI, 70%*/
-	I915_WRITE(GEN6_RP_UP_EI, 66000); /* 84.48ms, XXX: random? */
-	I915_WRITE(GEN6_RP_DOWN_EI, 350000); /* 448ms, XXX: random? */
+void intel_update_sprite_watermarks(struct drm_plane *plane,
+				    struct drm_crtc *crtc,
+				    uint32_t sprite_width,
+				    uint32_t sprite_height,
+				    int pixel_size,
+				    bool enabled, bool scaled)
+{
+	struct drm_i915_private *dev_priv = plane->dev->dev_private;
 
-	I915_WRITE(GEN6_RP_IDLE_HYSTERSIS, 10);
+	if (dev_priv->display.update_sprite_wm)
+		dev_priv->display.update_sprite_wm(plane, crtc,
+						   sprite_width, sprite_height,
+						   pixel_size, enabled, scaled);
+}
 
-	/* 5: Enable RPS */
-	I915_WRITE(GEN6_RP_CONTROL,
-		   GEN6_RP_MEDIA_TURBO |
-		   GEN6_RP_MEDIA_HW_NORMAL_MODE |
-		   GEN6_RP_MEDIA_IS_GFX |
-		   GEN6_RP_ENABLE |
-		   GEN6_RP_UP_BUSY_AVG |
-		   GEN6_RP_DOWN_IDLE_AVG);
+static struct drm_i915_gem_object *
+intel_alloc_context_page(struct drm_device *dev)
+{
+	struct drm_i915_gem_object *ctx;
+	int ret;
 
-	/* 6: Ring frequency + overclocking (our driver does this later */
+	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
 
-	gen6_set_rps(dev, (I915_READ(GEN6_GT_PERF_STATUS) & 0xff00) >> 8);
+	ctx = i915_gem_alloc_object(dev, 4096);
+	if (!ctx) {
+		DRM_DEBUG("failed to alloc power context, RC6 disabled\n");
+		return NULL;
+	}
 
-	gen8_enable_rps_interrupts(dev);
+	ret = i915_gem_object_ggtt_pin(ctx, 4096, 0);
+	if (ret) {
+		DRM_ERROR("failed to pin power context: %d\n", ret);
+		goto err_unref;
+	}
 
-	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
+	ret = i915_gem_object_set_to_gtt_domain(ctx, 1);
+	if (ret) {
+		DRM_ERROR("failed to set-domain on power context: %d\n", ret);
+		goto err_unpin;
+	}
+
+	return ctx;
+
+err_unpin:
+	i915_gem_object_ggtt_unpin(ctx);
+err_unref:
+	drm_gem_object_unreference(&ctx->base);
+	return NULL;
 }
 
-static void gen6_enable_rps(struct drm_device *dev)
+/**
+ * Lock protecting IPS related data structures
+ */
+DEFINE_SPINLOCK(mchdev_lock);
+
+/* Global for IPS driver to get at the current i915 device. Protected by
+ * mchdev_lock. */
+static struct drm_i915_private *i915_mch_dev;
+
+bool ironlake_set_drps(struct drm_device *dev, u8 val)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
-	u32 rp_state_cap;
-	u32 gt_perf_status;
-	u32 rc6vids, pcu_mbox = 0, rc6_mask = 0;
-	u32 gtfifodbg;
-	int rc6_mode;
-	int i, ret;
-
-	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
+	u16 rgvswctl;
 
-	/* Here begins a magic sequence of register writes to enable
-	 * auto-downclocking.
-	 *
-	 * Perhaps there might be some value in exposing these to
-	 * userspace...
-	 */
-	I915_WRITE(GEN6_RC_STATE, 0);
+	assert_spin_locked(&mchdev_lock);
 
-	/* Clear the DBG now so we don't confuse earlier errors */
-	if ((gtfifodbg = I915_READ(GTFIFODBG))) {
-		DRM_ERROR("GT fifo had a previous error %x\n", gtfifodbg);
-		I915_WRITE(GTFIFODBG, gtfifodbg);
+	rgvswctl = I915_READ16(MEMSWCTL);
+	if (rgvswctl & MEMCTL_CMD_STS) {
+		DRM_DEBUG("gpu busy, RCS change rejected\n");
+		return false; /* still busy with another command */
 	}
 
-	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
+	rgvswctl = (MEMCTL_CMD_CHFREQ << MEMCTL_CMD_SHIFT) |
+		(val << MEMCTL_FREQ_SHIFT) | MEMCTL_SFCAVM;
+	I915_WRITE16(MEMSWCTL, rgvswctl);
+	POSTING_READ16(MEMSWCTL);
 
-	rp_state_cap = I915_READ(GEN6_RP_STATE_CAP);
-	gt_perf_status = I915_READ(GEN6_GT_PERF_STATUS);
+	rgvswctl |= MEMCTL_CMD_STS;
+	I915_WRITE16(MEMSWCTL, rgvswctl);
 
-	parse_rp_state_cap(dev_priv, rp_state_cap);
+	return true;
+}
 
-	/* disable the counters and set deterministic thresholds */
-	I915_WRITE(GEN6_RC_CONTROL, 0);
+static void ironlake_enable_drps(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	u32 rgvmodectl = I915_READ(MEMMODECTL);
+	u8 fmax, fmin, fstart, vstart;
 
-	I915_WRITE(GEN6_RC1_WAKE_RATE_LIMIT, 1000 << 16);
-	I915_WRITE(GEN6_RC6_WAKE_RATE_LIMIT, 40 << 16 | 30);
-	I915_WRITE(GEN6_RC6pp_WAKE_RATE_LIMIT, 30);
-	I915_WRITE(GEN6_RC_EVALUATION_INTERVAL, 125000);
-	I915_WRITE(GEN6_RC_IDLE_HYSTERSIS, 25);
+	spin_lock_irq(&mchdev_lock);
 
-	for_each_ring(ring, dev_priv, i)
-		I915_WRITE(RING_MAX_IDLE(ring->mmio_base), 10);
+	/* Enable temp reporting */
+	I915_WRITE16(PMMISC, I915_READ(PMMISC) | MCPPCE_EN);
+	I915_WRITE16(TSC1, I915_READ(TSC1) | TSE);
 
-	I915_WRITE(GEN6_RC_SLEEP, 0);
-	I915_WRITE(GEN6_RC1e_THRESHOLD, 1000);
-	if (IS_IVYBRIDGE(dev))
-		I915_WRITE(GEN6_RC6_THRESHOLD, 125000);
-	else
-		I915_WRITE(GEN6_RC6_THRESHOLD, 50000);
-	I915_WRITE(GEN6_RC6p_THRESHOLD, 150000);
-	I915_WRITE(GEN6_RC6pp_THRESHOLD, 64000); /* unused */
+	/* 100ms RC evaluation intervals */
+	I915_WRITE(RCUPEI, 100000);
+	I915_WRITE(RCDNEI, 100000);
 
-	/* Check if we are enabling RC6 */
-	rc6_mode = intel_enable_rc6(dev_priv->dev);
-	if (rc6_mode & INTEL_RC6_ENABLE)
-		rc6_mask |= GEN6_RC_CTL_RC6_ENABLE;
+	/* Set max/min thresholds to 90ms and 80ms respectively */
+	I915_WRITE(RCBMAXAVG, 90000);
+	I915_WRITE(RCBMINAVG, 80000);
 
-	/* We don't use those on Haswell */
-	if (!IS_HASWELL(dev)) {
-		if (rc6_mode & INTEL_RC6p_ENABLE)
-			rc6_mask |= GEN6_RC_CTL_RC6p_ENABLE;
+	I915_WRITE(MEMIHYST, 1);
 
-		if (rc6_mode & INTEL_RC6pp_ENABLE)
-			rc6_mask |= GEN6_RC_CTL_RC6pp_ENABLE;
-	}
+	/* Set up min, max, and cur for interrupt handling */
+	fmax = (rgvmodectl & MEMMODE_FMAX_MASK) >> MEMMODE_FMAX_SHIFT;
+	fmin = (rgvmodectl & MEMMODE_FMIN_MASK);
+	fstart = (rgvmodectl & MEMMODE_FSTART_MASK) >>
+		MEMMODE_FSTART_SHIFT;
 
-	intel_print_rc6_info(dev, rc6_mask);
+	vstart = (I915_READ(PXVFREQ_BASE + (fstart * 4)) & PXVFREQ_PX_MASK) >>
+		PXVFREQ_PX_SHIFT;
 
-	I915_WRITE(GEN6_RC_CONTROL,
-		   rc6_mask |
-		   GEN6_RC_CTL_EI_MODE(1) |
-		   GEN6_RC_CTL_HW_ENABLE);
+	dev_priv->ips.fmax = fmax; /* IPS callback will increase this */
+	dev_priv->ips.fstart = fstart;
 
-	/* Power down if completely idle for over 50ms */
-	I915_WRITE(GEN6_RP_DOWN_TIMEOUT, 50000);
-	I915_WRITE(GEN6_RP_IDLE_HYSTERSIS, 10);
+	dev_priv->ips.max_delay = fstart;
+	dev_priv->ips.min_delay = fmin;
+	dev_priv->ips.cur_delay = fstart;
 
-	ret = sandybridge_pcode_write(dev_priv, GEN6_PCODE_WRITE_MIN_FREQ_TABLE, 0);
-	if (ret)
-		DRM_DEBUG_DRIVER("Failed to set the min frequency\n");
+	DRM_DEBUG_DRIVER("fmax: %d, fmin: %d, fstart: %d\n",
+			 fmax, fmin, fstart);
 
-	ret = sandybridge_pcode_read(dev_priv, GEN6_READ_OC_PARAMS, &pcu_mbox);
-	if (!ret && (pcu_mbox & (1<<31))) { /* OC supported */
-		DRM_DEBUG_DRIVER("Overclocking supported. Max: %dMHz, Overclock max: %dMHz\n",
-				 (dev_priv->rps.max_freq_softlimit & 0xff) * 50,
-				 (pcu_mbox & 0xff) * 50);
-		dev_priv->rps.max_freq = pcu_mbox & 0xff;
-	}
+	I915_WRITE(MEMINTREN, MEMINT_CX_SUPR_EN | MEMINT_EVAL_CHG_EN);
 
-	dev_priv->rps.power = HIGH_POWER; /* force a reset */
-	gen6_set_rps(dev_priv->dev, dev_priv->rps.min_freq_softlimit);
+	/*
+	 * Interrupts will be enabled in ironlake_irq_postinstall
+	 */
 
-	gen6_enable_rps_interrupts(dev);
+	I915_WRITE(VIDSTART, vstart);
+	POSTING_READ(VIDSTART);
 
-	rc6vids = 0;
-	ret = sandybridge_pcode_read(dev_priv, GEN6_PCODE_READ_RC6VIDS, &rc6vids);
-	if (IS_GEN6(dev) && ret) {
-		DRM_DEBUG_DRIVER("Couldn't check for BIOS workaround\n");
-	} else if (IS_GEN6(dev) && (GEN6_DECODE_RC6_VID(rc6vids & 0xff) < 450)) {
-		DRM_DEBUG_DRIVER("You should update your BIOS. Correcting minimum rc6 voltage (%dmV->%dmV)\n",
-			  GEN6_DECODE_RC6_VID(rc6vids & 0xff), 450);
-		rc6vids &= 0xffff00;
-		rc6vids |= GEN6_ENCODE_RC6_VID(450);
-		ret = sandybridge_pcode_write(dev_priv, GEN6_PCODE_WRITE_RC6VIDS, rc6vids);
-		if (ret)
-			DRM_ERROR("Couldn't fix incorrect rc6 voltage\n");
-	}
+	rgvmodectl |= MEMMODE_SWMODE_EN;
+	I915_WRITE(MEMMODECTL, rgvmodectl);
 
-	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
+	if (wait_for_atomic((I915_READ(MEMSWCTL) & MEMCTL_CMD_STS) == 0, 10))
+		DRM_ERROR("stuck trying to change perf mode\n");
+	mdelay(1);
+
+	ironlake_set_drps(dev, fstart);
+
+	dev_priv->ips.last_count1 = I915_READ(0x112e4) + I915_READ(0x112e8) +
+		I915_READ(0x112e0);
+	dev_priv->ips.last_time1 = jiffies_to_msecs(jiffies);
+	dev_priv->ips.last_count2 = I915_READ(0x112f4);
+	dev_priv->ips.last_time2 = ktime_get_raw_ns();
+
+	spin_unlock_irq(&mchdev_lock);
 }
 
-static void __gen6_update_ring_freq(struct drm_device *dev)
+static void ironlake_disable_drps(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	int min_freq = 15;
-	unsigned int gpu_freq;
-	unsigned int max_ia_freq, min_ring_freq;
-	int scaling_factor = 180;
-	struct cpufreq_policy *policy;
+	u16 rgvswctl;
 
-	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
+	spin_lock_irq(&mchdev_lock);
 
-	policy = cpufreq_cpu_get(0);
-	if (policy) {
-		max_ia_freq = policy->cpuinfo.max_freq;
-		cpufreq_cpu_put(policy);
-	} else {
-		/*
-		 * Default to measured freq if none found, PCU will ensure we
-		 * don't go over
-		 */
-		max_ia_freq = tsc_khz;
-	}
+	rgvswctl = I915_READ16(MEMSWCTL);
 
-	/* Convert from kHz to MHz */
-	max_ia_freq /= 1000;
+	/* Ack interrupts, disable EFC interrupt */
+	I915_WRITE(MEMINTREN, I915_READ(MEMINTREN) & ~MEMINT_EVAL_CHG_EN);
+	I915_WRITE(MEMINTRSTS, MEMINT_EVAL_CHG);
+	I915_WRITE(DEIER, I915_READ(DEIER) & ~DE_PCU_EVENT);
+	I915_WRITE(DEIIR, DE_PCU_EVENT);
+	I915_WRITE(DEIMR, I915_READ(DEIMR) | DE_PCU_EVENT);
 
-	min_ring_freq = I915_READ(DCLK) & 0xf;
-	/* convert DDR frequency from units of 266.6MHz to bandwidth */
-	min_ring_freq = mult_frac(min_ring_freq, 8, 3);
+	/* Go back to the starting frequency */
+	ironlake_set_drps(dev, dev_priv->ips.fstart);
+	mdelay(1);
+	rgvswctl |= MEMCTL_CMD_STS;
+	I915_WRITE(MEMSWCTL, rgvswctl);
+	mdelay(1);
 
-	/*
-	 * For each potential GPU frequency, load a ring frequency we'd like
-	 * to use for memory access.  We do this by specifying the IA frequency
-	 * the PCU should use as a reference to determine the ring frequency.
-	 */
-	for (gpu_freq = dev_priv->rps.max_freq_softlimit; gpu_freq >= dev_priv->rps.min_freq_softlimit;
-	     gpu_freq--) {
-		int diff = dev_priv->rps.max_freq_softlimit - gpu_freq;
-		unsigned int ia_freq = 0, ring_freq = 0;
+	spin_unlock_irq(&mchdev_lock);
+}
 
-		if (INTEL_INFO(dev)->gen >= 8) {
-			/* max(2 * GT, DDR). NB: GT is 50MHz units */
-			ring_freq = max(min_ring_freq, gpu_freq);
-		} else if (IS_HASWELL(dev)) {
-			ring_freq = mult_frac(gpu_freq, 5, 4);
-			ring_freq = max(min_ring_freq, ring_freq);
-			/* leave ia_freq as the default, chosen by cpufreq */
-		} else {
-			/* On older processors, there is no separate ring
-			 * clock domain, so in order to boost the bandwidth
-			 * of the ring, we need to upclock the CPU (ia_freq).
-			 *
-			 * For GPU frequencies less than 750MHz,
-			 * just use the lowest ring freq.
-			 */
-			if (gpu_freq < min_freq)
-				ia_freq = 800;
-			else
-				ia_freq = max_ia_freq - ((diff * scaling_factor) / 2);
-			ia_freq = DIV_ROUND_CLOSEST(ia_freq, 100);
-		}
+/* There's a funny hw issue where the hw returns all 0 when reading from
+ * GEN6_RP_INTERRUPT_LIMITS. Hence we always need to compute the desired value
+ * ourselves, instead of doing a rmw cycle (which might result in us clearing
+ * all limits and the gpu stuck at whatever frequency it is at atm).
+ */
+static u32 gen6_rps_limits(struct drm_i915_private *dev_priv, u8 val)
+{
+	u32 limits;
 
-		sandybridge_pcode_write(dev_priv,
-					GEN6_PCODE_WRITE_MIN_FREQ_TABLE,
-					ia_freq << GEN6_PCODE_FREQ_IA_RATIO_SHIFT |
-					ring_freq << GEN6_PCODE_FREQ_RING_RATIO_SHIFT |
-					gpu_freq);
-	}
+	/* Only set the down limit when we've reached the lowest level to avoid
+	 * getting more interrupts, otherwise leave this clear. This prevents a
+	 * race in the hw when coming out of rc6: There's a tiny window where
+	 * the hw runs at the minimal clock before selecting the desired
+	 * frequency, if the down threshold expires in that window we will not
+	 * receive a down interrupt. */
+	limits = dev_priv->rps.max_freq_softlimit << 24;
+	if (val <= dev_priv->rps.min_freq_softlimit)
+		limits |= dev_priv->rps.min_freq_softlimit << 16;
+
+	return limits;
 }
 
-void gen6_update_ring_freq(struct drm_device *dev)
+static void gen6_set_rps_thresholds(struct drm_i915_private *dev_priv, u8 val)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	int new_power;
 
-	if (INTEL_INFO(dev)->gen < 6 || IS_VALLEYVIEW(dev))
+	new_power = dev_priv->rps.power;
+	switch (dev_priv->rps.power) {
+	case LOW_POWER:
+		if (val > dev_priv->rps.efficient_freq + 1 && val > dev_priv->rps.cur_freq)
+			new_power = BETWEEN;
+		break;
+
+	case BETWEEN:
+		if (val <= dev_priv->rps.efficient_freq && val < dev_priv->rps.cur_freq)
+			new_power = LOW_POWER;
+		else if (val >= dev_priv->rps.rp0_freq && val > dev_priv->rps.cur_freq)
+			new_power = HIGH_POWER;
+		break;
+
+	case HIGH_POWER:
+		if (val < (dev_priv->rps.rp1_freq + dev_priv->rps.rp0_freq) >> 1 && val < dev_priv->rps.cur_freq)
+			new_power = BETWEEN;
+		break;
+	}
+	/* Max/min bins are special */
+	if (val <= dev_priv->rps.min_freq_softlimit)
+		new_power = LOW_POWER;
+	if (val >= dev_priv->rps.max_freq_softlimit)
+		new_power = HIGH_POWER;
+	if (new_power == dev_priv->rps.power)
 		return;
 
-	mutex_lock(&dev_priv->rps.hw_lock);
-	__gen6_update_ring_freq(dev);
-	mutex_unlock(&dev_priv->rps.hw_lock);
-}
+	/* Note the units here are not exactly 1us, but 1280ns. */
+	switch (new_power) {
+	case LOW_POWER:
+		/* Upclock if more than 95% busy over 16ms */
+		dev_priv->rps.up_threshold = 95;
+		I915_WRITE(GEN6_RP_UP_EI, 12500);
+		I915_WRITE(GEN6_RP_UP_THRESHOLD, 11800);
 
-static int cherryview_rps_max_freq(struct drm_i915_private *dev_priv)
-{
-	u32 val, rp0;
+		/* Downclock if less than 85% busy over 32ms */
+		dev_priv->rps.down_threshold = 85;
+		I915_WRITE(GEN6_RP_DOWN_EI, 25000);
+		I915_WRITE(GEN6_RP_DOWN_THRESHOLD, 21250);
 
-	val = vlv_punit_read(dev_priv, PUNIT_GPU_STATUS_REG);
-	rp0 = (val >> PUNIT_GPU_STATUS_MAX_FREQ_SHIFT) & PUNIT_GPU_STATUS_MAX_FREQ_MASK;
+		I915_WRITE(GEN6_RP_CONTROL,
+			   GEN6_RP_MEDIA_TURBO |
+			   GEN6_RP_MEDIA_HW_NORMAL_MODE |
+			   GEN6_RP_MEDIA_IS_GFX |
+			   GEN6_RP_ENABLE |
+			   GEN6_RP_UP_BUSY_AVG |
+			   GEN6_RP_DOWN_IDLE_AVG);
+		break;
 
-	return rp0;
-}
+	case BETWEEN:
+		/* Upclock if more than 90% busy over 13ms */
+		dev_priv->rps.up_threshold = 90;
+		I915_WRITE(GEN6_RP_UP_EI, 10250);
+		I915_WRITE(GEN6_RP_UP_THRESHOLD, 9225);
 
-static int cherryview_rps_rpe_freq(struct drm_i915_private *dev_priv)
-{
-	u32 val, rpe;
+		/* Downclock if less than 75% busy over 32ms */
+		dev_priv->rps.down_threshold = 75;
+		I915_WRITE(GEN6_RP_DOWN_EI, 25000);
+		I915_WRITE(GEN6_RP_DOWN_THRESHOLD, 18750);
 
-	val = vlv_punit_read(dev_priv, PUNIT_GPU_DUTYCYCLE_REG);
-	rpe = (val >> PUNIT_GPU_DUTYCYCLE_RPE_FREQ_SHIFT) & PUNIT_GPU_DUTYCYCLE_RPE_FREQ_MASK;
+		I915_WRITE(GEN6_RP_CONTROL,
+			   GEN6_RP_MEDIA_TURBO |
+			   GEN6_RP_MEDIA_HW_NORMAL_MODE |
+			   GEN6_RP_MEDIA_IS_GFX |
+			   GEN6_RP_ENABLE |
+			   GEN6_RP_UP_BUSY_AVG |
+			   GEN6_RP_DOWN_IDLE_AVG);
+		break;
 
-	return rpe;
-}
+	case HIGH_POWER:
+		/* Upclock if more than 85% busy over 10ms */
+		dev_priv->rps.up_threshold = 85;
+		I915_WRITE(GEN6_RP_UP_EI, 8000);
+		I915_WRITE(GEN6_RP_UP_THRESHOLD, 6800);
 
-static int cherryview_rps_guar_freq(struct drm_i915_private *dev_priv)
-{
-	u32 val, rp1;
+		/* Downclock if less than 60% busy over 32ms */
+		dev_priv->rps.down_threshold = 60;
+		I915_WRITE(GEN6_RP_DOWN_EI, 25000);
+		I915_WRITE(GEN6_RP_DOWN_THRESHOLD, 15000);
 
-	val = vlv_punit_read(dev_priv, PUNIT_REG_GPU_FREQ_STS);
-	rp1 = (val >> PUNIT_GPU_STATUS_MAX_FREQ_SHIFT) & PUNIT_GPU_STATUS_MAX_FREQ_MASK;
+		I915_WRITE(GEN6_RP_CONTROL,
+			   GEN6_RP_MEDIA_TURBO |
+			   GEN6_RP_MEDIA_HW_NORMAL_MODE |
+			   GEN6_RP_MEDIA_IS_GFX |
+			   GEN6_RP_ENABLE |
+			   GEN6_RP_UP_BUSY_AVG |
+			   GEN6_RP_DOWN_IDLE_AVG);
+		break;
+	}
 
-	return rp1;
+	dev_priv->rps.power = new_power;
+	dev_priv->rps.last_adj = 0;
 }
 
-static int cherryview_rps_min_freq(struct drm_i915_private *dev_priv)
+static u32 gen6_rps_pm_mask(struct drm_i915_private *dev_priv, u8 val)
 {
-	u32 val, rpn;
+	u32 mask = 0;
 
-	val = vlv_punit_read(dev_priv, PUNIT_GPU_STATUS_REG);
-	rpn = (val >> PUNIT_GPU_STATIS_GFX_MIN_FREQ_SHIFT) & PUNIT_GPU_STATUS_GFX_MIN_FREQ_MASK;
-	return rpn;
-}
+	if (val > dev_priv->rps.min_freq_softlimit)
+		mask |= GEN6_PM_RP_DOWN_EI_EXPIRED | GEN6_PM_RP_DOWN_THRESHOLD | GEN6_PM_RP_DOWN_TIMEOUT;
+	if (val < dev_priv->rps.max_freq_softlimit)
+		mask |= GEN6_PM_RP_UP_EI_EXPIRED | GEN6_PM_RP_UP_THRESHOLD;
 
-static int valleyview_rps_guar_freq(struct drm_i915_private *dev_priv)
-{
-	u32 val, rp1;
+	mask &= dev_priv->rps.pm_events;
 
-	val = vlv_nc_read(dev_priv, IOSF_NC_FB_GFX_FREQ_FUSE);
+	/* IVB and SNB hard hangs on looping batchbuffer
+	 * if GEN6_PM_UP_EI_EXPIRED is masked.
+	 */
+	if (INTEL_INFO(dev_priv->dev)->gen <= 7 && !IS_HASWELL(dev_priv->dev))
+		mask |= GEN6_PM_RP_UP_EI_EXPIRED;
 
-	rp1 = (val & FB_GFX_FGUARANTEED_FREQ_FUSE_MASK) >> FB_GFX_FGUARANTEED_FREQ_FUSE_SHIFT;
+	if (IS_GEN8(dev_priv->dev))
+		mask |= GEN8_PMINTR_REDIRECT_TO_NON_DISP;
 
-	return rp1;
+	return ~mask;
 }
 
-static int valleyview_rps_max_freq(struct drm_i915_private *dev_priv)
+/* gen6_set_rps is called to update the frequency request, but should also be
+ * called when the range (min_delay and max_delay) is modified so that we can
+ * update the GEN6_RP_INTERRUPT_LIMITS register accordingly. */
+void gen6_set_rps(struct drm_device *dev, u8 val)
 {
-	u32 val, rp0;
+	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	val = vlv_nc_read(dev_priv, IOSF_NC_FB_GFX_FREQ_FUSE);
+	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
+	WARN_ON(val > dev_priv->rps.max_freq);
+	WARN_ON(val < dev_priv->rps.min_freq);
 
-	rp0 = (val & FB_GFX_MAX_FREQ_FUSE_MASK) >> FB_GFX_MAX_FREQ_FUSE_SHIFT;
-	/* Clamp to max */
-	rp0 = min_t(u32, rp0, 0xea);
+	/* min/max delay may still have been modified so be sure to
+	 * write the limits value.
+	 */
+	if (val != dev_priv->rps.cur_freq) {
+		gen6_set_rps_thresholds(dev_priv, val);
 
-	return rp0;
+		if (IS_HASWELL(dev) || IS_BROADWELL(dev))
+			I915_WRITE(GEN6_RPNSWREQ,
+				   HSW_FREQUENCY(val));
+		else
+			I915_WRITE(GEN6_RPNSWREQ,
+				   GEN6_FREQUENCY(val) |
+				   GEN6_OFFSET(0) |
+				   GEN6_AGGRESSIVE_TURBO);
+	}
+
+	/* Make sure we continue to get interrupts
+	 * until we hit the minimum or maximum frequencies.
+	 */
+	I915_WRITE(GEN6_RP_INTERRUPT_LIMITS, gen6_rps_limits(dev_priv, val));
+	I915_WRITE(GEN6_PMINTRMSK, gen6_rps_pm_mask(dev_priv, val));
+
+	dev_priv->rps.cur_freq = val;
+	trace_intel_gpu_freq_change(val * 50);
 }
 
-static int valleyview_rps_rpe_freq(struct drm_i915_private *dev_priv)
+/* vlv_set_rps_idle: Set the frequency to Rpn if Gfx clocks are down
+ *
+ * * If Gfx is Idle, then
+ * 1. Mask Turbo interrupts
+ * 2. Bring up Gfx clock
+ * 3. Change the freq to Rpn and wait till P-Unit updates freq
+ * 4. Clear the Force GFX CLK ON bit so that Gfx can down
+ * 5. Unmask Turbo interrupts
+*/
+static void vlv_set_rps_idle(struct drm_i915_private *dev_priv)
 {
-	u32 val, rpe;
+	struct drm_device *dev = dev_priv->dev;
+	u32 val = dev_priv->rps.min_freq;
 
-	val = vlv_nc_read(dev_priv, IOSF_NC_FB_GFX_FMAX_FUSE_LO);
-	rpe = (val & FB_FMAX_VMIN_FREQ_LO_MASK) >> FB_FMAX_VMIN_FREQ_LO_SHIFT;
-	val = vlv_nc_read(dev_priv, IOSF_NC_FB_GFX_FMAX_FUSE_HI);
-	rpe |= (val & FB_FMAX_VMIN_FREQ_HI_MASK) << 5;
+	/* Latest VLV doesn't need to force the gfx clock */
+	if (dev->pdev->revision >= 0xd) {
+		valleyview_set_rps(dev_priv->dev, val);
+		return;
+	}
 
-	return rpe;
+	/*
+	 * When we are idle.  Drop to min voltage state.
+	 */
+
+	if (dev_priv->rps.cur_freq <= val)
+		return;
+
+	/* Mask turbo interrupt so that they will not come in between */
+	I915_WRITE(GEN6_PMINTRMSK, 0xffffffff);
+
+	vlv_force_gfx_clock(dev_priv, true);
+
+	dev_priv->rps.cur_freq = val;
+
+	vlv_punit_write(dev_priv, PUNIT_REG_GPU_FREQ_REQ, val);
+
+	if (wait_for(((vlv_punit_read(dev_priv, PUNIT_REG_GPU_FREQ_STS))
+				& GENFREQSTATUS) == 0, 5))
+		DRM_ERROR("timed out waiting for Punit\n");
+
+	gen6_set_rps_thresholds(dev_priv, val);
+	vlv_force_gfx_clock(dev_priv, false);
+
+	I915_WRITE(GEN6_PMINTRMSK, gen6_rps_pm_mask(dev_priv, val));
 }
 
-static int valleyview_rps_min_freq(struct drm_i915_private *dev_priv)
+void gen6_rps_busy(struct drm_i915_private *dev_priv)
 {
-	return vlv_punit_read(dev_priv, PUNIT_REG_GPU_LFM) & 0xff;
+	mutex_lock(&dev_priv->rps.hw_lock);
+	if (dev_priv->rps.enabled) {
+		if (dev_priv->rps.pm_events & (GEN6_PM_RP_DOWN_EI_EXPIRED | GEN6_PM_RP_UP_EI_EXPIRED))
+			gen6_rps_reset_ei(dev_priv);
+		I915_WRITE(GEN6_PMINTRMSK,
+			   gen6_rps_pm_mask(dev_priv, dev_priv->rps.cur_freq));
+	}
+	mutex_unlock(&dev_priv->rps.hw_lock);
 }
 
-/* Check that the pctx buffer wasn't move under us. */
-static void valleyview_check_pctx(struct drm_i915_private *dev_priv)
+void gen6_rps_idle(struct drm_i915_private *dev_priv)
 {
-	unsigned long pctx_addr = I915_READ(VLV_PCBR) & ~4095;
-
-	WARN_ON(pctx_addr != dev_priv->mm.stolen_base +
-			     dev_priv->vlv_pctx->stolen->start);
-}
+	struct drm_device *dev = dev_priv->dev;
 
+	mutex_lock(&dev_priv->rps.hw_lock);
+	if (dev_priv->rps.enabled) {
+		u32 val = dev_priv->rps.min_freq;
 
-/* Check that the pcbr address is not empty. */
-static void cherryview_check_pctx(struct drm_i915_private *dev_priv)
-{
-	unsigned long pctx_addr = I915_READ(VLV_PCBR) & ~4095;
+		if (IS_CHERRYVIEW(dev))
+			valleyview_set_rps(dev_priv->dev, val);
+		else if (IS_VALLEYVIEW(dev))
+			vlv_set_rps_idle(dev_priv);
+		else
+			gen6_set_rps(dev_priv->dev, val);
+		dev_priv->rps.last_adj = 0;
+		I915_WRITE(GEN6_PMINTRMSK, 0xffffffff);
+	}
 
-	WARN_ON((pctx_addr >> VLV_PCBR_ADDR_SHIFT) == 0);
+	while (!list_empty(&dev_priv->rps.clients))
+		list_del_init(dev_priv->rps.clients.next);
+	mutex_unlock(&dev_priv->rps.hw_lock);
 }
 
-static void cherryview_setup_pctx(struct drm_device *dev)
+void gen6_rps_boost(struct drm_i915_private *dev_priv,
+		    struct drm_i915_file_private *file_priv)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	unsigned long pctx_paddr, paddr;
-	struct i915_gtt *gtt = &dev_priv->gtt;
-	u32 pcbr;
-	int pctx_size = 32*1024;
-
-	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
+	struct drm_device *dev = dev_priv->dev;
+	u32 val;
 
-	pcbr = I915_READ(VLV_PCBR);
-	if ((pcbr >> VLV_PCBR_ADDR_SHIFT) == 0) {
-		paddr = (dev_priv->mm.stolen_base +
-			 (gtt->stolen_size - pctx_size));
+	mutex_lock(&dev_priv->rps.hw_lock);
+	val = dev_priv->rps.max_freq_softlimit;
+	if (dev_priv->rps.enabled &&
+	    dev_priv->mm.busy &&
+	    dev_priv->rps.cur_freq < val &&
+	    (file_priv == NULL || list_empty(&file_priv->rps_boost))) {
+		if (IS_VALLEYVIEW(dev))
+			valleyview_set_rps(dev_priv->dev, val);
+		else
+			gen6_set_rps(dev_priv->dev, val);
+		dev_priv->rps.last_adj = 0;
 
-		pctx_paddr = (paddr & (~4095));
-		I915_WRITE(VLV_PCBR, pctx_paddr);
+		if (file_priv != NULL) {
+			file_priv->rps_boosts++;
+			list_add(&file_priv->rps_boost, &dev_priv->rps.clients);
+		}
 	}
+	mutex_unlock(&dev_priv->rps.hw_lock);
 }
 
-static void valleyview_setup_pctx(struct drm_device *dev)
+void valleyview_set_rps(struct drm_device *dev, u8 val)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_i915_gem_object *pctx;
-	unsigned long pctx_paddr;
-	u32 pcbr;
-	int pctx_size = 24*1024;
 
-	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
-
-	pcbr = I915_READ(VLV_PCBR);
-	if (pcbr) {
-		/* BIOS set it up already, grab the pre-alloc'd space */
-		int pcbr_offset;
+	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
+	WARN_ON(val > dev_priv->rps.max_freq);
+	WARN_ON(val < dev_priv->rps.min_freq);
 
-		pcbr_offset = (pcbr & (~4095)) - dev_priv->mm.stolen_base;
-		pctx = i915_gem_object_create_stolen_for_preallocated(dev_priv->dev,
-								      pcbr_offset,
-								      I915_GTT_OFFSET_NONE,
-								      pctx_size);
-		goto out;
-	}
+	if (WARN_ONCE(IS_CHERRYVIEW(dev) && (val & 1),
+		      "Odd GPU freq value\n"))
+		val &= ~1;
 
-	/*
-	 * From the Gunit register HAS:
-	 * The Gfx driver is expected to program this register and ensure
-	 * proper allocation within Gfx stolen memory.  For example, this
-	 * register should be programmed such than the PCBR range does not
-	 * overlap with other ranges, such as the frame buffer, protected
-	 * memory, or any other relevant ranges.
-	 */
-	pctx = i915_gem_object_create_stolen(dev, pctx_size);
-	if (!pctx) {
-		DRM_DEBUG("not enough stolen space for PCTX, disabling\n");
-		return;
+	if (val != dev_priv->rps.cur_freq) {
+		vlv_punit_write(dev_priv, PUNIT_REG_GPU_FREQ_REQ, val);
+		gen6_set_rps_thresholds(dev_priv, val);
 	}
 
-	pctx_paddr = dev_priv->mm.stolen_base + pctx->stolen->start;
-	I915_WRITE(VLV_PCBR, pctx_paddr);
+	I915_WRITE(GEN6_PMINTRMSK, gen6_rps_pm_mask(dev_priv, val));
 
-out:
-	dev_priv->vlv_pctx = pctx;
+	dev_priv->rps.cur_freq = val;
+	trace_intel_gpu_freq_change(vlv_gpu_freq(dev_priv, val));
 }
 
-static void valleyview_cleanup_pctx(struct drm_device *dev)
+static void gen9_disable_rps(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	if (WARN_ON(!dev_priv->vlv_pctx))
-		return;
-
-	drm_gem_object_unreference(&dev_priv->vlv_pctx->base);
-	dev_priv->vlv_pctx = NULL;
+	I915_WRITE(GEN6_RC_CONTROL, 0);
 }
 
-static void valleyview_init_gt_powersave(struct drm_device *dev)
+static void gen6_disable_rps(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	valleyview_setup_pctx(dev);
+	I915_WRITE(GEN6_RC_CONTROL, 0);
+	I915_WRITE(GEN6_RPNSWREQ, 1 << 31);
+}
 
-	mutex_lock(&dev_priv->rps.hw_lock);
+static void cherryview_disable_rps(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	dev_priv->rps.max_freq = valleyview_rps_max_freq(dev_priv);
-	dev_priv->rps.rp0_freq = dev_priv->rps.max_freq;
-	DRM_DEBUG_DRIVER("max GPU freq: %d MHz (%u)\n",
-			 vlv_gpu_freq(dev_priv, dev_priv->rps.max_freq),
-			 dev_priv->rps.max_freq);
+	I915_WRITE(GEN6_RC_CONTROL, 0);
+}
 
-	dev_priv->rps.efficient_freq = valleyview_rps_rpe_freq(dev_priv);
-	DRM_DEBUG_DRIVER("RPe GPU freq: %d MHz (%u)\n",
-			 vlv_gpu_freq(dev_priv, dev_priv->rps.efficient_freq),
-			 dev_priv->rps.efficient_freq);
+static void valleyview_disable_rps(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	dev_priv->rps.rp1_freq = valleyview_rps_guar_freq(dev_priv);
-	DRM_DEBUG_DRIVER("RP1(Guar Freq) GPU freq: %d MHz (%u)\n",
-			 vlv_gpu_freq(dev_priv, dev_priv->rps.rp1_freq),
-			 dev_priv->rps.rp1_freq);
+	/* we're doing forcewake before Disabling RC6,
+	 * This what the BIOS expects when going into suspend */
+	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
 
-	dev_priv->rps.min_freq = valleyview_rps_min_freq(dev_priv);
-	DRM_DEBUG_DRIVER("min GPU freq: %d MHz (%u)\n",
-			 vlv_gpu_freq(dev_priv, dev_priv->rps.min_freq),
-			 dev_priv->rps.min_freq);
+	I915_WRITE(GEN6_RC_CONTROL, 0);
 
-	/* Preserve min/max settings in case of re-init */
-	if (dev_priv->rps.max_freq_softlimit == 0)
-		dev_priv->rps.max_freq_softlimit = dev_priv->rps.max_freq;
+	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
+}
 
-	if (dev_priv->rps.min_freq_softlimit == 0)
-		dev_priv->rps.min_freq_softlimit = dev_priv->rps.min_freq;
+static void intel_print_rc6_info(struct drm_device *dev, u32 mode)
+{
+	if (IS_VALLEYVIEW(dev)) {
+		if (mode & (GEN7_RC_CTL_TO_MODE | GEN6_RC_CTL_EI_MODE(1)))
+			mode = GEN6_RC_CTL_RC6_ENABLE;
+		else
+			mode = 0;
+	}
+	if (HAS_RC6p(dev))
+		DRM_DEBUG_KMS("Enabling RC6 states: RC6 %s RC6p %s RC6pp %s\n",
+			      (mode & GEN6_RC_CTL_RC6_ENABLE) ? "on" : "off",
+			      (mode & GEN6_RC_CTL_RC6p_ENABLE) ? "on" : "off",
+			      (mode & GEN6_RC_CTL_RC6pp_ENABLE) ? "on" : "off");
 
-	mutex_unlock(&dev_priv->rps.hw_lock);
+	else
+		DRM_DEBUG_KMS("Enabling RC6 states: RC6 %s\n",
+			      (mode & GEN6_RC_CTL_RC6_ENABLE) ? "on" : "off");
 }
 
-static void cherryview_init_gt_powersave(struct drm_device *dev)
+static int sanitize_rc6_option(const struct drm_device *dev, int enable_rc6)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	/* No RC6 before Ironlake */
+	if (INTEL_INFO(dev)->gen < 5)
+		return 0;
 
-	cherryview_setup_pctx(dev);
+	/* RC6 is only on Ironlake mobile not on desktop */
+	if (INTEL_INFO(dev)->gen == 5 && !IS_IRONLAKE_M(dev))
+		return 0;
 
-	mutex_lock(&dev_priv->rps.hw_lock);
+	/* Respect the kernel parameter if it is set */
+	if (enable_rc6 >= 0) {
+		int mask;
 
-	dev_priv->rps.max_freq = cherryview_rps_max_freq(dev_priv);
-	dev_priv->rps.rp0_freq = dev_priv->rps.max_freq;
-	DRM_DEBUG_DRIVER("max GPU freq: %d MHz (%u)\n",
-			 vlv_gpu_freq(dev_priv, dev_priv->rps.max_freq),
-			 dev_priv->rps.max_freq);
+		if (HAS_RC6p(dev))
+			mask = INTEL_RC6_ENABLE | INTEL_RC6p_ENABLE |
+			       INTEL_RC6pp_ENABLE;
+		else
+			mask = INTEL_RC6_ENABLE;
 
-	dev_priv->rps.efficient_freq = cherryview_rps_rpe_freq(dev_priv);
-	DRM_DEBUG_DRIVER("RPe GPU freq: %d MHz (%u)\n",
-			 vlv_gpu_freq(dev_priv, dev_priv->rps.efficient_freq),
-			 dev_priv->rps.efficient_freq);
+		if ((enable_rc6 & mask) != enable_rc6)
+			DRM_DEBUG_KMS("Adjusting RC6 mask to %d (requested %d, valid %d)\n",
+				      enable_rc6 & mask, enable_rc6, mask);
 
-	dev_priv->rps.rp1_freq = cherryview_rps_guar_freq(dev_priv);
-	DRM_DEBUG_DRIVER("RP1(Guar) GPU freq: %d MHz (%u)\n",
-			 vlv_gpu_freq(dev_priv, dev_priv->rps.rp1_freq),
-			 dev_priv->rps.rp1_freq);
+		return enable_rc6 & mask;
+	}
 
-	dev_priv->rps.min_freq = cherryview_rps_min_freq(dev_priv);
-	DRM_DEBUG_DRIVER("min GPU freq: %d MHz (%u)\n",
-			 vlv_gpu_freq(dev_priv, dev_priv->rps.min_freq),
-			 dev_priv->rps.min_freq);
+#ifdef CONFIG_INTEL_IOMMU
+	/* Ironlake + RC6 + VT-d empirically blows up */
+	if (IS_GEN5(dev) && intel_iommu_gfx_mapped)
+		return 0;
+#endif
+
+	if (IS_IVYBRIDGE(dev))
+		return (INTEL_RC6_ENABLE | INTEL_RC6p_ENABLE);
+
+	return INTEL_RC6_ENABLE;
+}
+
+int intel_enable_rc6(const struct drm_device *dev)
+{
+	return i915_module.enable_rc6;
+}
+
+static void parse_rp_state_cap(struct drm_i915_private *dev_priv, u32 rp_state_cap)
+{
+	/* All of these values are in units of 50MHz */
+	dev_priv->rps.cur_freq		= 0;
+	/* static values from HW: RP0 < RPe < RP1 < RPn (min_freq) */
+	dev_priv->rps.rp1_freq		= (rp_state_cap >>  8) & 0xff;
+	dev_priv->rps.rp0_freq		= (rp_state_cap >>  0) & 0xff;
+	dev_priv->rps.min_freq		= (rp_state_cap >> 16) & 0xff;
+	/* XXX: only BYT has a special efficient freq */
+	dev_priv->rps.efficient_freq	= dev_priv->rps.rp1_freq;
+	/* hw_max = RP0 until we check for overclocking */
+	dev_priv->rps.max_freq		= dev_priv->rps.rp0_freq;
 
 	/* Preserve min/max settings in case of re-init */
 	if (dev_priv->rps.max_freq_softlimit == 0)
@@ -4036,2743 +4664,2384 @@
 
 	if (dev_priv->rps.min_freq_softlimit == 0)
 		dev_priv->rps.min_freq_softlimit = dev_priv->rps.min_freq;
-
-	mutex_unlock(&dev_priv->rps.hw_lock);
-}
-
-static void valleyview_cleanup_gt_powersave(struct drm_device *dev)
-{
-	valleyview_cleanup_pctx(dev);
 }
 
-static void cherryview_enable_rps(struct drm_device *dev)
+static void gen9_enable_rps(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
-	u32 gtfifodbg, val, rc6_mode = 0, pcbr;
-	int i;
-
-	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
-
-	gtfifodbg = I915_READ(GTFIFODBG);
-	if (gtfifodbg) {
-		DRM_DEBUG_DRIVER("GT fifo had a previous error %x\n",
-				 gtfifodbg);
-		I915_WRITE(GTFIFODBG, gtfifodbg);
-	}
+	struct intel_engine_cs *engine;
+	uint32_t rc6_mask = 0;
+	int unused;
 
-	cherryview_check_pctx(dev_priv);
+	/* 1a: Software RC state - RC0 */
+	I915_WRITE(GEN6_RC_STATE, 0);
 
-	/* 1a & 1b: Get forcewake during program sequence. Although the driver
+	/* 1b: Get forcewake during program sequence. Although the driver
 	 * hasn't enabled a state yet where we need forcewake, BIOS may have.*/
 	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
 
-	/* 2a: Program RC6 thresholds.*/
-	I915_WRITE(GEN6_RC6_WAKE_RATE_LIMIT, 40 << 16);
+	/* 2a: Disable RC states. */
+	I915_WRITE(GEN6_RC_CONTROL, 0);
+
+	/* 2b: Program RC6 thresholds.*/
+	I915_WRITE(GEN6_RC6_WAKE_RATE_LIMIT, 54 << 16);
 	I915_WRITE(GEN6_RC_EVALUATION_INTERVAL, 125000); /* 12500 * 1280ns */
 	I915_WRITE(GEN6_RC_IDLE_HYSTERSIS, 25); /* 25 * 1280ns */
-
-	for_each_ring(ring, dev_priv, i)
-		I915_WRITE(RING_MAX_IDLE(ring->mmio_base), 10);
+	for_each_engine(engine, dev_priv, unused)
+		I915_WRITE(RING_MAX_IDLE(engine->mmio_base), 10);
 	I915_WRITE(GEN6_RC_SLEEP, 0);
+	I915_WRITE(GEN6_RC6_THRESHOLD, 37500); /* 37.5/125ms per EI */
 
-	I915_WRITE(GEN6_RC6_THRESHOLD, 50000); /* 50/125ms per EI */
+	/* 3a: Enable RC6 */
+	if (intel_enable_rc6(dev) & INTEL_RC6_ENABLE)
+		rc6_mask = GEN6_RC_CTL_RC6_ENABLE;
+	DRM_INFO("RC6 %s\n", (rc6_mask & GEN6_RC_CTL_RC6_ENABLE) ?
+			"on" : "off");
+	I915_WRITE(GEN6_RC_CONTROL, GEN6_RC_CTL_HW_ENABLE |
+				   GEN6_RC_CTL_EI_MODE(1) |
+				   rc6_mask);
 
-	/* allows RC6 residency counter to work */
-	I915_WRITE(VLV_COUNTER_CONTROL,
-		   _MASKED_BIT_ENABLE(VLV_COUNT_RANGE_HIGH |
-				      VLV_MEDIA_RC6_COUNT_EN |
-				      VLV_RENDER_RC6_COUNT_EN));
+	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
 
-	/* For now we assume BIOS is allocating and populating the PCBR  */
-	pcbr = I915_READ(VLV_PCBR);
+}
 
-	DRM_DEBUG_DRIVER("PCBR offset : 0x%x\n", pcbr);
+static void gen8_enable_rps(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_engine_cs *engine;
+	uint32_t rc6_mask = 0, rp_state_cap;
+	int unused;
 
-	/* 3: Enable RC6 */
-	if ((intel_enable_rc6(dev) & INTEL_RC6_ENABLE) &&
-						(pcbr >> VLV_PCBR_ADDR_SHIFT))
-		rc6_mode = GEN6_RC_CTL_EI_MODE(1);
+	/* 1a: Software RC state - RC0 */
+	I915_WRITE(GEN6_RC_STATE, 0);
 
-	I915_WRITE(GEN6_RC_CONTROL, rc6_mode);
+	/* 1c & 1d: Get forcewake during program sequence. Although the driver
+	 * hasn't enabled a state yet where we need forcewake, BIOS may have.*/
+	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
+
+	/* 2a: Disable RC states. */
+	I915_WRITE(GEN6_RC_CONTROL, 0);
+
+	rp_state_cap = I915_READ(GEN6_RP_STATE_CAP);
+	parse_rp_state_cap(dev_priv, rp_state_cap);
+
+	/* 2b: Program RC6 thresholds.*/
+	I915_WRITE(GEN6_RC6_WAKE_RATE_LIMIT, 40 << 16);
+	I915_WRITE(GEN6_RC_EVALUATION_INTERVAL, 125000); /* 12500 * 1280ns */
+	I915_WRITE(GEN6_RC_IDLE_HYSTERSIS, 25); /* 25 * 1280ns */
+	for_each_engine(engine, dev_priv, unused)
+		I915_WRITE(RING_MAX_IDLE(engine->mmio_base), 10);
+	I915_WRITE(GEN6_RC_SLEEP, 0);
+	if (IS_BROADWELL(dev))
+		I915_WRITE(GEN6_RC6_THRESHOLD, 625); /* 800us/1.28 for TO */
+	else
+		I915_WRITE(GEN6_RC6_THRESHOLD, 50000); /* 50/125ms per EI */
+
+	/* 3: Enable RC6 */
+	if (intel_enable_rc6(dev) & INTEL_RC6_ENABLE)
+		rc6_mask = GEN6_RC_CTL_RC6_ENABLE;
+	intel_print_rc6_info(dev, rc6_mask);
+	if (IS_BROADWELL(dev))
+		I915_WRITE(GEN6_RC_CONTROL, GEN6_RC_CTL_HW_ENABLE |
+				GEN7_RC_CTL_TO_MODE |
+				rc6_mask);
+	else
+		I915_WRITE(GEN6_RC_CONTROL, GEN6_RC_CTL_HW_ENABLE |
+				GEN6_RC_CTL_EI_MODE(1) |
+				rc6_mask);
 
 	/* 4 Program defaults and thresholds for RPS*/
-	I915_WRITE(GEN6_RP_UP_THRESHOLD, 59400);
-	I915_WRITE(GEN6_RP_DOWN_THRESHOLD, 245000);
-	I915_WRITE(GEN6_RP_UP_EI, 66000);
-	I915_WRITE(GEN6_RP_DOWN_EI, 350000);
+	I915_WRITE(GEN6_RPNSWREQ,
+		   HSW_FREQUENCY(dev_priv->rps.rp1_freq));
+	I915_WRITE(GEN6_RC_VIDEO_FREQ,
+		   HSW_FREQUENCY(dev_priv->rps.rp1_freq));
+	/* NB: Docs say 1s, and 1000000 - which aren't equivalent */
+	I915_WRITE(GEN6_RP_DOWN_TIMEOUT, 100000000 / 128); /* 1 second timeout */
 
-	I915_WRITE(GEN6_RP_IDLE_HYSTERSIS, 10);
+	/* Docs recommend 900MHz, and 300 MHz respectively */
+	I915_WRITE(GEN6_RP_INTERRUPT_LIMITS,
+		   dev_priv->rps.max_freq_softlimit << 24 |
+		   dev_priv->rps.min_freq_softlimit << 16);
 
-	/* WaDisablePwrmtrEvent:chv (pre-production hw) */
-	I915_WRITE(0xA80C, I915_READ(0xA80C) & 0x00ffffff);
-	I915_WRITE(0xA810, I915_READ(0xA810) & 0xffffff00);
+	I915_WRITE(GEN6_RP_UP_THRESHOLD, 7600000 / 128); /* 76ms busyness per EI, 90% */
+	I915_WRITE(GEN6_RP_DOWN_THRESHOLD, 31300000 / 128); /* 313ms busyness per EI, 70%*/
+	I915_WRITE(GEN6_RP_UP_EI, 66000); /* 84.48ms, XXX: random? */
+	I915_WRITE(GEN6_RP_DOWN_EI, 350000); /* 448ms, XXX: random? */
+
+	I915_WRITE(GEN6_RP_IDLE_HYSTERSIS, 10);
 
 	/* 5: Enable RPS */
 	I915_WRITE(GEN6_RP_CONTROL,
+		   GEN6_RP_MEDIA_TURBO |
 		   GEN6_RP_MEDIA_HW_NORMAL_MODE |
-		   GEN6_RP_MEDIA_IS_GFX | /* WaSetMaskForGfxBusyness:chv (pre-production hw ?) */
+		   GEN6_RP_MEDIA_IS_GFX |
 		   GEN6_RP_ENABLE |
 		   GEN6_RP_UP_BUSY_AVG |
 		   GEN6_RP_DOWN_IDLE_AVG);
 
-	val = vlv_punit_read(dev_priv, PUNIT_REG_GPU_FREQ_STS);
-
-	DRM_DEBUG_DRIVER("GPLL enabled? %s\n", val & 0x10 ? "yes" : "no");
-	DRM_DEBUG_DRIVER("GPU status: 0x%08x\n", val);
-
-	dev_priv->rps.cur_freq = (val >> 8) & 0xff;
-	DRM_DEBUG_DRIVER("current GPU freq: %d MHz (%u)\n",
-			 vlv_gpu_freq(dev_priv, dev_priv->rps.cur_freq),
-			 dev_priv->rps.cur_freq);
-
-	DRM_DEBUG_DRIVER("setting GPU freq to %d MHz (%u)\n",
-			 vlv_gpu_freq(dev_priv, dev_priv->rps.efficient_freq),
-			 dev_priv->rps.efficient_freq);
-
-	valleyview_set_rps(dev_priv->dev, dev_priv->rps.efficient_freq);
+	/* 6: Ring frequency + overclocking (our driver does this later */
 
-	gen8_enable_rps_interrupts(dev);
+	gen6_set_rps(dev, (I915_READ(GEN6_GT_PERF_STATUS) & 0xff00) >> 8);
 
 	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
 }
 
-static void valleyview_enable_rps(struct drm_device *dev)
+static void gen6_enable_rps(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
-	u32 gtfifodbg, val, rc6_mode = 0;
-	int i;
+	struct intel_engine_cs *engine;
+	u32 rp_state_cap;
+	u32 rc6vids, pcu_mbox = 0, rc6_mask = 0;
+	u32 gtfifodbg;
+	int rc6_mode;
+	int i, ret;
 
 	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
 
-	valleyview_check_pctx(dev_priv);
+	/* Here begins a magic sequence of register writes to enable
+	 * auto-downclocking.
+	 *
+	 * Perhaps there might be some value in exposing these to
+	 * userspace...
+	 */
+	I915_WRITE(GEN6_RC_STATE, 0);
 
+	/* Clear the DBG now so we don't confuse earlier errors */
 	if ((gtfifodbg = I915_READ(GTFIFODBG))) {
-		DRM_DEBUG_DRIVER("GT fifo had a previous error %x\n",
-				 gtfifodbg);
+		DRM_ERROR("GT fifo had a previous error %x\n", gtfifodbg);
 		I915_WRITE(GTFIFODBG, gtfifodbg);
 	}
 
-	/* If VLV, Forcewake all wells, else re-direct to regular path */
 	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
 
-	I915_WRITE(GEN6_RP_UP_THRESHOLD, 59400);
-	I915_WRITE(GEN6_RP_DOWN_THRESHOLD, 245000);
-	I915_WRITE(GEN6_RP_UP_EI, 66000);
-	I915_WRITE(GEN6_RP_DOWN_EI, 350000);
+	rp_state_cap = I915_READ(GEN6_RP_STATE_CAP);
 
-	I915_WRITE(GEN6_RP_IDLE_HYSTERSIS, 10);
-	I915_WRITE(GEN6_RP_DOWN_TIMEOUT, 0xf4240);
+	parse_rp_state_cap(dev_priv, rp_state_cap);
 
-	I915_WRITE(GEN6_RP_CONTROL,
-		   GEN6_RP_MEDIA_TURBO |
-		   GEN6_RP_MEDIA_HW_NORMAL_MODE |
-		   GEN6_RP_MEDIA_IS_GFX |
-		   GEN6_RP_ENABLE |
-		   GEN6_RP_UP_BUSY_AVG |
-		   GEN6_RP_DOWN_IDLE_CONT);
+	/* disable the counters and set deterministic thresholds */
+	I915_WRITE(GEN6_RC_CONTROL, 0);
 
-	I915_WRITE(GEN6_RC6_WAKE_RATE_LIMIT, 0x00280000);
+	I915_WRITE(GEN6_RC1_WAKE_RATE_LIMIT, 1000 << 16);
+	I915_WRITE(GEN6_RC6_WAKE_RATE_LIMIT, 40 << 16 | 30);
+	I915_WRITE(GEN6_RC6pp_WAKE_RATE_LIMIT, 30);
 	I915_WRITE(GEN6_RC_EVALUATION_INTERVAL, 125000);
 	I915_WRITE(GEN6_RC_IDLE_HYSTERSIS, 25);
 
-	for_each_ring(ring, dev_priv, i)
-		I915_WRITE(RING_MAX_IDLE(ring->mmio_base), 10);
+	for_each_engine(engine, dev_priv, i)
+		I915_WRITE(RING_MAX_IDLE(engine->mmio_base), 10);
 
-	I915_WRITE(GEN6_RC6_THRESHOLD, 0x557);
+	I915_WRITE(GEN6_RC_SLEEP, 0);
+	I915_WRITE(GEN6_RC1e_THRESHOLD, 1000);
+	if (IS_IVYBRIDGE(dev))
+		I915_WRITE(GEN6_RC6_THRESHOLD, 125000);
+	else
+		I915_WRITE(GEN6_RC6_THRESHOLD, 50000);
+	I915_WRITE(GEN6_RC6p_THRESHOLD, 150000);
+	I915_WRITE(GEN6_RC6pp_THRESHOLD, 64000); /* unused */
 
-	/* allows RC6 residency counter to work */
-	I915_WRITE(VLV_COUNTER_CONTROL,
-		   _MASKED_BIT_ENABLE(VLV_MEDIA_RC0_COUNT_EN |
-				      VLV_RENDER_RC0_COUNT_EN |
-				      VLV_MEDIA_RC6_COUNT_EN |
-				      VLV_RENDER_RC6_COUNT_EN));
+	/* Check if we are enabling RC6 */
+	rc6_mode = intel_enable_rc6(dev_priv->dev);
+	if (rc6_mode & INTEL_RC6_ENABLE)
+		rc6_mask |= GEN6_RC_CTL_RC6_ENABLE;
 
-	if (intel_enable_rc6(dev) & INTEL_RC6_ENABLE)
-		rc6_mode = GEN7_RC_CTL_TO_MODE | VLV_RC_CTL_CTX_RST_PARALLEL;
+	/* We don't use those on Haswell */
+	if (!IS_HASWELL(dev)) {
+		if (rc6_mode & INTEL_RC6p_ENABLE)
+			rc6_mask |= GEN6_RC_CTL_RC6p_ENABLE;
 
-	intel_print_rc6_info(dev, rc6_mode);
+		if (rc6_mode & INTEL_RC6pp_ENABLE)
+			rc6_mask |= GEN6_RC_CTL_RC6pp_ENABLE;
+	}
 
-	I915_WRITE(GEN6_RC_CONTROL, rc6_mode);
+	intel_print_rc6_info(dev, rc6_mask);
 
-	val = vlv_punit_read(dev_priv, PUNIT_REG_GPU_FREQ_STS);
+	I915_WRITE(GEN6_RC_CONTROL,
+		   rc6_mask |
+		   GEN6_RC_CTL_EI_MODE(1) |
+		   GEN6_RC_CTL_HW_ENABLE);
 
-	DRM_DEBUG_DRIVER("GPLL enabled? %s\n", val & 0x10 ? "yes" : "no");
-	DRM_DEBUG_DRIVER("GPU status: 0x%08x\n", val);
+	/* Power down if completely idle for over 50ms */
+	I915_WRITE(GEN6_RP_DOWN_TIMEOUT, 50000);
+	I915_WRITE(GEN6_RP_IDLE_HYSTERSIS, 10);
 
-	dev_priv->rps.cur_freq = (val >> 8) & 0xff;
-	DRM_DEBUG_DRIVER("current GPU freq: %d MHz (%u)\n",
-			 vlv_gpu_freq(dev_priv, dev_priv->rps.cur_freq),
-			 dev_priv->rps.cur_freq);
+	ret = sandybridge_pcode_write(dev_priv, GEN6_PCODE_WRITE_MIN_FREQ_TABLE, 0);
+	if (ret)
+		DRM_DEBUG_DRIVER("Failed to set the min frequency\n");
 
-	DRM_DEBUG_DRIVER("setting GPU freq to %d MHz (%u)\n",
-			 vlv_gpu_freq(dev_priv, dev_priv->rps.efficient_freq),
-			 dev_priv->rps.efficient_freq);
+	ret = sandybridge_pcode_read(dev_priv, GEN6_READ_OC_PARAMS, &pcu_mbox);
+	if (!ret && (pcu_mbox & (1<<31))) { /* OC supported */
+		DRM_DEBUG_DRIVER("Overclocking supported. Max: %dMHz, Overclock max: %dMHz\n",
+				 (dev_priv->rps.max_freq_softlimit & 0xff) * 50,
+				 (pcu_mbox & 0xff) * 50);
+		dev_priv->rps.max_freq = pcu_mbox & 0xff;
+	}
 
-	valleyview_set_rps(dev_priv->dev, dev_priv->rps.efficient_freq);
+	dev_priv->rps.power = HIGH_POWER; /* force a reset */
+	gen6_set_rps(dev_priv->dev, dev_priv->rps.min_freq);
 
-	gen6_enable_rps_interrupts(dev);
+	rc6vids = 0;
+	ret = sandybridge_pcode_read(dev_priv, GEN6_PCODE_READ_RC6VIDS, &rc6vids);
+	if (IS_GEN6(dev) && ret) {
+		DRM_DEBUG_DRIVER("Couldn't check for BIOS workaround\n");
+	} else if (IS_GEN6(dev) && (GEN6_DECODE_RC6_VID(rc6vids & 0xff) < 450)) {
+		DRM_DEBUG_DRIVER("You should update your BIOS. Correcting minimum rc6 voltage (%dmV->%dmV)\n",
+			  GEN6_DECODE_RC6_VID(rc6vids & 0xff), 450);
+		rc6vids &= 0xffff00;
+		rc6vids |= GEN6_ENCODE_RC6_VID(450);
+		ret = sandybridge_pcode_write(dev_priv, GEN6_PCODE_WRITE_RC6VIDS, rc6vids);
+		if (ret)
+			DRM_ERROR("Couldn't fix incorrect rc6 voltage\n");
+	}
 
 	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
 }
 
-void ironlake_teardown_rc6(struct drm_device *dev)
+static void __gen6_update_ring_freq(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
+	unsigned int max_ia_freq, min_ia_freq, min_ring_freq;
+	unsigned int gpu_freq;
+	struct cpufreq_policy *policy;
 
-	if (dev_priv->ips.renderctx) {
-		i915_gem_object_ggtt_unpin(dev_priv->ips.renderctx);
-		drm_gem_object_unreference(&dev_priv->ips.renderctx->base);
-		dev_priv->ips.renderctx = NULL;
-	}
+	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
 
-	if (dev_priv->ips.pwrctx) {
-		i915_gem_object_ggtt_unpin(dev_priv->ips.pwrctx);
-		drm_gem_object_unreference(&dev_priv->ips.pwrctx->base);
-		dev_priv->ips.pwrctx = NULL;
+	min_ring_freq = I915_READ(DCLK) & 0xf;
+	/* convert DDR frequency from units of 266.6MHz to bandwidth */
+	min_ring_freq = mult_frac(min_ring_freq, 8, 3);
+
+	policy = cpufreq_cpu_get(0);
+	if (policy) {
+		/* Convert from kHz to MHz */
+		max_ia_freq = policy->cpuinfo.max_freq / 1000;
+		min_ia_freq = policy->cpuinfo.min_freq / 1000;
+		cpufreq_cpu_put(policy);
+	} else {
+		/*
+		 * Default to measured freq if none found, PCU will ensure we
+		 * don't go over
+		 */
+		max_ia_freq = tsc_khz / 1000;
+		min_ia_freq = min_ring_freq;
 	}
-}
 
-static void ironlake_disable_rc6(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	/*
+	 * For each potential GPU frequency, load a ring frequency we'd like
+	 * to use for memory access.  We do this by specifying the IA frequency
+	 * the PCU should use as a reference to determine the ring frequency.
+	 */
+	for (gpu_freq = dev_priv->rps.max_freq; gpu_freq >= dev_priv->rps.min_freq;
+	     gpu_freq--) {
+		unsigned int ia_freq = 0, ring_freq = 0;
 
-	if (I915_READ(PWRCTXA)) {
-		/* Wake the GPU, prevent RC6, then restore RSTDBYCTL */
-		I915_WRITE(RSTDBYCTL, I915_READ(RSTDBYCTL) | RCX_SW_EXIT);
-		wait_for(((I915_READ(RSTDBYCTL) & RSX_STATUS_MASK) == RSX_STATUS_ON),
-			 50);
+		if (INTEL_INFO(dev)->gen >= 8) {
+			/* max(2 * GT, DDR). NB: GT is 50MHz units */
+			ring_freq = max(min_ring_freq, gpu_freq);
+		} else if (IS_HASWELL(dev)) {
+			ring_freq = mult_frac(gpu_freq, 5, 4);
+			ring_freq = max(min_ring_freq, ring_freq);
+			/* leave ia_freq as the default, chosen by cpufreq */
+		} else {
+			const int scaling_factor = 180;
+			int diff;
 
-		I915_WRITE(PWRCTXA, 0);
-		POSTING_READ(PWRCTXA);
+			/* On older processors, there is no separate ring
+			 * clock domain, so in order to boost the bandwidth
+			 * of the ring, we need to upclock the CPU (ia_freq).
+			 */
+			diff = dev_priv->rps.max_freq_softlimit - gpu_freq;
+			diff = max_ia_freq - diff * scaling_factor / 2;
+			ia_freq = max((int)min_ia_freq, diff);
+			ia_freq = DIV_ROUND_CLOSEST(ia_freq, 100);
+		}
 
-		I915_WRITE(RSTDBYCTL, I915_READ(RSTDBYCTL) & ~RCX_SW_EXIT);
-		POSTING_READ(RSTDBYCTL);
+		sandybridge_pcode_write(dev_priv,
+					GEN6_PCODE_WRITE_MIN_FREQ_TABLE,
+					ia_freq << GEN6_PCODE_FREQ_IA_RATIO_SHIFT |
+					ring_freq << GEN6_PCODE_FREQ_RING_RATIO_SHIFT |
+					gpu_freq);
 	}
 }
 
-static int ironlake_setup_rc6(struct drm_device *dev)
+void gen6_update_ring_freq(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	if (dev_priv->ips.renderctx == NULL)
-		dev_priv->ips.renderctx = intel_alloc_context_page(dev);
-	if (!dev_priv->ips.renderctx)
-		return -ENOMEM;
-
-	if (dev_priv->ips.pwrctx == NULL)
-		dev_priv->ips.pwrctx = intel_alloc_context_page(dev);
-	if (!dev_priv->ips.pwrctx) {
-		ironlake_teardown_rc6(dev);
-		return -ENOMEM;
-	}
+	if (INTEL_INFO(dev)->gen < 6 || IS_VALLEYVIEW(dev))
+		return;
 
-	return 0;
+	mutex_lock(&dev_priv->rps.hw_lock);
+	__gen6_update_ring_freq(dev);
+	mutex_unlock(&dev_priv->rps.hw_lock);
 }
 
-static void ironlake_enable_rc6(struct drm_device *dev)
+static int cherryview_rps_max_freq(struct drm_i915_private *dev_priv)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring = &dev_priv->ring[RCS];
-	bool was_interruptible;
-	int ret;
-
-	/* rc6 disabled by default due to repeated reports of hanging during
-	 * boot and resume.
-	 */
-	if (!intel_enable_rc6(dev))
-		return;
+	u32 val, rp0;
 
-	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
+	val = vlv_punit_read(dev_priv, PUNIT_GPU_STATUS_REG);
+	rp0 = (val >> PUNIT_GPU_STATUS_MAX_FREQ_SHIFT) & PUNIT_GPU_STATUS_MAX_FREQ_MASK;
 
-	ret = ironlake_setup_rc6(dev);
-	if (ret)
-		return;
+	return rp0;
+}
 
-	was_interruptible = dev_priv->mm.interruptible;
-	dev_priv->mm.interruptible = false;
+static int cherryview_rps_rpe_freq(struct drm_i915_private *dev_priv)
+{
+	u32 val, rpe;
 
-	/*
-	 * GPU can automatically power down the render unit if given a page
-	 * to save state.
-	 */
-	ret = intel_ring_begin(ring, 6);
-	if (ret) {
-		ironlake_teardown_rc6(dev);
-		dev_priv->mm.interruptible = was_interruptible;
-		return;
-	}
+	val = vlv_punit_read(dev_priv, PUNIT_GPU_DUTYCYCLE_REG);
+	rpe = (val >> PUNIT_GPU_DUTYCYCLE_RPE_FREQ_SHIFT) & PUNIT_GPU_DUTYCYCLE_RPE_FREQ_MASK;
 
-	intel_ring_emit(ring, MI_SUSPEND_FLUSH | MI_SUSPEND_FLUSH_EN);
-	intel_ring_emit(ring, MI_SET_CONTEXT);
-	intel_ring_emit(ring, i915_gem_obj_ggtt_offset(dev_priv->ips.renderctx) |
-			MI_MM_SPACE_GTT |
-			MI_SAVE_EXT_STATE_EN |
-			MI_RESTORE_EXT_STATE_EN |
-			MI_RESTORE_INHIBIT);
-	intel_ring_emit(ring, MI_SUSPEND_FLUSH);
-	intel_ring_emit(ring, MI_NOOP);
-	intel_ring_emit(ring, MI_FLUSH);
-	intel_ring_advance(ring);
+	return rpe;
+}
 
-	/*
-	 * Wait for the command parser to advance past MI_SET_CONTEXT. The HW
-	 * does an implicit flush, combined with MI_FLUSH above, it should be
-	 * safe to assume that renderctx is valid
-	 */
-	ret = intel_ring_idle(ring);
-	dev_priv->mm.interruptible = was_interruptible;
-	if (ret) {
-		DRM_ERROR("failed to enable ironlake power savings\n");
-		ironlake_teardown_rc6(dev);
-		return;
-	}
+static int cherryview_rps_guar_freq(struct drm_i915_private *dev_priv)
+{
+	u32 val, rp1;
 
-	I915_WRITE(PWRCTXA, i915_gem_obj_ggtt_offset(dev_priv->ips.pwrctx) | PWRCTX_EN);
-	I915_WRITE(RSTDBYCTL, I915_READ(RSTDBYCTL) & ~RCX_SW_EXIT);
+	val = vlv_punit_read(dev_priv, PUNIT_REG_GPU_FREQ_STS);
+	rp1 = (val >> PUNIT_GPU_STATUS_MAX_FREQ_SHIFT) & PUNIT_GPU_STATUS_MAX_FREQ_MASK;
 
-	intel_print_rc6_info(dev, GEN6_RC_CTL_RC6_ENABLE);
+	return rp1;
 }
 
-static unsigned long intel_pxfreq(u32 vidfreq)
+static int cherryview_rps_min_freq(struct drm_i915_private *dev_priv)
 {
-	unsigned long freq;
-	int div = (vidfreq & 0x3f0000) >> 16;
-	int post = (vidfreq & 0x3000) >> 12;
-	int pre = (vidfreq & 0x7);
-
-	if (!pre)
-		return 0;
-
-	freq = ((div * 133333) / ((1<<post) * pre));
+	u32 val, rpn;
 
-	return freq;
+	val = vlv_punit_read(dev_priv, PUNIT_GPU_STATUS_REG);
+	rpn = (val >> PUNIT_GPU_STATIS_GFX_MIN_FREQ_SHIFT) & PUNIT_GPU_STATUS_GFX_MIN_FREQ_MASK;
+	return rpn;
 }
 
-static const struct cparams {
-	u16 i;
-	u16 t;
-	u16 m;
-	u16 c;
-} cparams[] = {
-	{ 1, 1333, 301, 28664 },
-	{ 1, 1066, 294, 24460 },
-	{ 1, 800, 294, 25192 },
-	{ 0, 1333, 276, 27605 },
-	{ 0, 1066, 276, 27605 },
-	{ 0, 800, 231, 23784 },
-};
-
-static unsigned long __i915_chipset_val(struct drm_i915_private *dev_priv)
+static int valleyview_rps_guar_freq(struct drm_i915_private *dev_priv)
 {
-	u64 total_count, diff, ret;
-	u32 count1, count2, count3, m = 0, c = 0;
-	unsigned long now = jiffies_to_msecs(jiffies), diff1;
-	int i;
+	u32 val, rp1;
 
-	assert_spin_locked(&mchdev_lock);
+	val = vlv_nc_read(dev_priv, IOSF_NC_FB_GFX_FREQ_FUSE);
 
-	diff1 = now - dev_priv->ips.last_time1;
+	rp1 = (val & FB_GFX_FGUARANTEED_FREQ_FUSE_MASK) >> FB_GFX_FGUARANTEED_FREQ_FUSE_SHIFT;
 
-	/* Prevent division-by-zero if we are asking too fast.
-	 * Also, we don't get interesting results if we are polling
-	 * faster than once in 10ms, so just return the saved value
-	 * in such cases.
-	 */
-	if (diff1 <= 10)
-		return dev_priv->ips.chipset_power;
+	return rp1;
+}
 
-	count1 = I915_READ(DMIEC);
-	count2 = I915_READ(DDREC);
-	count3 = I915_READ(CSIEC);
+static int valleyview_rps_max_freq(struct drm_i915_private *dev_priv)
+{
+	u32 val, rp0;
 
-	total_count = count1 + count2 + count3;
+	val = vlv_nc_read(dev_priv, IOSF_NC_FB_GFX_FREQ_FUSE);
 
-	/* FIXME: handle per-counter overflow */
-	if (total_count < dev_priv->ips.last_count1) {
-		diff = ~0UL - dev_priv->ips.last_count1;
-		diff += total_count;
-	} else {
-		diff = total_count - dev_priv->ips.last_count1;
-	}
+	rp0 = (val & FB_GFX_MAX_FREQ_FUSE_MASK) >> FB_GFX_MAX_FREQ_FUSE_SHIFT;
+	/* Clamp to max */
+	rp0 = min_t(u32, rp0, 0xea);
 
-	for (i = 0; i < ARRAY_SIZE(cparams); i++) {
-		if (cparams[i].i == dev_priv->ips.c_m &&
-		    cparams[i].t == dev_priv->ips.r_t) {
-			m = cparams[i].m;
-			c = cparams[i].c;
-			break;
-		}
-	}
+	return rp0;
+}
 
-	diff = div_u64(diff, diff1);
-	ret = ((m * diff) + c);
-	ret = div_u64(ret, 10);
+static int valleyview_rps_rpe_freq(struct drm_i915_private *dev_priv)
+{
+	u32 val, rpe;
 
-	dev_priv->ips.last_count1 = total_count;
-	dev_priv->ips.last_time1 = now;
+	val = vlv_nc_read(dev_priv, IOSF_NC_FB_GFX_FMAX_FUSE_LO);
+	rpe = (val & FB_FMAX_VMIN_FREQ_LO_MASK) >> FB_FMAX_VMIN_FREQ_LO_SHIFT;
+	val = vlv_nc_read(dev_priv, IOSF_NC_FB_GFX_FMAX_FUSE_HI);
+	rpe |= (val & FB_FMAX_VMIN_FREQ_HI_MASK) << 5;
 
-	dev_priv->ips.chipset_power = ret;
+	return rpe;
+}
 
-	return ret;
+static int valleyview_rps_min_freq(struct drm_i915_private *dev_priv)
+{
+	return vlv_punit_read(dev_priv, PUNIT_REG_GPU_LFM) & 0xff;
 }
 
-unsigned long i915_chipset_val(struct drm_i915_private *dev_priv)
+/* Check that the pctx buffer wasn't move under us. */
+static void valleyview_check_pctx(struct drm_i915_private *dev_priv)
 {
-	struct drm_device *dev = dev_priv->dev;
-	unsigned long val;
+	unsigned long pctx_addr = I915_READ(VLV_PCBR) & ~4095;
 
-	if (INTEL_INFO(dev)->gen != 5)
-		return 0;
+	WARN_ON(pctx_addr != dev_priv->mm.stolen_base +
+			     dev_priv->vlv_pctx->stolen->start);
+}
 
-	spin_lock_irq(&mchdev_lock);
 
-	val = __i915_chipset_val(dev_priv);
+/* Check that the pcbr address is not empty. */
+static void cherryview_check_pctx(struct drm_i915_private *dev_priv)
+{
+	unsigned long pctx_addr = I915_READ(VLV_PCBR) & ~4095;
 
-	spin_unlock_irq(&mchdev_lock);
+	WARN_ON((pctx_addr >> VLV_PCBR_ADDR_SHIFT) == 0);
+}
 
-	return val;
-}
-
-unsigned long i915_mch_val(struct drm_i915_private *dev_priv)
+static void cherryview_setup_pctx(struct drm_device *dev)
 {
-	unsigned long m, x, b;
-	u32 tsfs;
-
-	tsfs = I915_READ(TSFS);
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	unsigned long pctx_paddr, paddr;
+	struct i915_gtt *gtt = &dev_priv->gtt;
+	u32 pcbr;
+	int pctx_size = 32*1024;
 
-	m = ((tsfs & TSFS_SLOPE_MASK) >> TSFS_SLOPE_SHIFT);
-	x = I915_READ8(TR1);
+	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
 
-	b = tsfs & TSFS_INTR_MASK;
+	pcbr = I915_READ(VLV_PCBR);
+	if ((pcbr >> VLV_PCBR_ADDR_SHIFT) == 0) {
+		DRM_DEBUG_DRIVER("BIOS didn't set up PCBR, fixing up\n");
+		paddr = (dev_priv->mm.stolen_base +
+			 (gtt->stolen_size - pctx_size));
 
-	return ((m * x) / 127) - b;
-}
+		pctx_paddr = (paddr & (~4095));
+		I915_WRITE(VLV_PCBR, pctx_paddr);
+	}
 
-static u16 pvid_to_extvid(struct drm_i915_private *dev_priv, u8 pxvid)
-{
-	struct drm_device *dev = dev_priv->dev;
-	static const struct v_table {
-		u16 vd; /* in .1 mil */
-		u16 vm; /* in .1 mil */
-	} v_table[] = {
-		{ 0, 0, },
-		{ 375, 0, },
-		{ 500, 0, },
-		{ 625, 0, },
-		{ 750, 0, },
-		{ 875, 0, },
-		{ 1000, 0, },
-		{ 1125, 0, },
-		{ 4125, 3000, },
-		{ 4125, 3000, },
-		{ 4125, 3000, },
-		{ 4125, 3000, },
-		{ 4125, 3000, },
-		{ 4125, 3000, },
-		{ 4125, 3000, },
-		{ 4125, 3000, },
-		{ 4125, 3000, },
-		{ 4125, 3000, },
-		{ 4125, 3000, },
-		{ 4125, 3000, },
-		{ 4125, 3000, },
-		{ 4125, 3000, },
-		{ 4125, 3000, },
-		{ 4125, 3000, },
-		{ 4125, 3000, },
-		{ 4125, 3000, },
-		{ 4125, 3000, },
-		{ 4125, 3000, },
-		{ 4125, 3000, },
-		{ 4125, 3000, },
-		{ 4125, 3000, },
-		{ 4125, 3000, },
-		{ 4250, 3125, },
-		{ 4375, 3250, },
-		{ 4500, 3375, },
-		{ 4625, 3500, },
-		{ 4750, 3625, },
-		{ 4875, 3750, },
-		{ 5000, 3875, },
-		{ 5125, 4000, },
-		{ 5250, 4125, },
-		{ 5375, 4250, },
-		{ 5500, 4375, },
-		{ 5625, 4500, },
-		{ 5750, 4625, },
-		{ 5875, 4750, },
-		{ 6000, 4875, },
-		{ 6125, 5000, },
-		{ 6250, 5125, },
-		{ 6375, 5250, },
-		{ 6500, 5375, },
-		{ 6625, 5500, },
-		{ 6750, 5625, },
-		{ 6875, 5750, },
-		{ 7000, 5875, },
-		{ 7125, 6000, },
-		{ 7250, 6125, },
-		{ 7375, 6250, },
-		{ 7500, 6375, },
-		{ 7625, 6500, },
-		{ 7750, 6625, },
-		{ 7875, 6750, },
-		{ 8000, 6875, },
-		{ 8125, 7000, },
-		{ 8250, 7125, },
-		{ 8375, 7250, },
-		{ 8500, 7375, },
-		{ 8625, 7500, },
-		{ 8750, 7625, },
-		{ 8875, 7750, },
-		{ 9000, 7875, },
-		{ 9125, 8000, },
-		{ 9250, 8125, },
-		{ 9375, 8250, },
-		{ 9500, 8375, },
-		{ 9625, 8500, },
-		{ 9750, 8625, },
-		{ 9875, 8750, },
-		{ 10000, 8875, },
-		{ 10125, 9000, },
-		{ 10250, 9125, },
-		{ 10375, 9250, },
-		{ 10500, 9375, },
-		{ 10625, 9500, },
-		{ 10750, 9625, },
-		{ 10875, 9750, },
-		{ 11000, 9875, },
-		{ 11125, 10000, },
-		{ 11250, 10125, },
-		{ 11375, 10250, },
-		{ 11500, 10375, },
-		{ 11625, 10500, },
-		{ 11750, 10625, },
-		{ 11875, 10750, },
-		{ 12000, 10875, },
-		{ 12125, 11000, },
-		{ 12250, 11125, },
-		{ 12375, 11250, },
-		{ 12500, 11375, },
-		{ 12625, 11500, },
-		{ 12750, 11625, },
-		{ 12875, 11750, },
-		{ 13000, 11875, },
-		{ 13125, 12000, },
-		{ 13250, 12125, },
-		{ 13375, 12250, },
-		{ 13500, 12375, },
-		{ 13625, 12500, },
-		{ 13750, 12625, },
-		{ 13875, 12750, },
-		{ 14000, 12875, },
-		{ 14125, 13000, },
-		{ 14250, 13125, },
-		{ 14375, 13250, },
-		{ 14500, 13375, },
-		{ 14625, 13500, },
-		{ 14750, 13625, },
-		{ 14875, 13750, },
-		{ 15000, 13875, },
-		{ 15125, 14000, },
-		{ 15250, 14125, },
-		{ 15375, 14250, },
-		{ 15500, 14375, },
-		{ 15625, 14500, },
-		{ 15750, 14625, },
-		{ 15875, 14750, },
-		{ 16000, 14875, },
-		{ 16125, 15000, },
-	};
-	if (INTEL_INFO(dev)->is_mobile)
-		return v_table[pxvid].vm;
-	else
-		return v_table[pxvid].vd;
+	DRM_DEBUG_DRIVER("PCBR: 0x%08x\n", I915_READ(VLV_PCBR));
 }
 
-static void __i915_update_gfx_val(struct drm_i915_private *dev_priv)
+static void valleyview_setup_pctx(struct drm_device *dev)
 {
-	u64 now, diff, diffms;
-	u32 count;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_gem_object *pctx;
+	unsigned long pctx_paddr;
+	u32 pcbr;
+	int pctx_size = 24*1024;
 
-	assert_spin_locked(&mchdev_lock);
+	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
 
-	now = ktime_get_raw_ns();
-	diffms = now - dev_priv->ips.last_time2;
-	do_div(diffms, NSEC_PER_MSEC);
+	pcbr = I915_READ(VLV_PCBR);
+	if (pcbr) {
+		/* BIOS set it up already, grab the pre-alloc'd space */
+		int pcbr_offset;
 
-	/* Don't divide by 0 */
-	if (!diffms)
-		return;
+		pcbr_offset = (pcbr & (~4095)) - dev_priv->mm.stolen_base;
+		pctx = i915_gem_object_create_stolen_for_preallocated(dev_priv->dev,
+								      pcbr_offset,
+								      I915_GTT_OFFSET_NONE,
+								      pctx_size);
+		goto out;
+	}
 
-	count = I915_READ(GFXEC);
+	DRM_DEBUG_DRIVER("BIOS didn't set up PCBR, fixing up\n");
 
-	if (count < dev_priv->ips.last_count2) {
-		diff = ~0UL - dev_priv->ips.last_count2;
-		diff += count;
-	} else {
-		diff = count - dev_priv->ips.last_count2;
+	/*
+	 * From the Gunit register HAS:
+	 * The Gfx driver is expected to program this register and ensure
+	 * proper allocation within Gfx stolen memory.  For example, this
+	 * register should be programmed such than the PCBR range does not
+	 * overlap with other ranges, such as the frame buffer, protected
+	 * memory, or any other relevant ranges.
+	 */
+	pctx = i915_gem_object_create_stolen(dev, pctx_size);
+	if (!pctx) {
+		DRM_DEBUG("not enough stolen space for PCTX, disabling\n");
+		return;
 	}
 
-	dev_priv->ips.last_count2 = count;
-	dev_priv->ips.last_time2 = now;
+	pctx_paddr = dev_priv->mm.stolen_base + pctx->stolen->start;
+	I915_WRITE(VLV_PCBR, pctx_paddr);
 
-	/* More magic constants... */
-	diff = diff * 1181;
-	diff = div_u64(diff, diffms * 10);
-	dev_priv->ips.gfx_power = diff;
+out:
+	DRM_DEBUG_DRIVER("PCBR: 0x%08x\n", I915_READ(VLV_PCBR));
+	dev_priv->vlv_pctx = pctx;
 }
 
-void i915_update_gfx_val(struct drm_i915_private *dev_priv)
+static void valleyview_cleanup_pctx(struct drm_device *dev)
 {
-	struct drm_device *dev = dev_priv->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	if (INTEL_INFO(dev)->gen != 5)
+	if (WARN_ON(!dev_priv->vlv_pctx))
 		return;
 
-	spin_lock_irq(&mchdev_lock);
-
-	__i915_update_gfx_val(dev_priv);
-
-	spin_unlock_irq(&mchdev_lock);
+	drm_gem_object_unreference(&dev_priv->vlv_pctx->base);
+	dev_priv->vlv_pctx = NULL;
 }
 
-static unsigned long __i915_gfx_val(struct drm_i915_private *dev_priv)
+static void valleyview_init_gt_powersave(struct drm_device *dev)
 {
-	unsigned long t, corr, state1, corr2, state2;
-	u32 pxvid, ext_v;
-
-	assert_spin_locked(&mchdev_lock);
-
-	pxvid = I915_READ(PXVFREQ_BASE + (dev_priv->rps.cur_freq * 4));
-	pxvid = (pxvid >> 24) & 0x7f;
-	ext_v = pvid_to_extvid(dev_priv, pxvid);
-
-	state1 = ext_v;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	u32 val;
 
-	t = i915_mch_val(dev_priv);
+	valleyview_setup_pctx(dev);
 
-	/* Revel in the empirically derived constants */
+	mutex_lock(&dev_priv->rps.hw_lock);
 
-	/* Correction factor in 1/100000 units */
-	if (t > 80)
-		corr = ((t * 2349) + 135940);
-	else if (t >= 50)
-		corr = ((t * 964) + 29317);
-	else /* < 50 */
-		corr = ((t * 301) + 1004);
+	val = vlv_punit_read(dev_priv, PUNIT_REG_GPU_FREQ_STS);
+	switch ((val >> 6) & 3) {
+	case 0:
+	case 1:
+		dev_priv->mem_freq = 800;
+		break;
+	case 2:
+		dev_priv->mem_freq = 1066;
+		break;
+	case 3:
+		dev_priv->mem_freq = 1333;
+		break;
+	}
+	DRM_DEBUG_DRIVER("DDR speed: %d MHz\n", dev_priv->mem_freq);
 
-	corr = corr * ((150142 * state1) / 10000 - 78642);
-	corr /= 100000;
-	corr2 = (corr * dev_priv->ips.corr);
+	dev_priv->rps.max_freq = valleyview_rps_max_freq(dev_priv);
+	dev_priv->rps.rp0_freq = dev_priv->rps.max_freq;
+	DRM_DEBUG_DRIVER("max GPU freq: %d MHz (%u)\n",
+			 vlv_gpu_freq(dev_priv, dev_priv->rps.max_freq),
+			 dev_priv->rps.max_freq);
 
-	state2 = (corr2 * state1) / 10000;
-	state2 /= 100; /* convert to mW */
+	dev_priv->rps.efficient_freq = valleyview_rps_rpe_freq(dev_priv);
+	DRM_DEBUG_DRIVER("RPe GPU freq: %d MHz (%u)\n",
+			 vlv_gpu_freq(dev_priv, dev_priv->rps.efficient_freq),
+			 dev_priv->rps.efficient_freq);
 
-	__i915_update_gfx_val(dev_priv);
+	dev_priv->rps.rp1_freq = valleyview_rps_guar_freq(dev_priv);
+	DRM_DEBUG_DRIVER("RP1(Guar Freq) GPU freq: %d MHz (%u)\n",
+			 vlv_gpu_freq(dev_priv, dev_priv->rps.rp1_freq),
+			 dev_priv->rps.rp1_freq);
 
-	return dev_priv->ips.gfx_power + state2;
-}
+	dev_priv->rps.min_freq = valleyview_rps_min_freq(dev_priv);
+	DRM_DEBUG_DRIVER("min GPU freq: %d MHz (%u)\n",
+			 vlv_gpu_freq(dev_priv, dev_priv->rps.min_freq),
+			 dev_priv->rps.min_freq);
 
-unsigned long i915_gfx_val(struct drm_i915_private *dev_priv)
-{
-	struct drm_device *dev = dev_priv->dev;
-	unsigned long val;
+	/* Preserve min/max settings in case of re-init */
+	if (dev_priv->rps.max_freq_softlimit == 0)
+		dev_priv->rps.max_freq_softlimit = dev_priv->rps.max_freq;
 
-	if (INTEL_INFO(dev)->gen != 5)
-		return 0;
+	if (dev_priv->rps.min_freq_softlimit == 0)
+		dev_priv->rps.min_freq_softlimit = dev_priv->rps.min_freq;
 
-	spin_lock_irq(&mchdev_lock);
+	mutex_unlock(&dev_priv->rps.hw_lock);
+}
 
-	val = __i915_gfx_val(dev_priv);
+static void cherryview_init_gt_powersave(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	u32 val;
 
-	spin_unlock_irq(&mchdev_lock);
+	cherryview_setup_pctx(dev);
 
-	return val;
-}
+	mutex_lock(&dev_priv->rps.hw_lock);
 
-/**
- * i915_read_mch_val - return value for IPS use
- *
- * Calculate and return a value for the IPS driver to use when deciding whether
- * we have thermal and power headroom to increase CPU or GPU power budget.
- */
-unsigned long i915_read_mch_val(void)
-{
-	struct drm_i915_private *dev_priv;
-	unsigned long chipset_val, graphics_val, ret = 0;
+	mutex_lock(&dev_priv->dpio_lock);
+	val = vlv_cck_read(dev_priv, CCK_FUSE_REG);
+	mutex_unlock(&dev_priv->dpio_lock);
 
-	spin_lock_irq(&mchdev_lock);
-	if (!i915_mch_dev)
-		goto out_unlock;
-	dev_priv = i915_mch_dev;
+	switch ((val >> 2) & 0x7) {
+	case 0:
+	case 1:
+		dev_priv->rps.cz_freq = 200;
+		dev_priv->mem_freq = 1600;
+		break;
+	case 2:
+		dev_priv->rps.cz_freq = 267;
+		dev_priv->mem_freq = 1600;
+		break;
+	case 3:
+		dev_priv->rps.cz_freq = 333;
+		dev_priv->mem_freq = 2000;
+		break;
+	case 4:
+		dev_priv->rps.cz_freq = 320;
+		dev_priv->mem_freq = 1600;
+		break;
+	case 5:
+		dev_priv->rps.cz_freq = 400;
+		dev_priv->mem_freq = 1600;
+		break;
+	}
+	DRM_DEBUG_DRIVER("DDR speed: %d MHz\n", dev_priv->mem_freq);
 
-	chipset_val = __i915_chipset_val(dev_priv);
-	graphics_val = __i915_gfx_val(dev_priv);
+	dev_priv->rps.max_freq = cherryview_rps_max_freq(dev_priv);
+	dev_priv->rps.rp0_freq = dev_priv->rps.max_freq;
+	DRM_DEBUG_DRIVER("max GPU freq: %d MHz (%u)\n",
+			 vlv_gpu_freq(dev_priv, dev_priv->rps.max_freq),
+			 dev_priv->rps.max_freq);
 
-	ret = chipset_val + graphics_val;
+	dev_priv->rps.efficient_freq = cherryview_rps_rpe_freq(dev_priv);
+	DRM_DEBUG_DRIVER("RPe GPU freq: %d MHz (%u)\n",
+			 vlv_gpu_freq(dev_priv, dev_priv->rps.efficient_freq),
+			 dev_priv->rps.efficient_freq);
 
-out_unlock:
-	spin_unlock_irq(&mchdev_lock);
+	dev_priv->rps.rp1_freq = cherryview_rps_guar_freq(dev_priv);
+	DRM_DEBUG_DRIVER("RP1(Guar) GPU freq: %d MHz (%u)\n",
+			 vlv_gpu_freq(dev_priv, dev_priv->rps.rp1_freq),
+			 dev_priv->rps.rp1_freq);
 
-	return ret;
-}
-EXPORT_SYMBOL_GPL(i915_read_mch_val);
+	dev_priv->rps.min_freq = cherryview_rps_min_freq(dev_priv);
+	DRM_DEBUG_DRIVER("min GPU freq: %d MHz (%u)\n",
+			 vlv_gpu_freq(dev_priv, dev_priv->rps.min_freq),
+			 dev_priv->rps.min_freq);
 
-/**
- * i915_gpu_raise - raise GPU frequency limit
- *
- * Raise the limit; IPS indicates we have thermal headroom.
- */
-bool i915_gpu_raise(void)
-{
-	struct drm_i915_private *dev_priv;
-	bool ret = true;
+	WARN_ONCE((dev_priv->rps.max_freq |
+		   dev_priv->rps.efficient_freq |
+		   dev_priv->rps.rp1_freq |
+		   dev_priv->rps.min_freq) & 1,
+		  "Odd GPU freq values\n");
 
-	spin_lock_irq(&mchdev_lock);
-	if (!i915_mch_dev) {
-		ret = false;
-		goto out_unlock;
-	}
-	dev_priv = i915_mch_dev;
+	/* Preserve min/max settings in case of re-init */
+	if (dev_priv->rps.max_freq_softlimit == 0)
+		dev_priv->rps.max_freq_softlimit = dev_priv->rps.max_freq;
 
-	if (dev_priv->ips.max_delay > dev_priv->ips.fmax)
-		dev_priv->ips.max_delay--;
+	if (dev_priv->rps.min_freq_softlimit == 0)
+		dev_priv->rps.min_freq_softlimit = dev_priv->rps.min_freq;
 
-out_unlock:
-	spin_unlock_irq(&mchdev_lock);
+	mutex_unlock(&dev_priv->rps.hw_lock);
+}
 
-	return ret;
+static void valleyview_cleanup_gt_powersave(struct drm_device *dev)
+{
+	valleyview_cleanup_pctx(dev);
 }
-EXPORT_SYMBOL_GPL(i915_gpu_raise);
 
-/**
- * i915_gpu_lower - lower GPU frequency limit
- *
- * IPS indicates we're close to a thermal limit, so throttle back the GPU
- * frequency maximum.
- */
-bool i915_gpu_lower(void)
+static void cherryview_enable_rps(struct drm_device *dev)
 {
-	struct drm_i915_private *dev_priv;
-	bool ret = true;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_engine_cs *engine;
+	u32 gtfifodbg, val, rc6_mode = 0, pcbr;
+	int i;
 
-	spin_lock_irq(&mchdev_lock);
-	if (!i915_mch_dev) {
-		ret = false;
-		goto out_unlock;
+	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
+
+	gtfifodbg = I915_READ(GTFIFODBG);
+	if (gtfifodbg) {
+		DRM_DEBUG_DRIVER("GT fifo had a previous error %x\n",
+				 gtfifodbg);
+		I915_WRITE(GTFIFODBG, gtfifodbg);
 	}
-	dev_priv = i915_mch_dev;
 
-	if (dev_priv->ips.max_delay < dev_priv->ips.min_delay)
-		dev_priv->ips.max_delay++;
+	cherryview_check_pctx(dev_priv);
 
-out_unlock:
-	spin_unlock_irq(&mchdev_lock);
+	/* 1a & 1b: Get forcewake during program sequence. Although the driver
+	 * hasn't enabled a state yet where we need forcewake, BIOS may have.*/
+	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
 
-	return ret;
-}
-EXPORT_SYMBOL_GPL(i915_gpu_lower);
+	/* 2a: Program RC6 thresholds.*/
+	I915_WRITE(GEN6_RC6_WAKE_RATE_LIMIT, 40 << 16);
+	I915_WRITE(GEN6_RC_EVALUATION_INTERVAL, 125000); /* 12500 * 1280ns */
+	I915_WRITE(GEN6_RC_IDLE_HYSTERSIS, 25); /* 25 * 1280ns */
 
-/**
- * i915_gpu_busy - indicate GPU business to IPS
- *
- * Tell the IPS driver whether or not the GPU is busy.
- */
-bool i915_gpu_busy(void)
-{
-	struct drm_i915_private *dev_priv;
-	struct intel_engine_cs *ring;
-	bool ret = false;
-	int i;
+	for_each_engine(engine, dev_priv, i)
+		I915_WRITE(RING_MAX_IDLE(engine->mmio_base), 10);
+	I915_WRITE(GEN6_RC_SLEEP, 0);
 
-	spin_lock_irq(&mchdev_lock);
-	if (!i915_mch_dev)
-		goto out_unlock;
-	dev_priv = i915_mch_dev;
+	I915_WRITE(GEN6_RC6_THRESHOLD, 50000); /* 50/125ms per EI */
 
-	for_each_ring(ring, dev_priv, i)
-		ret |= !list_empty(&ring->request_list);
+	/* allows RC6 residency counter to work */
+	I915_WRITE(VLV_COUNTER_CONTROL,
+		   _MASKED_BIT_ENABLE(VLV_COUNT_RANGE_HIGH |
+				      VLV_MEDIA_RC6_COUNT_EN |
+				      VLV_RENDER_RC6_COUNT_EN));
 
-out_unlock:
-	spin_unlock_irq(&mchdev_lock);
+	/* For now we assume BIOS is allocating and populating the PCBR  */
+	pcbr = I915_READ(VLV_PCBR);
 
-	return ret;
-}
-EXPORT_SYMBOL_GPL(i915_gpu_busy);
+	/* 3: Enable RC6 */
+	if ((intel_enable_rc6(dev) & INTEL_RC6_ENABLE) &&
+						(pcbr >> VLV_PCBR_ADDR_SHIFT))
+		rc6_mode = GEN6_RC_CTL_EI_MODE(1);
 
-/**
- * i915_gpu_turbo_disable - disable graphics turbo
- *
- * Disable graphics turbo by resetting the max frequency and setting the
- * current frequency to the default.
- */
-bool i915_gpu_turbo_disable(void)
-{
-	struct drm_i915_private *dev_priv;
-	bool ret = true;
+	I915_WRITE(GEN6_RC_CONTROL, rc6_mode);
 
-	spin_lock_irq(&mchdev_lock);
-	if (!i915_mch_dev) {
-		ret = false;
-		goto out_unlock;
-	}
-	dev_priv = i915_mch_dev;
+	/* 4 Program defaults and thresholds for RPS*/
+	I915_WRITE(GEN6_RP_UP_THRESHOLD, 59400);
+	I915_WRITE(GEN6_RP_DOWN_THRESHOLD, 245000);
+	I915_WRITE(GEN6_RP_UP_EI, 66000);
+	I915_WRITE(GEN6_RP_DOWN_EI, 350000);
 
-	dev_priv->ips.max_delay = dev_priv->ips.fstart;
+	I915_WRITE(GEN6_RP_IDLE_HYSTERSIS, 10);
 
-	if (!ironlake_set_drps(dev_priv->dev, dev_priv->ips.fstart))
-		ret = false;
+	/* WaDisablePwrmtrEvent:chv (pre-production hw) */
+	I915_WRITE(0xA80C, I915_READ(0xA80C) & 0x00ffffff);
+	I915_WRITE(0xA810, I915_READ(0xA810) & 0xffffff00);
 
-out_unlock:
-	spin_unlock_irq(&mchdev_lock);
+	/* 5: Enable RPS */
+	I915_WRITE(GEN6_RP_CONTROL,
+		   GEN6_RP_MEDIA_HW_NORMAL_MODE |
+		   GEN6_RP_MEDIA_IS_GFX | /* WaSetMaskForGfxBusyness:chv (pre-production hw ?) */
+		   GEN6_RP_ENABLE |
+		   GEN6_RP_UP_BUSY_AVG |
+		   GEN6_RP_DOWN_IDLE_AVG);
 
-	return ret;
-}
-EXPORT_SYMBOL_GPL(i915_gpu_turbo_disable);
+	val = vlv_punit_read(dev_priv, PUNIT_REG_GPU_FREQ_STS);
 
-/**
- * Tells the intel_ips driver that the i915 driver is now loaded, if
- * IPS got loaded first.
- *
- * This awkward dance is so that neither module has to depend on the
- * other in order for IPS to do the appropriate communication of
- * GPU turbo limits to i915.
- */
-static void
-ips_ping_for_i915_load(void)
-{
-	void (*link)(void);
+	/* RPS code assumes GPLL is used */
+	WARN_ONCE((val & GPLLENABLE) == 0, "GPLL not enabled\n");
 
-	link = symbol_get(ips_link_to_i915_driver);
-	if (link) {
-		link();
-		symbol_put(ips_link_to_i915_driver);
-	}
-}
+	DRM_DEBUG_DRIVER("GPLL enabled? %s\n", val & GPLLENABLE ? "yes" : "no");
+	DRM_DEBUG_DRIVER("GPU status: 0x%08x\n", val);
 
-void intel_gpu_ips_init(struct drm_i915_private *dev_priv)
-{
-	/* We only register the i915 ips part with intel-ips once everything is
-	 * set up, to avoid intel-ips sneaking in and reading bogus values. */
-	spin_lock_irq(&mchdev_lock);
-	i915_mch_dev = dev_priv;
-	spin_unlock_irq(&mchdev_lock);
+	dev_priv->rps.cur_freq = (val >> 8) & 0xff;
+	DRM_DEBUG_DRIVER("current GPU freq: %d MHz (%u)\n",
+			 vlv_gpu_freq(dev_priv, dev_priv->rps.cur_freq),
+			 dev_priv->rps.cur_freq);
 
-	ips_ping_for_i915_load();
-}
+	DRM_DEBUG_DRIVER("setting GPU freq to %d MHz (%u)\n",
+			 vlv_gpu_freq(dev_priv, dev_priv->rps.efficient_freq),
+			 dev_priv->rps.efficient_freq);
 
-void intel_gpu_ips_teardown(void)
-{
-	spin_lock_irq(&mchdev_lock);
-	i915_mch_dev = NULL;
-	spin_unlock_irq(&mchdev_lock);
+	valleyview_set_rps(dev_priv->dev, dev_priv->rps.efficient_freq);
+
+	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
 }
 
-static void intel_init_emon(struct drm_device *dev)
+static void valleyview_enable_rps(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	u32 lcfuse;
-	u8 pxw[16];
+	struct intel_engine_cs *engine;
+	u32 gtfifodbg, val, rc6_mode = 0;
 	int i;
 
-	/* Disable to program */
-	I915_WRITE(ECR, 0);
-	POSTING_READ(ECR);
+	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
 
-	/* Program energy weights for various events */
-	I915_WRITE(SDEW, 0x15040d00);
-	I915_WRITE(CSIEW0, 0x007f0000);
-	I915_WRITE(CSIEW1, 0x1e220004);
-	I915_WRITE(CSIEW2, 0x04000004);
+	valleyview_check_pctx(dev_priv);
 
-	for (i = 0; i < 5; i++)
-		I915_WRITE(PEW + (i * 4), 0);
-	for (i = 0; i < 3; i++)
-		I915_WRITE(DEW + (i * 4), 0);
+	if ((gtfifodbg = I915_READ(GTFIFODBG))) {
+		DRM_DEBUG_DRIVER("GT fifo had a previous error %x\n",
+				 gtfifodbg);
+		I915_WRITE(GTFIFODBG, gtfifodbg);
+	}
 
-	/* Program P-state weights to account for frequency power adjustment */
-	for (i = 0; i < 16; i++) {
-		u32 pxvidfreq = I915_READ(PXVFREQ_BASE + (i * 4));
-		unsigned long freq = intel_pxfreq(pxvidfreq);
-		unsigned long vid = (pxvidfreq & PXVFREQ_PX_MASK) >>
-			PXVFREQ_PX_SHIFT;
-		unsigned long val;
+	/* If VLV, Forcewake all wells, else re-direct to regular path */
+	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
 
-		val = vid * vid;
-		val *= (freq / 1000);
-		val *= 255;
-		val /= (127*127*900);
-		if (val > 0xff)
-			DRM_ERROR("bad pxval: %ld\n", val);
-		pxw[i] = val;
-	}
-	/* Render standby states get 0 weight */
-	pxw[14] = 0;
-	pxw[15] = 0;
+	I915_WRITE(GEN6_RP_UP_THRESHOLD, 59400);
+	I915_WRITE(GEN6_RP_DOWN_THRESHOLD, 245000);
+	I915_WRITE(GEN6_RP_UP_EI, 66000);
+	I915_WRITE(GEN6_RP_DOWN_EI, 350000);
 
-	for (i = 0; i < 4; i++) {
-		u32 val = (pxw[i*4] << 24) | (pxw[(i*4)+1] << 16) |
-			(pxw[(i*4)+2] << 8) | (pxw[(i*4)+3]);
-		I915_WRITE(PXW + (i * 4), val);
-	}
+	I915_WRITE(GEN6_RP_IDLE_HYSTERSIS, 10);
+	I915_WRITE(GEN6_RP_DOWN_TIMEOUT, 0xf4240);
 
-	/* Adjust magic regs to magic values (more experimental results) */
-	I915_WRITE(OGW0, 0);
-	I915_WRITE(OGW1, 0);
-	I915_WRITE(EG0, 0x00007f00);
-	I915_WRITE(EG1, 0x0000000e);
-	I915_WRITE(EG2, 0x000e0000);
-	I915_WRITE(EG3, 0x68000300);
-	I915_WRITE(EG4, 0x42000000);
-	I915_WRITE(EG5, 0x00140031);
-	I915_WRITE(EG6, 0);
-	I915_WRITE(EG7, 0);
+	I915_WRITE(GEN6_RP_CONTROL,
+		   GEN6_RP_MEDIA_TURBO |
+		   GEN6_RP_MEDIA_HW_NORMAL_MODE |
+		   GEN6_RP_MEDIA_IS_GFX |
+		   GEN6_RP_ENABLE |
+		   GEN6_RP_UP_BUSY_AVG |
+		   GEN6_RP_DOWN_IDLE_CONT);
 
-	for (i = 0; i < 8; i++)
-		I915_WRITE(PXWL + (i * 4), 0);
+	I915_WRITE(GEN6_RC6_WAKE_RATE_LIMIT, 0x00280000);
+	I915_WRITE(GEN6_RC_EVALUATION_INTERVAL, 125000);
+	I915_WRITE(GEN6_RC_IDLE_HYSTERSIS, 25);
 
-	/* Enable PMON + select events */
-	I915_WRITE(ECR, 0x80000019);
+	for_each_engine(engine, dev_priv, i)
+		I915_WRITE(RING_MAX_IDLE(engine->mmio_base), 10);
 
-	lcfuse = I915_READ(LCFUSE02);
+	I915_WRITE(GEN6_RC6_THRESHOLD, 0x557);
 
-	dev_priv->ips.corr = (lcfuse & LCFUSE_HIV_MASK);
-}
+	/* allows RC6 residency counter to work */
+	I915_WRITE(VLV_COUNTER_CONTROL,
+		   _MASKED_BIT_ENABLE(VLV_MEDIA_RC0_COUNT_EN |
+				      VLV_RENDER_RC0_COUNT_EN |
+				      VLV_MEDIA_RC6_COUNT_EN |
+				      VLV_RENDER_RC6_COUNT_EN));
 
-void intel_init_gt_powersave(struct drm_device *dev)
-{
-	i915.enable_rc6 = sanitize_rc6_option(dev, i915.enable_rc6);
+	if (intel_enable_rc6(dev) & INTEL_RC6_ENABLE)
+		rc6_mode = GEN7_RC_CTL_TO_MODE | VLV_RC_CTL_CTX_RST_PARALLEL;
 
-	if (IS_CHERRYVIEW(dev))
-		cherryview_init_gt_powersave(dev);
-	else if (IS_VALLEYVIEW(dev))
-		valleyview_init_gt_powersave(dev);
-}
+	intel_print_rc6_info(dev, rc6_mode);
 
-void intel_cleanup_gt_powersave(struct drm_device *dev)
-{
-	if (IS_CHERRYVIEW(dev))
-		return;
-	else if (IS_VALLEYVIEW(dev))
-		valleyview_cleanup_gt_powersave(dev);
-}
+	I915_WRITE(GEN6_RC_CONTROL, rc6_mode);
 
-/**
- * intel_suspend_gt_powersave - suspend PM work and helper threads
- * @dev: drm device
- *
- * We don't want to disable RC6 or other features here, we just want
- * to make sure any work we've queued has finished and won't bother
- * us while we're suspended.
- */
-void intel_suspend_gt_powersave(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	val = vlv_punit_read(dev_priv, PUNIT_REG_GPU_FREQ_STS);
 
-	/* Interrupts should be disabled already to avoid re-arming. */
-	WARN_ON(intel_irqs_enabled(dev_priv));
+	/* RPS code assumes GPLL is used */
+	WARN_ONCE((val & GPLLENABLE) == 0, "GPLL not enabled\n");
 
-	flush_delayed_work(&dev_priv->rps.delayed_resume_work);
+	DRM_DEBUG_DRIVER("GPLL enabled? %s\n", val & GPLLENABLE ? "yes" : "no");
+	DRM_DEBUG_DRIVER("GPU status: 0x%08x\n", val);
 
-	cancel_work_sync(&dev_priv->rps.work);
+	dev_priv->rps.cur_freq = (val >> 8) & 0xff;
+	DRM_DEBUG_DRIVER("current GPU freq: %d MHz (%u)\n",
+			 vlv_gpu_freq(dev_priv, dev_priv->rps.cur_freq),
+			 dev_priv->rps.cur_freq);
 
-	/* Force GPU to min freq during suspend */
-	gen6_rps_idle(dev_priv);
+	DRM_DEBUG_DRIVER("setting GPU freq to %d MHz (%u)\n",
+			 vlv_gpu_freq(dev_priv, dev_priv->rps.efficient_freq),
+			 dev_priv->rps.efficient_freq);
+
+	valleyview_set_rps(dev_priv->dev, dev_priv->rps.efficient_freq);
+
+	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
 }
 
-void intel_disable_gt_powersave(struct drm_device *dev)
+void ironlake_teardown_rc6(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	/* Interrupts should be disabled already to avoid re-arming. */
-	WARN_ON(intel_irqs_enabled(dev_priv));
-
-	if (IS_IRONLAKE_M(dev)) {
-		ironlake_disable_drps(dev);
-		ironlake_disable_rc6(dev);
-	} else if (INTEL_INFO(dev)->gen >= 6) {
-		intel_suspend_gt_powersave(dev);
-
-		mutex_lock(&dev_priv->rps.hw_lock);
-		if (IS_CHERRYVIEW(dev))
-			cherryview_disable_rps(dev);
-		else if (IS_VALLEYVIEW(dev))
-			valleyview_disable_rps(dev);
-		else
-			gen6_disable_rps(dev);
-		dev_priv->rps.enabled = false;
-		mutex_unlock(&dev_priv->rps.hw_lock);
+	if (dev_priv->ips.pwrctx) {
+		i915_gem_object_ggtt_unpin(dev_priv->ips.pwrctx);
+		drm_gem_object_unreference(&dev_priv->ips.pwrctx->base);
+		dev_priv->ips.pwrctx = NULL;
 	}
 }
 
-static void intel_gen6_powersave_work(struct work_struct *work)
+static void ironlake_disable_rc6(struct drm_device *dev)
 {
-	struct drm_i915_private *dev_priv =
-		container_of(work, struct drm_i915_private,
-			     rps.delayed_resume_work.work);
-	struct drm_device *dev = dev_priv->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	mutex_lock(&dev_priv->rps.hw_lock);
+	if (I915_READ(PWRCTXA)) {
+		/* Wake the GPU, prevent RC6, then restore RSTDBYCTL */
+		I915_WRITE(RSTDBYCTL, I915_READ(RSTDBYCTL) | RCX_SW_EXIT);
+		wait_for(((I915_READ(RSTDBYCTL) & RSX_STATUS_MASK) == RSX_STATUS_ON),
+			 50);
 
-	if (IS_CHERRYVIEW(dev)) {
-		cherryview_enable_rps(dev);
-	} else if (IS_VALLEYVIEW(dev)) {
-		valleyview_enable_rps(dev);
-	} else if (IS_BROADWELL(dev)) {
-		gen8_enable_rps(dev);
-		__gen6_update_ring_freq(dev);
-	} else {
-		gen6_enable_rps(dev);
-		__gen6_update_ring_freq(dev);
-	}
-	dev_priv->rps.enabled = true;
-	mutex_unlock(&dev_priv->rps.hw_lock);
+		I915_WRITE(PWRCTXA, 0);
+		POSTING_READ(PWRCTXA);
 
-	intel_runtime_pm_put(dev_priv);
+		I915_WRITE(RSTDBYCTL, I915_READ(RSTDBYCTL) & ~RCX_SW_EXIT);
+		POSTING_READ(RSTDBYCTL);
+	}
 }
 
-void intel_enable_gt_powersave(struct drm_device *dev)
+static int ironlake_setup_rc6(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	if (IS_IRONLAKE_M(dev)) {
-		mutex_lock(&dev->struct_mutex);
-		ironlake_enable_drps(dev);
-		ironlake_enable_rc6(dev);
-		intel_init_emon(dev);
-		mutex_unlock(&dev->struct_mutex);
-	} else if (INTEL_INFO(dev)->gen >= 6) {
-		/*
-		 * PCU communication is slow and this doesn't need to be
-		 * done at any specific time, so do this out of our fast path
-		 * to make resume and init faster.
-		 *
-		 * We depend on the HW RC6 power context save/restore
-		 * mechanism when entering D3 through runtime PM suspend. So
-		 * disable RPM until RPS/RC6 is properly setup. We can only
-		 * get here via the driver load/system resume/runtime resume
-		 * paths, so the _noresume version is enough (and in case of
-		 * runtime resume it's necessary).
-		 */
-		if (schedule_delayed_work(&dev_priv->rps.delayed_resume_work,
-					   round_jiffies_up_relative(HZ)))
-			intel_runtime_pm_get_noresume(dev_priv);
+	if (dev_priv->ips.pwrctx == NULL)
+		dev_priv->ips.pwrctx = intel_alloc_context_page(dev);
+	if (!dev_priv->ips.pwrctx) {
+		ironlake_teardown_rc6(dev);
+		return -ENOMEM;
 	}
-}
 
-void intel_reset_gt_powersave(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-
-	dev_priv->rps.enabled = false;
-	intel_enable_gt_powersave(dev);
+	return 0;
 }
 
-static void ibx_init_clock_gating(struct drm_device *dev)
+static void ironlake_enable_rc6(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	/*
-	 * On Ibex Peak and Cougar Point, we need to disable clock
-	 * gating for the panel power sequencer or it will fail to
-	 * start up when no ports are active.
+	/* rc6 disabled by default due to repeated reports of hanging during
+	 * boot and resume.
 	 */
-	I915_WRITE(SOUTH_DSPCLK_GATE_D, PCH_DPLSUNIT_CLOCK_GATE_DISABLE);
-}
+	if (!intel_enable_rc6(dev))
+		return;
 
-static void g4x_disable_trickle_feed(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	int pipe;
+	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
 
-	for_each_pipe(pipe) {
-		I915_WRITE(DSPCNTR(pipe),
-			   I915_READ(DSPCNTR(pipe)) |
-			   DISPPLANE_TRICKLE_FEED_DISABLE);
-		intel_flush_primary_plane(dev_priv, pipe);
+	if (ironlake_setup_rc6(dev)) {
+		DRM_ERROR("failed to enable ironlake power savings\n");
+		return;
 	}
-}
-
-static void ilk_init_lp_watermarks(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	I915_WRITE(WM3_LP_ILK, I915_READ(WM3_LP_ILK) & ~WM1_LP_SR_EN);
-	I915_WRITE(WM2_LP_ILK, I915_READ(WM2_LP_ILK) & ~WM1_LP_SR_EN);
-	I915_WRITE(WM1_LP_ILK, I915_READ(WM1_LP_ILK) & ~WM1_LP_SR_EN);
+	I915_WRITE(PWRCTXA, i915_gem_obj_ggtt_offset(dev_priv->ips.pwrctx) | PWRCTX_EN);
+	I915_WRITE(RSTDBYCTL, I915_READ(RSTDBYCTL) & ~RCX_SW_EXIT);
 
-	/*
-	 * Don't touch WM1S_LP_EN here.
-	 * Doing so could cause underruns.
-	 */
+	intel_print_rc6_info(dev, GEN6_RC_CTL_RC6_ENABLE);
 }
 
-static void ironlake_init_clock_gating(struct drm_device *dev)
+static unsigned long intel_pxfreq(u32 vidfreq)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	uint32_t dspclk_gate = ILK_VRHUNIT_CLOCK_GATE_DISABLE;
-
-	/*
-	 * Required for FBC
-	 * WaFbcDisableDpfcClockGating:ilk
-	 */
-	dspclk_gate |= ILK_DPFCRUNIT_CLOCK_GATE_DISABLE |
-		   ILK_DPFCUNIT_CLOCK_GATE_DISABLE |
-		   ILK_DPFDUNIT_CLOCK_GATE_ENABLE;
-
-	I915_WRITE(PCH_3DCGDIS0,
-		   MARIUNIT_CLOCK_GATE_DISABLE |
-		   SVSMUNIT_CLOCK_GATE_DISABLE);
-	I915_WRITE(PCH_3DCGDIS1,
-		   VFMUNIT_CLOCK_GATE_DISABLE);
+	unsigned long freq;
+	int div = (vidfreq & 0x3f0000) >> 16;
+	int post = (vidfreq & 0x3000) >> 12;
+	int pre = (vidfreq & 0x7);
 
-	/*
-	 * According to the spec the following bits should be set in
-	 * order to enable memory self-refresh
-	 * The bit 22/21 of 0x42004
-	 * The bit 5 of 0x42020
-	 * The bit 15 of 0x45000
-	 */
-	I915_WRITE(ILK_DISPLAY_CHICKEN2,
-		   (I915_READ(ILK_DISPLAY_CHICKEN2) |
-		    ILK_DPARB_GATE | ILK_VSDPFD_FULL));
-	dspclk_gate |= ILK_DPARBUNIT_CLOCK_GATE_ENABLE;
-	I915_WRITE(DISP_ARB_CTL,
-		   (I915_READ(DISP_ARB_CTL) |
-		    DISP_FBC_WM_DIS));
+	if (!pre)
+		return 0;
 
-	ilk_init_lp_watermarks(dev);
+	freq = ((div * 133333) / ((1<<post) * pre));
 
-	/*
-	 * Based on the document from hardware guys the following bits
-	 * should be set unconditionally in order to enable FBC.
-	 * The bit 22 of 0x42000
-	 * The bit 22 of 0x42004
-	 * The bit 7,8,9 of 0x42020.
-	 */
-	if (IS_IRONLAKE_M(dev)) {
-		/* WaFbcAsynchFlipDisableFbcQueue:ilk */
-		I915_WRITE(ILK_DISPLAY_CHICKEN1,
-			   I915_READ(ILK_DISPLAY_CHICKEN1) |
-			   ILK_FBCQ_DIS);
-		I915_WRITE(ILK_DISPLAY_CHICKEN2,
-			   I915_READ(ILK_DISPLAY_CHICKEN2) |
-			   ILK_DPARB_GATE);
-	}
+	return freq;
+}
 
-	I915_WRITE(ILK_DSPCLK_GATE_D, dspclk_gate);
+static const struct cparams {
+	u16 i;
+	u16 t;
+	u16 m;
+	u16 c;
+} cparams[] = {
+	{ 1, 1333, 301, 28664 },
+	{ 1, 1066, 294, 24460 },
+	{ 1, 800, 294, 25192 },
+	{ 0, 1333, 276, 27605 },
+	{ 0, 1066, 276, 27605 },
+	{ 0, 800, 231, 23784 },
+};
 
-	I915_WRITE(ILK_DISPLAY_CHICKEN2,
-		   I915_READ(ILK_DISPLAY_CHICKEN2) |
-		   ILK_ELPIN_409_SELECT);
-	I915_WRITE(_3D_CHICKEN2,
-		   _3D_CHICKEN2_WM_READ_PIPELINED << 16 |
-		   _3D_CHICKEN2_WM_READ_PIPELINED);
+static unsigned long __i915_chipset_val(struct drm_i915_private *dev_priv)
+{
+	u64 total_count, diff, ret;
+	u32 count1, count2, count3, m = 0, c = 0;
+	unsigned long now = jiffies_to_msecs(jiffies), diff1;
+	int i;
 
-	/* WaDisableRenderCachePipelinedFlush:ilk */
-	I915_WRITE(CACHE_MODE_0,
-		   _MASKED_BIT_ENABLE(CM0_PIPELINED_RENDER_FLUSH_DISABLE));
+	assert_spin_locked(&mchdev_lock);
 
-	/* WaDisable_RenderCache_OperationalFlush:ilk */
-	I915_WRITE(CACHE_MODE_0, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
+	diff1 = now - dev_priv->ips.last_time1;
 
-	g4x_disable_trickle_feed(dev);
+	/* Prevent division-by-zero if we are asking too fast.
+	 * Also, we don't get interesting results if we are polling
+	 * faster than once in 10ms, so just return the saved value
+	 * in such cases.
+	 */
+	if (diff1 <= 10)
+		return dev_priv->ips.chipset_power;
 
-	ibx_init_clock_gating(dev);
-}
+	count1 = I915_READ(DMIEC);
+	count2 = I915_READ(DDREC);
+	count3 = I915_READ(CSIEC);
 
-static void cpt_init_clock_gating(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	int pipe;
-	uint32_t val;
+	total_count = count1 + count2 + count3;
 
-	/*
-	 * On Ibex Peak and Cougar Point, we need to disable clock
-	 * gating for the panel power sequencer or it will fail to
-	 * start up when no ports are active.
-	 */
-	I915_WRITE(SOUTH_DSPCLK_GATE_D, PCH_DPLSUNIT_CLOCK_GATE_DISABLE |
-		   PCH_DPLUNIT_CLOCK_GATE_DISABLE |
-		   PCH_CPUNIT_CLOCK_GATE_DISABLE);
-	I915_WRITE(SOUTH_CHICKEN2, I915_READ(SOUTH_CHICKEN2) |
-		   DPLS_EDP_PPS_FIX_DIS);
-	/* The below fixes the weird display corruption, a few pixels shifted
-	 * downward, on (only) LVDS of some HP laptops with IVY.
-	 */
-	for_each_pipe(pipe) {
-		val = I915_READ(TRANS_CHICKEN2(pipe));
-		val |= TRANS_CHICKEN2_TIMING_OVERRIDE;
-		val &= ~TRANS_CHICKEN2_FDI_POLARITY_REVERSED;
-		if (dev_priv->vbt.fdi_rx_polarity_inverted)
-			val |= TRANS_CHICKEN2_FDI_POLARITY_REVERSED;
-		val &= ~TRANS_CHICKEN2_FRAME_START_DELAY_MASK;
-		val &= ~TRANS_CHICKEN2_DISABLE_DEEP_COLOR_COUNTER;
-		val &= ~TRANS_CHICKEN2_DISABLE_DEEP_COLOR_MODESWITCH;
-		I915_WRITE(TRANS_CHICKEN2(pipe), val);
+	/* FIXME: handle per-counter overflow */
+	if (total_count < dev_priv->ips.last_count1) {
+		diff = ~0UL - dev_priv->ips.last_count1;
+		diff += total_count;
+	} else {
+		diff = total_count - dev_priv->ips.last_count1;
 	}
-	/* WADP0ClockGatingDisable */
-	for_each_pipe(pipe) {
-		I915_WRITE(TRANS_CHICKEN1(pipe),
-			   TRANS_CHICKEN1_DP0UNIT_GC_DISABLE);
+
+	for (i = 0; i < ARRAY_SIZE(cparams); i++) {
+		if (cparams[i].i == dev_priv->ips.c_m &&
+		    cparams[i].t == dev_priv->ips.r_t) {
+			m = cparams[i].m;
+			c = cparams[i].c;
+			break;
+		}
 	}
-}
 
-static void gen6_check_mch_setup(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	uint32_t tmp;
+	diff = div_u64(diff, diff1);
+	ret = ((m * diff) + c);
+	ret = div_u64(ret, 10);
 
-	tmp = I915_READ(MCH_SSKPD);
-	if ((tmp & MCH_SSKPD_WM0_MASK) != MCH_SSKPD_WM0_VAL)
-		DRM_DEBUG_KMS("Wrong MCH_SSKPD value: 0x%08x This can cause underruns.\n",
-			      tmp);
+	dev_priv->ips.last_count1 = total_count;
+	dev_priv->ips.last_time1 = now;
+
+	dev_priv->ips.chipset_power = ret;
+
+	return ret;
 }
 
-static void gen6_init_clock_gating(struct drm_device *dev)
+unsigned long i915_chipset_val(struct drm_i915_private *dev_priv)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	uint32_t dspclk_gate = ILK_VRHUNIT_CLOCK_GATE_DISABLE;
-
-	I915_WRITE(ILK_DSPCLK_GATE_D, dspclk_gate);
+	struct drm_device *dev = dev_priv->dev;
+	unsigned long val;
 
-	I915_WRITE(ILK_DISPLAY_CHICKEN2,
-		   I915_READ(ILK_DISPLAY_CHICKEN2) |
-		   ILK_ELPIN_409_SELECT);
+	if (INTEL_INFO(dev)->gen != 5)
+		return 0;
 
-	/* WaDisableHiZPlanesWhenMSAAEnabled:snb */
-	I915_WRITE(_3D_CHICKEN,
-		   _MASKED_BIT_ENABLE(_3D_CHICKEN_HIZ_PLANE_DISABLE_MSAA_4X_SNB));
+	spin_lock_irq(&mchdev_lock);
 
-	/* WaSetupGtModeTdRowDispatch:snb */
-	if (IS_SNB_GT1(dev))
-		I915_WRITE(GEN6_GT_MODE,
-			   _MASKED_BIT_ENABLE(GEN6_TD_FOUR_ROW_DISPATCH_DISABLE));
+	val = __i915_chipset_val(dev_priv);
 
-	/* WaDisable_RenderCache_OperationalFlush:snb */
-	I915_WRITE(CACHE_MODE_0, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
+	spin_unlock_irq(&mchdev_lock);
 
-	/*
-	 * BSpec recoomends 8x4 when MSAA is used,
-	 * however in practice 16x4 seems fastest.
-	 *
-	 * Note that PS/WM thread counts depend on the WIZ hashing
-	 * disable bit, which we don't touch here, but it's good
-	 * to keep in mind (see 3DSTATE_PS and 3DSTATE_WM).
-	 */
-	I915_WRITE(GEN6_GT_MODE,
-		   GEN6_WIZ_HASHING_MASK | GEN6_WIZ_HASHING_16x4);
+	return val;
+}
 
-	ilk_init_lp_watermarks(dev);
+unsigned long i915_mch_val(struct drm_i915_private *dev_priv)
+{
+	unsigned long m, x, b;
+	u32 tsfs;
 
-	I915_WRITE(CACHE_MODE_0,
-		   _MASKED_BIT_DISABLE(CM0_STC_EVICT_DISABLE_LRA_SNB));
+	tsfs = I915_READ(TSFS);
 
-	I915_WRITE(GEN6_UCGCTL1,
-		   I915_READ(GEN6_UCGCTL1) |
-		   GEN6_BLBUNIT_CLOCK_GATE_DISABLE |
-		   GEN6_CSUNIT_CLOCK_GATE_DISABLE);
+	m = ((tsfs & TSFS_SLOPE_MASK) >> TSFS_SLOPE_SHIFT);
+	x = I915_READ8(TR1);
 
-	/* According to the BSpec vol1g, bit 12 (RCPBUNIT) clock
-	 * gating disable must be set.  Failure to set it results in
-	 * flickering pixels due to Z write ordering failures after
-	 * some amount of runtime in the Mesa "fire" demo, and Unigine
-	 * Sanctuary and Tropics, and apparently anything else with
-	 * alpha test or pixel discard.
-	 *
-	 * According to the spec, bit 11 (RCCUNIT) must also be set,
-	 * but we didn't debug actual testcases to find it out.
-	 *
-	 * WaDisableRCCUnitClockGating:snb
-	 * WaDisableRCPBUnitClockGating:snb
-	 */
-	I915_WRITE(GEN6_UCGCTL2,
-		   GEN6_RCPBUNIT_CLOCK_GATE_DISABLE |
-		   GEN6_RCCUNIT_CLOCK_GATE_DISABLE);
+	b = tsfs & TSFS_INTR_MASK;
 
-	/* WaStripsFansDisableFastClipPerformanceFix:snb */
-	I915_WRITE(_3D_CHICKEN3,
-		   _MASKED_BIT_ENABLE(_3D_CHICKEN3_SF_DISABLE_FASTCLIP_CULL));
+	return ((m * x) / 127) - b;
+}
 
-	/*
-	 * Bspec says:
-	 * "This bit must be set if 3DSTATE_CLIP clip mode is set to normal and
-	 * 3DSTATE_SF number of SF output attributes is more than 16."
-	 */
-	I915_WRITE(_3D_CHICKEN3,
-		   _MASKED_BIT_ENABLE(_3D_CHICKEN3_SF_DISABLE_PIPELINED_ATTR_FETCH));
+static u16 pvid_to_extvid(struct drm_i915_private *dev_priv, u8 pxvid)
+{
+	struct drm_device *dev = dev_priv->dev;
+	static const struct v_table {
+		u16 vd; /* in .1 mil */
+		u16 vm; /* in .1 mil */
+	} v_table[] = {
+		{ 0, 0, },
+		{ 375, 0, },
+		{ 500, 0, },
+		{ 625, 0, },
+		{ 750, 0, },
+		{ 875, 0, },
+		{ 1000, 0, },
+		{ 1125, 0, },
+		{ 4125, 3000, },
+		{ 4125, 3000, },
+		{ 4125, 3000, },
+		{ 4125, 3000, },
+		{ 4125, 3000, },
+		{ 4125, 3000, },
+		{ 4125, 3000, },
+		{ 4125, 3000, },
+		{ 4125, 3000, },
+		{ 4125, 3000, },
+		{ 4125, 3000, },
+		{ 4125, 3000, },
+		{ 4125, 3000, },
+		{ 4125, 3000, },
+		{ 4125, 3000, },
+		{ 4125, 3000, },
+		{ 4125, 3000, },
+		{ 4125, 3000, },
+		{ 4125, 3000, },
+		{ 4125, 3000, },
+		{ 4125, 3000, },
+		{ 4125, 3000, },
+		{ 4125, 3000, },
+		{ 4125, 3000, },
+		{ 4250, 3125, },
+		{ 4375, 3250, },
+		{ 4500, 3375, },
+		{ 4625, 3500, },
+		{ 4750, 3625, },
+		{ 4875, 3750, },
+		{ 5000, 3875, },
+		{ 5125, 4000, },
+		{ 5250, 4125, },
+		{ 5375, 4250, },
+		{ 5500, 4375, },
+		{ 5625, 4500, },
+		{ 5750, 4625, },
+		{ 5875, 4750, },
+		{ 6000, 4875, },
+		{ 6125, 5000, },
+		{ 6250, 5125, },
+		{ 6375, 5250, },
+		{ 6500, 5375, },
+		{ 6625, 5500, },
+		{ 6750, 5625, },
+		{ 6875, 5750, },
+		{ 7000, 5875, },
+		{ 7125, 6000, },
+		{ 7250, 6125, },
+		{ 7375, 6250, },
+		{ 7500, 6375, },
+		{ 7625, 6500, },
+		{ 7750, 6625, },
+		{ 7875, 6750, },
+		{ 8000, 6875, },
+		{ 8125, 7000, },
+		{ 8250, 7125, },
+		{ 8375, 7250, },
+		{ 8500, 7375, },
+		{ 8625, 7500, },
+		{ 8750, 7625, },
+		{ 8875, 7750, },
+		{ 9000, 7875, },
+		{ 9125, 8000, },
+		{ 9250, 8125, },
+		{ 9375, 8250, },
+		{ 9500, 8375, },
+		{ 9625, 8500, },
+		{ 9750, 8625, },
+		{ 9875, 8750, },
+		{ 10000, 8875, },
+		{ 10125, 9000, },
+		{ 10250, 9125, },
+		{ 10375, 9250, },
+		{ 10500, 9375, },
+		{ 10625, 9500, },
+		{ 10750, 9625, },
+		{ 10875, 9750, },
+		{ 11000, 9875, },
+		{ 11125, 10000, },
+		{ 11250, 10125, },
+		{ 11375, 10250, },
+		{ 11500, 10375, },
+		{ 11625, 10500, },
+		{ 11750, 10625, },
+		{ 11875, 10750, },
+		{ 12000, 10875, },
+		{ 12125, 11000, },
+		{ 12250, 11125, },
+		{ 12375, 11250, },
+		{ 12500, 11375, },
+		{ 12625, 11500, },
+		{ 12750, 11625, },
+		{ 12875, 11750, },
+		{ 13000, 11875, },
+		{ 13125, 12000, },
+		{ 13250, 12125, },
+		{ 13375, 12250, },
+		{ 13500, 12375, },
+		{ 13625, 12500, },
+		{ 13750, 12625, },
+		{ 13875, 12750, },
+		{ 14000, 12875, },
+		{ 14125, 13000, },
+		{ 14250, 13125, },
+		{ 14375, 13250, },
+		{ 14500, 13375, },
+		{ 14625, 13500, },
+		{ 14750, 13625, },
+		{ 14875, 13750, },
+		{ 15000, 13875, },
+		{ 15125, 14000, },
+		{ 15250, 14125, },
+		{ 15375, 14250, },
+		{ 15500, 14375, },
+		{ 15625, 14500, },
+		{ 15750, 14625, },
+		{ 15875, 14750, },
+		{ 16000, 14875, },
+		{ 16125, 15000, },
+	};
+	if (INTEL_INFO(dev)->is_mobile)
+		return v_table[pxvid].vm;
+	else
+		return v_table[pxvid].vd;
+}
 
-	/*
-	 * According to the spec the following bits should be
-	 * set in order to enable memory self-refresh and fbc:
-	 * The bit21 and bit22 of 0x42000
-	 * The bit21 and bit22 of 0x42004
-	 * The bit5 and bit7 of 0x42020
-	 * The bit14 of 0x70180
-	 * The bit14 of 0x71180
-	 *
-	 * WaFbcAsynchFlipDisableFbcQueue:snb
-	 */
-	I915_WRITE(ILK_DISPLAY_CHICKEN1,
-		   I915_READ(ILK_DISPLAY_CHICKEN1) |
-		   ILK_FBCQ_DIS | ILK_PABSTRETCH_DIS);
-	I915_WRITE(ILK_DISPLAY_CHICKEN2,
-		   I915_READ(ILK_DISPLAY_CHICKEN2) |
-		   ILK_DPARB_GATE | ILK_VSDPFD_FULL);
-	I915_WRITE(ILK_DSPCLK_GATE_D,
-		   I915_READ(ILK_DSPCLK_GATE_D) |
-		   ILK_DPARBUNIT_CLOCK_GATE_ENABLE  |
-		   ILK_DPFDUNIT_CLOCK_GATE_ENABLE);
+static void __i915_update_gfx_val(struct drm_i915_private *dev_priv)
+{
+	u64 now, diff, diffms;
+	u32 count;
 
-	g4x_disable_trickle_feed(dev);
+	assert_spin_locked(&mchdev_lock);
 
-	cpt_init_clock_gating(dev);
+	now = ktime_get_raw_ns();
+	diffms = now - dev_priv->ips.last_time2;
+	do_div(diffms, NSEC_PER_MSEC);
 
-	gen6_check_mch_setup(dev);
-}
+	/* Don't divide by 0 */
+	if (!diffms)
+		return;
 
-static void gen7_setup_fixed_func_scheduler(struct drm_i915_private *dev_priv)
-{
-	uint32_t reg = I915_READ(GEN7_FF_THREAD_MODE);
+	count = I915_READ(GFXEC);
 
-	/*
-	 * WaVSThreadDispatchOverride:ivb,vlv
-	 *
-	 * This actually overrides the dispatch
-	 * mode for all thread types.
-	 */
-	reg &= ~GEN7_FF_SCHED_MASK;
-	reg |= GEN7_FF_TS_SCHED_HW;
-	reg |= GEN7_FF_VS_SCHED_HW;
-	reg |= GEN7_FF_DS_SCHED_HW;
+	if (count < dev_priv->ips.last_count2) {
+		diff = ~0UL - dev_priv->ips.last_count2;
+		diff += count;
+	} else {
+		diff = count - dev_priv->ips.last_count2;
+	}
 
-	I915_WRITE(GEN7_FF_THREAD_MODE, reg);
+	dev_priv->ips.last_count2 = count;
+	dev_priv->ips.last_time2 = now;
+
+	/* More magic constants... */
+	diff = diff * 1181;
+	diff = div_u64(diff, diffms * 10);
+	dev_priv->ips.gfx_power = diff;
 }
 
-static void lpt_init_clock_gating(struct drm_device *dev)
+void i915_update_gfx_val(struct drm_i915_private *dev_priv)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-
-	/*
-	 * TODO: this bit should only be enabled when really needed, then
-	 * disabled when not needed anymore in order to save power.
-	 */
-	if (dev_priv->pch_id == INTEL_PCH_LPT_LP_DEVICE_ID_TYPE)
-		I915_WRITE(SOUTH_DSPCLK_GATE_D,
-			   I915_READ(SOUTH_DSPCLK_GATE_D) |
-			   PCH_LP_PARTITION_LEVEL_DISABLE);
+	struct drm_device *dev = dev_priv->dev;
 
-	/* WADPOClockGatingDisable:hsw */
-	I915_WRITE(_TRANSA_CHICKEN1,
-		   I915_READ(_TRANSA_CHICKEN1) |
-		   TRANS_CHICKEN1_DP0UNIT_GC_DISABLE);
-}
+	if (INTEL_INFO(dev)->gen != 5)
+		return;
 
-static void lpt_suspend_hw(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	spin_lock_irq(&mchdev_lock);
 
-	if (dev_priv->pch_id == INTEL_PCH_LPT_LP_DEVICE_ID_TYPE) {
-		uint32_t val = I915_READ(SOUTH_DSPCLK_GATE_D);
+	__i915_update_gfx_val(dev_priv);
 
-		val &= ~PCH_LP_PARTITION_LEVEL_DISABLE;
-		I915_WRITE(SOUTH_DSPCLK_GATE_D, val);
-	}
+	spin_unlock_irq(&mchdev_lock);
 }
 
-static void gen8_init_clock_gating(struct drm_device *dev)
+static unsigned long __i915_gfx_val(struct drm_i915_private *dev_priv)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	enum pipe pipe;
+	unsigned long t, corr, state1, corr2, state2;
+	u32 pxvid, ext_v;
 
-	I915_WRITE(WM3_LP_ILK, 0);
-	I915_WRITE(WM2_LP_ILK, 0);
-	I915_WRITE(WM1_LP_ILK, 0);
+	assert_spin_locked(&mchdev_lock);
 
-	/* FIXME(BDW): Check all the w/a, some might only apply to
-	 * pre-production hw. */
+	pxvid = I915_READ(PXVFREQ_BASE + (dev_priv->rps.cur_freq * 4));
+	pxvid = (pxvid >> 24) & 0x7f;
+	ext_v = pvid_to_extvid(dev_priv, pxvid);
 
-	/* WaDisablePartialInstShootdown:bdw */
-	I915_WRITE(GEN8_ROW_CHICKEN,
-		   _MASKED_BIT_ENABLE(PARTIAL_INSTRUCTION_SHOOTDOWN_DISABLE));
-
-	/* WaDisableThreadStallDopClockGating:bdw */
-	/* FIXME: Unclear whether we really need this on production bdw. */
-	I915_WRITE(GEN8_ROW_CHICKEN,
-		   _MASKED_BIT_ENABLE(STALL_DOP_GATING_DISABLE));
-
-	/*
-	 * This GEN8_CENTROID_PIXEL_OPT_DIS W/A is only needed for
-	 * pre-production hardware
-	 */
-	I915_WRITE(HALF_SLICE_CHICKEN3,
-		   _MASKED_BIT_ENABLE(GEN8_CENTROID_PIXEL_OPT_DIS));
-	I915_WRITE(HALF_SLICE_CHICKEN3,
-		   _MASKED_BIT_ENABLE(GEN8_SAMPLER_POWER_BYPASS_DIS));
-	I915_WRITE(GAMTARBMODE, _MASKED_BIT_ENABLE(ARB_MODE_BWGTLB_DISABLE));
+	state1 = ext_v;
 
-	I915_WRITE(_3D_CHICKEN3,
-		   _MASKED_BIT_ENABLE(_3D_CHICKEN_SDE_LIMIT_FIFO_POLY_DEPTH(2)));
+	t = i915_mch_val(dev_priv);
 
-	I915_WRITE(COMMON_SLICE_CHICKEN2,
-		   _MASKED_BIT_ENABLE(GEN8_CSC2_SBE_VUE_CACHE_CONSERVATIVE));
+	/* Revel in the empirically derived constants */
 
-	I915_WRITE(GEN7_HALF_SLICE_CHICKEN1,
-		   _MASKED_BIT_ENABLE(GEN7_SINGLE_SUBSCAN_DISPATCH_ENABLE));
+	/* Correction factor in 1/100000 units */
+	if (t > 80)
+		corr = ((t * 2349) + 135940);
+	else if (t >= 50)
+		corr = ((t * 964) + 29317);
+	else /* < 50 */
+		corr = ((t * 301) + 1004);
 
-	/* WaDisableDopClockGating:bdw May not be needed for production */
-	I915_WRITE(GEN7_ROW_CHICKEN2,
-		   _MASKED_BIT_ENABLE(DOP_CLOCK_GATING_DISABLE));
+	corr = corr * ((150142 * state1) / 10000 - 78642);
+	corr /= 100000;
+	corr2 = (corr * dev_priv->ips.corr);
 
-	/* WaSwitchSolVfFArbitrationPriority:bdw */
-	I915_WRITE(GAM_ECOCHK, I915_READ(GAM_ECOCHK) | HSW_ECOCHK_ARB_PRIO_SOL);
+	state2 = (corr2 * state1) / 10000;
+	state2 /= 100; /* convert to mW */
 
-	/* WaPsrDPAMaskVBlankInSRD:bdw */
-	I915_WRITE(CHICKEN_PAR1_1,
-		   I915_READ(CHICKEN_PAR1_1) | DPA_MASK_VBLANK_SRD);
+	__i915_update_gfx_val(dev_priv);
 
-	/* WaPsrDPRSUnmaskVBlankInSRD:bdw */
-	for_each_pipe(pipe) {
-		I915_WRITE(CHICKEN_PIPESL_1(pipe),
-			   I915_READ(CHICKEN_PIPESL_1(pipe)) |
-			   BDW_DPRS_MASK_VBLANK_SRD);
-	}
+	return dev_priv->ips.gfx_power + state2;
+}
 
-	/* Use Force Non-Coherent whenever executing a 3D context. This is a
-	 * workaround for for a possible hang in the unlikely event a TLB
-	 * invalidation occurs during a PSD flush.
-	 */
-	I915_WRITE(HDC_CHICKEN0,
-		   I915_READ(HDC_CHICKEN0) |
-		   _MASKED_BIT_ENABLE(HDC_FORCE_NON_COHERENT));
+unsigned long i915_gfx_val(struct drm_i915_private *dev_priv)
+{
+	struct drm_device *dev = dev_priv->dev;
+	unsigned long val;
 
-	/* WaVSRefCountFullforceMissDisable:bdw */
-	/* WaDSRefCountFullforceMissDisable:bdw */
-	I915_WRITE(GEN7_FF_THREAD_MODE,
-		   I915_READ(GEN7_FF_THREAD_MODE) &
-		   ~(GEN8_FF_DS_REF_CNT_FFME | GEN7_FF_VS_REF_CNT_FFME));
+	if (INTEL_INFO(dev)->gen != 5)
+		return 0;
 
-	/*
-	 * BSpec recommends 8x4 when MSAA is used,
-	 * however in practice 16x4 seems fastest.
-	 *
-	 * Note that PS/WM thread counts depend on the WIZ hashing
-	 * disable bit, which we don't touch here, but it's good
-	 * to keep in mind (see 3DSTATE_PS and 3DSTATE_WM).
-	 */
-	I915_WRITE(GEN7_GT_MODE,
-		   GEN6_WIZ_HASHING_MASK | GEN6_WIZ_HASHING_16x4);
+	spin_lock_irq(&mchdev_lock);
 
-	I915_WRITE(GEN6_RC_SLEEP_PSMI_CONTROL,
-		   _MASKED_BIT_ENABLE(GEN8_RC_SEMA_IDLE_MSG_DISABLE));
+	val = __i915_gfx_val(dev_priv);
 
-	/* WaDisableSDEUnitClockGating:bdw */
-	I915_WRITE(GEN8_UCGCTL6, I915_READ(GEN8_UCGCTL6) |
-		   GEN8_SDEUNIT_CLOCK_GATE_DISABLE);
+	spin_unlock_irq(&mchdev_lock);
 
-	/* Wa4x4STCOptimizationDisable:bdw */
-	I915_WRITE(CACHE_MODE_1,
-		   _MASKED_BIT_ENABLE(GEN8_4x4_STC_OPTIMIZATION_DISABLE));
+	return val;
 }
 
-static void haswell_init_clock_gating(struct drm_device *dev)
+/**
+ * i915_read_mch_val - return value for IPS use
+ *
+ * Calculate and return a value for the IPS driver to use when deciding whether
+ * we have thermal and power headroom to increase CPU or GPU power budget.
+ */
+unsigned long i915_read_mch_val(void)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv;
+	unsigned long chipset_val, graphics_val, ret = 0;
 
-	ilk_init_lp_watermarks(dev);
+	spin_lock_irq(&mchdev_lock);
+	if (!i915_mch_dev)
+		goto out_unlock;
+	dev_priv = i915_mch_dev;
 
-	/* L3 caching of data atomics doesn't work -- disable it. */
-	I915_WRITE(HSW_SCRATCH1, HSW_SCRATCH1_L3_DATA_ATOMICS_DISABLE);
-	I915_WRITE(HSW_ROW_CHICKEN3,
-		   _MASKED_BIT_ENABLE(HSW_ROW_CHICKEN3_L3_GLOBAL_ATOMICS_DISABLE));
+	chipset_val = __i915_chipset_val(dev_priv);
+	graphics_val = __i915_gfx_val(dev_priv);
 
-	/* This is required by WaCatErrorRejectionIssue:hsw */
-	I915_WRITE(GEN7_SQ_CHICKEN_MBCUNIT_CONFIG,
-			I915_READ(GEN7_SQ_CHICKEN_MBCUNIT_CONFIG) |
-			GEN7_SQ_CHICKEN_MBCUNIT_SQINTMOB);
+	ret = chipset_val + graphics_val;
 
-	/* WaVSRefCountFullforceMissDisable:hsw */
-	I915_WRITE(GEN7_FF_THREAD_MODE,
-		   I915_READ(GEN7_FF_THREAD_MODE) & ~GEN7_FF_VS_REF_CNT_FFME);
+out_unlock:
+	spin_unlock_irq(&mchdev_lock);
 
-	/* WaDisable_RenderCache_OperationalFlush:hsw */
-	I915_WRITE(CACHE_MODE_0_GEN7, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i915_read_mch_val);
 
-	/* enable HiZ Raw Stall Optimization */
-	I915_WRITE(CACHE_MODE_0_GEN7,
-		   _MASKED_BIT_DISABLE(HIZ_RAW_STALL_OPT_DISABLE));
+/**
+ * i915_gpu_raise - raise GPU frequency limit
+ *
+ * Raise the limit; IPS indicates we have thermal headroom.
+ */
+bool i915_gpu_raise(void)
+{
+	struct drm_i915_private *dev_priv;
+	bool ret = true;
 
-	/* WaDisable4x2SubspanOptimization:hsw */
-	I915_WRITE(CACHE_MODE_1,
-		   _MASKED_BIT_ENABLE(PIXEL_SUBSPAN_COLLECT_OPT_DISABLE));
+	spin_lock_irq(&mchdev_lock);
+	if (!i915_mch_dev) {
+		ret = false;
+		goto out_unlock;
+	}
+	dev_priv = i915_mch_dev;
 
-	/*
-	 * BSpec recommends 8x4 when MSAA is used,
-	 * however in practice 16x4 seems fastest.
-	 *
-	 * Note that PS/WM thread counts depend on the WIZ hashing
-	 * disable bit, which we don't touch here, but it's good
-	 * to keep in mind (see 3DSTATE_PS and 3DSTATE_WM).
-	 */
-	I915_WRITE(GEN7_GT_MODE,
-		   GEN6_WIZ_HASHING_MASK | GEN6_WIZ_HASHING_16x4);
+	if (dev_priv->ips.max_delay > dev_priv->ips.fmax)
+		dev_priv->ips.max_delay--;
+
+out_unlock:
+	spin_unlock_irq(&mchdev_lock);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i915_gpu_raise);
+
+/**
+ * i915_gpu_lower - lower GPU frequency limit
+ *
+ * IPS indicates we're close to a thermal limit, so throttle back the GPU
+ * frequency maximum.
+ */
+bool i915_gpu_lower(void)
+{
+	struct drm_i915_private *dev_priv;
+	bool ret = true;
 
-	/* WaSwitchSolVfFArbitrationPriority:hsw */
-	I915_WRITE(GAM_ECOCHK, I915_READ(GAM_ECOCHK) | HSW_ECOCHK_ARB_PRIO_SOL);
+	spin_lock_irq(&mchdev_lock);
+	if (!i915_mch_dev) {
+		ret = false;
+		goto out_unlock;
+	}
+	dev_priv = i915_mch_dev;
 
-	/* WaRsPkgCStateDisplayPMReq:hsw */
-	I915_WRITE(CHICKEN_PAR1_1,
-		   I915_READ(CHICKEN_PAR1_1) | FORCE_ARB_IDLE_PLANES);
+	if (dev_priv->ips.max_delay < dev_priv->ips.min_delay)
+		dev_priv->ips.max_delay++;
 
-	lpt_init_clock_gating(dev);
+out_unlock:
+	spin_unlock_irq(&mchdev_lock);
+
+	return ret;
 }
+EXPORT_SYMBOL_GPL(i915_gpu_lower);
 
-static void ivybridge_init_clock_gating(struct drm_device *dev)
+/**
+ * i915_gpu_busy - indicate GPU business to IPS
+ *
+ * Tell the IPS driver whether or not the GPU is busy.
+ */
+bool i915_gpu_busy(void)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	uint32_t snpcr;
-
-	ilk_init_lp_watermarks(dev);
-
-	I915_WRITE(ILK_DSPCLK_GATE_D, ILK_VRHUNIT_CLOCK_GATE_DISABLE);
+	struct drm_i915_private *dev_priv;
+	struct intel_engine_cs *engine;
+	bool ret = false;
+	int i;
 
-	/* WaDisableEarlyCull:ivb */
-	I915_WRITE(_3D_CHICKEN3,
-		   _MASKED_BIT_ENABLE(_3D_CHICKEN_SF_DISABLE_OBJEND_CULL));
+	spin_lock_irq(&mchdev_lock);
+	if (!i915_mch_dev)
+		goto out_unlock;
+	dev_priv = i915_mch_dev;
 
-	/* WaDisableBackToBackFlipFix:ivb */
-	I915_WRITE(IVB_CHICKEN3,
-		   CHICKEN3_DGMG_REQ_OUT_FIX_DISABLE |
-		   CHICKEN3_DGMG_DONE_FIX_DISABLE);
+	for_each_engine(engine, dev_priv, i)
+		ret |= engine->last_request != NULL;
 
-	/* WaDisablePSDDualDispatchEnable:ivb */
-	if (IS_IVB_GT1(dev))
-		I915_WRITE(GEN7_HALF_SLICE_CHICKEN1,
-			   _MASKED_BIT_ENABLE(GEN7_PSD_SINGLE_PORT_DISPATCH_ENABLE));
+out_unlock:
+	spin_unlock_irq(&mchdev_lock);
 
-	/* WaDisable_RenderCache_OperationalFlush:ivb */
-	I915_WRITE(CACHE_MODE_0_GEN7, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i915_gpu_busy);
 
-	/* Apply the WaDisableRHWOOptimizationForRenderHang:ivb workaround. */
-	I915_WRITE(GEN7_COMMON_SLICE_CHICKEN1,
-		   GEN7_CSC1_RHWO_OPT_DISABLE_IN_RCC);
+/**
+ * i915_gpu_turbo_disable - disable graphics turbo
+ *
+ * Disable graphics turbo by resetting the max frequency and setting the
+ * current frequency to the default.
+ */
+bool i915_gpu_turbo_disable(void)
+{
+	struct drm_i915_private *dev_priv;
+	bool ret = true;
 
-	/* WaApplyL3ControlAndL3ChickenMode:ivb */
-	I915_WRITE(GEN7_L3CNTLREG1,
-			GEN7_WA_FOR_GEN7_L3_CONTROL);
-	I915_WRITE(GEN7_L3_CHICKEN_MODE_REGISTER,
-		   GEN7_WA_L3_CHICKEN_MODE);
-	if (IS_IVB_GT1(dev))
-		I915_WRITE(GEN7_ROW_CHICKEN2,
-			   _MASKED_BIT_ENABLE(DOP_CLOCK_GATING_DISABLE));
-	else {
-		/* must write both registers */
-		I915_WRITE(GEN7_ROW_CHICKEN2,
-			   _MASKED_BIT_ENABLE(DOP_CLOCK_GATING_DISABLE));
-		I915_WRITE(GEN7_ROW_CHICKEN2_GT2,
-			   _MASKED_BIT_ENABLE(DOP_CLOCK_GATING_DISABLE));
+	spin_lock_irq(&mchdev_lock);
+	if (!i915_mch_dev) {
+		ret = false;
+		goto out_unlock;
 	}
+	dev_priv = i915_mch_dev;
 
-	/* WaForceL3Serialization:ivb */
-	I915_WRITE(GEN7_L3SQCREG4, I915_READ(GEN7_L3SQCREG4) &
-		   ~L3SQ_URB_READ_CAM_MATCH_DISABLE);
+	dev_priv->ips.max_delay = dev_priv->ips.fstart;
 
-	/*
-	 * According to the spec, bit 13 (RCZUNIT) must be set on IVB.
-	 * This implements the WaDisableRCZUnitClockGating:ivb workaround.
-	 */
-	I915_WRITE(GEN6_UCGCTL2,
-		   GEN6_RCZUNIT_CLOCK_GATE_DISABLE);
+	if (!ironlake_set_drps(dev_priv->dev, dev_priv->ips.fstart))
+		ret = false;
 
-	/* This is required by WaCatErrorRejectionIssue:ivb */
-	I915_WRITE(GEN7_SQ_CHICKEN_MBCUNIT_CONFIG,
-			I915_READ(GEN7_SQ_CHICKEN_MBCUNIT_CONFIG) |
-			GEN7_SQ_CHICKEN_MBCUNIT_SQINTMOB);
+out_unlock:
+	spin_unlock_irq(&mchdev_lock);
 
-	g4x_disable_trickle_feed(dev);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(i915_gpu_turbo_disable);
 
-	gen7_setup_fixed_func_scheduler(dev_priv);
+/**
+ * Tells the intel_ips driver that the i915 driver is now loaded, if
+ * IPS got loaded first.
+ *
+ * This awkward dance is so that neither module has to depend on the
+ * other in order for IPS to do the appropriate communication of
+ * GPU turbo limits to i915.
+ */
+static void
+ips_ping_for_i915_load(void)
+{
+	void (*link)(void);
 
-	if (0) { /* causes HiZ corruption on ivb:gt1 */
-		/* enable HiZ Raw Stall Optimization */
-		I915_WRITE(CACHE_MODE_0_GEN7,
-			   _MASKED_BIT_DISABLE(HIZ_RAW_STALL_OPT_DISABLE));
+	link = symbol_get(ips_link_to_i915_driver);
+	if (link) {
+		link();
+		symbol_put(ips_link_to_i915_driver);
 	}
+}
 
-	/* WaDisable4x2SubspanOptimization:ivb */
-	I915_WRITE(CACHE_MODE_1,
-		   _MASKED_BIT_ENABLE(PIXEL_SUBSPAN_COLLECT_OPT_DISABLE));
-
-	/*
-	 * BSpec recommends 8x4 when MSAA is used,
-	 * however in practice 16x4 seems fastest.
-	 *
-	 * Note that PS/WM thread counts depend on the WIZ hashing
-	 * disable bit, which we don't touch here, but it's good
-	 * to keep in mind (see 3DSTATE_PS and 3DSTATE_WM).
-	 */
-	I915_WRITE(GEN7_GT_MODE,
-		   GEN6_WIZ_HASHING_MASK | GEN6_WIZ_HASHING_16x4);
-
-	snpcr = I915_READ(GEN6_MBCUNIT_SNPCR);
-	snpcr &= ~GEN6_MBC_SNPCR_MASK;
-	snpcr |= GEN6_MBC_SNPCR_MED;
-	I915_WRITE(GEN6_MBCUNIT_SNPCR, snpcr);
+void intel_gpu_ips_init(struct drm_i915_private *dev_priv)
+{
+	/* We only register the i915 ips part with intel-ips once everything is
+	 * set up, to avoid intel-ips sneaking in and reading bogus values. */
+	spin_lock_irq(&mchdev_lock);
+	i915_mch_dev = dev_priv;
+	spin_unlock_irq(&mchdev_lock);
 
-	if (!HAS_PCH_NOP(dev))
-		cpt_init_clock_gating(dev);
+	ips_ping_for_i915_load();
+}
 
-	gen6_check_mch_setup(dev);
+void intel_gpu_ips_teardown(void)
+{
+	spin_lock_irq(&mchdev_lock);
+	i915_mch_dev = NULL;
+	spin_unlock_irq(&mchdev_lock);
 }
 
-static void valleyview_init_clock_gating(struct drm_device *dev)
+static void intel_init_emon(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	u32 val;
+	u32 lcfuse;
+	u8 pxw[16];
+	int i;
 
-	mutex_lock(&dev_priv->rps.hw_lock);
-	val = vlv_punit_read(dev_priv, PUNIT_REG_GPU_FREQ_STS);
-	mutex_unlock(&dev_priv->rps.hw_lock);
-	switch ((val >> 6) & 3) {
-	case 0:
-	case 1:
-		dev_priv->mem_freq = 800;
-		break;
-	case 2:
-		dev_priv->mem_freq = 1066;
-		break;
-	case 3:
-		dev_priv->mem_freq = 1333;
-		break;
-	}
-	DRM_DEBUG_DRIVER("DDR speed: %d MHz", dev_priv->mem_freq);
+	/* Disable to program */
+	I915_WRITE(ECR, 0);
+	POSTING_READ(ECR);
 
-	I915_WRITE(DSPCLK_GATE_D, VRHUNIT_CLOCK_GATE_DISABLE);
+	/* Program energy weights for various events */
+	I915_WRITE(SDEW, 0x15040d00);
+	I915_WRITE(CSIEW0, 0x007f0000);
+	I915_WRITE(CSIEW1, 0x1e220004);
+	I915_WRITE(CSIEW2, 0x04000004);
 
-	/* WaDisableEarlyCull:vlv */
-	I915_WRITE(_3D_CHICKEN3,
-		   _MASKED_BIT_ENABLE(_3D_CHICKEN_SF_DISABLE_OBJEND_CULL));
+	for (i = 0; i < 5; i++)
+		I915_WRITE(PEW + (i * 4), 0);
+	for (i = 0; i < 3; i++)
+		I915_WRITE(DEW + (i * 4), 0);
 
-	/* WaDisableBackToBackFlipFix:vlv */
-	I915_WRITE(IVB_CHICKEN3,
-		   CHICKEN3_DGMG_REQ_OUT_FIX_DISABLE |
-		   CHICKEN3_DGMG_DONE_FIX_DISABLE);
+	/* Program P-state weights to account for frequency power adjustment */
+	for (i = 0; i < 16; i++) {
+		u32 pxvidfreq = I915_READ(PXVFREQ_BASE + (i * 4));
+		unsigned long freq = intel_pxfreq(pxvidfreq);
+		unsigned long vid = (pxvidfreq & PXVFREQ_PX_MASK) >>
+			PXVFREQ_PX_SHIFT;
+		unsigned long val;
 
-	/* WaPsdDispatchEnable:vlv */
-	/* WaDisablePSDDualDispatchEnable:vlv */
-	I915_WRITE(GEN7_HALF_SLICE_CHICKEN1,
-		   _MASKED_BIT_ENABLE(GEN7_MAX_PS_THREAD_DEP |
-				      GEN7_PSD_SINGLE_PORT_DISPATCH_ENABLE));
+		val = vid * vid;
+		val *= (freq / 1000);
+		val *= 255;
+		val /= (127*127*900);
+		if (val > 0xff)
+			DRM_ERROR("bad pxval: %ld\n", val);
+		pxw[i] = val;
+	}
+	/* Render standby states get 0 weight */
+	pxw[14] = 0;
+	pxw[15] = 0;
+
+	for (i = 0; i < 4; i++) {
+		u32 val = (pxw[i*4] << 24) | (pxw[(i*4)+1] << 16) |
+			(pxw[(i*4)+2] << 8) | (pxw[(i*4)+3]);
+		I915_WRITE(PXW + (i * 4), val);
+	}
+
+	/* Adjust magic regs to magic values (more experimental results) */
+	I915_WRITE(OGW0, 0);
+	I915_WRITE(OGW1, 0);
+	I915_WRITE(EG0, 0x00007f00);
+	I915_WRITE(EG1, 0x0000000e);
+	I915_WRITE(EG2, 0x000e0000);
+	I915_WRITE(EG3, 0x68000300);
+	I915_WRITE(EG4, 0x42000000);
+	I915_WRITE(EG5, 0x00140031);
+	I915_WRITE(EG6, 0);
+	I915_WRITE(EG7, 0);
 
-	/* WaDisable_RenderCache_OperationalFlush:vlv */
-	I915_WRITE(CACHE_MODE_0_GEN7, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
+	for (i = 0; i < 8; i++)
+		I915_WRITE(PXWL + (i * 4), 0);
 
-	/* WaForceL3Serialization:vlv */
-	I915_WRITE(GEN7_L3SQCREG4, I915_READ(GEN7_L3SQCREG4) &
-		   ~L3SQ_URB_READ_CAM_MATCH_DISABLE);
+	/* Enable PMON + select events */
+	I915_WRITE(ECR, 0x80000019);
 
-	/* WaDisableDopClockGating:vlv */
-	I915_WRITE(GEN7_ROW_CHICKEN2,
-		   _MASKED_BIT_ENABLE(DOP_CLOCK_GATING_DISABLE));
+	lcfuse = I915_READ(LCFUSE02);
 
-	/* This is required by WaCatErrorRejectionIssue:vlv */
-	I915_WRITE(GEN7_SQ_CHICKEN_MBCUNIT_CONFIG,
-		   I915_READ(GEN7_SQ_CHICKEN_MBCUNIT_CONFIG) |
-		   GEN7_SQ_CHICKEN_MBCUNIT_SQINTMOB);
+	dev_priv->ips.corr = (lcfuse & LCFUSE_HIV_MASK);
+}
 
-	gen7_setup_fixed_func_scheduler(dev_priv);
+void intel_init_gt_powersave(struct drm_device *dev)
+{
+	i915_module.enable_rc6 = sanitize_rc6_option(dev, i915_module.enable_rc6);
 
-	/*
-	 * According to the spec, bit 13 (RCZUNIT) must be set on IVB.
-	 * This implements the WaDisableRCZUnitClockGating:vlv workaround.
-	 */
-	I915_WRITE(GEN6_UCGCTL2,
-		   GEN6_RCZUNIT_CLOCK_GATE_DISABLE);
+	if (IS_CHERRYVIEW(dev))
+		cherryview_init_gt_powersave(dev);
+	else if (IS_VALLEYVIEW(dev))
+		valleyview_init_gt_powersave(dev);
+}
 
-	/* WaDisableL3Bank2xClockGate:vlv
-	 * Disabling L3 clock gating- MMIO 940c[25] = 1
-	 * Set bit 25, to disable L3_BANK_2x_CLK_GATING */
-	I915_WRITE(GEN7_UCGCTL4,
-		   I915_READ(GEN7_UCGCTL4) | GEN7_L3BANK2X_CLOCK_GATE_DISABLE);
+void intel_cleanup_gt_powersave(struct drm_device *dev)
+{
+	if (IS_CHERRYVIEW(dev))
+		return;
+	else if (IS_VALLEYVIEW(dev))
+		valleyview_cleanup_gt_powersave(dev);
+}
 
-	I915_WRITE(MI_ARB_VLV, MI_ARB_DISPLAY_TRICKLE_FEED_DISABLE);
+/**
+ * intel_suspend_gt_powersave - suspend PM work and helper threads
+ * @dev: drm device
+ *
+ * We don't want to disable RC6 or other features here, we just want
+ * to make sure any work we've queued has finished and won't bother
+ * us while we're suspended.
+ */
+void intel_suspend_gt_powersave(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	/*
-	 * BSpec says this must be set, even though
-	 * WaDisable4x2SubspanOptimization isn't listed for VLV.
-	 */
-	I915_WRITE(CACHE_MODE_1,
-		   _MASKED_BIT_ENABLE(PIXEL_SUBSPAN_COLLECT_OPT_DISABLE));
+	if (INTEL_INFO(dev)->gen < 6)
+		return;
 
-	/*
-	 * WaIncreaseL3CreditsForVLVB0:vlv
-	 * This is the hardware default actually.
-	 */
-	I915_WRITE(GEN7_L3SQCREG1, VLV_B0_WA_L3SQCREG1_VALUE);
+	flush_delayed_work(&dev_priv->rps.delayed_resume_work);
 
 	/*
-	 * WaDisableVLVClockGating_VBIIssue:vlv
-	 * Disable clock gating on th GCFG unit to prevent a delay
-	 * in the reporting of vblank events.
+	 * TODO: disable RPS interrupts on GEN9+ too once RPS support
+	 * is added for it.
 	 */
-	I915_WRITE(VLV_GUNIT_CLOCK_GATE, GCFG_DIS);
+	if (INTEL_INFO(dev)->gen < 9)
+		gen6_disable_rps_interrupts(dev);
+
+	/* Force GPU to min freq during suspend */
+	gen6_rps_idle(dev_priv);
 }
 
-static void cherryview_init_clock_gating(struct drm_device *dev)
+void intel_disable_gt_powersave(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	u32 val;
 
-	mutex_lock(&dev_priv->rps.hw_lock);
-	val = vlv_punit_read(dev_priv, CCK_FUSE_REG);
-	mutex_unlock(&dev_priv->rps.hw_lock);
-	switch ((val >> 2) & 0x7) {
-	case 0:
-	case 1:
-			dev_priv->rps.cz_freq = CHV_CZ_CLOCK_FREQ_MODE_200;
-			dev_priv->mem_freq = 1600;
-			break;
-	case 2:
-			dev_priv->rps.cz_freq = CHV_CZ_CLOCK_FREQ_MODE_267;
-			dev_priv->mem_freq = 1600;
-			break;
-	case 3:
-			dev_priv->rps.cz_freq = CHV_CZ_CLOCK_FREQ_MODE_333;
-			dev_priv->mem_freq = 2000;
-			break;
-	case 4:
-			dev_priv->rps.cz_freq = CHV_CZ_CLOCK_FREQ_MODE_320;
-			dev_priv->mem_freq = 1600;
-			break;
-	case 5:
-			dev_priv->rps.cz_freq = CHV_CZ_CLOCK_FREQ_MODE_400;
-			dev_priv->mem_freq = 1600;
-			break;
-	}
-	DRM_DEBUG_DRIVER("DDR speed: %d MHz", dev_priv->mem_freq);
+	WARN_ON(dev_priv->mm.busy);
 
-	I915_WRITE(DSPCLK_GATE_D, VRHUNIT_CLOCK_GATE_DISABLE);
+	/* Interrupts should be disabled already to avoid re-arming. */
+	WARN_ON(intel_irqs_enabled(dev_priv));
 
-	I915_WRITE(MI_ARB_VLV, MI_ARB_DISPLAY_TRICKLE_FEED_DISABLE);
+	if (IS_IRONLAKE_M(dev)) {
+		ironlake_disable_drps(dev);
+		ironlake_disable_rc6(dev);
+	} else if (INTEL_INFO(dev)->gen >= 6) {
+		intel_suspend_gt_powersave(dev);
 
-	/* WaDisablePartialInstShootdown:chv */
-	I915_WRITE(GEN8_ROW_CHICKEN,
-		   _MASKED_BIT_ENABLE(PARTIAL_INSTRUCTION_SHOOTDOWN_DISABLE));
-
-	/* WaDisableThreadStallDopClockGating:chv */
-	I915_WRITE(GEN8_ROW_CHICKEN,
-		   _MASKED_BIT_ENABLE(STALL_DOP_GATING_DISABLE));
+		mutex_lock(&dev_priv->rps.hw_lock);
+		if (INTEL_INFO(dev)->gen >= 9)
+			gen9_disable_rps(dev);
+		else if (IS_CHERRYVIEW(dev))
+			cherryview_disable_rps(dev);
+		else if (IS_VALLEYVIEW(dev))
+			valleyview_disable_rps(dev);
+		else
+			gen6_disable_rps(dev);
 
-	/* WaVSRefCountFullforceMissDisable:chv */
-	/* WaDSRefCountFullforceMissDisable:chv */
-	I915_WRITE(GEN7_FF_THREAD_MODE,
-		   I915_READ(GEN7_FF_THREAD_MODE) &
-		   ~(GEN8_FF_DS_REF_CNT_FFME | GEN7_FF_VS_REF_CNT_FFME));
+		dev_priv->rps.enabled = false;
+		mutex_unlock(&dev_priv->rps.hw_lock);
+	}
+}
 
-	/* WaDisableSemaphoreAndSyncFlipWait:chv */
-	I915_WRITE(GEN6_RC_SLEEP_PSMI_CONTROL,
-		   _MASKED_BIT_ENABLE(GEN8_RC_SEMA_IDLE_MSG_DISABLE));
+static void intel_gen6_powersave_work(struct work_struct *work)
+{
+	struct drm_i915_private *dev_priv =
+		container_of(work, struct drm_i915_private,
+			     rps.delayed_resume_work.work);
+	struct drm_device *dev = dev_priv->dev;
 
-	/* WaDisableCSUnitClockGating:chv */
-	I915_WRITE(GEN6_UCGCTL1, I915_READ(GEN6_UCGCTL1) |
-		   GEN6_CSUNIT_CLOCK_GATE_DISABLE);
+	mutex_lock(&dev_priv->rps.hw_lock);
 
-	/* WaDisableSDEUnitClockGating:chv */
-	I915_WRITE(GEN8_UCGCTL6, I915_READ(GEN8_UCGCTL6) |
-		   GEN8_SDEUNIT_CLOCK_GATE_DISABLE);
+	/*
+	 * TODO: reset/enable RPS interrupts on GEN9+ too, once RPS support is
+	 * added for it.
+	 */
+	if (INTEL_INFO(dev)->gen < 9)
+		gen6_reset_rps_interrupts(dev);
+
+	if (IS_CHERRYVIEW(dev)) {
+		cherryview_enable_rps(dev);
+	} else if (IS_VALLEYVIEW(dev)) {
+		valleyview_enable_rps(dev);
+	} else if (INTEL_INFO(dev)->gen >= 9) {
+		gen9_enable_rps(dev);
+	} else if (IS_BROADWELL(dev)) {
+		gen8_enable_rps(dev);
+		__gen6_update_ring_freq(dev);
+	} else {
+		gen6_enable_rps(dev);
+		__gen6_update_ring_freq(dev);
+	}
+	dev_priv->rps.enabled = true;
 
-	/* WaDisableSamplerPowerBypass:chv (pre-production hw) */
-	I915_WRITE(HALF_SLICE_CHICKEN3,
-		   _MASKED_BIT_ENABLE(GEN8_SAMPLER_POWER_BYPASS_DIS));
-
-	/* WaDisableGunitClockGating:chv (pre-production hw) */
-	I915_WRITE(VLV_GUNIT_CLOCK_GATE, I915_READ(VLV_GUNIT_CLOCK_GATE) |
-		   GINT_DIS);
+	if (INTEL_INFO(dev)->gen < 9)
+		gen6_enable_rps_interrupts(dev);
 
-	/* WaDisableFfDopClockGating:chv (pre-production hw) */
-	I915_WRITE(GEN6_RC_SLEEP_PSMI_CONTROL,
-		   _MASKED_BIT_ENABLE(GEN8_FF_DOP_CLOCK_GATE_DISABLE));
+	mutex_unlock(&dev_priv->rps.hw_lock);
 
-	/* WaDisableDopClockGating:chv (pre-production hw) */
-	I915_WRITE(GEN7_ROW_CHICKEN2,
-		   _MASKED_BIT_ENABLE(DOP_CLOCK_GATING_DISABLE));
-	I915_WRITE(GEN6_UCGCTL1, I915_READ(GEN6_UCGCTL1) |
-		   GEN6_EU_TCUNIT_CLOCK_GATE_DISABLE);
+	intel_runtime_pm_put(dev_priv);
 }
 
-static void g4x_init_clock_gating(struct drm_device *dev)
+void intel_enable_gt_powersave(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	uint32_t dspclk_gate;
 
-	I915_WRITE(RENCLK_GATE_D1, 0);
-	I915_WRITE(RENCLK_GATE_D2, VF_UNIT_CLOCK_GATE_DISABLE |
-		   GS_UNIT_CLOCK_GATE_DISABLE |
-		   CL_UNIT_CLOCK_GATE_DISABLE);
-	I915_WRITE(RAMCLK_GATE_D, 0);
-	dspclk_gate = VRHUNIT_CLOCK_GATE_DISABLE |
-		OVRUNIT_CLOCK_GATE_DISABLE |
-		OVCUNIT_CLOCK_GATE_DISABLE;
-	if (IS_GM45(dev))
-		dspclk_gate |= DSSUNIT_CLOCK_GATE_DISABLE;
-	I915_WRITE(DSPCLK_GATE_D, dspclk_gate);
-
-	/* WaDisableRenderCachePipelinedFlush */
-	I915_WRITE(CACHE_MODE_0,
-		   _MASKED_BIT_ENABLE(CM0_PIPELINED_RENDER_FLUSH_DISABLE));
+	if (IS_IRONLAKE_M(dev)) {
+		mutex_lock(&dev->struct_mutex);
+		ironlake_enable_drps(dev);
+		ironlake_enable_rc6(dev);
+		intel_init_emon(dev);
+		mutex_unlock(&dev->struct_mutex);
+	} else if (INTEL_INFO(dev)->gen >= 6) {
+		/*
+		 * PCU communication is slow and this doesn't need to be
+		 * done at any specific time, so do this out of our fast path
+		 * to make resume and init faster.
+		 *
+		 * We depend on the HW RC6 power context save/restore
+		 * mechanism when entering D3 through runtime PM suspend. So
+		 * disable RPM until RPS/RC6 is properly setup. We can only
+		 * get here via the driver load/system resume/runtime resume
+		 * paths, so the _noresume version is enough (and in case of
+		 * runtime resume it's necessary).
+		 */
+		if (schedule_delayed_work(&dev_priv->rps.delayed_resume_work,
+					   round_jiffies_up_relative(HZ)))
+			intel_runtime_pm_get_noresume(dev_priv);
+	}
+}
 
-	/* WaDisable_RenderCache_OperationalFlush:g4x */
-	I915_WRITE(CACHE_MODE_0, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
+void intel_reset_gt_powersave(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	g4x_disable_trickle_feed(dev);
+	dev_priv->rps.enabled = false;
+	intel_enable_gt_powersave(dev);
 }
 
-static void crestline_init_clock_gating(struct drm_device *dev)
+static void ibx_init_clock_gating(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	I915_WRITE(RENCLK_GATE_D1, I965_RCC_CLOCK_GATE_DISABLE);
-	I915_WRITE(RENCLK_GATE_D2, 0);
-	I915_WRITE(DSPCLK_GATE_D, 0);
-	I915_WRITE(RAMCLK_GATE_D, 0);
-	I915_WRITE16(DEUC, 0);
-	I915_WRITE(MI_ARB_STATE,
-		   _MASKED_BIT_ENABLE(MI_ARB_DISPLAY_TRICKLE_FEED_DISABLE));
+	/*
+	 * On Ibex Peak and Cougar Point, we need to disable clock
+	 * gating for the panel power sequencer or it will fail to
+	 * start up when no ports are active.
+	 */
+	I915_WRITE(SOUTH_DSPCLK_GATE_D, PCH_DPLSUNIT_CLOCK_GATE_DISABLE);
+}
 
-	/* WaDisable_RenderCache_OperationalFlush:gen4 */
-	I915_WRITE(CACHE_MODE_0, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
+static void g4x_disable_trickle_feed(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	int pipe;
+
+	for_each_pipe(dev_priv, pipe) {
+		I915_WRITE(DSPCNTR(pipe),
+			   I915_READ(DSPCNTR(pipe)) |
+			   DISPPLANE_TRICKLE_FEED_DISABLE);
+		intel_flush_primary_plane(dev_priv, pipe);
+	}
 }
 
-static void broadwater_init_clock_gating(struct drm_device *dev)
+static void ilk_init_lp_watermarks(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	I915_WRITE(RENCLK_GATE_D1, I965_RCZ_CLOCK_GATE_DISABLE |
-		   I965_RCC_CLOCK_GATE_DISABLE |
-		   I965_RCPB_CLOCK_GATE_DISABLE |
-		   I965_ISC_CLOCK_GATE_DISABLE |
-		   I965_FBC_CLOCK_GATE_DISABLE);
-	I915_WRITE(RENCLK_GATE_D2, 0);
-	I915_WRITE(MI_ARB_STATE,
-		   _MASKED_BIT_ENABLE(MI_ARB_DISPLAY_TRICKLE_FEED_DISABLE));
+	I915_WRITE(WM3_LP_ILK, I915_READ(WM3_LP_ILK) & ~WM1_LP_SR_EN);
+	I915_WRITE(WM2_LP_ILK, I915_READ(WM2_LP_ILK) & ~WM1_LP_SR_EN);
+	I915_WRITE(WM1_LP_ILK, I915_READ(WM1_LP_ILK) & ~WM1_LP_SR_EN);
 
-	/* WaDisable_RenderCache_OperationalFlush:gen4 */
-	I915_WRITE(CACHE_MODE_0, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
+	/*
+	 * Don't touch WM1S_LP_EN here.
+	 * Doing so could cause underruns.
+	 */
 }
 
-static void gen3_init_clock_gating(struct drm_device *dev)
+static void ironlake_init_clock_gating(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	u32 dstate = I915_READ(D_STATE);
+	uint32_t dspclk_gate = ILK_VRHUNIT_CLOCK_GATE_DISABLE;
 
-	dstate |= DSTATE_PLL_D3_OFF | DSTATE_GFX_CLOCK_GATING |
-		DSTATE_DOT_CLOCK_GATING;
-	I915_WRITE(D_STATE, dstate);
+	/*
+	 * Required for FBC
+	 * WaFbcDisableDpfcClockGating:ilk
+	 */
+	dspclk_gate |= ILK_DPFCRUNIT_CLOCK_GATE_DISABLE |
+		   ILK_DPFCUNIT_CLOCK_GATE_DISABLE |
+		   ILK_DPFDUNIT_CLOCK_GATE_ENABLE;
 
-	if (IS_PINEVIEW(dev))
-		I915_WRITE(ECOSKPD, _MASKED_BIT_ENABLE(ECO_GATING_CX_ONLY));
+	I915_WRITE(PCH_3DCGDIS0,
+		   MARIUNIT_CLOCK_GATE_DISABLE |
+		   SVSMUNIT_CLOCK_GATE_DISABLE);
+	I915_WRITE(PCH_3DCGDIS1,
+		   VFMUNIT_CLOCK_GATE_DISABLE);
 
-	/* IIR "flip pending" means done if this bit is set */
-	I915_WRITE(ECOSKPD, _MASKED_BIT_DISABLE(ECO_FLIP_DONE));
+	/*
+	 * According to the spec the following bits should be set in
+	 * order to enable memory self-refresh
+	 * The bit 22/21 of 0x42004
+	 * The bit 5 of 0x42020
+	 * The bit 15 of 0x45000
+	 */
+	I915_WRITE(ILK_DISPLAY_CHICKEN2,
+		   (I915_READ(ILK_DISPLAY_CHICKEN2) |
+		    ILK_DPARB_GATE | ILK_VSDPFD_FULL));
+	dspclk_gate |= ILK_DPARBUNIT_CLOCK_GATE_ENABLE;
+	I915_WRITE(DISP_ARB_CTL,
+		   (I915_READ(DISP_ARB_CTL) |
+		    DISP_FBC_WM_DIS));
 
-	/* interrupts should cause a wake up from C3 */
-	I915_WRITE(INSTPM, _MASKED_BIT_ENABLE(INSTPM_AGPBUSY_INT_EN));
+	ilk_init_lp_watermarks(dev);
 
-	/* On GEN3 we really need to make sure the ARB C3 LP bit is set */
-	I915_WRITE(MI_ARB_STATE, _MASKED_BIT_ENABLE(MI_ARB_C3_LP_WRITE_ENABLE));
-}
+	/*
+	 * Based on the document from hardware guys the following bits
+	 * should be set unconditionally in order to enable FBC.
+	 * The bit 22 of 0x42000
+	 * The bit 22 of 0x42004
+	 * The bit 7,8,9 of 0x42020.
+	 */
+	if (IS_IRONLAKE_M(dev)) {
+		/* WaFbcAsynchFlipDisableFbcQueue:ilk */
+		I915_WRITE(ILK_DISPLAY_CHICKEN1,
+			   I915_READ(ILK_DISPLAY_CHICKEN1) |
+			   ILK_FBCQ_DIS);
+		I915_WRITE(ILK_DISPLAY_CHICKEN2,
+			   I915_READ(ILK_DISPLAY_CHICKEN2) |
+			   ILK_DPARB_GATE);
+	}
 
-static void i85x_init_clock_gating(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	I915_WRITE(ILK_DSPCLK_GATE_D, dspclk_gate);
 
-	I915_WRITE(RENCLK_GATE_D1, SV_CLOCK_GATE_DISABLE);
+	I915_WRITE(ILK_DISPLAY_CHICKEN2,
+		   I915_READ(ILK_DISPLAY_CHICKEN2) |
+		   ILK_ELPIN_409_SELECT);
+	I915_WRITE(_3D_CHICKEN2,
+		   _3D_CHICKEN2_WM_READ_PIPELINED << 16 |
+		   _3D_CHICKEN2_WM_READ_PIPELINED);
 
-	/* interrupts should cause a wake up from C3 */
-	I915_WRITE(MI_STATE, _MASKED_BIT_ENABLE(MI_AGPBUSY_INT_EN) |
-		   _MASKED_BIT_DISABLE(MI_AGPBUSY_830_MODE));
-}
+	/* WaDisableRenderCachePipelinedFlush:ilk */
+	I915_WRITE(CACHE_MODE_0,
+		   _MASKED_BIT_ENABLE(CM0_PIPELINED_RENDER_FLUSH_DISABLE));
 
-static void i830_init_clock_gating(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	/* WaDisable_RenderCache_OperationalFlush:ilk */
+	I915_WRITE(CACHE_MODE_0, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
 
-	I915_WRITE(DSPCLK_GATE_D, OVRUNIT_CLOCK_GATE_DISABLE);
+	g4x_disable_trickle_feed(dev);
+
+	ibx_init_clock_gating(dev);
 }
 
-void intel_init_clock_gating(struct drm_device *dev)
+static void cpt_init_clock_gating(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
+	int pipe;
+	uint32_t val;
 
-	dev_priv->display.init_clock_gating(dev);
+	/*
+	 * On Ibex Peak and Cougar Point, we need to disable clock
+	 * gating for the panel power sequencer or it will fail to
+	 * start up when no ports are active.
+	 */
+	I915_WRITE(SOUTH_DSPCLK_GATE_D, PCH_DPLSUNIT_CLOCK_GATE_DISABLE |
+		   PCH_DPLUNIT_CLOCK_GATE_DISABLE |
+		   PCH_CPUNIT_CLOCK_GATE_DISABLE);
+	I915_WRITE(SOUTH_CHICKEN2, I915_READ(SOUTH_CHICKEN2) |
+		   DPLS_EDP_PPS_FIX_DIS);
+	/* The below fixes the weird display corruption, a few pixels shifted
+	 * downward, on (only) LVDS of some HP laptops with IVY.
+	 */
+	for_each_pipe(dev_priv, pipe) {
+		val = I915_READ(TRANS_CHICKEN2(pipe));
+		val |= TRANS_CHICKEN2_TIMING_OVERRIDE;
+		val &= ~TRANS_CHICKEN2_FDI_POLARITY_REVERSED;
+		if (dev_priv->vbt.fdi_rx_polarity_inverted)
+			val |= TRANS_CHICKEN2_FDI_POLARITY_REVERSED;
+		val &= ~TRANS_CHICKEN2_FRAME_START_DELAY_MASK;
+		val &= ~TRANS_CHICKEN2_DISABLE_DEEP_COLOR_COUNTER;
+		val &= ~TRANS_CHICKEN2_DISABLE_DEEP_COLOR_MODESWITCH;
+		I915_WRITE(TRANS_CHICKEN2(pipe), val);
+	}
+	/* WADP0ClockGatingDisable */
+	for_each_pipe(dev_priv, pipe) {
+		I915_WRITE(TRANS_CHICKEN1(pipe),
+			   TRANS_CHICKEN1_DP0UNIT_GC_DISABLE);
+	}
 }
 
-void intel_suspend_hw(struct drm_device *dev)
+static void gen6_check_mch_setup(struct drm_device *dev)
 {
-	if (HAS_PCH_LPT(dev))
-		lpt_suspend_hw(dev);
-}
-
-#define for_each_power_well(i, power_well, domain_mask, power_domains)	\
-	for (i = 0;							\
-	     i < (power_domains)->power_well_count &&			\
-		 ((power_well) = &(power_domains)->power_wells[i]);	\
-	     i++)							\
-		if ((power_well)->domains & (domain_mask))
-
-#define for_each_power_well_rev(i, power_well, domain_mask, power_domains) \
-	for (i = (power_domains)->power_well_count - 1;			 \
-	     i >= 0 && ((power_well) = &(power_domains)->power_wells[i]);\
-	     i--)							 \
-		if ((power_well)->domains & (domain_mask))
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	uint32_t tmp;
 
-/**
- * We should only use the power well if we explicitly asked the hardware to
- * enable it, so check if it's enabled and also check if we've requested it to
- * be enabled.
- */
-static bool hsw_power_well_enabled(struct drm_i915_private *dev_priv,
-				   struct i915_power_well *power_well)
-{
-	return I915_READ(HSW_PWR_WELL_DRIVER) ==
-		     (HSW_PWR_WELL_ENABLE_REQUEST | HSW_PWR_WELL_STATE_ENABLED);
+	tmp = I915_READ(MCH_SSKPD);
+	if ((tmp & MCH_SSKPD_WM0_MASK) != MCH_SSKPD_WM0_VAL)
+		DRM_DEBUG_KMS("Wrong MCH_SSKPD value: 0x%08x This can cause underruns.\n",
+			      tmp);
 }
 
-bool intel_display_power_enabled_unlocked(struct drm_i915_private *dev_priv,
-					  enum intel_display_power_domain domain)
+static void gen6_init_clock_gating(struct drm_device *dev)
 {
-	struct i915_power_domains *power_domains;
-	struct i915_power_well *power_well;
-	bool is_enabled;
-	int i;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	uint32_t dspclk_gate = ILK_VRHUNIT_CLOCK_GATE_DISABLE;
 
-	if (dev_priv->pm.suspended)
-		return false;
+	I915_WRITE(ILK_DSPCLK_GATE_D, dspclk_gate);
 
-	power_domains = &dev_priv->power_domains;
+	I915_WRITE(ILK_DISPLAY_CHICKEN2,
+		   I915_READ(ILK_DISPLAY_CHICKEN2) |
+		   ILK_ELPIN_409_SELECT);
 
-	is_enabled = true;
+	/* WaDisableHiZPlanesWhenMSAAEnabled:snb */
+	I915_WRITE(_3D_CHICKEN,
+		   _MASKED_BIT_ENABLE(_3D_CHICKEN_HIZ_PLANE_DISABLE_MSAA_4X_SNB));
 
-	for_each_power_well_rev(i, power_well, BIT(domain), power_domains) {
-		if (power_well->always_on)
-			continue;
+	/* WaDisable_RenderCache_OperationalFlush:snb */
+	I915_WRITE(CACHE_MODE_0, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
 
-		if (!power_well->hw_enabled) {
-			is_enabled = false;
-			break;
-		}
-	}
+	I915_WRITE(GEN6_GT_MODE, _MASKED_BIT_DISABLE(0xffff));
 
-	return is_enabled;
-}
+	/*
+	 * BSpec recoomends 8x4 when MSAA is used,
+	 * however in practice 16x4 seems fastest.
+	 *
+	 * Note that PS/WM thread counts depend on the WIZ hashing
+	 * disable bit, which we don't touch here, but it's good
+	 * to keep in mind (see 3DSTATE_PS and 3DSTATE_WM).
+	 */
+	I915_WRITE(GEN6_GT_MODE,
+		   GEN6_WIZ_HASHING_MASK | GEN6_WIZ_HASHING_16x4);
 
-bool intel_display_power_enabled(struct drm_i915_private *dev_priv,
-				 enum intel_display_power_domain domain)
-{
-	struct i915_power_domains *power_domains;
-	bool ret;
+	/* WaSetupGtModeTdRowDispatch:snb */
+	if (IS_SNB_GT1(dev))
+		I915_WRITE(GEN6_GT_MODE,
+			   _MASKED_BIT_ENABLE(GEN6_TD_FOUR_ROW_DISPATCH_DISABLE));
 
-	power_domains = &dev_priv->power_domains;
+	ilk_init_lp_watermarks(dev);
 
-	mutex_lock(&power_domains->lock);
-	ret = intel_display_power_enabled_unlocked(dev_priv, domain);
-	mutex_unlock(&power_domains->lock);
+	I915_WRITE(CACHE_MODE_0,
+		   _MASKED_BIT_DISABLE(CM0_STC_EVICT_DISABLE_LRA_SNB));
 
-	return ret;
-}
+	I915_WRITE(GEN6_UCGCTL1,
+		   I915_READ(GEN6_UCGCTL1) |
+		   GEN6_BLBUNIT_CLOCK_GATE_DISABLE |
+		   GEN6_CSUNIT_CLOCK_GATE_DISABLE);
 
-/*
- * Starting with Haswell, we have a "Power Down Well" that can be turned off
- * when not needed anymore. We have 4 registers that can request the power well
- * to be enabled, and it will only be disabled if none of the registers is
- * requesting it to be enabled.
- */
-static void hsw_power_well_post_enable(struct drm_i915_private *dev_priv)
-{
-	struct drm_device *dev = dev_priv->dev;
+	/* According to the BSpec vol1g, bit 12 (RCPBUNIT) clock
+	 * gating disable must be set.  Failure to set it results in
+	 * flickering pixels due to Z write ordering failures after
+	 * some amount of runtime in the Mesa "fire" demo, and Unigine
+	 * Sanctuary and Tropics, and apparently anything else with
+	 * alpha test or pixel discard.
+	 *
+	 * According to the spec, bit 11 (RCCUNIT) must also be set,
+	 * but we didn't debug actual testcases to find it out.
+	 *
+	 * WaDisableRCCUnitClockGating:snb
+	 * WaDisableRCPBUnitClockGating:snb
+	 */
+	I915_WRITE(GEN6_UCGCTL2,
+		   GEN6_RCPBUNIT_CLOCK_GATE_DISABLE |
+		   GEN6_RCCUNIT_CLOCK_GATE_DISABLE);
+
+	/* WaStripsFansDisableFastClipPerformanceFix:snb */
+	I915_WRITE(_3D_CHICKEN3,
+		   _MASKED_BIT_ENABLE(_3D_CHICKEN3_SF_DISABLE_FASTCLIP_CULL));
 
 	/*
-	 * After we re-enable the power well, if we touch VGA register 0x3d5
-	 * we'll get unclaimed register interrupts. This stops after we write
-	 * anything to the VGA MSR register. The vgacon module uses this
-	 * register all the time, so if we unbind our driver and, as a
-	 * consequence, bind vgacon, we'll get stuck in an infinite loop at
-	 * console_unlock(). So make here we touch the VGA MSR register, making
-	 * sure vgacon can keep working normally without triggering interrupts
-	 * and error messages.
-	 */
-	vga_get_uninterruptible(dev->pdev, VGA_RSRC_LEGACY_IO);
-	outb(inb(VGA_MSR_READ), VGA_MSR_WRITE);
-	vga_put(dev->pdev, VGA_RSRC_LEGACY_IO);
+	 * Bspec says:
+	 * "This bit must be set if 3DSTATE_CLIP clip mode is set to normal and
+	 * 3DSTATE_SF number of SF output attributes is more than 16."
+	 */
+	I915_WRITE(_3D_CHICKEN3,
+		   _MASKED_BIT_ENABLE(_3D_CHICKEN3_SF_DISABLE_PIPELINED_ATTR_FETCH));
 
-	if (IS_BROADWELL(dev))
-		gen8_irq_power_well_post_enable(dev_priv);
-}
+	/*
+	 * According to the spec the following bits should be
+	 * set in order to enable memory self-refresh and fbc:
+	 * The bit21 and bit22 of 0x42000
+	 * The bit21 and bit22 of 0x42004
+	 * The bit5 and bit7 of 0x42020
+	 * The bit14 of 0x70180
+	 * The bit14 of 0x71180
+	 *
+	 * WaFbcAsynchFlipDisableFbcQueue:snb
+	 */
+	I915_WRITE(ILK_DISPLAY_CHICKEN1,
+		   I915_READ(ILK_DISPLAY_CHICKEN1) |
+		   ILK_FBCQ_DIS | ILK_PABSTRETCH_DIS);
+	I915_WRITE(ILK_DISPLAY_CHICKEN2,
+		   I915_READ(ILK_DISPLAY_CHICKEN2) |
+		   ILK_DPARB_GATE | ILK_VSDPFD_FULL);
+	I915_WRITE(ILK_DSPCLK_GATE_D,
+		   I915_READ(ILK_DSPCLK_GATE_D) |
+		   ILK_DPARBUNIT_CLOCK_GATE_ENABLE  |
+		   ILK_DPFDUNIT_CLOCK_GATE_ENABLE);
 
-static void hsw_set_power_well(struct drm_i915_private *dev_priv,
-			       struct i915_power_well *power_well, bool enable)
-{
-	bool is_enabled, enable_requested;
-	uint32_t tmp;
+	g4x_disable_trickle_feed(dev);
 
-	tmp = I915_READ(HSW_PWR_WELL_DRIVER);
-	is_enabled = tmp & HSW_PWR_WELL_STATE_ENABLED;
-	enable_requested = tmp & HSW_PWR_WELL_ENABLE_REQUEST;
-
-	if (enable) {
-		if (!enable_requested)
-			I915_WRITE(HSW_PWR_WELL_DRIVER,
-				   HSW_PWR_WELL_ENABLE_REQUEST);
-
-		if (!is_enabled) {
-			DRM_DEBUG_KMS("Enabling power well\n");
-			if (wait_for((I915_READ(HSW_PWR_WELL_DRIVER) &
-				      HSW_PWR_WELL_STATE_ENABLED), 20))
-				DRM_ERROR("Timeout enabling power well\n");
-		}
+	cpt_init_clock_gating(dev);
 
-		hsw_power_well_post_enable(dev_priv);
-	} else {
-		if (enable_requested) {
-			I915_WRITE(HSW_PWR_WELL_DRIVER, 0);
-			POSTING_READ(HSW_PWR_WELL_DRIVER);
-			DRM_DEBUG_KMS("Requesting to disable the power well\n");
-		}
-	}
+	gen6_check_mch_setup(dev);
 }
 
-static void hsw_power_well_sync_hw(struct drm_i915_private *dev_priv,
-				   struct i915_power_well *power_well)
+static void gen7_setup_fixed_func_scheduler(struct drm_i915_private *dev_priv)
 {
-	hsw_set_power_well(dev_priv, power_well, power_well->count > 0);
+	uint32_t reg = I915_READ(GEN7_FF_THREAD_MODE);
 
 	/*
-	 * We're taking over the BIOS, so clear any requests made by it since
-	 * the driver is in charge now.
+	 * WaVSThreadDispatchOverride:ivb,vlv
+	 *
+	 * This actually overrides the dispatch
+	 * mode for all thread types.
 	 */
-	if (I915_READ(HSW_PWR_WELL_BIOS) & HSW_PWR_WELL_ENABLE_REQUEST)
-		I915_WRITE(HSW_PWR_WELL_BIOS, 0);
-}
-
-static void hsw_power_well_enable(struct drm_i915_private *dev_priv,
-				  struct i915_power_well *power_well)
-{
-	hsw_set_power_well(dev_priv, power_well, true);
-}
-
-static void hsw_power_well_disable(struct drm_i915_private *dev_priv,
-				   struct i915_power_well *power_well)
-{
-	hsw_set_power_well(dev_priv, power_well, false);
-}
-
-static void i9xx_always_on_power_well_noop(struct drm_i915_private *dev_priv,
-					   struct i915_power_well *power_well)
-{
-}
+	reg &= ~GEN7_FF_SCHED_MASK;
+	reg |= GEN7_FF_TS_SCHED_HW;
+	reg |= GEN7_FF_VS_SCHED_HW;
+	reg |= GEN7_FF_DS_SCHED_HW;
 
-static bool i9xx_always_on_power_well_enabled(struct drm_i915_private *dev_priv,
-					     struct i915_power_well *power_well)
-{
-	return true;
+	I915_WRITE(GEN7_FF_THREAD_MODE, reg);
 }
 
-static void vlv_set_power_well(struct drm_i915_private *dev_priv,
-			       struct i915_power_well *power_well, bool enable)
+static void lpt_init_clock_gating(struct drm_device *dev)
 {
-	enum punit_power_well power_well_id = power_well->data;
-	u32 mask;
-	u32 state;
-	u32 ctrl;
-
-	mask = PUNIT_PWRGT_MASK(power_well_id);
-	state = enable ? PUNIT_PWRGT_PWR_ON(power_well_id) :
-			 PUNIT_PWRGT_PWR_GATE(power_well_id);
-
-	mutex_lock(&dev_priv->rps.hw_lock);
-
-#define COND \
-	((vlv_punit_read(dev_priv, PUNIT_REG_PWRGT_STATUS) & mask) == state)
-
-	if (COND)
-		goto out;
-
-	ctrl = vlv_punit_read(dev_priv, PUNIT_REG_PWRGT_CTRL);
-	ctrl &= ~mask;
-	ctrl |= state;
-	vlv_punit_write(dev_priv, PUNIT_REG_PWRGT_CTRL, ctrl);
-
-	if (wait_for(COND, 100))
-		DRM_ERROR("timout setting power well state %08x (%08x)\n",
-			  state,
-			  vlv_punit_read(dev_priv, PUNIT_REG_PWRGT_CTRL));
+	struct drm_i915_private *dev_priv = dev->dev_private;
 
-#undef COND
+	/*
+	 * TODO: this bit should only be enabled when really needed, then
+	 * disabled when not needed anymore in order to save power.
+	 */
+	if (dev_priv->pch_id == INTEL_PCH_LPT_LP_DEVICE_ID_TYPE)
+		I915_WRITE(SOUTH_DSPCLK_GATE_D,
+			   I915_READ(SOUTH_DSPCLK_GATE_D) |
+			   PCH_LP_PARTITION_LEVEL_DISABLE);
 
-out:
-	mutex_unlock(&dev_priv->rps.hw_lock);
+	/* WADPOClockGatingDisable:hsw */
+	I915_WRITE(_TRANSA_CHICKEN1,
+		   I915_READ(_TRANSA_CHICKEN1) |
+		   TRANS_CHICKEN1_DP0UNIT_GC_DISABLE);
 }
 
-static void vlv_power_well_sync_hw(struct drm_i915_private *dev_priv,
-				   struct i915_power_well *power_well)
+static void lpt_suspend_hw(struct drm_device *dev)
 {
-	vlv_set_power_well(dev_priv, power_well, power_well->count > 0);
-}
+	struct drm_i915_private *dev_priv = dev->dev_private;
 
-static void vlv_power_well_enable(struct drm_i915_private *dev_priv,
-				  struct i915_power_well *power_well)
-{
-	vlv_set_power_well(dev_priv, power_well, true);
-}
+	if (dev_priv->pch_id == INTEL_PCH_LPT_LP_DEVICE_ID_TYPE) {
+		uint32_t val = I915_READ(SOUTH_DSPCLK_GATE_D);
 
-static void vlv_power_well_disable(struct drm_i915_private *dev_priv,
-				   struct i915_power_well *power_well)
-{
-	vlv_set_power_well(dev_priv, power_well, false);
+		val &= ~PCH_LP_PARTITION_LEVEL_DISABLE;
+		I915_WRITE(SOUTH_DSPCLK_GATE_D, val);
+	}
 }
 
-static bool vlv_power_well_enabled(struct drm_i915_private *dev_priv,
-				   struct i915_power_well *power_well)
+static void broadwell_init_clock_gating(struct drm_device *dev)
 {
-	int power_well_id = power_well->data;
-	bool enabled = false;
-	u32 mask;
-	u32 state;
-	u32 ctrl;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	enum pipe pipe;
 
-	mask = PUNIT_PWRGT_MASK(power_well_id);
-	ctrl = PUNIT_PWRGT_PWR_ON(power_well_id);
+	I915_WRITE(WM3_LP_ILK, 0);
+	I915_WRITE(WM2_LP_ILK, 0);
+	I915_WRITE(WM1_LP_ILK, 0);
 
-	mutex_lock(&dev_priv->rps.hw_lock);
+	/* WaSwitchSolVfFArbitrationPriority:bdw */
+	I915_WRITE(GAM_ECOCHK, I915_READ(GAM_ECOCHK) | HSW_ECOCHK_ARB_PRIO_SOL);
 
-	state = vlv_punit_read(dev_priv, PUNIT_REG_PWRGT_STATUS) & mask;
-	/*
-	 * We only ever set the power-on and power-gate states, anything
-	 * else is unexpected.
-	 */
-	WARN_ON(state != PUNIT_PWRGT_PWR_ON(power_well_id) &&
-		state != PUNIT_PWRGT_PWR_GATE(power_well_id));
-	if (state == ctrl)
-		enabled = true;
+	/* WaPsrDPAMaskVBlankInSRD:bdw */
+	I915_WRITE(CHICKEN_PAR1_1,
+		   I915_READ(CHICKEN_PAR1_1) | DPA_MASK_VBLANK_SRD);
 
-	/*
-	 * A transient state at this point would mean some unexpected party
-	 * is poking at the power controls too.
-	 */
-	ctrl = vlv_punit_read(dev_priv, PUNIT_REG_PWRGT_CTRL) & mask;
-	WARN_ON(ctrl != state);
+	/* WaPsrDPRSUnmaskVBlankInSRD:bdw */
+	for_each_pipe(dev_priv, pipe) {
+		I915_WRITE(CHICKEN_PIPESL_1(pipe),
+			   I915_READ(CHICKEN_PIPESL_1(pipe)) |
+			   BDW_DPRS_MASK_VBLANK_SRD);
+	}
 
-	mutex_unlock(&dev_priv->rps.hw_lock);
+	/* WaVSRefCountFullforceMissDisable:bdw */
+	/* WaDSRefCountFullforceMissDisable:bdw */
+	I915_WRITE(GEN7_FF_THREAD_MODE,
+		   I915_READ(GEN7_FF_THREAD_MODE) &
+		   ~(GEN8_FF_DS_REF_CNT_FFME | GEN7_FF_VS_REF_CNT_FFME));
 
-	return enabled;
-}
+	I915_WRITE(GEN6_RC_SLEEP_PSMI_CONTROL,
+		   _MASKED_BIT_ENABLE(GEN8_RC_SEMA_IDLE_MSG_DISABLE));
 
-static void vlv_display_power_well_enable(struct drm_i915_private *dev_priv,
-					  struct i915_power_well *power_well)
-{
-	WARN_ON_ONCE(power_well->data != PUNIT_POWER_WELL_DISP2D);
+	/* WaDisableSDEUnitClockGating:bdw */
+	I915_WRITE(GEN8_UCGCTL6, I915_READ(GEN8_UCGCTL6) |
+		   GEN8_SDEUNIT_CLOCK_GATE_DISABLE);
 
-	vlv_set_power_well(dev_priv, power_well, true);
+	lpt_init_clock_gating(dev);
+}
 
-	spin_lock_irq(&dev_priv->irq_lock);
-	valleyview_enable_display_irqs(dev_priv);
-	spin_unlock_irq(&dev_priv->irq_lock);
+static void haswell_init_clock_gating(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	/*
-	 * During driver initialization/resume we can avoid restoring the
-	 * part of the HW/SW state that will be inited anyway explicitly.
-	 */
-	if (dev_priv->power_domains.initializing)
-		return;
+	ilk_init_lp_watermarks(dev);
 
-	intel_hpd_init(dev_priv->dev);
+	/* L3 caching of data atomics doesn't work -- disable it. */
+	I915_WRITE(HSW_SCRATCH1, HSW_SCRATCH1_L3_DATA_ATOMICS_DISABLE);
+	I915_WRITE(HSW_ROW_CHICKEN3,
+		   _MASKED_BIT_ENABLE(HSW_ROW_CHICKEN3_L3_GLOBAL_ATOMICS_DISABLE));
 
-	i915_redisable_vga_power_on(dev_priv->dev);
-}
+	/* This is required by WaCatErrorRejectionIssue:hsw */
+	I915_WRITE(GEN7_SQ_CHICKEN_MBCUNIT_CONFIG,
+			I915_READ(GEN7_SQ_CHICKEN_MBCUNIT_CONFIG) |
+			GEN7_SQ_CHICKEN_MBCUNIT_SQINTMOB);
 
-static void vlv_display_power_well_disable(struct drm_i915_private *dev_priv,
-					   struct i915_power_well *power_well)
-{
-	WARN_ON_ONCE(power_well->data != PUNIT_POWER_WELL_DISP2D);
+	/* WaVSRefCountFullforceMissDisable:hsw */
+	I915_WRITE(GEN7_FF_THREAD_MODE,
+		   I915_READ(GEN7_FF_THREAD_MODE) & ~GEN7_FF_VS_REF_CNT_FFME);
 
-	spin_lock_irq(&dev_priv->irq_lock);
-	valleyview_disable_display_irqs(dev_priv);
-	spin_unlock_irq(&dev_priv->irq_lock);
+	/* WaDisable_RenderCache_OperationalFlush:hsw */
+	I915_WRITE(CACHE_MODE_0_GEN7, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
 
-	vlv_set_power_well(dev_priv, power_well, false);
-}
+	/* enable HiZ Raw Stall Optimization */
+	I915_WRITE(CACHE_MODE_0_GEN7,
+		   _MASKED_BIT_DISABLE(HIZ_RAW_STALL_OPT_DISABLE));
 
-static void vlv_dpio_cmn_power_well_enable(struct drm_i915_private *dev_priv,
-					   struct i915_power_well *power_well)
-{
-	WARN_ON_ONCE(power_well->data != PUNIT_POWER_WELL_DPIO_CMN_BC);
+	/* WaDisable4x2SubspanOptimization:hsw */
+	I915_WRITE(CACHE_MODE_1,
+		   _MASKED_BIT_ENABLE(PIXEL_SUBSPAN_COLLECT_OPT_DISABLE));
 
 	/*
-	 * Enable the CRI clock source so we can get at the
-	 * display and the reference clock for VGA
-	 * hotplug / manual detection.
+	 * BSpec recommends 8x4 when MSAA is used,
+	 * however in practice 16x4 seems fastest.
+	 *
+	 * Note that PS/WM thread counts depend on the WIZ hashing
+	 * disable bit, which we don't touch here, but it's good
+	 * to keep in mind (see 3DSTATE_PS and 3DSTATE_WM).
 	 */
-	I915_WRITE(DPLL(PIPE_B), I915_READ(DPLL(PIPE_B)) |
-		   DPLL_REFA_CLK_ENABLE_VLV | DPLL_INTEGRATED_CRI_CLK_VLV);
-	udelay(1); /* >10ns for cmnreset, >0ns for sidereset */
+	I915_WRITE(GEN7_GT_MODE,
+		   GEN6_WIZ_HASHING_MASK | GEN6_WIZ_HASHING_16x4);
+
+	/* WaSwitchSolVfFArbitrationPriority:hsw */
+	I915_WRITE(GAM_ECOCHK, I915_READ(GAM_ECOCHK) | HSW_ECOCHK_ARB_PRIO_SOL);
 
-	vlv_set_power_well(dev_priv, power_well, true);
+	/* WaRsPkgCStateDisplayPMReq:hsw */
+	I915_WRITE(CHICKEN_PAR1_1,
+		   I915_READ(CHICKEN_PAR1_1) | FORCE_ARB_IDLE_PLANES);
 
-	/*
-	 * From VLV2A0_DP_eDP_DPIO_driver_vbios_notes_10.docx -
-	 *  6.	De-assert cmn_reset/side_reset. Same as VLV X0.
-	 *   a.	GUnit 0x2110 bit[0] set to 1 (def 0)
-	 *   b.	The other bits such as sfr settings / modesel may all
-	 *	be set to 0.
-	 *
-	 * This should only be done on init and resume from S3 with
-	 * both PLLs disabled, or we risk losing DPIO and PLL
-	 * synchronization.
-	 */
-	I915_WRITE(DPIO_CTL, I915_READ(DPIO_CTL) | DPIO_CMNRST);
+	lpt_init_clock_gating(dev);
 }
 
-static void vlv_dpio_cmn_power_well_disable(struct drm_i915_private *dev_priv,
-					    struct i915_power_well *power_well)
+static void ivybridge_init_clock_gating(struct drm_device *dev)
 {
-	struct drm_device *dev = dev_priv->dev;
-	enum pipe pipe;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	uint32_t snpcr;
+
+	ilk_init_lp_watermarks(dev);
 
-	WARN_ON_ONCE(power_well->data != PUNIT_POWER_WELL_DPIO_CMN_BC);
+	I915_WRITE(ILK_DSPCLK_GATE_D, ILK_VRHUNIT_CLOCK_GATE_DISABLE);
 
-	for_each_pipe(pipe)
-		assert_pll_disabled(dev_priv, pipe);
+	/* WaDisableEarlyCull:ivb */
+	I915_WRITE(_3D_CHICKEN3,
+		   _MASKED_BIT_ENABLE(_3D_CHICKEN_SF_DISABLE_OBJEND_CULL));
 
-	/* Assert common reset */
-	I915_WRITE(DPIO_CTL, I915_READ(DPIO_CTL) & ~DPIO_CMNRST);
+	/* WaDisableBackToBackFlipFix:ivb */
+	I915_WRITE(IVB_CHICKEN3,
+		   CHICKEN3_DGMG_REQ_OUT_FIX_DISABLE |
+		   CHICKEN3_DGMG_DONE_FIX_DISABLE);
 
-	vlv_set_power_well(dev_priv, power_well, false);
-}
+	/* WaDisablePSDDualDispatchEnable:ivb */
+	if (IS_IVB_GT1(dev))
+		I915_WRITE(GEN7_HALF_SLICE_CHICKEN1,
+			   _MASKED_BIT_ENABLE(GEN7_PSD_SINGLE_PORT_DISPATCH_ENABLE));
 
-static void check_power_well_state(struct drm_i915_private *dev_priv,
-				   struct i915_power_well *power_well)
-{
-	bool enabled = power_well->ops->is_enabled(dev_priv, power_well);
+	/* WaDisable_RenderCache_OperationalFlush:ivb */
+	I915_WRITE(CACHE_MODE_0_GEN7, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
 
-	if (power_well->always_on || !i915.disable_power_well) {
-		if (!enabled)
-			goto mismatch;
+	/* Apply the WaDisableRHWOOptimizationForRenderHang:ivb workaround. */
+	I915_WRITE(GEN7_COMMON_SLICE_CHICKEN1,
+		   GEN7_CSC1_RHWO_OPT_DISABLE_IN_RCC);
 
-		return;
+	/* WaApplyL3ControlAndL3ChickenMode:ivb */
+	I915_WRITE(GEN7_L3CNTLREG1,
+			GEN7_WA_FOR_GEN7_L3_CONTROL);
+	I915_WRITE(GEN7_L3_CHICKEN_MODE_REGISTER,
+		   GEN7_WA_L3_CHICKEN_MODE);
+	if (IS_IVB_GT1(dev))
+		I915_WRITE(GEN7_ROW_CHICKEN2,
+			   _MASKED_BIT_ENABLE(DOP_CLOCK_GATING_DISABLE));
+	else {
+		/* must write both registers */
+		I915_WRITE(GEN7_ROW_CHICKEN2,
+			   _MASKED_BIT_ENABLE(DOP_CLOCK_GATING_DISABLE));
+		I915_WRITE(GEN7_ROW_CHICKEN2_GT2,
+			   _MASKED_BIT_ENABLE(DOP_CLOCK_GATING_DISABLE));
 	}
 
-	if (enabled != (power_well->count > 0))
-		goto mismatch;
+	/* WaForceL3Serialization:ivb */
+	I915_WRITE(GEN7_L3SQCREG4, I915_READ(GEN7_L3SQCREG4) &
+		   ~L3SQ_URB_READ_CAM_MATCH_DISABLE);
 
-	return;
+	/*
+	 * According to the spec, bit 13 (RCZUNIT) must be set on IVB.
+	 * This implements the WaDisableRCZUnitClockGating:ivb workaround.
+	 */
+	I915_WRITE(GEN6_UCGCTL2,
+		   GEN6_RCZUNIT_CLOCK_GATE_DISABLE);
 
-mismatch:
-	WARN(1, "state mismatch for '%s' (always_on %d hw state %d use-count %d disable_power_well %d\n",
-		  power_well->name, power_well->always_on, enabled,
-		  power_well->count, i915.disable_power_well);
-}
+	/* This is required by WaCatErrorRejectionIssue:ivb */
+	I915_WRITE(GEN7_SQ_CHICKEN_MBCUNIT_CONFIG,
+			I915_READ(GEN7_SQ_CHICKEN_MBCUNIT_CONFIG) |
+			GEN7_SQ_CHICKEN_MBCUNIT_SQINTMOB);
 
-void intel_display_power_get(struct drm_i915_private *dev_priv,
-			     enum intel_display_power_domain domain)
-{
-	struct i915_power_domains *power_domains;
-	struct i915_power_well *power_well;
-	int i;
+	g4x_disable_trickle_feed(dev);
 
-	intel_runtime_pm_get(dev_priv);
+	gen7_setup_fixed_func_scheduler(dev_priv);
 
-	power_domains = &dev_priv->power_domains;
+	if (0) { /* causes HiZ corruption on ivb:gt1 */
+		/* enable HiZ Raw Stall Optimization */
+		I915_WRITE(CACHE_MODE_0_GEN7,
+			   _MASKED_BIT_DISABLE(HIZ_RAW_STALL_OPT_DISABLE));
+	}
 
-	mutex_lock(&power_domains->lock);
+	/* WaDisable4x2SubspanOptimization:ivb */
+	I915_WRITE(CACHE_MODE_1,
+		   _MASKED_BIT_ENABLE(PIXEL_SUBSPAN_COLLECT_OPT_DISABLE));
 
-	for_each_power_well(i, power_well, BIT(domain), power_domains) {
-		if (!power_well->count++) {
-			DRM_DEBUG_KMS("enabling %s\n", power_well->name);
-			power_well->ops->enable(dev_priv, power_well);
-			power_well->hw_enabled = true;
-		}
+	/*
+	 * BSpec recommends 8x4 when MSAA is used,
+	 * however in practice 16x4 seems fastest.
+	 *
+	 * Note that PS/WM thread counts depend on the WIZ hashing
+	 * disable bit, which we don't touch here, but it's good
+	 * to keep in mind (see 3DSTATE_PS and 3DSTATE_WM).
+	 */
+	I915_WRITE(GEN7_GT_MODE,
+		   GEN6_WIZ_HASHING_MASK | GEN6_WIZ_HASHING_16x4);
 
-		check_power_well_state(dev_priv, power_well);
-	}
+	snpcr = I915_READ(GEN6_MBCUNIT_SNPCR);
+	snpcr &= ~GEN6_MBC_SNPCR_MASK;
+	snpcr |= GEN6_MBC_SNPCR_MED;
+	I915_WRITE(GEN6_MBCUNIT_SNPCR, snpcr);
 
-	power_domains->domain_use_count[domain]++;
+	if (!HAS_PCH_NOP(dev))
+		cpt_init_clock_gating(dev);
 
-	mutex_unlock(&power_domains->lock);
+	gen6_check_mch_setup(dev);
 }
 
-void intel_display_power_put(struct drm_i915_private *dev_priv,
-			     enum intel_display_power_domain domain)
+static void valleyview_init_clock_gating(struct drm_device *dev)
 {
-	struct i915_power_domains *power_domains;
-	struct i915_power_well *power_well;
-	int i;
+	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	power_domains = &dev_priv->power_domains;
+	I915_WRITE(DSPCLK_GATE_D, VRHUNIT_CLOCK_GATE_DISABLE);
 
-	mutex_lock(&power_domains->lock);
+	/* WaDisableEarlyCull:vlv */
+	I915_WRITE(_3D_CHICKEN3,
+		   _MASKED_BIT_ENABLE(_3D_CHICKEN_SF_DISABLE_OBJEND_CULL));
 
-	WARN_ON(!power_domains->domain_use_count[domain]);
-	power_domains->domain_use_count[domain]--;
+	/* WaDisableBackToBackFlipFix:vlv */
+	I915_WRITE(IVB_CHICKEN3,
+		   CHICKEN3_DGMG_REQ_OUT_FIX_DISABLE |
+		   CHICKEN3_DGMG_DONE_FIX_DISABLE);
 
-	for_each_power_well_rev(i, power_well, BIT(domain), power_domains) {
-		WARN_ON(!power_well->count);
+	/* WaPsdDispatchEnable:vlv */
+	/* WaDisablePSDDualDispatchEnable:vlv */
+	I915_WRITE(GEN7_HALF_SLICE_CHICKEN1,
+		   _MASKED_BIT_ENABLE(GEN7_MAX_PS_THREAD_DEP |
+				      GEN7_PSD_SINGLE_PORT_DISPATCH_ENABLE));
 
-		if (!--power_well->count && i915.disable_power_well) {
-			DRM_DEBUG_KMS("disabling %s\n", power_well->name);
-			power_well->hw_enabled = false;
-			power_well->ops->disable(dev_priv, power_well);
-		}
+	/* WaDisable_RenderCache_OperationalFlush:vlv */
+	I915_WRITE(CACHE_MODE_0_GEN7, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
 
-		check_power_well_state(dev_priv, power_well);
-	}
+	/* WaForceL3Serialization:vlv */
+	I915_WRITE(GEN7_L3SQCREG4, I915_READ(GEN7_L3SQCREG4) &
+		   ~L3SQ_URB_READ_CAM_MATCH_DISABLE);
 
-	mutex_unlock(&power_domains->lock);
+	/* WaDisableDopClockGating:vlv */
+	I915_WRITE(GEN7_ROW_CHICKEN2,
+		   _MASKED_BIT_ENABLE(DOP_CLOCK_GATING_DISABLE));
 
-	intel_runtime_pm_put(dev_priv);
-}
+	/* This is required by WaCatErrorRejectionIssue:vlv */
+	I915_WRITE(GEN7_SQ_CHICKEN_MBCUNIT_CONFIG,
+		   I915_READ(GEN7_SQ_CHICKEN_MBCUNIT_CONFIG) |
+		   GEN7_SQ_CHICKEN_MBCUNIT_SQINTMOB);
 
-static struct i915_power_domains *hsw_pwr;
+	gen7_setup_fixed_func_scheduler(dev_priv);
 
-/* Display audio driver power well request */
-int i915_request_power_well(void)
-{
-	struct drm_i915_private *dev_priv;
+	/*
+	 * According to the spec, bit 13 (RCZUNIT) must be set on IVB.
+	 * This implements the WaDisableRCZUnitClockGating:vlv workaround.
+	 */
+	I915_WRITE(GEN6_UCGCTL2,
+		   GEN6_RCZUNIT_CLOCK_GATE_DISABLE);
 
-	if (!hsw_pwr)
-		return -ENODEV;
+	/* WaDisableL3Bank2xClockGate:vlv
+	 * Disabling L3 clock gating- MMIO 940c[25] = 1
+	 * Set bit 25, to disable L3_BANK_2x_CLK_GATING */
+	I915_WRITE(GEN7_UCGCTL4,
+		   I915_READ(GEN7_UCGCTL4) | GEN7_L3BANK2X_CLOCK_GATE_DISABLE);
 
-	dev_priv = container_of(hsw_pwr, struct drm_i915_private,
-				power_domains);
-	intel_display_power_get(dev_priv, POWER_DOMAIN_AUDIO);
-	return 0;
-}
-EXPORT_SYMBOL_GPL(i915_request_power_well);
+	I915_WRITE(MI_ARB_VLV, MI_ARB_DISPLAY_TRICKLE_FEED_DISABLE);
 
-/* Display audio driver power well release */
-int i915_release_power_well(void)
-{
-	struct drm_i915_private *dev_priv;
+	/*
+	 * BSpec says this must be set, even though
+	 * WaDisable4x2SubspanOptimization isn't listed for VLV.
+	 */
+	I915_WRITE(CACHE_MODE_1,
+		   _MASKED_BIT_ENABLE(PIXEL_SUBSPAN_COLLECT_OPT_DISABLE));
 
-	if (!hsw_pwr)
-		return -ENODEV;
+	/*
+	 * WaIncreaseL3CreditsForVLVB0:vlv
+	 * This is the hardware default actually.
+	 */
+	I915_WRITE(GEN7_L3SQCREG1, VLV_B0_WA_L3SQCREG1_VALUE);
 
-	dev_priv = container_of(hsw_pwr, struct drm_i915_private,
-				power_domains);
-	intel_display_power_put(dev_priv, POWER_DOMAIN_AUDIO);
-	return 0;
+	/*
+	 * WaDisableVLVClockGating_VBIIssue:vlv
+	 * Disable clock gating on th GCFG unit to prevent a delay
+	 * in the reporting of vblank events.
+	 */
+	I915_WRITE(VLV_GUNIT_CLOCK_GATE, GCFG_DIS);
 }
-EXPORT_SYMBOL_GPL(i915_release_power_well);
 
-/*
- * Private interface for the audio driver to get CDCLK in kHz.
- *
- * Caller must request power well using i915_request_power_well() prior to
- * making the call.
- */
-int i915_get_cdclk_freq(void)
+static void cherryview_init_clock_gating(struct drm_device *dev)
 {
-	struct drm_i915_private *dev_priv;
-
-	if (!hsw_pwr)
-		return -ENODEV;
-
-	dev_priv = container_of(hsw_pwr, struct drm_i915_private,
-				power_domains);
-
-	return intel_ddi_get_cdclk_freq(dev_priv);
-}
-EXPORT_SYMBOL_GPL(i915_get_cdclk_freq);
-
-
-#define POWER_DOMAIN_MASK (BIT(POWER_DOMAIN_NUM) - 1)
-
-#define HSW_ALWAYS_ON_POWER_DOMAINS (			\
-	BIT(POWER_DOMAIN_PIPE_A) |			\
-	BIT(POWER_DOMAIN_TRANSCODER_EDP) |		\
-	BIT(POWER_DOMAIN_PORT_DDI_A_2_LANES) |		\
-	BIT(POWER_DOMAIN_PORT_DDI_A_4_LANES) |		\
-	BIT(POWER_DOMAIN_PORT_DDI_B_2_LANES) |		\
-	BIT(POWER_DOMAIN_PORT_DDI_B_4_LANES) |		\
-	BIT(POWER_DOMAIN_PORT_DDI_C_2_LANES) |		\
-	BIT(POWER_DOMAIN_PORT_DDI_C_4_LANES) |		\
-	BIT(POWER_DOMAIN_PORT_DDI_D_2_LANES) |		\
-	BIT(POWER_DOMAIN_PORT_DDI_D_4_LANES) |		\
-	BIT(POWER_DOMAIN_PORT_CRT) |			\
-	BIT(POWER_DOMAIN_PLLS) |			\
-	BIT(POWER_DOMAIN_INIT))
-#define HSW_DISPLAY_POWER_DOMAINS (				\
-	(POWER_DOMAIN_MASK & ~HSW_ALWAYS_ON_POWER_DOMAINS) |	\
-	BIT(POWER_DOMAIN_INIT))
-
-#define BDW_ALWAYS_ON_POWER_DOMAINS (			\
-	HSW_ALWAYS_ON_POWER_DOMAINS |			\
-	BIT(POWER_DOMAIN_PIPE_A_PANEL_FITTER))
-#define BDW_DISPLAY_POWER_DOMAINS (				\
-	(POWER_DOMAIN_MASK & ~BDW_ALWAYS_ON_POWER_DOMAINS) |	\
-	BIT(POWER_DOMAIN_INIT))
-
-#define VLV_ALWAYS_ON_POWER_DOMAINS	BIT(POWER_DOMAIN_INIT)
-#define VLV_DISPLAY_POWER_DOMAINS	POWER_DOMAIN_MASK
-
-#define VLV_DPIO_CMN_BC_POWER_DOMAINS (		\
-	BIT(POWER_DOMAIN_PORT_DDI_B_2_LANES) |	\
-	BIT(POWER_DOMAIN_PORT_DDI_B_4_LANES) |	\
-	BIT(POWER_DOMAIN_PORT_DDI_C_2_LANES) |	\
-	BIT(POWER_DOMAIN_PORT_DDI_C_4_LANES) |	\
-	BIT(POWER_DOMAIN_PORT_CRT) |		\
-	BIT(POWER_DOMAIN_INIT))
-
-#define VLV_DPIO_TX_B_LANES_01_POWER_DOMAINS (	\
-	BIT(POWER_DOMAIN_PORT_DDI_B_2_LANES) |	\
-	BIT(POWER_DOMAIN_PORT_DDI_B_4_LANES) |	\
-	BIT(POWER_DOMAIN_INIT))
-
-#define VLV_DPIO_TX_B_LANES_23_POWER_DOMAINS (	\
-	BIT(POWER_DOMAIN_PORT_DDI_B_4_LANES) |	\
-	BIT(POWER_DOMAIN_INIT))
-
-#define VLV_DPIO_TX_C_LANES_01_POWER_DOMAINS (	\
-	BIT(POWER_DOMAIN_PORT_DDI_C_2_LANES) |	\
-	BIT(POWER_DOMAIN_PORT_DDI_C_4_LANES) |	\
-	BIT(POWER_DOMAIN_INIT))
-
-#define VLV_DPIO_TX_C_LANES_23_POWER_DOMAINS (	\
-	BIT(POWER_DOMAIN_PORT_DDI_C_4_LANES) |	\
-	BIT(POWER_DOMAIN_INIT))
-
-static const struct i915_power_well_ops i9xx_always_on_power_well_ops = {
-	.sync_hw = i9xx_always_on_power_well_noop,
-	.enable = i9xx_always_on_power_well_noop,
-	.disable = i9xx_always_on_power_well_noop,
-	.is_enabled = i9xx_always_on_power_well_enabled,
-};
+	struct drm_i915_private *dev_priv = dev->dev_private;
 
-static struct i915_power_well i9xx_always_on_power_well[] = {
-	{
-		.name = "always-on",
-		.always_on = 1,
-		.domains = POWER_DOMAIN_MASK,
-		.ops = &i9xx_always_on_power_well_ops,
-	},
-};
+	I915_WRITE(DSPCLK_GATE_D, VRHUNIT_CLOCK_GATE_DISABLE);
 
-static const struct i915_power_well_ops hsw_power_well_ops = {
-	.sync_hw = hsw_power_well_sync_hw,
-	.enable = hsw_power_well_enable,
-	.disable = hsw_power_well_disable,
-	.is_enabled = hsw_power_well_enabled,
-};
+	I915_WRITE(MI_ARB_VLV, MI_ARB_DISPLAY_TRICKLE_FEED_DISABLE);
 
-static struct i915_power_well hsw_power_wells[] = {
-	{
-		.name = "always-on",
-		.always_on = 1,
-		.domains = HSW_ALWAYS_ON_POWER_DOMAINS,
-		.ops = &i9xx_always_on_power_well_ops,
-	},
-	{
-		.name = "display",
-		.domains = HSW_DISPLAY_POWER_DOMAINS,
-		.ops = &hsw_power_well_ops,
-	},
-};
+	/* WaVSRefCountFullforceMissDisable:chv */
+	/* WaDSRefCountFullforceMissDisable:chv */
+	I915_WRITE(GEN7_FF_THREAD_MODE,
+		   I915_READ(GEN7_FF_THREAD_MODE) &
+		   ~(GEN8_FF_DS_REF_CNT_FFME | GEN7_FF_VS_REF_CNT_FFME));
 
-static struct i915_power_well bdw_power_wells[] = {
-	{
-		.name = "always-on",
-		.always_on = 1,
-		.domains = BDW_ALWAYS_ON_POWER_DOMAINS,
-		.ops = &i9xx_always_on_power_well_ops,
-	},
-	{
-		.name = "display",
-		.domains = BDW_DISPLAY_POWER_DOMAINS,
-		.ops = &hsw_power_well_ops,
-	},
-};
+	/* WaDisableSemaphoreAndSyncFlipWait:chv */
+	I915_WRITE(GEN6_RC_SLEEP_PSMI_CONTROL,
+		   _MASKED_BIT_ENABLE(GEN8_RC_SEMA_IDLE_MSG_DISABLE));
 
-static const struct i915_power_well_ops vlv_display_power_well_ops = {
-	.sync_hw = vlv_power_well_sync_hw,
-	.enable = vlv_display_power_well_enable,
-	.disable = vlv_display_power_well_disable,
-	.is_enabled = vlv_power_well_enabled,
-};
+	/* WaDisableCSUnitClockGating:chv */
+	I915_WRITE(GEN6_UCGCTL1, I915_READ(GEN6_UCGCTL1) |
+		   GEN6_CSUNIT_CLOCK_GATE_DISABLE);
 
-static const struct i915_power_well_ops vlv_dpio_cmn_power_well_ops = {
-	.sync_hw = vlv_power_well_sync_hw,
-	.enable = vlv_dpio_cmn_power_well_enable,
-	.disable = vlv_dpio_cmn_power_well_disable,
-	.is_enabled = vlv_power_well_enabled,
-};
+	/* WaDisableSDEUnitClockGating:chv */
+	I915_WRITE(GEN8_UCGCTL6, I915_READ(GEN8_UCGCTL6) |
+		   GEN8_SDEUNIT_CLOCK_GATE_DISABLE);
+}
 
-static const struct i915_power_well_ops vlv_dpio_power_well_ops = {
-	.sync_hw = vlv_power_well_sync_hw,
-	.enable = vlv_power_well_enable,
-	.disable = vlv_power_well_disable,
-	.is_enabled = vlv_power_well_enabled,
-};
+static void g4x_init_clock_gating(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	uint32_t dspclk_gate;
 
-static struct i915_power_well vlv_power_wells[] = {
-	{
-		.name = "always-on",
-		.always_on = 1,
-		.domains = VLV_ALWAYS_ON_POWER_DOMAINS,
-		.ops = &i9xx_always_on_power_well_ops,
-	},
-	{
-		.name = "display",
-		.domains = VLV_DISPLAY_POWER_DOMAINS,
-		.data = PUNIT_POWER_WELL_DISP2D,
-		.ops = &vlv_display_power_well_ops,
-	},
-	{
-		.name = "dpio-tx-b-01",
-		.domains = VLV_DPIO_TX_B_LANES_01_POWER_DOMAINS |
-			   VLV_DPIO_TX_B_LANES_23_POWER_DOMAINS |
-			   VLV_DPIO_TX_C_LANES_01_POWER_DOMAINS |
-			   VLV_DPIO_TX_C_LANES_23_POWER_DOMAINS,
-		.ops = &vlv_dpio_power_well_ops,
-		.data = PUNIT_POWER_WELL_DPIO_TX_B_LANES_01,
-	},
-	{
-		.name = "dpio-tx-b-23",
-		.domains = VLV_DPIO_TX_B_LANES_01_POWER_DOMAINS |
-			   VLV_DPIO_TX_B_LANES_23_POWER_DOMAINS |
-			   VLV_DPIO_TX_C_LANES_01_POWER_DOMAINS |
-			   VLV_DPIO_TX_C_LANES_23_POWER_DOMAINS,
-		.ops = &vlv_dpio_power_well_ops,
-		.data = PUNIT_POWER_WELL_DPIO_TX_B_LANES_23,
-	},
-	{
-		.name = "dpio-tx-c-01",
-		.domains = VLV_DPIO_TX_B_LANES_01_POWER_DOMAINS |
-			   VLV_DPIO_TX_B_LANES_23_POWER_DOMAINS |
-			   VLV_DPIO_TX_C_LANES_01_POWER_DOMAINS |
-			   VLV_DPIO_TX_C_LANES_23_POWER_DOMAINS,
-		.ops = &vlv_dpio_power_well_ops,
-		.data = PUNIT_POWER_WELL_DPIO_TX_C_LANES_01,
-	},
-	{
-		.name = "dpio-tx-c-23",
-		.domains = VLV_DPIO_TX_B_LANES_01_POWER_DOMAINS |
-			   VLV_DPIO_TX_B_LANES_23_POWER_DOMAINS |
-			   VLV_DPIO_TX_C_LANES_01_POWER_DOMAINS |
-			   VLV_DPIO_TX_C_LANES_23_POWER_DOMAINS,
-		.ops = &vlv_dpio_power_well_ops,
-		.data = PUNIT_POWER_WELL_DPIO_TX_C_LANES_23,
-	},
-	{
-		.name = "dpio-common",
-		.domains = VLV_DPIO_CMN_BC_POWER_DOMAINS,
-		.data = PUNIT_POWER_WELL_DPIO_CMN_BC,
-		.ops = &vlv_dpio_cmn_power_well_ops,
-	},
-};
+	I915_WRITE(RENCLK_GATE_D1, 0);
+	I915_WRITE(RENCLK_GATE_D2, VF_UNIT_CLOCK_GATE_DISABLE |
+		   GS_UNIT_CLOCK_GATE_DISABLE |
+		   CL_UNIT_CLOCK_GATE_DISABLE);
+	I915_WRITE(RAMCLK_GATE_D, 0);
+	dspclk_gate = VRHUNIT_CLOCK_GATE_DISABLE |
+		OVRUNIT_CLOCK_GATE_DISABLE |
+		OVCUNIT_CLOCK_GATE_DISABLE;
+	if (IS_GM45(dev))
+		dspclk_gate |= DSSUNIT_CLOCK_GATE_DISABLE;
+	I915_WRITE(DSPCLK_GATE_D, dspclk_gate);
 
-static struct i915_power_well *lookup_power_well(struct drm_i915_private *dev_priv,
-						 enum punit_power_well power_well_id)
-{
-	struct i915_power_domains *power_domains = &dev_priv->power_domains;
-	struct i915_power_well *power_well;
-	int i;
+	/* WaDisableRenderCachePipelinedFlush */
+	I915_WRITE(CACHE_MODE_0,
+		   _MASKED_BIT_ENABLE(CM0_PIPELINED_RENDER_FLUSH_DISABLE));
 
-	for_each_power_well(i, power_well, POWER_DOMAIN_MASK, power_domains) {
-		if (power_well->data == power_well_id)
-			return power_well;
-	}
+	/* WaDisable_RenderCache_OperationalFlush:g4x */
+	I915_WRITE(CACHE_MODE_0, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
 
-	return NULL;
+	g4x_disable_trickle_feed(dev);
 }
 
-#define set_power_wells(power_domains, __power_wells) ({		\
-	(power_domains)->power_wells = (__power_wells);			\
-	(power_domains)->power_well_count = ARRAY_SIZE(__power_wells);	\
-})
-
-int intel_power_domains_init(struct drm_i915_private *dev_priv)
+static void crestline_init_clock_gating(struct drm_device *dev)
 {
-	struct i915_power_domains *power_domains = &dev_priv->power_domains;
-
-	mutex_init(&power_domains->lock);
+	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	/*
-	 * The enabling order will be from lower to higher indexed wells,
-	 * the disabling order is reversed.
-	 */
-	if (IS_HASWELL(dev_priv->dev)) {
-		set_power_wells(power_domains, hsw_power_wells);
-		hsw_pwr = power_domains;
-	} else if (IS_BROADWELL(dev_priv->dev)) {
-		set_power_wells(power_domains, bdw_power_wells);
-		hsw_pwr = power_domains;
-	} else if (IS_VALLEYVIEW(dev_priv->dev)) {
-		set_power_wells(power_domains, vlv_power_wells);
-	} else {
-		set_power_wells(power_domains, i9xx_always_on_power_well);
-	}
+	I915_WRITE(RENCLK_GATE_D1, I965_RCC_CLOCK_GATE_DISABLE);
+	I915_WRITE(RENCLK_GATE_D2, 0);
+	I915_WRITE(DSPCLK_GATE_D, 0);
+	I915_WRITE(RAMCLK_GATE_D, 0);
+	I915_WRITE16(DEUC, 0);
+	I915_WRITE(MI_ARB_STATE,
+		   _MASKED_BIT_ENABLE(MI_ARB_DISPLAY_TRICKLE_FEED_DISABLE));
 
-	return 0;
+	/* WaDisable_RenderCache_OperationalFlush:gen4 */
+	I915_WRITE(CACHE_MODE_0, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
 }
 
-void intel_power_domains_remove(struct drm_i915_private *dev_priv)
+static void broadwater_init_clock_gating(struct drm_device *dev)
 {
-	hsw_pwr = NULL;
-}
+	struct drm_i915_private *dev_priv = dev->dev_private;
 
-static void intel_power_domains_resume(struct drm_i915_private *dev_priv)
-{
-	struct i915_power_domains *power_domains = &dev_priv->power_domains;
-	struct i915_power_well *power_well;
-	int i;
+	I915_WRITE(RENCLK_GATE_D1, I965_RCZ_CLOCK_GATE_DISABLE |
+		   I965_RCC_CLOCK_GATE_DISABLE |
+		   I965_RCPB_CLOCK_GATE_DISABLE |
+		   I965_ISC_CLOCK_GATE_DISABLE |
+		   I965_FBC_CLOCK_GATE_DISABLE);
+	I915_WRITE(RENCLK_GATE_D2, 0);
+	I915_WRITE(MI_ARB_STATE,
+		   _MASKED_BIT_ENABLE(MI_ARB_DISPLAY_TRICKLE_FEED_DISABLE));
 
-	mutex_lock(&power_domains->lock);
-	for_each_power_well(i, power_well, POWER_DOMAIN_MASK, power_domains) {
-		power_well->ops->sync_hw(dev_priv, power_well);
-		power_well->hw_enabled = power_well->ops->is_enabled(dev_priv,
-								     power_well);
-	}
-	mutex_unlock(&power_domains->lock);
+	/* WaDisable_RenderCache_OperationalFlush:gen4 */
+	I915_WRITE(CACHE_MODE_0, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
 }
 
-static void vlv_cmnlane_wa(struct drm_i915_private *dev_priv)
+static void gen3_init_clock_gating(struct drm_device *dev)
 {
-	struct i915_power_well *cmn =
-		lookup_power_well(dev_priv, PUNIT_POWER_WELL_DPIO_CMN_BC);
-	struct i915_power_well *disp2d =
-		lookup_power_well(dev_priv, PUNIT_POWER_WELL_DISP2D);
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	u32 dstate = I915_READ(D_STATE);
 
-	/* nothing to do if common lane is already off */
-	if (!cmn->ops->is_enabled(dev_priv, cmn))
-		return;
+	dstate |= DSTATE_PLL_D3_OFF | DSTATE_GFX_CLOCK_GATING |
+		DSTATE_DOT_CLOCK_GATING;
+	I915_WRITE(D_STATE, dstate);
 
-	/* If the display might be already active skip this */
-	if (disp2d->ops->is_enabled(dev_priv, disp2d) &&
-	    I915_READ(DPIO_CTL) & DPIO_CMNRST)
-		return;
+	if (IS_PINEVIEW(dev))
+		I915_WRITE(ECOSKPD, _MASKED_BIT_ENABLE(ECO_GATING_CX_ONLY));
+
+	/* IIR "flip pending" means done if this bit is set */
+	I915_WRITE(ECOSKPD, _MASKED_BIT_DISABLE(ECO_FLIP_DONE));
 
-	DRM_DEBUG_KMS("toggling display PHY side reset\n");
+	/* interrupts should cause a wake up from C3 */
+	I915_WRITE(INSTPM, _MASKED_BIT_ENABLE(INSTPM_AGPBUSY_INT_EN));
 
-	/* cmnlane needs DPLL registers */
-	disp2d->ops->enable(dev_priv, disp2d);
+	/* On GEN3 we really need to make sure the ARB C3 LP bit is set */
+	I915_WRITE(MI_ARB_STATE, _MASKED_BIT_ENABLE(MI_ARB_C3_LP_WRITE_ENABLE));
 
-	/*
-	 * From VLV2A0_DP_eDP_HDMI_DPIO_driver_vbios_notes_11.docx:
-	 * Need to assert and de-assert PHY SB reset by gating the
-	 * common lane power, then un-gating it.
-	 * Simply ungating isn't enough to reset the PHY enough to get
-	 * ports and lanes running.
-	 */
-	cmn->ops->disable(dev_priv, cmn);
+	I915_WRITE(MI_ARB_STATE,
+		   _MASKED_BIT_ENABLE(MI_ARB_DISPLAY_TRICKLE_FEED_DISABLE));
 }
 
-void intel_power_domains_init_hw(struct drm_i915_private *dev_priv)
+static void i85x_init_clock_gating(struct drm_device *dev)
 {
-	struct drm_device *dev = dev_priv->dev;
-	struct i915_power_domains *power_domains = &dev_priv->power_domains;
-
-	power_domains->initializing = true;
-
-	if (IS_VALLEYVIEW(dev) && !IS_CHERRYVIEW(dev)) {
-		mutex_lock(&power_domains->lock);
-		vlv_cmnlane_wa(dev_priv);
-		mutex_unlock(&power_domains->lock);
-	}
+	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	/* For now, we need the power well to be always enabled. */
-	intel_display_set_init_power(dev_priv, true);
-	intel_power_domains_resume(dev_priv);
-	power_domains->initializing = false;
-}
+	I915_WRITE(RENCLK_GATE_D1, SV_CLOCK_GATE_DISABLE);
 
-void intel_aux_display_runtime_get(struct drm_i915_private *dev_priv)
-{
-	intel_runtime_pm_get(dev_priv);
-}
+	/* interrupts should cause a wake up from C3 */
+	I915_WRITE(MI_STATE, _MASKED_BIT_ENABLE(MI_AGPBUSY_INT_EN) |
+		   _MASKED_BIT_DISABLE(MI_AGPBUSY_830_MODE));
 
-void intel_aux_display_runtime_put(struct drm_i915_private *dev_priv)
-{
-	intel_runtime_pm_put(dev_priv);
+	I915_WRITE(MEM_MODE,
+		   _MASKED_BIT_ENABLE(MEM_DISPLAY_TRICKLE_FEED_DISABLE));
 }
 
-void intel_runtime_pm_get(struct drm_i915_private *dev_priv)
+static void i830_init_clock_gating(struct drm_device *dev)
 {
-	struct drm_device *dev = dev_priv->dev;
-	struct device *device = &dev->pdev->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	if (!HAS_RUNTIME_PM(dev))
-		return;
+	I915_WRITE(DSPCLK_GATE_D, OVRUNIT_CLOCK_GATE_DISABLE);
 
-	pm_runtime_get_sync(device);
-	WARN(dev_priv->pm.suspended, "Device still suspended.\n");
+	I915_WRITE(MEM_MODE,
+		   _MASKED_BIT_ENABLE(MEM_DISPLAY_A_TRICKLE_FEED_DISABLE) |
+		   _MASKED_BIT_ENABLE(MEM_DISPLAY_B_TRICKLE_FEED_DISABLE));
 }
 
-void intel_runtime_pm_get_noresume(struct drm_i915_private *dev_priv)
+void intel_init_clock_gating(struct drm_device *dev)
 {
-	struct drm_device *dev = dev_priv->dev;
-	struct device *device = &dev->pdev->dev;
-
-	if (!HAS_RUNTIME_PM(dev))
-		return;
+	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	WARN(dev_priv->pm.suspended, "Getting nosync-ref while suspended.\n");
-	pm_runtime_get_noresume(device);
+	dev_priv->display.init_clock_gating(dev);
 }
 
-void intel_runtime_pm_put(struct drm_i915_private *dev_priv)
+void intel_suspend_hw(struct drm_device *dev)
 {
-	struct drm_device *dev = dev_priv->dev;
-	struct device *device = &dev->pdev->dev;
-
-	if (!HAS_RUNTIME_PM(dev))
-		return;
-
-	pm_runtime_mark_last_busy(device);
-	pm_runtime_put_autosuspend(device);
+	if (HAS_PCH_LPT(dev))
+		lpt_suspend_hw(dev);
 }
 
-void intel_init_runtime_pm(struct drm_i915_private *dev_priv)
+static void intel_init_fbc(struct drm_i915_private *dev_priv)
 {
-	struct drm_device *dev = dev_priv->dev;
-	struct device *device = &dev->pdev->dev;
-
-	if (!HAS_RUNTIME_PM(dev))
-		return;
-
-	pm_runtime_set_active(device);
-
-	/*
-	 * RPM depends on RC6 to save restore the GT HW context, so make RC6 a
-	 * requirement.
-	 */
-	if (!intel_enable_rc6(dev)) {
-		DRM_INFO("RC6 disabled, disabling runtime PM support\n");
+	if (!HAS_FBC(dev_priv)) {
+		dev_priv->fbc.enabled = false;
 		return;
 	}
 
-	pm_runtime_set_autosuspend_delay(device, 10000); /* 10s */
-	pm_runtime_mark_last_busy(device);
-	pm_runtime_use_autosuspend(device);
-
-	pm_runtime_put_autosuspend(device);
-}
-
-void intel_fini_runtime_pm(struct drm_i915_private *dev_priv)
-{
-	struct drm_device *dev = dev_priv->dev;
-	struct device *device = &dev->pdev->dev;
-
-	if (!HAS_RUNTIME_PM(dev))
-		return;
+	if (INTEL_INFO(dev_priv)->gen >= 7) {
+		dev_priv->display.fbc_enabled = ironlake_fbc_enabled;
+		dev_priv->display.enable_fbc = gen7_enable_fbc;
+		dev_priv->display.disable_fbc = ironlake_disable_fbc;
+	} else if (INTEL_INFO(dev_priv)->gen >= 5) {
+		dev_priv->display.fbc_enabled = ironlake_fbc_enabled;
+		dev_priv->display.enable_fbc = ironlake_enable_fbc;
+		dev_priv->display.disable_fbc = ironlake_disable_fbc;
+	} else if (IS_GM45(dev_priv)) {
+		dev_priv->display.fbc_enabled = g4x_fbc_enabled;
+		dev_priv->display.enable_fbc = g4x_enable_fbc;
+		dev_priv->display.disable_fbc = g4x_disable_fbc;
+	} else {
+		dev_priv->display.fbc_enabled = i8xx_fbc_enabled;
+		dev_priv->display.enable_fbc = i8xx_enable_fbc;
+		dev_priv->display.disable_fbc = i8xx_disable_fbc;
 
-	if (!intel_enable_rc6(dev))
-		return;
+		/* This value was pulled out of someone's hat */
+		I915_WRITE(FBC_CONTROL, 500 << FBC_CTL_INTERVAL_SHIFT);
+	}
 
-	/* Make sure we're not suspended first. */
-	pm_runtime_get_sync(device);
-	pm_runtime_disable(device);
+	dev_priv->fbc.enabled = dev_priv->display.fbc_enabled(dev_priv->dev);
 }
 
 /* Set up chip specific power management-related functions */
@@ -6780,28 +7049,7 @@
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 
-	if (HAS_FBC(dev)) {
-		if (INTEL_INFO(dev)->gen >= 7) {
-			dev_priv->display.fbc_enabled = ironlake_fbc_enabled;
-			dev_priv->display.enable_fbc = gen7_enable_fbc;
-			dev_priv->display.disable_fbc = ironlake_disable_fbc;
-		} else if (INTEL_INFO(dev)->gen >= 5) {
-			dev_priv->display.fbc_enabled = ironlake_fbc_enabled;
-			dev_priv->display.enable_fbc = ironlake_enable_fbc;
-			dev_priv->display.disable_fbc = ironlake_disable_fbc;
-		} else if (IS_GM45(dev)) {
-			dev_priv->display.fbc_enabled = g4x_fbc_enabled;
-			dev_priv->display.enable_fbc = g4x_enable_fbc;
-			dev_priv->display.disable_fbc = g4x_disable_fbc;
-		} else {
-			dev_priv->display.fbc_enabled = i8xx_fbc_enabled;
-			dev_priv->display.enable_fbc = i8xx_enable_fbc;
-			dev_priv->display.disable_fbc = i8xx_disable_fbc;
-
-			/* This value was pulled out of someone's hat */
-			I915_WRITE(FBC_CONTROL, 500 << FBC_CTL_INTERVAL_SHIFT);
-		}
-	}
+	intel_init_fbc(dev_priv);
 
 	/* For cxsr */
 	if (IS_PINEVIEW(dev))
@@ -6810,7 +7058,13 @@
 		i915_ironlake_get_mem_freq(dev);
 
 	/* For FIFO watermark updates */
-	if (HAS_PCH_SPLIT(dev)) {
+	if (INTEL_INFO(dev)->gen >= 9) {
+		skl_setup_wm_latency(dev);
+
+		dev_priv->display.init_clock_gating = gen9_init_clock_gating;
+		dev_priv->display.update_wm = skl_update_wm;
+		dev_priv->display.update_sprite_wm = skl_update_sprite_wm;
+	} else if (HAS_PCH_SPLIT(dev)) {
 		ilk_setup_wm_latency(dev);
 
 		if ((IS_GEN5(dev) && dev_priv->wm.pri_latency[1] &&
@@ -6833,13 +7087,15 @@
 		else if (IS_HASWELL(dev))
 			dev_priv->display.init_clock_gating = haswell_init_clock_gating;
 		else if (INTEL_INFO(dev)->gen == 8)
-			dev_priv->display.init_clock_gating = gen8_init_clock_gating;
+			dev_priv->display.init_clock_gating = broadwell_init_clock_gating;
 	} else if (IS_CHERRYVIEW(dev)) {
-		dev_priv->display.update_wm = valleyview_update_wm;
+		dev_priv->display.update_wm = cherryview_update_wm;
+		dev_priv->display.update_sprite_wm = valleyview_update_sprite_wm;
 		dev_priv->display.init_clock_gating =
 			cherryview_init_clock_gating;
 	} else if (IS_VALLEYVIEW(dev)) {
 		dev_priv->display.update_wm = valleyview_update_wm;
+		dev_priv->display.update_sprite_wm = valleyview_update_sprite_wm;
 		dev_priv->display.init_clock_gating =
 			valleyview_init_clock_gating;
 	} else if (IS_PINEVIEW(dev)) {
@@ -6889,7 +7145,7 @@
 	}
 }
 
-int sandybridge_pcode_read(struct drm_i915_private *dev_priv, u8 mbox, u32 *val)
+int sandybridge_pcode_read(struct drm_i915_private *dev_priv, u32 mbox, u32 *val)
 {
 	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
 
@@ -6899,6 +7155,7 @@
 	}
 
 	I915_WRITE(GEN6_PCODE_DATA, *val);
+	I915_WRITE(GEN6_PCODE_DATA1, 0);
 	I915_WRITE(GEN6_PCODE_MAILBOX, GEN6_PCODE_READY | mbox);
 
 	if (wait_for((I915_READ(GEN6_PCODE_MAILBOX) & GEN6_PCODE_READY) == 0,
@@ -6913,7 +7170,7 @@
 	return 0;
 }
 
-int sandybridge_pcode_write(struct drm_i915_private *dev_priv, u8 mbox, u32 val)
+int sandybridge_pcode_write(struct drm_i915_private *dev_priv, u32 mbox, u32 val)
 {
 	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
 
@@ -6936,98 +7193,66 @@
 	return 0;
 }
 
-static int byt_gpu_freq(struct drm_i915_private *dev_priv, int val)
+static int vlv_gpu_freq_div(unsigned int czclk_freq)
 {
-	int div;
-
-	/* 4 x czclk */
-	switch (dev_priv->mem_freq) {
-	case 800:
-		div = 10;
-		break;
-	case 1066:
-		div = 12;
-		break;
-	case 1333:
-		div = 16;
-		break;
+	switch (czclk_freq) {
+	case 200:
+		return 10;
+	case 267:
+		return 12;
+	case 320:
+	case 333:
+		return 16;
+	case 400:
+		return 20;
 	default:
 		return -1;
 	}
+}
+
+static int byt_gpu_freq(struct drm_i915_private *dev_priv, int val)
+{
+	int div, czclk_freq = DIV_ROUND_CLOSEST(dev_priv->mem_freq, 4);
 
-	return DIV_ROUND_CLOSEST(dev_priv->mem_freq * (val + 6 - 0xbd), 4 * div);
+	div = vlv_gpu_freq_div(czclk_freq);
+	if (div < 0)
+		return div;
+
+	return DIV_ROUND_CLOSEST(czclk_freq * (val + 6 - 0xbd), div);
 }
 
 static int byt_freq_opcode(struct drm_i915_private *dev_priv, int val)
 {
-	int mul;
+	int mul, czclk_freq = DIV_ROUND_CLOSEST(dev_priv->mem_freq, 4);
 
-	/* 4 x czclk */
-	switch (dev_priv->mem_freq) {
-	case 800:
-		mul = 10;
-		break;
-	case 1066:
-		mul = 12;
-		break;
-	case 1333:
-		mul = 16;
-		break;
-	default:
-		return -1;
-	}
+	mul = vlv_gpu_freq_div(czclk_freq);
+	if (mul < 0)
+		return mul;
 
-	return DIV_ROUND_CLOSEST(4 * mul * val, dev_priv->mem_freq) + 0xbd - 6;
+	return DIV_ROUND_CLOSEST(mul * val, czclk_freq) + 0xbd - 6;
 }
 
 static int chv_gpu_freq(struct drm_i915_private *dev_priv, int val)
 {
-	int div, freq;
-
-	switch (dev_priv->rps.cz_freq) {
-	case 200:
-		div = 5;
-		break;
-	case 267:
-		div = 6;
-		break;
-	case 320:
-	case 333:
-	case 400:
-		div = 8;
-		break;
-	default:
-		return -1;
-	}
+	int div, czclk_freq = dev_priv->rps.cz_freq;
 
-	freq = (DIV_ROUND_CLOSEST((dev_priv->rps.cz_freq * val), 2 * div) / 2);
+	div = vlv_gpu_freq_div(czclk_freq) / 2;
+	if (div < 0)
+		return div;
 
-	return freq;
+	return DIV_ROUND_CLOSEST(czclk_freq * val, 2 * div) / 2;
 }
 
 static int chv_freq_opcode(struct drm_i915_private *dev_priv, int val)
 {
-	int mul, opcode;
-
-	switch (dev_priv->rps.cz_freq) {
-	case 200:
-		mul = 5;
-		break;
-	case 267:
-		mul = 6;
-		break;
-	case 320:
-	case 333:
-	case 400:
-		mul = 8;
-		break;
-	default:
-		return -1;
-	}
+	int mul, czclk_freq = dev_priv->rps.cz_freq;
 
-	opcode = (DIV_ROUND_CLOSEST((val * 2 * mul), dev_priv->rps.cz_freq) * 2);
+	mul = vlv_gpu_freq_div(czclk_freq) / 2;
+	if (mul < 0)
+		return mul;
 
-	return opcode;
+	/* CHV needs even values */
+	return DIV_ROUND_CLOSEST(val * 2 * mul, czclk_freq) * 2;
 }
 
 int vlv_gpu_freq(struct drm_i915_private *dev_priv, int val)
@@ -7054,6 +7279,40 @@
 	return ret;
 }
 
+struct request_boost {
+	struct work_struct work;
+	struct i915_gem_request *rq;
+};
+
+static void __intel_rps_boost_work(struct work_struct *work)
+{
+	struct request_boost *boost = container_of(work, struct request_boost, work);
+
+	if (!i915_request_complete(boost->rq))
+		gen6_rps_boost(boost->rq->i915, NULL);
+
+	i915_request_put__unlocked(boost->rq);
+	kfree(boost);
+}
+
+void intel_queue_rps_boost_for_request(struct drm_device *dev,
+				       struct i915_gem_request *rq)
+{
+	struct request_boost *boost;
+
+	if (rq == NULL || INTEL_INFO(dev)->gen < 6)
+		return;
+
+	boost = kmalloc(sizeof(*boost), GFP_ATOMIC);
+	if (boost == NULL)
+		return;
+
+	INIT_WORK(&boost->work, __intel_rps_boost_work);
+	boost->rq = i915_request_get(rq);
+
+	queue_work(to_i915(dev)->wq, &boost->work);
+}
+
 void intel_pm_setup(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -7062,7 +7321,7 @@
 
 	INIT_DELAYED_WORK(&dev_priv->rps.delayed_resume_work,
 			  intel_gen6_powersave_work);
+	INIT_LIST_HEAD(&dev_priv->rps.clients);
 
 	dev_priv->pm.suspended = false;
-	dev_priv->pm._irqs_disabled = false;
 }
diff -urN a/drivers/gpu/drm/i915/intel_psr.c b/drivers/gpu/drm/i915/intel_psr.c
--- a/drivers/gpu/drm/i915/intel_psr.c	1969-12-31 17:00:00.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_psr.c	2014-11-22 14:37:49.342700417 -0700
@@ -0,0 +1,481 @@
+/*
+ * Copyright  2014 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
+ */
+
+/**
+ * DOC: Panel Self Refresh (PSR/SRD)
+ *
+ * Since Haswell Display controller supports Panel Self-Refresh on display
+ * panels witch have a remote frame buffer (RFB) implemented according to PSR
+ * spec in eDP1.3. PSR feature allows the display to go to lower standby states
+ * when system is idle but display is on as it eliminates display refresh
+ * request to DDR memory completely as long as the frame buffer for that
+ * display is unchanged.
+ *
+ * Panel Self Refresh must be supported by both Hardware (source) and
+ * Panel (sink).
+ *
+ * PSR saves power by caching the framebuffer in the panel RFB, which allows us
+ * to power down the link and memory controller. For DSI panels the same idea
+ * is called "manual mode".
+ *
+ * The implementation uses the hardware-based PSR support which automatically
+ * enters/exits self-refresh mode. The hardware takes care of sending the
+ * required DP aux message and could even retrain the link (that part isn't
+ * enabled yet though). The hardware also keeps track of any frontbuffer
+ * changes to know when to exit self-refresh mode again. Unfortunately that
+ * part doesn't work too well, hence why the i915 PSR support uses the
+ * software frontbuffer tracking to make sure it doesn't miss a screen
+ * update. For this integration intel_psr_invalidate() and intel_psr_flush()
+ * get called by the frontbuffer tracking code. Note that because of locking
+ * issues the self-refresh re-enable code is done from a work queue, which
+ * must be correctly synchronized/cancelled when shutting down the pipe."
+ */
+
+#include <drm/drmP.h>
+
+#include "intel_drv.h"
+#include "i915_drv.h"
+
+static bool is_edp_psr(struct intel_dp *intel_dp)
+{
+	return intel_dp->psr_dpcd[0] & DP_PSR_IS_SUPPORTED;
+}
+
+bool intel_psr_is_enabled(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	if (!HAS_PSR(dev))
+		return false;
+
+	return I915_READ(EDP_PSR_CTL(dev)) & EDP_PSR_ENABLE;
+}
+
+static void intel_psr_write_vsc(struct intel_dp *intel_dp,
+				    struct edp_vsc_psr *vsc_psr)
+{
+	struct intel_digital_port *dig_port = dp_to_dig_port(intel_dp);
+	struct drm_device *dev = dig_port->base.base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *crtc = to_intel_crtc(dig_port->base.base.crtc);
+	u32 ctl_reg = HSW_TVIDEO_DIP_CTL(crtc->config.cpu_transcoder);
+	u32 data_reg = HSW_TVIDEO_DIP_VSC_DATA(crtc->config.cpu_transcoder);
+	uint32_t *data = (uint32_t *) vsc_psr;
+	unsigned int i;
+
+	/* As per BSPec (Pipe Video Data Island Packet), we need to disable
+	   the video DIP being updated before program video DIP data buffer
+	   registers for DIP being updated. */
+	I915_WRITE(ctl_reg, 0);
+	POSTING_READ(ctl_reg);
+
+	for (i = 0; i < VIDEO_DIP_VSC_DATA_SIZE; i += 4) {
+		if (i < sizeof(struct edp_vsc_psr))
+			I915_WRITE(data_reg + i, *data++);
+		else
+			I915_WRITE(data_reg + i, 0);
+	}
+
+	I915_WRITE(ctl_reg, VIDEO_DIP_ENABLE_VSC_HSW);
+	POSTING_READ(ctl_reg);
+}
+
+static void intel_psr_setup_vsc(struct intel_dp *intel_dp)
+{
+	struct edp_vsc_psr psr_vsc;
+
+	/* Prepare VSC packet as per EDP 1.3 spec, Table 3.10 */
+	memset(&psr_vsc, 0, sizeof(psr_vsc));
+	psr_vsc.sdp_header.HB0 = 0;
+	psr_vsc.sdp_header.HB1 = 0x7;
+	psr_vsc.sdp_header.HB2 = 0x2;
+	psr_vsc.sdp_header.HB3 = 0x8;
+	intel_psr_write_vsc(intel_dp, &psr_vsc);
+}
+
+static void intel_psr_enable_sink(struct intel_dp *intel_dp)
+{
+	struct intel_digital_port *dig_port = dp_to_dig_port(intel_dp);
+	struct drm_device *dev = dig_port->base.base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	uint32_t aux_clock_divider;
+	int precharge = 0x3;
+	bool only_standby = false;
+	static const uint8_t aux_msg[] = {
+		[0] = DP_AUX_NATIVE_WRITE << 4,
+		[1] = DP_SET_POWER >> 8,
+		[2] = DP_SET_POWER & 0xff,
+		[3] = 1 - 1,
+		[4] = DP_SET_POWER_D0,
+	};
+	int i;
+
+	BUILD_BUG_ON(sizeof(aux_msg) > 20);
+
+	aux_clock_divider = intel_dp->get_aux_clock_divider(intel_dp, 0);
+
+	if (IS_BROADWELL(dev) && dig_port->port != PORT_A)
+		only_standby = true;
+
+	/* Enable PSR in sink */
+	if (intel_dp->psr_dpcd[1] & DP_PSR_NO_TRAIN_ON_EXIT || only_standby)
+		drm_dp_dpcd_writeb(&intel_dp->aux, DP_PSR_EN_CFG,
+				   DP_PSR_ENABLE & ~DP_PSR_MAIN_LINK_ACTIVE);
+	else
+		drm_dp_dpcd_writeb(&intel_dp->aux, DP_PSR_EN_CFG,
+				   DP_PSR_ENABLE | DP_PSR_MAIN_LINK_ACTIVE);
+
+	/* Setup AUX registers */
+	for (i = 0; i < sizeof(aux_msg); i += 4)
+		I915_WRITE(EDP_PSR_AUX_DATA1(dev) + i,
+			   intel_dp_pack_aux(&aux_msg[i], sizeof(aux_msg) - i));
+
+	I915_WRITE(EDP_PSR_AUX_CTL(dev),
+		   DP_AUX_CH_CTL_TIME_OUT_400us |
+		   (sizeof(aux_msg) << DP_AUX_CH_CTL_MESSAGE_SIZE_SHIFT) |
+		   (precharge << DP_AUX_CH_CTL_PRECHARGE_2US_SHIFT) |
+		   (aux_clock_divider << DP_AUX_CH_CTL_BIT_CLOCK_2X_SHIFT));
+}
+
+static void intel_psr_enable_source(struct intel_dp *intel_dp)
+{
+	struct intel_digital_port *dig_port = dp_to_dig_port(intel_dp);
+	struct drm_device *dev = dig_port->base.base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	uint32_t max_sleep_time = 0x1f;
+	uint32_t idle_frames = 1;
+	uint32_t val = 0x0;
+	const uint32_t link_entry_time = EDP_PSR_MIN_LINK_ENTRY_TIME_8_LINES;
+	bool only_standby = false;
+
+	if (IS_BROADWELL(dev) && dig_port->port != PORT_A)
+		only_standby = true;
+
+	if (intel_dp->psr_dpcd[1] & DP_PSR_NO_TRAIN_ON_EXIT || only_standby) {
+		val |= EDP_PSR_LINK_STANDBY;
+		val |= EDP_PSR_TP2_TP3_TIME_0us;
+		val |= EDP_PSR_TP1_TIME_0us;
+		val |= EDP_PSR_SKIP_AUX_EXIT;
+		val |= IS_BROADWELL(dev) ? BDW_PSR_SINGLE_FRAME : 0;
+	} else
+		val |= EDP_PSR_LINK_DISABLE;
+
+	I915_WRITE(EDP_PSR_CTL(dev), val |
+		   (IS_BROADWELL(dev) ? 0 : link_entry_time) |
+		   max_sleep_time << EDP_PSR_MAX_SLEEP_TIME_SHIFT |
+		   idle_frames << EDP_PSR_IDLE_FRAME_SHIFT |
+		   EDP_PSR_ENABLE);
+}
+
+static bool intel_psr_match_conditions(struct intel_dp *intel_dp)
+{
+	struct intel_digital_port *dig_port = dp_to_dig_port(intel_dp);
+	struct drm_device *dev = dig_port->base.base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_crtc *crtc = dig_port->base.base.crtc;
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+
+	lockdep_assert_held(&dev_priv->psr.lock);
+	WARN_ON(!drm_modeset_is_locked(&dev->mode_config.connection_mutex));
+	WARN_ON(!drm_modeset_is_locked(&crtc->mutex));
+
+	dev_priv->psr.source_ok = false;
+
+	if (IS_HASWELL(dev) && dig_port->port != PORT_A) {
+		DRM_DEBUG_KMS("HSW ties PSR to DDI A (eDP)\n");
+		return false;
+	}
+
+	if (!i915_module.enable_psr) {
+		DRM_DEBUG_KMS("PSR disable by flag\n");
+		return false;
+	}
+
+	/* Below limitations aren't valid for Broadwell */
+	if (IS_BROADWELL(dev))
+		goto out;
+
+	if (I915_READ(HSW_STEREO_3D_CTL(intel_crtc->config.cpu_transcoder)) &
+	    S3D_ENABLE) {
+		DRM_DEBUG_KMS("PSR condition failed: Stereo 3D is Enabled\n");
+		return false;
+	}
+
+	if (intel_crtc->config.adjusted_mode.flags & DRM_MODE_FLAG_INTERLACE) {
+		DRM_DEBUG_KMS("PSR condition failed: Interlaced is Enabled\n");
+		return false;
+	}
+
+ out:
+	dev_priv->psr.source_ok = true;
+	return true;
+}
+
+static void intel_psr_do_enable(struct intel_dp *intel_dp)
+{
+	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+	struct drm_device *dev = intel_dig_port->base.base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	WARN_ON(I915_READ(EDP_PSR_CTL(dev)) & EDP_PSR_ENABLE);
+	WARN_ON(dev_priv->psr.active);
+	lockdep_assert_held(&dev_priv->psr.lock);
+
+	/* Enable/Re-enable PSR on the host */
+	intel_psr_enable_source(intel_dp);
+
+	dev_priv->psr.active = true;
+}
+
+/**
+ * intel_psr_enable - Enable PSR
+ * @intel_dp: Intel DP
+ *
+ * This function can only be called after the pipe is fully trained and enabled.
+ */
+void intel_psr_enable(struct intel_dp *intel_dp)
+{
+	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+	struct drm_device *dev = intel_dig_port->base.base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	if (!HAS_PSR(dev)) {
+		DRM_DEBUG_KMS("PSR not supported on this platform\n");
+		return;
+	}
+
+	if (!is_edp_psr(intel_dp)) {
+		DRM_DEBUG_KMS("PSR not supported by this panel\n");
+		return;
+	}
+
+	mutex_lock(&dev_priv->psr.lock);
+	if (dev_priv->psr.enabled) {
+		DRM_DEBUG_KMS("PSR already in use\n");
+		goto unlock;
+	}
+
+	if (!intel_psr_match_conditions(intel_dp))
+		goto unlock;
+
+	dev_priv->psr.busy_frontbuffer_bits = 0;
+
+	intel_psr_setup_vsc(intel_dp);
+
+	/* Avoid continuous PSR exit by masking memup and hpd */
+	I915_WRITE(EDP_PSR_DEBUG_CTL(dev), EDP_PSR_DEBUG_MASK_MEMUP |
+		   EDP_PSR_DEBUG_MASK_HPD | EDP_PSR_DEBUG_MASK_LPSP);
+
+	/* Enable PSR on the panel */
+	intel_psr_enable_sink(intel_dp);
+
+	dev_priv->psr.enabled = intel_dp;
+unlock:
+	mutex_unlock(&dev_priv->psr.lock);
+}
+
+/**
+ * intel_psr_disable - Disable PSR
+ * @intel_dp: Intel DP
+ *
+ * This function needs to be called before disabling pipe.
+ */
+void intel_psr_disable(struct intel_dp *intel_dp)
+{
+	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+	struct drm_device *dev = intel_dig_port->base.base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	mutex_lock(&dev_priv->psr.lock);
+	if (!dev_priv->psr.enabled) {
+		mutex_unlock(&dev_priv->psr.lock);
+		return;
+	}
+
+	if (dev_priv->psr.active) {
+		I915_WRITE(EDP_PSR_CTL(dev),
+			   I915_READ(EDP_PSR_CTL(dev)) & ~EDP_PSR_ENABLE);
+
+		/* Wait till PSR is idle */
+		if (_wait_for((I915_READ(EDP_PSR_STATUS_CTL(dev)) &
+			       EDP_PSR_STATUS_STATE_MASK) == 0, 2000, 10))
+			DRM_ERROR("Timed out waiting for PSR Idle State\n");
+
+		dev_priv->psr.active = false;
+	} else {
+		WARN_ON(I915_READ(EDP_PSR_CTL(dev)) & EDP_PSR_ENABLE);
+	}
+
+	dev_priv->psr.enabled = NULL;
+	mutex_unlock(&dev_priv->psr.lock);
+
+	cancel_delayed_work_sync(&dev_priv->psr.work);
+}
+
+static void intel_psr_work(struct work_struct *work)
+{
+	struct drm_i915_private *dev_priv =
+		container_of(work, typeof(*dev_priv), psr.work.work);
+	struct intel_dp *intel_dp = dev_priv->psr.enabled;
+
+	/* We have to make sure PSR is ready for re-enable
+	 * otherwise it keeps disabled until next full enable/disable cycle.
+	 * PSR might take some time to get fully disabled
+	 * and be ready for re-enable.
+	 */
+	if (wait_for((I915_READ(EDP_PSR_STATUS_CTL(dev_priv->dev)) &
+		      EDP_PSR_STATUS_STATE_MASK) == 0, 50)) {
+		DRM_ERROR("Timed out waiting for PSR Idle for re-enable\n");
+		return;
+	}
+
+	mutex_lock(&dev_priv->psr.lock);
+	intel_dp = dev_priv->psr.enabled;
+
+	if (!intel_dp)
+		goto unlock;
+
+	/*
+	 * The delayed work can race with an invalidate hence we need to
+	 * recheck. Since psr_flush first clears this and then reschedules we
+	 * won't ever miss a flush when bailing out here.
+	 */
+	if (dev_priv->psr.busy_frontbuffer_bits)
+		goto unlock;
+
+	intel_psr_do_enable(intel_dp);
+unlock:
+	mutex_unlock(&dev_priv->psr.lock);
+}
+
+static void intel_psr_exit(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	if (dev_priv->psr.active) {
+		u32 val = I915_READ(EDP_PSR_CTL(dev));
+
+		WARN_ON(!(val & EDP_PSR_ENABLE));
+
+		I915_WRITE(EDP_PSR_CTL(dev), val & ~EDP_PSR_ENABLE);
+
+		dev_priv->psr.active = false;
+	}
+
+}
+
+/**
+ * intel_psr_invalidate - Invalidade PSR
+ * @dev: DRM device
+ * @frontbuffer_bits: frontbuffer plane tracking bits
+ *
+ * Since the hardware frontbuffer tracking has gaps we need to integrate
+ * with the software frontbuffer tracking. This function gets called every
+ * time frontbuffer rendering starts and a buffer gets dirtied. PSR must be
+ * disabled if the frontbuffer mask contains a buffer relevant to PSR.
+ *
+ * Dirty frontbuffers relevant to PSR are tracked in busy_frontbuffer_bits."
+ */
+void intel_psr_invalidate(struct drm_device *dev,
+			      unsigned frontbuffer_bits)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_crtc *crtc;
+	enum pipe pipe;
+
+	mutex_lock(&dev_priv->psr.lock);
+	if (!dev_priv->psr.enabled) {
+		mutex_unlock(&dev_priv->psr.lock);
+		return;
+	}
+
+	crtc = dp_to_dig_port(dev_priv->psr.enabled)->base.base.crtc;
+	pipe = to_intel_crtc(crtc)->pipe;
+
+	intel_psr_exit(dev);
+
+	frontbuffer_bits &= INTEL_FRONTBUFFER_ALL_MASK(pipe);
+
+	dev_priv->psr.busy_frontbuffer_bits |= frontbuffer_bits;
+	mutex_unlock(&dev_priv->psr.lock);
+}
+
+/**
+ * intel_psr_flush - Flush PSR
+ * @dev: DRM device
+ * @frontbuffer_bits: frontbuffer plane tracking bits
+ *
+ * Since the hardware frontbuffer tracking has gaps we need to integrate
+ * with the software frontbuffer tracking. This function gets called every
+ * time frontbuffer rendering has completed and flushed out to memory. PSR
+ * can be enabled again if no other frontbuffer relevant to PSR is dirty.
+ *
+ * Dirty frontbuffers relevant to PSR are tracked in busy_frontbuffer_bits.
+ */
+void intel_psr_flush(struct drm_device *dev,
+			 unsigned frontbuffer_bits)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_crtc *crtc;
+	enum pipe pipe;
+
+	mutex_lock(&dev_priv->psr.lock);
+	if (!dev_priv->psr.enabled) {
+		mutex_unlock(&dev_priv->psr.lock);
+		return;
+	}
+
+	crtc = dp_to_dig_port(dev_priv->psr.enabled)->base.base.crtc;
+	pipe = to_intel_crtc(crtc)->pipe;
+	dev_priv->psr.busy_frontbuffer_bits &= ~frontbuffer_bits;
+
+	/*
+	 * On Haswell sprite plane updates don't result in a psr invalidating
+	 * signal in the hardware. Which means we need to manually fake this in
+	 * software for all flushes, not just when we've seen a preceding
+	 * invalidation through frontbuffer rendering.
+	 */
+	if (IS_HASWELL(dev) &&
+	    (frontbuffer_bits & INTEL_FRONTBUFFER_SPRITE(pipe)))
+		intel_psr_exit(dev);
+
+	if (!dev_priv->psr.active && !dev_priv->psr.busy_frontbuffer_bits)
+		schedule_delayed_work(&dev_priv->psr.work,
+				      msecs_to_jiffies(100));
+	mutex_unlock(&dev_priv->psr.lock);
+}
+
+/**
+ * intel_psr_init - Init basic PSR work and mutex.
+ * @dev: DRM device
+ *
+ * This function is  called only once at driver load to initialize basic
+ * PSR stuff.
+ */
+void intel_psr_init(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	INIT_DELAYED_WORK(&dev_priv->psr.work, intel_psr_work);
+	mutex_init(&dev_priv->psr.lock);
+}
diff -urN a/drivers/gpu/drm/i915/intel_renderstate_gen8.c b/drivers/gpu/drm/i915/intel_renderstate_gen8.c
--- a/drivers/gpu/drm/i915/intel_renderstate_gen8.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_renderstate_gen8.c	2014-11-22 14:37:49.342700417 -0700
@@ -1,70 +1,575 @@
 #include "intel_renderstate.h"
 
 static const u32 gen8_null_state_relocs[] = {
-	0x00000048,
-	0x00000050,
-	0x00000060,
-	0x000003ec,
+	0x00000798,
+	0x000007a4,
+	0x000007ac,
+	0x000007bc,
 	-1,
 };
 
 static const u32 gen8_null_state_batch[] = {
+	0x7a000004,
+	0x01000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
 	0x69040000,
-	0x61020001,
+	0x78140000,
+	0x04000000,
+	0x7820000a,
+	0x00000000,
+	0x00000000,
+	0x80000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78130002,
+	0x00000000,
+	0x00000000,
+	0x02001808,
+	0x781f0002,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78510009,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78100007,
+	0x00000000,
+	0x00000000,
+	0x00010000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x781b0007,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000800,
+	0x00000000,
+	0x78110008,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x781e0003,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x781d0007,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78120002,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78500003,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x781c0002,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x780c0000,
+	0x00000000,
+	0x78520003,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78300000,
+	0x08010040,
+	0x78310000,
+	0x1e000000,
+	0x78320000,
+	0x1e000000,
+	0x78330000,
+	0x1e000000,
+	0x79190002,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x791a0002,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x791b0002,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x79120000,
+	0x00000000,
+	0x79130000,
+	0x00000000,
+	0x79140000,
+	0x00000000,
+	0x79150000,
+	0x00000000,
+	0x79160000,
+	0x00000000,
+	0x78150009,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78190009,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x781a0009,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78160009,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78170009,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78490001,
+	0x00000000,
+	0x00000000,
+	0x784a0000,
+	0x00000000,
+	0x784b0000,
+	0x00000004,
+	0x79170101,
+	0x00000000,
+	0x00000080,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
 	0x00000000,
 	0x00000000,
-	0x79120000,
 	0x00000000,
-	0x79130000,
 	0x00000000,
-	0x79140000,
 	0x00000000,
-	0x79150000,
 	0x00000000,
-	0x79160000,
 	0x00000000,
-	0x6101000e,
-	0x00000001,
 	0x00000000,
-	0x00000001,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x79180006,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x79180006,
+	0x20000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x79180006,
+	0x40000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x79180006,
+	0x60000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x6101000e,
 	0x00000001,	 /* reloc */
 	0x00000000,
+	0x00000000,
 	0x00000001,	 /* reloc */
 	0x00000000,
+	0x00000001,	 /* reloc */
 	0x00000000,
+	0x00000001,
 	0x00000000,
 	0x00000001,	 /* reloc */
 	0x00000000,
-	0xfffff001,
 	0x00001001,
-	0xfffff001,
 	0x00001001,
-	0x78230000,
-	0x000006e0,
-	0x78210000,
-	0x00000700,
-	0x78300000,
-	0x08010040,
-	0x78330000,
-	0x08000000,
-	0x78310000,
-	0x08000000,
-	0x78320000,
-	0x08000000,
-	0x78240000,
-	0x00000641,
-	0x780e0000,
-	0x00000601,
+	0x00000001,
+	0x00001001,
+	0x61020001,
+	0x00000000,
+	0x00000000,
+	0x79000002,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78050006,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x79040002,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x79040002,
+	0x40000000,
+	0x00000000,
+	0x00000000,
+	0x79040002,
+	0x80000000,
+	0x00000000,
+	0x00000000,
+	0x79040002,
+	0xc0000000,
+	0x00000000,
+	0x00000000,
+	0x79080001,
+	0x00000000,
+	0x00000000,
+	0x790a0001,
+	0x00000000,
+	0x00000000,
+	0x78060003,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78070003,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78040001,
+	0x00000000,
+	0x00000000,
+	0x79110000,
+	0x00000000,
 	0x780d0000,
 	0x00000000,
-	0x78180000,
-	0x00000001,
-	0x78520003,
+	0x79060000,
 	0x00000000,
+	0x7907001f,
 	0x00000000,
 	0x00000000,
 	0x00000000,
-	0x78190009,
 	0x00000000,
 	0x00000000,
 	0x00000000,
@@ -75,7 +580,6 @@
 	0x00000000,
 	0x00000000,
 	0x00000000,
-	0x781b0007,
 	0x00000000,
 	0x00000000,
 	0x00000000,
@@ -84,26 +588,22 @@
 	0x00000000,
 	0x00000000,
 	0x00000000,
-	0x78270000,
 	0x00000000,
-	0x782c0000,
 	0x00000000,
-	0x781c0002,
 	0x00000000,
 	0x00000000,
 	0x00000000,
-	0x78160009,
 	0x00000000,
 	0x00000000,
 	0x00000000,
 	0x00000000,
 	0x00000000,
 	0x00000000,
+	0x7902000f,
 	0x00000000,
 	0x00000000,
 	0x00000000,
 	0x00000000,
-	0x78110008,
 	0x00000000,
 	0x00000000,
 	0x00000000,
@@ -113,12 +613,10 @@
 	0x00000000,
 	0x00000000,
 	0x00000000,
-	0x78290000,
 	0x00000000,
-	0x782e0000,
 	0x00000000,
-	0x781a0009,
 	0x00000000,
+	0x790c000f,
 	0x00000000,
 	0x00000000,
 	0x00000000,
@@ -128,7 +626,6 @@
 	0x00000000,
 	0x00000000,
 	0x00000000,
-	0x781d0007,
 	0x00000000,
 	0x00000000,
 	0x00000000,
@@ -136,153 +633,153 @@
 	0x00000000,
 	0x00000000,
 	0x00000000,
+	0x780a0003,
 	0x00000000,
-	0x78280000,
 	0x00000000,
-	0x782d0000,
 	0x00000000,
-	0x78260000,
 	0x00000000,
-	0x782b0000,
+	0x78080083,
+	0x00004000,
 	0x00000000,
-	0x78150009,
 	0x00000000,
 	0x00000000,
+	0x04004000,
 	0x00000000,
 	0x00000000,
 	0x00000000,
+	0x08004000,
 	0x00000000,
 	0x00000000,
 	0x00000000,
+	0x0c004000,
 	0x00000000,
 	0x00000000,
-	0x78100007,
 	0x00000000,
+	0x10004000,
 	0x00000000,
 	0x00000000,
 	0x00000000,
+	0x14004000,
 	0x00000000,
 	0x00000000,
 	0x00000000,
+	0x18004000,
 	0x00000000,
-	0x781e0003,
 	0x00000000,
 	0x00000000,
+	0x1c004000,
 	0x00000000,
 	0x00000000,
-	0x78120002,
 	0x00000000,
+	0x20004000,
 	0x00000000,
 	0x00000000,
-	0x781f0002,
-	0x30400820,
 	0x00000000,
+	0x24004000,
 	0x00000000,
-	0x78510009,
 	0x00000000,
 	0x00000000,
+	0x28004000,
 	0x00000000,
 	0x00000000,
 	0x00000000,
+	0x2c004000,
 	0x00000000,
 	0x00000000,
 	0x00000000,
+	0x30004000,
 	0x00000000,
 	0x00000000,
-	0x78500003,
-	0x00210000,
 	0x00000000,
+	0x34004000,
 	0x00000000,
 	0x00000000,
-	0x78130002,
 	0x00000000,
+	0x38004000,
 	0x00000000,
 	0x00000000,
-	0x782a0000,
-	0x00000480,
-	0x782f0000,
-	0x00000540,
-	0x78140000,
-	0x00000800,
-	0x78170009,
 	0x00000000,
+	0x3c004000,
 	0x00000000,
 	0x00000000,
 	0x00000000,
+	0x40004000,
 	0x00000000,
 	0x00000000,
 	0x00000000,
+	0x44004000,
 	0x00000000,
 	0x00000000,
 	0x00000000,
-	0x7820000a,
-	0x00000580,
+	0x48004000,
 	0x00000000,
-	0x08080000,
 	0x00000000,
 	0x00000000,
-	0x1f000002,
-	0x00060000,
+	0x4c004000,
 	0x00000000,
 	0x00000000,
 	0x00000000,
+	0x50004000,
 	0x00000000,
-	0x784d0000,
-	0x40000000,
-	0x784f0000,
-	0x80000100,
-	0x780f0000,
-	0x00000740,
-	0x78050006,
 	0x00000000,
 	0x00000000,
+	0x54004000,
 	0x00000000,
 	0x00000000,
 	0x00000000,
+	0x58004000,
 	0x00000000,
 	0x00000000,
-	0x78070003,
 	0x00000000,
+	0x5c004000,
 	0x00000000,
 	0x00000000,
 	0x00000000,
-	0x78060003,
+	0x60004000,
 	0x00000000,
 	0x00000000,
 	0x00000000,
+	0x64004000,
 	0x00000000,
-	0x78040001,
 	0x00000000,
-	0x00000001,
-	0x79000002,
-	0xffffffff,
+	0x00000000,
+	0x68004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x6c004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x70004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x74004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78004000,
+	0x00000000,
 	0x00000000,
 	0x00000000,
-	0x78080003,
-	0x00006000,
-	0x000005e0,	 /* reloc */
+	0x7c004000,
 	0x00000000,
 	0x00000000,
-	0x78090005,
+	0x00000000,
+	0x80004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78090043,
 	0x02000000,
 	0x22220000,
-	0x02f60000,
-	0x11230000,
-	0x02850004,
-	0x11230000,
-	0x784b0000,
-	0x0000000f,
-	0x78490001,
 	0x00000000,
 	0x00000000,
-	0x7b000005,
 	0x00000000,
-	0x00000003,
 	0x00000000,
-	0x00000001,
 	0x00000000,
 	0x00000000,
-	0x05000000,	 /* cmds end */
 	0x00000000,
 	0x00000000,
 	0x00000000,
@@ -297,8 +794,6 @@
 	0x00000000,
 	0x00000000,
 	0x00000000,
-	0x000004c0,	 /* state start */
-	0x00000500,
 	0x00000000,
 	0x00000000,
 	0x00000000,
@@ -345,46 +840,65 @@
 	0x00000000,
 	0x00000000,
 	0x00000000,
+	0x680b0001,
+	0x78260000,
+	0x00000000,
+	0x78270000,
+	0x00000000,
+	0x78280000,
+	0x00000000,
+	0x78290000,
+	0x00000000,
+	0x782a0000,
+	0x00000000,
+	0x780e0000,
+	0x00000dc1,
+	0x78240000,
+	0x00000e01,
+	0x784f0000,
+	0x80000100,
+	0x784d0000,
+	0x40000000,
+	0x782b0000,
+	0x00000000,
+	0x782c0000,
+	0x00000000,
+	0x782d0000,
+	0x00000000,
+	0x782e0000,
+	0x00000000,
+	0x782f0000,
+	0x00000000,
+	0x780f0000,
 	0x00000000,
+	0x78230000,
+	0x00000e60,
+	0x78210000,
+	0x00000e80,
+	0x7b000005,
+	0x00000004,
+	0x00000001,
 	0x00000000,
+	0x00000001,
 	0x00000000,
-	0x00000092,
 	0x00000000,
+	0x05000000,	 /* cmds end */
 	0x00000000,
 	0x00000000,
 	0x00000000,
 	0x00000000,
 	0x00000000,
 	0x00000000,
+	0x00000000,	 /* state start */
 	0x00000000,
+	0x3f800000,
+	0x3f800000,
+	0x3f800000,
+	0x3f800000,
 	0x00000000,
 	0x00000000,
 	0x00000000,
 	0x00000000,
-	0x0060005a,
-	0x21403ae8,
-	0x3a0000c0,
-	0x008d0040,
-	0x0060005a,
-	0x21603ae8,
-	0x3a0000c0,
-	0x008d0080,
-	0x0060005a,
-	0x21803ae8,
-	0x3a0000d0,
-	0x008d0040,
-	0x0060005a,
-	0x21a03ae8,
-	0x3a0000d0,
-	0x008d0080,
-	0x02800031,
-	0x2e0022e8,
-	0x0e000140,
-	0x08840001,
-	0x05800031,
-	0x200022e0,
-	0x0e000e00,
-	0x90031000,
 	0x00000000,
 	0x00000000,
 	0x00000000,
@@ -410,38 +924,6 @@
 	0x00000000,
 	0x00000000,
 	0x00000000,
-	0x06200000,
-	0x00000002,
-	0x06200000,
-	0x00000002,
-	0x06200000,
-	0x00000002,
-	0x06200000,
-	0x00000002,
-	0x06200000,
-	0x00000002,
-	0x06200000,
-	0x00000002,
-	0x06200000,
-	0x00000002,
-	0x06200000,
-	0x00000002,
-	0x06200000,
-	0x00000002,
-	0x06200000,
-	0x00000002,
-	0x06200000,
-	0x00000002,
-	0x06200000,
-	0x00000002,
-	0x06200000,
-	0x00000002,
-	0x06200000,
-	0x00000002,
-	0x06200000,
-	0x00000002,
-	0x06200000,
-	0x00000002,
 	0x00000000,
 	0x00000000,
 	0x00000000,
@@ -449,8 +931,6 @@
 	0x00000000,
 	0x00000000,
 	0x00000000,
-	0xf99a130c,
-	0x799a130c,
 	0x00000000,
 	0x00000000,
 	0x00000000,
@@ -466,9 +946,7 @@
 	0x00000000,
 	0x00000000,
 	0x00000000,
-	0x3f800000,
 	0x00000000,
-	0x3f800000,
 	0x00000000,
 	0x00000000,
 	0x00000000,
diff -urN a/drivers/gpu/drm/i915/intel_renderstate_gen9.c b/drivers/gpu/drm/i915/intel_renderstate_gen9.c
--- a/drivers/gpu/drm/i915/intel_renderstate_gen9.c	1969-12-31 17:00:00.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_renderstate_gen9.c	2014-11-22 14:37:49.342700417 -0700
@@ -0,0 +1,974 @@
+#include "intel_renderstate.h"
+
+static const u32 gen9_null_state_relocs[] = {
+	0x000007a8,
+	0x000007b4,
+	0x000007bc,
+	0x000007cc,
+	-1,
+};
+
+static const u32 gen9_null_state_batch[] = {
+	0x7a000004,
+	0x01000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x69040300,
+	0x78140000,
+	0x04000000,
+	0x7820000a,
+	0x00000000,
+	0x00000000,
+	0x80000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78130002,
+	0x00000000,
+	0x00000000,
+	0x02001808,
+	0x781f0004,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78510009,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78100007,
+	0x00000000,
+	0x00000000,
+	0x00010000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x781b0007,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000800,
+	0x00000000,
+	0x78110008,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x781e0003,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x781d0009,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78120002,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78500003,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x781c0002,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x780c0000,
+	0x00000000,
+	0x78520003,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78300000,
+	0x08010040,
+	0x78310000,
+	0x1e000000,
+	0x78320000,
+	0x1e000000,
+	0x78330000,
+	0x1e000000,
+	0x79190002,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x791a0002,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x791b0002,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x79120000,
+	0x00000000,
+	0x79130000,
+	0x00000000,
+	0x79140000,
+	0x00000000,
+	0x79150000,
+	0x00000000,
+	0x79160000,
+	0x00000000,
+	0x78150009,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78190009,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x781a0009,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78160009,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78170009,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78490001,
+	0x00000000,
+	0x00000000,
+	0x784a0000,
+	0x00000000,
+	0x784b0000,
+	0x00000004,
+	0x79170101,
+	0x00000000,
+	0x00000080,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x79180006,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x79180006,
+	0x20000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x79180006,
+	0x40000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x79180006,
+	0x60000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x61010011,
+	0x00000001,	 /* reloc */
+	0x00000000,
+	0x00000000,
+	0x00000001,	 /* reloc */
+	0x00000000,
+	0x00000001,	 /* reloc */
+	0x00000000,
+	0x00000001,
+	0x00000000,
+	0x00000001,	 /* reloc */
+	0x00000000,
+	0x00001001,
+	0x00001001,
+	0x00000001,
+	0x00001001,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x61020001,
+	0x00000000,
+	0x00000000,
+	0x79000002,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78050006,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x79040002,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x79040002,
+	0x40000000,
+	0x00000000,
+	0x00000000,
+	0x79040002,
+	0x80000000,
+	0x00000000,
+	0x00000000,
+	0x79040002,
+	0xc0000000,
+	0x00000000,
+	0x00000000,
+	0x79080001,
+	0x00000000,
+	0x00000000,
+	0x790a0001,
+	0x00000000,
+	0x00000000,
+	0x78060003,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78070003,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78040001,
+	0x00000000,
+	0x00000000,
+	0x79110000,
+	0x00000000,
+	0x780d0000,
+	0x00000000,
+	0x79060000,
+	0x00000000,
+	0x7907001f,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x7902000f,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x790c000f,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x780a0003,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78080083,
+	0x00004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x04004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x08004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x0c004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x10004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x14004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x18004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x1c004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x20004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x24004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x28004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x2c004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x30004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x34004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x38004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x3c004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x40004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x44004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x48004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x4c004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x50004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x54004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x58004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x5c004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x60004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x64004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x68004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x6c004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x70004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x74004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x7c004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x80004000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78090043,
+	0x02000000,
+	0x22220000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x78550003,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x680b0001,
+	0x780e0000,
+	0x00000e01,
+	0x78240000,
+	0x00000e41,
+	0x784f0000,
+	0x80000100,
+	0x784d0000,
+	0x40000000,
+	0x782b0000,
+	0x00000000,
+	0x782c0000,
+	0x00000000,
+	0x782d0000,
+	0x00000000,
+	0x782e0000,
+	0x00000000,
+	0x782f0000,
+	0x00000000,
+	0x780f0000,
+	0x00000000,
+	0x78230000,
+	0x00000ea0,
+	0x78210000,
+	0x00000ec0,
+	0x78260000,
+	0x00000000,
+	0x78270000,
+	0x00000000,
+	0x78280000,
+	0x00000000,
+	0x78290000,
+	0x00000000,
+	0x782a0000,
+	0x00000000,
+	0x7b000005,
+	0x00000004,
+	0x00000001,
+	0x00000000,
+	0x00000001,
+	0x00000000,
+	0x00000000,
+	0x05000000,	 /* cmds end */
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,	 /* state start */
+	0x00000000,
+	0x3f800000,
+	0x3f800000,
+	0x3f800000,
+	0x3f800000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,
+	0x00000000,	 /* state end */
+};
+
+RO_RENDERSTATE(9);
diff -urN a/drivers/gpu/drm/i915/intel_renderstate.h b/drivers/gpu/drm/i915/intel_renderstate.h
--- a/drivers/gpu/drm/i915/intel_renderstate.h	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_renderstate.h	2014-11-22 14:37:49.342700417 -0700
@@ -35,6 +35,7 @@
 extern const struct intel_renderstate_rodata gen6_null_state;
 extern const struct intel_renderstate_rodata gen7_null_state;
 extern const struct intel_renderstate_rodata gen8_null_state;
+extern const struct intel_renderstate_rodata gen9_null_state;
 
 #define RO_RENDERSTATE(_g)						\
 	const struct intel_renderstate_rodata gen ## _g ## _null_state = { \
diff -urN a/drivers/gpu/drm/i915/intel_ringbuffer.c b/drivers/gpu/drm/i915/intel_ringbuffer.c
--- a/drivers/gpu/drm/i915/intel_ringbuffer.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_ringbuffer.c	2014-11-22 14:37:49.346700417 -0700
@@ -33,75 +33,39 @@
 #include "i915_trace.h"
 #include "intel_drv.h"
 
-/* Early gen2 devices have a cacheline of just 32 bytes, using 64 is overkill,
- * but keeps the logic simple. Indeed, the whole purpose of this macro is just
- * to give some inclination as to some of the magic values used in the various
- * workarounds!
- */
-#define CACHELINE_BYTES 64
-
-static inline int __ring_space(int head, int tail, int size)
-{
-	int space = head - (tail + I915_RING_FREE_SPACE);
-	if (space < 0)
-		space += size;
-	return space;
-}
-
-static inline int ring_space(struct intel_ringbuffer *ringbuf)
-{
-	return __ring_space(ringbuf->head & HEAD_ADDR, ringbuf->tail, ringbuf->size);
-}
-
-static bool intel_ring_stopped(struct intel_engine_cs *ring)
-{
-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
-	return dev_priv->gpu_error.stop_rings & intel_ring_flag(ring);
-}
-
-void __intel_ring_advance(struct intel_engine_cs *ring)
-{
-	struct intel_ringbuffer *ringbuf = ring->buffer;
-	ringbuf->tail &= ringbuf->size - 1;
-	if (intel_ring_stopped(ring))
-		return;
-	ring->write_tail(ring, ringbuf->tail);
-}
+/* Just userspace ABI convention to limit the wa batch bo to a resonable size */
+#define I830_BATCH_LIMIT (256*1024)
+#define I830_TLB_ENTRIES (2)
+#define I830_WA_SIZE max(I830_TLB_ENTRIES*4096, I830_BATCH_LIMIT)
 
 static int
-gen2_render_ring_flush(struct intel_engine_cs *ring,
-		       u32	invalidate_domains,
-		       u32	flush_domains)
+gen2_emit_flush(struct i915_gem_request *rq, u32 flags)
 {
+	struct intel_ringbuffer *ring;
 	u32 cmd;
-	int ret;
 
 	cmd = MI_FLUSH;
-	if (((invalidate_domains|flush_domains) & I915_GEM_DOMAIN_RENDER) == 0)
+	if ((flags & (I915_FLUSH_CACHES | I915_INVALIDATE_CACHES)) == 0)
 		cmd |= MI_NO_WRITE_FLUSH;
 
-	if (invalidate_domains & I915_GEM_DOMAIN_SAMPLER)
+	if (flags & I915_INVALIDATE_CACHES)
 		cmd |= MI_READ_FLUSH;
 
-	ret = intel_ring_begin(ring, 2);
-	if (ret)
-		return ret;
+	ring = intel_ring_begin(rq, 1);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
 	intel_ring_emit(ring, cmd);
-	intel_ring_emit(ring, MI_NOOP);
 	intel_ring_advance(ring);
 
 	return 0;
 }
 
 static int
-gen4_render_ring_flush(struct intel_engine_cs *ring,
-		       u32	invalidate_domains,
-		       u32	flush_domains)
+gen4_emit_flush(struct i915_gem_request *rq, u32 flags)
 {
-	struct drm_device *dev = ring->dev;
+	struct intel_ringbuffer *ring;
 	u32 cmd;
-	int ret;
 
 	/*
 	 * read/write caches:
@@ -131,22 +95,20 @@
 	 * are flushed at any MI_FLUSH.
 	 */
 
-	cmd = MI_FLUSH | MI_NO_WRITE_FLUSH;
-	if ((invalidate_domains|flush_domains) & I915_GEM_DOMAIN_RENDER)
-		cmd &= ~MI_NO_WRITE_FLUSH;
-	if (invalidate_domains & I915_GEM_DOMAIN_INSTRUCTION)
+	cmd = MI_FLUSH;
+	if ((flags & (I915_FLUSH_CACHES | I915_INVALIDATE_CACHES)) == 0)
+		cmd |= MI_NO_WRITE_FLUSH;
+	if (flags & I915_INVALIDATE_CACHES) {
 		cmd |= MI_EXE_FLUSH;
+		if (IS_G4X(rq->i915) || IS_GEN5(rq->i915))
+			cmd |= MI_INVALIDATE_ISP;
+	}
 
-	if (invalidate_domains & I915_GEM_DOMAIN_COMMAND &&
-	    (IS_G4X(dev) || IS_GEN5(dev)))
-		cmd |= MI_INVALIDATE_ISP;
-
-	ret = intel_ring_begin(ring, 2);
-	if (ret)
-		return ret;
+	ring = intel_ring_begin(rq, 1);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
 	intel_ring_emit(ring, cmd);
-	intel_ring_emit(ring, MI_NOOP);
 	intel_ring_advance(ring);
 
 	return 0;
@@ -190,100 +152,89 @@
  * really our business.  That leaves only stall at scoreboard.
  */
 static int
-intel_emit_post_sync_nonzero_flush(struct intel_engine_cs *ring)
+gen6_emit_post_sync_nonzero_flush(struct i915_gem_request *rq)
 {
-	u32 scratch_addr = ring->scratch.gtt_offset + 2 * CACHELINE_BYTES;
-	int ret;
-
+	const u32 scratch = rq->engine->scratch.gtt_offset + 2*CACHELINE_BYTES;
+	struct intel_ringbuffer *ring;
 
-	ret = intel_ring_begin(ring, 6);
-	if (ret)
-		return ret;
+	ring = intel_ring_begin(rq, 8);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
-	intel_ring_emit(ring, GFX_OP_PIPE_CONTROL(5));
+	intel_ring_emit(ring, GFX_OP_PIPE_CONTROL(4));
 	intel_ring_emit(ring, PIPE_CONTROL_CS_STALL |
 			PIPE_CONTROL_STALL_AT_SCOREBOARD);
-	intel_ring_emit(ring, scratch_addr | PIPE_CONTROL_GLOBAL_GTT); /* address */
-	intel_ring_emit(ring, 0); /* low dword */
-	intel_ring_emit(ring, 0); /* high dword */
-	intel_ring_emit(ring, MI_NOOP);
-	intel_ring_advance(ring);
-
-	ret = intel_ring_begin(ring, 6);
-	if (ret)
-		return ret;
+	intel_ring_emit(ring, scratch | PIPE_CONTROL_GLOBAL_GTT);
+	intel_ring_emit(ring, 0);
 
-	intel_ring_emit(ring, GFX_OP_PIPE_CONTROL(5));
+	intel_ring_emit(ring, GFX_OP_PIPE_CONTROL(4));
 	intel_ring_emit(ring, PIPE_CONTROL_QW_WRITE);
-	intel_ring_emit(ring, scratch_addr | PIPE_CONTROL_GLOBAL_GTT); /* address */
-	intel_ring_emit(ring, 0);
+	intel_ring_emit(ring, scratch | PIPE_CONTROL_GLOBAL_GTT);
 	intel_ring_emit(ring, 0);
-	intel_ring_emit(ring, MI_NOOP);
-	intel_ring_advance(ring);
 
+	intel_ring_advance(ring);
 	return 0;
 }
 
 static int
-gen6_render_ring_flush(struct intel_engine_cs *ring,
-                         u32 invalidate_domains, u32 flush_domains)
+gen6_render_emit_flush(struct i915_gem_request *rq, u32 flags)
 {
-	u32 flags = 0;
-	u32 scratch_addr = ring->scratch.gtt_offset + 2 * CACHELINE_BYTES;
+	const u32 scratch = rq->engine->scratch.gtt_offset + 2*CACHELINE_BYTES;
+	struct intel_ringbuffer *ring;
+	u32 cmd = 0;
 	int ret;
 
-	/* Force SNB workarounds for PIPE_CONTROL flushes */
-	ret = intel_emit_post_sync_nonzero_flush(ring);
-	if (ret)
-		return ret;
+	if (flags & I915_FLUSH_CACHES) {
+		/* Force SNB workarounds for PIPE_CONTROL flushes */
+		ret = gen6_emit_post_sync_nonzero_flush(rq);
+		if (ret)
+			return ret;
 
-	/* Just flush everything.  Experiments have shown that reducing the
-	 * number of bits based on the write domains has little performance
-	 * impact.
-	 */
-	if (flush_domains) {
-		flags |= PIPE_CONTROL_RENDER_TARGET_CACHE_FLUSH;
-		flags |= PIPE_CONTROL_DEPTH_CACHE_FLUSH;
-		/*
-		 * Ensure that any following seqno writes only happen
-		 * when the render cache is indeed flushed.
-		 */
-		flags |= PIPE_CONTROL_CS_STALL;
+		cmd |= PIPE_CONTROL_RENDER_TARGET_CACHE_FLUSH;
+		cmd |= PIPE_CONTROL_DEPTH_CACHE_FLUSH;
 	}
-	if (invalidate_domains) {
-		flags |= PIPE_CONTROL_TLB_INVALIDATE;
-		flags |= PIPE_CONTROL_INSTRUCTION_CACHE_INVALIDATE;
-		flags |= PIPE_CONTROL_TEXTURE_CACHE_INVALIDATE;
-		flags |= PIPE_CONTROL_VF_CACHE_INVALIDATE;
-		flags |= PIPE_CONTROL_CONST_CACHE_INVALIDATE;
-		flags |= PIPE_CONTROL_STATE_CACHE_INVALIDATE;
+	if (flags & I915_INVALIDATE_CACHES) {
+		cmd |= PIPE_CONTROL_TLB_INVALIDATE;
+		cmd |= PIPE_CONTROL_INSTRUCTION_CACHE_INVALIDATE;
+		cmd |= PIPE_CONTROL_TEXTURE_CACHE_INVALIDATE;
+		cmd |= PIPE_CONTROL_VF_CACHE_INVALIDATE;
+		cmd |= PIPE_CONTROL_CONST_CACHE_INVALIDATE;
+		cmd |= PIPE_CONTROL_STATE_CACHE_INVALIDATE;
 		/*
 		 * TLB invalidate requires a post-sync write.
 		 */
-		flags |= PIPE_CONTROL_QW_WRITE | PIPE_CONTROL_CS_STALL;
+		cmd |= PIPE_CONTROL_QW_WRITE | PIPE_CONTROL_CS_STALL;
 	}
+	if (flags & I915_COMMAND_BARRIER)
+		/*
+		 * Ensure that any following seqno writes only happen
+		 * when the render cache is indeed flushed.
+		 */
+		cmd |= PIPE_CONTROL_QW_WRITE | PIPE_CONTROL_CS_STALL;
 
-	ret = intel_ring_begin(ring, 4);
-	if (ret)
-		return ret;
-
-	intel_ring_emit(ring, GFX_OP_PIPE_CONTROL(4));
-	intel_ring_emit(ring, flags);
-	intel_ring_emit(ring, scratch_addr | PIPE_CONTROL_GLOBAL_GTT);
-	intel_ring_emit(ring, 0);
-	intel_ring_advance(ring);
+	if (cmd) {
+		ring = intel_ring_begin(rq, 4);
+		if (IS_ERR(ring))
+			return PTR_ERR(ring);
+
+		intel_ring_emit(ring, GFX_OP_PIPE_CONTROL(4));
+		intel_ring_emit(ring, cmd);
+		intel_ring_emit(ring, scratch | PIPE_CONTROL_GLOBAL_GTT);
+		intel_ring_emit(ring, 0);
+		intel_ring_advance(ring);
+	}
 
 	return 0;
 }
 
 static int
-gen7_render_ring_cs_stall_wa(struct intel_engine_cs *ring)
+gen7_render_ring_cs_stall_wa(struct i915_gem_request *rq)
 {
-	int ret;
+	struct intel_ringbuffer *ring;
 
-	ret = intel_ring_begin(ring, 4);
-	if (ret)
-		return ret;
+	ring = intel_ring_begin(rq, 4);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
 	intel_ring_emit(ring, GFX_OP_PIPE_CONTROL(4));
 	intel_ring_emit(ring, PIPE_CONTROL_CS_STALL |
@@ -295,35 +246,32 @@
 	return 0;
 }
 
-static int gen7_ring_fbc_flush(struct intel_engine_cs *ring, u32 value)
+static int gen7_ring_fbc_flush(struct i915_gem_request *rq, u32 value)
 {
-	int ret;
+	struct intel_ringbuffer *ring;
 
-	if (!ring->fbc_dirty)
-		return 0;
+	ring = intel_ring_begin(rq, 6);
+	if (IS_ERR(ring))
+		return PTR_ERR(rq);
 
-	ret = intel_ring_begin(ring, 6);
-	if (ret)
-		return ret;
 	/* WaFbcNukeOn3DBlt:ivb/hsw */
 	intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
 	intel_ring_emit(ring, MSG_FBC_REND_STATE);
 	intel_ring_emit(ring, value);
 	intel_ring_emit(ring, MI_STORE_REGISTER_MEM(1) | MI_SRM_LRM_GLOBAL_GTT);
 	intel_ring_emit(ring, MSG_FBC_REND_STATE);
-	intel_ring_emit(ring, ring->scratch.gtt_offset + 256);
+	intel_ring_emit(ring, rq->engine->scratch.gtt_offset + 256);
 	intel_ring_advance(ring);
 
-	ring->fbc_dirty = false;
 	return 0;
 }
 
 static int
-gen7_render_ring_flush(struct intel_engine_cs *ring,
-		       u32 invalidate_domains, u32 flush_domains)
+gen7_render_emit_flush(struct i915_gem_request *rq, u32 flags)
 {
-	u32 flags = 0;
-	u32 scratch_addr = ring->scratch.gtt_offset + 2 * CACHELINE_BYTES;
+	const u32 scratch_addr = rq->engine->scratch.gtt_offset + 2 * CACHELINE_BYTES;
+	struct intel_ringbuffer *ring;
+	u32 cmd = 0;
 	int ret;
 
 	/*
@@ -334,63 +282,72 @@
 	 * read-cache invalidate bits set) must have the CS_STALL bit set. We
 	 * don't try to be clever and just set it unconditionally.
 	 */
-	flags |= PIPE_CONTROL_CS_STALL;
+	cmd |= PIPE_CONTROL_CS_STALL;
 
 	/* Just flush everything.  Experiments have shown that reducing the
 	 * number of bits based on the write domains has little performance
 	 * impact.
 	 */
-	if (flush_domains) {
-		flags |= PIPE_CONTROL_RENDER_TARGET_CACHE_FLUSH;
-		flags |= PIPE_CONTROL_DEPTH_CACHE_FLUSH;
-	}
-	if (invalidate_domains) {
-		flags |= PIPE_CONTROL_TLB_INVALIDATE;
-		flags |= PIPE_CONTROL_INSTRUCTION_CACHE_INVALIDATE;
-		flags |= PIPE_CONTROL_TEXTURE_CACHE_INVALIDATE;
-		flags |= PIPE_CONTROL_VF_CACHE_INVALIDATE;
-		flags |= PIPE_CONTROL_CONST_CACHE_INVALIDATE;
-		flags |= PIPE_CONTROL_STATE_CACHE_INVALIDATE;
+	if (flags & I915_FLUSH_CACHES) {
+		cmd |= PIPE_CONTROL_RENDER_TARGET_CACHE_FLUSH;
+		cmd |= PIPE_CONTROL_DEPTH_CACHE_FLUSH;
+	}
+	if (flags & I915_INVALIDATE_CACHES) {
+		cmd |= PIPE_CONTROL_TLB_INVALIDATE;
+		cmd |= PIPE_CONTROL_INSTRUCTION_CACHE_INVALIDATE;
+		cmd |= PIPE_CONTROL_TEXTURE_CACHE_INVALIDATE;
+		cmd |= PIPE_CONTROL_VF_CACHE_INVALIDATE;
+		cmd |= PIPE_CONTROL_CONST_CACHE_INVALIDATE;
+		cmd |= PIPE_CONTROL_STATE_CACHE_INVALIDATE;
 		/*
 		 * TLB invalidate requires a post-sync write.
 		 */
-		flags |= PIPE_CONTROL_QW_WRITE;
-		flags |= PIPE_CONTROL_GLOBAL_GTT_IVB;
+		cmd |= PIPE_CONTROL_QW_WRITE | PIPE_CONTROL_GLOBAL_GTT_IVB;
 
 		/* Workaround: we must issue a pipe_control with CS-stall bit
 		 * set before a pipe_control command that has the state cache
 		 * invalidate bit set. */
-		gen7_render_ring_cs_stall_wa(ring);
+		ret = gen7_render_ring_cs_stall_wa(rq);
+		if (ret)
+			return ret;
 	}
+	if ((flags & (I915_COMMAND_BARRIER | I915_FLUSH_CACHES)) == I915_COMMAND_BARRIER)
+		cmd |= PIPE_CONTROL_STALL_AT_SCOREBOARD;
 
-	ret = intel_ring_begin(ring, 4);
-	if (ret)
-		return ret;
+	ring = intel_ring_begin(rq, 4);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
 	intel_ring_emit(ring, GFX_OP_PIPE_CONTROL(4));
-	intel_ring_emit(ring, flags);
+	intel_ring_emit(ring, cmd);
 	intel_ring_emit(ring, scratch_addr);
 	intel_ring_emit(ring, 0);
 	intel_ring_advance(ring);
 
-	if (!invalidate_domains && flush_domains)
-		return gen7_ring_fbc_flush(ring, FBC_REND_NUKE);
+	if (flags & I915_KICK_FBC) {
+		ret = gen7_ring_fbc_flush(rq, FBC_REND_NUKE);
+		if (ret)
+			return ret;
+	}
 
 	return 0;
 }
 
 static int
-gen8_emit_pipe_control(struct intel_engine_cs *ring,
-		       u32 flags, u32 scratch_addr)
+gen8_emit_pipe_control(struct i915_gem_request *rq,
+		       u32 cmd, u32 scratch_addr)
 {
-	int ret;
+	struct intel_ringbuffer *ring;
 
-	ret = intel_ring_begin(ring, 6);
-	if (ret)
-		return ret;
+	if (cmd == 0)
+		return 0;
+
+	ring = intel_ring_begin(rq, 6);
+	if (IS_ERR(rq))
+		return PTR_ERR(rq);
 
 	intel_ring_emit(ring, GFX_OP_PIPE_CONTROL(6));
-	intel_ring_emit(ring, flags);
+	intel_ring_emit(ring, cmd);
 	intel_ring_emit(ring, scratch_addr);
 	intel_ring_emit(ring, 0);
 	intel_ring_emit(ring, 0);
@@ -401,31 +358,29 @@
 }
 
 static int
-gen8_render_ring_flush(struct intel_engine_cs *ring,
-		       u32 invalidate_domains, u32 flush_domains)
+gen8_render_emit_flush(struct i915_gem_request *rq,
+		       u32 flags)
 {
-	u32 flags = 0;
-	u32 scratch_addr = ring->scratch.gtt_offset + 2 * CACHELINE_BYTES;
+	const u32 scratch_addr = rq->engine->scratch.gtt_offset + 2 * CACHELINE_BYTES;
+	u32 cmd = 0;
 	int ret;
 
-	flags |= PIPE_CONTROL_CS_STALL;
-
-	if (flush_domains) {
-		flags |= PIPE_CONTROL_RENDER_TARGET_CACHE_FLUSH;
-		flags |= PIPE_CONTROL_DEPTH_CACHE_FLUSH;
-	}
-	if (invalidate_domains) {
-		flags |= PIPE_CONTROL_TLB_INVALIDATE;
-		flags |= PIPE_CONTROL_INSTRUCTION_CACHE_INVALIDATE;
-		flags |= PIPE_CONTROL_TEXTURE_CACHE_INVALIDATE;
-		flags |= PIPE_CONTROL_VF_CACHE_INVALIDATE;
-		flags |= PIPE_CONTROL_CONST_CACHE_INVALIDATE;
-		flags |= PIPE_CONTROL_STATE_CACHE_INVALIDATE;
-		flags |= PIPE_CONTROL_QW_WRITE;
-		flags |= PIPE_CONTROL_GLOBAL_GTT_IVB;
+	if (flags & I915_FLUSH_CACHES) {
+		cmd |= PIPE_CONTROL_RENDER_TARGET_CACHE_FLUSH;
+		cmd |= PIPE_CONTROL_DEPTH_CACHE_FLUSH;
+	}
+	if (flags & I915_INVALIDATE_CACHES) {
+		cmd |= PIPE_CONTROL_TLB_INVALIDATE;
+		cmd |= PIPE_CONTROL_INSTRUCTION_CACHE_INVALIDATE;
+		cmd |= PIPE_CONTROL_TEXTURE_CACHE_INVALIDATE;
+		cmd |= PIPE_CONTROL_VF_CACHE_INVALIDATE;
+		cmd |= PIPE_CONTROL_CONST_CACHE_INVALIDATE;
+		cmd |= PIPE_CONTROL_STATE_CACHE_INVALIDATE;
+		cmd |= PIPE_CONTROL_QW_WRITE;
+		cmd |= PIPE_CONTROL_GLOBAL_GTT_IVB;
 
 		/* WaCsStallBeforeStateCacheInvalidate:bdw,chv */
-		ret = gen8_emit_pipe_control(ring,
+		ret = gen8_emit_pipe_control(rq,
 					     PIPE_CONTROL_CS_STALL |
 					     PIPE_CONTROL_STALL_AT_SCOREBOARD,
 					     0);
@@ -433,200 +388,419 @@
 			return ret;
 	}
 
-	return gen8_emit_pipe_control(ring, flags, scratch_addr);
+	if (flags & I915_COMMAND_BARRIER) {
+		cmd |= PIPE_CONTROL_CS_STALL;
+		cmd |= PIPE_CONTROL_QW_WRITE;
+		cmd |= PIPE_CONTROL_GLOBAL_GTT_IVB;
+	}
+
+	ret = gen8_emit_pipe_control(rq, cmd, scratch_addr);
+	if (ret)
+		return ret;
+
+	if (flags & I915_KICK_FBC) {
+		ret = gen7_ring_fbc_flush(rq, FBC_REND_NUKE);
+		if (ret)
+			return ret;
+	}
+
+	return 0;
 }
 
-static void ring_write_tail(struct intel_engine_cs *ring,
+static void ring_write_tail(struct intel_engine_cs *engine,
 			    u32 value)
 {
-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
-	I915_WRITE_TAIL(ring, value);
+	struct drm_i915_private *dev_priv = engine->i915;
+	I915_WRITE_TAIL(engine, value);
 }
 
-u64 intel_ring_get_active_head(struct intel_engine_cs *ring)
+u64 intel_engine_get_active_head(struct intel_engine_cs *engine)
 {
-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
+	struct drm_i915_private *dev_priv = engine->i915;
 	u64 acthd;
 
-	if (INTEL_INFO(ring->dev)->gen >= 8)
-		acthd = I915_READ64_2x32(RING_ACTHD(ring->mmio_base),
-					 RING_ACTHD_UDW(ring->mmio_base));
-	else if (INTEL_INFO(ring->dev)->gen >= 4)
-		acthd = I915_READ(RING_ACTHD(ring->mmio_base));
+	if (INTEL_INFO(dev_priv)->gen >= 8)
+		acthd = I915_READ64_2x32(RING_ACTHD(engine->mmio_base),
+					 RING_ACTHD_UDW(engine->mmio_base));
+	else if (INTEL_INFO(dev_priv)->gen >= 4)
+		acthd = I915_READ(RING_ACTHD(engine->mmio_base));
 	else
 		acthd = I915_READ(ACTHD);
 
 	return acthd;
 }
 
-static void ring_setup_phys_status_page(struct intel_engine_cs *ring)
-{
-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
-	u32 addr;
-
-	addr = dev_priv->status_page_dmah->busaddr;
-	if (INTEL_INFO(ring->dev)->gen >= 4)
-		addr |= (dev_priv->status_page_dmah->busaddr >> 28) & 0xf0;
-	I915_WRITE(HWS_PGA, addr);
-}
-
-static bool stop_ring(struct intel_engine_cs *ring)
+static bool engine_stop(struct intel_engine_cs *engine)
 {
-	struct drm_i915_private *dev_priv = to_i915(ring->dev);
+	struct drm_i915_private *dev_priv = engine->i915;
 
-	if (!IS_GEN2(ring->dev)) {
-		I915_WRITE_MODE(ring, _MASKED_BIT_ENABLE(STOP_RING));
-		if (wait_for_atomic((I915_READ_MODE(ring) & MODE_IDLE) != 0, 1000)) {
-			DRM_ERROR("%s :timed out trying to stop ring\n", ring->name);
-			return false;
+	if (!IS_GEN2(dev_priv)) {
+		I915_WRITE_MODE(engine, _MASKED_BIT_ENABLE(STOP_RING));
+		if (wait_for((I915_READ_MODE(engine) & MODE_IDLE) != 0, 1000)) {
+			DRM_ERROR("%s : timed out trying to stop ring\n", engine->name);
+			/* Sometimes we observe that the idle flag is not
+			 * set even though the ring is empty. So double
+			 * check before giving up.
+			 */
+			if (I915_READ_HEAD(engine) != I915_READ_TAIL(engine))
+				return false;
 		}
 	}
 
-	I915_WRITE_CTL(ring, 0);
-	I915_WRITE_HEAD(ring, 0);
-	ring->write_tail(ring, 0);
+	I915_WRITE_CTL(engine, 0);
+	I915_WRITE_HEAD(engine, 0);
+	engine->write_tail(engine, 0);
 
-	if (!IS_GEN2(ring->dev)) {
-		(void)I915_READ_CTL(ring);
-		I915_WRITE_MODE(ring, _MASKED_BIT_DISABLE(STOP_RING));
+	if (!IS_GEN2(dev_priv)) {
+		(void)I915_READ_CTL(engine);
+		I915_WRITE_MODE(engine, _MASKED_BIT_DISABLE(STOP_RING));
 	}
 
-	return (I915_READ_HEAD(ring) & HEAD_ADDR) == 0;
+	return (I915_READ_HEAD(engine) & HEAD_ADDR) == 0;
 }
 
-static int init_ring_common(struct intel_engine_cs *ring)
+static int engine_suspend(struct intel_engine_cs *engine)
 {
-	struct drm_device *dev = ring->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_ringbuffer *ringbuf = ring->buffer;
-	struct drm_i915_gem_object *obj = ringbuf->obj;
+	struct intel_ringbuffer *ring;
+
+	if (!engine_stop(engine))
+		return -EIO;
+
+	ring = engine->default_context->ring[engine->id].ring;
+	i915_gem_object_ggtt_unpin(ring->obj);
+	return 0;
+}
+
+static int enable_status_page(struct intel_engine_cs *engine)
+{
+	struct drm_i915_private *dev_priv = engine->i915;
+	u32 mmio, addr;
 	int ret = 0;
 
-	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
+	if (!I915_NEED_GFX_HWS(dev_priv)) {
+		addr = dev_priv->status_page_dmah->busaddr;
+		if (INTEL_INFO(dev_priv)->gen >= 4)
+			addr |= (dev_priv->status_page_dmah->busaddr >> 28) & 0xf0;
+		mmio = HWS_PGA;
+	} else {
+		addr = engine->status_page.gfx_addr;
+		/* The ring status page addresses are no longer next to the rest of
+		 * the ring registers as of gen7.
+		 */
+		if (IS_GEN7(dev_priv)) {
+			switch (engine->id) {
+			default:
+			case RCS:
+				mmio = RENDER_HWS_PGA_GEN7;
+				break;
+			case BCS:
+				mmio = BLT_HWS_PGA_GEN7;
+				break;
+				/*
+				 * VCS2 actually doesn't exist on Gen7. Only shut up
+				 * gcc switch check warning
+				 */
+			case VCS2:
+			case VCS:
+				mmio = BSD_HWS_PGA_GEN7;
+				break;
+			case VECS:
+				mmio = VEBOX_HWS_PGA_GEN7;
+				break;
+			}
+		} else if (IS_GEN6(dev_priv)) {
+			mmio = RING_HWS_PGA_GEN6(engine->mmio_base);
+		} else {
+			/* XXX: gen8 returns to sanity */
+			mmio = RING_HWS_PGA(engine->mmio_base);
+		}
+	}
 
-	if (!stop_ring(ring)) {
-		/* G45 ring initialization often fails to reset head to zero */
-		DRM_DEBUG_KMS("%s head not reset to zero "
-			      "ctl %08x head %08x tail %08x start %08x\n",
-			      ring->name,
-			      I915_READ_CTL(ring),
-			      I915_READ_HEAD(ring),
-			      I915_READ_TAIL(ring),
-			      I915_READ_START(ring));
+	if (IS_GEN5(dev_priv) || IS_G4X(dev_priv)) {
+		if (wait_for((I915_READ(mmio) & 1) == 0, 1000))
+			DRM_ERROR("%s: wait for Translation-in-Progress to complete for HWS timed out\n",
+				  engine->name);
+	}
 
-		if (!stop_ring(ring)) {
-			DRM_ERROR("failed to set %s head to zero "
-				  "ctl %08x head %08x tail %08x start %08x\n",
-				  ring->name,
-				  I915_READ_CTL(ring),
-				  I915_READ_HEAD(ring),
-				  I915_READ_TAIL(ring),
-				  I915_READ_START(ring));
+	I915_WRITE(mmio, addr);
+	POSTING_READ(mmio);
+
+	/*
+	 * Flush the TLB for this page
+	 *
+	 * FIXME: These two bits have disappeared on gen8, so a question
+	 * arises: do we still need this and if so how should we go about
+	 * invalidating the TLB?
+	 */
+	if (INTEL_INFO(dev_priv)->gen >= 6 && INTEL_INFO(dev_priv)->gen < 8) {
+		u32 reg = RING_INSTPM(engine->mmio_base);
+
+		/* ring should be idle before issuing a sync flush*/
+		WARN_ON((I915_READ_MODE(engine) & MODE_IDLE) == 0);
+
+		I915_WRITE(reg,
+			   _MASKED_BIT_ENABLE(INSTPM_TLB_INVALIDATE |
+					      INSTPM_SYNC_FLUSH));
+		if (wait_for((I915_READ(reg) & INSTPM_SYNC_FLUSH) == 0,
+			     1000)) {
+			DRM_ERROR("%s: wait for SyncFlush to complete for TLB invalidation timed out\n",
+				  engine->name);
 			ret = -EIO;
-			goto out;
 		}
 	}
 
-	if (I915_NEED_GFX_HWS(dev))
-		intel_ring_setup_status_page(ring);
-	else
-		ring_setup_phys_status_page(ring);
+	return ret;
+}
+
+static int engine_resume(struct intel_engine_cs *engine)
+{
+	struct drm_i915_private *dev_priv = engine->i915;
+	struct intel_ringbuffer *ring;
+	int retry = 3, ret;
+
+	ring = engine->default_context->ring[engine->id].ring;
+	if (WARN_ON(ring == NULL))
+		return -ENODEV;
+
+	ret = i915_gem_object_ggtt_pin(ring->obj, 0, 0);
+	if (ret)
+		return ret;
+
+	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
+
+	ret = enable_status_page(engine);
 
+reset:
 	/* Enforce ordering by reading HEAD register back */
-	I915_READ_HEAD(ring);
+	engine->write_tail(engine, ring->tail);
+	I915_WRITE_HEAD(engine, ring->head);
+	(void)I915_READ_HEAD(engine);
 
 	/* Initialize the ring. This must happen _after_ we've cleared the ring
 	 * registers with the above sequence (the readback of the HEAD registers
 	 * also enforces ordering), otherwise the hw might lose the new ring
 	 * register values. */
-	I915_WRITE_START(ring, i915_gem_obj_ggtt_offset(obj));
-	I915_WRITE_CTL(ring,
-			((ringbuf->size - PAGE_SIZE) & RING_NR_PAGES)
-			| RING_VALID);
-
-	/* If the head is still not zero, the ring is dead */
-	if (wait_for((I915_READ_CTL(ring) & RING_VALID) != 0 &&
-		     I915_READ_START(ring) == i915_gem_obj_ggtt_offset(obj) &&
-		     (I915_READ_HEAD(ring) & HEAD_ADDR) == 0, 50)) {
-		DRM_ERROR("%s initialization failed "
-			  "ctl %08x (valid? %d) head %08x tail %08x start %08x [expected %08lx]\n",
-			  ring->name,
-			  I915_READ_CTL(ring), I915_READ_CTL(ring) & RING_VALID,
-			  I915_READ_HEAD(ring), I915_READ_TAIL(ring),
-			  I915_READ_START(ring), (unsigned long)i915_gem_obj_ggtt_offset(obj));
-		ret = -EIO;
-		goto out;
-	}
+	I915_WRITE_START(engine, i915_gem_obj_ggtt_offset(ring->obj));
 
-	if (!drm_core_check_feature(ring->dev, DRIVER_MODESET))
-		i915_kernel_lost_context(ring->dev);
-	else {
-		ringbuf->head = I915_READ_HEAD(ring);
-		ringbuf->tail = I915_READ_TAIL(ring) & TAIL_ADDR;
-		ringbuf->space = ring_space(ringbuf);
-		ringbuf->last_retired_head = -1;
+	/* WaClearRingBufHeadRegAtInit:ctg,elk */
+	if (I915_READ_HEAD(engine) != ring->head)
+		DRM_DEBUG("%s initialization failed [head=%08x], fudging\n",
+			  engine->name, I915_READ_HEAD(engine));
+	I915_WRITE_HEAD(engine, ring->head);
+	(void)I915_READ_HEAD(engine);
+
+	I915_WRITE_CTL(engine,
+		       ((ring->size - PAGE_SIZE) & RING_NR_PAGES)
+		       | RING_VALID);
+
+	if (wait_for((I915_READ_CTL(engine) & RING_VALID) != 0, 50)) {
+		if (retry-- && engine_stop(engine))
+			goto reset;
 	}
 
-	memset(&ring->hangcheck, 0, sizeof(ring->hangcheck));
+	if ((I915_READ_CTL(engine) & RING_VALID) == 0 ||
+	    I915_READ_START(engine) != i915_gem_obj_ggtt_offset(ring->obj)) {
+		DRM_ERROR("%s initialization failed "
+			  "ctl %08x (valid? %d) head %08x [expected %08x], tail %08x [expected %08x], start %08x [expected %08lx]\n",
+			  engine->name,
+			  I915_READ_CTL(engine), I915_READ_CTL(engine) & RING_VALID,
+			  I915_READ_HEAD(engine), ring->head,
+			  I915_READ_TAIL(engine), ring->tail,
+			  I915_READ_START(engine), (unsigned long)i915_gem_obj_ggtt_offset(ring->obj));
+		ret = -EIO;
+	}
 
-out:
 	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
-
 	return ret;
 }
 
+static int engine_add_request(struct i915_gem_request *rq)
+{
+	rq->engine->write_tail(rq->engine, rq->tail);
+	list_add_tail(&rq->engine_link, &rq->engine->requests);
+	return 0;
+}
+
+static bool engine_rq_is_complete(struct i915_gem_request *rq)
+{
+	return __i915_seqno_passed(intel_engine_get_seqno(rq->engine),
+				   rq->seqno);
+}
+
 static int
-init_pipe_control(struct intel_engine_cs *ring)
+init_broken_cs_tlb_wa(struct intel_engine_cs *engine)
 {
+	struct drm_i915_gem_object *obj;
 	int ret;
 
-	if (ring->scratch.obj)
-		return 0;
+	obj = i915_gem_object_create_stolen(engine->i915->dev, I830_WA_SIZE);
+	if (obj == NULL)
+		obj = i915_gem_alloc_object(engine->i915->dev, I830_WA_SIZE);
+	if (obj == NULL) {
+		DRM_ERROR("Failed to allocate batch bo\n");
+		return -ENOMEM;
+	}
+
+	ret = i915_gem_object_ggtt_pin(obj, 0, 0);
+	if (ret != 0) {
+		drm_gem_object_unreference(&obj->base);
+		DRM_ERROR("Failed to ping batch bo\n");
+		return ret;
+	}
+
+	engine->scratch.obj = obj;
+	engine->scratch.gtt_offset = i915_gem_obj_ggtt_offset(obj);
+	return 0;
+}
+
+static int
+init_pipe_control(struct intel_engine_cs *engine)
+{
+	int ret;
 
-	ring->scratch.obj = i915_gem_alloc_object(ring->dev, 4096);
-	if (ring->scratch.obj == NULL) {
-		DRM_ERROR("Failed to allocate seqno page\n");
+	engine->scratch.obj = i915_gem_alloc_object(engine->i915->dev, 4096);
+	if (engine->scratch.obj == NULL) {
 		ret = -ENOMEM;
 		goto err;
 	}
 
-	ret = i915_gem_object_set_cache_level(ring->scratch.obj, I915_CACHE_LLC);
+	ret = i915_gem_object_set_cache_level(engine->scratch.obj,
+					      I915_CACHE_LLC);
 	if (ret)
 		goto err_unref;
 
-	ret = i915_gem_obj_ggtt_pin(ring->scratch.obj, 4096, 0);
+	ret = i915_gem_object_ggtt_pin(engine->scratch.obj, 4096, 0);
 	if (ret)
 		goto err_unref;
 
-	ring->scratch.gtt_offset = i915_gem_obj_ggtt_offset(ring->scratch.obj);
-	ring->scratch.cpu_page = kmap(sg_page(ring->scratch.obj->pages->sgl));
-	if (ring->scratch.cpu_page == NULL) {
-		ret = -ENOMEM;
-		goto err_unpin;
-	}
-
+	engine->scratch.gtt_offset =
+		i915_gem_obj_ggtt_offset(engine->scratch.obj);
 	DRM_DEBUG_DRIVER("%s pipe control offset: 0x%08x\n",
-			 ring->name, ring->scratch.gtt_offset);
+			 engine->name, engine->scratch.gtt_offset);
 	return 0;
 
-err_unpin:
-	i915_gem_object_ggtt_unpin(ring->scratch.obj);
 err_unref:
-	drm_gem_object_unreference(&ring->scratch.obj->base);
+	drm_gem_object_unreference(&engine->scratch.obj->base);
+	engine->scratch.obj = NULL;
 err:
+	DRM_ERROR("Failed to allocate seqno page [%d]\n", ret);
 	return ret;
 }
 
-static int init_render_ring(struct intel_engine_cs *ring)
+static int
+emit_lri(struct i915_gem_request *rq,
+	 int num_registers,
+	 ...)
+{
+	struct intel_ringbuffer *ring;
+	va_list ap;
+
+	BUG_ON(num_registers > 60);
+
+	ring = intel_ring_begin(rq, 2*num_registers + 1);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
+
+	intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(num_registers));
+	va_start(ap, num_registers);
+	while (num_registers--) {
+		intel_ring_emit(ring, va_arg(ap, u32));
+		intel_ring_emit(ring, va_arg(ap, u32));
+	}
+	va_end(ap);
+	intel_ring_advance(ring);
+
+	return 0;
+}
+
+static int bdw_render_init_context(struct i915_gem_request *rq)
 {
-	struct drm_device *dev = ring->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	int ret = init_ring_common(ring);
+	int ret;
+
+	ret = emit_lri(rq, 6,
+
+	/* FIXME: Unclear whether we really need this on production bdw. */
+	GEN8_ROW_CHICKEN,
+	/* WaDisablePartialInstShootdown:bdw */
+	_MASKED_BIT_ENABLE(PARTIAL_INSTRUCTION_SHOOTDOWN_DISABLE) |
+	/* WaDisableThreadStallDopClockGating:bdw (pre-production) */
+	_MASKED_BIT_ENABLE(STALL_DOP_GATING_DISABLE),
+
+	GEN7_ROW_CHICKEN2,
+	/* WaDisableDopClockGating:bdw */
+	_MASKED_BIT_ENABLE(DOP_CLOCK_GATING_DISABLE),
+
+	HALF_SLICE_CHICKEN3,
+	_MASKED_BIT_ENABLE(GEN8_SAMPLER_POWER_BYPASS_DIS),
+
+	/* Use Force Non-Coherent whenever executing a 3D context. This is a
+	 * workaround for for a possible hang in the unlikely event a TLB
+	 * invalidation occurs during a PSD flush.
+	 */
+	HDC_CHICKEN0,
+	_MASKED_BIT_ENABLE(HDC_FORCE_NON_COHERENT) |
+	/* WaDisableFenceDestinationToSLM:bdw (GT3 pre-production) */
+	_MASKED_BIT_ENABLE(IS_BDW_GT3(rq->i915) ? HDC_FENCE_DEST_SLM_DISABLE : 0),
+
+	CACHE_MODE_1,
+	/* Wa4x4STCOptimizationDisable:bdw */
+	_MASKED_BIT_ENABLE(GEN8_4x4_STC_OPTIMIZATION_DISABLE),
+
+	/*
+	 * BSpec recommends 8x4 when MSAA is used,
+	 * however in practice 16x4 seems fastest.
+	 *
+	 * Note that PS/WM thread counts depend on the WIZ hashing
+	 * disable bit, which we don't touch here, but it's good
+	 * to keep in mind (see 3DSTATE_PS and 3DSTATE_WM).
+	 */
+	GEN7_GT_MODE,
+	GEN6_WIZ_HASHING_MASK | GEN6_WIZ_HASHING_16x4);
+	if (ret)
+		return ret;
+
+	return i915_gem_render_state_init(rq);
+}
+
+static int chv_render_init_context(struct i915_gem_request *rq)
+{
+	int ret;
+
+	ret = emit_lri(rq, 2,
+
+	GEN8_ROW_CHICKEN,
+	/* WaDisablePartialInstShootdown:chv */
+	_MASKED_BIT_ENABLE(PARTIAL_INSTRUCTION_SHOOTDOWN_DISABLE) |
+	/* WaDisableThreadStallDopClockGating:chv */
+	_MASKED_BIT_ENABLE(STALL_DOP_GATING_DISABLE),
+
+	/* Use Force Non-Coherent whenever executing a 3D context. This is a
+	 * workaround for a possible hang in the unlikely event a TLB
+	 * invalidation occurs during a PSD flush.
+	 */
+	HDC_CHICKEN0,
+	/* WaForceEnableNonCoherent:chv */
+	HDC_FORCE_NON_COHERENT |
+	/* WaHdcDisableFetchWhenMasked:chv */
+	HDC_DONOT_FETCH_MEM_WHEN_MASKED);
+
+	if (ret)
+		return ret;
+
+	return i915_gem_render_state_init(rq);
+}
+
+static int render_resume(struct intel_engine_cs *engine)
+{
+	struct drm_i915_private *dev_priv = engine->i915;
+	int ret;
+
+	ret = engine_resume(engine);
 	if (ret)
 		return ret;
 
 	/* WaTimedSingleVertexDispatch:cl,bw,ctg,elk,ilk,snb */
-	if (INTEL_INFO(dev)->gen >= 4 && INTEL_INFO(dev)->gen < 7)
+	if (INTEL_INFO(dev_priv)->gen >= 4 && INTEL_INFO(dev_priv)->gen < 7)
 		I915_WRITE(MI_MODE, _MASKED_BIT_ENABLE(VS_TIMER_DISPATCH));
 
 	/* We need to disable the AsyncFlip performance optimisations in order
@@ -635,28 +809,22 @@
 	 *
 	 * WaDisableAsyncFlipPerfMode:snb,ivb,hsw,vlv,bdw,chv
 	 */
-	if (INTEL_INFO(dev)->gen >= 6)
+	if (INTEL_INFO(dev_priv)->gen >= 6 && INTEL_INFO(dev_priv)->gen < 9)
 		I915_WRITE(MI_MODE, _MASKED_BIT_ENABLE(ASYNC_FLIP_PERF_DISABLE));
 
 	/* Required for the hardware to program scanline values for waiting */
 	/* WaEnableFlushTlbInvalidationMode:snb */
-	if (INTEL_INFO(dev)->gen == 6)
+	if (INTEL_INFO(dev_priv)->gen == 6)
 		I915_WRITE(GFX_MODE,
 			   _MASKED_BIT_ENABLE(GFX_TLB_INVALIDATE_EXPLICIT));
 
 	/* WaBCSVCSTlbInvalidationMode:ivb,vlv,hsw */
-	if (IS_GEN7(dev))
+	if (IS_GEN7(dev_priv))
 		I915_WRITE(GFX_MODE_GEN7,
 			   _MASKED_BIT_ENABLE(GFX_TLB_INVALIDATE_EXPLICIT) |
 			   _MASKED_BIT_ENABLE(GFX_REPLAY_MODE));
 
-	if (INTEL_INFO(dev)->gen >= 5) {
-		ret = init_pipe_control(ring);
-		if (ret)
-			return ret;
-	}
-
-	if (IS_GEN6(dev)) {
+	if (IS_GEN6(dev_priv)) {
 		/* From the Sandybridge PRM, volume 1 part 3, page 24:
 		 * "If this bit is set, STCunit will have LRA as replacement
 		 *  policy. [...] This bit must be reset.  LRA replacement
@@ -666,19 +834,50 @@
 			   _MASKED_BIT_DISABLE(CM0_STC_EVICT_DISABLE_LRA_SNB));
 	}
 
-	if (INTEL_INFO(dev)->gen >= 6)
+	if (INTEL_INFO(dev_priv)->gen >= 6)
 		I915_WRITE(INSTPM, _MASKED_BIT_ENABLE(INSTPM_FORCE_ORDERING));
 
-	if (HAS_L3_DPF(dev))
-		I915_WRITE_IMR(ring, ~GT_PARITY_ERROR(dev));
+	return 0;
+}
 
-	return ret;
+static void cleanup_status_page(struct intel_engine_cs *engine)
+{
+	struct drm_i915_gem_object *obj;
+
+	obj = engine->status_page.obj;
+	if (obj == NULL)
+		return;
+
+	kunmap(sg_page(obj->pages->sgl));
+	i915_gem_object_ggtt_unpin(obj);
+	drm_gem_object_unreference(&obj->base);
+	engine->status_page.obj = NULL;
+}
+
+static void engine_cleanup(struct intel_engine_cs *engine)
+{
+	cleanup_status_page(engine);
+	i915_cmd_parser_fini_engine(engine);
+}
+
+static bool engine_is_idle(struct intel_engine_cs *engine)
+{
+	struct drm_i915_private *dev_priv = engine->i915;
+	return I915_READ_MODE(engine) & MODE_IDLE;
+}
+
+static bool i8xx_is_idle(struct intel_engine_cs *engine)
+{
+	struct drm_i915_private *dev_priv = engine->i915;
+	return (I915_READ_HEAD(engine) == I915_READ(RING_ACTHD(engine->mmio_base)) &&
+		I915_READ_HEAD(engine) == I915_READ_TAIL(engine));
 }
 
-static void render_ring_cleanup(struct intel_engine_cs *ring)
+static void render_cleanup(struct intel_engine_cs *engine)
 {
-	struct drm_device *dev = ring->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = engine->i915;
+
+	engine_cleanup(engine);
 
 	if (dev_priv->semaphore_obj) {
 		i915_gem_object_ggtt_unpin(dev_priv->semaphore_obj);
@@ -686,163 +885,86 @@
 		dev_priv->semaphore_obj = NULL;
 	}
 
-	if (ring->scratch.obj == NULL)
-		return;
-
-	if (INTEL_INFO(dev)->gen >= 5) {
-		kunmap(sg_page(ring->scratch.obj->pages->sgl));
-		i915_gem_object_ggtt_unpin(ring->scratch.obj);
+	if (engine->scratch.obj) {
+		i915_gem_object_ggtt_unpin(engine->scratch.obj);
+		drm_gem_object_unreference(&engine->scratch.obj->base);
+		engine->scratch.obj = NULL;
 	}
-
-	drm_gem_object_unreference(&ring->scratch.obj->base);
-	ring->scratch.obj = NULL;
 }
 
-static int gen8_rcs_signal(struct intel_engine_cs *signaller,
-			   unsigned int num_dwords)
+static int
+gen8_rcs_emit_signal(struct i915_gem_request *rq, int id)
 {
-#define MBOX_UPDATE_DWORDS 8
-	struct drm_device *dev = signaller->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *waiter;
-	int i, ret, num_rings;
-
-	num_rings = hweight32(INTEL_INFO(dev)->ring_mask);
-	num_dwords += (num_rings-1) * MBOX_UPDATE_DWORDS;
-#undef MBOX_UPDATE_DWORDS
+	u64 offset = GEN8_SEMAPHORE_OFFSET(rq->i915, rq->engine->id, id);
+	struct intel_ringbuffer *ring;
 
-	ret = intel_ring_begin(signaller, num_dwords);
-	if (ret)
-		return ret;
+	ring = intel_ring_begin(rq, 8);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
-	for_each_ring(waiter, dev_priv, i) {
-		u64 gtt_offset = signaller->semaphore.signal_ggtt[i];
-		if (gtt_offset == MI_SEMAPHORE_SYNC_INVALID)
-			continue;
-
-		intel_ring_emit(signaller, GFX_OP_PIPE_CONTROL(6));
-		intel_ring_emit(signaller, PIPE_CONTROL_GLOBAL_GTT_IVB |
-					   PIPE_CONTROL_QW_WRITE |
-					   PIPE_CONTROL_FLUSH_ENABLE);
-		intel_ring_emit(signaller, lower_32_bits(gtt_offset));
-		intel_ring_emit(signaller, upper_32_bits(gtt_offset));
-		intel_ring_emit(signaller, signaller->outstanding_lazy_seqno);
-		intel_ring_emit(signaller, 0);
-		intel_ring_emit(signaller, MI_SEMAPHORE_SIGNAL |
-					   MI_SEMAPHORE_TARGET(waiter->id));
-		intel_ring_emit(signaller, 0);
-	}
-
-	return 0;
-}
-
-static int gen8_xcs_signal(struct intel_engine_cs *signaller,
-			   unsigned int num_dwords)
-{
-#define MBOX_UPDATE_DWORDS 6
-	struct drm_device *dev = signaller->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *waiter;
-	int i, ret, num_rings;
-
-	num_rings = hweight32(INTEL_INFO(dev)->ring_mask);
-	num_dwords += (num_rings-1) * MBOX_UPDATE_DWORDS;
-#undef MBOX_UPDATE_DWORDS
-
-	ret = intel_ring_begin(signaller, num_dwords);
-	if (ret)
-		return ret;
-
-	for_each_ring(waiter, dev_priv, i) {
-		u64 gtt_offset = signaller->semaphore.signal_ggtt[i];
-		if (gtt_offset == MI_SEMAPHORE_SYNC_INVALID)
-			continue;
-
-		intel_ring_emit(signaller, (MI_FLUSH_DW + 1) |
-					   MI_FLUSH_DW_OP_STOREDW);
-		intel_ring_emit(signaller, lower_32_bits(gtt_offset) |
-					   MI_FLUSH_DW_USE_GTT);
-		intel_ring_emit(signaller, upper_32_bits(gtt_offset));
-		intel_ring_emit(signaller, signaller->outstanding_lazy_seqno);
-		intel_ring_emit(signaller, MI_SEMAPHORE_SIGNAL |
-					   MI_SEMAPHORE_TARGET(waiter->id));
-		intel_ring_emit(signaller, 0);
-	}
+	intel_ring_emit(ring, GFX_OP_PIPE_CONTROL(6));
+	intel_ring_emit(ring,
+			PIPE_CONTROL_GLOBAL_GTT_IVB |
+			PIPE_CONTROL_QW_WRITE |
+			PIPE_CONTROL_FLUSH_ENABLE);
+	intel_ring_emit(ring, lower_32_bits(offset));
+	intel_ring_emit(ring, upper_32_bits(offset));
+	intel_ring_emit(ring, rq->seqno);
+	intel_ring_emit(ring, 0);
+	intel_ring_emit(ring,
+			MI_SEMAPHORE_SIGNAL |
+			MI_SEMAPHORE_TARGET(id));
+	intel_ring_emit(ring, 0);
+	intel_ring_advance(ring);
 
 	return 0;
 }
 
-static int gen6_signal(struct intel_engine_cs *signaller,
-		       unsigned int num_dwords)
+static int
+gen8_xcs_emit_signal(struct i915_gem_request *rq, int id)
 {
-	struct drm_device *dev = signaller->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *useless;
-	int i, ret, num_rings;
-
-#define MBOX_UPDATE_DWORDS 3
-	num_rings = hweight32(INTEL_INFO(dev)->ring_mask);
-	num_dwords += round_up((num_rings-1) * MBOX_UPDATE_DWORDS, 2);
-#undef MBOX_UPDATE_DWORDS
-
-	ret = intel_ring_begin(signaller, num_dwords);
-	if (ret)
-		return ret;
+	u64 offset = GEN8_SEMAPHORE_OFFSET(rq->i915, rq->engine->id, id);
+	struct intel_ringbuffer *ring;
 
-	for_each_ring(useless, dev_priv, i) {
-		u32 mbox_reg = signaller->semaphore.mbox.signal[i];
-		if (mbox_reg != GEN6_NOSYNC) {
-			intel_ring_emit(signaller, MI_LOAD_REGISTER_IMM(1));
-			intel_ring_emit(signaller, mbox_reg);
-			intel_ring_emit(signaller, signaller->outstanding_lazy_seqno);
-		}
-	}
+	ring = intel_ring_begin(rq, 6);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
-	/* If num_dwords was rounded, make sure the tail pointer is correct */
-	if (num_rings % 2 == 0)
-		intel_ring_emit(signaller, MI_NOOP);
+	intel_ring_emit(ring,
+			MI_FLUSH_DW |
+			MI_FLUSH_DW_OP_STOREDW |
+			(4 - 2));
+	intel_ring_emit(ring,
+			lower_32_bits(offset) |
+			MI_FLUSH_DW_USE_GTT);
+	intel_ring_emit(ring, upper_32_bits(offset));
+	intel_ring_emit(ring, rq->seqno);
+	intel_ring_emit(ring,
+			MI_SEMAPHORE_SIGNAL |
+			MI_SEMAPHORE_TARGET(id));
+	intel_ring_emit(ring, 0);
+	intel_ring_advance(ring);
 
 	return 0;
 }
 
-/**
- * gen6_add_request - Update the semaphore mailbox registers
- * 
- * @ring - ring that is adding a request
- * @seqno - return seqno stuck into the ring
- *
- * Update the mailbox registers in the *other* rings with the current seqno.
- * This acts like a signal in the canonical semaphore.
- */
 static int
-gen6_add_request(struct intel_engine_cs *ring)
+gen6_emit_signal(struct i915_gem_request *rq, int id)
 {
-	int ret;
-
-	if (ring->semaphore.signal)
-		ret = ring->semaphore.signal(ring, 4);
-	else
-		ret = intel_ring_begin(ring, 4);
+	struct intel_ringbuffer *ring;
 
-	if (ret)
-		return ret;
+	ring = intel_ring_begin(rq, 3);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
-	intel_ring_emit(ring, MI_STORE_DWORD_INDEX);
-	intel_ring_emit(ring, I915_GEM_HWS_INDEX << MI_STORE_DWORD_INDEX_SHIFT);
-	intel_ring_emit(ring, ring->outstanding_lazy_seqno);
-	intel_ring_emit(ring, MI_USER_INTERRUPT);
-	__intel_ring_advance(ring);
+	intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
+	intel_ring_emit(ring, rq->engine->semaphore.mbox.signal[id]);
+	intel_ring_emit(ring, rq->seqno);
+	intel_ring_advance(ring);
 
 	return 0;
 }
 
-static inline bool i915_gem_has_seqno_wrapped(struct drm_device *dev,
-					      u32 seqno)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	return dev_priv->last_seqno < seqno;
-}
-
 /**
  * intel_ring_sync - sync the waiter to the signaller on seqno
  *
@@ -852,504 +974,292 @@
  */
 
 static int
-gen8_ring_sync(struct intel_engine_cs *waiter,
-	       struct intel_engine_cs *signaller,
-	       u32 seqno)
+gen8_emit_wait(struct i915_gem_request *waiter,
+	       struct i915_gem_request *signaller)
 {
-	struct drm_i915_private *dev_priv = waiter->dev->dev_private;
-	int ret;
+	u64 offset = GEN8_SEMAPHORE_OFFSET(waiter->i915, signaller->engine->id, waiter->engine->id);
+	struct intel_ringbuffer *ring;
 
-	ret = intel_ring_begin(waiter, 4);
-	if (ret)
-		return ret;
+	ring = intel_ring_begin(waiter, 4);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
-	intel_ring_emit(waiter, MI_SEMAPHORE_WAIT |
-				MI_SEMAPHORE_GLOBAL_GTT |
-				MI_SEMAPHORE_POLL |
-				MI_SEMAPHORE_SAD_GTE_SDD);
-	intel_ring_emit(waiter, seqno);
-	intel_ring_emit(waiter,
-			lower_32_bits(GEN8_WAIT_OFFSET(waiter, signaller->id)));
-	intel_ring_emit(waiter,
-			upper_32_bits(GEN8_WAIT_OFFSET(waiter, signaller->id)));
-	intel_ring_advance(waiter);
+	intel_ring_emit(ring,
+			MI_SEMAPHORE_WAIT |
+			MI_SEMAPHORE_GLOBAL_GTT |
+			MI_SEMAPHORE_POLL |
+			MI_SEMAPHORE_SAD_GTE_SDD);
+	intel_ring_emit(ring, signaller->breadcrumb[waiter->engine->id]);
+	intel_ring_emit(ring, lower_32_bits(offset));
+	intel_ring_emit(ring, upper_32_bits(offset));
+	intel_ring_advance(ring);
 	return 0;
 }
 
 static int
-gen6_ring_sync(struct intel_engine_cs *waiter,
-	       struct intel_engine_cs *signaller,
-	       u32 seqno)
+gen6_emit_wait(struct i915_gem_request *waiter,
+	       struct i915_gem_request *signaller)
 {
 	u32 dw1 = MI_SEMAPHORE_MBOX |
 		  MI_SEMAPHORE_COMPARE |
 		  MI_SEMAPHORE_REGISTER;
-	u32 wait_mbox = signaller->semaphore.mbox.wait[waiter->id];
-	int ret;
-
-	/* Throughout all of the GEM code, seqno passed implies our current
-	 * seqno is >= the last seqno executed. However for hardware the
-	 * comparison is strictly greater than.
-	 */
-	seqno -= 1;
+	u32 wait_mbox = signaller->engine->semaphore.mbox.wait[waiter->engine->id];
+	struct intel_ringbuffer *ring;
 
 	WARN_ON(wait_mbox == MI_SEMAPHORE_SYNC_INVALID);
 
-	ret = intel_ring_begin(waiter, 4);
-	if (ret)
-		return ret;
-
-	/* If seqno wrap happened, omit the wait with no-ops */
-	if (likely(!i915_gem_has_seqno_wrapped(waiter->dev, seqno))) {
-		intel_ring_emit(waiter, dw1 | wait_mbox);
-		intel_ring_emit(waiter, seqno);
-		intel_ring_emit(waiter, 0);
-		intel_ring_emit(waiter, MI_NOOP);
-	} else {
-		intel_ring_emit(waiter, MI_NOOP);
-		intel_ring_emit(waiter, MI_NOOP);
-		intel_ring_emit(waiter, MI_NOOP);
-		intel_ring_emit(waiter, MI_NOOP);
-	}
-	intel_ring_advance(waiter);
-
-	return 0;
-}
-
-#define PIPE_CONTROL_FLUSH(ring__, addr__)					\
-do {									\
-	intel_ring_emit(ring__, GFX_OP_PIPE_CONTROL(4) | PIPE_CONTROL_QW_WRITE |		\
-		 PIPE_CONTROL_DEPTH_STALL);				\
-	intel_ring_emit(ring__, (addr__) | PIPE_CONTROL_GLOBAL_GTT);			\
-	intel_ring_emit(ring__, 0);							\
-	intel_ring_emit(ring__, 0);							\
-} while (0)
-
-static int
-pc_render_add_request(struct intel_engine_cs *ring)
-{
-	u32 scratch_addr = ring->scratch.gtt_offset + 2 * CACHELINE_BYTES;
-	int ret;
+	ring = intel_ring_begin(waiter, 3);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
-	/* For Ironlake, MI_USER_INTERRUPT was deprecated and apparently
-	 * incoherent with writes to memory, i.e. completely fubar,
-	 * so we need to use PIPE_NOTIFY instead.
-	 *
-	 * However, we also need to workaround the qword write
-	 * incoherence by flushing the 6 PIPE_NOTIFY buffers out to
-	 * memory before requesting an interrupt.
+	intel_ring_emit(ring, dw1 | wait_mbox);
+	/* Throughout all of the GEM code, seqno passed implies our current
+	 * seqno is >= the last seqno executed. However for hardware the
+	 * comparison is strictly greater than.
 	 */
-	ret = intel_ring_begin(ring, 32);
-	if (ret)
-		return ret;
-
-	intel_ring_emit(ring, GFX_OP_PIPE_CONTROL(4) | PIPE_CONTROL_QW_WRITE |
-			PIPE_CONTROL_WRITE_FLUSH |
-			PIPE_CONTROL_TEXTURE_CACHE_INVALIDATE);
-	intel_ring_emit(ring, ring->scratch.gtt_offset | PIPE_CONTROL_GLOBAL_GTT);
-	intel_ring_emit(ring, ring->outstanding_lazy_seqno);
+	intel_ring_emit(ring, signaller->breadcrumb[waiter->engine->id] - 1);
 	intel_ring_emit(ring, 0);
-	PIPE_CONTROL_FLUSH(ring, scratch_addr);
-	scratch_addr += 2 * CACHELINE_BYTES; /* write to separate cachelines */
-	PIPE_CONTROL_FLUSH(ring, scratch_addr);
-	scratch_addr += 2 * CACHELINE_BYTES;
-	PIPE_CONTROL_FLUSH(ring, scratch_addr);
-	scratch_addr += 2 * CACHELINE_BYTES;
-	PIPE_CONTROL_FLUSH(ring, scratch_addr);
-	scratch_addr += 2 * CACHELINE_BYTES;
-	PIPE_CONTROL_FLUSH(ring, scratch_addr);
-	scratch_addr += 2 * CACHELINE_BYTES;
-	PIPE_CONTROL_FLUSH(ring, scratch_addr);
-
-	intel_ring_emit(ring, GFX_OP_PIPE_CONTROL(4) | PIPE_CONTROL_QW_WRITE |
-			PIPE_CONTROL_WRITE_FLUSH |
-			PIPE_CONTROL_TEXTURE_CACHE_INVALIDATE |
-			PIPE_CONTROL_NOTIFY);
-	intel_ring_emit(ring, ring->scratch.gtt_offset | PIPE_CONTROL_GLOBAL_GTT);
-	intel_ring_emit(ring, ring->outstanding_lazy_seqno);
-	intel_ring_emit(ring, 0);
-	__intel_ring_advance(ring);
+	intel_ring_advance(ring);
 
 	return 0;
 }
 
-static u32
-gen6_ring_get_seqno(struct intel_engine_cs *ring, bool lazy_coherency)
-{
-	/* Workaround to force correct ordering between irq and seqno writes on
-	 * ivb (and maybe also on snb) by reading from a CS register (like
-	 * ACTHD) before reading the status page. */
-	if (!lazy_coherency) {
-		struct drm_i915_private *dev_priv = ring->dev->dev_private;
-		POSTING_READ(RING_ACTHD(ring->mmio_base));
-	}
-
-	return intel_read_status_page(ring, I915_GEM_HWS_INDEX);
-}
-
-static u32
-ring_get_seqno(struct intel_engine_cs *ring, bool lazy_coherency)
-{
-	return intel_read_status_page(ring, I915_GEM_HWS_INDEX);
-}
-
 static void
-ring_set_seqno(struct intel_engine_cs *ring, u32 seqno)
+gen5_irq_get(struct intel_engine_cs *engine)
 {
-	intel_write_status_page(ring, I915_GEM_HWS_INDEX, seqno);
-}
-
-static u32
-pc_render_get_seqno(struct intel_engine_cs *ring, bool lazy_coherency)
-{
-	return ring->scratch.cpu_page[0];
-}
-
-static void
-pc_render_set_seqno(struct intel_engine_cs *ring, u32 seqno)
-{
-	ring->scratch.cpu_page[0] = seqno;
-}
-
-static bool
-gen5_ring_get_irq(struct intel_engine_cs *ring)
-{
-	struct drm_device *dev = ring->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_private *i915 = engine->i915;
 	unsigned long flags;
 
-	if (!dev->irq_enabled)
-		return false;
-
-	spin_lock_irqsave(&dev_priv->irq_lock, flags);
-	if (ring->irq_refcount++ == 0)
-		gen5_enable_gt_irq(dev_priv, ring->irq_enable_mask);
-	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
-
-	return true;
+	spin_lock_irqsave(&i915->irq_lock, flags);
+	if (engine->irq_refcount++ == 0)
+		gen5_enable_gt_irq(i915, engine->irq_enable_mask);
+	spin_unlock_irqrestore(&i915->irq_lock, flags);
 }
 
 static void
-gen5_ring_put_irq(struct intel_engine_cs *ring)
+gen5_irq_put(struct intel_engine_cs *engine)
 {
-	struct drm_device *dev = ring->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_private *i915 = engine->i915;
 	unsigned long flags;
 
-	spin_lock_irqsave(&dev_priv->irq_lock, flags);
-	if (--ring->irq_refcount == 0)
-		gen5_disable_gt_irq(dev_priv, ring->irq_enable_mask);
-	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
+	spin_lock_irqsave(&i915->irq_lock, flags);
+	if (--engine->irq_refcount == 0)
+		gen5_disable_gt_irq(i915, engine->irq_enable_mask);
+	spin_unlock_irqrestore(&i915->irq_lock, flags);
 }
 
-static bool
-i9xx_ring_get_irq(struct intel_engine_cs *ring)
+static void
+i9xx_irq_get(struct intel_engine_cs *engine)
 {
-	struct drm_device *dev = ring->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = engine->i915;
 	unsigned long flags;
 
-	if (!dev->irq_enabled)
-		return false;
-
 	spin_lock_irqsave(&dev_priv->irq_lock, flags);
-	if (ring->irq_refcount++ == 0) {
-		dev_priv->irq_mask &= ~ring->irq_enable_mask;
+	if (engine->irq_refcount++ == 0) {
+		dev_priv->irq_mask &= ~engine->irq_enable_mask;
 		I915_WRITE(IMR, dev_priv->irq_mask);
 		POSTING_READ(IMR);
 	}
 	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
-
-	return true;
 }
 
 static void
-i9xx_ring_put_irq(struct intel_engine_cs *ring)
+i9xx_irq_put(struct intel_engine_cs *engine)
 {
-	struct drm_device *dev = ring->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = engine->i915;
 	unsigned long flags;
 
 	spin_lock_irqsave(&dev_priv->irq_lock, flags);
-	if (--ring->irq_refcount == 0) {
-		dev_priv->irq_mask |= ring->irq_enable_mask;
+	if (--engine->irq_refcount == 0) {
+		dev_priv->irq_mask |= engine->irq_enable_mask;
 		I915_WRITE(IMR, dev_priv->irq_mask);
 		POSTING_READ(IMR);
 	}
 	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
 }
 
-static bool
-i8xx_ring_get_irq(struct intel_engine_cs *ring)
+static void
+i8xx_irq_get(struct intel_engine_cs *engine)
 {
-	struct drm_device *dev = ring->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = engine->i915;
 	unsigned long flags;
 
-	if (!dev->irq_enabled)
-		return false;
-
 	spin_lock_irqsave(&dev_priv->irq_lock, flags);
-	if (ring->irq_refcount++ == 0) {
-		dev_priv->irq_mask &= ~ring->irq_enable_mask;
+	if (engine->irq_refcount++ == 0) {
+		dev_priv->irq_mask &= ~engine->irq_enable_mask;
 		I915_WRITE16(IMR, dev_priv->irq_mask);
 		POSTING_READ16(IMR);
 	}
 	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
-
-	return true;
 }
 
 static void
-i8xx_ring_put_irq(struct intel_engine_cs *ring)
+i8xx_irq_put(struct intel_engine_cs *engine)
 {
-	struct drm_device *dev = ring->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = engine->i915;
 	unsigned long flags;
 
 	spin_lock_irqsave(&dev_priv->irq_lock, flags);
-	if (--ring->irq_refcount == 0) {
-		dev_priv->irq_mask |= ring->irq_enable_mask;
+	if (--engine->irq_refcount == 0) {
+		dev_priv->irq_mask |= engine->irq_enable_mask;
 		I915_WRITE16(IMR, dev_priv->irq_mask);
 		POSTING_READ16(IMR);
 	}
 	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
 }
 
-void intel_ring_setup_status_page(struct intel_engine_cs *ring)
-{
-	struct drm_device *dev = ring->dev;
-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
-	u32 mmio = 0;
-
-	/* The ring status page addresses are no longer next to the rest of
-	 * the ring registers as of gen7.
-	 */
-	if (IS_GEN7(dev)) {
-		switch (ring->id) {
-		case RCS:
-			mmio = RENDER_HWS_PGA_GEN7;
-			break;
-		case BCS:
-			mmio = BLT_HWS_PGA_GEN7;
-			break;
-		/*
-		 * VCS2 actually doesn't exist on Gen7. Only shut up
-		 * gcc switch check warning
-		 */
-		case VCS2:
-		case VCS:
-			mmio = BSD_HWS_PGA_GEN7;
-			break;
-		case VECS:
-			mmio = VEBOX_HWS_PGA_GEN7;
-			break;
-		}
-	} else if (IS_GEN6(ring->dev)) {
-		mmio = RING_HWS_PGA_GEN6(ring->mmio_base);
-	} else {
-		/* XXX: gen8 returns to sanity */
-		mmio = RING_HWS_PGA(ring->mmio_base);
-	}
-
-	I915_WRITE(mmio, (u32)ring->status_page.gfx_addr);
-	POSTING_READ(mmio);
-
-	/*
-	 * Flush the TLB for this page
-	 *
-	 * FIXME: These two bits have disappeared on gen8, so a question
-	 * arises: do we still need this and if so how should we go about
-	 * invalidating the TLB?
-	 */
-	if (INTEL_INFO(dev)->gen >= 6 && INTEL_INFO(dev)->gen < 8) {
-		u32 reg = RING_INSTPM(ring->mmio_base);
-
-		/* ring should be idle before issuing a sync flush*/
-		WARN_ON((I915_READ_MODE(ring) & MODE_IDLE) == 0);
-
-		I915_WRITE(reg,
-			   _MASKED_BIT_ENABLE(INSTPM_TLB_INVALIDATE |
-					      INSTPM_SYNC_FLUSH));
-		if (wait_for((I915_READ(reg) & INSTPM_SYNC_FLUSH) == 0,
-			     1000))
-			DRM_ERROR("%s: wait for SyncFlush to complete for TLB invalidation timed out\n",
-				  ring->name);
-	}
-}
-
 static int
-bsd_ring_flush(struct intel_engine_cs *ring,
-	       u32     invalidate_domains,
-	       u32     flush_domains)
+bsd_emit_flush(struct i915_gem_request *rq,
+	       u32 flags)
 {
-	int ret;
+	struct intel_ringbuffer *ring;
 
-	ret = intel_ring_begin(ring, 2);
-	if (ret)
-		return ret;
+	ring = intel_ring_begin(rq, 1);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
 	intel_ring_emit(ring, MI_FLUSH);
-	intel_ring_emit(ring, MI_NOOP);
 	intel_ring_advance(ring);
 	return 0;
 }
 
 static int
-i9xx_add_request(struct intel_engine_cs *ring)
+emit_breadcrumb(struct i915_gem_request *rq)
 {
-	int ret;
+	struct intel_ringbuffer *ring;
 
-	ret = intel_ring_begin(ring, 4);
-	if (ret)
-		return ret;
+	ring = intel_ring_begin(rq, 4);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
 	intel_ring_emit(ring, MI_STORE_DWORD_INDEX);
 	intel_ring_emit(ring, I915_GEM_HWS_INDEX << MI_STORE_DWORD_INDEX_SHIFT);
-	intel_ring_emit(ring, ring->outstanding_lazy_seqno);
+	intel_ring_emit(ring, rq->seqno);
 	intel_ring_emit(ring, MI_USER_INTERRUPT);
-	__intel_ring_advance(ring);
+	intel_ring_advance(ring);
 
 	return 0;
 }
 
-static bool
-gen6_ring_get_irq(struct intel_engine_cs *ring)
+static void
+gen6_irq_get(struct intel_engine_cs *engine)
 {
-	struct drm_device *dev = ring->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = engine->i915;
 	unsigned long flags;
 
-	if (!dev->irq_enabled)
-	       return false;
-
 	spin_lock_irqsave(&dev_priv->irq_lock, flags);
-	if (ring->irq_refcount++ == 0) {
-		if (HAS_L3_DPF(dev) && ring->id == RCS)
-			I915_WRITE_IMR(ring,
-				       ~(ring->irq_enable_mask |
-					 GT_PARITY_ERROR(dev)));
-		else
-			I915_WRITE_IMR(ring, ~ring->irq_enable_mask);
-		gen5_enable_gt_irq(dev_priv, ring->irq_enable_mask);
+	if (engine->irq_refcount++ == 0) {
+		I915_WRITE_IMR(engine,
+			       ~(engine->irq_enable_mask |
+				 engine->irq_keep_mask));
+		gen5_enable_gt_irq(dev_priv, engine->irq_enable_mask);
 	}
 	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
 
-	return true;
+	/* Keep the device awake to save expensive CPU cycles when
+	 * reading the registers.
+	 */
+	gen6_gt_force_wake_get(dev_priv, engine->power_domains);
+}
+
+static void
+gen6_irq_barrier(struct intel_engine_cs *engine)
+{
+	/* w/a for lax serialisation of GPU writes with IRQs */
+	struct drm_i915_private *dev_priv = engine->i915;
+	(void)I915_READ(RING_ACTHD(engine->mmio_base));
 }
 
 static void
-gen6_ring_put_irq(struct intel_engine_cs *ring)
+gen6_irq_put(struct intel_engine_cs *engine)
 {
-	struct drm_device *dev = ring->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = engine->i915;
 	unsigned long flags;
 
+	gen6_gt_force_wake_put(dev_priv, engine->power_domains);
+
 	spin_lock_irqsave(&dev_priv->irq_lock, flags);
-	if (--ring->irq_refcount == 0) {
-		if (HAS_L3_DPF(dev) && ring->id == RCS)
-			I915_WRITE_IMR(ring, ~GT_PARITY_ERROR(dev));
-		else
-			I915_WRITE_IMR(ring, ~0);
-		gen5_disable_gt_irq(dev_priv, ring->irq_enable_mask);
+	if (--engine->irq_refcount == 0) {
+		I915_WRITE_IMR(engine, ~engine->irq_keep_mask);
+		gen5_disable_gt_irq(dev_priv, engine->irq_enable_mask);
 	}
 	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
 }
 
-static bool
-hsw_vebox_get_irq(struct intel_engine_cs *ring)
+static void
+hsw_vebox_irq_get(struct intel_engine_cs *engine)
 {
-	struct drm_device *dev = ring->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = engine->i915;
 	unsigned long flags;
 
-	if (!dev->irq_enabled)
-		return false;
-
 	spin_lock_irqsave(&dev_priv->irq_lock, flags);
-	if (ring->irq_refcount++ == 0) {
-		I915_WRITE_IMR(ring, ~ring->irq_enable_mask);
-		gen6_enable_pm_irq(dev_priv, ring->irq_enable_mask);
+	if (engine->irq_refcount++ == 0) {
+		I915_WRITE_IMR(engine,
+			       ~(engine->irq_enable_mask |
+				 engine->irq_keep_mask));
+		gen6_enable_pm_irq(dev_priv, engine->irq_enable_mask);
 	}
 	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
 
-	return true;
+	gen6_gt_force_wake_get(dev_priv, engine->power_domains);
 }
 
 static void
-hsw_vebox_put_irq(struct intel_engine_cs *ring)
+hsw_vebox_irq_put(struct intel_engine_cs *engine)
 {
-	struct drm_device *dev = ring->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = engine->i915;
 	unsigned long flags;
 
-	if (!dev->irq_enabled)
-		return;
+	gen6_gt_force_wake_put(dev_priv, engine->power_domains);
 
 	spin_lock_irqsave(&dev_priv->irq_lock, flags);
-	if (--ring->irq_refcount == 0) {
-		I915_WRITE_IMR(ring, ~0);
-		gen6_disable_pm_irq(dev_priv, ring->irq_enable_mask);
+	if (--engine->irq_refcount == 0) {
+		I915_WRITE_IMR(engine, ~engine->irq_keep_mask);
+		gen6_disable_pm_irq(dev_priv, engine->irq_enable_mask);
 	}
 	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
 }
 
-static bool
-gen8_ring_get_irq(struct intel_engine_cs *ring)
+static void
+gen8_irq_get(struct intel_engine_cs *engine)
 {
-	struct drm_device *dev = ring->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = engine->i915;
 	unsigned long flags;
 
-	if (!dev->irq_enabled)
-		return false;
-
 	spin_lock_irqsave(&dev_priv->irq_lock, flags);
-	if (ring->irq_refcount++ == 0) {
-		if (HAS_L3_DPF(dev) && ring->id == RCS) {
-			I915_WRITE_IMR(ring,
-				       ~(ring->irq_enable_mask |
-					 GT_RENDER_L3_PARITY_ERROR_INTERRUPT));
-		} else {
-			I915_WRITE_IMR(ring, ~ring->irq_enable_mask);
-		}
-		POSTING_READ(RING_IMR(ring->mmio_base));
+	if (engine->irq_refcount++ == 0) {
+		I915_WRITE_IMR(engine,
+			       ~(engine->irq_enable_mask |
+				 engine->irq_keep_mask));
+		POSTING_READ(RING_IMR(engine->mmio_base));
 	}
 	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
-
-	return true;
 }
 
 static void
-gen8_ring_put_irq(struct intel_engine_cs *ring)
+gen8_irq_put(struct intel_engine_cs *engine)
 {
-	struct drm_device *dev = ring->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_private *dev_priv = engine->i915;
 	unsigned long flags;
 
 	spin_lock_irqsave(&dev_priv->irq_lock, flags);
-	if (--ring->irq_refcount == 0) {
-		if (HAS_L3_DPF(dev) && ring->id == RCS) {
-			I915_WRITE_IMR(ring,
-				       ~GT_RENDER_L3_PARITY_ERROR_INTERRUPT);
-		} else {
-			I915_WRITE_IMR(ring, ~0);
-		}
-		POSTING_READ(RING_IMR(ring->mmio_base));
+	if (--engine->irq_refcount == 0) {
+		I915_WRITE_IMR(engine, ~engine->irq_keep_mask);
+		POSTING_READ(RING_IMR(engine->mmio_base));
 	}
 	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
 }
 
 static int
-i965_dispatch_execbuffer(struct intel_engine_cs *ring,
-			 u64 offset, u32 length,
-			 unsigned flags)
-{
-	int ret;
-
-	ret = intel_ring_begin(ring, 2);
-	if (ret)
-		return ret;
+i965_emit_batchbuffer(struct i915_gem_request *rq,
+		      u64 offset, u32 length,
+		      unsigned flags)
+{
+	struct intel_ringbuffer *ring;
+
+	ring = intel_ring_begin(rq, 2);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
 	intel_ring_emit(ring,
 			MI_BATCH_BUFFER_START |
@@ -1361,38 +1271,35 @@
 	return 0;
 }
 
-/* Just userspace ABI convention to limit the wa batch bo to a resonable size */
-#define I830_BATCH_LIMIT (256*1024)
-#define I830_TLB_ENTRIES (2)
-#define I830_WA_SIZE max(I830_TLB_ENTRIES*4096, I830_BATCH_LIMIT)
 static int
-i830_dispatch_execbuffer(struct intel_engine_cs *ring,
-				u64 offset, u32 len,
-				unsigned flags)
-{
-	u32 cs_offset = ring->scratch.gtt_offset;
-	int ret;
-
-	ret = intel_ring_begin(ring, 6);
-	if (ret)
-		return ret;
-
-	/* Evict the invalid PTE TLBs */
-	intel_ring_emit(ring, COLOR_BLT_CMD | BLT_WRITE_RGBA);
-	intel_ring_emit(ring, BLT_DEPTH_32 | BLT_ROP_COLOR_COPY | 4096);
-	intel_ring_emit(ring, I830_TLB_ENTRIES << 16 | 4); /* load each page */
-	intel_ring_emit(ring, cs_offset);
-	intel_ring_emit(ring, 0xdeadbeef);
-	intel_ring_emit(ring, MI_NOOP);
-	intel_ring_advance(ring);
+i830_emit_batchbuffer(struct i915_gem_request *rq,
+		      u64 offset, u32 len,
+		      unsigned flags)
+{
+	u32 cs_offset = rq->engine->scratch.gtt_offset;
+	struct intel_ringbuffer *ring;
+
+	if (rq->batch->vm->dirty) {
+		ring = intel_ring_begin(rq, 5);
+		if (IS_ERR(ring))
+			return PTR_ERR(ring);
+
+		/* Evict the invalid PTE TLBs */
+		intel_ring_emit(ring, COLOR_BLT_CMD | BLT_WRITE_RGBA);
+		intel_ring_emit(ring, BLT_DEPTH_32 | BLT_ROP_COLOR_COPY | 4096);
+		intel_ring_emit(ring, I830_TLB_ENTRIES << 16 | 4);
+		intel_ring_emit(ring, cs_offset);
+		intel_ring_emit(ring, 0xdeadbeef);
+		intel_ring_advance(ring);
+	}
 
 	if ((flags & I915_DISPATCH_PINNED) == 0) {
 		if (len > I830_BATCH_LIMIT)
 			return -ENOSPC;
 
-		ret = intel_ring_begin(ring, 6 + 2);
-		if (ret)
-			return ret;
+		ring = intel_ring_begin(rq, 6 + 1);
+		if (IS_ERR(ring))
+			return PTR_ERR(ring);
 
 		/* Blit the batch (which has now all relocs applied) to the
 		 * stable batch scratch bo area (so that the CS never
@@ -1406,36 +1313,34 @@
 		intel_ring_emit(ring, offset);
 
 		intel_ring_emit(ring, MI_FLUSH);
-		intel_ring_emit(ring, MI_NOOP);
 		intel_ring_advance(ring);
 
 		/* ... and execute it. */
 		offset = cs_offset;
 	}
 
-	ret = intel_ring_begin(ring, 4);
-	if (ret)
-		return ret;
+	ring = intel_ring_begin(rq, 3);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
 	intel_ring_emit(ring, MI_BATCH_BUFFER);
 	intel_ring_emit(ring, offset | (flags & I915_DISPATCH_SECURE ? 0 : MI_BATCH_NON_SECURE));
 	intel_ring_emit(ring, offset + len - 8);
-	intel_ring_emit(ring, MI_NOOP);
 	intel_ring_advance(ring);
 
 	return 0;
 }
 
 static int
-i915_dispatch_execbuffer(struct intel_engine_cs *ring,
-			 u64 offset, u32 len,
-			 unsigned flags)
-{
-	int ret;
-
-	ret = intel_ring_begin(ring, 2);
-	if (ret)
-		return ret;
+i915_emit_batchbuffer(struct i915_gem_request *rq,
+		      u64 offset, u32 len,
+		      unsigned flags)
+{
+	struct intel_ringbuffer *ring;
+
+	ring = intel_ring_begin(rq, 2);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
 	intel_ring_emit(ring, MI_BATCH_BUFFER_START | MI_BATCH_GTT);
 	intel_ring_emit(ring, offset | (flags & I915_DISPATCH_SECURE ? 0 : MI_BATCH_NON_SECURE));
@@ -1444,485 +1349,434 @@
 	return 0;
 }
 
-static void cleanup_status_page(struct intel_engine_cs *ring)
-{
-	struct drm_i915_gem_object *obj;
-
-	obj = ring->status_page.obj;
-	if (obj == NULL)
-		return;
-
-	kunmap(sg_page(obj->pages->sgl));
-	i915_gem_object_ggtt_unpin(obj);
-	drm_gem_object_unreference(&obj->base);
-	ring->status_page.obj = NULL;
-}
-
-static int init_status_page(struct intel_engine_cs *ring)
+static int setup_status_page(struct intel_engine_cs *engine)
 {
 	struct drm_i915_gem_object *obj;
+	unsigned flags;
+	int ret;
 
-	if ((obj = ring->status_page.obj) == NULL) {
-		unsigned flags;
-		int ret;
-
-		obj = i915_gem_alloc_object(ring->dev, 4096);
-		if (obj == NULL) {
-			DRM_ERROR("Failed to allocate status page\n");
-			return -ENOMEM;
-		}
+	obj = i915_gem_alloc_object(engine->i915->dev, 4096);
+	if (obj == NULL) {
+		DRM_ERROR("Failed to allocate status page\n");
+		return -ENOMEM;
+	}
 
-		ret = i915_gem_object_set_cache_level(obj, I915_CACHE_LLC);
-		if (ret)
-			goto err_unref;
+	ret = i915_gem_object_set_cache_level(obj, I915_CACHE_LLC);
+	if (ret)
+		goto err_unref;
 
-		flags = 0;
-		if (!HAS_LLC(ring->dev))
-			/* On g33, we cannot place HWS above 256MiB, so
-			 * restrict its pinning to the low mappable arena.
-			 * Though this restriction is not documented for
-			 * gen4, gen5, or byt, they also behave similarly
-			 * and hang if the HWS is placed at the top of the
-			 * GTT. To generalise, it appears that all !llc
-			 * platforms have issues with us placing the HWS
-			 * above the mappable region (even though we never
-			 * actualy map it).
-			 */
-			flags |= PIN_MAPPABLE;
-		ret = i915_gem_obj_ggtt_pin(obj, 4096, flags);
-		if (ret) {
+	flags = 0;
+	if (!HAS_LLC(engine->i915))
+		/* On g33, we cannot place HWS above 256MiB, so
+		 * restrict its pinning to the low mappable arena.
+		 * Though this restriction is not documented for
+		 * gen4, gen5, or byt, they also behave similarly
+		 * and hang if the HWS is placed at the top of the
+		 * GTT. To generalise, it appears that all !llc
+		 * platforms have issues with us placing the HWS
+		 * above the mappable region (even though we never
+		 * actualy map it).
+		 */
+		flags |= PIN_MAPPABLE;
+	ret = i915_gem_object_ggtt_pin(obj, 4096, flags);
+	if (ret) {
 err_unref:
-			drm_gem_object_unreference(&obj->base);
-			return ret;
-		}
-
-		ring->status_page.obj = obj;
+		drm_gem_object_unreference(&obj->base);
+		return ret;
 	}
 
-	ring->status_page.gfx_addr = i915_gem_obj_ggtt_offset(obj);
-	ring->status_page.page_addr = kmap(sg_page(obj->pages->sgl));
-	memset(ring->status_page.page_addr, 0, PAGE_SIZE);
+	engine->status_page.obj = obj;
 
-	DRM_DEBUG_DRIVER("%s hws offset: 0x%08x\n",
-			ring->name, ring->status_page.gfx_addr);
+	engine->status_page.gfx_addr = i915_gem_obj_ggtt_offset(obj);
+	engine->status_page.page_addr = kmap(sg_page(obj->pages->sgl));
+	memset(engine->status_page.page_addr, 0, PAGE_SIZE);
 
+	DRM_DEBUG_DRIVER("%s hws offset: 0x%08x\n",
+			engine->name, engine->status_page.gfx_addr);
 	return 0;
 }
 
-static int init_phys_status_page(struct intel_engine_cs *ring)
+static int setup_phys_status_page(struct intel_engine_cs *engine)
 {
-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
+	struct drm_i915_private *i915 = engine->i915;
 
-	if (!dev_priv->status_page_dmah) {
-		dev_priv->status_page_dmah =
-			drm_pci_alloc(ring->dev, PAGE_SIZE, PAGE_SIZE);
-		if (!dev_priv->status_page_dmah)
-			return -ENOMEM;
-	}
+	i915->status_page_dmah =
+		drm_pci_alloc(i915->dev, PAGE_SIZE, PAGE_SIZE);
+	if (!i915->status_page_dmah)
+		return -ENOMEM;
 
-	ring->status_page.page_addr = dev_priv->status_page_dmah->vaddr;
-	memset(ring->status_page.page_addr, 0, PAGE_SIZE);
+	engine->status_page.page_addr = i915->status_page_dmah->vaddr;
+	memset(engine->status_page.page_addr, 0, PAGE_SIZE);
 
 	return 0;
 }
 
-static void intel_destroy_ringbuffer_obj(struct intel_ringbuffer *ringbuf)
+void intel_ring_free(struct intel_ringbuffer *ring)
 {
-	if (!ringbuf->obj)
-		return;
+	if (ring->obj) {
+		if (ring->iomap) {
+			i915_gem_object_ggtt_unpin(ring->obj);
+			iounmap(ring->virtual_start);
+		} else
+			vunmap(ring->virtual_start);
+		i915_gem_object_unpin_pages(ring->obj);
+		drm_gem_object_unreference(&ring->obj->base);
+	}
 
-	iounmap(ringbuf->virtual_start);
-	i915_gem_object_ggtt_unpin(ringbuf->obj);
-	drm_gem_object_unreference(&ringbuf->obj->base);
-	ringbuf->obj = NULL;
+	list_del(&ring->engine_link);
+	kfree(ring);
 }
 
-static int intel_alloc_ringbuffer_obj(struct drm_device *dev,
-				      struct intel_ringbuffer *ringbuf)
+static void *obj_vmap(struct drm_i915_gem_object *obj)
 {
-	struct drm_i915_private *dev_priv = to_i915(dev);
-	struct drm_i915_gem_object *obj;
-	int ret;
+	struct page **pages;
+	struct sg_page_iter sg_iter;
+	void *vaddr;
+	int i;
 
-	if (ringbuf->obj)
-		return 0;
+	if (!HAS_LLC(obj->base.dev))
+		return NULL;
 
-	obj = NULL;
-	if (!HAS_LLC(dev))
-		obj = i915_gem_object_create_stolen(dev, ringbuf->size);
-	if (obj == NULL)
-		obj = i915_gem_alloc_object(dev, ringbuf->size);
-	if (obj == NULL)
-		return -ENOMEM;
-
-	/* mark ring buffers as read-only from GPU side by default */
-	obj->gt_ro = 1;
-
-	ret = i915_gem_obj_ggtt_pin(obj, PAGE_SIZE, PIN_MAPPABLE);
-	if (ret)
-		goto err_unref;
+	if (obj->base.filp == NULL)
+		return NULL;
 
-	ret = i915_gem_object_set_to_gtt_domain(obj, true);
-	if (ret)
-		goto err_unpin;
+	pages = drm_malloc_ab(obj->base.size >> PAGE_SHIFT, sizeof(*pages));
+	if (pages == NULL)
+		return NULL;
 
-	ringbuf->virtual_start =
-		ioremap_wc(dev_priv->gtt.mappable_base + i915_gem_obj_ggtt_offset(obj),
-				ringbuf->size);
-	if (ringbuf->virtual_start == NULL) {
-		ret = -EINVAL;
-		goto err_unpin;
-	}
+	i = 0;
+	for_each_sg_page(obj->pages->sgl, &sg_iter, obj->pages->nents, 0)
+		pages[i++] = sg_page_iter_page(&sg_iter);
 
-	ringbuf->obj = obj;
-	return 0;
+	vaddr = vmap(pages, i, 0, PAGE_KERNEL);
+	drm_free_large(pages);
 
-err_unpin:
-	i915_gem_object_ggtt_unpin(obj);
-err_unref:
-	drm_gem_object_unreference(&obj->base);
-	return ret;
+	return vaddr;
 }
 
-static int intel_init_ring_buffer(struct drm_device *dev,
-				  struct intel_engine_cs *ring)
+struct intel_ringbuffer *
+intel_engine_alloc_ring(struct intel_engine_cs *engine,
+			struct intel_context *ctx,
+			int size)
 {
-	struct intel_ringbuffer *ringbuf = ring->buffer;
+	struct drm_i915_private *i915 = engine->i915;
+	struct intel_ringbuffer *ring;
+	struct drm_i915_gem_object *obj;
 	int ret;
 
-	if (ringbuf == NULL) {
-		ringbuf = kzalloc(sizeof(*ringbuf), GFP_KERNEL);
-		if (!ringbuf)
-			return -ENOMEM;
-		ring->buffer = ringbuf;
-	}
+	DRM_DEBUG("creating ringbuffer for %s, size %d\n", engine->name, size);
 
-	ring->dev = dev;
-	INIT_LIST_HEAD(&ring->active_list);
-	INIT_LIST_HEAD(&ring->request_list);
-	ringbuf->size = 32 * PAGE_SIZE;
-	memset(ring->semaphore.sync_seqno, 0, sizeof(ring->semaphore.sync_seqno));
+	if (WARN_ON(!is_power_of_2(size)))
+		return ERR_PTR(-EINVAL);
 
-	init_waitqueue_head(&ring->irq_queue);
+	ring = kzalloc(sizeof(*ring), GFP_KERNEL);
+	if (ring == NULL)
+		return ERR_PTR(-ENOMEM);
 
-	if (I915_NEED_GFX_HWS(dev)) {
-		ret = init_status_page(ring);
-		if (ret)
-			goto error;
-	} else {
-		BUG_ON(ring->id != RCS);
-		ret = init_phys_status_page(ring);
-		if (ret)
-			goto error;
-	}
+	ring->engine = engine;
+	ring->ctx = ctx;
+
+	obj = NULL;
+	if (!HAS_LLC(i915))
+		obj = i915_gem_object_create_stolen(i915->dev, size);
+	if (obj == NULL)
+		obj = i915_gem_alloc_object(i915->dev, size);
+	if (obj == NULL)
+		return ERR_PTR(-ENOMEM);
+
+	/* mark ring buffers as read-only from GPU side by default */
+	obj->gt_ro = 1;
 
-	ret = intel_alloc_ringbuffer_obj(dev, ringbuf);
+	ret = i915_gem_object_get_pages(obj);
 	if (ret) {
-		DRM_ERROR("Failed to allocate ringbuffer %s: %d\n", ring->name, ret);
-		goto error;
+		DRM_ERROR("failed to get ringbuffer pages\n");
+		goto err_unref;
+	}
+
+	i915_gem_object_pin_pages(obj);
+
+	ring->virtual_start = obj_vmap(obj);
+	if (ring->virtual_start == NULL) {
+		ret = i915_gem_object_ggtt_pin(obj, PAGE_SIZE, PIN_MAPPABLE);
+		if (ret) {
+			DRM_ERROR("failed pin ringbuffer into GGTT\n");
+			goto err_unref;
+		}
+
+		ret = i915_gem_object_set_to_gtt_domain(obj, true);
+		if (ret) {
+			DRM_ERROR("failed mark ringbuffer for GTT writes\n");
+			goto err_unpin;
+		}
+
+		ring->virtual_start =
+			ioremap_wc(i915->gtt.mappable_base + i915_gem_obj_ggtt_offset(obj),
+					size);
+		if (ring->virtual_start == NULL) {
+			DRM_ERROR("failed to map ringbuffer through GTT\n");
+			ret = -EINVAL;
+			goto err_unpin;
+		}
+		ring->iomap = true;
 	}
 
+	ring->obj = obj;
+	ring->size = size;
+
 	/* Workaround an erratum on the i830 which causes a hang if
 	 * the TAIL pointer points to within the last 2 cachelines
 	 * of the buffer.
 	 */
-	ringbuf->effective_size = ringbuf->size;
-	if (IS_I830(dev) || IS_845G(dev))
-		ringbuf->effective_size -= 2 * CACHELINE_BYTES;
+	ring->effective_size = size;
+	if (IS_I830(i915) || IS_845G(i915))
+		ring->effective_size -= 2 * CACHELINE_BYTES;
+
+	ring->space = intel_ring_space(ring);
+	ring->retired_head = -1;
+
+	INIT_LIST_HEAD(&ring->requests);
+	INIT_LIST_HEAD(&ring->breadcrumbs);
+	list_add_tail(&ring->engine_link, &engine->rings);
 
-	ret = i915_cmd_parser_init_ring(ring);
-	if (ret)
-		goto error;
-
-	ret = ring->init(ring);
-	if (ret)
-		goto error;
+	return ring;
 
-	return 0;
-
-error:
-	kfree(ringbuf);
-	ring->buffer = NULL;
-	return ret;
+err_unpin:
+	i915_gem_object_ggtt_unpin(obj);
+err_unref:
+	i915_gem_object_unpin_pages(obj);
+	drm_gem_object_unreference(&obj->base);
+	return ERR_PTR(ret);
 }
 
-void intel_cleanup_ring_buffer(struct intel_engine_cs *ring)
+static void
+nop_irq_barrier(struct intel_engine_cs *engine)
 {
-	struct drm_i915_private *dev_priv = to_i915(ring->dev);
-	struct intel_ringbuffer *ringbuf = ring->buffer;
-
-	if (!intel_ring_initialized(ring))
-		return;
-
-	intel_stop_ring_buffer(ring);
-	WARN_ON(!IS_GEN2(ring->dev) && (I915_READ_MODE(ring) & MODE_IDLE) == 0);
-
-	intel_destroy_ringbuffer_obj(ringbuf);
-	ring->preallocated_lazy_request = NULL;
-	ring->outstanding_lazy_seqno = 0;
-
-	if (ring->cleanup)
-		ring->cleanup(ring);
-
-	cleanup_status_page(ring);
+}
 
-	i915_cmd_parser_fini_ring(ring);
+static size_t get_context_alignment(struct drm_i915_private *i915)
+{
+	/* This is a HW constraint. The value below is the largest known
+	 * requirement I've seen in a spec to date, and that was a
+	 * workaround for a non-shipping part. It should be safe to
+	 * decrease this, but it's more future proof as is.
+	 */
+	if (INTEL_INFO(i915)->gen == 6)
+		return 64 << 10;
 
-	kfree(ringbuf);
-	ring->buffer = NULL;
+	return 0;
 }
 
-static int intel_ring_wait_request(struct intel_engine_cs *ring, int n)
+static struct intel_ringbuffer *
+engine_get_ring(struct intel_engine_cs *engine)
 {
-	struct intel_ringbuffer *ringbuf = ring->buffer;
-	struct drm_i915_gem_request *request;
-	u32 seqno = 0;
+	struct drm_i915_private *dev_priv = engine->i915;
+	struct intel_ringbuffer *ring;
 	int ret;
 
-	if (ringbuf->last_retired_head != -1) {
-		ringbuf->head = ringbuf->last_retired_head;
-		ringbuf->last_retired_head = -1;
+	ring = engine->default_context->ring[engine->id].ring;
+	if (ring)
+		return ring;
 
-		ringbuf->space = ring_space(ringbuf);
-		if (ringbuf->space >= n)
-			return 0;
+	if (I915_NEED_GFX_HWS(dev_priv)) {
+		ret = setup_status_page(engine);
+	} else {
+		BUG_ON(engine->id != RCS);
+		ret = setup_phys_status_page(engine);
 	}
+	if (ret)
+		return ERR_PTR(ret);
 
-	list_for_each_entry(request, &ring->request_list, list) {
-		if (__ring_space(request->tail, ringbuf->tail, ringbuf->size) >= n) {
-			seqno = request->seqno;
-			break;
-		}
+	ring = intel_engine_alloc_ring(engine, NULL, 32 * PAGE_SIZE);
+	if (IS_ERR(ring)) {
+		DRM_ERROR("Failed to allocate ringbuffer for %s: %ld\n", engine->name, PTR_ERR(ring));
+		return ring;
 	}
 
-	if (seqno == 0)
-		return -ENOSPC;
-
-	ret = i915_wait_seqno(ring, seqno);
-	if (ret)
-		return ret;
-
-	i915_gem_retire_requests_ring(ring);
-	ringbuf->head = ringbuf->last_retired_head;
-	ringbuf->last_retired_head = -1;
+	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
+	if (!engine_stop(engine)) {
+		/* G45 ring initialization often fails to reset head to zero */
+		DRM_DEBUG_KMS("%s head not reset to zero "
+			      "ctl %08x head %08x tail %08x start %08x\n",
+			      engine->name,
+			      I915_READ_CTL(engine),
+			      I915_READ_HEAD(engine),
+			      I915_READ_TAIL(engine),
+			      I915_READ_START(engine));
+		if (!engine_stop(engine)) {
+			DRM_ERROR("failed to set %s head to zero "
+				  "ctl %08x head %08x tail %08x start %08x\n",
+				  engine->name,
+				  I915_READ_CTL(engine),
+				  I915_READ_HEAD(engine),
+				  I915_READ_TAIL(engine),
+				  I915_READ_START(engine));
+			ret = -EIO;
+		}
+	}
+	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
+	if (ret) {
+		intel_ring_free(ring);
+		return ERR_PTR(ret);
+	}
 
-	ringbuf->space = ring_space(ringbuf);
-	return 0;
+	return engine->default_context->ring[engine->id].ring = ring;
 }
 
-static int ring_wait_for_space(struct intel_engine_cs *ring, int n)
+static struct intel_ringbuffer *
+engine_pin_context(struct intel_engine_cs *engine,
+		   struct intel_context *ctx)
 {
-	struct drm_device *dev = ring->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_ringbuffer *ringbuf = ring->buffer;
-	unsigned long end;
+	struct intel_ringbuffer *ring;
 	int ret;
 
-	ret = intel_ring_wait_request(ring, n);
-	if (ret != -ENOSPC)
-		return ret;
-
-	/* force the tail write in case we have been skipping them */
-	__intel_ring_advance(ring);
-
-	/* With GEM the hangcheck timer should kick us out of the loop,
-	 * leaving it early runs the risk of corrupting GEM state (due
-	 * to running on almost untested codepaths). But on resume
-	 * timers don't work yet, so prevent a complete hang in that
-	 * case by choosing an insanely large timeout. */
-	end = jiffies + 60 * HZ;
-
-	trace_i915_ring_wait_begin(ring);
-	do {
-		ringbuf->head = I915_READ_HEAD(ring);
-		ringbuf->space = ring_space(ringbuf);
-		if (ringbuf->space >= n) {
-			ret = 0;
-			break;
-		}
-
-		if (!drm_core_check_feature(dev, DRIVER_MODESET) &&
-		    dev->primary->master) {
-			struct drm_i915_master_private *master_priv = dev->primary->master->driver_priv;
-			if (master_priv->sarea_priv)
-				master_priv->sarea_priv->perf_boxes |= I915_BOX_WAIT;
-		}
-
-		msleep(1);
-
-		if (dev_priv->mm.interruptible && signal_pending(current)) {
-			ret = -ERESTARTSYS;
-			break;
-		}
+	if (ctx->ring[engine->id].state) {
+		struct drm_i915_gem_object *obj = ctx->ring[engine->id].state;
 
-		ret = i915_gem_check_wedge(&dev_priv->gpu_error,
-					   dev_priv->mm.interruptible);
+		ret = i915_gem_object_ggtt_pin(obj,
+					       get_context_alignment(engine->i915),
+					       0);
 		if (ret)
-			break;
+			goto err;
 
-		if (time_after(jiffies, end)) {
-			ret = -EBUSY;
-			break;
-		}
-	} while (1);
-	trace_i915_ring_wait_end(ring);
-	return ret;
-}
-
-static int intel_wrap_ring_buffer(struct intel_engine_cs *ring)
-{
-	uint32_t __iomem *virt;
-	struct intel_ringbuffer *ringbuf = ring->buffer;
-	int rem = ringbuf->size - ringbuf->tail;
+		/*
+		 * Clear this page out of any CPU caches for coherent swap-in/out. Note
+		 * that thanks to write = false in this call and us not setting any gpu
+		 * write domains when putting a context object onto the active list
+		 * (when switching away from it), this won't block.
+		 *
+		 * XXX: We need a real interface to do this instead of trickery.
+		 */
+		ret = i915_gem_object_set_to_gtt_domain(obj, false);
+		if (ret)
+			goto err_ctx;
+	}
 
-	if (ringbuf->space < rem) {
-		int ret = ring_wait_for_space(ring, rem);
+	if (ctx->ppgtt) {
+		ret = i915_gem_object_ggtt_pin(ctx->ppgtt->state,
+					       ctx->ppgtt->alignment,
+					       0);
 		if (ret)
-			return ret;
+			goto err_ctx;
 	}
 
-	virt = ringbuf->virtual_start + ringbuf->tail;
-	rem /= 4;
-	while (rem--)
-		iowrite32(MI_NOOP, virt++);
+	ring = engine_get_ring(engine);
+	if (IS_ERR(ring)) {
+		ret = PTR_ERR(ring);
+		goto err_mm;
+	}
 
-	ringbuf->tail = 0;
-	ringbuf->space = ring_space(ringbuf);
+	return ring;
 
-	return 0;
+err_mm:
+	if (ctx->ppgtt)
+		i915_gem_object_ggtt_unpin(ctx->ppgtt->state);
+err_ctx:
+	if (ctx->ring[engine->id].state)
+		i915_gem_object_ggtt_unpin(ctx->ring[engine->id].state);
+err:
+	return ERR_PTR(ret);
 }
 
-int intel_ring_idle(struct intel_engine_cs *ring)
+static struct drm_i915_gem_object *
+rq_add_ggtt(struct i915_gem_request *rq,
+	    struct drm_i915_gem_object *obj)
 {
-	u32 seqno;
-	int ret;
-
-	/* We need to add any requests required to flush the objects and ring */
-	if (ring->outstanding_lazy_seqno) {
-		ret = i915_add_request(ring, NULL);
-		if (ret)
-			return ret;
-	}
-
-	/* Wait upon the last request to be completed */
-	if (list_empty(&ring->request_list))
-		return 0;
-
-	seqno = list_entry(ring->request_list.prev,
-			   struct drm_i915_gem_request,
-			   list)->seqno;
+	obj->base.pending_read_domains = I915_GEM_DOMAIN_INSTRUCTION;
+	/* obj is kept alive until the next request by its active ref */
+	drm_gem_object_reference(&obj->base);
+	i915_request_add_vma(rq, i915_gem_obj_get_ggtt(obj), 0);
 
-	return i915_wait_seqno(ring, seqno);
+	return obj;
 }
 
-static int
-intel_ring_alloc_seqno(struct intel_engine_cs *ring)
+static void engine_add_context(struct i915_gem_request *rq,
+			       struct intel_context *ctx)
 {
-	if (ring->outstanding_lazy_seqno)
-		return 0;
-
-	if (ring->preallocated_lazy_request == NULL) {
-		struct drm_i915_gem_request *request;
+	if (ctx->ring[rq->engine->id].state)
+		/* As long as MI_SET_CONTEXT is serializing, ie. it flushes the
+		 * whole damn pipeline, we don't need to explicitly mark the
+		 * object dirty. The only exception is that the context must be
+		 * correct in case the object gets swapped out. Ideally we'd be
+		 * able to defer doing this until we know the object would be
+		 * swapped, but there is no way to do that yet.
+		 */
+		rq_add_ggtt(rq, ctx->ring[rq->engine->id].state)->dirty = 1;
 
-		request = kmalloc(sizeof(*request), GFP_KERNEL);
-		if (request == NULL)
-			return -ENOMEM;
+	if (ctx->ppgtt)
+		rq_add_ggtt(rq, ctx->ppgtt->state);
+}
 
-		ring->preallocated_lazy_request = request;
-	}
+static void
+engine_unpin_context(struct intel_engine_cs *engine,
+		     struct intel_context *ctx)
+{
+	if (ctx->ppgtt)
+		i915_gem_object_ggtt_unpin(ctx->ppgtt->state);
+	if (ctx->ring[engine->id].state)
+		i915_gem_object_ggtt_unpin(ctx->ring[engine->id].state);
+}
 
-	return i915_gem_get_seqno(ring->dev, &ring->outstanding_lazy_seqno);
+static void engine_free_context(struct intel_engine_cs *engine,
+				struct intel_context *ctx)
+{
+	if (ctx->ring[engine->id].state)
+		drm_gem_object_unreference(&ctx->ring[engine->id].state->base);
+	if (ctx == engine->default_context)
+		intel_ring_free(ctx->ring[engine->id].ring);
 }
 
-static int __intel_ring_prepare(struct intel_engine_cs *ring,
-				int bytes)
+static int intel_engine_init(struct intel_engine_cs *engine,
+			     struct drm_i915_private *i915)
 {
-	struct intel_ringbuffer *ringbuf = ring->buffer;
 	int ret;
 
-	if (unlikely(ringbuf->tail + bytes > ringbuf->effective_size)) {
-		ret = intel_wrap_ring_buffer(ring);
-		if (unlikely(ret))
-			return ret;
-	}
+	engine->i915 = i915;
 
-	if (unlikely(ringbuf->space < bytes)) {
-		ret = ring_wait_for_space(ring, bytes);
-		if (unlikely(ret))
-			return ret;
-	}
+	INIT_LIST_HEAD(&engine->rings);
+	INIT_LIST_HEAD(&engine->vma_list);
+	INIT_LIST_HEAD(&engine->read_list);
+	INIT_LIST_HEAD(&engine->write_list);
+	INIT_LIST_HEAD(&engine->requests);
+	INIT_LIST_HEAD(&engine->pending);
+	INIT_LIST_HEAD(&engine->submitted);
 
-	return 0;
-}
+	spin_lock_init(&engine->lock);
+	spin_lock_init(&engine->irqlock);
 
-int intel_ring_begin(struct intel_engine_cs *ring,
-		     int num_dwords)
-{
-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
-	int ret;
+	engine->suspend = engine_suspend;
+	engine->resume = engine_resume;
+	engine->cleanup = engine_cleanup;
+	engine->is_idle = engine_is_idle;
 
-	ret = i915_gem_check_wedge(&dev_priv->gpu_error,
-				   dev_priv->mm.interruptible);
-	if (ret)
-		return ret;
+	engine->irq_barrier = nop_irq_barrier;
+	engine->emit_breadcrumb = emit_breadcrumb;
 
-	ret = __intel_ring_prepare(ring, num_dwords * sizeof(uint32_t));
-	if (ret)
-		return ret;
+	engine->power_domains = FORCEWAKE_ALL;
 
-	/* Preallocate the olr before touching the ring */
-	ret = intel_ring_alloc_seqno(ring);
-	if (ret)
-		return ret;
+	engine->semaphore.wait = NULL;
 
-	ring->buffer->space -= num_dwords * sizeof(uint32_t);
-	return 0;
-}
+	engine->pin_context = engine_pin_context;
+	engine->add_context = engine_add_context;
+	engine->unpin_context = engine_unpin_context;
+	engine->free_context = engine_free_context;
 
-/* Align the ring tail to a cacheline boundary */
-int intel_ring_cacheline_align(struct intel_engine_cs *ring)
-{
-	int num_dwords = (ring->buffer->tail & (CACHELINE_BYTES - 1)) / sizeof(uint32_t);
-	int ret;
+	engine->add_request = engine_add_request;
+	engine->write_tail = ring_write_tail;
+	engine->is_complete = engine_rq_is_complete;
 
-	if (num_dwords == 0)
-		return 0;
+	init_waitqueue_head(&engine->irq_queue);
 
-	num_dwords = CACHELINE_BYTES / sizeof(uint32_t) - num_dwords;
-	ret = intel_ring_begin(ring, num_dwords);
+	ret = i915_cmd_parser_init_engine(engine);
 	if (ret)
 		return ret;
 
-	while (num_dwords--)
-		intel_ring_emit(ring, MI_NOOP);
-
-	intel_ring_advance(ring);
-
 	return 0;
 }
 
-void intel_ring_init_seqno(struct intel_engine_cs *ring, u32 seqno)
-{
-	struct drm_device *dev = ring->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-
-	BUG_ON(ring->outstanding_lazy_seqno);
-
-	if (INTEL_INFO(dev)->gen == 6 || INTEL_INFO(dev)->gen == 7) {
-		I915_WRITE(RING_SYNC_0(ring->mmio_base), 0);
-		I915_WRITE(RING_SYNC_1(ring->mmio_base), 0);
-		if (HAS_VEBOX(dev))
-			I915_WRITE(RING_SYNC_2(ring->mmio_base), 0);
-	}
-
-	ring->set_seqno(ring, seqno);
-	ring->hangcheck.seqno = seqno;
-}
-
-static void gen6_bsd_ring_write_tail(struct intel_engine_cs *ring,
+static void gen6_bsd_ring_write_tail(struct intel_engine_cs *engine,
 				     u32 value)
 {
-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
+	struct drm_i915_private *dev_priv = engine->i915;
 
        /* Every tail move must follow the sequence below */
 
@@ -1942,8 +1796,8 @@
 		DRM_ERROR("timed out waiting for the BSD ring to wake up\n");
 
 	/* Now that the ring is fully powered up, update the tail */
-	I915_WRITE_TAIL(ring, value);
-	POSTING_READ(RING_TAIL(ring->mmio_base));
+	I915_WRITE_TAIL(engine, value);
+	POSTING_READ(RING_TAIL(engine->mmio_base));
 
 	/* Let the ring send IDLE messages to the GT again,
 	 * and so let it sleep to conserve power when idle.
@@ -1952,79 +1806,77 @@
 		   _MASKED_BIT_DISABLE(GEN6_BSD_SLEEP_MSG_DISABLE));
 }
 
-static int gen6_bsd_ring_flush(struct intel_engine_cs *ring,
-			       u32 invalidate, u32 flush)
+static int gen6_bsd_emit_flush(struct i915_gem_request *rq,
+			       u32 flags)
 {
+	struct intel_ringbuffer *ring;
 	uint32_t cmd;
-	int ret;
-
-	ret = intel_ring_begin(ring, 4);
-	if (ret)
-		return ret;
 
-	cmd = MI_FLUSH_DW;
-	if (INTEL_INFO(ring->dev)->gen >= 8)
+	cmd = 3;
+	if (INTEL_INFO(rq->i915)->gen >= 8)
 		cmd += 1;
+
+	ring = intel_ring_begin(rq, cmd);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
+
 	/*
 	 * Bspec vol 1c.5 - video engine command streamer:
 	 * "If ENABLED, all TLBs will be invalidated once the flush
 	 * operation is complete. This bit is only valid when the
 	 * Post-Sync Operation field is a value of 1h or 3h."
 	 */
-	if (invalidate & I915_GEM_GPU_DOMAINS)
-		cmd |= MI_INVALIDATE_TLB | MI_INVALIDATE_BSD |
-			MI_FLUSH_DW_STORE_INDEX | MI_FLUSH_DW_OP_STOREDW;
+	cmd = MI_FLUSH_DW | (cmd - 2);
+	if (flags & I915_INVALIDATE_CACHES)
+		cmd |= (MI_INVALIDATE_TLB |
+			MI_INVALIDATE_BSD |
+			MI_FLUSH_DW_STORE_INDEX |
+			MI_FLUSH_DW_OP_STOREDW);
 	intel_ring_emit(ring, cmd);
 	intel_ring_emit(ring, I915_GEM_HWS_SCRATCH_ADDR | MI_FLUSH_DW_USE_GTT);
-	if (INTEL_INFO(ring->dev)->gen >= 8) {
+	if (INTEL_INFO(rq->i915)->gen >= 8)
 		intel_ring_emit(ring, 0); /* upper addr */
-		intel_ring_emit(ring, 0); /* value */
-	} else  {
-		intel_ring_emit(ring, 0);
-		intel_ring_emit(ring, MI_NOOP);
-	}
+	intel_ring_emit(ring, 0); /* value */
 	intel_ring_advance(ring);
 	return 0;
 }
 
 static int
-gen8_ring_dispatch_execbuffer(struct intel_engine_cs *ring,
-			      u64 offset, u32 len,
-			      unsigned flags)
-{
-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
-	bool ppgtt = dev_priv->mm.aliasing_ppgtt != NULL &&
-		!(flags & I915_DISPATCH_SECURE);
-	int ret;
-
-	ret = intel_ring_begin(ring, 4);
-	if (ret)
-		return ret;
+gen8_emit_batchbuffer(struct i915_gem_request *rq,
+		      u64 offset, u32 len,
+		      unsigned flags)
+{
+	bool ppgtt = USES_PPGTT(ring->dev) && !(flags & I915_DISPATCH_SECURE);
+	struct intel_ringbuffer *ring;
+
+	ring = intel_ring_begin(rq, 3);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
 	/* FIXME(BDW): Address space and security selectors. */
 	intel_ring_emit(ring, MI_BATCH_BUFFER_START_GEN8 | (ppgtt<<8));
 	intel_ring_emit(ring, lower_32_bits(offset));
 	intel_ring_emit(ring, upper_32_bits(offset));
-	intel_ring_emit(ring, MI_NOOP);
 	intel_ring_advance(ring);
 
 	return 0;
 }
 
 static int
-hsw_ring_dispatch_execbuffer(struct intel_engine_cs *ring,
-			      u64 offset, u32 len,
-			      unsigned flags)
-{
-	int ret;
-
-	ret = intel_ring_begin(ring, 2);
-	if (ret)
-		return ret;
+hsw_emit_batchbuffer(struct i915_gem_request *rq,
+		     u64 offset, u32 len,
+		     unsigned flags)
+{
+	struct intel_ringbuffer *ring;
+
+	ring = intel_ring_begin(rq, 2);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
 	intel_ring_emit(ring,
-			MI_BATCH_BUFFER_START | MI_BATCH_PPGTT_HSW |
-			(flags & I915_DISPATCH_SECURE ? 0 : MI_BATCH_NON_SECURE_HSW));
+			MI_BATCH_BUFFER_START |
+			(flags & I915_DISPATCH_SECURE ?
+			 0 : MI_BATCH_PPGTT_HSW | MI_BATCH_NON_SECURE_HSW));
 	/* bit0-7 is the length on GEN6+ */
 	intel_ring_emit(ring, offset);
 	intel_ring_advance(ring);
@@ -2033,15 +1885,15 @@
 }
 
 static int
-gen6_ring_dispatch_execbuffer(struct intel_engine_cs *ring,
-			      u64 offset, u32 len,
-			      unsigned flags)
-{
-	int ret;
-
-	ret = intel_ring_begin(ring, 2);
-	if (ret)
-		return ret;
+gen6_emit_batchbuffer(struct i915_gem_request *rq,
+		      u64 offset, u32 len,
+		      unsigned flags)
+{
+	struct intel_ringbuffer *ring;
+
+	ring = intel_ring_begin(rq, 2);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
 
 	intel_ring_emit(ring,
 			MI_BATCH_BUFFER_START |
@@ -2055,100 +1907,141 @@
 
 /* Blitter support (SandyBridge+) */
 
-static int gen6_ring_flush(struct intel_engine_cs *ring,
-			   u32 invalidate, u32 flush)
+static int gen6_blt_emit_flush(struct i915_gem_request *rq,
+			       u32 flags)
 {
-	struct drm_device *dev = ring->dev;
+	struct intel_ringbuffer *ring;
 	uint32_t cmd;
-	int ret;
-
-	ret = intel_ring_begin(ring, 4);
-	if (ret)
-		return ret;
 
-	cmd = MI_FLUSH_DW;
-	if (INTEL_INFO(ring->dev)->gen >= 8)
+	cmd = 3;
+	if (INTEL_INFO(rq->i915)->gen >= 8)
 		cmd += 1;
+
+	ring = intel_ring_begin(rq, cmd);
+	if (IS_ERR(ring))
+		return PTR_ERR(ring);
+
 	/*
 	 * Bspec vol 1c.3 - blitter engine command streamer:
 	 * "If ENABLED, all TLBs will be invalidated once the flush
 	 * operation is complete. This bit is only valid when the
 	 * Post-Sync Operation field is a value of 1h or 3h."
 	 */
-	if (invalidate & I915_GEM_DOMAIN_RENDER)
-		cmd |= MI_INVALIDATE_TLB | MI_FLUSH_DW_STORE_INDEX |
-			MI_FLUSH_DW_OP_STOREDW;
+	cmd = MI_FLUSH_DW | (cmd - 2);
+	if (flags & I915_INVALIDATE_CACHES)
+		cmd |= (MI_INVALIDATE_TLB |
+			MI_FLUSH_DW_STORE_INDEX |
+			MI_FLUSH_DW_OP_STOREDW);
 	intel_ring_emit(ring, cmd);
 	intel_ring_emit(ring, I915_GEM_HWS_SCRATCH_ADDR | MI_FLUSH_DW_USE_GTT);
-	if (INTEL_INFO(ring->dev)->gen >= 8) {
+	if (INTEL_INFO(rq->i915)->gen >= 8)
 		intel_ring_emit(ring, 0); /* upper addr */
-		intel_ring_emit(ring, 0); /* value */
-	} else  {
-		intel_ring_emit(ring, 0);
-		intel_ring_emit(ring, MI_NOOP);
-	}
+	intel_ring_emit(ring, 0); /* value */
 	intel_ring_advance(ring);
 
-	if (IS_GEN7(dev) && !invalidate && flush)
-		return gen7_ring_fbc_flush(ring, FBC_REND_CACHE_CLEAN);
+	if (flags & I915_KICK_FBC) {
+		if (IS_GEN7(rq->i915))
+			return gen7_ring_fbc_flush(rq, FBC_REND_CACHE_CLEAN);
+		if (IS_BROADWELL(rq->i915))
+			rq->i915->fbc.need_sw_cache_clean = true; /* XXX */
+	}
 
 	return 0;
 }
 
-int intel_init_render_ring_buffer(struct drm_device *dev)
+static void gen8_engine_init_semaphore(struct intel_engine_cs *engine)
+{
+	if (engine->i915->semaphore_obj == NULL)
+		return;
+
+	engine->semaphore.wait = gen8_emit_wait;
+	engine->semaphore.signal =
+		engine->id == RCS ? gen8_rcs_emit_signal : gen8_xcs_emit_signal;
+}
+
+static bool semaphores_enabled(struct drm_i915_private *dev_priv)
+{
+	if (INTEL_INFO(dev_priv)->gen < 6)
+		return false;
+
+	if (i915_module.semaphores >= 0)
+		return i915_module.semaphores;
+
+	/* Until we get further testing... */
+	if (IS_GEN8(dev_priv))
+		return false;
+
+#ifdef CONFIG_INTEL_IOMMU
+	/* Enable semaphores on SNB when IO remapping is off */
+	if (INTEL_INFO(dev_priv)->gen == 6 && intel_iommu_gfx_mapped)
+		return false;
+#endif
+
+	return true;
+}
+
+int intel_init_render_engine(struct drm_i915_private *dev_priv)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring = &dev_priv->ring[RCS];
+	struct intel_engine_cs *engine = &dev_priv->engine[RCS];
 	struct drm_i915_gem_object *obj;
 	int ret;
 
-	ring->name = "render ring";
-	ring->id = RCS;
-	ring->mmio_base = RENDER_RING_BASE;
-
-	if (INTEL_INFO(dev)->gen >= 8) {
-		if (i915_semaphore_is_enabled(dev)) {
-			obj = i915_gem_alloc_object(dev, 4096);
+	ret = intel_engine_init(engine, dev_priv);
+	if (ret)
+		return ret;
+
+	engine->name = "render ring";
+	engine->id = RCS;
+	engine->mmio_base = RENDER_RING_BASE;
+	if (IS_VALLEYVIEW(dev_priv))
+		engine->power_domains = FORCEWAKE_RENDER;
+
+	engine->init_context = i915_gem_render_state_init;
+
+	if (HAS_L3_DPF(dev_priv)) {
+		if (INTEL_INFO(dev_priv)->gen >= 8)
+			engine->irq_keep_mask |= GT_RENDER_L3_PARITY_ERROR_INTERRUPT;
+		else
+			engine->irq_keep_mask |= GT_PARITY_ERROR(dev_priv);
+	}
+
+	if (INTEL_INFO(dev_priv)->gen >= 8) {
+		if (semaphores_enabled(dev_priv)) {
+			obj = i915_gem_alloc_object(dev_priv->dev, 4096);
 			if (obj == NULL) {
 				DRM_ERROR("Failed to allocate semaphore bo. Disabling semaphores\n");
-				i915.semaphores = 0;
+				i915_module.semaphores = 0;
 			} else {
 				i915_gem_object_set_cache_level(obj, I915_CACHE_LLC);
-				ret = i915_gem_obj_ggtt_pin(obj, 0, PIN_NONBLOCK);
+				ret = i915_gem_object_ggtt_pin(obj, 0, PIN_NONBLOCK);
 				if (ret != 0) {
 					drm_gem_object_unreference(&obj->base);
 					DRM_ERROR("Failed to pin semaphore bo. Disabling semaphores\n");
-					i915.semaphores = 0;
+					i915_module.semaphores = 0;
 				} else
 					dev_priv->semaphore_obj = obj;
 			}
 		}
-		ring->add_request = gen6_add_request;
-		ring->flush = gen8_render_ring_flush;
-		ring->irq_get = gen8_ring_get_irq;
-		ring->irq_put = gen8_ring_put_irq;
-		ring->irq_enable_mask = GT_RENDER_USER_INTERRUPT;
-		ring->get_seqno = gen6_ring_get_seqno;
-		ring->set_seqno = ring_set_seqno;
-		if (i915_semaphore_is_enabled(dev)) {
-			WARN_ON(!dev_priv->semaphore_obj);
-			ring->semaphore.sync_to = gen8_ring_sync;
-			ring->semaphore.signal = gen8_rcs_signal;
-			GEN8_RING_SEMAPHORE_INIT;
-		}
-	} else if (INTEL_INFO(dev)->gen >= 6) {
-		ring->add_request = gen6_add_request;
-		ring->flush = gen7_render_ring_flush;
-		if (INTEL_INFO(dev)->gen == 6)
-			ring->flush = gen6_render_ring_flush;
-		ring->irq_get = gen6_ring_get_irq;
-		ring->irq_put = gen6_ring_put_irq;
-		ring->irq_enable_mask = GT_RENDER_USER_INTERRUPT;
-		ring->get_seqno = gen6_ring_get_seqno;
-		ring->set_seqno = ring_set_seqno;
-		if (i915_semaphore_is_enabled(dev)) {
-			ring->semaphore.sync_to = gen6_ring_sync;
-			ring->semaphore.signal = gen6_signal;
+		if (IS_CHERRYVIEW(dev_priv))
+			engine->init_context = chv_render_init_context;
+		else
+			engine->init_context = bdw_render_init_context;
+		engine->emit_flush = gen8_render_emit_flush;
+		engine->irq_get = gen8_irq_get;
+		engine->irq_put = gen8_irq_put;
+		engine->irq_enable_mask = GT_RENDER_USER_INTERRUPT;
+		gen8_engine_init_semaphore(engine);
+	} else if (INTEL_INFO(dev_priv)->gen >= 6) {
+		engine->emit_flush = gen7_render_emit_flush;
+		if (INTEL_INFO(dev_priv)->gen == 6)
+			engine->emit_flush = gen6_render_emit_flush;
+		engine->irq_get = gen6_irq_get;
+		engine->irq_barrier = gen6_irq_barrier;
+		engine->irq_put = gen6_irq_put;
+		engine->irq_enable_mask = GT_RENDER_USER_INTERRUPT;
+		if (semaphores_enabled(dev_priv)) {
+			engine->semaphore.wait = gen6_emit_wait;
+			engine->semaphore.signal = gen6_emit_signal;
 			/*
 			 * The current semaphore is only applied on pre-gen8
 			 * platform.  And there is no VCS2 ring on the pre-gen8
@@ -2156,312 +2049,204 @@
 			 * initialized as INVALID.  Gen8 will initialize the
 			 * sema between VCS2 and RCS later.
 			 */
-			ring->semaphore.mbox.wait[RCS] = MI_SEMAPHORE_SYNC_INVALID;
-			ring->semaphore.mbox.wait[VCS] = MI_SEMAPHORE_SYNC_RV;
-			ring->semaphore.mbox.wait[BCS] = MI_SEMAPHORE_SYNC_RB;
-			ring->semaphore.mbox.wait[VECS] = MI_SEMAPHORE_SYNC_RVE;
-			ring->semaphore.mbox.wait[VCS2] = MI_SEMAPHORE_SYNC_INVALID;
-			ring->semaphore.mbox.signal[RCS] = GEN6_NOSYNC;
-			ring->semaphore.mbox.signal[VCS] = GEN6_VRSYNC;
-			ring->semaphore.mbox.signal[BCS] = GEN6_BRSYNC;
-			ring->semaphore.mbox.signal[VECS] = GEN6_VERSYNC;
-			ring->semaphore.mbox.signal[VCS2] = GEN6_NOSYNC;
-		}
-	} else if (IS_GEN5(dev)) {
-		ring->add_request = pc_render_add_request;
-		ring->flush = gen4_render_ring_flush;
-		ring->get_seqno = pc_render_get_seqno;
-		ring->set_seqno = pc_render_set_seqno;
-		ring->irq_get = gen5_ring_get_irq;
-		ring->irq_put = gen5_ring_put_irq;
-		ring->irq_enable_mask = GT_RENDER_USER_INTERRUPT |
-					GT_RENDER_PIPECTL_NOTIFY_INTERRUPT;
+			engine->semaphore.mbox.wait[RCS] = MI_SEMAPHORE_SYNC_INVALID;
+			engine->semaphore.mbox.wait[VCS] = MI_SEMAPHORE_SYNC_RV;
+			engine->semaphore.mbox.wait[BCS] = MI_SEMAPHORE_SYNC_RB;
+			engine->semaphore.mbox.wait[VECS] = MI_SEMAPHORE_SYNC_RVE;
+			engine->semaphore.mbox.wait[VCS2] = MI_SEMAPHORE_SYNC_INVALID;
+			engine->semaphore.mbox.signal[RCS] = GEN6_NOSYNC;
+			engine->semaphore.mbox.signal[VCS] = GEN6_VRSYNC;
+			engine->semaphore.mbox.signal[BCS] = GEN6_BRSYNC;
+			engine->semaphore.mbox.signal[VECS] = GEN6_VERSYNC;
+			engine->semaphore.mbox.signal[VCS2] = GEN6_NOSYNC;
+		}
+	} else if (IS_GEN5(dev_priv)) {
+		engine->emit_flush = gen4_emit_flush;
+		engine->irq_get = gen5_irq_get;
+		engine->irq_put = gen5_irq_put;
+		engine->irq_enable_mask =
+			GT_RENDER_USER_INTERRUPT |
+			GT_RENDER_PIPECTL_NOTIFY_INTERRUPT;
 	} else {
-		ring->add_request = i9xx_add_request;
-		if (INTEL_INFO(dev)->gen < 4)
-			ring->flush = gen2_render_ring_flush;
+		if (INTEL_INFO(dev_priv)->gen < 4)
+			engine->emit_flush = gen2_emit_flush;
 		else
-			ring->flush = gen4_render_ring_flush;
-		ring->get_seqno = ring_get_seqno;
-		ring->set_seqno = ring_set_seqno;
-		if (IS_GEN2(dev)) {
-			ring->irq_get = i8xx_ring_get_irq;
-			ring->irq_put = i8xx_ring_put_irq;
+			engine->emit_flush = gen4_emit_flush;
+		if (IS_GEN2(dev_priv)) {
+			engine->irq_get = i8xx_irq_get;
+			engine->irq_put = i8xx_irq_put;
+			engine->is_idle = i8xx_is_idle;
 		} else {
-			ring->irq_get = i9xx_ring_get_irq;
-			ring->irq_put = i9xx_ring_put_irq;
+			engine->irq_get = i9xx_irq_get;
+			engine->irq_put = i9xx_irq_put;
 		}
-		ring->irq_enable_mask = I915_USER_INTERRUPT;
+		engine->irq_enable_mask = I915_USER_INTERRUPT;
 	}
-	ring->write_tail = ring_write_tail;
 
-	if (IS_HASWELL(dev))
-		ring->dispatch_execbuffer = hsw_ring_dispatch_execbuffer;
-	else if (IS_GEN8(dev))
-		ring->dispatch_execbuffer = gen8_ring_dispatch_execbuffer;
-	else if (INTEL_INFO(dev)->gen >= 6)
-		ring->dispatch_execbuffer = gen6_ring_dispatch_execbuffer;
-	else if (INTEL_INFO(dev)->gen >= 4)
-		ring->dispatch_execbuffer = i965_dispatch_execbuffer;
-	else if (IS_I830(dev) || IS_845G(dev))
-		ring->dispatch_execbuffer = i830_dispatch_execbuffer;
+	if (IS_GEN8(dev_priv))
+		engine->emit_batchbuffer = gen8_emit_batchbuffer;
+	else if (IS_HASWELL(dev_priv))
+		engine->emit_batchbuffer = hsw_emit_batchbuffer;
+	else if (INTEL_INFO(dev_priv)->gen >= 6)
+		engine->emit_batchbuffer = gen6_emit_batchbuffer;
+	else if (INTEL_INFO(dev_priv)->gen >= 4)
+		engine->emit_batchbuffer = i965_emit_batchbuffer;
+	else if (IS_I830(dev_priv) || IS_845G(dev_priv))
+		engine->emit_batchbuffer = i830_emit_batchbuffer;
 	else
-		ring->dispatch_execbuffer = i915_dispatch_execbuffer;
-	ring->init = init_render_ring;
-	ring->cleanup = render_ring_cleanup;
-
-	/* Workaround batchbuffer to combat CS tlb bug. */
-	if (HAS_BROKEN_CS_TLB(dev)) {
-		obj = i915_gem_alloc_object(dev, I830_WA_SIZE);
-		if (obj == NULL) {
-			DRM_ERROR("Failed to allocate batch bo\n");
-			return -ENOMEM;
-		}
+		engine->emit_batchbuffer = i915_emit_batchbuffer;
 
-		ret = i915_gem_obj_ggtt_pin(obj, 0, 0);
-		if (ret != 0) {
-			drm_gem_object_unreference(&obj->base);
-			DRM_ERROR("Failed to ping batch bo\n");
-			return ret;
-		}
+	engine->resume = render_resume;
+	engine->cleanup = render_cleanup;
 
-		ring->scratch.obj = obj;
-		ring->scratch.gtt_offset = i915_gem_obj_ggtt_offset(obj);
-	}
+	if (HAS_BROKEN_CS_TLB(dev_priv))
+		/* Workaround batchbuffer to combat CS tlb bug. */
+		ret = init_broken_cs_tlb_wa(engine);
+	else if (INTEL_INFO(dev_priv)->gen >= 6)
+		ret = init_pipe_control(engine);
+	if (ret)
+		return ret;
 
-	return intel_init_ring_buffer(dev, ring);
+	return intel_engine_enable_execlists(engine);
 }
 
-int intel_render_ring_init_dri(struct drm_device *dev, u64 start, u32 size)
+int intel_init_bsd_engine(struct drm_i915_private *dev_priv)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring = &dev_priv->ring[RCS];
-	struct intel_ringbuffer *ringbuf = ring->buffer;
+	struct intel_engine_cs *engine = &dev_priv->engine[VCS];
 	int ret;
 
-	if (ringbuf == NULL) {
-		ringbuf = kzalloc(sizeof(*ringbuf), GFP_KERNEL);
-		if (!ringbuf)
-			return -ENOMEM;
-		ring->buffer = ringbuf;
-	}
-
-	ring->name = "render ring";
-	ring->id = RCS;
-	ring->mmio_base = RENDER_RING_BASE;
-
-	if (INTEL_INFO(dev)->gen >= 6) {
-		/* non-kms not supported on gen6+ */
-		ret = -ENODEV;
-		goto err_ringbuf;
-	}
-
-	/* Note: gem is not supported on gen5/ilk without kms (the corresponding
-	 * gem_init ioctl returns with -ENODEV). Hence we do not need to set up
-	 * the special gen5 functions. */
-	ring->add_request = i9xx_add_request;
-	if (INTEL_INFO(dev)->gen < 4)
-		ring->flush = gen2_render_ring_flush;
-	else
-		ring->flush = gen4_render_ring_flush;
-	ring->get_seqno = ring_get_seqno;
-	ring->set_seqno = ring_set_seqno;
-	if (IS_GEN2(dev)) {
-		ring->irq_get = i8xx_ring_get_irq;
-		ring->irq_put = i8xx_ring_put_irq;
-	} else {
-		ring->irq_get = i9xx_ring_get_irq;
-		ring->irq_put = i9xx_ring_put_irq;
-	}
-	ring->irq_enable_mask = I915_USER_INTERRUPT;
-	ring->write_tail = ring_write_tail;
-	if (INTEL_INFO(dev)->gen >= 4)
-		ring->dispatch_execbuffer = i965_dispatch_execbuffer;
-	else if (IS_I830(dev) || IS_845G(dev))
-		ring->dispatch_execbuffer = i830_dispatch_execbuffer;
-	else
-		ring->dispatch_execbuffer = i915_dispatch_execbuffer;
-	ring->init = init_render_ring;
-	ring->cleanup = render_ring_cleanup;
-
-	ring->dev = dev;
-	INIT_LIST_HEAD(&ring->active_list);
-	INIT_LIST_HEAD(&ring->request_list);
-
-	ringbuf->size = size;
-	ringbuf->effective_size = ringbuf->size;
-	if (IS_I830(ring->dev) || IS_845G(ring->dev))
-		ringbuf->effective_size -= 2 * CACHELINE_BYTES;
-
-	ringbuf->virtual_start = ioremap_wc(start, size);
-	if (ringbuf->virtual_start == NULL) {
-		DRM_ERROR("can not ioremap virtual address for"
-			  " ring buffer\n");
-		ret = -ENOMEM;
-		goto err_ringbuf;
-	}
-
-	if (!I915_NEED_GFX_HWS(dev)) {
-		ret = init_phys_status_page(ring);
-		if (ret)
-			goto err_vstart;
-	}
-
-	return 0;
-
-err_vstart:
-	iounmap(ringbuf->virtual_start);
-err_ringbuf:
-	kfree(ringbuf);
-	ring->buffer = NULL;
-	return ret;
-}
+	ret = intel_engine_init(engine, dev_priv);
+	if (ret)
+		return ret;
 
-int intel_init_bsd_ring_buffer(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring = &dev_priv->ring[VCS];
+	engine->name = "bsd ring";
+	engine->id = VCS;
+	if (IS_VALLEYVIEW(dev_priv))
+		engine->power_domains = FORCEWAKE_MEDIA;
 
-	ring->name = "bsd ring";
-	ring->id = VCS;
+	if (INTEL_INFO(dev_priv)->gen >= 6) {
+		engine->mmio_base = GEN6_BSD_RING_BASE;
 
-	ring->write_tail = ring_write_tail;
-	if (INTEL_INFO(dev)->gen >= 6) {
-		ring->mmio_base = GEN6_BSD_RING_BASE;
 		/* gen6 bsd needs a special wa for tail updates */
-		if (IS_GEN6(dev))
-			ring->write_tail = gen6_bsd_ring_write_tail;
-		ring->flush = gen6_bsd_ring_flush;
-		ring->add_request = gen6_add_request;
-		ring->get_seqno = gen6_ring_get_seqno;
-		ring->set_seqno = ring_set_seqno;
-		if (INTEL_INFO(dev)->gen >= 8) {
-			ring->irq_enable_mask =
+		if (IS_GEN6(dev_priv))
+			engine->write_tail = gen6_bsd_ring_write_tail;
+		engine->emit_flush = gen6_bsd_emit_flush;
+		if (INTEL_INFO(dev_priv)->gen >= 8) {
+			engine->irq_enable_mask =
 				GT_RENDER_USER_INTERRUPT << GEN8_VCS1_IRQ_SHIFT;
-			ring->irq_get = gen8_ring_get_irq;
-			ring->irq_put = gen8_ring_put_irq;
-			ring->dispatch_execbuffer =
-				gen8_ring_dispatch_execbuffer;
-			if (i915_semaphore_is_enabled(dev)) {
-				ring->semaphore.sync_to = gen8_ring_sync;
-				ring->semaphore.signal = gen8_xcs_signal;
-				GEN8_RING_SEMAPHORE_INIT;
-			}
+			engine->irq_get = gen8_irq_get;
+			engine->irq_put = gen8_irq_put;
+			engine->emit_batchbuffer = gen8_emit_batchbuffer;
+			gen8_engine_init_semaphore(engine);
 		} else {
-			ring->irq_enable_mask = GT_BSD_USER_INTERRUPT;
-			ring->irq_get = gen6_ring_get_irq;
-			ring->irq_put = gen6_ring_put_irq;
-			ring->dispatch_execbuffer =
-				gen6_ring_dispatch_execbuffer;
-			if (i915_semaphore_is_enabled(dev)) {
-				ring->semaphore.sync_to = gen6_ring_sync;
-				ring->semaphore.signal = gen6_signal;
-				ring->semaphore.mbox.wait[RCS] = MI_SEMAPHORE_SYNC_VR;
-				ring->semaphore.mbox.wait[VCS] = MI_SEMAPHORE_SYNC_INVALID;
-				ring->semaphore.mbox.wait[BCS] = MI_SEMAPHORE_SYNC_VB;
-				ring->semaphore.mbox.wait[VECS] = MI_SEMAPHORE_SYNC_VVE;
-				ring->semaphore.mbox.wait[VCS2] = MI_SEMAPHORE_SYNC_INVALID;
-				ring->semaphore.mbox.signal[RCS] = GEN6_RVSYNC;
-				ring->semaphore.mbox.signal[VCS] = GEN6_NOSYNC;
-				ring->semaphore.mbox.signal[BCS] = GEN6_BVSYNC;
-				ring->semaphore.mbox.signal[VECS] = GEN6_VEVSYNC;
-				ring->semaphore.mbox.signal[VCS2] = GEN6_NOSYNC;
+			engine->irq_enable_mask = GT_BSD_USER_INTERRUPT;
+			engine->irq_get = gen6_irq_get;
+			engine->irq_barrier = gen6_irq_barrier;
+			engine->irq_put = gen6_irq_put;
+			engine->emit_batchbuffer = gen6_emit_batchbuffer;
+			if (semaphores_enabled(dev_priv)) {
+				engine->semaphore.wait = gen6_emit_wait;
+				engine->semaphore.signal = gen6_emit_signal;
+				engine->semaphore.mbox.wait[RCS] = MI_SEMAPHORE_SYNC_VR;
+				engine->semaphore.mbox.wait[VCS] = MI_SEMAPHORE_SYNC_INVALID;
+				engine->semaphore.mbox.wait[BCS] = MI_SEMAPHORE_SYNC_VB;
+				engine->semaphore.mbox.wait[VECS] = MI_SEMAPHORE_SYNC_VVE;
+				engine->semaphore.mbox.wait[VCS2] = MI_SEMAPHORE_SYNC_INVALID;
+				engine->semaphore.mbox.signal[RCS] = GEN6_RVSYNC;
+				engine->semaphore.mbox.signal[VCS] = GEN6_NOSYNC;
+				engine->semaphore.mbox.signal[BCS] = GEN6_BVSYNC;
+				engine->semaphore.mbox.signal[VECS] = GEN6_VEVSYNC;
+				engine->semaphore.mbox.signal[VCS2] = GEN6_NOSYNC;
 			}
 		}
 	} else {
-		ring->mmio_base = BSD_RING_BASE;
-		ring->flush = bsd_ring_flush;
-		ring->add_request = i9xx_add_request;
-		ring->get_seqno = ring_get_seqno;
-		ring->set_seqno = ring_set_seqno;
-		if (IS_GEN5(dev)) {
-			ring->irq_enable_mask = ILK_BSD_USER_INTERRUPT;
-			ring->irq_get = gen5_ring_get_irq;
-			ring->irq_put = gen5_ring_put_irq;
+		engine->mmio_base = BSD_RING_BASE;
+
+		engine->emit_flush = bsd_emit_flush;
+		if (IS_GEN5(dev_priv)) {
+			engine->irq_enable_mask = ILK_BSD_USER_INTERRUPT;
+			engine->irq_get = gen5_irq_get;
+			engine->irq_put = gen5_irq_put;
 		} else {
-			ring->irq_enable_mask = I915_BSD_USER_INTERRUPT;
-			ring->irq_get = i9xx_ring_get_irq;
-			ring->irq_put = i9xx_ring_put_irq;
+			engine->irq_enable_mask = I915_BSD_USER_INTERRUPT;
+			engine->irq_get = i9xx_irq_get;
+			engine->irq_put = i9xx_irq_put;
 		}
-		ring->dispatch_execbuffer = i965_dispatch_execbuffer;
+		engine->emit_batchbuffer = i965_emit_batchbuffer;
 	}
-	ring->init = init_ring_common;
 
-	return intel_init_ring_buffer(dev, ring);
+	return intel_engine_enable_execlists(engine);
 }
 
 /**
  * Initialize the second BSD ring for Broadwell GT3.
  * It is noted that this only exists on Broadwell GT3.
  */
-int intel_init_bsd2_ring_buffer(struct drm_device *dev)
+int intel_init_bsd2_engine(struct drm_i915_private *dev_priv)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring = &dev_priv->ring[VCS2];
+	struct intel_engine_cs *engine = &dev_priv->engine[VCS2];
+	int ret;
 
-	if ((INTEL_INFO(dev)->gen != 8)) {
+	if ((INTEL_INFO(dev_priv)->gen != 8)) {
 		DRM_ERROR("No dual-BSD ring on non-BDW machine\n");
 		return -EINVAL;
 	}
 
-	ring->name = "bsd2 ring";
-	ring->id = VCS2;
+	ret = intel_engine_init(engine, dev_priv);
+	if (ret)
+		return ret;
+
+	engine->name = "bsd2 ring";
+	engine->id = VCS2;
+	engine->mmio_base = GEN8_BSD2_RING_BASE;
+	if (IS_VALLEYVIEW(dev_priv))
+		engine->power_domains = FORCEWAKE_MEDIA;
 
-	ring->write_tail = ring_write_tail;
-	ring->mmio_base = GEN8_BSD2_RING_BASE;
-	ring->flush = gen6_bsd_ring_flush;
-	ring->add_request = gen6_add_request;
-	ring->get_seqno = gen6_ring_get_seqno;
-	ring->set_seqno = ring_set_seqno;
-	ring->irq_enable_mask =
+	engine->emit_flush = gen6_bsd_emit_flush;
+	engine->emit_batchbuffer = gen8_emit_batchbuffer;
+	engine->irq_enable_mask =
 			GT_RENDER_USER_INTERRUPT << GEN8_VCS2_IRQ_SHIFT;
-	ring->irq_get = gen8_ring_get_irq;
-	ring->irq_put = gen8_ring_put_irq;
-	ring->dispatch_execbuffer =
-			gen8_ring_dispatch_execbuffer;
-	if (i915_semaphore_is_enabled(dev)) {
-		ring->semaphore.sync_to = gen8_ring_sync;
-		ring->semaphore.signal = gen8_xcs_signal;
-		GEN8_RING_SEMAPHORE_INIT;
-	}
-	ring->init = init_ring_common;
-
-	return intel_init_ring_buffer(dev, ring);
-}
-
-int intel_init_blt_ring_buffer(struct drm_device *dev)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring = &dev_priv->ring[BCS];
-
-	ring->name = "blitter ring";
-	ring->id = BCS;
-
-	ring->mmio_base = BLT_RING_BASE;
-	ring->write_tail = ring_write_tail;
-	ring->flush = gen6_ring_flush;
-	ring->add_request = gen6_add_request;
-	ring->get_seqno = gen6_ring_get_seqno;
-	ring->set_seqno = ring_set_seqno;
-	if (INTEL_INFO(dev)->gen >= 8) {
-		ring->irq_enable_mask =
+	engine->irq_get = gen8_irq_get;
+	engine->irq_put = gen8_irq_put;
+	gen8_engine_init_semaphore(engine);
+
+	return intel_engine_enable_execlists(engine);
+}
+
+int intel_init_blt_engine(struct drm_i915_private *dev_priv)
+{
+	struct intel_engine_cs *engine = &dev_priv->engine[BCS];
+	int ret;
+
+	ret = intel_engine_init(engine, dev_priv);
+	if (ret)
+		return ret;
+
+	engine->name = "blitter ring";
+	engine->id = BCS;
+	engine->mmio_base = BLT_RING_BASE;
+	if (IS_VALLEYVIEW(dev_priv))
+		engine->power_domains = FORCEWAKE_MEDIA;
+	else if (IS_GEN9(dev_priv))
+		engine->power_domains = FORCEWAKE_BLITTER;
+
+	engine->emit_flush = gen6_blt_emit_flush;
+	if (INTEL_INFO(dev_priv)->gen >= 8) {
+		engine->irq_enable_mask =
 			GT_RENDER_USER_INTERRUPT << GEN8_BCS_IRQ_SHIFT;
-		ring->irq_get = gen8_ring_get_irq;
-		ring->irq_put = gen8_ring_put_irq;
-		ring->dispatch_execbuffer = gen8_ring_dispatch_execbuffer;
-		if (i915_semaphore_is_enabled(dev)) {
-			ring->semaphore.sync_to = gen8_ring_sync;
-			ring->semaphore.signal = gen8_xcs_signal;
-			GEN8_RING_SEMAPHORE_INIT;
-		}
+		engine->irq_get = gen8_irq_get;
+		engine->irq_put = gen8_irq_put;
+		engine->emit_batchbuffer = gen8_emit_batchbuffer;
+		gen8_engine_init_semaphore(engine);
 	} else {
-		ring->irq_enable_mask = GT_BLT_USER_INTERRUPT;
-		ring->irq_get = gen6_ring_get_irq;
-		ring->irq_put = gen6_ring_put_irq;
-		ring->dispatch_execbuffer = gen6_ring_dispatch_execbuffer;
-		if (i915_semaphore_is_enabled(dev)) {
-			ring->semaphore.signal = gen6_signal;
-			ring->semaphore.sync_to = gen6_ring_sync;
+		engine->irq_enable_mask = GT_BLT_USER_INTERRUPT;
+		engine->irq_get = gen6_irq_get;
+		engine->irq_barrier = gen6_irq_barrier;
+		engine->irq_put = gen6_irq_put;
+		engine->emit_batchbuffer = gen6_emit_batchbuffer;
+		if (semaphores_enabled(dev_priv)) {
+			engine->semaphore.signal = gen6_emit_signal;
+			engine->semaphore.wait = gen6_emit_wait;
 			/*
 			 * The current semaphore is only applied on pre-gen8
 			 * platform.  And there is no VCS2 ring on the pre-gen8
@@ -2469,124 +2254,439 @@
 			 * initialized as INVALID.  Gen8 will initialize the
 			 * sema between BCS and VCS2 later.
 			 */
-			ring->semaphore.mbox.wait[RCS] = MI_SEMAPHORE_SYNC_BR;
-			ring->semaphore.mbox.wait[VCS] = MI_SEMAPHORE_SYNC_BV;
-			ring->semaphore.mbox.wait[BCS] = MI_SEMAPHORE_SYNC_INVALID;
-			ring->semaphore.mbox.wait[VECS] = MI_SEMAPHORE_SYNC_BVE;
-			ring->semaphore.mbox.wait[VCS2] = MI_SEMAPHORE_SYNC_INVALID;
-			ring->semaphore.mbox.signal[RCS] = GEN6_RBSYNC;
-			ring->semaphore.mbox.signal[VCS] = GEN6_VBSYNC;
-			ring->semaphore.mbox.signal[BCS] = GEN6_NOSYNC;
-			ring->semaphore.mbox.signal[VECS] = GEN6_VEBSYNC;
-			ring->semaphore.mbox.signal[VCS2] = GEN6_NOSYNC;
+			engine->semaphore.mbox.wait[RCS] = MI_SEMAPHORE_SYNC_BR;
+			engine->semaphore.mbox.wait[VCS] = MI_SEMAPHORE_SYNC_BV;
+			engine->semaphore.mbox.wait[BCS] = MI_SEMAPHORE_SYNC_INVALID;
+			engine->semaphore.mbox.wait[VECS] = MI_SEMAPHORE_SYNC_BVE;
+			engine->semaphore.mbox.wait[VCS2] = MI_SEMAPHORE_SYNC_INVALID;
+			engine->semaphore.mbox.signal[RCS] = GEN6_RBSYNC;
+			engine->semaphore.mbox.signal[VCS] = GEN6_VBSYNC;
+			engine->semaphore.mbox.signal[BCS] = GEN6_NOSYNC;
+			engine->semaphore.mbox.signal[VECS] = GEN6_VEBSYNC;
+			engine->semaphore.mbox.signal[VCS2] = GEN6_NOSYNC;
 		}
 	}
-	ring->init = init_ring_common;
 
-	return intel_init_ring_buffer(dev, ring);
+	return intel_engine_enable_execlists(engine);
 }
 
-int intel_init_vebox_ring_buffer(struct drm_device *dev)
+int intel_init_vebox_engine(struct drm_i915_private *dev_priv)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring = &dev_priv->ring[VECS];
+	struct intel_engine_cs *engine = &dev_priv->engine[VECS];
+	int ret;
+
+	ret = intel_engine_init(engine, dev_priv);
+	if (ret)
+		return ret;
 
-	ring->name = "video enhancement ring";
-	ring->id = VECS;
+	engine->name = "video enhancement ring";
+	engine->id = VECS;
+	engine->mmio_base = VEBOX_RING_BASE;
+	if (IS_VALLEYVIEW(dev_priv))
+		engine->power_domains = FORCEWAKE_MEDIA;
 
-	ring->mmio_base = VEBOX_RING_BASE;
-	ring->write_tail = ring_write_tail;
-	ring->flush = gen6_ring_flush;
-	ring->add_request = gen6_add_request;
-	ring->get_seqno = gen6_ring_get_seqno;
-	ring->set_seqno = ring_set_seqno;
+	engine->emit_flush = gen6_blt_emit_flush;
 
-	if (INTEL_INFO(dev)->gen >= 8) {
-		ring->irq_enable_mask =
+	if (INTEL_INFO(dev_priv)->gen >= 8) {
+		engine->irq_enable_mask =
 			GT_RENDER_USER_INTERRUPT << GEN8_VECS_IRQ_SHIFT;
-		ring->irq_get = gen8_ring_get_irq;
-		ring->irq_put = gen8_ring_put_irq;
-		ring->dispatch_execbuffer = gen8_ring_dispatch_execbuffer;
-		if (i915_semaphore_is_enabled(dev)) {
-			ring->semaphore.sync_to = gen8_ring_sync;
-			ring->semaphore.signal = gen8_xcs_signal;
-			GEN8_RING_SEMAPHORE_INIT;
-		}
+		engine->irq_get = gen8_irq_get;
+		engine->irq_put = gen8_irq_put;
+		engine->emit_batchbuffer = gen8_emit_batchbuffer;
+		gen8_engine_init_semaphore(engine);
 	} else {
-		ring->irq_enable_mask = PM_VEBOX_USER_INTERRUPT;
-		ring->irq_get = hsw_vebox_get_irq;
-		ring->irq_put = hsw_vebox_put_irq;
-		ring->dispatch_execbuffer = gen6_ring_dispatch_execbuffer;
-		if (i915_semaphore_is_enabled(dev)) {
-			ring->semaphore.sync_to = gen6_ring_sync;
-			ring->semaphore.signal = gen6_signal;
-			ring->semaphore.mbox.wait[RCS] = MI_SEMAPHORE_SYNC_VER;
-			ring->semaphore.mbox.wait[VCS] = MI_SEMAPHORE_SYNC_VEV;
-			ring->semaphore.mbox.wait[BCS] = MI_SEMAPHORE_SYNC_VEB;
-			ring->semaphore.mbox.wait[VECS] = MI_SEMAPHORE_SYNC_INVALID;
-			ring->semaphore.mbox.wait[VCS2] = MI_SEMAPHORE_SYNC_INVALID;
-			ring->semaphore.mbox.signal[RCS] = GEN6_RVESYNC;
-			ring->semaphore.mbox.signal[VCS] = GEN6_VVESYNC;
-			ring->semaphore.mbox.signal[BCS] = GEN6_BVESYNC;
-			ring->semaphore.mbox.signal[VECS] = GEN6_NOSYNC;
-			ring->semaphore.mbox.signal[VCS2] = GEN6_NOSYNC;
+		engine->irq_enable_mask = PM_VEBOX_USER_INTERRUPT;
+		engine->irq_get = hsw_vebox_irq_get;
+		engine->irq_barrier = gen6_irq_barrier;
+		engine->irq_put = hsw_vebox_irq_put;
+		engine->emit_batchbuffer = gen6_emit_batchbuffer;
+		if (semaphores_enabled(dev_priv)) {
+			engine->semaphore.wait = gen6_emit_wait;
+			engine->semaphore.signal = gen6_emit_signal;
+			engine->semaphore.mbox.wait[RCS] = MI_SEMAPHORE_SYNC_VER;
+			engine->semaphore.mbox.wait[VCS] = MI_SEMAPHORE_SYNC_VEV;
+			engine->semaphore.mbox.wait[BCS] = MI_SEMAPHORE_SYNC_VEB;
+			engine->semaphore.mbox.wait[VECS] = MI_SEMAPHORE_SYNC_INVALID;
+			engine->semaphore.mbox.wait[VCS2] = MI_SEMAPHORE_SYNC_INVALID;
+			engine->semaphore.mbox.signal[RCS] = GEN6_RVESYNC;
+			engine->semaphore.mbox.signal[VCS] = GEN6_VVESYNC;
+			engine->semaphore.mbox.signal[BCS] = GEN6_BVESYNC;
+			engine->semaphore.mbox.signal[VECS] = GEN6_NOSYNC;
+			engine->semaphore.mbox.signal[VCS2] = GEN6_NOSYNC;
 		}
 	}
-	ring->init = init_ring_common;
 
-	return intel_init_ring_buffer(dev, ring);
+	return intel_engine_enable_execlists(engine);
 }
 
 int
-intel_ring_flush_all_caches(struct intel_engine_cs *ring)
+intel_engine_flush(struct intel_engine_cs *engine,
+		   struct intel_context *ctx)
 {
+	struct i915_gem_request *rq;
 	int ret;
 
-	if (!ring->gpu_caches_dirty)
+	rq = i915_request_create(ctx, engine);
+	if (IS_ERR(rq))
+		return PTR_ERR(rq);
+
+	ret = i915_request_emit_breadcrumb(rq);
+	if (ret == 0)
+		ret = i915_request_commit(rq);
+	i915_request_put(rq);
+
+	return ret;
+}
+
+int intel_engine_sync(struct intel_engine_cs *engine)
+{
+	/* Wait upon the last request to be completed */
+	if (engine->last_request == NULL)
 		return 0;
 
-	ret = ring->flush(ring, 0, I915_GEM_GPU_DOMAINS);
-	if (ret)
-		return ret;
+	return i915_request_wait(engine->last_request);
+}
+
+struct i915_gem_request *
+intel_engine_seqno_to_request(struct intel_engine_cs *engine,
+			      u32 seqno)
+{
+	struct i915_gem_request *rq;
+
+	list_for_each_entry(rq, &engine->requests, engine_link) {
+		if (rq->seqno == seqno)
+			return rq;
+
+		if (__i915_seqno_passed(rq->seqno, seqno))
+			break;
+	}
+
+	return NULL;
+}
+
+void intel_engine_cleanup(struct intel_engine_cs *engine)
+{
+	WARN_ON(engine->last_request);
+
+	if (engine->cleanup)
+		engine->cleanup(engine);
+}
+
+static void intel_engine_clear_rings(struct intel_engine_cs *engine)
+{
+	struct intel_ringbuffer *ring;
+
+	list_for_each_entry(ring, &engine->rings, engine_link) {
+		if (ring->retired_head != -1) {
+			ring->head = ring->retired_head;
+			ring->retired_head = -1;
 
-	trace_i915_gem_ring_flush(ring, 0, I915_GEM_GPU_DOMAINS);
+			ring->space = intel_ring_space(ring);
+		}
+
+		ring->pending_flush = 0;
+	}
+
+	if (engine->last_context) {
+		engine->unpin_context(engine, engine->last_context);
+		i915_gem_context_unreference(engine->last_context);
+		engine->last_context = NULL;
+	}
+}
+
+int intel_engine_suspend(struct intel_engine_cs *engine)
+{
+	struct drm_i915_private *dev_priv = engine->i915;
+	int ret = 0;
+
+	if (WARN_ON(!intel_engine_initialized(engine)))
+		return 0;
+
+	I915_WRITE_IMR(engine, ~0);
+
+	if (engine->suspend)
+		ret = engine->suspend(engine);
+
+	intel_engine_clear_rings(engine);
+
+	return ret;
+}
+
+int intel_engine_resume(struct intel_engine_cs *engine)
+{
+	struct drm_i915_private *dev_priv = engine->i915;
+	int ret = 0;
+
+	if (WARN_ON(!intel_engine_initialized(engine)))
+		return 0;
+
+	if (engine->resume)
+		ret = engine->resume(engine);
+
+	I915_WRITE_IMR(engine, ~engine->irq_keep_mask);
+	return ret;
+}
+
+int intel_engine_retire(struct intel_engine_cs *engine,
+			u32 seqno)
+{
+	int count;
+
+	if (engine->retire)
+		engine->retire(engine, seqno);
+
+	count = 0;
+	while (!list_empty(&engine->requests)) {
+		struct i915_gem_request *rq;
+
+		rq = list_first_entry(&engine->requests,
+				      struct i915_gem_request,
+				      engine_link);
+
+		if (!__i915_seqno_passed(seqno, rq->seqno))
+			break;
+
+		i915_request_retire(rq);
+		count++;
+	}
+
+	if (unlikely(engine->trace_irq_seqno &&
+		     __i915_seqno_passed(seqno, engine->trace_irq_seqno))) {
+		engine->irq_put(engine);
+		engine->trace_irq_seqno = 0;
+	}
+
+	return count;
+}
+
+static struct i915_gem_request *
+find_active_batch(struct list_head *list)
+{
+	struct i915_gem_request *rq, *last = NULL;
+
+	list_for_each_entry(rq, list, engine_link) {
+		if (rq->batch == NULL)
+			continue;
+
+		if (!__i915_request_complete__wa(rq))
+			return rq;
+
+		last = rq;
+	}
+
+	return last;
+}
+
+static bool context_is_banned(const struct intel_context *ctx,
+			      unsigned long now)
+{
+	const struct i915_ctx_hang_stats *hs = &ctx->hang_stats;
+
+	if (hs->banned)
+		return true;
+
+	if (hs->ban_period_seconds == 0)
+		return false;
+
+	if (now - hs->guilty_ts <= hs->ban_period_seconds) {
+		if (!i915_gem_context_is_default(ctx)) {
+			DRM_DEBUG("context hanging too fast, banning!\n");
+			return true;
+		} else if (i915_stop_ring_allow_ban(ctx->i915)) {
+			if (i915_stop_ring_allow_warn(ctx->i915))
+				DRM_ERROR("gpu hanging too fast, banning!\n");
+			return true;
+		}
+	}
+
+	return false;
+}
+
+static void
+intel_engine_hangstats(struct intel_engine_cs *engine)
+{
+	struct i915_ctx_hang_stats *hs;
+	struct i915_gem_request *rq;
+
+	rq = find_active_batch(&engine->requests);
+	if (rq == NULL)
+		return;
+
+	hs = &rq->ctx->hang_stats;
+	if (engine->hangcheck.score >= HANGCHECK_SCORE_RING_HUNG) {
+		unsigned long now = get_seconds();
+		hs->banned = context_is_banned(rq->ctx, now);
+		hs->guilty_ts = now;
+		hs->batch_active++;
+	} else
+		hs->batch_pending++;
+
+	list_for_each_entry_continue(rq, &engine->requests, engine_link) {
+		if (rq->batch == NULL)
+			continue;
+
+		if (__i915_request_complete__wa(rq))
+			continue;
+
+		rq->ctx->hang_stats.batch_pending++;
+	}
+}
+
+void intel_engine_reset(struct intel_engine_cs *engine)
+{
+	if (WARN_ON(!intel_engine_initialized(engine)))
+		return;
+
+	if (engine->reset)
+		engine->reset(engine);
+
+	memset(&engine->hangcheck, 0, sizeof(engine->hangcheck));
+	intel_engine_hangstats(engine);
+
+	intel_engine_retire(engine, engine->i915->next_seqno);
+	intel_engine_clear_rings(engine);
+}
+
+static int ring_wait(struct intel_ringbuffer *ring, int n)
+{
+	int ret;
+
+	trace_intel_ringbuffer_wait(ring, n);
+
+	do {
+		struct i915_gem_request *rq;
+
+		i915_gem_retire_requests__engine(ring->engine);
+		if (ring->retired_head != -1) {
+			ring->head = ring->retired_head;
+			ring->retired_head = -1;
+
+			ring->space = intel_ring_space(ring);
+			if (ring->space >= n)
+				return 0;
+		}
+
+		list_for_each_entry(rq, &ring->breadcrumbs, breadcrumb_link)
+			if (__intel_ring_space(rq->tail, ring->tail,
+					       ring->size, I915_RING_RSVD) >= n)
+				break;
+
+		if (WARN_ON(&rq->breadcrumb_link == &ring->breadcrumbs))
+			return -EDEADLK;
+
+		ret = i915_request_wait(rq);
+	} while (ret == 0);
+
+	return ret;
+}
+
+static int ring_wrap(struct intel_ringbuffer *ring, int bytes)
+{
+	uint32_t __iomem *virt;
+	int rem;
+
+	rem = ring->size - ring->tail;
+	if (unlikely(ring->space < rem)) {
+		rem = ring_wait(ring, rem);
+		if (rem)
+			return rem;
+	}
+
+	trace_intel_ringbuffer_wrap(ring, rem);
+
+	virt = ring->virtual_start + ring->tail;
+	rem = ring->size - ring->tail;
+
+	ring->space -= rem;
+	ring->tail = 0;
+
+	rem /= 4;
+	while (rem--)
+		iowrite32(MI_NOOP, virt++);
 
-	ring->gpu_caches_dirty = false;
 	return 0;
 }
 
-int
-intel_ring_invalidate_all_caches(struct intel_engine_cs *ring)
+static int __intel_ring_prepare(struct intel_ringbuffer *ring,
+				int bytes)
 {
-	uint32_t flush_domains;
 	int ret;
 
-	flush_domains = 0;
-	if (ring->gpu_caches_dirty)
-		flush_domains = I915_GEM_GPU_DOMAINS;
+	trace_intel_ringbuffer_begin(ring, bytes);
 
-	ret = ring->flush(ring, I915_GEM_GPU_DOMAINS, flush_domains);
-	if (ret)
-		return ret;
+	if (unlikely(ring->tail + bytes > ring->effective_size)) {
+		ret = ring_wrap(ring, bytes);
+		if (unlikely(ret))
+			return ret;
+	}
 
-	trace_i915_gem_ring_flush(ring, I915_GEM_GPU_DOMAINS, flush_domains);
+	if (unlikely(ring->space < bytes)) {
+		ret = ring_wait(ring, bytes);
+		if (unlikely(ret))
+			return ret;
+	}
 
-	ring->gpu_caches_dirty = false;
 	return 0;
 }
 
-void
-intel_stop_ring_buffer(struct intel_engine_cs *ring)
+struct intel_ringbuffer *
+intel_ring_begin(struct i915_gem_request *rq,
+		 int num_dwords)
 {
+	struct intel_ringbuffer *ring = rq->ring;
 	int ret;
 
-	if (!intel_ring_initialized(ring))
-		return;
+	/* TAIL updates must be aligned to a qword, so make sure we
+	 * reserve space for any implicit padding required for this
+	 * command.
+	 */
+	ret = __intel_ring_prepare(ring,
+				   ALIGN(num_dwords, 2) * sizeof(uint32_t));
+	if (ret)
+		return ERR_PTR(ret);
+
+	ring->space -= num_dwords * sizeof(uint32_t);
+
+	return ring;
+}
+
+/* Align the ring tail to a cacheline boundary */
+int intel_ring_cacheline_align(struct i915_gem_request *rq)
+{
+	struct intel_ringbuffer *ring;
+	int tail, num_dwords;
+
+	do {
+		tail = rq->ring->tail;
+		num_dwords = (tail & (CACHELINE_BYTES - 1)) / sizeof(uint32_t);
+		if (num_dwords == 0)
+			return 0;
+
+		num_dwords = CACHELINE_BYTES / sizeof(uint32_t) - num_dwords;
+		ring = intel_ring_begin(rq, num_dwords);
+		if (IS_ERR(ring))
+			return PTR_ERR(ring);
+	} while (tail != rq->ring->tail);
+
+	while (num_dwords--)
+		intel_ring_emit(ring, MI_NOOP);
+
+	intel_ring_advance(ring);
+
+	return 0;
+}
+
+struct i915_gem_request *
+intel_engine_find_active_batch(struct intel_engine_cs *engine)
+{
+	struct i915_gem_request *rq;
+	unsigned long flags;
 
-	ret = intel_ring_idle(ring);
-	if (ret && !i915_reset_in_progress(&to_i915(ring->dev)->gpu_error))
-		DRM_ERROR("failed to quiesce %s whilst cleaning up: %d\n",
-			  ring->name, ret);
+	spin_lock_irqsave(&engine->irqlock, flags);
+	rq = find_active_batch(&engine->submitted);
+	spin_unlock_irqrestore(&engine->irqlock, flags);
+	if (rq)
+		return rq;
 
-	stop_ring(ring);
+	return find_active_batch(&engine->requests);
 }
diff -urN a/drivers/gpu/drm/i915/intel_ringbuffer.h b/drivers/gpu/drm/i915/intel_ringbuffer.h
--- a/drivers/gpu/drm/i915/intel_ringbuffer.h	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_ringbuffer.h	2014-11-22 14:37:49.346700417 -0700
@@ -5,6 +5,13 @@
 
 #define I915_CMD_HASH_ORDER 9
 
+/* Early gen2 devices have a cacheline of just 32 bytes, using 64 is overkill,
+ * but keeps the logic simple. Indeed, the whole purpose of this macro is just
+ * to give some inclination as to some of the magic values used in the various
+ * workarounds!
+ */
+#define CACHELINE_BYTES 64
+
 /*
  * Gen2 BSpec "1. Programming Environment" / 1.4.4.6 "Ring Buffer Use"
  * Gen3 BSpec "vol1c Memory Interface Functions" / 2.3.4.5 "Ring Buffer Use"
@@ -13,61 +20,48 @@
  * "If the Ring Buffer Head Pointer and the Tail Pointer are on the same
  * cacheline, the Head Pointer must not be greater than the Tail
  * Pointer."
+ *
+ * To also accommodate errata on 830/845 which makes the last pair of
+ * cachelines in the ringbuffer unavailable, reduce the available space
+ * further.
  */
-#define I915_RING_FREE_SPACE 64
+#define I915_RING_RSVD (2*CACHELINE_BYTES)
 
-struct  intel_hw_status_page {
+struct intel_hw_status_page {
 	u32		*page_addr;
 	unsigned int	gfx_addr;
 	struct		drm_i915_gem_object *obj;
 };
 
-#define I915_READ_TAIL(ring) I915_READ(RING_TAIL((ring)->mmio_base))
-#define I915_WRITE_TAIL(ring, val) I915_WRITE(RING_TAIL((ring)->mmio_base), val)
+#define I915_READ_TAIL(engine) I915_READ(RING_TAIL((engine)->mmio_base))
+#define I915_WRITE_TAIL(engine, val) I915_WRITE(RING_TAIL((engine)->mmio_base), val)
 
-#define I915_READ_START(ring) I915_READ(RING_START((ring)->mmio_base))
-#define I915_WRITE_START(ring, val) I915_WRITE(RING_START((ring)->mmio_base), val)
+#define I915_READ_START(engine) I915_READ(RING_START((engine)->mmio_base))
+#define I915_WRITE_START(engine, val) I915_WRITE(RING_START((engine)->mmio_base), val)
 
-#define I915_READ_HEAD(ring)  I915_READ(RING_HEAD((ring)->mmio_base))
-#define I915_WRITE_HEAD(ring, val) I915_WRITE(RING_HEAD((ring)->mmio_base), val)
+#define I915_READ_HEAD(engine)  I915_READ(RING_HEAD((engine)->mmio_base))
+#define I915_WRITE_HEAD(engine, val) I915_WRITE(RING_HEAD((engine)->mmio_base), val)
 
-#define I915_READ_CTL(ring) I915_READ(RING_CTL((ring)->mmio_base))
-#define I915_WRITE_CTL(ring, val) I915_WRITE(RING_CTL((ring)->mmio_base), val)
+#define I915_READ_CTL(engine) I915_READ(RING_CTL((engine)->mmio_base))
+#define I915_WRITE_CTL(engine, val) I915_WRITE(RING_CTL((engine)->mmio_base), val)
 
-#define I915_READ_IMR(ring) I915_READ(RING_IMR((ring)->mmio_base))
-#define I915_WRITE_IMR(ring, val) I915_WRITE(RING_IMR((ring)->mmio_base), val)
+#define I915_READ_IMR(engine) I915_READ(RING_IMR((engine)->mmio_base))
+#define I915_WRITE_IMR(engine, val) I915_WRITE(RING_IMR((engine)->mmio_base), val)
 
-#define I915_READ_MODE(ring) I915_READ(RING_MI_MODE((ring)->mmio_base))
-#define I915_WRITE_MODE(ring, val) I915_WRITE(RING_MI_MODE((ring)->mmio_base), val)
+#define I915_READ_MODE(engine) I915_READ(RING_MI_MODE((engine)->mmio_base))
+#define I915_WRITE_MODE(engine, val) I915_WRITE(RING_MI_MODE((engine)->mmio_base), val)
 
 /* seqno size is actually only a uint32, but since we plan to use MI_FLUSH_DW to
  * do the writes, and that must have qw aligned offsets, simply pretend it's 8b.
  */
 #define i915_semaphore_seqno_size sizeof(uint64_t)
-#define GEN8_SIGNAL_OFFSET(__ring, to)			     \
-	(i915_gem_obj_ggtt_offset(dev_priv->semaphore_obj) + \
-	((__ring)->id * I915_NUM_RINGS * i915_semaphore_seqno_size) +	\
-	(i915_semaphore_seqno_size * (to)))
-
-#define GEN8_WAIT_OFFSET(__ring, from)			     \
-	(i915_gem_obj_ggtt_offset(dev_priv->semaphore_obj) + \
-	((from) * I915_NUM_RINGS * i915_semaphore_seqno_size) + \
-	(i915_semaphore_seqno_size * (__ring)->id))
-
-#define GEN8_RING_SEMAPHORE_INIT do { \
-	if (!dev_priv->semaphore_obj) { \
-		break; \
-	} \
-	ring->semaphore.signal_ggtt[RCS] = GEN8_SIGNAL_OFFSET(ring, RCS); \
-	ring->semaphore.signal_ggtt[VCS] = GEN8_SIGNAL_OFFSET(ring, VCS); \
-	ring->semaphore.signal_ggtt[BCS] = GEN8_SIGNAL_OFFSET(ring, BCS); \
-	ring->semaphore.signal_ggtt[VECS] = GEN8_SIGNAL_OFFSET(ring, VECS); \
-	ring->semaphore.signal_ggtt[VCS2] = GEN8_SIGNAL_OFFSET(ring, VCS2); \
-	ring->semaphore.signal_ggtt[ring->id] = MI_SEMAPHORE_SYNC_INVALID; \
-	} while(0)
+#define GEN8_SEMAPHORE_OFFSET(__dp, __from, __to)			     \
+	(i915_gem_obj_ggtt_offset((__dp)->semaphore_obj) + \
+	 ((__from) * I915_NUM_ENGINES + (__to)) * i915_semaphore_seqno_size)
 
-enum intel_ring_hangcheck_action {
+enum intel_engine_hangcheck_action {
 	HANGCHECK_IDLE = 0,
+	HANGCHECK_IDLE_WAITERS,
 	HANGCHECK_WAIT,
 	HANGCHECK_ACTIVE,
 	HANGCHECK_ACTIVE_LOOP,
@@ -77,38 +71,61 @@
 
 #define HANGCHECK_SCORE_RING_HUNG 31
 
-struct intel_ring_hangcheck {
+struct intel_engine_hangcheck {
 	u64 acthd;
 	u64 max_acthd;
 	u32 seqno;
+	u32 interrupts;
 	int score;
-	enum intel_ring_hangcheck_action action;
+	enum intel_engine_hangcheck_action action;
 	int deadlock;
 };
 
+struct i915_gem_request;
+struct intel_context;
+struct intel_engine_cs;
+
 struct intel_ringbuffer {
+	struct intel_engine_cs *engine;
+	struct intel_context *ctx;
+	struct list_head engine_link;
+
 	struct drm_i915_gem_object *obj;
 	void __iomem *virtual_start;
+	uint32_t ggtt_offset;
+
+	/**
+	 * List of breadcrumbs associated with GPU requests currently
+	 * outstanding.
+	 */
+	struct list_head requests;
+	struct list_head breadcrumbs;
 
-	u32 head;
-	u32 tail;
+	int head;
+	int tail;
 	int space;
+
 	int size;
 	int effective_size;
 
 	/** We track the position of the requests in the ring buffer, and
-	 * when each is retired we increment last_retired_head as the GPU
+	 * when each is retired we increment retired_head as the GPU
 	 * must have finished processing the request and so we know we
 	 * can advance the ringbuffer up to that position.
 	 *
-	 * last_retired_head is set to -1 after the value is consumed so
+	 * retired_head is set to -1 after the value is consumed so
 	 * we can detect new retirements.
 	 */
-	u32 last_retired_head;
+	int retired_head;
+	int breadcrumb_tail;
+
+	unsigned pending_flush:4;
+	unsigned iomap:1;
 };
 
-struct  intel_engine_cs {
-	const char	*name;
+struct intel_engine_cs {
+	struct drm_i915_private *i915;
+	const char *name;
 	enum intel_ring_id {
 		RCS = 0x0,
 		VCS,
@@ -116,44 +133,77 @@
 		VECS,
 		VCS2
 	} id;
-#define I915_NUM_RINGS 5
+#define I915_NUM_ENGINES 5
+#define I915_NUM_ENGINE_BITS 4
 #define LAST_USER_RING (VECS + 1)
-	u32		mmio_base;
-	struct		drm_device *dev;
-	struct intel_ringbuffer *buffer;
+	u32 mmio_base;
+	u32 power_domains;
+
+	/* protects requests against hangcheck */
+	spinlock_t lock;
+	/* protects exlists: pending + submitted */
+	spinlock_t irqlock;
+
+	atomic_t interrupts;
+	u32 breadcrumb[I915_NUM_ENGINES];
+	u16 tag, next_tag;
+
+	struct list_head rings;
+	struct list_head requests;
+	struct list_head pending, submitted;
+	struct i915_gem_request *last_request;
+	struct intel_context *last_context;
 
 	struct intel_hw_status_page status_page;
 
-	unsigned irq_refcount; /* protected by dev_priv->irq_lock */
-	u32		irq_enable_mask;	/* bitmask to enable ring interrupt */
+	unsigned irq_refcount; /* protected by i915->irq_lock */
+	u32		irq_enable_mask; /* bitmask to enable ring interrupt */
+	u32             irq_keep_mask; /* never mask these interrupts */
 	u32		trace_irq_seqno;
-	bool __must_check (*irq_get)(struct intel_engine_cs *ring);
-	void		(*irq_put)(struct intel_engine_cs *ring);
+	void		(*irq_get)(struct intel_engine_cs *engine);
+	void		(*irq_barrier)(struct intel_engine_cs *engine);
+	void		(*irq_put)(struct intel_engine_cs *engine);
+
+	void		(*retire)(struct intel_engine_cs *engine,
+				  u32 seqno);
+	void		(*reset)(struct intel_engine_cs *engine);
+	int		(*suspend)(struct intel_engine_cs *engine);
+	int		(*resume)(struct intel_engine_cs *engine);
+	void		(*cleanup)(struct intel_engine_cs *engine);
+
+	int		(*init_context)(struct i915_gem_request *rq);
+
+	int __must_check (*emit_flush)(struct i915_gem_request *rq,
+				       u32 domains);
+#define I915_COMMAND_BARRIER 0x1
+#define I915_FLUSH_CACHES 0x2
+#define I915_INVALIDATE_CACHES 0x4
+#define I915_KICK_FBC 0x8
+	int __must_check (*emit_batchbuffer)(struct i915_gem_request *rq,
+					     u64 offset, u32 length,
+					     unsigned flags);
+#define I915_DISPATCH_SECURE 0x1
+#define I915_DISPATCH_PINNED 0x2
+	int __must_check (*emit_breadcrumb)(struct i915_gem_request *rq);
 
-	int		(*init)(struct intel_engine_cs *ring);
+	struct intel_ringbuffer *  __must_check
+		(*pin_context)(struct intel_engine_cs *engine,
+			       struct intel_context *ctx);
+	void (*add_context)(struct i915_gem_request *rq,
+			    struct intel_context *ctx);
+	void (*unpin_context)(struct intel_engine_cs *engine,
+			      struct intel_context *ctx);
+	void (*free_context)(struct intel_engine_cs *engine,
+			     struct intel_context *ctx);
 
-	void		(*write_tail)(struct intel_engine_cs *ring,
+
+	int __must_check (*add_request)(struct i915_gem_request *rq);
+	void		(*write_tail)(struct intel_engine_cs *engine,
 				      u32 value);
-	int __must_check (*flush)(struct intel_engine_cs *ring,
-				  u32	invalidate_domains,
-				  u32	flush_domains);
-	int		(*add_request)(struct intel_engine_cs *ring);
-	/* Some chipsets are not quite as coherent as advertised and need
-	 * an expensive kick to force a true read of the up-to-date seqno.
-	 * However, the up-to-date seqno is not always required and the last
-	 * seen value is good enough. Note that the seqno will always be
-	 * monotonic, even if not coherent.
-	 */
-	u32		(*get_seqno)(struct intel_engine_cs *ring,
-				     bool lazy_coherency);
-	void		(*set_seqno)(struct intel_engine_cs *ring,
-				     u32 seqno);
-	int		(*dispatch_execbuffer)(struct intel_engine_cs *ring,
-					       u64 offset, u32 length,
-					       unsigned flags);
-#define I915_DISPATCH_SECURE 0x1
-#define I915_DISPATCH_PINNED 0x2
-	void		(*cleanup)(struct intel_engine_cs *ring);
+
+	bool (*is_complete)(struct i915_gem_request *rq);
+	bool (*is_idle)(struct intel_engine_cs *engine);
+
 
 	/* GEN8 signal/wait table - never trust comments!
 	 *	  signal to	signal to    signal to   signal to      signal to
@@ -193,27 +243,25 @@
 	 *  ie. transpose of f(x, y)
 	 */
 	struct {
-		u32	sync_seqno[I915_NUM_RINGS-1];
+		struct {
+			/* our mbox written by others */
+			u32		wait[I915_NUM_ENGINES];
+			/* mboxes this ring signals to */
+			u32		signal[I915_NUM_ENGINES];
+		} mbox;
+
+		int	(*wait)(struct i915_gem_request *waiter,
+				struct i915_gem_request *signaller);
+		int	(*signal)(struct i915_gem_request *rq, int id);
 
-		union {
-			struct {
-				/* our mbox written by others */
-				u32		wait[I915_NUM_RINGS];
-				/* mboxes this ring signals to */
-				u32		signal[I915_NUM_RINGS];
-			} mbox;
-			u64		signal_ggtt[I915_NUM_RINGS];
-		};
-
-		/* AKA wait() */
-		int	(*sync_to)(struct intel_engine_cs *ring,
-				   struct intel_engine_cs *to,
-				   u32 seqno);
-		int	(*signal)(struct intel_engine_cs *signaller,
-				  /* num_dwords needed by caller */
-				  unsigned int num_dwords);
+		u32 sync[I915_NUM_ENGINES];
 	} semaphore;
 
+	/* Execlists */
+	bool execlists_enabled;
+	u32 execlists_submitted;
+	u8 next_context_status_buffer;
+
 	/**
 	 * List of objects currently involved in rendering from the
 	 * ringbuffer.
@@ -224,33 +272,19 @@
 	 *
 	 * A reference is held on the buffer while on this list.
 	 */
-	struct list_head active_list;
+	struct list_head vma_list, read_list, write_list, fence_list;
 
-	/**
-	 * List of breadcrumbs associated with GPU requests currently
-	 * outstanding.
-	 */
-	struct list_head request_list;
-
-	/**
-	 * Do we have some not yet emitted requests outstanding?
-	 */
-	struct drm_i915_gem_request *preallocated_lazy_request;
-	u32 outstanding_lazy_seqno;
-	bool gpu_caches_dirty;
-	bool fbc_dirty;
+	u64 pmu_sample[3];
 
 	wait_queue_head_t irq_queue;
 
 	struct intel_context *default_context;
-	struct intel_context *last_context;
 
-	struct intel_ring_hangcheck hangcheck;
+	struct intel_engine_hangcheck hangcheck;
 
 	struct {
 		struct drm_i915_gem_object *obj;
 		u32 gtt_offset;
-		volatile u32 *cpu_page;
 	} scratch;
 
 	bool needs_cmd_parser;
@@ -288,52 +322,31 @@
 };
 
 static inline bool
-intel_ring_initialized(struct intel_engine_cs *ring)
+intel_engine_initialized(struct intel_engine_cs *engine)
 {
-	return ring->buffer && ring->buffer->obj;
+	return engine->default_context;
 }
 
 static inline unsigned
-intel_ring_flag(struct intel_engine_cs *ring)
-{
-	return 1 << ring->id;
-}
-
-static inline u32
-intel_ring_sync_index(struct intel_engine_cs *ring,
-		      struct intel_engine_cs *other)
+intel_engine_flag(struct intel_engine_cs *engine)
 {
-	int idx;
-
-	/*
-	 * rcs -> 0 = vcs, 1 = bcs, 2 = vecs, 3 = vcs2;
-	 * vcs -> 0 = bcs, 1 = vecs, 2 = vcs2, 3 = rcs;
-	 * bcs -> 0 = vecs, 1 = vcs2. 2 = rcs, 3 = vcs;
-	 * vecs -> 0 = vcs2, 1 = rcs, 2 = vcs, 3 = bcs;
-	 * vcs2 -> 0 = rcs, 1 = vcs, 2 = bcs, 3 = vecs;
-	 */
-
-	idx = (other - ring) - 1;
-	if (idx < 0)
-		idx += I915_NUM_RINGS;
-
-	return idx;
+	return 1 << engine->id;
 }
 
 static inline u32
-intel_read_status_page(struct intel_engine_cs *ring,
+intel_read_status_page(struct intel_engine_cs *engine,
 		       int reg)
 {
 	/* Ensure that the compiler doesn't optimize away the load. */
 	barrier();
-	return ring->status_page.page_addr[reg];
+	return engine->status_page.page_addr[reg];
 }
 
 static inline void
-intel_write_status_page(struct intel_engine_cs *ring,
+intel_write_status_page(struct intel_engine_cs *engine,
 			int reg, u32 value)
 {
-	ring->status_page.page_addr[reg] = value;
+	engine->status_page.page_addr[reg] = value;
 }
 
 /**
@@ -355,57 +368,79 @@
 #define I915_GEM_HWS_SCRATCH_INDEX	0x30
 #define I915_GEM_HWS_SCRATCH_ADDR (I915_GEM_HWS_SCRATCH_INDEX << MI_STORE_DWORD_INDEX_SHIFT)
 
-void intel_stop_ring_buffer(struct intel_engine_cs *ring);
-void intel_cleanup_ring_buffer(struct intel_engine_cs *ring);
+static inline u32
+intel_engine_get_seqno(struct intel_engine_cs *engine)
+{
+	return intel_read_status_page(engine, I915_GEM_HWS_INDEX);
+}
 
-int __must_check intel_ring_begin(struct intel_engine_cs *ring, int n);
-int __must_check intel_ring_cacheline_align(struct intel_engine_cs *ring);
-static inline void intel_ring_emit(struct intel_engine_cs *ring,
+struct intel_ringbuffer *
+intel_engine_alloc_ring(struct intel_engine_cs *engine,
+			struct intel_context *ctx,
+			int size);
+void intel_ring_free(struct intel_ringbuffer *ring);
+
+struct intel_ringbuffer *__must_check
+intel_ring_begin(struct i915_gem_request *rq, int n);
+int __must_check intel_ring_cacheline_align(struct i915_gem_request *rq);
+static inline void intel_ring_emit(struct intel_ringbuffer *ring,
 				   u32 data)
 {
-	struct intel_ringbuffer *ringbuf = ring->buffer;
-	iowrite32(data, ringbuf->virtual_start + ringbuf->tail);
-	ringbuf->tail += 4;
+	iowrite32(data, ring->virtual_start + ring->tail);
+	ring->tail += 4;
 }
-static inline void intel_ring_advance(struct intel_engine_cs *ring)
+static inline void intel_ring_advance(struct intel_ringbuffer *ring)
 {
-	struct intel_ringbuffer *ringbuf = ring->buffer;
-	ringbuf->tail &= ringbuf->size - 1;
+	ring->tail &= ring->size - 1;
 }
-void __intel_ring_advance(struct intel_engine_cs *ring);
-
-int __must_check intel_ring_idle(struct intel_engine_cs *ring);
-void intel_ring_init_seqno(struct intel_engine_cs *ring, u32 seqno);
-int intel_ring_flush_all_caches(struct intel_engine_cs *ring);
-int intel_ring_invalidate_all_caches(struct intel_engine_cs *ring);
-
-int intel_init_render_ring_buffer(struct drm_device *dev);
-int intel_init_bsd_ring_buffer(struct drm_device *dev);
-int intel_init_bsd2_ring_buffer(struct drm_device *dev);
-int intel_init_blt_ring_buffer(struct drm_device *dev);
-int intel_init_vebox_ring_buffer(struct drm_device *dev);
 
-u64 intel_ring_get_active_head(struct intel_engine_cs *ring);
-void intel_ring_setup_status_page(struct intel_engine_cs *ring);
-
-static inline u32 intel_ring_get_tail(struct intel_ringbuffer *ringbuf)
+static inline int __intel_ring_space(int head, int tail, int size, int rsvd)
 {
-	return ringbuf->tail;
+	int space = head - (tail + 8);
+	if (space < 0)
+		space += size;
+	return space - rsvd;
 }
 
-static inline u32 intel_ring_get_seqno(struct intel_engine_cs *ring)
+static inline int intel_ring_space(struct intel_ringbuffer *ring)
 {
-	BUG_ON(ring->outstanding_lazy_seqno == 0);
-	return ring->outstanding_lazy_seqno;
+	return __intel_ring_space(ring->head, ring->tail,
+				  ring->size, I915_RING_RSVD);
 }
 
-static inline void i915_trace_irq_get(struct intel_engine_cs *ring, u32 seqno)
+
+struct i915_gem_request *
+intel_engine_find_active_batch(struct intel_engine_cs *engine);
+
+struct i915_gem_request *
+intel_engine_seqno_to_request(struct intel_engine_cs *engine,
+			      u32 seqno);
+
+int intel_init_render_engine(struct drm_i915_private *i915);
+int intel_init_bsd_engine(struct drm_i915_private *i915);
+int intel_init_bsd2_engine(struct drm_i915_private *i915);
+int intel_init_blt_engine(struct drm_i915_private *i915);
+int intel_init_vebox_engine(struct drm_i915_private *i915);
+
+int __must_check intel_engine_sync(struct intel_engine_cs *engine);
+int __must_check intel_engine_flush(struct intel_engine_cs *engine,
+				    struct intel_context *ctx);
+
+int intel_engine_retire(struct intel_engine_cs *engine, u32 seqno);
+void intel_engine_reset(struct intel_engine_cs *engine);
+int intel_engine_suspend(struct intel_engine_cs *engine);
+int intel_engine_resume(struct intel_engine_cs *engine);
+void intel_engine_cleanup(struct intel_engine_cs *engine);
+
+
+u64 intel_engine_get_active_head(struct intel_engine_cs *engine);
+
+static inline void i915_trace_irq_get(struct intel_engine_cs *engine, u32 seqno)
 {
-	if (ring->trace_irq_seqno == 0 && ring->irq_get(ring))
-		ring->trace_irq_seqno = seqno;
-}
+	if (engine->trace_irq_seqno == 0)
+		engine->irq_get(engine);
 
-/* DRI warts */
-int intel_render_ring_init_dri(struct drm_device *dev, u64 start, u32 size);
+	engine->trace_irq_seqno = seqno;
+}
 
 #endif /* _INTEL_RINGBUFFER_H_ */
diff -urN a/drivers/gpu/drm/i915/intel_runtime_pm.c b/drivers/gpu/drm/i915/intel_runtime_pm.c
--- a/drivers/gpu/drm/i915/intel_runtime_pm.c	1969-12-31 17:00:00.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_runtime_pm.c	2014-11-22 14:37:49.346700417 -0700
@@ -0,0 +1,1406 @@
+/*
+ * Copyright  2012-2014 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ *
+ * Authors:
+ *    Eugeni Dodonov <eugeni.dodonov@intel.com>
+ *    Daniel Vetter <daniel.vetter@ffwll.ch>
+ *
+ */
+
+#include <linux/pm_runtime.h>
+#include <linux/vgaarb.h>
+
+#include "i915_drv.h"
+#include "intel_drv.h"
+#include <drm/i915_powerwell.h>
+
+/**
+ * DOC: runtime pm
+ *
+ * The i915 driver supports dynamic enabling and disabling of entire hardware
+ * blocks at runtime. This is especially important on the display side where
+ * software is supposed to control many power gates manually on recent hardware,
+ * since on the GT side a lot of the power management is done by the hardware.
+ * But even there some manual control at the device level is required.
+ *
+ * Since i915 supports a diverse set of platforms with a unified codebase and
+ * hardware engineers just love to shuffle functionality around between power
+ * domains there's a sizeable amount of indirection required. This file provides
+ * generic functions to the driver for grabbing and releasing references for
+ * abstract power domains. It then maps those to the actual power wells
+ * present for a given platform.
+ */
+
+static struct i915_power_domains *hsw_pwr;
+
+#define for_each_power_well(i, power_well, domain_mask, power_domains)	\
+	for (i = 0;							\
+	     i < (power_domains)->power_well_count &&			\
+		 ((power_well) = &(power_domains)->power_wells[i]);	\
+	     i++)							\
+		if ((power_well)->domains & (domain_mask))
+
+#define for_each_power_well_rev(i, power_well, domain_mask, power_domains) \
+	for (i = (power_domains)->power_well_count - 1;			 \
+	     i >= 0 && ((power_well) = &(power_domains)->power_wells[i]);\
+	     i--)							 \
+		if ((power_well)->domains & (domain_mask))
+
+/*
+ * We should only use the power well if we explicitly asked the hardware to
+ * enable it, so check if it's enabled and also check if we've requested it to
+ * be enabled.
+ */
+static bool hsw_power_well_enabled(struct drm_i915_private *dev_priv,
+				   struct i915_power_well *power_well)
+{
+	return I915_READ(HSW_PWR_WELL_DRIVER) ==
+		     (HSW_PWR_WELL_ENABLE_REQUEST | HSW_PWR_WELL_STATE_ENABLED);
+}
+
+/**
+ * __intel_display_power_is_enabled - unlocked check for a power domain
+ * @dev_priv: i915 device instance
+ * @domain: power domain to check
+ *
+ * This is the unlocked version of intel_display_power_is_enabled() and should
+ * only be used from error capture and recovery code where deadlocks are
+ * possible.
+ *
+ * Returns:
+ * True when the power domain is enabled, false otherwise.
+ */
+bool __intel_display_power_is_enabled(struct drm_i915_private *dev_priv,
+				      enum intel_display_power_domain domain)
+{
+	struct i915_power_domains *power_domains;
+	struct i915_power_well *power_well;
+	bool is_enabled;
+	int i;
+
+	if (dev_priv->pm.suspended)
+		return false;
+
+	power_domains = &dev_priv->power_domains;
+
+	is_enabled = true;
+
+	for_each_power_well_rev(i, power_well, BIT(domain), power_domains) {
+		if (power_well->always_on)
+			continue;
+
+		if (!power_well->hw_enabled) {
+			is_enabled = false;
+			break;
+		}
+	}
+
+	return is_enabled;
+}
+
+/**
+ * intel_display_power_is_enabled - unlocked check for a power domain
+ * @dev_priv: i915 device instance
+ * @domain: power domain to check
+ *
+ * This function can be used to check the hw power domain state. It is mostly
+ * used in hardware state readout functions. Everywhere else code should rely
+ * upon explicit power domain reference counting to ensure that the hardware
+ * block is powered up before accessing it.
+ *
+ * Callers must hold the relevant modesetting locks to ensure that concurrent
+ * threads can't disable the power well while the caller tries to read a few
+ * registers.
+ *
+ * Returns:
+ * True when the power domain is enabled, false otherwise.
+ */
+bool intel_display_power_is_enabled(struct drm_i915_private *dev_priv,
+				    enum intel_display_power_domain domain)
+{
+	struct i915_power_domains *power_domains;
+	bool ret;
+
+	power_domains = &dev_priv->power_domains;
+
+	mutex_lock(&power_domains->lock);
+	ret = __intel_display_power_is_enabled(dev_priv, domain);
+	mutex_unlock(&power_domains->lock);
+
+	return ret;
+}
+
+/**
+ * intel_display_set_init_power - set the initial power domain state
+ * @dev_priv: i915 device instance
+ * @enable: whether to enable or disable the initial power domain state
+ *
+ * For simplicity our driver load/unload and system suspend/resume code assumes
+ * that all power domains are always enabled. This functions controls the state
+ * of this little hack. While the initial power domain state is enabled runtime
+ * pm is effectively disabled.
+ */
+void intel_display_set_init_power(struct drm_i915_private *dev_priv,
+				  bool enable)
+{
+	if (dev_priv->power_domains.init_power_on == enable)
+		return;
+
+	if (enable)
+		intel_display_power_get(dev_priv, POWER_DOMAIN_INIT);
+	else
+		intel_display_power_put(dev_priv, POWER_DOMAIN_INIT);
+
+	dev_priv->power_domains.init_power_on = enable;
+}
+
+/*
+ * Starting with Haswell, we have a "Power Down Well" that can be turned off
+ * when not needed anymore. We have 4 registers that can request the power well
+ * to be enabled, and it will only be disabled if none of the registers is
+ * requesting it to be enabled.
+ */
+static void hsw_power_well_post_enable(struct drm_i915_private *dev_priv)
+{
+	struct drm_device *dev = dev_priv->dev;
+
+	/*
+	 * After we re-enable the power well, if we touch VGA register 0x3d5
+	 * we'll get unclaimed register interrupts. This stops after we write
+	 * anything to the VGA MSR register. The vgacon module uses this
+	 * register all the time, so if we unbind our driver and, as a
+	 * consequence, bind vgacon, we'll get stuck in an infinite loop at
+	 * console_unlock(). So make here we touch the VGA MSR register, making
+	 * sure vgacon can keep working normally without triggering interrupts
+	 * and error messages.
+	 */
+	vga_get_uninterruptible(dev->pdev, VGA_RSRC_LEGACY_IO);
+	outb(inb(VGA_MSR_READ), VGA_MSR_WRITE);
+	vga_put(dev->pdev, VGA_RSRC_LEGACY_IO);
+
+	if (IS_BROADWELL(dev) || (INTEL_INFO(dev)->gen >= 9))
+		gen8_irq_power_well_post_enable(dev_priv);
+}
+
+static void hsw_set_power_well(struct drm_i915_private *dev_priv,
+			       struct i915_power_well *power_well, bool enable)
+{
+	bool is_enabled, enable_requested;
+	uint32_t tmp;
+
+	tmp = I915_READ(HSW_PWR_WELL_DRIVER);
+	is_enabled = tmp & HSW_PWR_WELL_STATE_ENABLED;
+	enable_requested = tmp & HSW_PWR_WELL_ENABLE_REQUEST;
+
+	if (enable) {
+		if (!enable_requested)
+			I915_WRITE(HSW_PWR_WELL_DRIVER,
+				   HSW_PWR_WELL_ENABLE_REQUEST);
+
+		if (!is_enabled) {
+			DRM_DEBUG_KMS("Enabling power well\n");
+			if (wait_for((I915_READ(HSW_PWR_WELL_DRIVER) &
+				      HSW_PWR_WELL_STATE_ENABLED), 20))
+				DRM_ERROR("Timeout enabling power well\n");
+			hsw_power_well_post_enable(dev_priv);
+		}
+
+	} else {
+		if (enable_requested) {
+			I915_WRITE(HSW_PWR_WELL_DRIVER, 0);
+			POSTING_READ(HSW_PWR_WELL_DRIVER);
+			DRM_DEBUG_KMS("Requesting to disable the power well\n");
+		}
+	}
+}
+
+static void hsw_power_well_sync_hw(struct drm_i915_private *dev_priv,
+				   struct i915_power_well *power_well)
+{
+	hsw_set_power_well(dev_priv, power_well, power_well->count > 0);
+
+	/*
+	 * We're taking over the BIOS, so clear any requests made by it since
+	 * the driver is in charge now.
+	 */
+	if (I915_READ(HSW_PWR_WELL_BIOS) & HSW_PWR_WELL_ENABLE_REQUEST)
+		I915_WRITE(HSW_PWR_WELL_BIOS, 0);
+}
+
+static void hsw_power_well_enable(struct drm_i915_private *dev_priv,
+				  struct i915_power_well *power_well)
+{
+	hsw_set_power_well(dev_priv, power_well, true);
+}
+
+static void hsw_power_well_disable(struct drm_i915_private *dev_priv,
+				   struct i915_power_well *power_well)
+{
+	hsw_set_power_well(dev_priv, power_well, false);
+}
+
+static void i9xx_always_on_power_well_noop(struct drm_i915_private *dev_priv,
+					   struct i915_power_well *power_well)
+{
+}
+
+static bool i9xx_always_on_power_well_enabled(struct drm_i915_private *dev_priv,
+					     struct i915_power_well *power_well)
+{
+	return true;
+}
+
+static void vlv_set_power_well(struct drm_i915_private *dev_priv,
+			       struct i915_power_well *power_well, bool enable)
+{
+	enum punit_power_well power_well_id = power_well->data;
+	u32 mask;
+	u32 state;
+	u32 ctrl;
+
+	mask = PUNIT_PWRGT_MASK(power_well_id);
+	state = enable ? PUNIT_PWRGT_PWR_ON(power_well_id) :
+			 PUNIT_PWRGT_PWR_GATE(power_well_id);
+
+	mutex_lock(&dev_priv->rps.hw_lock);
+
+#define COND \
+	((vlv_punit_read(dev_priv, PUNIT_REG_PWRGT_STATUS) & mask) == state)
+
+	if (COND)
+		goto out;
+
+	ctrl = vlv_punit_read(dev_priv, PUNIT_REG_PWRGT_CTRL);
+	ctrl &= ~mask;
+	ctrl |= state;
+	vlv_punit_write(dev_priv, PUNIT_REG_PWRGT_CTRL, ctrl);
+
+	if (wait_for(COND, 100))
+		DRM_ERROR("timout setting power well state %08x (%08x)\n",
+			  state,
+			  vlv_punit_read(dev_priv, PUNIT_REG_PWRGT_CTRL));
+
+#undef COND
+
+out:
+	mutex_unlock(&dev_priv->rps.hw_lock);
+}
+
+static void vlv_power_well_sync_hw(struct drm_i915_private *dev_priv,
+				   struct i915_power_well *power_well)
+{
+	vlv_set_power_well(dev_priv, power_well, power_well->count > 0);
+}
+
+static void vlv_power_well_enable(struct drm_i915_private *dev_priv,
+				  struct i915_power_well *power_well)
+{
+	vlv_set_power_well(dev_priv, power_well, true);
+}
+
+static void vlv_power_well_disable(struct drm_i915_private *dev_priv,
+				   struct i915_power_well *power_well)
+{
+	vlv_set_power_well(dev_priv, power_well, false);
+}
+
+static bool vlv_power_well_enabled(struct drm_i915_private *dev_priv,
+				   struct i915_power_well *power_well)
+{
+	int power_well_id = power_well->data;
+	bool enabled = false;
+	u32 mask;
+	u32 state;
+	u32 ctrl;
+
+	mask = PUNIT_PWRGT_MASK(power_well_id);
+	ctrl = PUNIT_PWRGT_PWR_ON(power_well_id);
+
+	mutex_lock(&dev_priv->rps.hw_lock);
+
+	state = vlv_punit_read(dev_priv, PUNIT_REG_PWRGT_STATUS) & mask;
+	/*
+	 * We only ever set the power-on and power-gate states, anything
+	 * else is unexpected.
+	 */
+	WARN_ON(state != PUNIT_PWRGT_PWR_ON(power_well_id) &&
+		state != PUNIT_PWRGT_PWR_GATE(power_well_id));
+	if (state == ctrl)
+		enabled = true;
+
+	/*
+	 * A transient state at this point would mean some unexpected party
+	 * is poking at the power controls too.
+	 */
+	ctrl = vlv_punit_read(dev_priv, PUNIT_REG_PWRGT_CTRL) & mask;
+	WARN_ON(ctrl != state);
+
+	mutex_unlock(&dev_priv->rps.hw_lock);
+
+	return enabled;
+}
+
+static void vlv_display_power_well_enable(struct drm_i915_private *dev_priv,
+					  struct i915_power_well *power_well)
+{
+	WARN_ON_ONCE(power_well->data != PUNIT_POWER_WELL_DISP2D);
+
+	vlv_set_power_well(dev_priv, power_well, true);
+
+	spin_lock_irq(&dev_priv->irq_lock);
+	valleyview_enable_display_irqs(dev_priv);
+	spin_unlock_irq(&dev_priv->irq_lock);
+
+	/*
+	 * During driver initialization/resume we can avoid restoring the
+	 * part of the HW/SW state that will be inited anyway explicitly.
+	 */
+	if (dev_priv->power_domains.initializing)
+		return;
+
+	intel_hpd_init(dev_priv);
+
+	i915_redisable_vga_power_on(dev_priv->dev);
+}
+
+static void vlv_display_power_well_disable(struct drm_i915_private *dev_priv,
+					   struct i915_power_well *power_well)
+{
+	WARN_ON_ONCE(power_well->data != PUNIT_POWER_WELL_DISP2D);
+
+	spin_lock_irq(&dev_priv->irq_lock);
+	valleyview_disable_display_irqs(dev_priv);
+	spin_unlock_irq(&dev_priv->irq_lock);
+
+	vlv_set_power_well(dev_priv, power_well, false);
+
+	vlv_power_sequencer_reset(dev_priv);
+}
+
+static void vlv_dpio_cmn_power_well_enable(struct drm_i915_private *dev_priv,
+					   struct i915_power_well *power_well)
+{
+	WARN_ON_ONCE(power_well->data != PUNIT_POWER_WELL_DPIO_CMN_BC);
+
+	/*
+	 * Enable the CRI clock source so we can get at the
+	 * display and the reference clock for VGA
+	 * hotplug / manual detection.
+	 */
+	I915_WRITE(DPLL(PIPE_B), I915_READ(DPLL(PIPE_B)) |
+		   DPLL_REFA_CLK_ENABLE_VLV | DPLL_INTEGRATED_CRI_CLK_VLV);
+	udelay(1); /* >10ns for cmnreset, >0ns for sidereset */
+
+	vlv_set_power_well(dev_priv, power_well, true);
+
+	/*
+	 * From VLV2A0_DP_eDP_DPIO_driver_vbios_notes_10.docx -
+	 *  6.	De-assert cmn_reset/side_reset. Same as VLV X0.
+	 *   a.	GUnit 0x2110 bit[0] set to 1 (def 0)
+	 *   b.	The other bits such as sfr settings / modesel may all
+	 *	be set to 0.
+	 *
+	 * This should only be done on init and resume from S3 with
+	 * both PLLs disabled, or we risk losing DPIO and PLL
+	 * synchronization.
+	 */
+	I915_WRITE(DPIO_CTL, I915_READ(DPIO_CTL) | DPIO_CMNRST);
+}
+
+static void vlv_dpio_cmn_power_well_disable(struct drm_i915_private *dev_priv,
+					    struct i915_power_well *power_well)
+{
+	enum pipe pipe;
+
+	WARN_ON_ONCE(power_well->data != PUNIT_POWER_WELL_DPIO_CMN_BC);
+
+	for_each_pipe(dev_priv, pipe)
+		assert_pll_disabled(dev_priv, pipe);
+
+	/* Assert common reset */
+	I915_WRITE(DPIO_CTL, I915_READ(DPIO_CTL) & ~DPIO_CMNRST);
+
+	vlv_set_power_well(dev_priv, power_well, false);
+}
+
+static void chv_dpio_cmn_power_well_enable(struct drm_i915_private *dev_priv,
+					   struct i915_power_well *power_well)
+{
+	enum dpio_phy phy;
+
+	WARN_ON_ONCE(power_well->data != PUNIT_POWER_WELL_DPIO_CMN_BC &&
+		     power_well->data != PUNIT_POWER_WELL_DPIO_CMN_D);
+
+	/*
+	 * Enable the CRI clock source so we can get at the
+	 * display and the reference clock for VGA
+	 * hotplug / manual detection.
+	 */
+	if (power_well->data == PUNIT_POWER_WELL_DPIO_CMN_BC) {
+		phy = DPIO_PHY0;
+		I915_WRITE(DPLL(PIPE_B), I915_READ(DPLL(PIPE_B)) |
+			   DPLL_REFA_CLK_ENABLE_VLV);
+		I915_WRITE(DPLL(PIPE_B), I915_READ(DPLL(PIPE_B)) |
+			   DPLL_REFA_CLK_ENABLE_VLV | DPLL_INTEGRATED_CRI_CLK_VLV);
+	} else {
+		phy = DPIO_PHY1;
+		I915_WRITE(DPLL(PIPE_C), I915_READ(DPLL(PIPE_C)) |
+			   DPLL_REFA_CLK_ENABLE_VLV | DPLL_INTEGRATED_CRI_CLK_VLV);
+	}
+	udelay(1); /* >10ns for cmnreset, >0ns for sidereset */
+	vlv_set_power_well(dev_priv, power_well, true);
+
+	/* Poll for phypwrgood signal */
+	if (wait_for(I915_READ(DISPLAY_PHY_STATUS) & PHY_POWERGOOD(phy), 1))
+		DRM_ERROR("Display PHY %d is not power up\n", phy);
+
+	I915_WRITE(DISPLAY_PHY_CONTROL, I915_READ(DISPLAY_PHY_CONTROL) |
+		   PHY_COM_LANE_RESET_DEASSERT(phy));
+}
+
+static void chv_dpio_cmn_power_well_disable(struct drm_i915_private *dev_priv,
+					    struct i915_power_well *power_well)
+{
+	enum dpio_phy phy;
+
+	WARN_ON_ONCE(power_well->data != PUNIT_POWER_WELL_DPIO_CMN_BC &&
+		     power_well->data != PUNIT_POWER_WELL_DPIO_CMN_D);
+
+	if (power_well->data == PUNIT_POWER_WELL_DPIO_CMN_BC) {
+		phy = DPIO_PHY0;
+		assert_pll_disabled(dev_priv, PIPE_A);
+		assert_pll_disabled(dev_priv, PIPE_B);
+	} else {
+		phy = DPIO_PHY1;
+		assert_pll_disabled(dev_priv, PIPE_C);
+	}
+
+	I915_WRITE(DISPLAY_PHY_CONTROL, I915_READ(DISPLAY_PHY_CONTROL) &
+		   ~PHY_COM_LANE_RESET_DEASSERT(phy));
+
+	vlv_set_power_well(dev_priv, power_well, false);
+}
+
+static bool chv_pipe_power_well_enabled(struct drm_i915_private *dev_priv,
+					struct i915_power_well *power_well)
+{
+	enum pipe pipe = power_well->data;
+	bool enabled;
+	u32 state, ctrl;
+
+	mutex_lock(&dev_priv->rps.hw_lock);
+
+	state = vlv_punit_read(dev_priv, PUNIT_REG_DSPFREQ) & DP_SSS_MASK(pipe);
+	/*
+	 * We only ever set the power-on and power-gate states, anything
+	 * else is unexpected.
+	 */
+	WARN_ON(state != DP_SSS_PWR_ON(pipe) && state != DP_SSS_PWR_GATE(pipe));
+	enabled = state == DP_SSS_PWR_ON(pipe);
+
+	/*
+	 * A transient state at this point would mean some unexpected party
+	 * is poking at the power controls too.
+	 */
+	ctrl = vlv_punit_read(dev_priv, PUNIT_REG_DSPFREQ) & DP_SSC_MASK(pipe);
+	WARN_ON(ctrl << 16 != state);
+
+	mutex_unlock(&dev_priv->rps.hw_lock);
+
+	return enabled;
+}
+
+static void chv_set_pipe_power_well(struct drm_i915_private *dev_priv,
+				    struct i915_power_well *power_well,
+				    bool enable)
+{
+	enum pipe pipe = power_well->data;
+	u32 state;
+	u32 ctrl;
+
+	state = enable ? DP_SSS_PWR_ON(pipe) : DP_SSS_PWR_GATE(pipe);
+
+	mutex_lock(&dev_priv->rps.hw_lock);
+
+#define COND \
+	((vlv_punit_read(dev_priv, PUNIT_REG_DSPFREQ) & DP_SSS_MASK(pipe)) == state)
+
+	if (COND)
+		goto out;
+
+	ctrl = vlv_punit_read(dev_priv, PUNIT_REG_DSPFREQ);
+	ctrl &= ~DP_SSC_MASK(pipe);
+	ctrl |= enable ? DP_SSC_PWR_ON(pipe) : DP_SSC_PWR_GATE(pipe);
+	vlv_punit_write(dev_priv, PUNIT_REG_DSPFREQ, ctrl);
+
+	if (wait_for(COND, 100))
+		DRM_ERROR("timout setting power well state %08x (%08x)\n",
+			  state,
+			  vlv_punit_read(dev_priv, PUNIT_REG_DSPFREQ));
+
+#undef COND
+
+out:
+	mutex_unlock(&dev_priv->rps.hw_lock);
+}
+
+static void chv_pipe_power_well_sync_hw(struct drm_i915_private *dev_priv,
+					struct i915_power_well *power_well)
+{
+	chv_set_pipe_power_well(dev_priv, power_well, power_well->count > 0);
+}
+
+static void chv_pipe_power_well_enable(struct drm_i915_private *dev_priv,
+				       struct i915_power_well *power_well)
+{
+	WARN_ON_ONCE(power_well->data != PIPE_A &&
+		     power_well->data != PIPE_B &&
+		     power_well->data != PIPE_C);
+
+	chv_set_pipe_power_well(dev_priv, power_well, true);
+
+	if (power_well->data == PIPE_A) {
+		spin_lock_irq(&dev_priv->irq_lock);
+		valleyview_enable_display_irqs(dev_priv);
+		spin_unlock_irq(&dev_priv->irq_lock);
+
+		/*
+		 * During driver initialization/resume we can avoid restoring the
+		 * part of the HW/SW state that will be inited anyway explicitly.
+		 */
+		if (dev_priv->power_domains.initializing)
+			return;
+
+		intel_hpd_init(dev_priv);
+
+		i915_redisable_vga_power_on(dev_priv->dev);
+	}
+}
+
+static void chv_pipe_power_well_disable(struct drm_i915_private *dev_priv,
+					struct i915_power_well *power_well)
+{
+	WARN_ON_ONCE(power_well->data != PIPE_A &&
+		     power_well->data != PIPE_B &&
+		     power_well->data != PIPE_C);
+
+	if (power_well->data == PIPE_A) {
+		spin_lock_irq(&dev_priv->irq_lock);
+		valleyview_disable_display_irqs(dev_priv);
+		spin_unlock_irq(&dev_priv->irq_lock);
+	}
+
+	chv_set_pipe_power_well(dev_priv, power_well, false);
+
+	if (power_well->data == PIPE_A)
+		vlv_power_sequencer_reset(dev_priv);
+}
+
+static void check_power_well_state(struct drm_i915_private *dev_priv,
+				   struct i915_power_well *power_well)
+{
+	bool enabled = power_well->ops->is_enabled(dev_priv, power_well);
+
+	if (power_well->always_on || !i915_module.disable_power_well) {
+		if (!enabled)
+			goto mismatch;
+
+		return;
+	}
+
+	if (enabled != (power_well->count > 0))
+		goto mismatch;
+
+	return;
+
+mismatch:
+	WARN(1, "state mismatch for '%s' (always_on %d hw state %d use-count %d disable_power_well %d\n",
+		  power_well->name, power_well->always_on, enabled,
+		  power_well->count, i915_module.disable_power_well);
+}
+
+/**
+ * intel_display_power_get - grab a power domain reference
+ * @dev_priv: i915 device instance
+ * @domain: power domain to reference
+ *
+ * This function grabs a power domain reference for @domain and ensures that the
+ * power domain and all its parents are powered up. Therefore users should only
+ * grab a reference to the innermost power domain they need.
+ *
+ * Any power domain reference obtained by this function must have a symmetric
+ * call to intel_display_power_put() to release the reference again.
+ */
+void intel_display_power_get(struct drm_i915_private *dev_priv,
+			     enum intel_display_power_domain domain)
+{
+	struct i915_power_domains *power_domains;
+	struct i915_power_well *power_well;
+	int i;
+
+	intel_runtime_pm_get(dev_priv);
+
+	power_domains = &dev_priv->power_domains;
+
+	mutex_lock(&power_domains->lock);
+
+	for_each_power_well(i, power_well, BIT(domain), power_domains) {
+		if (!power_well->count++) {
+			DRM_DEBUG_KMS("enabling %s\n", power_well->name);
+			power_well->ops->enable(dev_priv, power_well);
+			power_well->hw_enabled = true;
+		}
+
+		check_power_well_state(dev_priv, power_well);
+	}
+
+	power_domains->domain_use_count[domain]++;
+
+	mutex_unlock(&power_domains->lock);
+}
+
+/**
+ * intel_display_power_put - release a power domain reference
+ * @dev_priv: i915 device instance
+ * @domain: power domain to reference
+ *
+ * This function drops the power domain reference obtained by
+ * intel_display_power_get() and might power down the corresponding hardware
+ * block right away if this is the last reference.
+ */
+void intel_display_power_put(struct drm_i915_private *dev_priv,
+			     enum intel_display_power_domain domain)
+{
+	struct i915_power_domains *power_domains;
+	struct i915_power_well *power_well;
+	int i;
+
+	power_domains = &dev_priv->power_domains;
+
+	mutex_lock(&power_domains->lock);
+
+	WARN_ON(!power_domains->domain_use_count[domain]);
+	power_domains->domain_use_count[domain]--;
+
+	for_each_power_well_rev(i, power_well, BIT(domain), power_domains) {
+		WARN_ON(!power_well->count);
+
+		if (!--power_well->count && i915_module.disable_power_well) {
+			DRM_DEBUG_KMS("disabling %s\n", power_well->name);
+			power_well->hw_enabled = false;
+			power_well->ops->disable(dev_priv, power_well);
+		}
+
+		check_power_well_state(dev_priv, power_well);
+	}
+
+	mutex_unlock(&power_domains->lock);
+
+	intel_runtime_pm_put(dev_priv);
+}
+
+#define POWER_DOMAIN_MASK (BIT(POWER_DOMAIN_NUM) - 1)
+
+#define HSW_ALWAYS_ON_POWER_DOMAINS (			\
+	BIT(POWER_DOMAIN_PIPE_A) |			\
+	BIT(POWER_DOMAIN_TRANSCODER_EDP) |		\
+	BIT(POWER_DOMAIN_PORT_DDI_A_2_LANES) |		\
+	BIT(POWER_DOMAIN_PORT_DDI_A_4_LANES) |		\
+	BIT(POWER_DOMAIN_PORT_DDI_B_2_LANES) |		\
+	BIT(POWER_DOMAIN_PORT_DDI_B_4_LANES) |		\
+	BIT(POWER_DOMAIN_PORT_DDI_C_2_LANES) |		\
+	BIT(POWER_DOMAIN_PORT_DDI_C_4_LANES) |		\
+	BIT(POWER_DOMAIN_PORT_DDI_D_2_LANES) |		\
+	BIT(POWER_DOMAIN_PORT_DDI_D_4_LANES) |		\
+	BIT(POWER_DOMAIN_PORT_CRT) |			\
+	BIT(POWER_DOMAIN_PLLS) |			\
+	BIT(POWER_DOMAIN_INIT))
+#define HSW_DISPLAY_POWER_DOMAINS (				\
+	(POWER_DOMAIN_MASK & ~HSW_ALWAYS_ON_POWER_DOMAINS) |	\
+	BIT(POWER_DOMAIN_INIT))
+
+#define BDW_ALWAYS_ON_POWER_DOMAINS (			\
+	HSW_ALWAYS_ON_POWER_DOMAINS |			\
+	BIT(POWER_DOMAIN_PIPE_A_PANEL_FITTER))
+#define BDW_DISPLAY_POWER_DOMAINS (				\
+	(POWER_DOMAIN_MASK & ~BDW_ALWAYS_ON_POWER_DOMAINS) |	\
+	BIT(POWER_DOMAIN_INIT))
+
+#define VLV_ALWAYS_ON_POWER_DOMAINS	BIT(POWER_DOMAIN_INIT)
+#define VLV_DISPLAY_POWER_DOMAINS	POWER_DOMAIN_MASK
+
+#define VLV_DPIO_CMN_BC_POWER_DOMAINS (		\
+	BIT(POWER_DOMAIN_PORT_DDI_B_2_LANES) |	\
+	BIT(POWER_DOMAIN_PORT_DDI_B_4_LANES) |	\
+	BIT(POWER_DOMAIN_PORT_DDI_C_2_LANES) |	\
+	BIT(POWER_DOMAIN_PORT_DDI_C_4_LANES) |	\
+	BIT(POWER_DOMAIN_PORT_CRT) |		\
+	BIT(POWER_DOMAIN_INIT))
+
+#define VLV_DPIO_TX_B_LANES_01_POWER_DOMAINS (	\
+	BIT(POWER_DOMAIN_PORT_DDI_B_2_LANES) |	\
+	BIT(POWER_DOMAIN_PORT_DDI_B_4_LANES) |	\
+	BIT(POWER_DOMAIN_INIT))
+
+#define VLV_DPIO_TX_B_LANES_23_POWER_DOMAINS (	\
+	BIT(POWER_DOMAIN_PORT_DDI_B_4_LANES) |	\
+	BIT(POWER_DOMAIN_INIT))
+
+#define VLV_DPIO_TX_C_LANES_01_POWER_DOMAINS (	\
+	BIT(POWER_DOMAIN_PORT_DDI_C_2_LANES) |	\
+	BIT(POWER_DOMAIN_PORT_DDI_C_4_LANES) |	\
+	BIT(POWER_DOMAIN_INIT))
+
+#define VLV_DPIO_TX_C_LANES_23_POWER_DOMAINS (	\
+	BIT(POWER_DOMAIN_PORT_DDI_C_4_LANES) |	\
+	BIT(POWER_DOMAIN_INIT))
+
+#define CHV_PIPE_A_POWER_DOMAINS (	\
+	BIT(POWER_DOMAIN_PIPE_A) |	\
+	BIT(POWER_DOMAIN_INIT))
+
+#define CHV_PIPE_B_POWER_DOMAINS (	\
+	BIT(POWER_DOMAIN_PIPE_B) |	\
+	BIT(POWER_DOMAIN_INIT))
+
+#define CHV_PIPE_C_POWER_DOMAINS (	\
+	BIT(POWER_DOMAIN_PIPE_C) |	\
+	BIT(POWER_DOMAIN_INIT))
+
+#define CHV_DPIO_CMN_BC_POWER_DOMAINS (		\
+	BIT(POWER_DOMAIN_PORT_DDI_B_2_LANES) |	\
+	BIT(POWER_DOMAIN_PORT_DDI_B_4_LANES) |	\
+	BIT(POWER_DOMAIN_PORT_DDI_C_2_LANES) |	\
+	BIT(POWER_DOMAIN_PORT_DDI_C_4_LANES) |	\
+	BIT(POWER_DOMAIN_INIT))
+
+#define CHV_DPIO_CMN_D_POWER_DOMAINS (		\
+	BIT(POWER_DOMAIN_PORT_DDI_D_2_LANES) |	\
+	BIT(POWER_DOMAIN_PORT_DDI_D_4_LANES) |	\
+	BIT(POWER_DOMAIN_INIT))
+
+#define CHV_DPIO_TX_D_LANES_01_POWER_DOMAINS (	\
+	BIT(POWER_DOMAIN_PORT_DDI_D_2_LANES) |	\
+	BIT(POWER_DOMAIN_PORT_DDI_D_4_LANES) |	\
+	BIT(POWER_DOMAIN_INIT))
+
+#define CHV_DPIO_TX_D_LANES_23_POWER_DOMAINS (	\
+	BIT(POWER_DOMAIN_PORT_DDI_D_4_LANES) |	\
+	BIT(POWER_DOMAIN_INIT))
+
+static const struct i915_power_well_ops i9xx_always_on_power_well_ops = {
+	.sync_hw = i9xx_always_on_power_well_noop,
+	.enable = i9xx_always_on_power_well_noop,
+	.disable = i9xx_always_on_power_well_noop,
+	.is_enabled = i9xx_always_on_power_well_enabled,
+};
+
+static const struct i915_power_well_ops chv_pipe_power_well_ops = {
+	.sync_hw = chv_pipe_power_well_sync_hw,
+	.enable = chv_pipe_power_well_enable,
+	.disable = chv_pipe_power_well_disable,
+	.is_enabled = chv_pipe_power_well_enabled,
+};
+
+static const struct i915_power_well_ops chv_dpio_cmn_power_well_ops = {
+	.sync_hw = vlv_power_well_sync_hw,
+	.enable = chv_dpio_cmn_power_well_enable,
+	.disable = chv_dpio_cmn_power_well_disable,
+	.is_enabled = vlv_power_well_enabled,
+};
+
+static struct i915_power_well i9xx_always_on_power_well[] = {
+	{
+		.name = "always-on",
+		.always_on = 1,
+		.domains = POWER_DOMAIN_MASK,
+		.ops = &i9xx_always_on_power_well_ops,
+	},
+};
+
+static const struct i915_power_well_ops hsw_power_well_ops = {
+	.sync_hw = hsw_power_well_sync_hw,
+	.enable = hsw_power_well_enable,
+	.disable = hsw_power_well_disable,
+	.is_enabled = hsw_power_well_enabled,
+};
+
+static struct i915_power_well hsw_power_wells[] = {
+	{
+		.name = "always-on",
+		.always_on = 1,
+		.domains = HSW_ALWAYS_ON_POWER_DOMAINS,
+		.ops = &i9xx_always_on_power_well_ops,
+	},
+	{
+		.name = "display",
+		.domains = HSW_DISPLAY_POWER_DOMAINS,
+		.ops = &hsw_power_well_ops,
+	},
+};
+
+static struct i915_power_well bdw_power_wells[] = {
+	{
+		.name = "always-on",
+		.always_on = 1,
+		.domains = BDW_ALWAYS_ON_POWER_DOMAINS,
+		.ops = &i9xx_always_on_power_well_ops,
+	},
+	{
+		.name = "display",
+		.domains = BDW_DISPLAY_POWER_DOMAINS,
+		.ops = &hsw_power_well_ops,
+	},
+};
+
+static const struct i915_power_well_ops vlv_display_power_well_ops = {
+	.sync_hw = vlv_power_well_sync_hw,
+	.enable = vlv_display_power_well_enable,
+	.disable = vlv_display_power_well_disable,
+	.is_enabled = vlv_power_well_enabled,
+};
+
+static const struct i915_power_well_ops vlv_dpio_cmn_power_well_ops = {
+	.sync_hw = vlv_power_well_sync_hw,
+	.enable = vlv_dpio_cmn_power_well_enable,
+	.disable = vlv_dpio_cmn_power_well_disable,
+	.is_enabled = vlv_power_well_enabled,
+};
+
+static const struct i915_power_well_ops vlv_dpio_power_well_ops = {
+	.sync_hw = vlv_power_well_sync_hw,
+	.enable = vlv_power_well_enable,
+	.disable = vlv_power_well_disable,
+	.is_enabled = vlv_power_well_enabled,
+};
+
+static struct i915_power_well vlv_power_wells[] = {
+	{
+		.name = "always-on",
+		.always_on = 1,
+		.domains = VLV_ALWAYS_ON_POWER_DOMAINS,
+		.ops = &i9xx_always_on_power_well_ops,
+	},
+	{
+		.name = "display",
+		.domains = VLV_DISPLAY_POWER_DOMAINS,
+		.data = PUNIT_POWER_WELL_DISP2D,
+		.ops = &vlv_display_power_well_ops,
+	},
+	{
+		.name = "dpio-tx-b-01",
+		.domains = VLV_DPIO_TX_B_LANES_01_POWER_DOMAINS |
+			   VLV_DPIO_TX_B_LANES_23_POWER_DOMAINS |
+			   VLV_DPIO_TX_C_LANES_01_POWER_DOMAINS |
+			   VLV_DPIO_TX_C_LANES_23_POWER_DOMAINS,
+		.ops = &vlv_dpio_power_well_ops,
+		.data = PUNIT_POWER_WELL_DPIO_TX_B_LANES_01,
+	},
+	{
+		.name = "dpio-tx-b-23",
+		.domains = VLV_DPIO_TX_B_LANES_01_POWER_DOMAINS |
+			   VLV_DPIO_TX_B_LANES_23_POWER_DOMAINS |
+			   VLV_DPIO_TX_C_LANES_01_POWER_DOMAINS |
+			   VLV_DPIO_TX_C_LANES_23_POWER_DOMAINS,
+		.ops = &vlv_dpio_power_well_ops,
+		.data = PUNIT_POWER_WELL_DPIO_TX_B_LANES_23,
+	},
+	{
+		.name = "dpio-tx-c-01",
+		.domains = VLV_DPIO_TX_B_LANES_01_POWER_DOMAINS |
+			   VLV_DPIO_TX_B_LANES_23_POWER_DOMAINS |
+			   VLV_DPIO_TX_C_LANES_01_POWER_DOMAINS |
+			   VLV_DPIO_TX_C_LANES_23_POWER_DOMAINS,
+		.ops = &vlv_dpio_power_well_ops,
+		.data = PUNIT_POWER_WELL_DPIO_TX_C_LANES_01,
+	},
+	{
+		.name = "dpio-tx-c-23",
+		.domains = VLV_DPIO_TX_B_LANES_01_POWER_DOMAINS |
+			   VLV_DPIO_TX_B_LANES_23_POWER_DOMAINS |
+			   VLV_DPIO_TX_C_LANES_01_POWER_DOMAINS |
+			   VLV_DPIO_TX_C_LANES_23_POWER_DOMAINS,
+		.ops = &vlv_dpio_power_well_ops,
+		.data = PUNIT_POWER_WELL_DPIO_TX_C_LANES_23,
+	},
+	{
+		.name = "dpio-common",
+		.domains = VLV_DPIO_CMN_BC_POWER_DOMAINS,
+		.data = PUNIT_POWER_WELL_DPIO_CMN_BC,
+		.ops = &vlv_dpio_cmn_power_well_ops,
+	},
+};
+
+static struct i915_power_well chv_power_wells[] = {
+	{
+		.name = "always-on",
+		.always_on = 1,
+		.domains = VLV_ALWAYS_ON_POWER_DOMAINS,
+		.ops = &i9xx_always_on_power_well_ops,
+	},
+#if 0
+	{
+		.name = "display",
+		.domains = VLV_DISPLAY_POWER_DOMAINS,
+		.data = PUNIT_POWER_WELL_DISP2D,
+		.ops = &vlv_display_power_well_ops,
+	},
+#endif
+	{
+		.name = "pipe-a",
+		/*
+		 * FIXME: pipe A power well seems to be the new disp2d well.
+		 * At least all registers seem to be housed there. Figure
+		 * out if this a a temporary situation in pre-production
+		 * hardware or a permanent state of affairs.
+		 */
+		.domains = CHV_PIPE_A_POWER_DOMAINS | VLV_DISPLAY_POWER_DOMAINS,
+		.data = PIPE_A,
+		.ops = &chv_pipe_power_well_ops,
+	},
+#if 0
+	{
+		.name = "pipe-b",
+		.domains = CHV_PIPE_B_POWER_DOMAINS,
+		.data = PIPE_B,
+		.ops = &chv_pipe_power_well_ops,
+	},
+	{
+		.name = "pipe-c",
+		.domains = CHV_PIPE_C_POWER_DOMAINS,
+		.data = PIPE_C,
+		.ops = &chv_pipe_power_well_ops,
+	},
+#endif
+	{
+		.name = "dpio-common-bc",
+		/*
+		 * XXX: cmnreset for one PHY seems to disturb the other.
+		 * As a workaround keep both powered on at the same
+		 * time for now.
+		 */
+		.domains = CHV_DPIO_CMN_BC_POWER_DOMAINS | CHV_DPIO_CMN_D_POWER_DOMAINS,
+		.data = PUNIT_POWER_WELL_DPIO_CMN_BC,
+		.ops = &chv_dpio_cmn_power_well_ops,
+	},
+	{
+		.name = "dpio-common-d",
+		/*
+		 * XXX: cmnreset for one PHY seems to disturb the other.
+		 * As a workaround keep both powered on at the same
+		 * time for now.
+		 */
+		.domains = CHV_DPIO_CMN_BC_POWER_DOMAINS | CHV_DPIO_CMN_D_POWER_DOMAINS,
+		.data = PUNIT_POWER_WELL_DPIO_CMN_D,
+		.ops = &chv_dpio_cmn_power_well_ops,
+	},
+#if 0
+	{
+		.name = "dpio-tx-b-01",
+		.domains = VLV_DPIO_TX_B_LANES_01_POWER_DOMAINS |
+			   VLV_DPIO_TX_B_LANES_23_POWER_DOMAINS,
+		.ops = &vlv_dpio_power_well_ops,
+		.data = PUNIT_POWER_WELL_DPIO_TX_B_LANES_01,
+	},
+	{
+		.name = "dpio-tx-b-23",
+		.domains = VLV_DPIO_TX_B_LANES_01_POWER_DOMAINS |
+			   VLV_DPIO_TX_B_LANES_23_POWER_DOMAINS,
+		.ops = &vlv_dpio_power_well_ops,
+		.data = PUNIT_POWER_WELL_DPIO_TX_B_LANES_23,
+	},
+	{
+		.name = "dpio-tx-c-01",
+		.domains = VLV_DPIO_TX_C_LANES_01_POWER_DOMAINS |
+			   VLV_DPIO_TX_C_LANES_23_POWER_DOMAINS,
+		.ops = &vlv_dpio_power_well_ops,
+		.data = PUNIT_POWER_WELL_DPIO_TX_C_LANES_01,
+	},
+	{
+		.name = "dpio-tx-c-23",
+		.domains = VLV_DPIO_TX_C_LANES_01_POWER_DOMAINS |
+			   VLV_DPIO_TX_C_LANES_23_POWER_DOMAINS,
+		.ops = &vlv_dpio_power_well_ops,
+		.data = PUNIT_POWER_WELL_DPIO_TX_C_LANES_23,
+	},
+	{
+		.name = "dpio-tx-d-01",
+		.domains = CHV_DPIO_TX_D_LANES_01_POWER_DOMAINS |
+			   CHV_DPIO_TX_D_LANES_23_POWER_DOMAINS,
+		.ops = &vlv_dpio_power_well_ops,
+		.data = PUNIT_POWER_WELL_DPIO_TX_D_LANES_01,
+	},
+	{
+		.name = "dpio-tx-d-23",
+		.domains = CHV_DPIO_TX_D_LANES_01_POWER_DOMAINS |
+			   CHV_DPIO_TX_D_LANES_23_POWER_DOMAINS,
+		.ops = &vlv_dpio_power_well_ops,
+		.data = PUNIT_POWER_WELL_DPIO_TX_D_LANES_23,
+	},
+#endif
+};
+
+static struct i915_power_well *lookup_power_well(struct drm_i915_private *dev_priv,
+						 enum punit_power_well power_well_id)
+{
+	struct i915_power_domains *power_domains = &dev_priv->power_domains;
+	struct i915_power_well *power_well;
+	int i;
+
+	for_each_power_well(i, power_well, POWER_DOMAIN_MASK, power_domains) {
+		if (power_well->data == power_well_id)
+			return power_well;
+	}
+
+	return NULL;
+}
+
+#define set_power_wells(power_domains, __power_wells) ({		\
+	(power_domains)->power_wells = (__power_wells);			\
+	(power_domains)->power_well_count = ARRAY_SIZE(__power_wells);	\
+})
+
+/**
+ * intel_power_domains_init - initializes the power domain structures
+ * @dev_priv: i915 device instance
+ *
+ * Initializes the power domain structures for @dev_priv depending upon the
+ * supported platform.
+ */
+int intel_power_domains_init(struct drm_i915_private *dev_priv)
+{
+	struct i915_power_domains *power_domains = &dev_priv->power_domains;
+
+	mutex_init(&power_domains->lock);
+
+	/*
+	 * The enabling order will be from lower to higher indexed wells,
+	 * the disabling order is reversed.
+	 */
+	if (IS_HASWELL(dev_priv->dev)) {
+		set_power_wells(power_domains, hsw_power_wells);
+		hsw_pwr = power_domains;
+	} else if (IS_BROADWELL(dev_priv->dev)) {
+		set_power_wells(power_domains, bdw_power_wells);
+		hsw_pwr = power_domains;
+	} else if (IS_CHERRYVIEW(dev_priv->dev)) {
+		set_power_wells(power_domains, chv_power_wells);
+	} else if (IS_VALLEYVIEW(dev_priv->dev)) {
+		set_power_wells(power_domains, vlv_power_wells);
+	} else {
+		set_power_wells(power_domains, i9xx_always_on_power_well);
+	}
+
+	return 0;
+}
+
+static void intel_runtime_pm_disable(struct drm_i915_private *dev_priv)
+{
+	struct drm_device *dev = dev_priv->dev;
+	struct device *device = &dev->pdev->dev;
+
+	if (!HAS_RUNTIME_PM(dev))
+		return;
+
+	if (!intel_enable_rc6(dev))
+		return;
+
+	/* Make sure we're not suspended first. */
+	pm_runtime_get_sync(device);
+	pm_runtime_disable(device);
+}
+
+/**
+ * intel_power_domains_fini - finalizes the power domain structures
+ * @dev_priv: i915 device instance
+ *
+ * Finalizes the power domain structures for @dev_priv depending upon the
+ * supported platform. This function also disables runtime pm and ensures that
+ * the device stays powered up so that the driver can be reloaded.
+ */
+void intel_power_domains_fini(struct drm_i915_private *dev_priv)
+{
+	intel_runtime_pm_disable(dev_priv);
+
+	/* The i915.ko module is still not prepared to be loaded when
+	 * the power well is not enabled, so just enable it in case
+	 * we're going to unload/reload. */
+	intel_display_set_init_power(dev_priv, true);
+
+	hsw_pwr = NULL;
+}
+
+static void intel_power_domains_resume(struct drm_i915_private *dev_priv)
+{
+	struct i915_power_domains *power_domains = &dev_priv->power_domains;
+	struct i915_power_well *power_well;
+	int i;
+
+	mutex_lock(&power_domains->lock);
+	for_each_power_well(i, power_well, POWER_DOMAIN_MASK, power_domains) {
+		power_well->ops->sync_hw(dev_priv, power_well);
+		power_well->hw_enabled = power_well->ops->is_enabled(dev_priv,
+								     power_well);
+	}
+	mutex_unlock(&power_domains->lock);
+}
+
+static void vlv_cmnlane_wa(struct drm_i915_private *dev_priv)
+{
+	struct i915_power_well *cmn =
+		lookup_power_well(dev_priv, PUNIT_POWER_WELL_DPIO_CMN_BC);
+	struct i915_power_well *disp2d =
+		lookup_power_well(dev_priv, PUNIT_POWER_WELL_DISP2D);
+
+	/* If the display might be already active skip this */
+	if (cmn->ops->is_enabled(dev_priv, cmn) &&
+	    disp2d->ops->is_enabled(dev_priv, disp2d) &&
+	    I915_READ(DPIO_CTL) & DPIO_CMNRST)
+		return;
+
+	DRM_DEBUG_KMS("toggling display PHY side reset\n");
+
+	/* cmnlane needs DPLL registers */
+	disp2d->ops->enable(dev_priv, disp2d);
+
+	/*
+	 * From VLV2A0_DP_eDP_HDMI_DPIO_driver_vbios_notes_11.docx:
+	 * Need to assert and de-assert PHY SB reset by gating the
+	 * common lane power, then un-gating it.
+	 * Simply ungating isn't enough to reset the PHY enough to get
+	 * ports and lanes running.
+	 */
+	cmn->ops->disable(dev_priv, cmn);
+}
+
+/**
+ * intel_power_domains_init_hw - initialize hardware power domain state
+ * @dev_priv: i915 device instance
+ *
+ * This function initializes the hardware power domain state and enables all
+ * power domains using intel_display_set_init_power().
+ */
+void intel_power_domains_init_hw(struct drm_i915_private *dev_priv)
+{
+	struct drm_device *dev = dev_priv->dev;
+	struct i915_power_domains *power_domains = &dev_priv->power_domains;
+
+	power_domains->initializing = true;
+
+	if (IS_VALLEYVIEW(dev) && !IS_CHERRYVIEW(dev)) {
+		mutex_lock(&power_domains->lock);
+		vlv_cmnlane_wa(dev_priv);
+		mutex_unlock(&power_domains->lock);
+	}
+
+	/* For now, we need the power well to be always enabled. */
+	intel_display_set_init_power(dev_priv, true);
+	intel_power_domains_resume(dev_priv);
+	power_domains->initializing = false;
+}
+
+/**
+ * intel_aux_display_runtime_get - grab an auxilliary power domain reference
+ * @dev_priv: i915 device instance
+ *
+ * This function grabs a power domain reference for the auxiliary power domain
+ * (for access to the GMBUS and DP AUX blocks) and ensures that it and all its
+ * parents are powered up. Therefore users should only grab a reference to the
+ * innermost power domain they need.
+ *
+ * Any power domain reference obtained by this function must have a symmetric
+ * call to intel_aux_display_runtime_put() to release the reference again.
+ */
+void intel_aux_display_runtime_get(struct drm_i915_private *dev_priv)
+{
+	intel_runtime_pm_get(dev_priv);
+}
+
+/**
+ * intel_aux_display_runtime_put - release an auxilliary power domain reference
+ * @dev_priv: i915 device instance
+ *
+ * This function drops the auxilliary power domain reference obtained by
+ * intel_aux_display_runtime_get() and might power down the corresponding
+ * hardware block right away if this is the last reference.
+ */
+void intel_aux_display_runtime_put(struct drm_i915_private *dev_priv)
+{
+	intel_runtime_pm_put(dev_priv);
+}
+
+/**
+ * intel_runtime_pm_get - grab a runtime pm reference
+ * @dev_priv: i915 device instance
+ *
+ * This function grabs a device-level runtime pm reference (mostly used for GEM
+ * code to ensure the GTT or GT is on) and ensures that it is powered up.
+ *
+ * Any runtime pm reference obtained by this function must have a symmetric
+ * call to intel_runtime_pm_put() to release the reference again.
+ */
+void intel_runtime_pm_get(struct drm_i915_private *dev_priv)
+{
+	struct drm_device *dev = dev_priv->dev;
+	struct device *device = &dev->pdev->dev;
+
+	if (!HAS_RUNTIME_PM(dev))
+		return;
+
+	pm_runtime_get_sync(device);
+	WARN(dev_priv->pm.suspended, "Device still suspended.\n");
+}
+
+/**
+ * intel_runtime_pm_get_noresume - grab a runtime pm reference
+ * @dev_priv: i915 device instance
+ *
+ * This function grabs a device-level runtime pm reference (mostly used for GEM
+ * code to ensure the GTT or GT is on).
+ *
+ * It will _not_ power up the device but instead only check that it's powered
+ * on.  Therefore it is only valid to call this functions from contexts where
+ * the device is known to be powered up and where trying to power it up would
+ * result in hilarity and deadlocks. That pretty much means only the system
+ * suspend/resume code where this is used to grab runtime pm references for
+ * delayed setup down in work items.
+ *
+ * Any runtime pm reference obtained by this function must have a symmetric
+ * call to intel_runtime_pm_put() to release the reference again.
+ */
+void intel_runtime_pm_get_noresume(struct drm_i915_private *dev_priv)
+{
+	struct drm_device *dev = dev_priv->dev;
+	struct device *device = &dev->pdev->dev;
+
+	if (!HAS_RUNTIME_PM(dev))
+		return;
+
+	WARN(dev_priv->pm.suspended, "Getting nosync-ref while suspended.\n");
+	pm_runtime_get_noresume(device);
+}
+
+/**
+ * intel_runtime_pm_put - release a runtime pm reference
+ * @dev_priv: i915 device instance
+ *
+ * This function drops the device-level runtime pm reference obtained by
+ * intel_runtime_pm_get() and might power down the corresponding
+ * hardware block right away if this is the last reference.
+ */
+void intel_runtime_pm_put(struct drm_i915_private *dev_priv)
+{
+	struct drm_device *dev = dev_priv->dev;
+	struct device *device = &dev->pdev->dev;
+
+	if (!HAS_RUNTIME_PM(dev))
+		return;
+
+	pm_runtime_mark_last_busy(device);
+	pm_runtime_put_autosuspend(device);
+}
+
+/**
+ * intel_runtime_pm_enable - enable runtime pm
+ * @dev_priv: i915 device instance
+ *
+ * This function enables runtime pm at the end of the driver load sequence.
+ *
+ * Note that this function does currently not enable runtime pm for the
+ * subordinate display power domains. That is only done on the first modeset
+ * using intel_display_set_init_power().
+ */
+void intel_runtime_pm_enable(struct drm_i915_private *dev_priv)
+{
+	struct drm_device *dev = dev_priv->dev;
+	struct device *device = &dev->pdev->dev;
+
+	if (!HAS_RUNTIME_PM(dev))
+		return;
+
+	pm_runtime_set_active(device);
+
+	/*
+	 * RPM depends on RC6 to save restore the GT HW context, so make RC6 a
+	 * requirement.
+	 */
+	if (!intel_enable_rc6(dev)) {
+		DRM_INFO("RC6 disabled, disabling runtime PM support\n");
+		return;
+	}
+
+	pm_runtime_set_autosuspend_delay(device, 10000); /* 10s */
+	pm_runtime_mark_last_busy(device);
+	pm_runtime_use_autosuspend(device);
+
+	pm_runtime_put_autosuspend(device);
+}
+
+/* Display audio driver power well request */
+int i915_request_power_well(void)
+{
+	struct drm_i915_private *dev_priv;
+
+	if (!hsw_pwr)
+		return -ENODEV;
+
+	dev_priv = container_of(hsw_pwr, struct drm_i915_private,
+				power_domains);
+	intel_display_power_get(dev_priv, POWER_DOMAIN_AUDIO);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(i915_request_power_well);
+
+/* Display audio driver power well release */
+int i915_release_power_well(void)
+{
+	struct drm_i915_private *dev_priv;
+
+	if (!hsw_pwr)
+		return -ENODEV;
+
+	dev_priv = container_of(hsw_pwr, struct drm_i915_private,
+				power_domains);
+	intel_display_power_put(dev_priv, POWER_DOMAIN_AUDIO);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(i915_release_power_well);
+
+/*
+ * Private interface for the audio driver to get CDCLK in kHz.
+ *
+ * Caller must request power well using i915_request_power_well() prior to
+ * making the call.
+ */
+int i915_get_cdclk_freq(void)
+{
+	struct drm_i915_private *dev_priv;
+
+	if (!hsw_pwr)
+		return -ENODEV;
+
+	dev_priv = container_of(hsw_pwr, struct drm_i915_private,
+				power_domains);
+
+	return intel_ddi_get_cdclk_freq(dev_priv);
+}
+EXPORT_SYMBOL_GPL(i915_get_cdclk_freq);
diff -urN a/drivers/gpu/drm/i915/intel_sdvo.c b/drivers/gpu/drm/i915/intel_sdvo.c
--- a/drivers/gpu/drm/i915/intel_sdvo.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_sdvo.c	2014-11-22 14:37:49.346700417 -0700
@@ -52,6 +52,8 @@
 #define IS_DIGITAL(c) (c->output_flag & (SDVO_TMDS_MASK | SDVO_LVDS_MASK))
 
 
+static void intel_sdvo_get_lvds_modes(struct drm_connector *connector);
+
 static const char *tv_format_names[] = {
 	"NTSC_M"   , "NTSC_J"  , "NTSC_443",
 	"PAL_B"    , "PAL_D"   , "PAL_G"   ,
@@ -773,9 +775,9 @@
 	args.height = height;
 	args.interlace = 0;
 
-	if (intel_sdvo->is_lvds &&
-	   (intel_sdvo->sdvo_lvds_fixed_mode->hdisplay != width ||
-	    intel_sdvo->sdvo_lvds_fixed_mode->vdisplay != height))
+	if (intel_sdvo->sdvo_lvds_fixed_mode &&
+	    (intel_sdvo->sdvo_lvds_fixed_mode->hdisplay != width ||
+	     intel_sdvo->sdvo_lvds_fixed_mode->vdisplay != height))
 		args.scaled = 1;
 
 	return intel_sdvo_set_value(intel_sdvo,
@@ -1211,7 +1213,7 @@
 		return;
 
 	/* lvds has a special fixed output timing. */
-	if (intel_sdvo->is_lvds)
+	if (intel_sdvo->sdvo_lvds_fixed_mode)
 		intel_sdvo_get_dtd_from_mode(&output_dtd,
 					     intel_sdvo->sdvo_lvds_fixed_mode);
 	else
@@ -1564,7 +1566,7 @@
 	if (intel_sdvo->pixel_clock_max < mode->clock)
 		return MODE_CLOCK_HIGH;
 
-	if (intel_sdvo->is_lvds) {
+	if (intel_sdvo->sdvo_lvds_fixed_mode) {
 		if (mode->hdisplay > intel_sdvo->sdvo_lvds_fixed_mode->hdisplay)
 			return MODE_PANEL;
 
@@ -1724,6 +1726,17 @@
 	return status;
 }
 
+static enum drm_connector_status
+intel_sdvo_lvds_detect(struct drm_connector *connector)
+{
+	struct intel_sdvo *intel_sdvo = intel_attached_sdvo(connector);
+
+	intel_sdvo_get_lvds_modes(connector);
+	return (intel_sdvo->sdvo_lvds_fixed_mode ?
+		connector_status_connected :
+		connector_status_disconnected);
+}
+
 static bool
 intel_sdvo_connector_matches_edid(struct intel_sdvo_connector *sdvo,
 				  struct edid *edid)
@@ -1756,11 +1769,13 @@
 		      response & 0xff, response >> 8,
 		      intel_sdvo_connector->output_flag);
 
-	if (response == 0)
-		return connector_status_disconnected;
-
 	intel_sdvo->attached_output = response;
 
+	/* Discard the fixed mode, LVDS will reattach during detect */
+	if (intel_sdvo->sdvo_lvds_fixed_mode != NULL)
+		drm_mode_destroy(connector->dev,
+				 intel_sdvo->sdvo_lvds_fixed_mode);
+
 	intel_sdvo->has_hdmi_monitor = false;
 	intel_sdvo->has_hdmi_audio = false;
 	intel_sdvo->rgb_quant_range_selectable = false;
@@ -1769,6 +1784,8 @@
 		ret = connector_status_disconnected;
 	else if (IS_TMDS(intel_sdvo_connector))
 		ret = intel_sdvo_tmds_sink_detect(connector);
+	else if (IS_LVDS(intel_sdvo_connector))
+		ret = intel_sdvo_lvds_detect(connector);
 	else {
 		struct edid *edid;
 
@@ -1796,7 +1813,7 @@
 		if (response & SDVO_TV_MASK)
 			intel_sdvo->is_tv = true;
 		if (response & SDVO_LVDS_MASK)
-			intel_sdvo->is_lvds = intel_sdvo->sdvo_lvds_fixed_mode != NULL;
+			intel_sdvo->is_lvds = true;
 	}
 
 	return ret;
@@ -1966,13 +1983,13 @@
 	 */
 	intel_ddc_get_modes(connector, &intel_sdvo->ddc);
 
-	list_for_each_entry(newmode, &connector->probed_modes, head) {
-		if (newmode->type & DRM_MODE_TYPE_PREFERRED) {
-			intel_sdvo->sdvo_lvds_fixed_mode =
-				drm_mode_duplicate(connector->dev, newmode);
-
-			intel_sdvo->is_lvds = true;
-			break;
+	if (intel_sdvo->sdvo_lvds_fixed_mode == NULL) {
+		list_for_each_entry(newmode, &connector->probed_modes, head) {
+			if (newmode->type & DRM_MODE_TYPE_PREFERRED) {
+				intel_sdvo->sdvo_lvds_fixed_mode =
+					drm_mode_duplicate(connector->dev, newmode);
+				break;
+			}
 		}
 	}
 }
@@ -1991,57 +2008,10 @@
 	return !list_empty(&connector->probed_modes);
 }
 
-static void
-intel_sdvo_destroy_enhance_property(struct drm_connector *connector)
-{
-	struct intel_sdvo_connector *intel_sdvo_connector = to_intel_sdvo_connector(connector);
-	struct drm_device *dev = connector->dev;
-
-	if (intel_sdvo_connector->left)
-		drm_property_destroy(dev, intel_sdvo_connector->left);
-	if (intel_sdvo_connector->right)
-		drm_property_destroy(dev, intel_sdvo_connector->right);
-	if (intel_sdvo_connector->top)
-		drm_property_destroy(dev, intel_sdvo_connector->top);
-	if (intel_sdvo_connector->bottom)
-		drm_property_destroy(dev, intel_sdvo_connector->bottom);
-	if (intel_sdvo_connector->hpos)
-		drm_property_destroy(dev, intel_sdvo_connector->hpos);
-	if (intel_sdvo_connector->vpos)
-		drm_property_destroy(dev, intel_sdvo_connector->vpos);
-	if (intel_sdvo_connector->saturation)
-		drm_property_destroy(dev, intel_sdvo_connector->saturation);
-	if (intel_sdvo_connector->contrast)
-		drm_property_destroy(dev, intel_sdvo_connector->contrast);
-	if (intel_sdvo_connector->hue)
-		drm_property_destroy(dev, intel_sdvo_connector->hue);
-	if (intel_sdvo_connector->sharpness)
-		drm_property_destroy(dev, intel_sdvo_connector->sharpness);
-	if (intel_sdvo_connector->flicker_filter)
-		drm_property_destroy(dev, intel_sdvo_connector->flicker_filter);
-	if (intel_sdvo_connector->flicker_filter_2d)
-		drm_property_destroy(dev, intel_sdvo_connector->flicker_filter_2d);
-	if (intel_sdvo_connector->flicker_filter_adaptive)
-		drm_property_destroy(dev, intel_sdvo_connector->flicker_filter_adaptive);
-	if (intel_sdvo_connector->tv_luma_filter)
-		drm_property_destroy(dev, intel_sdvo_connector->tv_luma_filter);
-	if (intel_sdvo_connector->tv_chroma_filter)
-		drm_property_destroy(dev, intel_sdvo_connector->tv_chroma_filter);
-	if (intel_sdvo_connector->dot_crawl)
-		drm_property_destroy(dev, intel_sdvo_connector->dot_crawl);
-	if (intel_sdvo_connector->brightness)
-		drm_property_destroy(dev, intel_sdvo_connector->brightness);
-}
-
 static void intel_sdvo_destroy(struct drm_connector *connector)
 {
 	struct intel_sdvo_connector *intel_sdvo_connector = to_intel_sdvo_connector(connector);
 
-	if (intel_sdvo_connector->tv_format)
-		drm_property_destroy(connector->dev,
-				     intel_sdvo_connector->tv_format);
-
-	intel_sdvo_destroy_enhance_property(connector);
 	drm_connector_cleanup(connector);
 	kfree(intel_sdvo_connector);
 }
diff -urN a/drivers/gpu/drm/i915/intel_sprite.c b/drivers/gpu/drm/i915/intel_sprite.c
--- a/drivers/gpu/drm/i915/intel_sprite.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_sprite.c	2014-11-22 14:37:49.346700417 -0700
@@ -37,6 +37,20 @@
 #include <drm/i915_drm.h>
 #include "i915_drv.h"
 
+static bool
+format_is_yuv(uint32_t format)
+{
+	switch (format) {
+	case DRM_FORMAT_YUYV:
+	case DRM_FORMAT_UYVY:
+	case DRM_FORMAT_VYUY:
+	case DRM_FORMAT_YVYU:
+		return true;
+	default:
+		return false;
+	}
+}
+
 static int usecs_to_scanlines(const struct drm_display_mode *mode, int usecs)
 {
 	/* paranoia */
@@ -46,17 +60,32 @@
 	return DIV_ROUND_UP(usecs * mode->crtc_clock, 1000 * mode->crtc_htotal);
 }
 
-static bool intel_pipe_update_start(struct intel_crtc *crtc, uint32_t *start_vbl_count)
+/**
+ * intel_pipe_update_start() - start update of a set of display registers
+ * @crtc: the crtc of which the registers are going to be updated
+ * @start_vbl_count: vblank counter return pointer used for error checking
+ *
+ * Mark the start of an update to pipe registers that should be updated
+ * atomically regarding vblank. If the next vblank will happens within
+ * the next 100 us, this function waits until the vblank passes.
+ *
+ * After a successful call to this function, interrupts will be disabled
+ * until a subsequent call to intel_pipe_update_end(). That is done to
+ * avoid random delays. The value written to @start_vbl_count should be
+ * supplied to intel_pipe_update_end() for error checking.
+ *
+ * Return: true if the call was successful
+ */
+bool intel_pipe_update_start(struct intel_crtc *crtc, uint32_t *start_vbl_count)
 {
 	struct drm_device *dev = crtc->base.dev;
 	const struct drm_display_mode *mode = &crtc->config.adjusted_mode;
 	enum pipe pipe = crtc->pipe;
 	long timeout = msecs_to_jiffies_timeout(1);
 	int scanline, min, max, vblank_start;
+	wait_queue_head_t *wq = drm_crtc_vblank_waitqueue(&crtc->base);
 	DEFINE_WAIT(wait);
 
-	WARN_ON(!drm_modeset_is_locked(&crtc->base.mutex));
-
 	vblank_start = mode->crtc_vblank_start;
 	if (mode->flags & DRM_MODE_FLAG_INTERLACE)
 		vblank_start = DIV_ROUND_UP(vblank_start, 2);
@@ -81,7 +110,7 @@
 		 * other CPUs can see the task state update by the time we
 		 * read the scanline.
 		 */
-		prepare_to_wait(&crtc->vbl_wait, &wait, TASK_UNINTERRUPTIBLE);
+		prepare_to_wait(wq, &wait, TASK_UNINTERRUPTIBLE);
 
 		scanline = intel_get_crtc_scanline(crtc);
 		if (scanline < min || scanline > max)
@@ -100,7 +129,7 @@
 		local_irq_disable();
 	}
 
-	finish_wait(&crtc->vbl_wait, &wait);
+	finish_wait(wq, &wait);
 
 	drm_vblank_put(dev, pipe);
 
@@ -111,7 +140,16 @@
 	return true;
 }
 
-static void intel_pipe_update_end(struct intel_crtc *crtc, u32 start_vbl_count)
+/**
+ * intel_pipe_update_end() - end update of a set of display registers
+ * @crtc: the crtc of which the registers were updated
+ * @start_vbl_count: start vblank counter (used for error checking)
+ *
+ * Mark the end of an update started with intel_pipe_update_start(). This
+ * re-enables interrupts and verifies the update was actually completed
+ * before a vblank using the value of @start_vbl_count.
+ */
+void intel_pipe_update_end(struct intel_crtc *crtc, u32 start_vbl_count)
 {
 	struct drm_device *dev = crtc->base.dev;
 	enum pipe pipe = crtc->pipe;
@@ -138,6 +176,226 @@
 }
 
 static void
+skl_update_plane(struct drm_plane *drm_plane, struct drm_crtc *crtc,
+		 struct drm_framebuffer *fb,
+		 struct drm_i915_gem_object *obj, int crtc_x, int crtc_y,
+		 unsigned int crtc_w, unsigned int crtc_h,
+		 uint32_t x, uint32_t y,
+		 uint32_t src_w, uint32_t src_h)
+{
+	struct drm_device *dev = drm_plane->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_plane *intel_plane = to_intel_plane(drm_plane);
+	const int pipe = intel_plane->pipe;
+	const int plane = intel_plane->plane + 1;
+	u32 plane_ctl, stride;
+	int pixel_size = drm_format_plane_cpp(fb->pixel_format, 0);
+
+	plane_ctl = I915_READ(PLANE_CTL(pipe, plane));
+
+	/* Mask out pixel format bits in case we change it */
+	plane_ctl &= ~PLANE_CTL_FORMAT_MASK;
+	plane_ctl &= ~PLANE_CTL_ORDER_RGBX;
+	plane_ctl &= ~PLANE_CTL_YUV422_ORDER_MASK;
+	plane_ctl &= ~PLANE_CTL_TILED_MASK;
+	plane_ctl &= ~PLANE_CTL_ALPHA_MASK;
+	plane_ctl &= ~PLANE_CTL_ROTATE_MASK;
+
+	/* Trickle feed has to be enabled */
+	plane_ctl &= ~PLANE_CTL_TRICKLE_FEED_DISABLE;
+
+	switch (fb->pixel_format) {
+	case DRM_FORMAT_RGB565:
+		plane_ctl |= PLANE_CTL_FORMAT_RGB_565;
+		break;
+	case DRM_FORMAT_XBGR8888:
+		plane_ctl |= PLANE_CTL_FORMAT_XRGB_8888 | PLANE_CTL_ORDER_RGBX;
+		break;
+	case DRM_FORMAT_XRGB8888:
+		plane_ctl |= PLANE_CTL_FORMAT_XRGB_8888;
+		break;
+	/*
+	 * XXX: For ARBG/ABGR formats we default to expecting scanout buffers
+	 * to be already pre-multiplied. We need to add a knob (or a different
+	 * DRM_FORMAT) for user-space to configure that.
+	 */
+	case DRM_FORMAT_ABGR8888:
+		plane_ctl |= PLANE_CTL_FORMAT_XRGB_8888 |
+			     PLANE_CTL_ORDER_RGBX |
+			     PLANE_CTL_ALPHA_SW_PREMULTIPLY;
+		break;
+	case DRM_FORMAT_ARGB8888:
+		plane_ctl |= PLANE_CTL_FORMAT_XRGB_8888 |
+			     PLANE_CTL_ALPHA_SW_PREMULTIPLY;
+		break;
+	case DRM_FORMAT_YUYV:
+		plane_ctl |= PLANE_CTL_FORMAT_YUV422 | PLANE_CTL_YUV422_YUYV;
+		break;
+	case DRM_FORMAT_YVYU:
+		plane_ctl |= PLANE_CTL_FORMAT_YUV422 | PLANE_CTL_YUV422_YVYU;
+		break;
+	case DRM_FORMAT_UYVY:
+		plane_ctl |= PLANE_CTL_FORMAT_YUV422 | PLANE_CTL_YUV422_UYVY;
+		break;
+	case DRM_FORMAT_VYUY:
+		plane_ctl |= PLANE_CTL_FORMAT_YUV422 | PLANE_CTL_YUV422_VYUY;
+		break;
+	default:
+		BUG();
+	}
+
+	switch (obj->tiling_mode) {
+	case I915_TILING_NONE:
+		stride = fb->pitches[0] >> 6;
+		break;
+	case I915_TILING_X:
+		plane_ctl |= PLANE_CTL_TILED_X;
+		stride = fb->pitches[0] >> 9;
+		break;
+	default:
+		BUG();
+	}
+	if (intel_plane->rotation == BIT(DRM_ROTATE_180))
+		plane_ctl |= PLANE_CTL_ROTATE_180;
+
+	plane_ctl |= PLANE_CTL_ENABLE;
+	plane_ctl |= PLANE_CTL_PIPE_CSC_ENABLE;
+
+	intel_update_sprite_watermarks(drm_plane, crtc, src_w, src_h,
+				       pixel_size, true,
+				       src_w != crtc_w || src_h != crtc_h);
+
+	/* Sizes are 0 based */
+	src_w--;
+	src_h--;
+	crtc_w--;
+	crtc_h--;
+
+	I915_WRITE(PLANE_OFFSET(pipe, plane), (y << 16) | x);
+	I915_WRITE(PLANE_STRIDE(pipe, plane), stride);
+	I915_WRITE(PLANE_POS(pipe, plane), (crtc_y << 16) | crtc_x);
+	I915_WRITE(PLANE_SIZE(pipe, plane), (crtc_h << 16) | crtc_w);
+	I915_WRITE(PLANE_CTL(pipe, plane), plane_ctl);
+	I915_WRITE(PLANE_SURF(pipe, plane), i915_gem_obj_ggtt_offset(obj));
+	POSTING_READ(PLANE_SURF(pipe, plane));
+}
+
+static void
+skl_disable_plane(struct drm_plane *drm_plane, struct drm_crtc *crtc)
+{
+	struct drm_device *dev = drm_plane->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_plane *intel_plane = to_intel_plane(drm_plane);
+	const int pipe = intel_plane->pipe;
+	const int plane = intel_plane->plane + 1;
+
+	I915_WRITE(PLANE_CTL(pipe, plane),
+		   I915_READ(PLANE_CTL(pipe, plane)) & ~PLANE_CTL_ENABLE);
+
+	/* Activate double buffered register update */
+	I915_WRITE(PLANE_CTL(pipe, plane), 0);
+	POSTING_READ(PLANE_CTL(pipe, plane));
+
+	intel_update_sprite_watermarks(drm_plane, crtc, 0, 0, 0, false, false);
+}
+
+static int
+skl_update_colorkey(struct drm_plane *drm_plane,
+		    struct drm_intel_sprite_colorkey *key)
+{
+	struct drm_device *dev = drm_plane->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_plane *intel_plane = to_intel_plane(drm_plane);
+	const int pipe = intel_plane->pipe;
+	const int plane = intel_plane->plane;
+	u32 plane_ctl;
+
+	I915_WRITE(PLANE_KEYVAL(pipe, plane), key->min_value);
+	I915_WRITE(PLANE_KEYMAX(pipe, plane), key->max_value);
+	I915_WRITE(PLANE_KEYMSK(pipe, plane), key->channel_mask);
+
+	plane_ctl = I915_READ(PLANE_CTL(pipe, plane));
+	plane_ctl &= ~PLANE_CTL_KEY_ENABLE_MASK;
+	if (key->flags & I915_SET_COLORKEY_DESTINATION)
+		plane_ctl |= PLANE_CTL_KEY_ENABLE_DESTINATION;
+	else if (key->flags & I915_SET_COLORKEY_SOURCE)
+		plane_ctl |= PLANE_CTL_KEY_ENABLE_SOURCE;
+	I915_WRITE(PLANE_CTL(pipe, plane), plane_ctl);
+
+	POSTING_READ(PLANE_CTL(pipe, plane));
+
+	return 0;
+}
+
+static void
+skl_get_colorkey(struct drm_plane *drm_plane,
+		 struct drm_intel_sprite_colorkey *key)
+{
+	struct drm_device *dev = drm_plane->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_plane *intel_plane = to_intel_plane(drm_plane);
+	const int pipe = intel_plane->pipe;
+	const int plane = intel_plane->plane;
+	u32 plane_ctl;
+
+	key->min_value = I915_READ(PLANE_KEYVAL(pipe, plane));
+	key->max_value = I915_READ(PLANE_KEYMAX(pipe, plane));
+	key->channel_mask = I915_READ(PLANE_KEYMSK(pipe, plane));
+
+	plane_ctl = I915_READ(PLANE_CTL(pipe, plane));
+
+	switch (plane_ctl & PLANE_CTL_KEY_ENABLE_MASK) {
+	case PLANE_CTL_KEY_ENABLE_DESTINATION:
+		key->flags = I915_SET_COLORKEY_DESTINATION;
+		break;
+	case PLANE_CTL_KEY_ENABLE_SOURCE:
+		key->flags = I915_SET_COLORKEY_SOURCE;
+		break;
+	default:
+		key->flags = I915_SET_COLORKEY_NONE;
+	}
+}
+
+static void
+chv_update_csc(struct intel_plane *intel_plane, uint32_t format)
+{
+	struct drm_i915_private *dev_priv = intel_plane->base.dev->dev_private;
+	int plane = intel_plane->plane;
+
+	/* Seems RGB data bypasses the CSC always */
+	if (!format_is_yuv(format))
+		return;
+
+	/*
+	 * BT.601 limited range YCbCr -> full range RGB
+	 *
+	 * |r|   | 6537 4769     0|   |cr  |
+	 * |g| = |-3330 4769 -1605| x |y-64|
+	 * |b|   |    0 4769  8263|   |cb  |
+	 *
+	 * Cb and Cr apparently come in as signed already, so no
+	 * need for any offset. For Y we need to remove the offset.
+	 */
+	I915_WRITE(SPCSCYGOFF(plane), SPCSC_OOFF(0) | SPCSC_IOFF(-64));
+	I915_WRITE(SPCSCCBOFF(plane), SPCSC_OOFF(0) | SPCSC_IOFF(0));
+	I915_WRITE(SPCSCCROFF(plane), SPCSC_OOFF(0) | SPCSC_IOFF(0));
+
+	I915_WRITE(SPCSCC01(plane), SPCSC_C1(4769) | SPCSC_C0(6537));
+	I915_WRITE(SPCSCC23(plane), SPCSC_C1(-3330) | SPCSC_C0(0));
+	I915_WRITE(SPCSCC45(plane), SPCSC_C1(-1605) | SPCSC_C0(4769));
+	I915_WRITE(SPCSCC67(plane), SPCSC_C1(4769) | SPCSC_C0(0));
+	I915_WRITE(SPCSCC8(plane), SPCSC_C0(8263));
+
+	I915_WRITE(SPCSCYGICLAMP(plane), SPCSC_IMAX(940) | SPCSC_IMIN(64));
+	I915_WRITE(SPCSCCBICLAMP(plane), SPCSC_IMAX(448) | SPCSC_IMIN(-448));
+	I915_WRITE(SPCSCCRICLAMP(plane), SPCSC_IMAX(448) | SPCSC_IMIN(-448));
+
+	I915_WRITE(SPCSCYGOCLAMP(plane), SPCSC_OMAX(1023) | SPCSC_OMIN(0));
+	I915_WRITE(SPCSCCBOCLAMP(plane), SPCSC_OMAX(1023) | SPCSC_OMIN(0));
+	I915_WRITE(SPCSCCROCLAMP(plane), SPCSC_OMAX(1023) | SPCSC_OMIN(0));
+}
+
+static void
 vlv_update_plane(struct drm_plane *dplane, struct drm_crtc *crtc,
 		 struct drm_framebuffer *fb,
 		 struct drm_i915_gem_object *obj, int crtc_x, int crtc_y,
@@ -163,6 +421,7 @@
 	sprctl &= ~SP_PIXFORMAT_MASK;
 	sprctl &= ~SP_YUV_BYTE_ORDER_MASK;
 	sprctl &= ~SP_TILED;
+	sprctl &= ~SP_ROTATE_180;
 
 	switch (fb->pixel_format) {
 	case DRM_FORMAT_YUYV:
@@ -235,10 +494,21 @@
 							fb->pitches[0]);
 	linear_offset -= sprsurf_offset;
 
+	if (intel_plane->rotation == BIT(DRM_ROTATE_180)) {
+		sprctl |= SP_ROTATE_180;
+
+		x += src_w;
+		y += src_h;
+		linear_offset += src_h * fb->pitches[0] + src_w * pixel_size;
+	}
+
 	atomic_update = intel_pipe_update_start(intel_crtc, &start_vbl_count);
 
 	intel_update_primary_plane(intel_crtc);
 
+	if (IS_CHERRYVIEW(dev) && pipe == PIPE_B)
+		chv_update_csc(intel_plane, fb->pixel_format);
+
 	I915_WRITE(SPSTRIDE(pipe, plane), fb->pitches[0]);
 	I915_WRITE(SPPOS(pipe, plane), (crtc_y << 16) | crtc_x);
 
@@ -247,6 +517,8 @@
 	else
 		I915_WRITE(SPLINOFF(pipe, plane), linear_offset);
 
+	I915_WRITE(SPCONSTALPHA(pipe, plane), 0);
+
 	I915_WRITE(SPSIZE(pipe, plane), (crtc_h << 16) | crtc_w);
 	I915_WRITE(SPCNTR(pipe, plane), sprctl);
 	I915_WRITE(SPSURF(pipe, plane), i915_gem_obj_ggtt_offset(obj) +
@@ -364,6 +636,7 @@
 	sprctl &= ~SPRITE_RGB_ORDER_RGBX;
 	sprctl &= ~SPRITE_YUV_BYTE_ORDER_MASK;
 	sprctl &= ~SPRITE_TILED;
+	sprctl &= ~SPRITE_ROTATE_180;
 
 	switch (fb->pixel_format) {
 	case DRM_FORMAT_XBGR8888:
@@ -426,6 +699,18 @@
 					       pixel_size, fb->pitches[0]);
 	linear_offset -= sprsurf_offset;
 
+	if (intel_plane->rotation == BIT(DRM_ROTATE_180)) {
+		sprctl |= SPRITE_ROTATE_180;
+
+		/* HSW and BDW does this automagically in hardware */
+		if (!IS_HASWELL(dev) && !IS_BROADWELL(dev)) {
+			x += src_w;
+			y += src_h;
+			linear_offset += src_h * fb->pitches[0] +
+				src_w * pixel_size;
+		}
+	}
+
 	atomic_update = intel_pipe_update_start(intel_crtc, &start_vbl_count);
 
 	intel_update_primary_plane(intel_crtc);
@@ -571,6 +856,7 @@
 	dvscntr &= ~DVS_RGB_ORDER_XBGR;
 	dvscntr &= ~DVS_YUV_BYTE_ORDER_MASK;
 	dvscntr &= ~DVS_TILED;
+	dvscntr &= ~DVS_ROTATE_180;
 
 	switch (fb->pixel_format) {
 	case DRM_FORMAT_XBGR8888:
@@ -628,6 +914,14 @@
 					       pixel_size, fb->pitches[0]);
 	linear_offset -= dvssurf_offset;
 
+	if (intel_plane->rotation == BIT(DRM_ROTATE_180)) {
+		dvscntr |= DVS_ROTATE_180;
+
+		x += src_w;
+		y += src_h;
+		linear_offset += src_h * fb->pitches[0] + src_w * pixel_size;
+	}
+
 	atomic_update = intel_pipe_update_start(intel_crtc, &start_vbl_count);
 
 	intel_update_primary_plane(intel_crtc);
@@ -789,20 +1083,6 @@
 		key->flags = I915_SET_COLORKEY_NONE;
 }
 
-static bool
-format_is_yuv(uint32_t format)
-{
-	switch (format) {
-	case DRM_FORMAT_YUYV:
-	case DRM_FORMAT_UYVY:
-	case DRM_FORMAT_VYUY:
-	case DRM_FORMAT_YVYU:
-		return true;
-	default:
-		return false;
-	}
-}
-
 static bool colorkey_enabled(struct intel_plane *intel_plane)
 {
 	struct drm_intel_sprite_colorkey key;
@@ -813,57 +1093,23 @@
 }
 
 static int
-intel_update_plane(struct drm_plane *plane, struct drm_crtc *crtc,
-		   struct drm_framebuffer *fb, int crtc_x, int crtc_y,
-		   unsigned int crtc_w, unsigned int crtc_h,
-		   uint32_t src_x, uint32_t src_y,
-		   uint32_t src_w, uint32_t src_h)
+intel_check_sprite_plane(struct drm_plane *plane,
+			 struct intel_plane_state *state)
 {
-	struct drm_device *dev = plane->dev;
-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	struct intel_crtc *intel_crtc = to_intel_crtc(state->crtc);
 	struct intel_plane *intel_plane = to_intel_plane(plane);
-	enum pipe pipe = intel_crtc->pipe;
-	struct intel_framebuffer *intel_fb = to_intel_framebuffer(fb);
-	struct drm_i915_gem_object *obj = intel_fb->obj;
-	struct drm_i915_gem_object *old_obj = intel_plane->obj;
-	int ret;
-	bool primary_enabled;
-	bool visible;
+	struct drm_framebuffer *fb = state->fb;
+	struct drm_i915_gem_object *obj = intel_fb_obj(fb);
+	int crtc_x, crtc_y;
+	unsigned int crtc_w, crtc_h;
+	uint32_t src_x, src_y, src_w, src_h;
+	struct drm_rect *src = &state->src;
+	struct drm_rect *dst = &state->dst;
+	struct drm_rect *orig_src = &state->orig_src;
+	const struct drm_rect *clip = &state->clip;
 	int hscale, vscale;
 	int max_scale, min_scale;
 	int pixel_size = drm_format_plane_cpp(fb->pixel_format, 0);
-	struct drm_rect src = {
-		/* sample coordinates in 16.16 fixed point */
-		.x1 = src_x,
-		.x2 = src_x + src_w,
-		.y1 = src_y,
-		.y2 = src_y + src_h,
-	};
-	struct drm_rect dst = {
-		/* integer pixels */
-		.x1 = crtc_x,
-		.x2 = crtc_x + crtc_w,
-		.y1 = crtc_y,
-		.y2 = crtc_y + crtc_h,
-	};
-	const struct drm_rect clip = {
-		.x2 = intel_crtc->active ? intel_crtc->config.pipe_src_w : 0,
-		.y2 = intel_crtc->active ? intel_crtc->config.pipe_src_h : 0,
-	};
-	const struct {
-		int crtc_x, crtc_y;
-		unsigned int crtc_w, crtc_h;
-		uint32_t src_x, src_y, src_w, src_h;
-	} orig = {
-		.crtc_x = crtc_x,
-		.crtc_y = crtc_y,
-		.crtc_w = crtc_w,
-		.crtc_h = crtc_h,
-		.src_x = src_x,
-		.src_y = src_y,
-		.src_w = src_w,
-		.src_h = src_h,
-	};
 
 	/* Don't modify another pipe's plane */
 	if (intel_plane->pipe != intel_crtc->pipe) {
@@ -895,49 +1141,55 @@
 	max_scale = intel_plane->max_downscale << 16;
 	min_scale = intel_plane->can_scale ? 1 : (1 << 16);
 
-	hscale = drm_rect_calc_hscale_relaxed(&src, &dst, min_scale, max_scale);
+	drm_rect_rotate(src, fb->width << 16, fb->height << 16,
+			intel_plane->rotation);
+
+	hscale = drm_rect_calc_hscale_relaxed(src, dst, min_scale, max_scale);
 	BUG_ON(hscale < 0);
 
-	vscale = drm_rect_calc_vscale_relaxed(&src, &dst, min_scale, max_scale);
+	vscale = drm_rect_calc_vscale_relaxed(src, dst, min_scale, max_scale);
 	BUG_ON(vscale < 0);
 
-	visible = drm_rect_clip_scaled(&src, &dst, &clip, hscale, vscale);
+	state->visible =  drm_rect_clip_scaled(src, dst, clip, hscale, vscale);
 
-	crtc_x = dst.x1;
-	crtc_y = dst.y1;
-	crtc_w = drm_rect_width(&dst);
-	crtc_h = drm_rect_height(&dst);
+	crtc_x = dst->x1;
+	crtc_y = dst->y1;
+	crtc_w = drm_rect_width(dst);
+	crtc_h = drm_rect_height(dst);
 
-	if (visible) {
+	if (state->visible) {
 		/* check again in case clipping clamped the results */
-		hscale = drm_rect_calc_hscale(&src, &dst, min_scale, max_scale);
+		hscale = drm_rect_calc_hscale(src, dst, min_scale, max_scale);
 		if (hscale < 0) {
 			DRM_DEBUG_KMS("Horizontal scaling factor out of limits\n");
-			drm_rect_debug_print(&src, true);
-			drm_rect_debug_print(&dst, false);
+			drm_rect_debug_print(src, true);
+			drm_rect_debug_print(dst, false);
 
 			return hscale;
 		}
 
-		vscale = drm_rect_calc_vscale(&src, &dst, min_scale, max_scale);
+		vscale = drm_rect_calc_vscale(src, dst, min_scale, max_scale);
 		if (vscale < 0) {
 			DRM_DEBUG_KMS("Vertical scaling factor out of limits\n");
-			drm_rect_debug_print(&src, true);
-			drm_rect_debug_print(&dst, false);
+			drm_rect_debug_print(src, true);
+			drm_rect_debug_print(dst, false);
 
 			return vscale;
 		}
 
 		/* Make the source viewport size an exact multiple of the scaling factors. */
-		drm_rect_adjust_size(&src,
-				     drm_rect_width(&dst) * hscale - drm_rect_width(&src),
-				     drm_rect_height(&dst) * vscale - drm_rect_height(&src));
+		drm_rect_adjust_size(src,
+				     drm_rect_width(dst) * hscale - drm_rect_width(src),
+				     drm_rect_height(dst) * vscale - drm_rect_height(src));
+
+		drm_rect_rotate_inv(src, fb->width << 16, fb->height << 16,
+				    intel_plane->rotation);
 
 		/* sanity check to make sure the src viewport wasn't enlarged */
-		WARN_ON(src.x1 < (int) src_x ||
-			src.y1 < (int) src_y ||
-			src.x2 > (int) (src_x + src_w) ||
-			src.y2 > (int) (src_y + src_h));
+		WARN_ON(src->x1 < (int) orig_src->x1 ||
+			src->y1 < (int) orig_src->y1 ||
+			src->x2 > (int) orig_src->x2 ||
+			src->y2 > (int) orig_src->y2);
 
 		/*
 		 * Hardware doesn't handle subpixel coordinates.
@@ -945,10 +1197,10 @@
 		 * increase the source viewport size, because that could
 		 * push the downscaling factor out of bounds.
 		 */
-		src_x = src.x1 >> 16;
-		src_w = drm_rect_width(&src) >> 16;
-		src_y = src.y1 >> 16;
-		src_h = drm_rect_height(&src) >> 16;
+		src_x = src->x1 >> 16;
+		src_w = drm_rect_width(src) >> 16;
+		src_y = src->y1 >> 16;
+		src_h = drm_rect_height(src) >> 16;
 
 		if (format_is_yuv(fb->pixel_format)) {
 			src_x &= ~1;
@@ -962,12 +1214,12 @@
 				crtc_w &= ~1;
 
 			if (crtc_w == 0)
-				visible = false;
+				state->visible = false;
 		}
 	}
 
 	/* Check size restrictions when scaling */
-	if (visible && (src_w != crtc_w || src_h != crtc_h)) {
+	if (state->visible && (src_w != crtc_w || src_h != crtc_h)) {
 		unsigned int width_bytes;
 
 		WARN_ON(!intel_plane->can_scale);
@@ -975,12 +1227,13 @@
 		/* FIXME interlacing min height is 6 */
 
 		if (crtc_w < 3 || crtc_h < 3)
-			visible = false;
+			state->visible = false;
 
 		if (src_w < 3 || src_h < 3)
-			visible = false;
+			state->visible = false;
 
-		width_bytes = ((src_x * pixel_size) & 63) + src_w * pixel_size;
+		width_bytes = ((src_x * pixel_size) & 63) +
+					src_w * pixel_size;
 
 		if (src_w > 2048 || src_h > 2048 ||
 		    width_bytes > 4096 || fb->pitches[0] > 4096) {
@@ -989,42 +1242,90 @@
 		}
 	}
 
-	dst.x1 = crtc_x;
-	dst.x2 = crtc_x + crtc_w;
-	dst.y1 = crtc_y;
-	dst.y2 = crtc_y + crtc_h;
+	if (state->visible) {
+		src->x1 = src_x;
+		src->x2 = src_x + src_w;
+		src->y1 = src_y;
+		src->y2 = src_y + src_h;
+	}
+
+	dst->x1 = crtc_x;
+	dst->x2 = crtc_x + crtc_w;
+	dst->y1 = crtc_y;
+	dst->y2 = crtc_y + crtc_h;
 
-	/*
-	 * If the sprite is completely covering the primary plane,
-	 * we can disable the primary and save power.
-	 */
-	primary_enabled = !drm_rect_equals(&dst, &clip) || colorkey_enabled(intel_plane);
-	WARN_ON(!primary_enabled && !visible && intel_crtc->active);
+	return 0;
+}
 
-	mutex_lock(&dev->struct_mutex);
+static int
+intel_prepare_sprite_plane(struct drm_plane *plane,
+			   struct intel_plane_state *state)
+{
+	struct drm_device *dev = plane->dev;
+	struct drm_crtc *crtc = state->crtc;
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	struct intel_plane *intel_plane = to_intel_plane(plane);
+	enum pipe pipe = intel_crtc->pipe;
+	struct drm_framebuffer *fb = state->fb;
+	struct drm_i915_gem_object *obj = intel_fb_obj(fb);
+	struct drm_i915_gem_object *old_obj = intel_plane->obj;
+	int ret;
 
-	/* Note that this will apply the VT-d workaround for scanouts,
-	 * which is more restrictive than required for sprites. (The
-	 * primary plane requires 256KiB alignment with 64 PTE padding,
-	 * the sprite planes only require 128KiB alignment and 32 PTE padding.
-	 */
-	ret = intel_pin_and_fence_fb_obj(dev, obj, NULL);
+	if (old_obj != obj) {
+		mutex_lock(&dev->struct_mutex);
 
-	i915_gem_track_fb(old_obj, obj,
-			  INTEL_FRONTBUFFER_SPRITE(pipe));
-	mutex_unlock(&dev->struct_mutex);
+		/* Note that this will apply the VT-d workaround for scanouts,
+		 * which is more restrictive than required for sprites. (The
+		 * primary plane requires 256KiB alignment with 64 PTE padding,
+		 * the sprite planes only require 128KiB alignment and 32 PTE
+		 * padding.
+		 */
+		ret = intel_pin_and_fence_fb_obj(plane, fb, NULL);
+		if (ret == 0)
+			i915_gem_track_fb(old_obj, obj,
+					  INTEL_FRONTBUFFER_SPRITE(pipe));
+		mutex_unlock(&dev->struct_mutex);
+		if (ret)
+			return ret;
+	}
 
-	if (ret)
-		return ret;
+	return 0;
+}
 
-	intel_plane->crtc_x = orig.crtc_x;
-	intel_plane->crtc_y = orig.crtc_y;
-	intel_plane->crtc_w = orig.crtc_w;
-	intel_plane->crtc_h = orig.crtc_h;
-	intel_plane->src_x = orig.src_x;
-	intel_plane->src_y = orig.src_y;
-	intel_plane->src_w = orig.src_w;
-	intel_plane->src_h = orig.src_h;
+static void
+intel_commit_sprite_plane(struct drm_plane *plane,
+			  struct intel_plane_state *state)
+{
+	struct drm_device *dev = plane->dev;
+	struct drm_crtc *crtc = state->crtc;
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	struct intel_plane *intel_plane = to_intel_plane(plane);
+	enum pipe pipe = intel_crtc->pipe;
+	struct drm_framebuffer *fb = state->fb;
+	struct drm_i915_gem_object *obj = intel_fb_obj(fb);
+	struct drm_i915_gem_object *old_obj = intel_plane->obj;
+	int crtc_x, crtc_y;
+	unsigned int crtc_w, crtc_h;
+	uint32_t src_x, src_y, src_w, src_h;
+	struct drm_rect *dst = &state->dst;
+	const struct drm_rect *clip = &state->clip;
+	bool primary_enabled;
+
+	/*
+	 * If the sprite is completely covering the primary plane,
+	 * we can disable the primary and save power.
+	 */
+	primary_enabled = !drm_rect_equals(dst, clip) || colorkey_enabled(intel_plane);
+	WARN_ON(!primary_enabled && !state->visible && intel_crtc->active);
+
+	intel_plane->crtc_x = state->orig_dst.x1;
+	intel_plane->crtc_y = state->orig_dst.y1;
+	intel_plane->crtc_w = drm_rect_width(&state->orig_dst);
+	intel_plane->crtc_h = drm_rect_height(&state->orig_dst);
+	intel_plane->src_x = state->orig_src.x1;
+	intel_plane->src_y = state->orig_src.y1;
+	intel_plane->src_w = drm_rect_width(&state->orig_src);
+	intel_plane->src_h = drm_rect_height(&state->orig_src);
 	intel_plane->obj = obj;
 
 	if (intel_crtc->active) {
@@ -1038,12 +1339,22 @@
 		if (primary_was_enabled && !primary_enabled)
 			intel_pre_disable_primary(crtc);
 
-		if (visible)
+		if (state->visible) {
+			crtc_x = state->dst.x1;
+			crtc_y = state->dst.y1;
+			crtc_w = drm_rect_width(&state->dst);
+			crtc_h = drm_rect_height(&state->dst);
+			src_x = state->src.x1;
+			src_y = state->src.y1;
+			src_w = drm_rect_width(&state->src);
+			src_h = drm_rect_height(&state->src);
 			intel_plane->update_plane(plane, crtc, fb, obj,
 						  crtc_x, crtc_y, crtc_w, crtc_h,
 						  src_x, src_y, src_w, src_h);
-		else
+		} else {
 			intel_plane->disable_plane(plane, crtc);
+		}
+
 
 		intel_frontbuffer_flip(dev, INTEL_FRONTBUFFER_SPRITE(pipe));
 
@@ -1052,21 +1363,65 @@
 	}
 
 	/* Unpin old obj after new one is active to avoid ugliness */
-	if (old_obj) {
+	if (old_obj && old_obj != obj) {
+
 		/*
 		 * It's fairly common to simply update the position of
 		 * an existing object.  In that case, we don't need to
 		 * wait for vblank to avoid ugliness, we only need to
 		 * do the pin & ref bookkeeping.
 		 */
-		if (old_obj != obj && intel_crtc->active)
+		if (intel_crtc->active)
 			intel_wait_for_vblank(dev, intel_crtc->pipe);
 
 		mutex_lock(&dev->struct_mutex);
 		intel_unpin_fb_obj(old_obj);
 		mutex_unlock(&dev->struct_mutex);
 	}
+}
+
+static int
+intel_update_plane(struct drm_plane *plane, struct drm_crtc *crtc,
+		   struct drm_framebuffer *fb, int crtc_x, int crtc_y,
+		   unsigned int crtc_w, unsigned int crtc_h,
+		   uint32_t src_x, uint32_t src_y,
+		   uint32_t src_w, uint32_t src_h)
+{
+	struct intel_plane_state state;
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	int ret;
+
+	state.crtc = crtc;
+	state.fb = fb;
+
+	/* sample coordinates in 16.16 fixed point */
+	state.src.x1 = src_x;
+	state.src.x2 = src_x + src_w;
+	state.src.y1 = src_y;
+	state.src.y2 = src_y + src_h;
+
+	/* integer pixels */
+	state.dst.x1 = crtc_x;
+	state.dst.x2 = crtc_x + crtc_w;
+	state.dst.y1 = crtc_y;
+	state.dst.y2 = crtc_y + crtc_h;
+
+	state.clip.x1 = 0;
+	state.clip.y1 = 0;
+	state.clip.x2 = intel_crtc->active ? intel_crtc->config.pipe_src_w : 0;
+	state.clip.y2 = intel_crtc->active ? intel_crtc->config.pipe_src_h : 0;
+	state.orig_src = state.src;
+	state.orig_dst = state.dst;
+
+	ret = intel_check_sprite_plane(plane, &state);
+	if (ret)
+		return ret;
+
+	ret = intel_prepare_sprite_plane(plane, &state);
+	if (ret)
+		return ret;
 
+	intel_commit_sprite_plane(plane, &state);
 	return 0;
 }
 
@@ -1180,18 +1535,45 @@
 	return ret;
 }
 
-void intel_plane_restore(struct drm_plane *plane)
+int intel_plane_set_property(struct drm_plane *plane,
+			     struct drm_property *prop,
+			     uint64_t val)
+{
+	struct drm_device *dev = plane->dev;
+	struct intel_plane *intel_plane = to_intel_plane(plane);
+	uint64_t old_val;
+	int ret = -ENOENT;
+
+	if (prop == dev->mode_config.rotation_property) {
+		/* exactly one rotation angle please */
+		if (hweight32(val & 0xf) != 1)
+			return -EINVAL;
+
+		if (intel_plane->rotation == val)
+			return 0;
+
+		old_val = intel_plane->rotation;
+		intel_plane->rotation = val;
+		ret = intel_plane_restore(plane);
+		if (ret)
+			intel_plane->rotation = old_val;
+	}
+
+	return ret;
+}
+
+int intel_plane_restore(struct drm_plane *plane)
 {
 	struct intel_plane *intel_plane = to_intel_plane(plane);
 
 	if (!plane->crtc || !plane->fb)
-		return;
+		return 0;
 
-	intel_update_plane(plane, plane->crtc, plane->fb,
-			   intel_plane->crtc_x, intel_plane->crtc_y,
-			   intel_plane->crtc_w, intel_plane->crtc_h,
-			   intel_plane->src_x, intel_plane->src_y,
-			   intel_plane->src_w, intel_plane->src_h);
+	return plane->funcs->update_plane(plane, plane->crtc, plane->fb,
+				  intel_plane->crtc_x, intel_plane->crtc_y,
+				  intel_plane->crtc_w, intel_plane->crtc_h,
+				  intel_plane->src_x, intel_plane->src_y,
+				  intel_plane->src_w, intel_plane->src_h);
 }
 
 void intel_plane_disable(struct drm_plane *plane)
@@ -1206,6 +1588,7 @@
 	.update_plane = intel_update_plane,
 	.disable_plane = intel_disable_plane,
 	.destroy = intel_destroy_plane,
+	.set_property = intel_plane_set_property,
 };
 
 static uint32_t ilk_plane_formats[] = {
@@ -1239,6 +1622,18 @@
 	DRM_FORMAT_VYUY,
 };
 
+static uint32_t skl_plane_formats[] = {
+	DRM_FORMAT_RGB565,
+	DRM_FORMAT_ABGR8888,
+	DRM_FORMAT_ARGB8888,
+	DRM_FORMAT_XBGR8888,
+	DRM_FORMAT_XRGB8888,
+	DRM_FORMAT_YUYV,
+	DRM_FORMAT_YVYU,
+	DRM_FORMAT_UYVY,
+	DRM_FORMAT_VYUY,
+};
+
 int
 intel_plane_init(struct drm_device *dev, enum pipe pipe, int plane)
 {
@@ -1302,7 +1697,21 @@
 			num_plane_formats = ARRAY_SIZE(snb_plane_formats);
 		}
 		break;
+	case 9:
+		/*
+		 * FIXME: Skylake planes can be scaled (with some restrictions),
+		 * but this is for another time.
+		 */
+		intel_plane->can_scale = false;
+		intel_plane->max_downscale = 1;
+		intel_plane->update_plane = skl_update_plane;
+		intel_plane->disable_plane = skl_disable_plane;
+		intel_plane->update_colorkey = skl_update_colorkey;
+		intel_plane->get_colorkey = skl_get_colorkey;
 
+		plane_formats = skl_plane_formats;
+		num_plane_formats = ARRAY_SIZE(skl_plane_formats);
+		break;
 	default:
 		kfree(intel_plane);
 		return -ENODEV;
@@ -1310,13 +1719,28 @@
 
 	intel_plane->pipe = pipe;
 	intel_plane->plane = plane;
+	intel_plane->rotation = BIT(DRM_ROTATE_0);
 	possible_crtcs = (1 << pipe);
-	ret = drm_plane_init(dev, &intel_plane->base, possible_crtcs,
-			     &intel_plane_funcs,
-			     plane_formats, num_plane_formats,
-			     false);
-	if (ret)
+	ret = drm_universal_plane_init(dev, &intel_plane->base, possible_crtcs,
+				       &intel_plane_funcs,
+				       plane_formats, num_plane_formats,
+				       DRM_PLANE_TYPE_OVERLAY);
+	if (ret) {
 		kfree(intel_plane);
+		goto out;
+	}
+
+	if (!dev->mode_config.rotation_property)
+		dev->mode_config.rotation_property =
+			drm_mode_create_rotation_property(dev,
+							  BIT(DRM_ROTATE_0) |
+							  BIT(DRM_ROTATE_180));
+
+	if (dev->mode_config.rotation_property)
+		drm_object_attach_property(&intel_plane->base.base,
+					   dev->mode_config.rotation_property,
+					   intel_plane->rotation);
 
+ out:
 	return ret;
 }
diff -urN a/drivers/gpu/drm/i915/intel_tv.c b/drivers/gpu/drm/i915/intel_tv.c
--- a/drivers/gpu/drm/i915/intel_tv.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_tv.c	2014-11-22 14:37:49.346700417 -0700
@@ -1173,7 +1173,7 @@
  * \return true if TV is connected.
  * \return false if TV is disconnected.
  */
-static int
+static enum drm_connector_status
 intel_tv_detect_type(struct intel_tv *intel_tv,
 		      struct drm_connector *connector)
 {
@@ -1182,18 +1182,17 @@
 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 	struct drm_device *dev = encoder->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	unsigned long irqflags;
+	enum drm_connector_status status;
 	u32 tv_ctl, save_tv_ctl;
 	u32 tv_dac, save_tv_dac;
-	int type;
 
 	/* Disable TV interrupts around load detect or we'll recurse */
 	if (connector->polled & DRM_CONNECTOR_POLL_HPD) {
-		spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
+		spin_lock_irq(&dev_priv->irq_lock);
 		i915_disable_pipestat(dev_priv, 0,
 				      PIPE_HOTPLUG_INTERRUPT_STATUS |
 				      PIPE_HOTPLUG_TV_INTERRUPT_STATUS);
-		spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
+		spin_unlock_irq(&dev_priv->irq_lock);
 	}
 
 	save_tv_dac = tv_dac = I915_READ(TV_DAC);
@@ -1233,7 +1232,6 @@
 	intel_wait_for_vblank(intel_tv->base.base.dev,
 			      to_intel_crtc(intel_tv->base.base.crtc)->pipe);
 
-	type = -1;
 	tv_dac = I915_READ(TV_DAC);
 	DRM_DEBUG_KMS("TV detected: %x, %x\n", tv_ctl, tv_dac);
 	/*
@@ -1242,18 +1240,19 @@
 	 *  1 0 X svideo
 	 *  0 0 0 Component
 	 */
+	status = connector_status_connected;
 	if ((tv_dac & TVDAC_SENSE_MASK) == (TVDAC_B_SENSE | TVDAC_C_SENSE)) {
 		DRM_DEBUG_KMS("Detected Composite TV connection\n");
-		type = DRM_MODE_CONNECTOR_Composite;
+		intel_tv->type = DRM_MODE_CONNECTOR_Composite;
 	} else if ((tv_dac & (TVDAC_A_SENSE|TVDAC_B_SENSE)) == TVDAC_A_SENSE) {
 		DRM_DEBUG_KMS("Detected S-Video TV connection\n");
-		type = DRM_MODE_CONNECTOR_SVIDEO;
+		intel_tv->type = DRM_MODE_CONNECTOR_SVIDEO;
 	} else if ((tv_dac & TVDAC_SENSE_MASK) == 0) {
 		DRM_DEBUG_KMS("Detected Component TV connection\n");
-		type = DRM_MODE_CONNECTOR_Component;
+		intel_tv->type = DRM_MODE_CONNECTOR_Component;
 	} else {
 		DRM_DEBUG_KMS("Unrecognised TV connection\n");
-		type = -1;
+		status = connector_status_disconnected;
 	}
 
 	I915_WRITE(TV_DAC, save_tv_dac & ~TVDAC_STATE_CHG_EN);
@@ -1266,14 +1265,14 @@
 
 	/* Restore interrupt config */
 	if (connector->polled & DRM_CONNECTOR_POLL_HPD) {
-		spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
+		spin_lock_irq(&dev_priv->irq_lock);
 		i915_enable_pipestat(dev_priv, 0,
 				     PIPE_HOTPLUG_INTERRUPT_STATUS |
 				     PIPE_HOTPLUG_TV_INTERRUPT_STATUS);
-		spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
+		spin_unlock_irq(&dev_priv->irq_lock);
 	}
 
-	return type;
+	return status;
 }
 
 /*
@@ -1313,43 +1312,33 @@
 static enum drm_connector_status
 intel_tv_detect(struct drm_connector *connector, bool force)
 {
-	struct drm_display_mode mode;
 	struct intel_tv *intel_tv = intel_attached_tv(connector);
+	struct drm_display_mode mode = reported_modes[0];
+	struct intel_load_detect_pipe tmp;
+	struct drm_modeset_acquire_ctx ctx;
 	enum drm_connector_status status;
-	int type;
 
 	DRM_DEBUG_KMS("[CONNECTOR:%d:%s] force=%d\n",
 		      connector->base.id, connector->name,
 		      force);
+	if (!force)
+		return connector->status;
 
-	mode = reported_modes[0];
-
-	if (force) {
-		struct intel_load_detect_pipe tmp;
-		struct drm_modeset_acquire_ctx ctx;
-
-		drm_modeset_acquire_init(&ctx, 0);
-
-		if (intel_get_load_detect_pipe(connector, &mode, &tmp, &ctx)) {
-			type = intel_tv_detect_type(intel_tv, connector);
-			intel_release_load_detect_pipe(connector, &tmp);
-			status = type < 0 ?
-				connector_status_disconnected :
-				connector_status_connected;
-		} else
-			status = connector_status_unknown;
+	drm_modeset_acquire_init(&ctx, 0);
 
-		drm_modeset_drop_locks(&ctx);
-		drm_modeset_acquire_fini(&ctx);
+	if (intel_get_load_detect_pipe(connector, &mode, &tmp, &ctx)) {
+		status = intel_tv_detect_type(intel_tv, connector);
+		intel_release_load_detect_pipe(connector, &tmp);
 	} else
-		return connector->status;
+		status = connector_status_unknown;
+
+	drm_modeset_drop_locks(&ctx);
+	drm_modeset_acquire_fini(&ctx);
 
 	if (status != connector_status_connected)
 		return status;
 
-	intel_tv->type = type;
 	intel_tv_find_better_format(connector);
-
 	return connector_status_connected;
 }
 
diff -urN a/drivers/gpu/drm/i915/intel_uncore.c b/drivers/gpu/drm/i915/intel_uncore.c
--- a/drivers/gpu/drm/i915/intel_uncore.c	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/intel_uncore.c	2014-11-22 14:37:49.346700417 -0700
@@ -24,6 +24,8 @@
 #include "i915_drv.h"
 #include "intel_drv.h"
 
+#include <linux/pm_runtime.h>
+
 #define FORCEWAKE_ACK_TIMEOUT_MS 2
 
 #define __raw_i915_read8(dev_priv__, reg__) readb((dev_priv__)->regs + (reg__))
@@ -49,17 +51,11 @@
 
 static void __gen6_gt_wait_for_thread_c0(struct drm_i915_private *dev_priv)
 {
-	u32 gt_thread_status_mask;
-
-	if (IS_HASWELL(dev_priv->dev))
-		gt_thread_status_mask = GEN6_GT_THREAD_STATUS_CORE_MASK_HSW;
-	else
-		gt_thread_status_mask = GEN6_GT_THREAD_STATUS_CORE_MASK;
-
 	/* w/a for a sporadic read returning 0 by waiting for the GT
 	 * thread to wake up.
 	 */
-	if (wait_for_atomic_us((__raw_i915_read32(dev_priv, GEN6_GT_THREAD_STATUS_REG) & gt_thread_status_mask) == 0, 500))
+	if (wait_for_atomic_us((__raw_i915_read32(dev_priv, GEN6_GT_THREAD_STATUS_REG) &
+				GEN6_GT_THREAD_STATUS_CORE_MASK) == 0, 500))
 		DRM_ERROR("GT thread status wait timed out\n");
 }
 
@@ -71,7 +67,7 @@
 }
 
 static void __gen6_gt_force_wake_get(struct drm_i915_private *dev_priv,
-							int fw_engine)
+				     int fw_engine)
 {
 	if (wait_for_atomic((__raw_i915_read32(dev_priv, FORCEWAKE_ACK) & 1) == 0,
 			    FORCEWAKE_ACK_TIMEOUT_MS))
@@ -97,11 +93,11 @@
 }
 
 static void __gen7_gt_force_wake_mt_get(struct drm_i915_private *dev_priv,
-							int fw_engine)
+					int fw_engine)
 {
 	u32 forcewake_ack;
 
-	if (IS_HASWELL(dev_priv->dev) || IS_GEN8(dev_priv->dev))
+	if (IS_HASWELL(dev_priv->dev) || IS_BROADWELL(dev_priv->dev))
 		forcewake_ack = FORCEWAKE_ACK_HSW;
 	else
 		forcewake_ack = FORCEWAKE_MT_ACK;
@@ -120,8 +116,7 @@
 		DRM_ERROR("Timed out waiting for forcewake to ack request.\n");
 
 	/* WaRsForcewakeWaitTC0:ivb,hsw */
-	if (INTEL_INFO(dev_priv->dev)->gen < 8)
-		__gen6_gt_wait_for_thread_c0(dev_priv);
+	__gen6_gt_wait_for_thread_c0(dev_priv);
 }
 
 static void gen6_gt_check_fifodbg(struct drm_i915_private *dev_priv)
@@ -134,7 +129,7 @@
 }
 
 static void __gen6_gt_force_wake_put(struct drm_i915_private *dev_priv,
-							int fw_engine)
+				     int fw_engine)
 {
 	__raw_i915_write32(dev_priv, FORCEWAKE, 0);
 	/* something from same cacheline, but !FORCEWAKE */
@@ -143,7 +138,7 @@
 }
 
 static void __gen7_gt_force_wake_mt_put(struct drm_i915_private *dev_priv,
-							int fw_engine)
+					int fw_engine)
 {
 	__raw_i915_write32(dev_priv, FORCEWAKE_MT,
 			   _MASKED_BIT_DISABLE(FORCEWAKE_KERNEL));
@@ -192,7 +187,7 @@
 }
 
 static void __vlv_force_wake_get(struct drm_i915_private *dev_priv,
-						int fw_engine)
+				 int fw_engine)
 {
 	/* Check for Render Engine */
 	if (FORCEWAKE_RENDER & fw_engine) {
@@ -229,16 +224,11 @@
 					FORCEWAKE_ACK_TIMEOUT_MS))
 			DRM_ERROR("Timed out: waiting for media to ack.\n");
 	}
-
-	/* WaRsForcewakeWaitTC0:vlv */
-	if (!IS_CHERRYVIEW(dev_priv->dev))
-		__gen6_gt_wait_for_thread_c0(dev_priv);
 }
 
 static void __vlv_force_wake_put(struct drm_i915_private *dev_priv,
-					int fw_engine)
+				 int fw_engine)
 {
-
 	/* Check for Render Engine */
 	if (FORCEWAKE_RENDER & fw_engine)
 		__raw_i915_write32(dev_priv, FORCEWAKE_VLV,
@@ -256,73 +246,124 @@
 		gen6_gt_check_fifodbg(dev_priv);
 }
 
-static void vlv_force_wake_get(struct drm_i915_private *dev_priv, int fw_engine)
+static void __gen9_gt_force_wake_mt_reset(struct drm_i915_private *dev_priv)
 {
-	unsigned long irqflags;
-
-	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
+	__raw_i915_write32(dev_priv, FORCEWAKE_RENDER_GEN9,
+			_MASKED_BIT_DISABLE(0xffff));
 
-	if (fw_engine & FORCEWAKE_RENDER &&
-	    dev_priv->uncore.fw_rendercount++ != 0)
-		fw_engine &= ~FORCEWAKE_RENDER;
-	if (fw_engine & FORCEWAKE_MEDIA &&
-	    dev_priv->uncore.fw_mediacount++ != 0)
-		fw_engine &= ~FORCEWAKE_MEDIA;
+	__raw_i915_write32(dev_priv, FORCEWAKE_MEDIA_GEN9,
+			_MASKED_BIT_DISABLE(0xffff));
 
-	if (fw_engine)
-		dev_priv->uncore.funcs.force_wake_get(dev_priv, fw_engine);
-
-	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
+	__raw_i915_write32(dev_priv, FORCEWAKE_BLITTER_GEN9,
+			_MASKED_BIT_DISABLE(0xffff));
 }
 
-static void vlv_force_wake_put(struct drm_i915_private *dev_priv, int fw_engine)
+static void
+__gen9_force_wake_get(struct drm_i915_private *dev_priv, int fw_engine)
 {
-	unsigned long irqflags;
+	/* Check for Render Engine */
+	if (FORCEWAKE_RENDER & fw_engine) {
+		if (wait_for_atomic((__raw_i915_read32(dev_priv,
+						FORCEWAKE_ACK_RENDER_GEN9) &
+						FORCEWAKE_KERNEL) == 0,
+					FORCEWAKE_ACK_TIMEOUT_MS))
+			DRM_ERROR("Timed out: Render forcewake old ack to clear.\n");
 
-	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
+		__raw_i915_write32(dev_priv, FORCEWAKE_RENDER_GEN9,
+				   _MASKED_BIT_ENABLE(FORCEWAKE_KERNEL));
 
-	if (fw_engine & FORCEWAKE_RENDER) {
-		WARN_ON(!dev_priv->uncore.fw_rendercount);
-		if (--dev_priv->uncore.fw_rendercount != 0)
-			fw_engine &= ~FORCEWAKE_RENDER;
+		if (wait_for_atomic((__raw_i915_read32(dev_priv,
+						FORCEWAKE_ACK_RENDER_GEN9) &
+						FORCEWAKE_KERNEL),
+					FORCEWAKE_ACK_TIMEOUT_MS))
+			DRM_ERROR("Timed out: waiting for Render to ack.\n");
 	}
 
-	if (fw_engine & FORCEWAKE_MEDIA) {
-		WARN_ON(!dev_priv->uncore.fw_mediacount);
-		if (--dev_priv->uncore.fw_mediacount != 0)
-			fw_engine &= ~FORCEWAKE_MEDIA;
+	/* Check for Media Engine */
+	if (FORCEWAKE_MEDIA & fw_engine) {
+		if (wait_for_atomic((__raw_i915_read32(dev_priv,
+						FORCEWAKE_ACK_MEDIA_GEN9) &
+						FORCEWAKE_KERNEL) == 0,
+					FORCEWAKE_ACK_TIMEOUT_MS))
+			DRM_ERROR("Timed out: Media forcewake old ack to clear.\n");
+
+		__raw_i915_write32(dev_priv, FORCEWAKE_MEDIA_GEN9,
+				   _MASKED_BIT_ENABLE(FORCEWAKE_KERNEL));
+
+		if (wait_for_atomic((__raw_i915_read32(dev_priv,
+						FORCEWAKE_ACK_MEDIA_GEN9) &
+						FORCEWAKE_KERNEL),
+					FORCEWAKE_ACK_TIMEOUT_MS))
+			DRM_ERROR("Timed out: waiting for Media to ack.\n");
 	}
 
-	if (fw_engine)
-		dev_priv->uncore.funcs.force_wake_put(dev_priv, fw_engine);
+	/* Check for Blitter Engine */
+	if (FORCEWAKE_BLITTER & fw_engine) {
+		if (wait_for_atomic((__raw_i915_read32(dev_priv,
+						FORCEWAKE_ACK_BLITTER_GEN9) &
+						FORCEWAKE_KERNEL) == 0,
+					FORCEWAKE_ACK_TIMEOUT_MS))
+			DRM_ERROR("Timed out: Blitter forcewake old ack to clear.\n");
 
-	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
+		__raw_i915_write32(dev_priv, FORCEWAKE_BLITTER_GEN9,
+				   _MASKED_BIT_ENABLE(FORCEWAKE_KERNEL));
+
+		if (wait_for_atomic((__raw_i915_read32(dev_priv,
+						FORCEWAKE_ACK_BLITTER_GEN9) &
+						FORCEWAKE_KERNEL),
+					FORCEWAKE_ACK_TIMEOUT_MS))
+			DRM_ERROR("Timed out: waiting for Blitter to ack.\n");
+	}
+}
+
+static void
+__gen9_force_wake_put(struct drm_i915_private *dev_priv, int fw_engine)
+{
+	/* Check for Render Engine */
+	if (FORCEWAKE_RENDER & fw_engine)
+		__raw_i915_write32(dev_priv, FORCEWAKE_RENDER_GEN9,
+				_MASKED_BIT_DISABLE(FORCEWAKE_KERNEL));
+
+	/* Check for Media Engine */
+	if (FORCEWAKE_MEDIA & fw_engine)
+		__raw_i915_write32(dev_priv, FORCEWAKE_MEDIA_GEN9,
+				_MASKED_BIT_DISABLE(FORCEWAKE_KERNEL));
+
+	/* Check for Blitter Engine */
+	if (FORCEWAKE_BLITTER & fw_engine)
+		__raw_i915_write32(dev_priv, FORCEWAKE_BLITTER_GEN9,
+				_MASKED_BIT_DISABLE(FORCEWAKE_KERNEL));
 }
 
 static void gen6_force_wake_timer(unsigned long arg)
 {
-	struct drm_i915_private *dev_priv = (void *)arg;
+	struct intel_uncore_forcewake_domain *domain = (void *)arg;
 	unsigned long irqflags;
 
-	assert_device_not_suspended(dev_priv);
+	assert_device_not_suspended(domain->i915);
 
-	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
-	WARN_ON(!dev_priv->uncore.forcewake_count);
-
-	if (--dev_priv->uncore.forcewake_count == 0)
-		dev_priv->uncore.funcs.force_wake_put(dev_priv, FORCEWAKE_ALL);
-	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
+	spin_lock_irqsave(&domain->i915->uncore.lock, irqflags);
+	WARN_ON(!domain->wake_count);
 
-	intel_runtime_pm_put(dev_priv);
+	if (--domain->wake_count == 0)
+		domain->i915->uncore.funcs.force_wake_put(domain->i915,
+							  1 << domain->id);
+	spin_unlock_irqrestore(&domain->i915->uncore.lock, irqflags);
 }
 
 void intel_uncore_forcewake_reset(struct drm_device *dev, bool restore)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	unsigned long irqflags;
+	int i;
 
-	if (del_timer_sync(&dev_priv->uncore.force_wake_timer))
-		gen6_force_wake_timer((unsigned long)dev_priv);
+	for (i = 0; i < FW_DOMAIN_COUNT; i++ ) {
+		if ((dev_priv->uncore.fw_domains & (1 << i)) == 0)
+			continue;
+
+		if (del_timer_sync(&dev_priv->uncore.fw_domain[i].timer))
+			gen6_force_wake_timer((unsigned long)&dev_priv->uncore.fw_domain[i]);
+	}
 
 	/* Hold uncore.lock across reset to prevent any register access
 	 * with forcewake not set correctly
@@ -334,23 +375,20 @@
 	else if (IS_GEN6(dev) || IS_GEN7(dev))
 		__gen6_gt_force_wake_reset(dev_priv);
 
-	if (IS_IVYBRIDGE(dev) || IS_HASWELL(dev) || IS_GEN8(dev))
+	if (IS_IVYBRIDGE(dev) || IS_HASWELL(dev) || IS_BROADWELL(dev))
 		__gen7_gt_force_wake_mt_reset(dev_priv);
 
+	if (IS_GEN9(dev))
+		__gen9_gt_force_wake_mt_reset(dev_priv);
+
 	if (restore) { /* If reset with a user forcewake, try to restore */
 		unsigned fw = 0;
+		int i;
 
-		if (IS_VALLEYVIEW(dev)) {
-			if (dev_priv->uncore.fw_rendercount)
-				fw |= FORCEWAKE_RENDER;
-
-			if (dev_priv->uncore.fw_mediacount)
-				fw |= FORCEWAKE_MEDIA;
-		} else {
-			if (dev_priv->uncore.forcewake_count)
-				fw = FORCEWAKE_ALL;
+		for (i = 0; i < FW_DOMAIN_COUNT; i++) {
+			if (dev_priv->uncore.fw_domain[i].wake_count)
+				fw |= 1 << i;
 		}
-
 		if (fw)
 			dev_priv->uncore.funcs.force_wake_get(dev_priv, fw);
 
@@ -363,7 +401,8 @@
 	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
 }
 
-void intel_uncore_early_sanitize(struct drm_device *dev, bool restore_forcewake)
+static void __intel_uncore_early_sanitize(struct drm_device *dev,
+					  bool restore_forcewake)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 
@@ -389,6 +428,12 @@
 	intel_uncore_forcewake_reset(dev, restore_forcewake);
 }
 
+void intel_uncore_early_sanitize(struct drm_device *dev, bool restore_forcewake)
+{
+	__intel_uncore_early_sanitize(dev, restore_forcewake);
+	i915_check_and_clear_faults(dev);
+}
+
 void intel_uncore_sanitize(struct drm_device *dev)
 {
 	/* BIOS often leaves RC6 enabled, but disable it for hw init */
@@ -401,65 +446,76 @@
  * be called at the beginning of the sequence followed by a call to
  * gen6_gt_force_wake_put() at the end of the sequence.
  */
-void gen6_gt_force_wake_get(struct drm_i915_private *dev_priv, int fw_engine)
+void gen6_gt_force_wake_get(struct drm_i915_private *dev_priv,
+			    unsigned fw_domains)
 {
 	unsigned long irqflags;
+	int i;
 
 	if (!dev_priv->uncore.funcs.force_wake_get)
 		return;
 
-	intel_runtime_pm_get(dev_priv);
+	WARN_ON(!pm_runtime_active(&dev_priv->dev->pdev->dev));
 
-	/* Redirect to VLV specific routine */
-	if (IS_VALLEYVIEW(dev_priv->dev))
-		return vlv_force_wake_get(dev_priv, fw_engine);
+	fw_domains &= dev_priv->uncore.fw_domains;
 
 	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
-	if (dev_priv->uncore.forcewake_count++ == 0)
-		dev_priv->uncore.funcs.force_wake_get(dev_priv, FORCEWAKE_ALL);
+
+	for (i = 0; i < FW_DOMAIN_COUNT; i++) {
+		if ((fw_domains & (1 << i)) == 0)
+			continue;
+
+		if (dev_priv->uncore.fw_domain[i].wake_count++)
+			fw_domains &= ~(1 << i);
+	}
+
+	if (fw_domains)
+		dev_priv->uncore.funcs.force_wake_get(dev_priv, fw_domains);
+
 	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
 }
 
 /*
  * see gen6_gt_force_wake_get()
  */
-void gen6_gt_force_wake_put(struct drm_i915_private *dev_priv, int fw_engine)
+void gen6_gt_force_wake_put(struct drm_i915_private *dev_priv,
+			    unsigned fw_domains)
 {
 	unsigned long irqflags;
-	bool delayed = false;
+	int i;
 
 	if (!dev_priv->uncore.funcs.force_wake_put)
 		return;
 
-	/* Redirect to VLV specific routine */
-	if (IS_VALLEYVIEW(dev_priv->dev)) {
-		vlv_force_wake_put(dev_priv, fw_engine);
-		goto out;
-	}
-
+	fw_domains &= dev_priv->uncore.fw_domains;
 
 	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
-	WARN_ON(!dev_priv->uncore.forcewake_count);
 
-	if (--dev_priv->uncore.forcewake_count == 0) {
-		dev_priv->uncore.forcewake_count++;
-		delayed = true;
-		mod_timer_pinned(&dev_priv->uncore.force_wake_timer,
+	for (i = 0; i < FW_DOMAIN_COUNT; i++) {
+		if ((fw_domains & (1 << i)) == 0)
+			continue;
+
+		WARN_ON(!dev_priv->uncore.fw_domain[i].wake_count);
+		if (--dev_priv->uncore.fw_domain[i].wake_count)
+			continue;
+
+		dev_priv->uncore.fw_domain[i].wake_count++;
+		mod_timer_pinned(&dev_priv->uncore.fw_domain[i].timer,
 				 jiffies + 1);
 	}
-	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
 
-out:
-	if (!delayed)
-		intel_runtime_pm_put(dev_priv);
+	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
 }
 
 void assert_force_wake_inactive(struct drm_i915_private *dev_priv)
 {
+	int i;
+
 	if (!dev_priv->uncore.funcs.force_wake_get)
 		return;
 
-	WARN_ON(dev_priv->uncore.forcewake_count > 0);
+	for (i = 0; i < FW_DOMAIN_COUNT; i++)
+		WARN_ON(dev_priv->uncore.fw_domain[i].wake_count > 0);
 }
 
 /* We give fast paths for the really cool registers */
@@ -520,7 +576,7 @@
 	const char *op = read ? "reading" : "writing to";
 	const char *when = before ? "before" : "after";
 
-	if (!i915.mmio_debug)
+	if (!i915_module.mmio_debug)
 		return;
 
 	if (__raw_i915_read32(dev_priv, FPGA_DBG) & FPGA_DBG_RM_NOCLAIM) {
@@ -533,105 +589,128 @@
 static void
 hsw_unclaimed_reg_detect(struct drm_i915_private *dev_priv)
 {
-	if (i915.mmio_debug)
+	if (!i915_module.mmio_debug)
 		return;
 
 	if (__raw_i915_read32(dev_priv, FPGA_DBG) & FPGA_DBG_RM_NOCLAIM) {
-		DRM_ERROR("Unclaimed register detected. Please use the i915.mmio_debug=1 to debug this problem.");
+		DRM_ERROR_ONCE("Unclaimed register detected. Please use the i915.mmio_debug=1 to debug this problem.");
 		__raw_i915_write32(dev_priv, FPGA_DBG, FPGA_DBG_RM_NOCLAIM);
 	}
 }
 
-#define REG_READ_HEADER(x) \
-	unsigned long irqflags; \
+#define GEN2_READ_HEADER(x) \
 	u##x val = 0; \
-	assert_device_not_suspended(dev_priv); \
-	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags)
+	assert_device_not_suspended(dev_priv);
 
-#define REG_READ_FOOTER \
-	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags); \
+#define GEN2_READ_FOOTER \
 	trace_i915_reg_rw(false, reg, val, sizeof(val), trace); \
 	return val
 
-#define __gen4_read(x) \
+#define __gen2_read(x) \
 static u##x \
-gen4_read##x(struct drm_i915_private *dev_priv, off_t reg, bool trace) { \
-	REG_READ_HEADER(x); \
+gen2_read##x(struct drm_i915_private *dev_priv, off_t reg, bool trace) { \
+	GEN2_READ_HEADER(x); \
 	val = __raw_i915_read##x(dev_priv, reg); \
-	REG_READ_FOOTER; \
+	GEN2_READ_FOOTER; \
 }
 
 #define __gen5_read(x) \
 static u##x \
 gen5_read##x(struct drm_i915_private *dev_priv, off_t reg, bool trace) { \
-	REG_READ_HEADER(x); \
+	GEN2_READ_HEADER(x); \
 	ilk_dummy_write(dev_priv); \
 	val = __raw_i915_read##x(dev_priv, reg); \
-	REG_READ_FOOTER; \
+	GEN2_READ_FOOTER; \
+}
+
+__gen5_read(8)
+__gen5_read(16)
+__gen5_read(32)
+__gen5_read(64)
+__gen2_read(8)
+__gen2_read(16)
+__gen2_read(32)
+__gen2_read(64)
+
+#undef __gen5_read
+#undef __gen2_read
+
+#undef GEN2_READ_FOOTER
+#undef GEN2_READ_HEADER
+
+#define GEN6_READ_HEADER(x) \
+	unsigned long irqflags; \
+	u##x val = 0; \
+	assert_device_not_suspended(dev_priv); \
+	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags)
+
+#define GEN6_READ_FOOTER \
+	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags); \
+	trace_i915_reg_rw(false, reg, val, sizeof(val), trace); \
+	return val
+
+static inline void __force_wake_get(struct drm_i915_private *dev_priv,
+				    unsigned fw_domains)
+{
+	int i;
+
+	/* Ideally GCC would be constant-fold and eliminate this loop */
+
+	for (i = 0; i < FW_DOMAIN_COUNT; i++) {
+		if ((fw_domains & (1 << i)) == 0)
+			continue;
+
+		if (dev_priv->uncore.fw_domain[i].wake_count) {
+			fw_domains &= ~(1 << i);
+			continue;
+		}
+
+		dev_priv->uncore.fw_domain[i].wake_count++;
+		mod_timer_pinned(&dev_priv->uncore.fw_domain[i].timer,
+				 jiffies + 1);
+	}
+
+	if (fw_domains)
+		dev_priv->uncore.funcs.force_wake_get(dev_priv, fw_domains);
 }
 
 #define __gen6_read(x) \
 static u##x \
 gen6_read##x(struct drm_i915_private *dev_priv, off_t reg, bool trace) { \
-	REG_READ_HEADER(x); \
+	GEN6_READ_HEADER(x); \
 	hsw_unclaimed_reg_debug(dev_priv, reg, true, true); \
-	if (dev_priv->uncore.forcewake_count == 0 && \
-	    NEEDS_FORCE_WAKE((dev_priv), (reg))) { \
-		dev_priv->uncore.funcs.force_wake_get(dev_priv, \
-						      FORCEWAKE_ALL); \
-		val = __raw_i915_read##x(dev_priv, reg); \
-		dev_priv->uncore.funcs.force_wake_put(dev_priv, \
-						      FORCEWAKE_ALL); \
-	} else { \
-		val = __raw_i915_read##x(dev_priv, reg); \
-	} \
+	if (NEEDS_FORCE_WAKE((dev_priv), (reg))) \
+		__force_wake_get(dev_priv, FORCEWAKE_RENDER); \
+	val = __raw_i915_read##x(dev_priv, reg); \
 	hsw_unclaimed_reg_debug(dev_priv, reg, true, false); \
-	REG_READ_FOOTER; \
+	GEN6_READ_FOOTER; \
 }
 
 #define __vlv_read(x) \
 static u##x \
 vlv_read##x(struct drm_i915_private *dev_priv, off_t reg, bool trace) { \
-	unsigned fwengine = 0; \
-	REG_READ_HEADER(x); \
-	if (FORCEWAKE_VLV_RENDER_RANGE_OFFSET(reg)) { \
-		if (dev_priv->uncore.fw_rendercount == 0) \
-			fwengine = FORCEWAKE_RENDER; \
-	} else if (FORCEWAKE_VLV_MEDIA_RANGE_OFFSET(reg)) { \
-		if (dev_priv->uncore.fw_mediacount == 0) \
-			fwengine = FORCEWAKE_MEDIA; \
-	}  \
-	if (fwengine) \
-		dev_priv->uncore.funcs.force_wake_get(dev_priv, fwengine); \
+	GEN6_READ_HEADER(x); \
+	if (FORCEWAKE_VLV_RENDER_RANGE_OFFSET(reg)) \
+		__force_wake_get(dev_priv, FORCEWAKE_RENDER); \
+	else if (FORCEWAKE_VLV_MEDIA_RANGE_OFFSET(reg)) \
+		__force_wake_get(dev_priv, FORCEWAKE_MEDIA); \
 	val = __raw_i915_read##x(dev_priv, reg); \
-	if (fwengine) \
-		dev_priv->uncore.funcs.force_wake_put(dev_priv, fwengine); \
-	REG_READ_FOOTER; \
+	GEN6_READ_FOOTER; \
 }
 
 #define __chv_read(x) \
 static u##x \
 chv_read##x(struct drm_i915_private *dev_priv, off_t reg, bool trace) { \
-	unsigned fwengine = 0; \
-	REG_READ_HEADER(x); \
-	if (FORCEWAKE_CHV_RENDER_RANGE_OFFSET(reg)) { \
-		if (dev_priv->uncore.fw_rendercount == 0) \
-			fwengine = FORCEWAKE_RENDER; \
-	} else if (FORCEWAKE_CHV_MEDIA_RANGE_OFFSET(reg)) { \
-		if (dev_priv->uncore.fw_mediacount == 0) \
-			fwengine = FORCEWAKE_MEDIA; \
-	} else if (FORCEWAKE_CHV_COMMON_RANGE_OFFSET(reg)) { \
-		if (dev_priv->uncore.fw_rendercount == 0) \
-			fwengine |= FORCEWAKE_RENDER; \
-		if (dev_priv->uncore.fw_mediacount == 0) \
-			fwengine |= FORCEWAKE_MEDIA; \
-	} \
-	if (fwengine) \
-		dev_priv->uncore.funcs.force_wake_get(dev_priv, fwengine); \
+	GEN6_READ_HEADER(x); \
+	if (FORCEWAKE_CHV_RENDER_RANGE_OFFSET(reg)) \
+		__force_wake_get(dev_priv, FORCEWAKE_RENDER); \
+	else if (FORCEWAKE_CHV_MEDIA_RANGE_OFFSET(reg)) \
+		__force_wake_get(dev_priv, FORCEWAKE_MEDIA); \
+	else if (FORCEWAKE_CHV_COMMON_RANGE_OFFSET(reg)) \
+		__force_wake_get(dev_priv, \
+				 FORCEWAKE_RENDER | FORCEWAKE_MEDIA); \
 	val = __raw_i915_read##x(dev_priv, reg); \
-	if (fwengine) \
-		dev_priv->uncore.funcs.force_wake_put(dev_priv, fwengine); \
-	REG_READ_FOOTER; \
+	GEN6_READ_FOOTER; \
 }
 
 __chv_read(8)
@@ -646,54 +725,65 @@
 __gen6_read(16)
 __gen6_read(32)
 __gen6_read(64)
-__gen5_read(8)
-__gen5_read(16)
-__gen5_read(32)
-__gen5_read(64)
-__gen4_read(8)
-__gen4_read(16)
-__gen4_read(32)
-__gen4_read(64)
 
 #undef __chv_read
 #undef __vlv_read
 #undef __gen6_read
-#undef __gen5_read
-#undef __gen4_read
-#undef REG_READ_FOOTER
-#undef REG_READ_HEADER
+#undef GEN6_READ_FOOTER
+#undef GEN6_READ_HEADER
 
-#define REG_WRITE_HEADER \
-	unsigned long irqflags; \
+#define GEN2_WRITE_HEADER \
 	trace_i915_reg_rw(true, reg, val, sizeof(val), trace); \
 	assert_device_not_suspended(dev_priv); \
-	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags)
 
-#define REG_WRITE_FOOTER \
-	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags)
+#define GEN2_WRITE_FOOTER
 
-#define __gen4_write(x) \
+#define __gen2_write(x) \
 static void \
-gen4_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
-	REG_WRITE_HEADER; \
+gen2_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
+	GEN2_WRITE_HEADER; \
 	__raw_i915_write##x(dev_priv, reg, val); \
-	REG_WRITE_FOOTER; \
+	GEN2_WRITE_FOOTER; \
 }
 
 #define __gen5_write(x) \
 static void \
 gen5_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
-	REG_WRITE_HEADER; \
+	GEN2_WRITE_HEADER; \
 	ilk_dummy_write(dev_priv); \
 	__raw_i915_write##x(dev_priv, reg, val); \
-	REG_WRITE_FOOTER; \
+	GEN2_WRITE_FOOTER; \
 }
 
+__gen5_write(8)
+__gen5_write(16)
+__gen5_write(32)
+__gen5_write(64)
+__gen2_write(8)
+__gen2_write(16)
+__gen2_write(32)
+__gen2_write(64)
+
+#undef __gen5_write
+#undef __gen2_write
+
+#undef GEN2_WRITE_FOOTER
+#undef GEN2_WRITE_HEADER
+
+#define GEN6_WRITE_HEADER \
+	unsigned long irqflags; \
+	trace_i915_reg_rw(true, reg, val, sizeof(val), trace); \
+	assert_device_not_suspended(dev_priv); \
+	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags)
+
+#define GEN6_WRITE_FOOTER \
+	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags)
+
 #define __gen6_write(x) \
 static void \
 gen6_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
 	u32 __fifo_ret = 0; \
-	REG_WRITE_HEADER; \
+	GEN6_WRITE_HEADER; \
 	if (NEEDS_FORCE_WAKE((dev_priv), (reg))) { \
 		__fifo_ret = __gen6_gt_wait_for_fifo(dev_priv); \
 	} \
@@ -701,14 +791,14 @@
 	if (unlikely(__fifo_ret)) { \
 		gen6_gt_check_fifodbg(dev_priv); \
 	} \
-	REG_WRITE_FOOTER; \
+	GEN6_WRITE_FOOTER; \
 }
 
 #define __hsw_write(x) \
 static void \
 hsw_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
 	u32 __fifo_ret = 0; \
-	REG_WRITE_HEADER; \
+	GEN6_WRITE_HEADER; \
 	if (NEEDS_FORCE_WAKE((dev_priv), (reg))) { \
 		__fifo_ret = __gen6_gt_wait_for_fifo(dev_priv); \
 	} \
@@ -719,7 +809,7 @@
 	} \
 	hsw_unclaimed_reg_debug(dev_priv, reg, false, false); \
 	hsw_unclaimed_reg_detect(dev_priv); \
-	REG_WRITE_FOOTER; \
+	GEN6_WRITE_FOOTER; \
 }
 
 static const u32 gen8_shadowed_regs[] = {
@@ -746,50 +836,31 @@
 #define __gen8_write(x) \
 static void \
 gen8_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
-	REG_WRITE_HEADER; \
+	GEN6_WRITE_HEADER; \
 	hsw_unclaimed_reg_debug(dev_priv, reg, false, true); \
-	if (reg < 0x40000 && !is_gen8_shadowed(dev_priv, reg)) { \
-		if (dev_priv->uncore.forcewake_count == 0) \
-			dev_priv->uncore.funcs.force_wake_get(dev_priv,	\
-							      FORCEWAKE_ALL); \
-		__raw_i915_write##x(dev_priv, reg, val); \
-		if (dev_priv->uncore.forcewake_count == 0) \
-			dev_priv->uncore.funcs.force_wake_put(dev_priv, \
-							      FORCEWAKE_ALL); \
-	} else { \
-		__raw_i915_write##x(dev_priv, reg, val); \
-	} \
+	if (reg < 0x40000 && !is_gen8_shadowed(dev_priv, reg)) \
+		__force_wake_get(dev_priv, FORCEWAKE_RENDER); \
+	__raw_i915_write##x(dev_priv, reg, val); \
 	hsw_unclaimed_reg_debug(dev_priv, reg, false, false); \
 	hsw_unclaimed_reg_detect(dev_priv); \
-	REG_WRITE_FOOTER; \
+	GEN6_WRITE_FOOTER; \
 }
 
 #define __chv_write(x) \
 static void \
 chv_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
-	unsigned fwengine = 0; \
 	bool shadowed = is_gen8_shadowed(dev_priv, reg); \
-	REG_WRITE_HEADER; \
+	GEN6_WRITE_HEADER; \
 	if (!shadowed) { \
-		if (FORCEWAKE_CHV_RENDER_RANGE_OFFSET(reg)) { \
-			if (dev_priv->uncore.fw_rendercount == 0) \
-				fwengine = FORCEWAKE_RENDER; \
-		} else if (FORCEWAKE_CHV_MEDIA_RANGE_OFFSET(reg)) { \
-			if (dev_priv->uncore.fw_mediacount == 0) \
-				fwengine = FORCEWAKE_MEDIA; \
-		} else if (FORCEWAKE_CHV_COMMON_RANGE_OFFSET(reg)) { \
-			if (dev_priv->uncore.fw_rendercount == 0) \
-				fwengine |= FORCEWAKE_RENDER; \
-			if (dev_priv->uncore.fw_mediacount == 0) \
-				fwengine |= FORCEWAKE_MEDIA; \
-		} \
+		if (FORCEWAKE_CHV_RENDER_RANGE_OFFSET(reg)) \
+			__force_wake_get(dev_priv, FORCEWAKE_RENDER); \
+		else if (FORCEWAKE_CHV_MEDIA_RANGE_OFFSET(reg)) \
+			__force_wake_get(dev_priv, FORCEWAKE_MEDIA); \
+		else if (FORCEWAKE_CHV_COMMON_RANGE_OFFSET(reg)) \
+			__force_wake_get(dev_priv, FORCEWAKE_RENDER | FORCEWAKE_MEDIA); \
 	} \
-	if (fwengine) \
-		dev_priv->uncore.funcs.force_wake_get(dev_priv, fwengine); \
 	__raw_i915_write##x(dev_priv, reg, val); \
-	if (fwengine) \
-		dev_priv->uncore.funcs.force_wake_put(dev_priv, fwengine); \
-	REG_WRITE_FOOTER; \
+	GEN6_WRITE_FOOTER; \
 }
 
 __chv_write(8)
@@ -808,39 +879,49 @@
 __gen6_write(16)
 __gen6_write(32)
 __gen6_write(64)
-__gen5_write(8)
-__gen5_write(16)
-__gen5_write(32)
-__gen5_write(64)
-__gen4_write(8)
-__gen4_write(16)
-__gen4_write(32)
-__gen4_write(64)
 
 #undef __chv_write
 #undef __gen8_write
 #undef __hsw_write
 #undef __gen6_write
-#undef __gen5_write
-#undef __gen4_write
-#undef REG_WRITE_FOOTER
-#undef REG_WRITE_HEADER
+#undef GEN6_WRITE_FOOTER
+#undef GEN6_WRITE_HEADER
+
+#define ASSIGN_WRITE_MMIO_VFUNCS(x) \
+do { \
+	dev_priv->uncore.funcs.mmio_writeb = x##_write8; \
+	dev_priv->uncore.funcs.mmio_writew = x##_write16; \
+	dev_priv->uncore.funcs.mmio_writel = x##_write32; \
+	dev_priv->uncore.funcs.mmio_writeq = x##_write64; \
+} while (0)
+
+#define ASSIGN_READ_MMIO_VFUNCS(x) \
+do { \
+	dev_priv->uncore.funcs.mmio_readb = x##_read8; \
+	dev_priv->uncore.funcs.mmio_readw = x##_read16; \
+	dev_priv->uncore.funcs.mmio_readl = x##_read32; \
+	dev_priv->uncore.funcs.mmio_readq = x##_read64; \
+} while (0)
 
 void intel_uncore_init(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
+	int i;
 
-	setup_timer(&dev_priv->uncore.force_wake_timer,
-		    gen6_force_wake_timer, (unsigned long)dev_priv);
-
-	intel_uncore_early_sanitize(dev, false);
+	__intel_uncore_early_sanitize(dev, false);
 
-	if (IS_VALLEYVIEW(dev)) {
+	if (IS_GEN9(dev)) {
+		dev_priv->uncore.funcs.force_wake_get = __gen9_force_wake_get;
+		dev_priv->uncore.funcs.force_wake_put = __gen9_force_wake_put;
+		dev_priv->uncore.fw_domains = FORCEWAKE_RENDER | FORCEWAKE_MEDIA | FORCEWAKE_BLITTER;
+	} else if (IS_VALLEYVIEW(dev)) {
 		dev_priv->uncore.funcs.force_wake_get = __vlv_force_wake_get;
 		dev_priv->uncore.funcs.force_wake_put = __vlv_force_wake_put;
-	} else if (IS_HASWELL(dev) || IS_GEN8(dev)) {
+		dev_priv->uncore.fw_domains = FORCEWAKE_RENDER | FORCEWAKE_MEDIA;
+	} else if (IS_HASWELL(dev) || IS_BROADWELL(dev)) {
 		dev_priv->uncore.funcs.force_wake_get = __gen7_gt_force_wake_mt_get;
 		dev_priv->uncore.funcs.force_wake_put = __gen7_gt_force_wake_mt_put;
+		dev_priv->uncore.fw_domains = FORCEWAKE_RENDER;
 	} else if (IS_IVYBRIDGE(dev)) {
 		u32 ecobus;
 
@@ -872,86 +953,65 @@
 			dev_priv->uncore.funcs.force_wake_put =
 				__gen6_gt_force_wake_put;
 		}
+		dev_priv->uncore.fw_domains = FORCEWAKE_RENDER;
 	} else if (IS_GEN6(dev)) {
 		dev_priv->uncore.funcs.force_wake_get =
 			__gen6_gt_force_wake_get;
 		dev_priv->uncore.funcs.force_wake_put =
 			__gen6_gt_force_wake_put;
+		dev_priv->uncore.fw_domains = FORCEWAKE_RENDER;
+	}
+
+	for (i = 0; i < FW_DOMAIN_COUNT; i++) {
+		dev_priv->uncore.fw_domain[i].i915 = dev_priv;
+		dev_priv->uncore.fw_domain[i].id = i;
+
+		setup_timer(&dev_priv->uncore.fw_domain[i].timer,
+			    gen6_force_wake_timer,
+			    (unsigned long)&dev_priv->uncore.fw_domain[i]);
 	}
 
 	switch (INTEL_INFO(dev)->gen) {
 	default:
 		if (IS_CHERRYVIEW(dev)) {
-			dev_priv->uncore.funcs.mmio_writeb  = chv_write8;
-			dev_priv->uncore.funcs.mmio_writew  = chv_write16;
-			dev_priv->uncore.funcs.mmio_writel  = chv_write32;
-			dev_priv->uncore.funcs.mmio_writeq  = chv_write64;
-			dev_priv->uncore.funcs.mmio_readb  = chv_read8;
-			dev_priv->uncore.funcs.mmio_readw  = chv_read16;
-			dev_priv->uncore.funcs.mmio_readl  = chv_read32;
-			dev_priv->uncore.funcs.mmio_readq  = chv_read64;
+			ASSIGN_WRITE_MMIO_VFUNCS(chv);
+			ASSIGN_READ_MMIO_VFUNCS(chv);
 
 		} else {
-			dev_priv->uncore.funcs.mmio_writeb  = gen8_write8;
-			dev_priv->uncore.funcs.mmio_writew  = gen8_write16;
-			dev_priv->uncore.funcs.mmio_writel  = gen8_write32;
-			dev_priv->uncore.funcs.mmio_writeq  = gen8_write64;
-			dev_priv->uncore.funcs.mmio_readb  = gen6_read8;
-			dev_priv->uncore.funcs.mmio_readw  = gen6_read16;
-			dev_priv->uncore.funcs.mmio_readl  = gen6_read32;
-			dev_priv->uncore.funcs.mmio_readq  = gen6_read64;
+			ASSIGN_WRITE_MMIO_VFUNCS(gen8);
+			ASSIGN_READ_MMIO_VFUNCS(gen6);
 		}
 		break;
 	case 7:
 	case 6:
 		if (IS_HASWELL(dev)) {
-			dev_priv->uncore.funcs.mmio_writeb  = hsw_write8;
-			dev_priv->uncore.funcs.mmio_writew  = hsw_write16;
-			dev_priv->uncore.funcs.mmio_writel  = hsw_write32;
-			dev_priv->uncore.funcs.mmio_writeq  = hsw_write64;
+			ASSIGN_WRITE_MMIO_VFUNCS(hsw);
 		} else {
-			dev_priv->uncore.funcs.mmio_writeb  = gen6_write8;
-			dev_priv->uncore.funcs.mmio_writew  = gen6_write16;
-			dev_priv->uncore.funcs.mmio_writel  = gen6_write32;
-			dev_priv->uncore.funcs.mmio_writeq  = gen6_write64;
+			ASSIGN_WRITE_MMIO_VFUNCS(gen6);
 		}
 
 		if (IS_VALLEYVIEW(dev)) {
-			dev_priv->uncore.funcs.mmio_readb  = vlv_read8;
-			dev_priv->uncore.funcs.mmio_readw  = vlv_read16;
-			dev_priv->uncore.funcs.mmio_readl  = vlv_read32;
-			dev_priv->uncore.funcs.mmio_readq  = vlv_read64;
+			ASSIGN_READ_MMIO_VFUNCS(vlv);
 		} else {
-			dev_priv->uncore.funcs.mmio_readb  = gen6_read8;
-			dev_priv->uncore.funcs.mmio_readw  = gen6_read16;
-			dev_priv->uncore.funcs.mmio_readl  = gen6_read32;
-			dev_priv->uncore.funcs.mmio_readq  = gen6_read64;
+			ASSIGN_READ_MMIO_VFUNCS(gen6);
 		}
 		break;
 	case 5:
-		dev_priv->uncore.funcs.mmio_writeb  = gen5_write8;
-		dev_priv->uncore.funcs.mmio_writew  = gen5_write16;
-		dev_priv->uncore.funcs.mmio_writel  = gen5_write32;
-		dev_priv->uncore.funcs.mmio_writeq  = gen5_write64;
-		dev_priv->uncore.funcs.mmio_readb  = gen5_read8;
-		dev_priv->uncore.funcs.mmio_readw  = gen5_read16;
-		dev_priv->uncore.funcs.mmio_readl  = gen5_read32;
-		dev_priv->uncore.funcs.mmio_readq  = gen5_read64;
+		ASSIGN_WRITE_MMIO_VFUNCS(gen5);
+		ASSIGN_READ_MMIO_VFUNCS(gen5);
 		break;
 	case 4:
 	case 3:
 	case 2:
-		dev_priv->uncore.funcs.mmio_writeb  = gen4_write8;
-		dev_priv->uncore.funcs.mmio_writew  = gen4_write16;
-		dev_priv->uncore.funcs.mmio_writel  = gen4_write32;
-		dev_priv->uncore.funcs.mmio_writeq  = gen4_write64;
-		dev_priv->uncore.funcs.mmio_readb  = gen4_read8;
-		dev_priv->uncore.funcs.mmio_readw  = gen4_read16;
-		dev_priv->uncore.funcs.mmio_readl  = gen4_read32;
-		dev_priv->uncore.funcs.mmio_readq  = gen4_read64;
+		ASSIGN_WRITE_MMIO_VFUNCS(gen2);
+		ASSIGN_READ_MMIO_VFUNCS(gen2);
 		break;
 	}
+
+	i915_check_and_clear_faults(dev);
 }
+#undef ASSIGN_WRITE_MMIO_VFUNCS
+#undef ASSIGN_READ_MMIO_VFUNCS
 
 void intel_uncore_fini(struct drm_device *dev)
 {
@@ -968,7 +1028,7 @@
 	/* supported gens, 0x10 for 4, 0x30 for 4 and 5, etc. */
 	uint32_t gen_bitmask;
 } whitelist[] = {
-	{ RING_TIMESTAMP(RENDER_RING_BASE), 8, GEN_RANGE(4, 8) },
+	{ RING_TIMESTAMP(RENDER_RING_BASE), 8, GEN_RANGE(4, 9) },
 };
 
 int i915_reg_read_ioctl(struct drm_device *dev,
@@ -1026,9 +1086,6 @@
 	if (args->flags || args->pad)
 		return -EINVAL;
 
-	if (args->ctx_id == DEFAULT_CONTEXT_HANDLE && !capable(CAP_SYS_ADMIN))
-		return -EPERM;
-
 	ret = mutex_lock_interruptible(&dev->struct_mutex);
 	if (ret)
 		return ret;
diff -urN a/drivers/gpu/drm/i915/Kconfig b/drivers/gpu/drm/i915/Kconfig
--- a/drivers/gpu/drm/i915/Kconfig	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/Kconfig	2014-11-22 14:37:49.326700418 -0700
@@ -6,6 +6,7 @@
 	select INTEL_GTT
 	select AGP_INTEL if AGP
 	select INTERVAL_TREE
+	select ZLIB_DEFLATE
 	# we need shmfs for the swappable backing store, and in particular
 	# the shmem_readpage() which depends upon tmpfs
 	select SHMEM
diff -urN a/drivers/gpu/drm/i915/Makefile b/drivers/gpu/drm/i915/Makefile
--- a/drivers/gpu/drm/i915/Makefile	2014-11-21 10:24:10.000000000 -0700
+++ b/drivers/gpu/drm/i915/Makefile	2014-11-22 14:37:49.326700418 -0700
@@ -11,39 +11,48 @@
 	  i915_params.o \
           i915_suspend.o \
 	  i915_sysfs.o \
-	  intel_pm.o
+	  intel_pm.o \
+	  intel_runtime_pm.o
+
 i915-$(CONFIG_COMPAT)   += i915_ioc32.o
 i915-$(CONFIG_DEBUG_FS) += i915_debugfs.o
+i915-$(CONFIG_PERF_EVENTS) += i915_perf.o
 
 # GEM code
 i915-y += i915_cmd_parser.o \
+	  i915_gem.o \
 	  i915_gem_context.o \
 	  i915_gem_render_state.o \
-	  i915_gem_debug.o \
 	  i915_gem_dmabuf.o \
 	  i915_gem_evict.o \
 	  i915_gem_execbuffer.o \
 	  i915_gem_gtt.o \
-	  i915_gem.o \
+	  i915_gem_request.o \
 	  i915_gem_stolen.o \
 	  i915_gem_tiling.o \
 	  i915_gem_userptr.o \
 	  i915_gpu_error.o \
 	  i915_irq.o \
 	  i915_trace_points.o \
+	  intel_lrc.o \
 	  intel_ringbuffer.o \
 	  intel_uncore.o
 
 # autogenerated null render state
 i915-y += intel_renderstate_gen6.o \
 	  intel_renderstate_gen7.o \
-	  intel_renderstate_gen8.o
+	  intel_renderstate_gen8.o \
+	  intel_renderstate_gen9.o
 
 # modesetting core code
-i915-y += intel_bios.o \
+i915-y += intel_audio.o \
+	  intel_bios.o \
 	  intel_display.o \
+	  intel_fifo_underrun.o \
+	  intel_frontbuffer.o \
 	  intel_modes.o \
 	  intel_overlay.o \
+	  intel_psr.o \
 	  intel_sideband.o \
 	  intel_sprite.o
 i915-$(CONFIG_ACPI)		+= intel_acpi.o intel_opregion.o
diff -urN a/ickle_bp.patch b/ickle_bp.patch
--- a/ickle_bp.patch	1969-12-31 17:00:00.000000000 -0700
+++ b/ickle_bp.patch	2014-11-22 09:55:35.134321061 -0700
@@ -0,0 +1,60203 @@
+diff -urN -x arch a/drivers/gpu/drm/drm_crtc.c b/drivers/gpu/drm/drm_crtc.c
+--- a/drivers/gpu/drm/drm_crtc.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/drm_crtc.c	2014-11-21 15:02:56.262381203 -0700
+@@ -3900,7 +3900,7 @@
+ }
+ 
+ static struct drm_property_blob *drm_property_create_blob(struct drm_device *dev, int length,
+-							  void *data)
++							  const void *data)
+ {
+ 	struct drm_property_blob *blob;
+ 	int ret;
+@@ -3981,7 +3981,7 @@
+ }
+ 
+ int drm_mode_connector_set_path_property(struct drm_connector *connector,
+-					 char *path)
++					 const char *path)
+ {
+ 	struct drm_device *dev = connector->dev;
+ 	int ret, size;
+@@ -4011,7 +4011,7 @@
+  * Zero on success, errno on failure.
+  */
+ int drm_mode_connector_update_edid_property(struct drm_connector *connector,
+-					    struct edid *edid)
++					    const struct edid *edid)
+ {
+ 	struct drm_device *dev = connector->dev;
+ 	int ret, size;
+diff -urN -x arch a/drivers/gpu/drm/drm_irq.c b/drivers/gpu/drm/drm_irq.c
+--- a/drivers/gpu/drm/drm_irq.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/drm_irq.c	2014-11-21 21:14:17.903047986 -0700
+@@ -914,16 +914,20 @@
+  */
+ int drm_vblank_get(struct drm_device *dev, int crtc)
+ {
++	struct drm_vblank_crtc *vblank = &dev->vblank[crtc];
+ 	unsigned long irqflags;
+ 	int ret = 0;
+ 
++	if (WARN_ON(crtc >= dev->num_crtcs))
++		return -EINVAL;
++
+ 	spin_lock_irqsave(&dev->vbl_lock, irqflags);
+ 	/* Going from 0->1 means we have to enable interrupts again */
+-	if (atomic_add_return(1, &dev->vblank[crtc].refcount) == 1) {
++	if (atomic_add_return(1, &vblank->refcount) == 1) {
+ 		ret = drm_vblank_enable(dev, crtc);
+ 	} else {
+-		if (!dev->vblank[crtc].enabled) {
+-			atomic_dec(&dev->vblank[crtc].refcount);
++		if (!vblank->enabled) {
++			atomic_dec(&vblank->refcount);
+ 			ret = -EINVAL;
+ 		}
+ 	}
+@@ -989,6 +993,36 @@
+ EXPORT_SYMBOL(drm_crtc_vblank_put);
+ 
+ /**
++ * drm_wait_one_vblank - wait for one vblank
++ * @dev: DRM device
++ * @crtc: crtc index
++ *
++ * This waits for one vblank to pass on @crtc, using the irq driver interfaces.
++ * It is a failure to call this when the vblank irq for @crtc is disabled, e.g.
++ * due to lack of driver support or because the crtc is off.
++ */
++void drm_wait_one_vblank(struct drm_device *dev, int crtc)
++{
++	int ret;
++	u32 last;
++
++	ret = drm_vblank_get(dev, crtc);
++	if (WARN(ret, "vblank not available on crtc %i, ret=%i\n", crtc, ret))
++		return;
++
++	last = drm_vblank_count(dev, crtc);
++
++	ret = wait_event_timeout(dev->vblank[crtc].queue,
++				 last != drm_vblank_count(dev, crtc),
++				 msecs_to_jiffies(100));
++
++	WARN(ret == 0, "vblank wait timed out on crtc %i\n", crtc);
++
++	drm_vblank_put(dev, crtc);
++}
++EXPORT_SYMBOL(drm_wait_one_vblank);
++
++/**
+  * drm_vblank_off - disable vblank events on a CRTC
+  * @dev: DRM device
+  * @crtc: CRTC in question
+@@ -1004,19 +1038,34 @@
+  */
+ void drm_vblank_off(struct drm_device *dev, int crtc)
+ {
++	struct drm_vblank_crtc *vblank = &dev->vblank[crtc];
+ 	struct drm_pending_vblank_event *e, *t;
+ 	struct timeval now;
+ 	unsigned long irqflags;
+ 	unsigned int seq;
+ 
+-	spin_lock_irqsave(&dev->vbl_lock, irqflags);
++	if (WARN_ON(crtc >= dev->num_crtcs))
++		return;
++
++	spin_lock_irqsave(&dev->event_lock, irqflags);
++
++	spin_lock(&dev->vbl_lock);
+ 	vblank_disable_and_save(dev, crtc);
+-	wake_up(&dev->vblank[crtc].queue);
++	wake_up(&vblank->queue);
++
++	/*
++	 * Prevent subsequent drm_vblank_get() from re-enabling
++	 * the vblank interrupt by bumping the refcount.
++	 */
++	if (!vblank->inmodeset) {
++		atomic_inc(&vblank->refcount);
++		vblank->inmodeset = 1;
++	}
++	spin_unlock(&dev->vbl_lock);
+ 
+ 	/* Send any queued vblank events, lest the natives grow disquiet */
+ 	seq = drm_vblank_count_and_time(dev, crtc, &now);
+ 
+-	spin_lock(&dev->event_lock);
+ 	list_for_each_entry_safe(e, t, &dev->vblank_event_list, base.link) {
+ 		if (e->pipe != crtc)
+ 			continue;
+@@ -1027,9 +1076,7 @@
+ 		drm_vblank_put(dev, e->pipe);
+ 		send_vblank_event(dev, e, seq, &now);
+ 	}
+-	spin_unlock(&dev->event_lock);
+-
+-	spin_unlock_irqrestore(&dev->vbl_lock, irqflags);
++	spin_unlock_irqrestore(&dev->event_lock, irqflags);
+ }
+ EXPORT_SYMBOL(drm_vblank_off);
+ 
+@@ -1066,11 +1113,35 @@
+  */
+ void drm_vblank_on(struct drm_device *dev, int crtc)
+ {
++	struct drm_vblank_crtc *vblank = &dev->vblank[crtc];
+ 	unsigned long irqflags;
+ 
++	if (WARN_ON(crtc >= dev->num_crtcs))
++		return;
++
+ 	spin_lock_irqsave(&dev->vbl_lock, irqflags);
+-	/* re-enable interrupts if there's are users left */
+-	if (atomic_read(&dev->vblank[crtc].refcount) != 0)
++	/* Drop our private "prevent drm_vblank_get" refcount */
++	if (vblank->inmodeset) {
++		atomic_dec(&vblank->refcount);
++		vblank->inmodeset = 0;
++	}
++
++	/*
++	 * sample the current counter to avoid random jumps
++	 * when drm_vblank_enable() applies the diff
++	 *
++	 * -1 to make sure user will never see the same
++	 * vblank counter value before and after a modeset
++	 */
++	vblank->last =
++		(dev->driver->get_vblank_counter(dev, crtc) - 1) &
++		dev->max_vblank_count;
++	/*
++	 * re-enable interrupts if there are users left, or the
++	 * user wishes vblank interrupts to be enabled all the time.
++	 */
++	if (atomic_read(&vblank->refcount) != 0 ||
++	    (!dev->vblank_disable_immediate && drm_vblank_offdelay == 0))
+ 		WARN_ON(drm_vblank_enable(dev, crtc));
+ 	spin_unlock_irqrestore(&dev->vbl_lock, irqflags);
+ }
+diff -urN -x arch a/drivers/gpu/drm/drm_pci.c b/drivers/gpu/drm/drm_pci.c
+--- a/drivers/gpu/drm/drm_pci.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/drm_pci.c	2014-11-21 19:10:13.764230497 -0700
+@@ -127,7 +127,7 @@
+ 	return pci_domain_nr(dev->pdev->bus);
+ }
+ 
+-static int drm_pci_set_busid(struct drm_device *dev, struct drm_master *master)
++int drm_pci_set_busid(struct drm_device *dev, struct drm_master *master)
+ {
+ 	int len, ret;
+ 	master->unique_len = 40;
+@@ -155,6 +155,7 @@
+ err:
+ 	return ret;
+ }
++EXPORT_SYMBOL(drm_pci_set_busid);
+ 
+ int drm_pci_set_unique(struct drm_device *dev,
+ 		       struct drm_master *master,
+diff -urN -x arch a/drivers/gpu/drm/i915/dvo_ns2501.c b/drivers/gpu/drm/i915/dvo_ns2501.c
+--- a/drivers/gpu/drm/i915/dvo_ns2501.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/dvo_ns2501.c	2014-11-20 09:53:37.956762838 -0700
+@@ -60,16 +60,297 @@
+ 
+ #define NS2501_REGC 0x0c
+ 
++enum {
++	MODE_640x480,
++	MODE_800x600,
++	MODE_1024x768,
++};
++
++struct ns2501_reg {
++	 uint8_t offset;
++	 uint8_t value;
++};
++
++/*
++ * Magic values based on what the BIOS on
++ * Fujitsu-Siemens Lifebook S6010 programs (1024x768 panel).
++ */
++static const struct ns2501_reg regs_1024x768[][86] = {
++	[MODE_640x480] = {
++		[0] = { .offset = 0x0a, .value = 0x81, },
++		[1] = { .offset = 0x18, .value = 0x07, },
++		[2] = { .offset = 0x19, .value = 0x00, },
++		[3] = { .offset = 0x1a, .value = 0x00, },
++		[4] = { .offset = 0x1b, .value = 0x11, },
++		[5] = { .offset = 0x1c, .value = 0x54, },
++		[6] = { .offset = 0x1d, .value = 0x03, },
++		[7] = { .offset = 0x1e, .value = 0x02, },
++		[8] = { .offset = 0xf3, .value = 0x90, },
++		[9] = { .offset = 0xf9, .value = 0x00, },
++		[10] = { .offset = 0xc1, .value = 0x90, },
++		[11] = { .offset = 0xc2, .value = 0x00, },
++		[12] = { .offset = 0xc3, .value = 0x0f, },
++		[13] = { .offset = 0xc4, .value = 0x03, },
++		[14] = { .offset = 0xc5, .value = 0x16, },
++		[15] = { .offset = 0xc6, .value = 0x00, },
++		[16] = { .offset = 0xc7, .value = 0x02, },
++		[17] = { .offset = 0xc8, .value = 0x02, },
++		[18] = { .offset = 0xf4, .value = 0x00, },
++		[19] = { .offset = 0x80, .value = 0xff, },
++		[20] = { .offset = 0x81, .value = 0x07, },
++		[21] = { .offset = 0x82, .value = 0x3d, },
++		[22] = { .offset = 0x83, .value = 0x05, },
++		[23] = { .offset = 0x94, .value = 0x00, },
++		[24] = { .offset = 0x95, .value = 0x00, },
++		[25] = { .offset = 0x96, .value = 0x05, },
++		[26] = { .offset = 0x97, .value = 0x00, },
++		[27] = { .offset = 0x9a, .value = 0x88, },
++		[28] = { .offset = 0x9b, .value = 0x00, },
++		[29] = { .offset = 0x98, .value = 0x00, },
++		[30] = { .offset = 0x99, .value = 0x00, },
++		[31] = { .offset = 0xf7, .value = 0x88, },
++		[32] = { .offset = 0xf8, .value = 0x0a, },
++		[33] = { .offset = 0x9c, .value = 0x24, },
++		[34] = { .offset = 0x9d, .value = 0x00, },
++		[35] = { .offset = 0x9e, .value = 0x25, },
++		[36] = { .offset = 0x9f, .value = 0x03, },
++		[37] = { .offset = 0xa0, .value = 0x28, },
++		[38] = { .offset = 0xa1, .value = 0x01, },
++		[39] = { .offset = 0xa2, .value = 0x28, },
++		[40] = { .offset = 0xa3, .value = 0x05, },
++		[41] = { .offset = 0xb6, .value = 0x09, },
++		[42] = { .offset = 0xb8, .value = 0x00, },
++		[43] = { .offset = 0xb9, .value = 0xa0, },
++		[44] = { .offset = 0xba, .value = 0x00, },
++		[45] = { .offset = 0xbb, .value = 0x20, },
++		[46] = { .offset = 0x10, .value = 0x00, },
++		[47] = { .offset = 0x11, .value = 0xa0, },
++		[48] = { .offset = 0x12, .value = 0x02, },
++		[49] = { .offset = 0x20, .value = 0x00, },
++		[50] = { .offset = 0x22, .value = 0x00, },
++		[51] = { .offset = 0x23, .value = 0x00, },
++		[52] = { .offset = 0x24, .value = 0x00, },
++		[53] = { .offset = 0x25, .value = 0x00, },
++		[54] = { .offset = 0x8c, .value = 0x10, },
++		[55] = { .offset = 0x8d, .value = 0x02, },
++		[56] = { .offset = 0x8e, .value = 0x10, },
++		[57] = { .offset = 0x8f, .value = 0x00, },
++		[58] = { .offset = 0x90, .value = 0xff, },
++		[59] = { .offset = 0x91, .value = 0x07, },
++		[60] = { .offset = 0x92, .value = 0xa0, },
++		[61] = { .offset = 0x93, .value = 0x02, },
++		[62] = { .offset = 0xa5, .value = 0x00, },
++		[63] = { .offset = 0xa6, .value = 0x00, },
++		[64] = { .offset = 0xa7, .value = 0x00, },
++		[65] = { .offset = 0xa8, .value = 0x00, },
++		[66] = { .offset = 0xa9, .value = 0x04, },
++		[67] = { .offset = 0xaa, .value = 0x70, },
++		[68] = { .offset = 0xab, .value = 0x4f, },
++		[69] = { .offset = 0xac, .value = 0x00, },
++		[70] = { .offset = 0xa4, .value = 0x84, },
++		[71] = { .offset = 0x7e, .value = 0x18, },
++		[72] = { .offset = 0x84, .value = 0x00, },
++		[73] = { .offset = 0x85, .value = 0x00, },
++		[74] = { .offset = 0x86, .value = 0x00, },
++		[75] = { .offset = 0x87, .value = 0x00, },
++		[76] = { .offset = 0x88, .value = 0x00, },
++		[77] = { .offset = 0x89, .value = 0x00, },
++		[78] = { .offset = 0x8a, .value = 0x00, },
++		[79] = { .offset = 0x8b, .value = 0x00, },
++		[80] = { .offset = 0x26, .value = 0x00, },
++		[81] = { .offset = 0x27, .value = 0x00, },
++		[82] = { .offset = 0xad, .value = 0x00, },
++		[83] = { .offset = 0x08, .value = 0x30, }, /* 0x31 */
++		[84] = { .offset = 0x41, .value = 0x00, },
++		[85] = { .offset = 0xc0, .value = 0x05, },
++	},
++	[MODE_800x600] = {
++		[0] = { .offset = 0x0a, .value = 0x81, },
++		[1] = { .offset = 0x18, .value = 0x07, },
++		[2] = { .offset = 0x19, .value = 0x00, },
++		[3] = { .offset = 0x1a, .value = 0x00, },
++		[4] = { .offset = 0x1b, .value = 0x19, },
++		[5] = { .offset = 0x1c, .value = 0x64, },
++		[6] = { .offset = 0x1d, .value = 0x02, },
++		[7] = { .offset = 0x1e, .value = 0x02, },
++		[8] = { .offset = 0xf3, .value = 0x90, },
++		[9] = { .offset = 0xf9, .value = 0x00, },
++		[10] = { .offset = 0xc1, .value = 0xd7, },
++		[11] = { .offset = 0xc2, .value = 0x00, },
++		[12] = { .offset = 0xc3, .value = 0xf8, },
++		[13] = { .offset = 0xc4, .value = 0x03, },
++		[14] = { .offset = 0xc5, .value = 0x1a, },
++		[15] = { .offset = 0xc6, .value = 0x00, },
++		[16] = { .offset = 0xc7, .value = 0x73, },
++		[17] = { .offset = 0xc8, .value = 0x02, },
++		[18] = { .offset = 0xf4, .value = 0x00, },
++		[19] = { .offset = 0x80, .value = 0x27, },
++		[20] = { .offset = 0x81, .value = 0x03, },
++		[21] = { .offset = 0x82, .value = 0x41, },
++		[22] = { .offset = 0x83, .value = 0x05, },
++		[23] = { .offset = 0x94, .value = 0x00, },
++		[24] = { .offset = 0x95, .value = 0x00, },
++		[25] = { .offset = 0x96, .value = 0x05, },
++		[26] = { .offset = 0x97, .value = 0x00, },
++		[27] = { .offset = 0x9a, .value = 0x88, },
++		[28] = { .offset = 0x9b, .value = 0x00, },
++		[29] = { .offset = 0x98, .value = 0x00, },
++		[30] = { .offset = 0x99, .value = 0x00, },
++		[31] = { .offset = 0xf7, .value = 0x88, },
++		[32] = { .offset = 0xf8, .value = 0x06, },
++		[33] = { .offset = 0x9c, .value = 0x23, },
++		[34] = { .offset = 0x9d, .value = 0x00, },
++		[35] = { .offset = 0x9e, .value = 0x25, },
++		[36] = { .offset = 0x9f, .value = 0x03, },
++		[37] = { .offset = 0xa0, .value = 0x28, },
++		[38] = { .offset = 0xa1, .value = 0x01, },
++		[39] = { .offset = 0xa2, .value = 0x28, },
++		[40] = { .offset = 0xa3, .value = 0x05, },
++		[41] = { .offset = 0xb6, .value = 0x09, },
++		[42] = { .offset = 0xb8, .value = 0x30, },
++		[43] = { .offset = 0xb9, .value = 0xc8, },
++		[44] = { .offset = 0xba, .value = 0x00, },
++		[45] = { .offset = 0xbb, .value = 0x20, },
++		[46] = { .offset = 0x10, .value = 0x20, },
++		[47] = { .offset = 0x11, .value = 0xc8, },
++		[48] = { .offset = 0x12, .value = 0x02, },
++		[49] = { .offset = 0x20, .value = 0x00, },
++		[50] = { .offset = 0x22, .value = 0x00, },
++		[51] = { .offset = 0x23, .value = 0x00, },
++		[52] = { .offset = 0x24, .value = 0x00, },
++		[53] = { .offset = 0x25, .value = 0x00, },
++		[54] = { .offset = 0x8c, .value = 0x10, },
++		[55] = { .offset = 0x8d, .value = 0x02, },
++		[56] = { .offset = 0x8e, .value = 0x04, },
++		[57] = { .offset = 0x8f, .value = 0x00, },
++		[58] = { .offset = 0x90, .value = 0xff, },
++		[59] = { .offset = 0x91, .value = 0x07, },
++		[60] = { .offset = 0x92, .value = 0xa0, },
++		[61] = { .offset = 0x93, .value = 0x02, },
++		[62] = { .offset = 0xa5, .value = 0x00, },
++		[63] = { .offset = 0xa6, .value = 0x00, },
++		[64] = { .offset = 0xa7, .value = 0x00, },
++		[65] = { .offset = 0xa8, .value = 0x00, },
++		[66] = { .offset = 0xa9, .value = 0x83, },
++		[67] = { .offset = 0xaa, .value = 0x40, },
++		[68] = { .offset = 0xab, .value = 0x32, },
++		[69] = { .offset = 0xac, .value = 0x00, },
++		[70] = { .offset = 0xa4, .value = 0x80, },
++		[71] = { .offset = 0x7e, .value = 0x18, },
++		[72] = { .offset = 0x84, .value = 0x00, },
++		[73] = { .offset = 0x85, .value = 0x00, },
++		[74] = { .offset = 0x86, .value = 0x00, },
++		[75] = { .offset = 0x87, .value = 0x00, },
++		[76] = { .offset = 0x88, .value = 0x00, },
++		[77] = { .offset = 0x89, .value = 0x00, },
++		[78] = { .offset = 0x8a, .value = 0x00, },
++		[79] = { .offset = 0x8b, .value = 0x00, },
++		[80] = { .offset = 0x26, .value = 0x00, },
++		[81] = { .offset = 0x27, .value = 0x00, },
++		[82] = { .offset = 0xad, .value = 0x00, },
++		[83] = { .offset = 0x08, .value = 0x30, }, /* 0x31 */
++		[84] = { .offset = 0x41, .value = 0x00, },
++		[85] = { .offset = 0xc0, .value = 0x07, },
++	},
++	[MODE_1024x768] = {
++		[0] = { .offset = 0x0a, .value = 0x81, },
++		[1] = { .offset = 0x18, .value = 0x07, },
++		[2] = { .offset = 0x19, .value = 0x00, },
++		[3] = { .offset = 0x1a, .value = 0x00, },
++		[4] = { .offset = 0x1b, .value = 0x11, },
++		[5] = { .offset = 0x1c, .value = 0x54, },
++		[6] = { .offset = 0x1d, .value = 0x03, },
++		[7] = { .offset = 0x1e, .value = 0x02, },
++		[8] = { .offset = 0xf3, .value = 0x90, },
++		[9] = { .offset = 0xf9, .value = 0x00, },
++		[10] = { .offset = 0xc1, .value = 0x90, },
++		[11] = { .offset = 0xc2, .value = 0x00, },
++		[12] = { .offset = 0xc3, .value = 0x0f, },
++		[13] = { .offset = 0xc4, .value = 0x03, },
++		[14] = { .offset = 0xc5, .value = 0x16, },
++		[15] = { .offset = 0xc6, .value = 0x00, },
++		[16] = { .offset = 0xc7, .value = 0x02, },
++		[17] = { .offset = 0xc8, .value = 0x02, },
++		[18] = { .offset = 0xf4, .value = 0x00, },
++		[19] = { .offset = 0x80, .value = 0xff, },
++		[20] = { .offset = 0x81, .value = 0x07, },
++		[21] = { .offset = 0x82, .value = 0x3d, },
++		[22] = { .offset = 0x83, .value = 0x05, },
++		[23] = { .offset = 0x94, .value = 0x00, },
++		[24] = { .offset = 0x95, .value = 0x00, },
++		[25] = { .offset = 0x96, .value = 0x05, },
++		[26] = { .offset = 0x97, .value = 0x00, },
++		[27] = { .offset = 0x9a, .value = 0x88, },
++		[28] = { .offset = 0x9b, .value = 0x00, },
++		[29] = { .offset = 0x98, .value = 0x00, },
++		[30] = { .offset = 0x99, .value = 0x00, },
++		[31] = { .offset = 0xf7, .value = 0x88, },
++		[32] = { .offset = 0xf8, .value = 0x0a, },
++		[33] = { .offset = 0x9c, .value = 0x24, },
++		[34] = { .offset = 0x9d, .value = 0x00, },
++		[35] = { .offset = 0x9e, .value = 0x25, },
++		[36] = { .offset = 0x9f, .value = 0x03, },
++		[37] = { .offset = 0xa0, .value = 0x28, },
++		[38] = { .offset = 0xa1, .value = 0x01, },
++		[39] = { .offset = 0xa2, .value = 0x28, },
++		[40] = { .offset = 0xa3, .value = 0x05, },
++		[41] = { .offset = 0xb6, .value = 0x09, },
++		[42] = { .offset = 0xb8, .value = 0x00, },
++		[43] = { .offset = 0xb9, .value = 0xa0, },
++		[44] = { .offset = 0xba, .value = 0x00, },
++		[45] = { .offset = 0xbb, .value = 0x20, },
++		[46] = { .offset = 0x10, .value = 0x00, },
++		[47] = { .offset = 0x11, .value = 0xa0, },
++		[48] = { .offset = 0x12, .value = 0x02, },
++		[49] = { .offset = 0x20, .value = 0x00, },
++		[50] = { .offset = 0x22, .value = 0x00, },
++		[51] = { .offset = 0x23, .value = 0x00, },
++		[52] = { .offset = 0x24, .value = 0x00, },
++		[53] = { .offset = 0x25, .value = 0x00, },
++		[54] = { .offset = 0x8c, .value = 0x10, },
++		[55] = { .offset = 0x8d, .value = 0x02, },
++		[56] = { .offset = 0x8e, .value = 0x10, },
++		[57] = { .offset = 0x8f, .value = 0x00, },
++		[58] = { .offset = 0x90, .value = 0xff, },
++		[59] = { .offset = 0x91, .value = 0x07, },
++		[60] = { .offset = 0x92, .value = 0xa0, },
++		[61] = { .offset = 0x93, .value = 0x02, },
++		[62] = { .offset = 0xa5, .value = 0x00, },
++		[63] = { .offset = 0xa6, .value = 0x00, },
++		[64] = { .offset = 0xa7, .value = 0x00, },
++		[65] = { .offset = 0xa8, .value = 0x00, },
++		[66] = { .offset = 0xa9, .value = 0x04, },
++		[67] = { .offset = 0xaa, .value = 0x70, },
++		[68] = { .offset = 0xab, .value = 0x4f, },
++		[69] = { .offset = 0xac, .value = 0x00, },
++		[70] = { .offset = 0xa4, .value = 0x84, },
++		[71] = { .offset = 0x7e, .value = 0x18, },
++		[72] = { .offset = 0x84, .value = 0x00, },
++		[73] = { .offset = 0x85, .value = 0x00, },
++		[74] = { .offset = 0x86, .value = 0x00, },
++		[75] = { .offset = 0x87, .value = 0x00, },
++		[76] = { .offset = 0x88, .value = 0x00, },
++		[77] = { .offset = 0x89, .value = 0x00, },
++		[78] = { .offset = 0x8a, .value = 0x00, },
++		[79] = { .offset = 0x8b, .value = 0x00, },
++		[80] = { .offset = 0x26, .value = 0x00, },
++		[81] = { .offset = 0x27, .value = 0x00, },
++		[82] = { .offset = 0xad, .value = 0x00, },
++		[83] = { .offset = 0x08, .value = 0x34, }, /* 0x35 */
++		[84] = { .offset = 0x41, .value = 0x00, },
++		[85] = { .offset = 0xc0, .value = 0x01, },
++	},
++};
++
++static const struct ns2501_reg regs_init[] = {
++	[0] = { .offset = 0x35, .value = 0xff, },
++	[1] = { .offset = 0x34, .value = 0x00, },
++	[2] = { .offset = 0x08, .value = 0x30, },
++};
++
+ struct ns2501_priv {
+-	//I2CDevRec d;
+ 	bool quiet;
+-	int reg_8_shadow;
+-	int reg_8_set;
+-	// Shadow registers for i915
+-	int dvoc;
+-	int pll_a;
+-	int srcdim;
+-	int fw_blc;
++	const struct ns2501_reg *regs;
+ };
+ 
+ #define NSPTR(d) ((NS2501Ptr)(d->DriverPrivate.ptr))
+@@ -205,11 +486,9 @@
+ 		goto out;
+ 	}
+ 	ns->quiet = false;
+-	ns->reg_8_set = 0;
+-	ns->reg_8_shadow =
+-	    NS2501_8_PD | NS2501_8_BPAS | NS2501_8_VEN | NS2501_8_HEN;
+ 
+ 	DRM_DEBUG_KMS("init ns2501 dvo controller successfully!\n");
++
+ 	return true;
+ 
+ out:
+@@ -242,9 +521,9 @@
+ 	 * of the panel in here so we could always accept it
+ 	 * by disabling the scaler.
+ 	 */
+-	if ((mode->hdisplay == 800 && mode->vdisplay == 600) ||
+-	    (mode->hdisplay == 640 && mode->vdisplay == 480) ||
+-	    (mode->hdisplay == 1024 && mode->vdisplay == 768)) {
++	if ((mode->hdisplay == 640 && mode->vdisplay == 480 && mode->clock == 25175) ||
++	    (mode->hdisplay == 800 && mode->vdisplay == 600 && mode->clock == 40000) ||
++	    (mode->hdisplay == 1024 && mode->vdisplay == 768 && mode->clock == 65000)) {
+ 		return MODE_OK;
+ 	} else {
+ 		return MODE_ONE_SIZE;	/* Is this a reasonable error? */
+@@ -255,180 +534,30 @@
+ 			    struct drm_display_mode *mode,
+ 			    struct drm_display_mode *adjusted_mode)
+ {
+-	bool ok;
+-	int retries = 10;
+ 	struct ns2501_priv *ns = (struct ns2501_priv *)(dvo->dev_priv);
++	int mode_idx, i;
+ 
+ 	DRM_DEBUG_KMS
+ 	    ("set mode (hdisplay=%d,htotal=%d,vdisplay=%d,vtotal=%d).\n",
+ 	     mode->hdisplay, mode->htotal, mode->vdisplay, mode->vtotal);
+ 
+-	/*
+-	 * Where do I find the native resolution for which scaling is not required???
+-	 *
+-	 * First trigger the DVO on as otherwise the chip does not appear on the i2c
+-	 * bus.
+-	 */
+-	do {
+-		ok = true;
++	if (mode->hdisplay == 640 && mode->vdisplay == 480)
++		mode_idx = MODE_640x480;
++	else if (mode->hdisplay == 800 && mode->vdisplay == 600)
++		mode_idx = MODE_800x600;
++	else if (mode->hdisplay == 1024 && mode->vdisplay == 768)
++		mode_idx = MODE_1024x768;
++	else
++		return;
++
++	/* Hopefully doing it every time won't hurt... */
++	for (i = 0; i < ARRAY_SIZE(regs_init); i++)
++		ns2501_writeb(dvo, regs_init[i].offset, regs_init[i].value);
+ 
+-		if (mode->hdisplay == 800 && mode->vdisplay == 600) {
+-			/* mode 277 */
+-			ns->reg_8_shadow &= ~NS2501_8_BPAS;
+-			DRM_DEBUG_KMS("switching to 800x600\n");
+-
+-			/*
+-			 * No, I do not know where this data comes from.
+-			 * It is just what the video bios left in the DVO, so
+-			 * I'm just copying it here over.
+-			 * This also means that I cannot support any other modes
+-			 * except the ones supported by the bios.
+-			 */
+-			ok &= ns2501_writeb(dvo, 0x11, 0xc8);	// 0xc7 also works.
+-			ok &= ns2501_writeb(dvo, 0x1b, 0x19);
+-			ok &= ns2501_writeb(dvo, 0x1c, 0x62);	// VBIOS left 0x64 here, but 0x62 works nicer
+-			ok &= ns2501_writeb(dvo, 0x1d, 0x02);
+-
+-			ok &= ns2501_writeb(dvo, 0x34, 0x03);
+-			ok &= ns2501_writeb(dvo, 0x35, 0xff);
+-
+-			ok &= ns2501_writeb(dvo, 0x80, 0x27);
+-			ok &= ns2501_writeb(dvo, 0x81, 0x03);
+-			ok &= ns2501_writeb(dvo, 0x82, 0x41);
+-			ok &= ns2501_writeb(dvo, 0x83, 0x05);
+-
+-			ok &= ns2501_writeb(dvo, 0x8d, 0x02);
+-			ok &= ns2501_writeb(dvo, 0x8e, 0x04);
+-			ok &= ns2501_writeb(dvo, 0x8f, 0x00);
+-
+-			ok &= ns2501_writeb(dvo, 0x90, 0xfe);	/* vertical. VBIOS left 0xff here, but 0xfe works better */
+-			ok &= ns2501_writeb(dvo, 0x91, 0x07);
+-			ok &= ns2501_writeb(dvo, 0x94, 0x00);
+-			ok &= ns2501_writeb(dvo, 0x95, 0x00);
+-
+-			ok &= ns2501_writeb(dvo, 0x96, 0x00);
+-
+-			ok &= ns2501_writeb(dvo, 0x99, 0x00);
+-			ok &= ns2501_writeb(dvo, 0x9a, 0x88);
+-
+-			ok &= ns2501_writeb(dvo, 0x9c, 0x23);	/* Looks like first and last line of the image. */
+-			ok &= ns2501_writeb(dvo, 0x9d, 0x00);
+-			ok &= ns2501_writeb(dvo, 0x9e, 0x25);
+-			ok &= ns2501_writeb(dvo, 0x9f, 0x03);
+-
+-			ok &= ns2501_writeb(dvo, 0xa4, 0x80);
+-
+-			ok &= ns2501_writeb(dvo, 0xb6, 0x00);
+-
+-			ok &= ns2501_writeb(dvo, 0xb9, 0xc8);	/* horizontal? */
+-			ok &= ns2501_writeb(dvo, 0xba, 0x00);	/* horizontal? */
+-
+-			ok &= ns2501_writeb(dvo, 0xc0, 0x05);	/* horizontal? */
+-			ok &= ns2501_writeb(dvo, 0xc1, 0xd7);
+-
+-			ok &= ns2501_writeb(dvo, 0xc2, 0x00);
+-			ok &= ns2501_writeb(dvo, 0xc3, 0xf8);
+-
+-			ok &= ns2501_writeb(dvo, 0xc4, 0x03);
+-			ok &= ns2501_writeb(dvo, 0xc5, 0x1a);
+-
+-			ok &= ns2501_writeb(dvo, 0xc6, 0x00);
+-			ok &= ns2501_writeb(dvo, 0xc7, 0x73);
+-			ok &= ns2501_writeb(dvo, 0xc8, 0x02);
+-
+-		} else if (mode->hdisplay == 640 && mode->vdisplay == 480) {
+-			/* mode 274 */
+-			DRM_DEBUG_KMS("switching to 640x480\n");
+-			/*
+-			 * No, I do not know where this data comes from.
+-			 * It is just what the video bios left in the DVO, so
+-			 * I'm just copying it here over.
+-			 * This also means that I cannot support any other modes
+-			 * except the ones supported by the bios.
+-			 */
+-			ns->reg_8_shadow &= ~NS2501_8_BPAS;
+-
+-			ok &= ns2501_writeb(dvo, 0x11, 0xa0);
+-			ok &= ns2501_writeb(dvo, 0x1b, 0x11);
+-			ok &= ns2501_writeb(dvo, 0x1c, 0x54);
+-			ok &= ns2501_writeb(dvo, 0x1d, 0x03);
+-
+-			ok &= ns2501_writeb(dvo, 0x34, 0x03);
+-			ok &= ns2501_writeb(dvo, 0x35, 0xff);
+-
+-			ok &= ns2501_writeb(dvo, 0x80, 0xff);
+-			ok &= ns2501_writeb(dvo, 0x81, 0x07);
+-			ok &= ns2501_writeb(dvo, 0x82, 0x3d);
+-			ok &= ns2501_writeb(dvo, 0x83, 0x05);
+-
+-			ok &= ns2501_writeb(dvo, 0x8d, 0x02);
+-			ok &= ns2501_writeb(dvo, 0x8e, 0x10);
+-			ok &= ns2501_writeb(dvo, 0x8f, 0x00);
+-
+-			ok &= ns2501_writeb(dvo, 0x90, 0xff);	/* vertical */
+-			ok &= ns2501_writeb(dvo, 0x91, 0x07);
+-			ok &= ns2501_writeb(dvo, 0x94, 0x00);
+-			ok &= ns2501_writeb(dvo, 0x95, 0x00);
+-
+-			ok &= ns2501_writeb(dvo, 0x96, 0x05);
+-
+-			ok &= ns2501_writeb(dvo, 0x99, 0x00);
+-			ok &= ns2501_writeb(dvo, 0x9a, 0x88);
+-
+-			ok &= ns2501_writeb(dvo, 0x9c, 0x24);
+-			ok &= ns2501_writeb(dvo, 0x9d, 0x00);
+-			ok &= ns2501_writeb(dvo, 0x9e, 0x25);
+-			ok &= ns2501_writeb(dvo, 0x9f, 0x03);
+-
+-			ok &= ns2501_writeb(dvo, 0xa4, 0x84);
+-
+-			ok &= ns2501_writeb(dvo, 0xb6, 0x09);
+-
+-			ok &= ns2501_writeb(dvo, 0xb9, 0xa0);	/* horizontal? */
+-			ok &= ns2501_writeb(dvo, 0xba, 0x00);	/* horizontal? */
+-
+-			ok &= ns2501_writeb(dvo, 0xc0, 0x05);	/* horizontal? */
+-			ok &= ns2501_writeb(dvo, 0xc1, 0x90);
+-
+-			ok &= ns2501_writeb(dvo, 0xc2, 0x00);
+-			ok &= ns2501_writeb(dvo, 0xc3, 0x0f);
+-
+-			ok &= ns2501_writeb(dvo, 0xc4, 0x03);
+-			ok &= ns2501_writeb(dvo, 0xc5, 0x16);
+-
+-			ok &= ns2501_writeb(dvo, 0xc6, 0x00);
+-			ok &= ns2501_writeb(dvo, 0xc7, 0x02);
+-			ok &= ns2501_writeb(dvo, 0xc8, 0x02);
+-
+-		} else if (mode->hdisplay == 1024 && mode->vdisplay == 768) {
+-			/* mode 280 */
+-			DRM_DEBUG_KMS("switching to 1024x768\n");
+-			/*
+-			 * This might or might not work, actually. I'm silently
+-			 * assuming here that the native panel resolution is
+-			 * 1024x768. If not, then this leaves the scaler disabled
+-			 * generating a picture that is likely not the expected.
+-			 *
+-			 * Problem is that I do not know where to take the panel
+-			 * dimensions from.
+-			 *
+-			 * Enable the bypass, scaling not required.
+-			 *
+-			 * The scaler registers are irrelevant here....
+-			 *
+-			 */
+-			ns->reg_8_shadow |= NS2501_8_BPAS;
+-			ok &= ns2501_writeb(dvo, 0x37, 0x44);
+-		} else {
+-			/*
+-			 * Data not known. Bummer!
+-			 * Hopefully, the code should not go here
+-			 * as mode_OK delivered no other modes.
+-			 */
+-			ns->reg_8_shadow |= NS2501_8_BPAS;
+-		}
+-		ok &= ns2501_writeb(dvo, NS2501_REG8, ns->reg_8_shadow);
+-	} while (!ok && retries--);
++	ns->regs = regs_1024x768[mode_idx];
++
++	for (i = 0; i < 84; i++)
++		ns2501_writeb(dvo, ns->regs[i].offset, ns->regs[i].value);
+ }
+ 
+ /* set the NS2501 power state */
+@@ -439,60 +568,46 @@
+ 	if (!ns2501_readb(dvo, NS2501_REG8, &ch))
+ 		return false;
+ 
+-	if (ch & NS2501_8_PD)
+-		return true;
+-	else
+-		return false;
++	return ch & NS2501_8_PD;
+ }
+ 
+ /* set the NS2501 power state */
+ static void ns2501_dpms(struct intel_dvo_device *dvo, bool enable)
+ {
+-	bool ok;
+-	int retries = 10;
+ 	struct ns2501_priv *ns = (struct ns2501_priv *)(dvo->dev_priv);
+-	unsigned char ch;
+ 
+ 	DRM_DEBUG_KMS("Trying set the dpms of the DVO to %i\n", enable);
+ 
+-	ch = ns->reg_8_shadow;
++	if (enable) {
++		if (WARN_ON(ns->regs[83].offset != 0x08 ||
++			    ns->regs[84].offset != 0x41 ||
++			    ns->regs[85].offset != 0xc0))
++			return;
+ 
+-	if (enable)
+-		ch |= NS2501_8_PD;
+-	else
+-		ch &= ~NS2501_8_PD;
++		ns2501_writeb(dvo, 0xc0, ns->regs[85].value | 0x08);
+ 
+-	if (ns->reg_8_set == 0 || ns->reg_8_shadow != ch) {
+-		ns->reg_8_set = 1;
+-		ns->reg_8_shadow = ch;
+-
+-		do {
+-			ok = true;
+-			ok &= ns2501_writeb(dvo, NS2501_REG8, ch);
+-			ok &=
+-			    ns2501_writeb(dvo, 0x34,
+-					  enable ? 0x03 : 0x00);
+-			ok &=
+-			    ns2501_writeb(dvo, 0x35,
+-					  enable ? 0xff : 0x00);
+-		} while (!ok && retries--);
+-	}
+-}
++		ns2501_writeb(dvo, 0x41, ns->regs[84].value);
+ 
+-static void ns2501_dump_regs(struct intel_dvo_device *dvo)
+-{
+-	uint8_t val;
++		ns2501_writeb(dvo, 0x34, 0x01);
++		msleep(15);
++
++		ns2501_writeb(dvo, 0x08, 0x35);
++		if (!(ns->regs[83].value & NS2501_8_BPAS))
++			ns2501_writeb(dvo, 0x08, 0x31);
++		msleep(200);
+ 
+-	ns2501_readb(dvo, NS2501_FREQ_LO, &val);
+-	DRM_DEBUG_KMS("NS2501_FREQ_LO: 0x%02x\n", val);
+-	ns2501_readb(dvo, NS2501_FREQ_HI, &val);
+-	DRM_DEBUG_KMS("NS2501_FREQ_HI: 0x%02x\n", val);
+-	ns2501_readb(dvo, NS2501_REG8, &val);
+-	DRM_DEBUG_KMS("NS2501_REG8: 0x%02x\n", val);
+-	ns2501_readb(dvo, NS2501_REG9, &val);
+-	DRM_DEBUG_KMS("NS2501_REG9: 0x%02x\n", val);
+-	ns2501_readb(dvo, NS2501_REGC, &val);
+-	DRM_DEBUG_KMS("NS2501_REGC: 0x%02x\n", val);
++		ns2501_writeb(dvo, 0x34, 0x03);
++
++		ns2501_writeb(dvo, 0xc0, ns->regs[85].value);
++	} else {
++		ns2501_writeb(dvo, 0x34, 0x01);
++		msleep(200);
++
++		ns2501_writeb(dvo, 0x08, 0x34);
++		msleep(15);
++
++		ns2501_writeb(dvo, 0x34, 0x00);
++	}
+ }
+ 
+ static void ns2501_destroy(struct intel_dvo_device *dvo)
+@@ -512,6 +627,5 @@
+ 	.mode_set = ns2501_mode_set,
+ 	.dpms = ns2501_dpms,
+ 	.get_hw_state = ns2501_get_hw_state,
+-	.dump_regs = ns2501_dump_regs,
+ 	.destroy = ns2501_destroy,
+ };
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_cmd_parser.c b/drivers/gpu/drm/i915/i915_cmd_parser.c
+--- a/drivers/gpu/drm/i915/i915_cmd_parser.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/i915_cmd_parser.c	2014-11-20 09:53:37.960762838 -0700
+@@ -73,7 +73,7 @@
+  * those commands required by the parser. This generally works because command
+  * opcode ranges have standard command length encodings. So for commands that
+  * the parser does not need to check, it can easily skip them. This is
+- * implementated via a per-ring length decoding vfunc.
++ * implemented via a per-ring length decoding vfunc.
+  *
+  * Unfortunately, there are a number of commands that do not follow the standard
+  * length encoding for their opcode range, primarily amongst the MI_* commands.
+@@ -138,6 +138,11 @@
+ 			.mask = MI_GLOBAL_GTT,
+ 			.expected = 0,
+ 	      }},						       ),
++	/*
++	 * MI_BATCH_BUFFER_START requires some special handling. It's not
++	 * really a 'skip' action but it doesn't seem like it's worth adding
++	 * a new action. See i915_parse_cmds().
++	 */
+ 	CMD(  MI_BATCH_BUFFER_START,            SMI,   !F,  0xFF,   S  ),
+ };
+ 
+@@ -408,6 +413,8 @@
+ 	REG64(PS_INVOCATION_COUNT),
+ 	REG64(PS_DEPTH_COUNT),
+ 	OACONTROL, /* Only allowed for LRI and SRM. See below. */
++	REG64(MI_PREDICATE_SRC0),
++	REG64(MI_PREDICATE_SRC1),
+ 	GEN7_3DPRIM_END_OFFSET,
+ 	GEN7_3DPRIM_START_VERTEX,
+ 	GEN7_3DPRIM_VERTEX_COUNT,
+@@ -501,7 +508,7 @@
+ 	return 0;
+ }
+ 
+-static bool validate_cmds_sorted(struct intel_engine_cs *ring,
++static bool validate_cmds_sorted(struct intel_engine_cs *engine,
+ 				 const struct drm_i915_cmd_table *cmd_tables,
+ 				 int cmd_table_count)
+ {
+@@ -523,7 +530,7 @@
+ 
+ 			if (curr < previous) {
+ 				DRM_ERROR("CMD: table not sorted ring=%d table=%d entry=%d cmd=0x%08X prev=0x%08X\n",
+-					  ring->id, i, j, curr, previous);
++					  engine->id, i, j, curr, previous);
+ 				ret = false;
+ 			}
+ 
+@@ -555,11 +562,11 @@
+ 	return ret;
+ }
+ 
+-static bool validate_regs_sorted(struct intel_engine_cs *ring)
++static bool validate_regs_sorted(struct intel_engine_cs *engine)
+ {
+-	return check_sorted(ring->id, ring->reg_table, ring->reg_count) &&
+-		check_sorted(ring->id, ring->master_reg_table,
+-			     ring->master_reg_count);
++	return check_sorted(engine->id, engine->reg_table, engine->reg_count) &&
++		check_sorted(engine->id, engine->master_reg_table,
++			     engine->master_reg_count);
+ }
+ 
+ struct cmd_node {
+@@ -583,13 +590,13 @@
+  */
+ #define CMD_HASH_MASK STD_MI_OPCODE_MASK
+ 
+-static int init_hash_table(struct intel_engine_cs *ring,
++static int init_hash_table(struct intel_engine_cs *engine,
+ 			   const struct drm_i915_cmd_table *cmd_tables,
+ 			   int cmd_table_count)
+ {
+ 	int i, j;
+ 
+-	hash_init(ring->cmd_hash);
++	hash_init(engine->cmd_hash);
+ 
+ 	for (i = 0; i < cmd_table_count; i++) {
+ 		const struct drm_i915_cmd_table *table = &cmd_tables[i];
+@@ -604,7 +611,7 @@
+ 				return -ENOMEM;
+ 
+ 			desc_node->desc = desc;
+-			hash_add(ring->cmd_hash, &desc_node->node,
++			hash_add(engine->cmd_hash, &desc_node->node,
+ 				 desc->cmd.value & CMD_HASH_MASK);
+ 		}
+ 	}
+@@ -612,21 +619,21 @@
+ 	return 0;
+ }
+ 
+-static void fini_hash_table(struct intel_engine_cs *ring)
++static void fini_hash_table(struct intel_engine_cs *engine)
+ {
+ 	struct hlist_node *tmp;
+ 	struct cmd_node *desc_node;
+ 	int i;
+ 
+-	hash_for_each_safe(ring->cmd_hash, i, tmp, desc_node, node) {
++	hash_for_each_safe(engine->cmd_hash, i, tmp, desc_node, node) {
+ 		hash_del(&desc_node->node);
+ 		kfree(desc_node);
+ 	}
+ }
+ 
+ /**
+- * i915_cmd_parser_init_ring() - set cmd parser related fields for a ringbuffer
+- * @ring: the ringbuffer to initialize
++ * i915_cmd_parser_init_engine() - set cmd parser related fields for a ringbuffer
++ * @engine: the ringbuffer to initialize
+  *
+  * Optionally initializes fields related to batch buffer command parsing in the
+  * struct intel_engine_cs based on whether the platform requires software
+@@ -634,18 +641,18 @@
+  *
+  * Return: non-zero if initialization fails
+  */
+-int i915_cmd_parser_init_ring(struct intel_engine_cs *ring)
++int i915_cmd_parser_init_engine(struct intel_engine_cs *engine)
+ {
+ 	const struct drm_i915_cmd_table *cmd_tables;
+ 	int cmd_table_count;
+ 	int ret;
+ 
+-	if (!IS_GEN7(ring->dev))
++	if (!IS_GEN7(engine->i915))
+ 		return 0;
+ 
+-	switch (ring->id) {
++	switch (engine->id) {
+ 	case RCS:
+-		if (IS_HASWELL(ring->dev)) {
++		if (IS_HASWELL(engine->i915)) {
+ 			cmd_tables = hsw_render_ring_cmds;
+ 			cmd_table_count =
+ 				ARRAY_SIZE(hsw_render_ring_cmds);
+@@ -654,26 +661,26 @@
+ 			cmd_table_count = ARRAY_SIZE(gen7_render_cmds);
+ 		}
+ 
+-		ring->reg_table = gen7_render_regs;
+-		ring->reg_count = ARRAY_SIZE(gen7_render_regs);
++		engine->reg_table = gen7_render_regs;
++		engine->reg_count = ARRAY_SIZE(gen7_render_regs);
+ 
+-		if (IS_HASWELL(ring->dev)) {
+-			ring->master_reg_table = hsw_master_regs;
+-			ring->master_reg_count = ARRAY_SIZE(hsw_master_regs);
++		if (IS_HASWELL(engine->i915)) {
++			engine->master_reg_table = hsw_master_regs;
++			engine->master_reg_count = ARRAY_SIZE(hsw_master_regs);
+ 		} else {
+-			ring->master_reg_table = ivb_master_regs;
+-			ring->master_reg_count = ARRAY_SIZE(ivb_master_regs);
++			engine->master_reg_table = ivb_master_regs;
++			engine->master_reg_count = ARRAY_SIZE(ivb_master_regs);
+ 		}
+ 
+-		ring->get_cmd_length_mask = gen7_render_get_cmd_length_mask;
++		engine->get_cmd_length_mask = gen7_render_get_cmd_length_mask;
+ 		break;
+ 	case VCS:
+ 		cmd_tables = gen7_video_cmds;
+ 		cmd_table_count = ARRAY_SIZE(gen7_video_cmds);
+-		ring->get_cmd_length_mask = gen7_bsd_get_cmd_length_mask;
++		engine->get_cmd_length_mask = gen7_bsd_get_cmd_length_mask;
+ 		break;
+ 	case BCS:
+-		if (IS_HASWELL(ring->dev)) {
++		if (IS_HASWELL(engine->i915)) {
+ 			cmd_tables = hsw_blt_ring_cmds;
+ 			cmd_table_count = ARRAY_SIZE(hsw_blt_ring_cmds);
+ 		} else {
+@@ -681,70 +688,68 @@
+ 			cmd_table_count = ARRAY_SIZE(gen7_blt_cmds);
+ 		}
+ 
+-		ring->reg_table = gen7_blt_regs;
+-		ring->reg_count = ARRAY_SIZE(gen7_blt_regs);
++		engine->reg_table = gen7_blt_regs;
++		engine->reg_count = ARRAY_SIZE(gen7_blt_regs);
+ 
+-		if (IS_HASWELL(ring->dev)) {
+-			ring->master_reg_table = hsw_master_regs;
+-			ring->master_reg_count = ARRAY_SIZE(hsw_master_regs);
++		if (IS_HASWELL(engine->i915)) {
++			engine->master_reg_table = hsw_master_regs;
++			engine->master_reg_count = ARRAY_SIZE(hsw_master_regs);
+ 		} else {
+-			ring->master_reg_table = ivb_master_regs;
+-			ring->master_reg_count = ARRAY_SIZE(ivb_master_regs);
++			engine->master_reg_table = ivb_master_regs;
++			engine->master_reg_count = ARRAY_SIZE(ivb_master_regs);
+ 		}
+ 
+-		ring->get_cmd_length_mask = gen7_blt_get_cmd_length_mask;
++		engine->get_cmd_length_mask = gen7_blt_get_cmd_length_mask;
+ 		break;
+ 	case VECS:
+ 		cmd_tables = hsw_vebox_cmds;
+ 		cmd_table_count = ARRAY_SIZE(hsw_vebox_cmds);
+ 		/* VECS can use the same length_mask function as VCS */
+-		ring->get_cmd_length_mask = gen7_bsd_get_cmd_length_mask;
++		engine->get_cmd_length_mask = gen7_bsd_get_cmd_length_mask;
+ 		break;
+ 	default:
+-		DRM_ERROR("CMD: cmd_parser_init with unknown ring: %d\n",
+-			  ring->id);
++		DRM_ERROR("CMD: cmd_parser_init with unknown engine: %d\n",
++			  engine->id);
+ 		BUG();
+ 	}
+ 
+-	BUG_ON(!validate_cmds_sorted(ring, cmd_tables, cmd_table_count));
+-	BUG_ON(!validate_regs_sorted(ring));
++	BUG_ON(!validate_cmds_sorted(engine, cmd_tables, cmd_table_count));
++	BUG_ON(!validate_regs_sorted(engine));
+ 
+-	if (hash_empty(ring->cmd_hash)) {
+-		ret = init_hash_table(ring, cmd_tables, cmd_table_count);
+-		if (ret) {
+-			DRM_ERROR("CMD: cmd_parser_init failed!\n");
+-			fini_hash_table(ring);
+-			return ret;
+-		}
++	ret = init_hash_table(engine, cmd_tables, cmd_table_count);
++	if (ret) {
++		DRM_ERROR("CMD: cmd_parser_init failed!\n");
++		fini_hash_table(engine);
++		return ret;
+ 	}
+ 
+-	ring->needs_cmd_parser = true;
++	engine->needs_cmd_parser = true;
+ 
+ 	return 0;
+ }
+ 
+ /**
+- * i915_cmd_parser_fini_ring() - clean up cmd parser related fields
+- * @ring: the ringbuffer to clean up
++ * i915_cmd_parser_fini_engine() - clean up cmd parser related fields
++ * @engine: the ringbuffer to clean up
+  *
+  * Releases any resources related to command parsing that may have been
+- * initialized for the specified ring.
++ * initialized for the specified engine.
+  */
+-void i915_cmd_parser_fini_ring(struct intel_engine_cs *ring)
++void i915_cmd_parser_fini_engine(struct intel_engine_cs *engine)
+ {
+-	if (!ring->needs_cmd_parser)
++	if (!engine->needs_cmd_parser)
+ 		return;
+ 
+-	fini_hash_table(ring);
++	fini_hash_table(engine);
+ }
+ 
+ static const struct drm_i915_cmd_descriptor*
+-find_cmd_in_table(struct intel_engine_cs *ring,
++find_cmd_in_table(struct intel_engine_cs *engine,
+ 		  u32 cmd_header)
+ {
+ 	struct cmd_node *desc_node;
+ 
+-	hash_for_each_possible(ring->cmd_hash, desc_node, node,
++	hash_for_each_possible(engine->cmd_hash, desc_node, node,
+ 			       cmd_header & CMD_HASH_MASK) {
+ 		const struct drm_i915_cmd_descriptor *desc = desc_node->desc;
+ 		u32 masked_cmd = desc->cmd.mask & cmd_header;
+@@ -761,23 +766,23 @@
+  * Returns a pointer to a descriptor for the command specified by cmd_header.
+  *
+  * The caller must supply space for a default descriptor via the default_desc
+- * parameter. If no descriptor for the specified command exists in the ring's
++ * parameter. If no descriptor for the specified command exists in the engine's
+  * command parser tables, this function fills in default_desc based on the
+- * ring's default length encoding and returns default_desc.
++ * engine's default length encoding and returns default_desc.
+  */
+ static const struct drm_i915_cmd_descriptor*
+-find_cmd(struct intel_engine_cs *ring,
++find_cmd(struct intel_engine_cs *engine,
+ 	 u32 cmd_header,
+ 	 struct drm_i915_cmd_descriptor *default_desc)
+ {
+ 	const struct drm_i915_cmd_descriptor *desc;
+ 	u32 mask;
+ 
+-	desc = find_cmd_in_table(ring, cmd_header);
++	desc = find_cmd_in_table(engine, cmd_header);
+ 	if (desc)
+ 		return desc;
+ 
+-	mask = ring->get_cmd_length_mask(cmd_header);
++	mask = engine->get_cmd_length_mask(cmd_header);
+ 	if (!mask)
+ 		return NULL;
+ 
+@@ -834,33 +839,26 @@
+ }
+ 
+ /**
+- * i915_needs_cmd_parser() - should a given ring use software command parsing?
+- * @ring: the ring in question
++ * i915_needs_cmd_parser() - should a given engine use software command parsing?
++ * @engine: the engine in question
+  *
+  * Only certain platforms require software batch buffer command parsing, and
+- * only when enabled via module paramter.
++ * only when enabled via module parameter.
+  *
+- * Return: true if the ring requires software command parsing
++ * Return: true if the engine requires software command parsing
+  */
+-bool i915_needs_cmd_parser(struct intel_engine_cs *ring)
++bool i915_needs_cmd_parser(struct intel_engine_cs *engine)
+ {
+-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
+-
+-	if (!ring->needs_cmd_parser)
++	if (!engine->needs_cmd_parser)
+ 		return false;
+ 
+-	/*
+-	 * XXX: VLV is Gen7 and therefore has cmd_tables, but has PPGTT
+-	 * disabled. That will cause all of the parser's PPGTT checks to
+-	 * fail. For now, disable parsing when PPGTT is off.
+-	 */
+-	if (!dev_priv->mm.aliasing_ppgtt)
++	if (USES_PPGTT(engine->dev))
+ 		return false;
+ 
+-	return (i915.enable_cmd_parser == 1);
++	return (i915_module.enable_cmd_parser == 1);
+ }
+ 
+-static bool check_cmd(const struct intel_engine_cs *ring,
++static bool check_cmd(const struct intel_engine_cs *engine,
+ 		      const struct drm_i915_cmd_descriptor *desc,
+ 		      const u32 *cmd,
+ 		      const bool is_master,
+@@ -890,23 +888,25 @@
+ 		 * OACONTROL writes to only MI_LOAD_REGISTER_IMM commands.
+ 		 */
+ 		if (reg_addr == OACONTROL) {
+-			if (desc->cmd.value == MI_LOAD_REGISTER_MEM)
++			if (desc->cmd.value == MI_LOAD_REGISTER_MEM) {
++				DRM_DEBUG_DRIVER("CMD: Rejected LRM to OACONTROL\n");
+ 				return false;
++			}
+ 
+ 			if (desc->cmd.value == MI_LOAD_REGISTER_IMM(1))
+ 				*oacontrol_set = (cmd[2] != 0);
+ 		}
+ 
+-		if (!valid_reg(ring->reg_table,
+-			       ring->reg_count, reg_addr)) {
++		if (!valid_reg(engine->reg_table,
++			       engine->reg_count, reg_addr)) {
+ 			if (!is_master ||
+-			    !valid_reg(ring->master_reg_table,
+-				       ring->master_reg_count,
++			    !valid_reg(engine->master_reg_table,
++				       engine->master_reg_count,
+ 				       reg_addr)) {
+-				DRM_DEBUG_DRIVER("CMD: Rejected register 0x%08X in command: 0x%08X (ring=%d)\n",
++				DRM_DEBUG_DRIVER("CMD: Rejected register 0x%08X in command: 0x%08X (engine=%d)\n",
+ 						 reg_addr,
+ 						 *cmd,
+-						 ring->id);
++						 engine->id);
+ 				return false;
+ 			}
+ 		}
+@@ -935,11 +935,11 @@
+ 				desc->bits[i].mask;
+ 
+ 			if (dword != desc->bits[i].expected) {
+-				DRM_DEBUG_DRIVER("CMD: Rejected command 0x%08X for bitmask 0x%08X (exp=0x%08X act=0x%08X) (ring=%d)\n",
++				DRM_DEBUG_DRIVER("CMD: Rejected command 0x%08X for bitmask 0x%08X (exp=0x%08X act=0x%08X) (engine=%d)\n",
+ 						 *cmd,
+ 						 desc->bits[i].mask,
+ 						 desc->bits[i].expected,
+-						 dword, ring->id);
++						 dword, engine->id);
+ 				return false;
+ 			}
+ 		}
+@@ -952,7 +952,7 @@
+ 
+ /**
+  * i915_parse_cmds() - parse a submitted batch buffer for privilege violations
+- * @ring: the ring on which the batch is to execute
++ * @engine: the engine on which the batch is to execute
+  * @batch_obj: the batch buffer in question
+  * @batch_start_offset: byte offset in the batch at which execution starts
+  * @is_master: is the submitting process the drm master?
+@@ -960,9 +960,10 @@
+  * Parses the specified batch buffer looking for privilege violations as
+  * described in the overview.
+  *
+- * Return: non-zero if the parser finds violations or otherwise fails
++ * Return: non-zero if the parser finds violations or otherwise fails; -EACCES
++ * if the batch appears legal but should use hardware parsing
+  */
+-int i915_parse_cmds(struct intel_engine_cs *ring,
++int i915_parse_cmds(struct intel_engine_cs *engine,
+ 		    struct drm_i915_gem_object *batch_obj,
+ 		    u32 batch_start_offset,
+ 		    bool is_master)
+@@ -999,7 +1000,7 @@
+ 		if (*cmd == MI_BATCH_BUFFER_END)
+ 			break;
+ 
+-		desc = find_cmd(ring, *cmd, &default_desc);
++		desc = find_cmd(engine, *cmd, &default_desc);
+ 		if (!desc) {
+ 			DRM_DEBUG_DRIVER("CMD: Unrecognized command: 0x%08X\n",
+ 					 *cmd);
+@@ -1007,6 +1008,16 @@
+ 			break;
+ 		}
+ 
++		/*
++		 * If the batch buffer contains a chained batch, return an
++		 * error that tells the caller to abort and dispatch the
++		 * workload as a non-secure batch.
++		 */
++		if (desc->cmd.value == MI_BATCH_BUFFER_START) {
++			ret = -EACCES;
++			break;
++		}
++
+ 		if (desc->flags & CMD_DESC_FIXED)
+ 			length = desc->length.fixed;
+ 		else
+@@ -1021,7 +1032,7 @@
+ 			break;
+ 		}
+ 
+-		if (!check_cmd(ring, desc, cmd, is_master, &oacontrol_set)) {
++		if (!check_cmd(engine, desc, cmd, is_master, &oacontrol_set)) {
+ 			ret = -EINVAL;
+ 			break;
+ 		}
+@@ -1061,6 +1072,8 @@
+ 	 *
+ 	 * 1. Initial version. Checks batches and reports violations, but leaves
+ 	 *    hardware parsing enabled (so does not allow new use cases).
++	 * 2. Allow access to the MI_PREDICATE_SRC0 and
++	 *    MI_PREDICATE_SRC1 registers.
+ 	 */
+-	return 1;
++	return 2;
+ }
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_debugfs.c b/drivers/gpu/drm/i915/i915_debugfs.c
+--- a/drivers/gpu/drm/i915/i915_debugfs.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/i915_debugfs.c	2014-11-20 09:53:37.960762838 -0700
+@@ -116,32 +116,35 @@
+ 
+ static inline const char *get_global_flag(struct drm_i915_gem_object *obj)
+ {
+-	return obj->has_global_gtt_mapping ? "g" : " ";
++	return i915_gem_obj_to_ggtt(obj) ? "g" : " ";
+ }
+ 
+ static void
+ describe_obj(struct seq_file *m, struct drm_i915_gem_object *obj)
+ {
+ 	struct i915_vma *vma;
+-	int pin_count = 0;
++	int pin_count = 0, n;
+ 
+-	seq_printf(m, "%pK: %s%s%s %8zdKiB %02x %02x %u %u %u%s%s%s",
++	seq_printf(m, "%pK: %s%s%s %8zdKiB %02x %02x [",
+ 		   &obj->base,
+ 		   get_pin_flag(obj),
+ 		   get_tiling_flag(obj),
+ 		   get_global_flag(obj),
+ 		   obj->base.size / 1024,
+ 		   obj->base.read_domains,
+-		   obj->base.write_domain,
+-		   obj->last_read_seqno,
+-		   obj->last_write_seqno,
+-		   obj->last_fenced_seqno,
+-		   i915_cache_level_str(obj->cache_level),
++		   obj->base.write_domain);
++	for (n = 0; n < ARRAY_SIZE(obj->last_read); n++)
++		seq_printf(m, " %x",
++			   i915_request_seqno(obj->last_read[n].request));
++	seq_printf(m, " ] %x %x%s%s%s",
++		   i915_request_seqno(obj->last_write.request),
++		   i915_request_seqno(obj->last_fence.request),
++		   i915_cache_level_str(to_i915(obj->base.dev), obj->cache_level),
+ 		   obj->dirty ? " dirty" : "",
+ 		   obj->madv == I915_MADV_DONTNEED ? " purgeable" : "");
+ 	if (obj->base.name)
+ 		seq_printf(m, " (name: %d)", obj->base.name);
+-	list_for_each_entry(vma, &obj->vma_list, vma_link)
++	list_for_each_entry(vma, &obj->vma_list, obj_link)
+ 		if (vma->pin_count > 0)
+ 			pin_count++;
+ 		seq_printf(m, " (pinned x %d)", pin_count);
+@@ -149,7 +152,7 @@
+ 		seq_printf(m, " (display)");
+ 	if (obj->fence_reg != I915_FENCE_REG_NONE)
+ 		seq_printf(m, " (fence: %d)", obj->fence_reg);
+-	list_for_each_entry(vma, &obj->vma_list, vma_link) {
++	list_for_each_entry(vma, &obj->vma_list, obj_link) {
+ 		if (!i915_is_ggtt(vma->vm))
+ 			seq_puts(m, " (pp");
+ 		else
+@@ -168,15 +171,15 @@
+ 		*t = '\0';
+ 		seq_printf(m, " (%s mappable)", s);
+ 	}
+-	if (obj->ring != NULL)
+-		seq_printf(m, " (%s)", obj->ring->name);
++	if (obj->last_write.request)
++		seq_printf(m, " (%s)", obj->last_write.request->engine->name);
+ 	if (obj->frontbuffer_bits)
+ 		seq_printf(m, " (frontbuffer: 0x%03x)", obj->frontbuffer_bits);
+ }
+ 
+ static void describe_ctx(struct seq_file *m, struct intel_context *ctx)
+ {
+-	seq_putc(m, ctx->legacy_hw_ctx.initialized ? 'I' : 'i');
++	seq_putc(m, ctx->ring[RCS].initialized ? 'I' : 'i');
+ 	seq_putc(m, ctx->remap_slice ? 'R' : 'r');
+ 	seq_putc(m, ' ');
+ }
+@@ -321,7 +324,7 @@
+ 		stats->shared += obj->base.size;
+ 
+ 	if (USES_FULL_PPGTT(obj->base.dev)) {
+-		list_for_each_entry(vma, &obj->vma_list, vma_link) {
++		list_for_each_entry(vma, &obj->vma_list, obj_link) {
+ 			struct i915_hw_ppgtt *ppgtt;
+ 
+ 			if (!drm_mm_node_allocated(&vma->node))
+@@ -333,10 +336,10 @@
+ 			}
+ 
+ 			ppgtt = container_of(vma->vm, struct i915_hw_ppgtt, base);
+-			if (ppgtt->ctx && ppgtt->ctx->file_priv != stats->file_priv)
++			if (ppgtt->file_priv != stats->file_priv)
+ 				continue;
+ 
+-			if (obj->ring) /* XXX per-vma statistic */
++			if (obj->active) /* XXX per-vma statistic */
+ 				stats->active += obj->base.size;
+ 			else
+ 				stats->inactive += obj->base.size;
+@@ -346,7 +349,7 @@
+ 	} else {
+ 		if (i915_gem_obj_ggtt_bound(obj)) {
+ 			stats->global += obj->base.size;
+-			if (obj->ring)
++			if (obj->active)
+ 				stats->active += obj->base.size;
+ 			else
+ 				stats->inactive += obj->base.size;
+@@ -476,6 +479,89 @@
+ 	return 0;
+ }
+ 
++static int i915_gem_gtt_contents(struct seq_file *m, struct drm_device *dev)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	gen6_gtt_pte_t __iomem *gtt_entries;
++	gen6_gtt_pte_t scratch_pte;
++	gen6_gtt_pte_t zero[8] = {};
++	int i, j, last_zero = 0;
++	int ret;
++
++	if (INTEL_INFO(dev)->gen < 6)
++		return 0;
++
++	ret = mutex_lock_interruptible(&dev->struct_mutex);
++	if (ret)
++		return ret;
++
++	gtt_entries = (gen6_gtt_pte_t __iomem *)dev_priv->gtt.gsm;
++	scratch_pte = dev_priv->gtt.base.pte_encode(dev_priv->gtt.base.scratch.addr, I915_CACHE_LLC, true, 0);
++	for (i = 0; i < gtt_total_entries(dev_priv->gtt); i += 8) {
++		gen6_gtt_pte_t pte[8];
++		int this_zero;
++
++		for (j = 0; j < 8; j++) {
++			pte[j] = ioread32(&gtt_entries[i+j]);
++			if (pte[j] == scratch_pte)
++				pte[j] = 0;
++			if ((pte[j] & 1) == 0)
++				pte[j] = 0;
++		}
++
++		this_zero = memcmp(pte, zero, sizeof(pte)) == 0;
++		if (last_zero && this_zero) {
++			if (last_zero++ == 1)
++				seq_puts(m, "...\n");
++			continue;
++		}
++
++		seq_printf(m, "[%08x] %08x %08x %08x %08x %08x %08x %08x %08x\n",
++			   i, pte[0], pte[1], pte[2], pte[3], pte[4], pte[5], pte[6], pte[7]);
++		last_zero = this_zero;
++	}
++
++	mutex_unlock(&dev->struct_mutex);
++
++	return 0;
++}
++
++static int obj_rank_by_ggtt(void *priv, struct list_head *A, struct list_head *B)
++{
++	struct drm_i915_gem_object *a = list_entry(A, typeof(*a), obj_exec_link);
++	struct drm_i915_gem_object *b = list_entry(B, typeof(*b), obj_exec_link);
++
++	return i915_gem_obj_ggtt_offset(a) - i915_gem_obj_ggtt_offset(b);
++}
++
++static int i915_gem_aperture_info(struct seq_file *m, void *data)
++{
++	struct drm_info_node *node = m->private;
++	struct drm_i915_gem_get_aperture arg;
++	int ret;
++
++	ret = i915_gem_get_aperture_ioctl(node->minor->dev, &arg, NULL);
++	if (ret)
++		return ret;
++
++	seq_printf(m, "Total size of the GTT: %llu bytes\n",
++		   arg.aper_size);
++	seq_printf(m, "Available space in the GTT: %llu bytes\n",
++		   arg.aper_available_size);
++	seq_printf(m, "Total size of the mappable aperture: %llu bytes\n",
++		   arg.map_total_size);
++	seq_printf(m, "Available space in the mappable aperture: %llu bytes\n",
++		   arg.map_available_size);
++	seq_printf(m, "Single largest space in the mappable aperture: %llu bytes\n",
++		   arg.map_largest_size);
++	seq_printf(m, "Available space for fences: %llu bytes\n",
++		   arg.fence_available_size);
++	seq_printf(m, "Single largest fence available: %llu bytes\n",
++		   arg.fence_largest_size);
++
++	return 0;
++}
++
+ static int i915_gem_gtt_info(struct seq_file *m, void *data)
+ {
+ 	struct drm_info_node *node = m->private;
+@@ -483,6 +569,7 @@
+ 	uintptr_t list = (uintptr_t) node->info_ent->data;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct drm_i915_gem_object *obj;
++	struct list_head sorted;
+ 	size_t total_obj_size, total_gtt_size;
+ 	int count, ret;
+ 
+@@ -490,11 +577,27 @@
+ 	if (ret)
+ 		return ret;
+ 
++	INIT_LIST_HEAD(&sorted);
++
+ 	total_obj_size = total_gtt_size = count = 0;
+ 	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list) {
+-		if (list == PINNED_LIST && !i915_gem_obj_is_pinned(obj))
++		struct i915_vma *vma = i915_gem_obj_to_ggtt(obj);
++
++		if (vma == NULL)
+ 			continue;
+ 
++		if (list == PINNED_LIST && vma->pin_count == 0)
++			continue;
++
++		list_add(&obj->obj_exec_link, &sorted);
++	}
++
++	list_sort(NULL, &sorted, obj_rank_by_ggtt);
++
++	while (!list_empty(&sorted)) {
++		obj = list_first_entry(&sorted, typeof(*obj), obj_exec_link);
++		list_del_init(&obj->obj_exec_link);
++
+ 		seq_puts(m, "   ");
+ 		describe_obj(m, obj);
+ 		seq_putc(m, '\n');
+@@ -508,14 +611,17 @@
+ 	seq_printf(m, "Total %d objects, %zu bytes, %zu GTT size\n",
+ 		   count, total_obj_size, total_gtt_size);
+ 
+-	return 0;
++	if (list == PINNED_LIST)
++		return 0;
++
++	return i915_gem_gtt_contents(m, dev);
+ }
+ 
+ static int i915_gem_pageflip_info(struct seq_file *m, void *data)
+ {
+ 	struct drm_info_node *node = m->private;
+ 	struct drm_device *dev = node->minor->dev;
+-	unsigned long flags;
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct intel_crtc *crtc;
+ 	int ret;
+ 
+@@ -528,12 +634,14 @@
+ 		const char plane = plane_name(crtc->plane);
+ 		struct intel_unpin_work *work;
+ 
+-		spin_lock_irqsave(&dev->event_lock, flags);
++		spin_lock_irq(&dev->event_lock);
+ 		work = crtc->unpin_work;
+ 		if (work == NULL) {
+ 			seq_printf(m, "No flip due on pipe %c (plane %c)\n",
+ 				   pipe, plane);
+ 		} else {
++			u32 addr;
++
+ 			if (atomic_read(&work->pending) < INTEL_FLIP_COMPLETE) {
+ 				seq_printf(m, "Flip queued on pipe %c (plane %c)\n",
+ 					   pipe, plane);
+@@ -541,26 +649,38 @@
+ 				seq_printf(m, "Flip pending (waiting for vsync) on pipe %c (plane %c)\n",
+ 					   pipe, plane);
+ 			}
++			if (work->flip_queued_request) {
++				struct i915_gem_request *rq =
++					work->flip_queued_request;
++				seq_printf(m, "Flip queued on %s at seqno %u, next seqno %u [current breadcrumb %u], completed? %d\n",
++					   rq->engine->name,
++					   rq->seqno, rq->i915->next_seqno,
++					   intel_engine_get_seqno(rq->engine),
++					   __i915_request_complete__wa(rq));
++			} else
++				seq_printf(m, "Flip not associated with any ring\n");
++			seq_printf(m, "Flip queued on frame %d, (was ready on frame %d), now %d\n",
++				   work->flip_queued_vblank,
++				   work->flip_ready_vblank,
++				   drm_vblank_count(dev, crtc->pipe));
+ 			if (work->enable_stall_check)
+ 				seq_puts(m, "Stall check enabled, ");
+ 			else
+ 				seq_puts(m, "Stall check waiting for page flip ioctl, ");
+ 			seq_printf(m, "%d prepares\n", atomic_read(&work->pending));
+ 
+-			if (work->old_fb_obj) {
+-				struct drm_i915_gem_object *obj = work->old_fb_obj;
+-				if (obj)
+-					seq_printf(m, "Old framebuffer gtt_offset 0x%08lx\n",
+-						   i915_gem_obj_ggtt_offset(obj));
+-			}
++			if (INTEL_INFO(dev)->gen >= 4)
++				addr = I915_HI_DISPBASE(I915_READ(DSPSURF(crtc->plane)));
++			else
++				addr = I915_READ(DSPADDR(crtc->plane));
++			seq_printf(m, "Current scanout address 0x%08x\n", addr);
++
+ 			if (work->pending_flip_obj) {
+-				struct drm_i915_gem_object *obj = work->pending_flip_obj;
+-				if (obj)
+-					seq_printf(m, "New framebuffer gtt_offset 0x%08lx\n",
+-						   i915_gem_obj_ggtt_offset(obj));
++				seq_printf(m, "New framebuffer address 0x%08lx\n", (long)work->gtt_offset);
++				seq_printf(m, "MMIO update completed? %d\n",  addr == work->gtt_offset);
+ 			}
+ 		}
+-		spin_unlock_irqrestore(&dev->event_lock, flags);
++		spin_unlock_irq(&dev->event_lock);
+ 	}
+ 
+ 	mutex_unlock(&dev->struct_mutex);
+@@ -573,8 +693,8 @@
+ 	struct drm_info_node *node = m->private;
+ 	struct drm_device *dev = node->minor->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring;
+-	struct drm_i915_gem_request *gem_request;
++	struct intel_engine_cs *engine;
++	struct i915_gem_request *rq;
+ 	int ret, count, i;
+ 
+ 	ret = mutex_lock_interruptible(&dev->struct_mutex);
+@@ -582,17 +702,15 @@
+ 		return ret;
+ 
+ 	count = 0;
+-	for_each_ring(ring, dev_priv, i) {
+-		if (list_empty(&ring->request_list))
++	for_each_engine(engine, dev_priv, i) {
++		if (list_empty(&engine->requests))
+ 			continue;
+ 
+-		seq_printf(m, "%s requests:\n", ring->name);
+-		list_for_each_entry(gem_request,
+-				    &ring->request_list,
+-				    list) {
++		seq_printf(m, "%s requests:\n", engine->name);
++		list_for_each_entry(rq, &engine->requests, engine_link) {
+ 			seq_printf(m, "    %d @ %d\n",
+-				   gem_request->seqno,
+-				   (int) (jiffies - gem_request->emitted_jiffies));
++				   rq->seqno,
++				   rq->emitted_jiffies ? (int)(jiffies - rq->emitted_jiffies) : 0);
+ 		}
+ 		count++;
+ 	}
+@@ -604,13 +722,17 @@
+ 	return 0;
+ }
+ 
+-static void i915_ring_seqno_info(struct seq_file *m,
+-				 struct intel_engine_cs *ring)
++static void i915_engine_seqno_info(struct seq_file *m,
++				   struct intel_engine_cs *engine)
+ {
+-	if (ring->get_seqno) {
+-		seq_printf(m, "Current sequence (%s): %u\n",
+-			   ring->name, ring->get_seqno(ring, false));
+-	}
++	seq_printf(m, "Current sequence (%s): seqno=%u, tag=%u [last breadcrumb %u, last request %u], next seqno=%u, next tag=%u\n",
++		   engine->name,
++		   intel_engine_get_seqno(engine),
++		   engine->tag,
++		   engine->breadcrumb[engine->id],
++		   engine->last_request ? engine->last_request->seqno : 0,
++		   engine->i915->next_seqno,
++		   engine->next_tag);
+ }
+ 
+ static int i915_gem_seqno_info(struct seq_file *m, void *data)
+@@ -618,7 +740,7 @@
+ 	struct drm_info_node *node = m->private;
+ 	struct drm_device *dev = node->minor->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring;
++	struct intel_engine_cs *engine;
+ 	int ret, i;
+ 
+ 	ret = mutex_lock_interruptible(&dev->struct_mutex);
+@@ -626,8 +748,8 @@
+ 		return ret;
+ 	intel_runtime_pm_get(dev_priv);
+ 
+-	for_each_ring(ring, dev_priv, i)
+-		i915_ring_seqno_info(m, ring);
++	for_each_engine(engine, dev_priv, i)
++		i915_engine_seqno_info(m, engine);
+ 
+ 	intel_runtime_pm_put(dev_priv);
+ 	mutex_unlock(&dev->struct_mutex);
+@@ -641,7 +763,7 @@
+ 	struct drm_info_node *node = m->private;
+ 	struct drm_device *dev = node->minor->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring;
++	struct intel_engine_cs *engine;
+ 	int ret, i, pipe;
+ 
+ 	ret = mutex_lock_interruptible(&dev->struct_mutex);
+@@ -650,7 +772,6 @@
+ 	intel_runtime_pm_get(dev_priv);
+ 
+ 	if (IS_CHERRYVIEW(dev)) {
+-		int i;
+ 		seq_printf(m, "Master Interrupt Control:\t%08x\n",
+ 			   I915_READ(GEN8_MASTER_IRQ));
+ 
+@@ -662,7 +783,7 @@
+ 			   I915_READ(VLV_IIR_RW));
+ 		seq_printf(m, "Display IMR:\t%08x\n",
+ 			   I915_READ(VLV_IMR));
+-		for_each_pipe(pipe)
++		for_each_pipe(dev_priv, pipe)
+ 			seq_printf(m, "Pipe %c stat:\t%08x\n",
+ 				   pipe_name(pipe),
+ 				   I915_READ(PIPESTAT(pipe)));
+@@ -702,7 +823,13 @@
+ 				   i, I915_READ(GEN8_GT_IER(i)));
+ 		}
+ 
+-		for_each_pipe(pipe) {
++		for_each_pipe(dev_priv, pipe) {
++			if (!intel_display_power_is_enabled(dev_priv,
++						POWER_DOMAIN_PIPE(pipe))) {
++				seq_printf(m, "Pipe %c power disabled\n",
++					   pipe_name(pipe));
++				continue;
++			}
+ 			seq_printf(m, "Pipe %c IMR:\t%08x\n",
+ 				   pipe_name(pipe),
+ 				   I915_READ(GEN8_DE_PIPE_IMR(pipe)));
+@@ -743,7 +870,7 @@
+ 			   I915_READ(VLV_IIR_RW));
+ 		seq_printf(m, "Display IMR:\t%08x\n",
+ 			   I915_READ(VLV_IMR));
+-		for_each_pipe(pipe)
++		for_each_pipe(dev_priv, pipe)
+ 			seq_printf(m, "Pipe %c stat:\t%08x\n",
+ 				   pipe_name(pipe),
+ 				   I915_READ(PIPESTAT(pipe)));
+@@ -779,7 +906,7 @@
+ 			   I915_READ(IIR));
+ 		seq_printf(m, "Interrupt mask:      %08x\n",
+ 			   I915_READ(IMR));
+-		for_each_pipe(pipe)
++		for_each_pipe(dev_priv, pipe)
+ 			seq_printf(m, "Pipe %c stat:         %08x\n",
+ 				   pipe_name(pipe),
+ 				   I915_READ(PIPESTAT(pipe)));
+@@ -803,13 +930,13 @@
+ 		seq_printf(m, "Graphics Interrupt mask:		%08x\n",
+ 			   I915_READ(GTIMR));
+ 	}
+-	for_each_ring(ring, dev_priv, i) {
++	for_each_engine(engine, dev_priv, i) {
+ 		if (INTEL_INFO(dev)->gen >= 6) {
+ 			seq_printf(m,
+ 				   "Graphics Interrupt mask (%s):	%08x\n",
+-				   ring->name, I915_READ_IMR(ring));
++				   engine->name, I915_READ_IMR(engine));
+ 		}
+-		i915_ring_seqno_info(m, ring);
++		i915_engine_seqno_info(m, engine);
+ 	}
+ 	intel_runtime_pm_put(dev_priv);
+ 	mutex_unlock(&dev->struct_mutex);
+@@ -851,12 +978,12 @@
+ 	struct drm_info_node *node = m->private;
+ 	struct drm_device *dev = node->minor->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring;
++	struct intel_engine_cs *engine;
+ 	const u32 *hws;
+ 	int i;
+ 
+-	ring = &dev_priv->ring[(uintptr_t)node->info_ent->data];
+-	hws = ring->status_page.page_addr;
++	engine = &dev_priv->engine[(uintptr_t)node->info_ent->data];
++	hws = engine->status_page.page_addr;
+ 	if (hws == NULL)
+ 		return 0;
+ 
+@@ -927,7 +1054,7 @@
+ 	ssize_t ret_count = 0;
+ 	int ret;
+ 
+-	ret = i915_error_state_buf_init(&error_str, count, *pos);
++	ret = i915_error_state_buf_init(&error_str, to_i915(error_priv->dev), count, *pos);
+ 	if (ret)
+ 		return ret;
+ 
+@@ -980,7 +1107,7 @@
+ 	struct drm_device *dev = data;
+ 	int ret;
+ 
+-	ret = mutex_lock_interruptible(&dev->struct_mutex);
++	ret = i915_mutex_lock_interruptible(dev);
+ 	if (ret)
+ 		return ret;
+ 
+@@ -1024,6 +1151,7 @@
+ 		u32 rpstat, cagf, reqf;
+ 		u32 rpupei, rpcurup, rpprevup;
+ 		u32 rpdownei, rpcurdown, rpprevdown;
++		u32 pm_ier, pm_imr, pm_isr, pm_iir, pm_mask;
+ 		int max_freq;
+ 
+ 		/* RPSTAT1 is in the GT power well */
+@@ -1061,12 +1189,21 @@
+ 		gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
+ 		mutex_unlock(&dev->struct_mutex);
+ 
++		if (IS_GEN6(dev) || IS_GEN7(dev)) {
++			pm_ier = I915_READ(GEN6_PMIER);
++			pm_imr = I915_READ(GEN6_PMIMR);
++			pm_isr = I915_READ(GEN6_PMISR);
++			pm_iir = I915_READ(GEN6_PMIIR);
++			pm_mask = I915_READ(GEN6_PMINTRMSK);
++		} else {
++			pm_ier = I915_READ(GEN8_GT_IER(2));
++			pm_imr = I915_READ(GEN8_GT_IMR(2));
++			pm_isr = I915_READ(GEN8_GT_ISR(2));
++			pm_iir = I915_READ(GEN8_GT_IIR(2));
++			pm_mask = I915_READ(GEN6_PMINTRMSK);
++		}
+ 		seq_printf(m, "PM IER=0x%08x IMR=0x%08x ISR=0x%08x IIR=0x%08x, MASK=0x%08x\n",
+-			   I915_READ(GEN6_PMIER),
+-			   I915_READ(GEN6_PMIMR),
+-			   I915_READ(GEN6_PMISR),
+-			   I915_READ(GEN6_PMIIR),
+-			   I915_READ(GEN6_PMINTRMSK));
++			   pm_ier, pm_imr, pm_isr, pm_iir, pm_mask);
+ 		seq_printf(m, "GT_PERF_STATUS: 0x%08x\n", gt_perf_status);
+ 		seq_printf(m, "Render p-state ratio: %d\n",
+ 			   (gt_perf_status & 0xff00) >> 8);
+@@ -1205,14 +1342,37 @@
+ 	return 0;
+ }
+ 
+-static int vlv_drpc_info(struct seq_file *m)
++static int i915_gen6_forcewake_count_info(struct seq_file *m, void *data)
+ {
++	struct drm_info_node *node = m->private;
++	struct drm_device *dev = node->minor->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	const char *domain_names[] = {
++		"render",
++		"media",
++	};
++	int i;
++
++	spin_lock_irq(&dev_priv->uncore.lock);
++	for (i = 0; i < FW_DOMAIN_COUNT; i++) {
++		if ((dev_priv->uncore.fw_domains & (1 << i)) == 0)
++			continue;
+ 
++		seq_printf(m, "%s.wake_count = %u\n",
++			   domain_names[i],
++			   dev_priv->uncore.fw_domain[i].wake_count);
++	}
++	spin_unlock_irq(&dev_priv->uncore.lock);
++
++	return 0;
++}
++
++static int vlv_drpc_info(struct seq_file *m)
++{
+ 	struct drm_info_node *node = m->private;
+ 	struct drm_device *dev = node->minor->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	u32 rpmodectl1, rcctl1;
+-	unsigned fw_rendercount = 0, fw_mediacount = 0;
+ 
+ 	intel_runtime_pm_get(dev_priv);
+ 
+@@ -1245,22 +1405,11 @@
+ 	seq_printf(m, "Media RC6 residency since boot: %u\n",
+ 		   I915_READ(VLV_GT_MEDIA_RC6));
+ 
+-	spin_lock_irq(&dev_priv->uncore.lock);
+-	fw_rendercount = dev_priv->uncore.fw_rendercount;
+-	fw_mediacount = dev_priv->uncore.fw_mediacount;
+-	spin_unlock_irq(&dev_priv->uncore.lock);
+-
+-	seq_printf(m, "Forcewake Render Count = %u\n", fw_rendercount);
+-	seq_printf(m, "Forcewake Media Count = %u\n", fw_mediacount);
+-
+-
+-	return 0;
++	return i915_gen6_forcewake_count_info(m, NULL);
+ }
+ 
+-
+ static int gen6_drpc_info(struct seq_file *m)
+ {
+-
+ 	struct drm_info_node *node = m->private;
+ 	struct drm_device *dev = node->minor->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+@@ -1274,7 +1423,7 @@
+ 	intel_runtime_pm_get(dev_priv);
+ 
+ 	spin_lock_irq(&dev_priv->uncore.lock);
+-	forcewake_count = dev_priv->uncore.forcewake_count;
++	forcewake_count = dev_priv->uncore.fw_domain[FW_DOMAIN_RENDER].wake_count;
+ 	spin_unlock_irq(&dev_priv->uncore.lock);
+ 
+ 	if (forcewake_count) {
+@@ -1365,7 +1514,7 @@
+ 
+ 	if (IS_VALLEYVIEW(dev))
+ 		return vlv_drpc_info(m);
+-	else if (IS_GEN6(dev) || IS_GEN7(dev))
++	else if (INTEL_INFO(dev)->gen >= 6)
+ 		return gen6_drpc_info(m);
+ 	else
+ 		return ironlake_drpc_info(m);
+@@ -1433,6 +1582,47 @@
+ 	return 0;
+ }
+ 
++static int i915_fbc_fc_get(void *data, u64 *val)
++{
++	struct drm_device *dev = data;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++
++	if (INTEL_INFO(dev)->gen < 7 || !HAS_FBC(dev))
++		return -ENODEV;
++
++	drm_modeset_lock_all(dev);
++	*val = dev_priv->fbc.false_color;
++	drm_modeset_unlock_all(dev);
++
++	return 0;
++}
++
++static int i915_fbc_fc_set(void *data, u64 val)
++{
++	struct drm_device *dev = data;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	u32 reg;
++
++	if (INTEL_INFO(dev)->gen < 7 || !HAS_FBC(dev))
++		return -ENODEV;
++
++	drm_modeset_lock_all(dev);
++
++	reg = I915_READ(ILK_DPFC_CONTROL);
++	dev_priv->fbc.false_color = val;
++
++	I915_WRITE(ILK_DPFC_CONTROL, val ?
++		   (reg | FBC_CTL_FALSE_COLOR) :
++		   (reg & ~FBC_CTL_FALSE_COLOR));
++
++	drm_modeset_unlock_all(dev);
++	return 0;
++}
++
++DEFINE_SIMPLE_ATTRIBUTE(i915_fbc_fc_fops,
++			i915_fbc_fc_get, i915_fbc_fc_set,
++			"%llu\n");
++
+ static int i915_ips_status(struct seq_file *m, void *unused)
+ {
+ 	struct drm_info_node *node = m->private;
+@@ -1447,7 +1637,7 @@
+ 	intel_runtime_pm_get(dev_priv);
+ 
+ 	seq_printf(m, "Enabled by kernel parameter: %s\n",
+-		   yesno(i915.enable_ips));
++		   yesno(i915_module.enable_ips));
+ 
+ 	if (INTEL_INFO(dev)->gen >= 8) {
+ 		seq_puts(m, "Currently: unknown\n");
+@@ -1630,12 +1820,18 @@
+ 	return 0;
+ }
+ 
++static void describe_ring(struct seq_file *m, struct intel_ringbuffer *ring)
++{
++	seq_printf(m, " (ringbuffer, space: %d, head: %u, tail: %u, last head: %d)",
++		   ring->space, ring->head, ring->tail, ring->retired_head);
++}
++
+ static int i915_context_status(struct seq_file *m, void *unused)
+ {
+ 	struct drm_info_node *node = m->private;
+ 	struct drm_device *dev = node->minor->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring;
++	struct intel_engine_cs *engine;
+ 	struct intel_context *ctx;
+ 	int ret, i;
+ 
+@@ -1649,23 +1845,28 @@
+ 		seq_putc(m, '\n');
+ 	}
+ 
+-	if (dev_priv->ips.renderctx) {
+-		seq_puts(m, "render context ");
+-		describe_obj(m, dev_priv->ips.renderctx);
+-		seq_putc(m, '\n');
+-	}
+-
+ 	list_for_each_entry(ctx, &dev_priv->context_list, link) {
+-		if (ctx->legacy_hw_ctx.rcs_state == NULL)
+-			continue;
+-
+ 		seq_puts(m, "HW context ");
+ 		describe_ctx(m, ctx);
+-		for_each_ring(ring, dev_priv, i)
+-			if (ring->default_context == ctx)
+-				seq_printf(m, "(default context %s) ", ring->name);
++		for_each_engine(engine, dev_priv, i) {
++			if (engine->default_context == ctx)
++				seq_printf(m, "(default context %s) ",
++					   engine->name);
++		}
++
++		seq_putc(m, '\n');
++		for_each_engine(engine, dev_priv, i) {
++			struct drm_i915_gem_object *obj = ctx->ring[i].state;
++			struct intel_ringbuffer *ring = ctx->ring[i].ring;
++
++			seq_printf(m, "%s: ", engine->name);
++			if (obj)
++				describe_obj(m, obj);
++			if (ring)
++				describe_ring(m, ring);
++			seq_putc(m, '\n');
++		}
+ 
+-		describe_obj(m, ctx->legacy_hw_ctx.rcs_state);
+ 		seq_putc(m, '\n');
+ 	}
+ 
+@@ -1674,26 +1875,146 @@
+ 	return 0;
+ }
+ 
+-static int i915_gen6_forcewake_count_info(struct seq_file *m, void *data)
++static int i915_dump_lrc(struct seq_file *m, void *unused)
+ {
+-	struct drm_info_node *node = m->private;
++	struct drm_info_node *node = (struct drm_info_node *) m->private;
++	struct drm_device *dev = node->minor->dev;
++	struct intel_engine_cs *engine;
++	int ret, i;
++
++	ret = mutex_lock_interruptible(&dev->struct_mutex);
++	if (ret)
++		return ret;
++
++	for_each_engine(engine, to_i915(dev), i) {
++		struct intel_ringbuffer *ring;
++
++		list_for_each_entry(ring, &engine->rings, engine_link) {
++			struct intel_context *ctx = ring->ctx;
++			struct task_struct *task;
++
++			if (ctx == NULL)
++				continue;
++
++			seq_printf(m, "CONTEXT: %s", engine->name);
++
++			rcu_read_lock();
++			task = ctx->file_priv ? pid_task(ctx->file_priv->file->pid, PIDTYPE_PID) : NULL;
++			seq_printf(m, " %d:%d\n", task ? task->pid : 0, ctx->file_priv ? ctx->user_handle : 0);
++			rcu_read_unlock();
++
++			if (ctx->ring[engine->id].state) {
++				struct drm_i915_gem_object *obj;
++				struct page *page;
++				uint32_t *reg_state;
++				int j;
++
++				obj = ctx->ring[engine->id].state;
++				page = i915_gem_object_get_page(obj, 1);
++				reg_state = kmap_atomic(page);
++
++				seq_printf(m, "\tLRCA:\n");
++				for (j = 0; j < 0x600 / sizeof(u32) / 4; j += 4) {
++					seq_printf(m, "\t[0x%08lx] 0x%08x 0x%08x 0x%08x 0x%08x\n",
++					i915_gem_obj_ggtt_offset(obj) + 4096 + (j * 4),
++					reg_state[j], reg_state[j + 1],
++					reg_state[j + 2], reg_state[j + 3]);
++				}
++				kunmap_atomic(reg_state);
++
++				seq_putc(m, '\n');
++			}
++		}
++	}
++
++	mutex_unlock(&dev->struct_mutex);
++
++	return 0;
++}
++
++static int i915_execlists(struct seq_file *m, void *data)
++{
++	struct drm_info_node *node = (struct drm_info_node *)m->private;
+ 	struct drm_device *dev = node->minor->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	unsigned forcewake_count = 0, fw_rendercount = 0, fw_mediacount = 0;
++	struct intel_engine_cs *engine;
++	u32 status_pointer;
++	u8 read_pointer;
++	u8 write_pointer;
++	u32 status;
++	u32 ctx_id;
++	struct list_head *cursor;
++	int ring_id, i;
++	int ret;
+ 
+-	spin_lock_irq(&dev_priv->uncore.lock);
+-	if (IS_VALLEYVIEW(dev)) {
+-		fw_rendercount = dev_priv->uncore.fw_rendercount;
+-		fw_mediacount = dev_priv->uncore.fw_mediacount;
+-	} else
+-		forcewake_count = dev_priv->uncore.forcewake_count;
+-	spin_unlock_irq(&dev_priv->uncore.lock);
++	ret = mutex_lock_interruptible(&dev->struct_mutex);
++	if (ret)
++		return ret;
+ 
+-	if (IS_VALLEYVIEW(dev)) {
+-		seq_printf(m, "fw_rendercount = %u\n", fw_rendercount);
+-		seq_printf(m, "fw_mediacount = %u\n", fw_mediacount);
+-	} else
+-		seq_printf(m, "forcewake count = %u\n", forcewake_count);
++	intel_runtime_pm_get(dev_priv);
++
++	for_each_engine(engine, dev_priv, ring_id) {
++		struct i915_gem_request *rq = NULL;
++		int count = 0;
++		unsigned long flags;
++
++		seq_printf(m, "%s\n", engine->name);
++
++		if (!engine->execlists_enabled) {
++			seq_puts(m, "\tExeclists are disabled\n");
++			continue;
++		}
++
++		status = I915_READ(RING_EXECLIST_STATUS(engine));
++		ctx_id = I915_READ(RING_EXECLIST_STATUS(engine) + 4);
++		seq_printf(m, "\tExeclist status: 0x%08X, context: %u\n",
++			   status, ctx_id);
++
++		status_pointer = I915_READ(RING_CONTEXT_STATUS_PTR(engine));
++		seq_printf(m, "\tStatus pointer: 0x%08X\n", status_pointer);
++
++		read_pointer = engine->next_context_status_buffer;
++		write_pointer = status_pointer & 0x07;
++		if (read_pointer > write_pointer)
++			write_pointer += 6;
++		seq_printf(m, "\tRead pointer: 0x%08X, write pointer 0x%08X\n",
++			   read_pointer, write_pointer);
++
++		for (i = 0; i < 6; i++) {
++			status = I915_READ(RING_CONTEXT_STATUS_BUF(engine) + 8*i);
++			ctx_id = I915_READ(RING_CONTEXT_STATUS_BUF(engine) + 8*i + 4);
++
++			seq_printf(m, "\tStatus buffer %d: 0x%08X, context: %u\n",
++				   i, status, ctx_id);
++		}
++
++		spin_lock_irqsave(&engine->irqlock, flags);
++		list_for_each(cursor, &engine->pending)
++			count++;
++		rq = list_first_entry_or_null(&engine->pending, typeof(*rq), engine_link);
++		spin_unlock_irqrestore(&engine->irqlock, flags);
++
++		seq_printf(m, "\t%d requests in queue\n", count);
++		if (rq) {
++			struct intel_context *ctx = rq->ctx;
++			struct task_struct *task;
++
++			seq_printf(m, "\tHead request ctx:");
++
++			rcu_read_lock();
++			task = ctx->file_priv ? pid_task(ctx->file_priv->file->pid, PIDTYPE_PID) : NULL;
++			seq_printf(m, " %d:%d\n", task ? task->pid : 0, ctx->file_priv ? ctx->user_handle : 0);
++			rcu_read_unlock();
++
++			seq_printf(m, "\tHead request tail: %u\n", rq->tail);
++			seq_printf(m, "\tHead request seqno: %d\n", rq->seqno);
++		}
++
++		seq_putc(m, '\n');
++	}
++
++	intel_runtime_pm_put(dev_priv);
++	mutex_unlock(&dev->struct_mutex);
+ 
+ 	return 0;
+ }
+@@ -1755,7 +2076,7 @@
+ 			   I915_READ(MAD_DIMM_C2));
+ 		seq_printf(m, "TILECTL = 0x%08x\n",
+ 			   I915_READ(TILECTL));
+-		if (IS_GEN8(dev))
++		if (INTEL_INFO(dev)->gen >= 8)
+ 			seq_printf(m, "GAMTARBMODE = 0x%08x\n",
+ 				   I915_READ(GAMTARBMODE));
+ 		else
+@@ -1774,7 +2095,13 @@
+ {
+ 	struct intel_context *ctx = ptr;
+ 	struct seq_file *m = data;
+-	struct i915_hw_ppgtt *ppgtt = ctx_to_ppgtt(ctx);
++	struct i915_hw_ppgtt *ppgtt = ctx->ppgtt;
++
++	if (!ppgtt) {
++		seq_printf(m, "  no ppgtt for context %d\n",
++			   ctx->user_handle);
++		return 0;
++	}
+ 
+ 	if (i915_gem_context_is_default(ctx))
+ 		seq_puts(m, "  default context:\n");
+@@ -1788,7 +2115,7 @@
+ static void gen8_ppgtt_info(struct seq_file *m, struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring;
++	struct intel_engine_cs *engine;
+ 	struct i915_hw_ppgtt *ppgtt = dev_priv->mm.aliasing_ppgtt;
+ 	int unused, i;
+ 
+@@ -1797,13 +2124,13 @@
+ 
+ 	seq_printf(m, "Page directories: %d\n", ppgtt->num_pd_pages);
+ 	seq_printf(m, "Page tables: %d\n", ppgtt->num_pd_entries);
+-	for_each_ring(ring, dev_priv, unused) {
+-		seq_printf(m, "%s\n", ring->name);
++	for_each_engine(engine, dev_priv, unused) {
++		seq_printf(m, "%s\n", engine->name);
+ 		for (i = 0; i < 4; i++) {
+ 			u32 offset = 0x270 + i * 8;
+-			u64 pdp = I915_READ(ring->mmio_base + offset + 4);
++			u64 pdp = I915_READ(engine->mmio_base + offset + 4);
+ 			pdp <<= 32;
+-			pdp |= I915_READ(ring->mmio_base + offset);
++			pdp |= I915_READ(engine->mmio_base + offset);
+ 			seq_printf(m, "\tPDP%d 0x%016llx\n", i, pdp);
+ 		}
+ 	}
+@@ -1812,30 +2139,30 @@
+ static void gen6_ppgtt_info(struct seq_file *m, struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring;
++	struct intel_engine_cs *engine;
+ 	struct drm_file *file;
+ 	int i;
+ 
+ 	if (INTEL_INFO(dev)->gen == 6)
+ 		seq_printf(m, "GFX_MODE: 0x%08x\n", I915_READ(GFX_MODE));
+ 
+-	for_each_ring(ring, dev_priv, i) {
+-		seq_printf(m, "%s\n", ring->name);
++	for_each_engine(engine, dev_priv, i) {
++		seq_printf(m, "%s\n", engine->name);
+ 		if (INTEL_INFO(dev)->gen == 7)
+-			seq_printf(m, "GFX_MODE: 0x%08x\n", I915_READ(RING_MODE_GEN7(ring)));
+-		seq_printf(m, "PP_DIR_BASE: 0x%08x\n", I915_READ(RING_PP_DIR_BASE(ring)));
+-		seq_printf(m, "PP_DIR_BASE_READ: 0x%08x\n", I915_READ(RING_PP_DIR_BASE_READ(ring)));
+-		seq_printf(m, "PP_DIR_DCLV: 0x%08x\n", I915_READ(RING_PP_DIR_DCLV(ring)));
++			seq_printf(m, "GFX_MODE: 0x%08x\n", I915_READ(RING_MODE_GEN7(engine)));
++		seq_printf(m, "PP_DIR_BASE: 0x%08x\n", I915_READ(RING_PP_DIR_BASE(engine)));
++		seq_printf(m, "PP_DIR_BASE_READ: 0x%08x\n", I915_READ(RING_PP_DIR_BASE_READ(engine)));
++		seq_printf(m, "PP_DIR_DCLV: 0x%08x\n", I915_READ(RING_PP_DIR_DCLV(engine)));
+ 	}
+ 	if (dev_priv->mm.aliasing_ppgtt) {
+ 		struct i915_hw_ppgtt *ppgtt = dev_priv->mm.aliasing_ppgtt;
+ 
+ 		seq_puts(m, "aliasing PPGTT:\n");
+-		seq_printf(m, "pd gtt offset: 0x%08x\n", ppgtt->pd_offset);
++		seq_printf(m, "pd gtt offset: 0x%08llx\n",
++			   (long long)i915_gem_obj_to_ggtt(ppgtt->state)->node.start);
+ 
+ 		ppgtt->debug_dump(ppgtt, m);
+-	} else
+-		return;
++	}
+ 
+ 	list_for_each_entry_reverse(file, &dev->filelist, lhead) {
+ 		struct drm_i915_file_private *file_priv = file->driver_priv;
+@@ -2313,68 +2640,63 @@
+ 	struct drm_info_node *node = (struct drm_info_node *) m->private;
+ 	struct drm_device *dev = node->minor->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring;
++	struct intel_engine_cs *engine;
+ 	int num_rings = hweight32(INTEL_INFO(dev)->ring_mask);
+ 	int i, j, ret;
+ 
+-	if (!i915_semaphore_is_enabled(dev)) {
+-		seq_puts(m, "Semaphores are disabled\n");
+-		return 0;
+-	}
+-
+ 	ret = mutex_lock_interruptible(&dev->struct_mutex);
+ 	if (ret)
+ 		return ret;
+ 	intel_runtime_pm_get(dev_priv);
+ 
+-	if (IS_BROADWELL(dev)) {
+-		struct page *page;
+-		uint64_t *seqno;
+-
+-		page = i915_gem_object_get_page(dev_priv->semaphore_obj, 0);
+-
+-		seqno = (uint64_t *)kmap_atomic(page);
+-		for_each_ring(ring, dev_priv, i) {
+-			uint64_t offset;
++	seq_puts(m, "  Last breadcrumb:");
++	for_each_engine(engine, dev_priv, i)
++		for (j = 0; j < num_rings; j++)
++			seq_printf(m, "0x%08x\n",
++				   engine->breadcrumb[j]);
++	seq_putc(m, '\n');
+ 
+-			seq_printf(m, "%s\n", ring->name);
++	if (engine->semaphore.wait) {
++		if (IS_BROADWELL(dev)) {
++			struct page *page;
++			uint64_t *seqno;
++
++			page = i915_gem_object_get_page(dev_priv->semaphore_obj, 0);
++
++			seqno = (uint64_t *)kmap_atomic(page);
++			for_each_engine(engine, dev_priv, i) {
++				uint64_t offset;
++
++				seq_printf(m, "%s\n", engine->name);
++
++				seq_puts(m, "  Last signal:");
++				for (j = 0; j < num_rings; j++) {
++					offset = i * I915_NUM_ENGINES + j;
++					seq_printf(m, "0x%08llx (0x%02llx) ",
++						   seqno[offset], offset * 8);
++				}
++				seq_putc(m, '\n');
++
++				seq_puts(m, "  Last wait:  ");
++				for (j = 0; j < num_rings; j++) {
++					offset = i + (j * I915_NUM_ENGINES);
++					seq_printf(m, "0x%08llx (0x%02llx) ",
++						   seqno[offset], offset * 8);
++				}
++				seq_putc(m, '\n');
+ 
+-			seq_puts(m, "  Last signal:");
+-			for (j = 0; j < num_rings; j++) {
+-				offset = i * I915_NUM_RINGS + j;
+-				seq_printf(m, "0x%08llx (0x%02llx) ",
+-					   seqno[offset], offset * 8);
+-			}
+-			seq_putc(m, '\n');
+-
+-			seq_puts(m, "  Last wait:  ");
+-			for (j = 0; j < num_rings; j++) {
+-				offset = i + (j * I915_NUM_RINGS);
+-				seq_printf(m, "0x%08llx (0x%02llx) ",
+-					   seqno[offset], offset * 8);
+ 			}
++			kunmap_atomic(seqno);
++		} else {
++			seq_puts(m, "  Last signal:");
++			for_each_engine(engine, dev_priv, i)
++				for (j = 0; j < num_rings; j++)
++					seq_printf(m, "0x%08x\n",
++						   I915_READ(engine->semaphore.mbox.signal[j]));
+ 			seq_putc(m, '\n');
+-
+ 		}
+-		kunmap_atomic(seqno);
+-	} else {
+-		seq_puts(m, "  Last signal:");
+-		for_each_ring(ring, dev_priv, i)
+-			for (j = 0; j < num_rings; j++)
+-				seq_printf(m, "0x%08x\n",
+-					   I915_READ(ring->semaphore.mbox.signal[j]));
+-		seq_putc(m, '\n');
+ 	}
+ 
+-	seq_puts(m, "\nSync seqno:\n");
+-	for_each_ring(ring, dev_priv, i) {
+-		for (j = 0; j < num_rings; j++) {
+-			seq_printf(m, "  0x%08x ", ring->semaphore.sync_seqno[j]);
+-		}
+-		seq_putc(m, '\n');
+-	}
+-	seq_putc(m, '\n');
+-
+ 	intel_runtime_pm_put(dev_priv);
+ 	mutex_unlock(&dev->struct_mutex);
+ 	return 0;
+@@ -2392,15 +2714,86 @@
+ 		struct intel_shared_dpll *pll = &dev_priv->shared_dplls[i];
+ 
+ 		seq_printf(m, "DPLL%i: %s, id: %i\n", i, pll->name, pll->id);
+-		seq_printf(m, " refcount: %i, active: %i, on: %s\n", pll->refcount,
+-			   pll->active, yesno(pll->on));
++		seq_printf(m, " crtc_mask: 0x%08x, active: %d, on: %s\n",
++			   pll->config.crtc_mask, pll->active, yesno(pll->on));
+ 		seq_printf(m, " tracked hardware state:\n");
+-		seq_printf(m, " dpll:    0x%08x\n", pll->hw_state.dpll);
+-		seq_printf(m, " dpll_md: 0x%08x\n", pll->hw_state.dpll_md);
+-		seq_printf(m, " fp0:     0x%08x\n", pll->hw_state.fp0);
+-		seq_printf(m, " fp1:     0x%08x\n", pll->hw_state.fp1);
+-		seq_printf(m, " wrpll:   0x%08x\n", pll->hw_state.wrpll);
++		seq_printf(m, " dpll:    0x%08x\n", pll->config.hw_state.dpll);
++		seq_printf(m, " dpll_md: 0x%08x\n",
++			   pll->config.hw_state.dpll_md);
++		seq_printf(m, " fp0:     0x%08x\n", pll->config.hw_state.fp0);
++		seq_printf(m, " fp1:     0x%08x\n", pll->config.hw_state.fp1);
++		seq_printf(m, " wrpll:   0x%08x\n", pll->config.hw_state.wrpll);
++	}
++	drm_modeset_unlock_all(dev);
++
++	return 0;
++}
++
++static int i915_wa_registers(struct seq_file *m, void *unused)
++{
++	int i;
++	int ret;
++	struct drm_info_node *node = (struct drm_info_node *) m->private;
++	struct drm_device *dev = node->minor->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++
++	ret = mutex_lock_interruptible(&dev->struct_mutex);
++	if (ret)
++		return ret;
++
++	intel_runtime_pm_get(dev_priv);
++
++	seq_printf(m, "Workarounds applied: %d\n", dev_priv->workarounds.count);
++	for (i = 0; i < dev_priv->workarounds.count; ++i) {
++		u32 addr, mask, value, read;
++		bool ok;
++
++		addr = dev_priv->workarounds.reg[i].addr;
++		mask = dev_priv->workarounds.reg[i].mask;
++		value = dev_priv->workarounds.reg[i].value;
++		read = I915_READ(addr);
++		ok = (value & mask) == (read & mask);
++		seq_printf(m, "0x%X: 0x%08X, mask: 0x%08X, read: 0x%08x, status: %s\n",
++			   addr, value, mask, read, ok ? "OK" : "FAIL");
++	}
++
++	intel_runtime_pm_put(dev_priv);
++	mutex_unlock(&dev->struct_mutex);
++
++	return 0;
++}
++
++static int i915_ddb_info(struct seq_file *m, void *unused)
++{
++	struct drm_info_node *node = m->private;
++	struct drm_device *dev = node->minor->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct skl_ddb_allocation *ddb;
++	struct skl_ddb_entry *entry;
++	enum pipe pipe;
++	int plane;
++
++	drm_modeset_lock_all(dev);
++
++	ddb = &dev_priv->wm.skl_hw.ddb;
++
++	seq_printf(m, "%-15s%8s%8s%8s\n", "", "Start", "End", "Size");
++
++	for_each_pipe(dev_priv, pipe) {
++		seq_printf(m, "Pipe %c\n", pipe_name(pipe));
++
++		for_each_plane(pipe, plane) {
++			entry = &ddb->plane[pipe][plane];
++			seq_printf(m, "  Plane%-8d%8u%8u%8u\n", plane + 1,
++				   entry->start, entry->end,
++				   skl_ddb_entry_size(entry));
++		}
++
++		entry = &ddb->cursor[pipe];
++		seq_printf(m, "  %-13s%8u%8u%8u\n", "Cursor", entry->start,
++			   entry->end, skl_ddb_entry_size(entry));
+ 	}
++
+ 	drm_modeset_unlock_all(dev);
+ 
+ 	return 0;
+@@ -2667,8 +3060,7 @@
+ 	*source = INTEL_PIPE_CRC_SOURCE_PIPE;
+ 
+ 	drm_modeset_lock_all(dev);
+-	list_for_each_entry(encoder, &dev->mode_config.encoder_list,
+-			    base.head) {
++	for_each_intel_encoder(dev, encoder) {
+ 		if (!encoder->base.crtc)
+ 			continue;
+ 
+@@ -2700,6 +3092,8 @@
+ 				break;
+ 			}
+ 			break;
++		default:
++			break;
+ 		}
+ 	}
+ 	drm_modeset_unlock_all(dev);
+@@ -2987,6 +3381,8 @@
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct intel_pipe_crc *pipe_crc = &dev_priv->pipe_crc[pipe];
++	struct intel_crtc *crtc = to_intel_crtc(intel_get_crtc_for_pipe(dev,
++									pipe));
+ 	u32 val = 0; /* shut up gcc */
+ 	int ret;
+ 
+@@ -3022,6 +3418,14 @@
+ 		if (!pipe_crc->entries)
+ 			return -ENOMEM;
+ 
++		/*
++		 * When IPS gets enabled, the pipe CRC changes. Since IPS gets
++		 * enabled and disabled dynamically based on package C states,
++		 * user space can't make reliable use of the CRCs, so let's just
++		 * completely disable it.
++		 */
++		hsw_disable_ips(crtc);
++
+ 		spin_lock_irq(&pipe_crc->lock);
+ 		pipe_crc->head = 0;
+ 		pipe_crc->tail = 0;
+@@ -3060,6 +3464,8 @@
+ 			vlv_undo_pipe_scramble_reset(dev, pipe);
+ 		else if (IS_HASWELL(dev) && pipe == PIPE_A)
+ 			hsw_undo_trans_edp_pipe_A_crc_wa(dev);
++
++		hsw_enable_ips(crtc);
+ 	}
+ 
+ 	return 0;
+@@ -3237,7 +3643,7 @@
+ 	.write = display_crc_ctl_write
+ };
+ 
+-static void wm_latency_show(struct seq_file *m, const uint16_t wm[5])
++static void wm_latency_show(struct seq_file *m, const uint16_t wm[8])
+ {
+ 	struct drm_device *dev = m->private;
+ 	int num_levels = ilk_wm_max_level(dev) + 1;
+@@ -3248,13 +3654,17 @@
+ 	for (level = 0; level < num_levels; level++) {
+ 		unsigned int latency = wm[level];
+ 
+-		/* WM1+ latency values in 0.5us units */
+-		if (level > 0)
++		/*
++		 * - WM1+ latency values in 0.5us units
++		 * - latencies are in us on gen9
++		 */
++		if (INTEL_INFO(dev)->gen >= 9)
++			latency *= 10;
++		else if (level > 0)
+ 			latency *= 5;
+ 
+ 		seq_printf(m, "WM%d %u (%u.%u usec)\n",
+-			   level, wm[level],
+-			   latency / 10, latency % 10);
++			   level, wm[level], latency / 10, latency % 10);
+ 	}
+ 
+ 	drm_modeset_unlock_all(dev);
+@@ -3263,8 +3673,15 @@
+ static int pri_wm_latency_show(struct seq_file *m, void *data)
+ {
+ 	struct drm_device *dev = m->private;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	const uint16_t *latencies;
+ 
+-	wm_latency_show(m, to_i915(dev)->wm.pri_latency);
++	if (INTEL_INFO(dev)->gen >= 9)
++		latencies = dev_priv->wm.skl_latency;
++	else
++		latencies = to_i915(dev)->wm.pri_latency;
++
++	wm_latency_show(m, latencies);
+ 
+ 	return 0;
+ }
+@@ -3272,8 +3689,15 @@
+ static int spr_wm_latency_show(struct seq_file *m, void *data)
+ {
+ 	struct drm_device *dev = m->private;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	const uint16_t *latencies;
+ 
+-	wm_latency_show(m, to_i915(dev)->wm.spr_latency);
++	if (INTEL_INFO(dev)->gen >= 9)
++		latencies = dev_priv->wm.skl_latency;
++	else
++		latencies = to_i915(dev)->wm.spr_latency;
++
++	wm_latency_show(m, latencies);
+ 
+ 	return 0;
+ }
+@@ -3281,8 +3705,15 @@
+ static int cur_wm_latency_show(struct seq_file *m, void *data)
+ {
+ 	struct drm_device *dev = m->private;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	const uint16_t *latencies;
+ 
+-	wm_latency_show(m, to_i915(dev)->wm.cur_latency);
++	if (INTEL_INFO(dev)->gen >= 9)
++		latencies = dev_priv->wm.skl_latency;
++	else
++		latencies = to_i915(dev)->wm.cur_latency;
++
++	wm_latency_show(m, latencies);
+ 
+ 	return 0;
+ }
+@@ -3318,11 +3749,11 @@
+ }
+ 
+ static ssize_t wm_latency_write(struct file *file, const char __user *ubuf,
+-				size_t len, loff_t *offp, uint16_t wm[5])
++				size_t len, loff_t *offp, uint16_t wm[8])
+ {
+ 	struct seq_file *m = file->private_data;
+ 	struct drm_device *dev = m->private;
+-	uint16_t new[5] = { 0 };
++	uint16_t new[8] = { 0 };
+ 	int num_levels = ilk_wm_max_level(dev) + 1;
+ 	int level;
+ 	int ret;
+@@ -3336,7 +3767,9 @@
+ 
+ 	tmp[len] = '\0';
+ 
+-	ret = sscanf(tmp, "%hu %hu %hu %hu %hu", &new[0], &new[1], &new[2], &new[3], &new[4]);
++	ret = sscanf(tmp, "%hu %hu %hu %hu %hu %hu %hu %hu",
++		     &new[0], &new[1], &new[2], &new[3],
++		     &new[4], &new[5], &new[6], &new[7]);
+ 	if (ret != num_levels)
+ 		return -EINVAL;
+ 
+@@ -3356,8 +3789,15 @@
+ {
+ 	struct seq_file *m = file->private_data;
+ 	struct drm_device *dev = m->private;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	uint16_t *latencies;
+ 
+-	return wm_latency_write(file, ubuf, len, offp, to_i915(dev)->wm.pri_latency);
++	if (INTEL_INFO(dev)->gen >= 9)
++		latencies = dev_priv->wm.skl_latency;
++	else
++		latencies = to_i915(dev)->wm.pri_latency;
++
++	return wm_latency_write(file, ubuf, len, offp, latencies);
+ }
+ 
+ static ssize_t spr_wm_latency_write(struct file *file, const char __user *ubuf,
+@@ -3365,8 +3805,15 @@
+ {
+ 	struct seq_file *m = file->private_data;
+ 	struct drm_device *dev = m->private;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	uint16_t *latencies;
+ 
+-	return wm_latency_write(file, ubuf, len, offp, to_i915(dev)->wm.spr_latency);
++	if (INTEL_INFO(dev)->gen >= 9)
++		latencies = dev_priv->wm.skl_latency;
++	else
++		latencies = to_i915(dev)->wm.spr_latency;
++
++	return wm_latency_write(file, ubuf, len, offp, latencies);
+ }
+ 
+ static ssize_t cur_wm_latency_write(struct file *file, const char __user *ubuf,
+@@ -3374,8 +3821,15 @@
+ {
+ 	struct seq_file *m = file->private_data;
+ 	struct drm_device *dev = m->private;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	uint16_t *latencies;
+ 
+-	return wm_latency_write(file, ubuf, len, offp, to_i915(dev)->wm.cur_latency);
++	if (INTEL_INFO(dev)->gen >= 9)
++		latencies = dev_priv->wm.skl_latency;
++	else
++		latencies = to_i915(dev)->wm.cur_latency;
++
++	return wm_latency_write(file, ubuf, len, offp, latencies);
+ }
+ 
+ static const struct file_operations i915_pri_wm_latency_fops = {
+@@ -3557,9 +4011,6 @@
+ {
+ 	struct drm_device *dev = data;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct drm_i915_gem_object *obj, *next;
+-	struct i915_address_space *vm;
+-	struct i915_vma *vma, *x;
+ 	int ret;
+ 
+ 	DRM_DEBUG("Dropping caches: 0x%08llx\n", val);
+@@ -3579,29 +4030,11 @@
+ 	if (val & (DROP_RETIRE | DROP_ACTIVE))
+ 		i915_gem_retire_requests(dev);
+ 
+-	if (val & DROP_BOUND) {
+-		list_for_each_entry(vm, &dev_priv->vm_list, global_link) {
+-			list_for_each_entry_safe(vma, x, &vm->inactive_list,
+-						 mm_list) {
+-				if (vma->pin_count)
+-					continue;
+-
+-				ret = i915_vma_unbind(vma);
+-				if (ret)
+-					goto unlock;
+-			}
+-		}
+-	}
++	if (val & DROP_BOUND)
++		i915_gem_shrink(dev_priv, LONG_MAX, I915_SHRINK_BOUND);
+ 
+-	if (val & DROP_UNBOUND) {
+-		list_for_each_entry_safe(obj, next, &dev_priv->mm.unbound_list,
+-					 global_list)
+-			if (obj->pages_pin_count == 0) {
+-				ret = i915_gem_object_put_pages(obj);
+-				if (ret)
+-					goto unlock;
+-			}
+-	}
++	if (val & DROP_UNBOUND)
++		i915_gem_shrink(dev_priv, LONG_MAX, I915_SHRINK_UNBOUND);
+ 
+ unlock:
+ 	mutex_unlock(&dev->struct_mutex);
+@@ -3839,6 +4272,7 @@
+ 	if (INTEL_INFO(dev)->gen < 6)
+ 		return 0;
+ 
++	intel_runtime_pm_get(dev_priv);
+ 	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
+ 
+ 	return 0;
+@@ -3853,6 +4287,7 @@
+ 		return 0;
+ 
+ 	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
++	intel_runtime_pm_put(dev_priv);
+ 
+ 	return 0;
+ }
+@@ -3899,6 +4334,7 @@
+ static const struct drm_info_list i915_debugfs_list[] = {
+ 	{"i915_capabilities", i915_capabilities, 0},
+ 	{"i915_gem_objects", i915_gem_object_info, 0},
++	{"i915_gem_aperture", i915_gem_aperture_info, 0},
+ 	{"i915_gem_gtt", i915_gem_gtt_info, 0},
+ 	{"i915_gem_pinned", i915_gem_gtt_info, 0, (void *) PINNED_LIST},
+ 	{"i915_gem_active", i915_gem_object_list_info, 0, (void *) ACTIVE_LIST},
+@@ -3923,6 +4359,8 @@
+ 	{"i915_opregion", i915_opregion, 0},
+ 	{"i915_gem_framebuffer", i915_gem_framebuffer_info, 0},
+ 	{"i915_context_status", i915_context_status, 0},
++	{"i915_dump_lrc", i915_dump_lrc, 0},
++	{"i915_execlists", i915_execlists, 0},
+ 	{"i915_gen6_forcewake_count", i915_gen6_forcewake_count_info, 0},
+ 	{"i915_swizzle_info", i915_swizzle_info, 0},
+ 	{"i915_ppgtt_info", i915_ppgtt_info, 0},
+@@ -3936,6 +4374,8 @@
+ 	{"i915_semaphore_status", i915_semaphore_status, 0},
+ 	{"i915_shared_dplls_info", i915_shared_dplls_info, 0},
+ 	{"i915_dp_mst_info", i915_dp_mst_info, 0},
++	{"i915_wa_registers", i915_wa_registers, 0},
++	{"i915_ddb_info", i915_ddb_info, 0},
+ };
+ #define I915_DEBUGFS_ENTRIES ARRAY_SIZE(i915_debugfs_list)
+ 
+@@ -3957,6 +4397,7 @@
+ 	{"i915_pri_wm_latency", &i915_pri_wm_latency_fops},
+ 	{"i915_spr_wm_latency", &i915_spr_wm_latency_fops},
+ 	{"i915_cur_wm_latency", &i915_cur_wm_latency_fops},
++	{"i915_fbc_false_color", &i915_fbc_fc_fops},
+ };
+ 
+ void intel_display_crc_init(struct drm_device *dev)
+@@ -3964,7 +4405,7 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	enum pipe pipe;
+ 
+-	for_each_pipe(pipe) {
++	for_each_pipe(dev_priv, pipe) {
+ 		struct intel_pipe_crc *pipe_crc = &dev_priv->pipe_crc[pipe];
+ 
+ 		pipe_crc->opened = false;
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_dma.c b/drivers/gpu/drm/i915/i915_dma.c
+--- a/drivers/gpu/drm/i915/i915_dma.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/i915_dma.c	2014-11-20 09:53:37.960762838 -0700
+@@ -28,9 +28,11 @@
+ 
+ #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+ 
++#include <linux/async.h>
+ #include <drm/drmP.h>
+ #include <drm/drm_crtc_helper.h>
+ #include <drm/drm_fb_helper.h>
++#include <drm/drm_legacy.h>
+ #include "intel_drv.h"
+ #include <drm/i915_drm.h>
+ #include "i915_drv.h"
+@@ -48,883 +50,58 @@
+ #include <linux/pm_runtime.h>
+ #include <linux/oom.h>
+ 
+-#define LP_RING(d) (&((struct drm_i915_private *)(d))->ring[RCS])
+-
+-#define BEGIN_LP_RING(n) \
+-	intel_ring_begin(LP_RING(dev_priv), (n))
+-
+-#define OUT_RING(x) \
+-	intel_ring_emit(LP_RING(dev_priv), x)
+-
+-#define ADVANCE_LP_RING() \
+-	__intel_ring_advance(LP_RING(dev_priv))
+-
+-/**
+- * Lock test for when it's just for synchronization of ring access.
+- *
+- * In that case, we don't need to do it when GEM is initialized as nobody else
+- * has access to the ring.
+- */
+-#define RING_LOCK_TEST_WITH_RETURN(dev, file) do {			\
+-	if (LP_RING(dev->dev_private)->buffer->obj == NULL)			\
+-		LOCK_TEST_WITH_RETURN(dev, file);			\
+-} while (0)
+-
+-static inline u32
+-intel_read_legacy_status_page(struct drm_i915_private *dev_priv, int reg)
+-{
+-	if (I915_NEED_GFX_HWS(dev_priv->dev))
+-		return ioread32(dev_priv->dri1.gfx_hws_cpu_addr + reg);
+-	else
+-		return intel_read_status_page(LP_RING(dev_priv), reg);
+-}
+-
+-#define READ_HWSP(dev_priv, reg) intel_read_legacy_status_page(dev_priv, reg)
+-#define READ_BREADCRUMB(dev_priv) READ_HWSP(dev_priv, I915_BREADCRUMB_INDEX)
+-#define I915_BREADCRUMB_INDEX		0x21
+-
+-void i915_update_dri1_breadcrumb(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct drm_i915_master_private *master_priv;
+-
+-	/*
+-	 * The dri breadcrumb update races against the drm master disappearing.
+-	 * Instead of trying to fix this (this is by far not the only ums issue)
+-	 * just don't do the update in kms mode.
+-	 */
+-	if (drm_core_check_feature(dev, DRIVER_MODESET))
+-		return;
+-
+-	if (dev->primary->master) {
+-		master_priv = dev->primary->master->driver_priv;
+-		if (master_priv->sarea_priv)
+-			master_priv->sarea_priv->last_dispatch =
+-				READ_BREADCRUMB(dev_priv);
+-	}
+-}
+-
+-static void i915_write_hws_pga(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	u32 addr;
+-
+-	addr = dev_priv->status_page_dmah->busaddr;
+-	if (INTEL_INFO(dev)->gen >= 4)
+-		addr |= (dev_priv->status_page_dmah->busaddr >> 28) & 0xf0;
+-	I915_WRITE(HWS_PGA, addr);
+-}
+-
+-/**
+- * Frees the hardware status page, whether it's a physical address or a virtual
+- * address set up by the X Server.
+- */
+-static void i915_free_hws(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring = LP_RING(dev_priv);
+-
+-	if (dev_priv->status_page_dmah) {
+-		drm_pci_free(dev, dev_priv->status_page_dmah);
+-		dev_priv->status_page_dmah = NULL;
+-	}
+-
+-	if (ring->status_page.gfx_addr) {
+-		ring->status_page.gfx_addr = 0;
+-		iounmap(dev_priv->dri1.gfx_hws_cpu_addr);
+-	}
+-
+-	/* Need to rewrite hardware status page */
+-	I915_WRITE(HWS_PGA, 0x1ffff000);
+-}
+-
+-void i915_kernel_lost_context(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct drm_i915_master_private *master_priv;
+-	struct intel_engine_cs *ring = LP_RING(dev_priv);
+-	struct intel_ringbuffer *ringbuf = ring->buffer;
+-
+-	/*
+-	 * We should never lose context on the ring with modesetting
+-	 * as we don't expose it to userspace
+-	 */
+-	if (drm_core_check_feature(dev, DRIVER_MODESET))
+-		return;
+-
+-	ringbuf->head = I915_READ_HEAD(ring) & HEAD_ADDR;
+-	ringbuf->tail = I915_READ_TAIL(ring) & TAIL_ADDR;
+-	ringbuf->space = ringbuf->head - (ringbuf->tail + I915_RING_FREE_SPACE);
+-	if (ringbuf->space < 0)
+-		ringbuf->space += ringbuf->size;
+-
+-	if (!dev->primary->master)
+-		return;
+-
+-	master_priv = dev->primary->master->driver_priv;
+-	if (ringbuf->head == ringbuf->tail && master_priv->sarea_priv)
+-		master_priv->sarea_priv->perf_boxes |= I915_BOX_RING_EMPTY;
+-}
+-
+-static int i915_dma_cleanup(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	int i;
+-
+-	/* Make sure interrupts are disabled here because the uninstall ioctl
+-	 * may not have been called from userspace and after dev_private
+-	 * is freed, it's too late.
+-	 */
+-	if (dev->irq_enabled)
+-		drm_irq_uninstall(dev);
+-
+-	mutex_lock(&dev->struct_mutex);
+-	for (i = 0; i < I915_NUM_RINGS; i++)
+-		intel_cleanup_ring_buffer(&dev_priv->ring[i]);
+-	mutex_unlock(&dev->struct_mutex);
+-
+-	/* Clear the HWS virtual address at teardown */
+-	if (I915_NEED_GFX_HWS(dev))
+-		i915_free_hws(dev);
+-
+-	return 0;
+-}
+-
+-static int i915_initialize(struct drm_device *dev, drm_i915_init_t *init)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct drm_i915_master_private *master_priv = dev->primary->master->driver_priv;
+-	int ret;
+-
+-	master_priv->sarea = drm_getsarea(dev);
+-	if (master_priv->sarea) {
+-		master_priv->sarea_priv = (drm_i915_sarea_t *)
+-			((u8 *)master_priv->sarea->handle + init->sarea_priv_offset);
+-	} else {
+-		DRM_DEBUG_DRIVER("sarea not found assuming DRI2 userspace\n");
+-	}
+-
+-	if (init->ring_size != 0) {
+-		if (LP_RING(dev_priv)->buffer->obj != NULL) {
+-			i915_dma_cleanup(dev);
+-			DRM_ERROR("Client tried to initialize ringbuffer in "
+-				  "GEM mode\n");
+-			return -EINVAL;
+-		}
+-
+-		ret = intel_render_ring_init_dri(dev,
+-						 init->ring_start,
+-						 init->ring_size);
+-		if (ret) {
+-			i915_dma_cleanup(dev);
+-			return ret;
+-		}
+-	}
+-
+-	dev_priv->dri1.cpp = init->cpp;
+-	dev_priv->dri1.back_offset = init->back_offset;
+-	dev_priv->dri1.front_offset = init->front_offset;
+-	dev_priv->dri1.current_page = 0;
+-	if (master_priv->sarea_priv)
+-		master_priv->sarea_priv->pf_current_page = 0;
+-
+-	/* Allow hardware batchbuffers unless told otherwise.
+-	 */
+-	dev_priv->dri1.allow_batchbuffer = 1;
+-
+-	return 0;
+-}
+-
+-static int i915_dma_resume(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring = LP_RING(dev_priv);
+-
+-	DRM_DEBUG_DRIVER("%s\n", __func__);
+-
+-	if (ring->buffer->virtual_start == NULL) {
+-		DRM_ERROR("can not ioremap virtual address for"
+-			  " ring buffer\n");
+-		return -ENOMEM;
+-	}
+-
+-	/* Program Hardware Status Page */
+-	if (!ring->status_page.page_addr) {
+-		DRM_ERROR("Can not find hardware status page\n");
+-		return -EINVAL;
+-	}
+-	DRM_DEBUG_DRIVER("hw status page @ %p\n",
+-				ring->status_page.page_addr);
+-	if (ring->status_page.gfx_addr != 0)
+-		intel_ring_setup_status_page(ring);
+-	else
+-		i915_write_hws_pga(dev);
+-
+-	DRM_DEBUG_DRIVER("Enabled hardware status page\n");
+-
+-	return 0;
+-}
+-
+ static int i915_dma_init(struct drm_device *dev, void *data,
+ 			 struct drm_file *file_priv)
+ {
+-	drm_i915_init_t *init = data;
+-	int retcode = 0;
+-
+-	if (drm_core_check_feature(dev, DRIVER_MODESET))
+-		return -ENODEV;
+-
+-	switch (init->func) {
+-	case I915_INIT_DMA:
+-		retcode = i915_initialize(dev, init);
+-		break;
+-	case I915_CLEANUP_DMA:
+-		retcode = i915_dma_cleanup(dev);
+-		break;
+-	case I915_RESUME_DMA:
+-		retcode = i915_dma_resume(dev);
+-		break;
+-	default:
+-		retcode = -EINVAL;
+-		break;
+-	}
+-
+-	return retcode;
+-}
+-
+-/* Implement basically the same security restrictions as hardware does
+- * for MI_BATCH_NON_SECURE.  These can be made stricter at any time.
+- *
+- * Most of the calculations below involve calculating the size of a
+- * particular instruction.  It's important to get the size right as
+- * that tells us where the next instruction to check is.  Any illegal
+- * instruction detected will be given a size of zero, which is a
+- * signal to abort the rest of the buffer.
+- */
+-static int validate_cmd(int cmd)
+-{
+-	switch (((cmd >> 29) & 0x7)) {
+-	case 0x0:
+-		switch ((cmd >> 23) & 0x3f) {
+-		case 0x0:
+-			return 1;	/* MI_NOOP */
+-		case 0x4:
+-			return 1;	/* MI_FLUSH */
+-		default:
+-			return 0;	/* disallow everything else */
+-		}
+-		break;
+-	case 0x1:
+-		return 0;	/* reserved */
+-	case 0x2:
+-		return (cmd & 0xff) + 2;	/* 2d commands */
+-	case 0x3:
+-		if (((cmd >> 24) & 0x1f) <= 0x18)
+-			return 1;
+-
+-		switch ((cmd >> 24) & 0x1f) {
+-		case 0x1c:
+-			return 1;
+-		case 0x1d:
+-			switch ((cmd >> 16) & 0xff) {
+-			case 0x3:
+-				return (cmd & 0x1f) + 2;
+-			case 0x4:
+-				return (cmd & 0xf) + 2;
+-			default:
+-				return (cmd & 0xffff) + 2;
+-			}
+-		case 0x1e:
+-			if (cmd & (1 << 23))
+-				return (cmd & 0xffff) + 1;
+-			else
+-				return 1;
+-		case 0x1f:
+-			if ((cmd & (1 << 23)) == 0)	/* inline vertices */
+-				return (cmd & 0x1ffff) + 2;
+-			else if (cmd & (1 << 17))	/* indirect random */
+-				if ((cmd & 0xffff) == 0)
+-					return 0;	/* unknown length, too hard */
+-				else
+-					return (((cmd & 0xffff) + 1) / 2) + 1;
+-			else
+-				return 2;	/* indirect sequential */
+-		default:
+-			return 0;
+-		}
+-	default:
+-		return 0;
+-	}
+-
+-	return 0;
+-}
+-
+-static int i915_emit_cmds(struct drm_device *dev, int *buffer, int dwords)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	int i, ret;
+-
+-	if ((dwords+1) * sizeof(int) >= LP_RING(dev_priv)->buffer->size - 8)
+-		return -EINVAL;
+-
+-	for (i = 0; i < dwords;) {
+-		int sz = validate_cmd(buffer[i]);
+-
+-		if (sz == 0 || i + sz > dwords)
+-			return -EINVAL;
+-		i += sz;
+-	}
+-
+-	ret = BEGIN_LP_RING((dwords+1)&~1);
+-	if (ret)
+-		return ret;
+-
+-	for (i = 0; i < dwords; i++)
+-		OUT_RING(buffer[i]);
+-	if (dwords & 1)
+-		OUT_RING(0);
+-
+-	ADVANCE_LP_RING();
+-
+-	return 0;
+-}
+-
+-int
+-i915_emit_box(struct drm_device *dev,
+-	      struct drm_clip_rect *box,
+-	      int DR1, int DR4)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	int ret;
+-
+-	if (box->y2 <= box->y1 || box->x2 <= box->x1 ||
+-	    box->y2 <= 0 || box->x2 <= 0) {
+-		DRM_ERROR("Bad box %d,%d..%d,%d\n",
+-			  box->x1, box->y1, box->x2, box->y2);
+-		return -EINVAL;
+-	}
+-
+-	if (INTEL_INFO(dev)->gen >= 4) {
+-		ret = BEGIN_LP_RING(4);
+-		if (ret)
+-			return ret;
+-
+-		OUT_RING(GFX_OP_DRAWRECT_INFO_I965);
+-		OUT_RING((box->x1 & 0xffff) | (box->y1 << 16));
+-		OUT_RING(((box->x2 - 1) & 0xffff) | ((box->y2 - 1) << 16));
+-		OUT_RING(DR4);
+-	} else {
+-		ret = BEGIN_LP_RING(6);
+-		if (ret)
+-			return ret;
+-
+-		OUT_RING(GFX_OP_DRAWRECT_INFO);
+-		OUT_RING(DR1);
+-		OUT_RING((box->x1 & 0xffff) | (box->y1 << 16));
+-		OUT_RING(((box->x2 - 1) & 0xffff) | ((box->y2 - 1) << 16));
+-		OUT_RING(DR4);
+-		OUT_RING(0);
+-	}
+-	ADVANCE_LP_RING();
+-
+-	return 0;
+-}
+-
+-/* XXX: Emitting the counter should really be moved to part of the IRQ
+- * emit. For now, do it in both places:
+- */
+-
+-static void i915_emit_breadcrumb(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct drm_i915_master_private *master_priv = dev->primary->master->driver_priv;
+-
+-	dev_priv->dri1.counter++;
+-	if (dev_priv->dri1.counter > 0x7FFFFFFFUL)
+-		dev_priv->dri1.counter = 0;
+-	if (master_priv->sarea_priv)
+-		master_priv->sarea_priv->last_enqueue = dev_priv->dri1.counter;
+-
+-	if (BEGIN_LP_RING(4) == 0) {
+-		OUT_RING(MI_STORE_DWORD_INDEX);
+-		OUT_RING(I915_BREADCRUMB_INDEX << MI_STORE_DWORD_INDEX_SHIFT);
+-		OUT_RING(dev_priv->dri1.counter);
+-		OUT_RING(0);
+-		ADVANCE_LP_RING();
+-	}
+-}
+-
+-static int i915_dispatch_cmdbuffer(struct drm_device *dev,
+-				   drm_i915_cmdbuffer_t *cmd,
+-				   struct drm_clip_rect *cliprects,
+-				   void *cmdbuf)
+-{
+-	int nbox = cmd->num_cliprects;
+-	int i = 0, count, ret;
+-
+-	if (cmd->sz & 0x3) {
+-		DRM_ERROR("alignment");
+-		return -EINVAL;
+-	}
+-
+-	i915_kernel_lost_context(dev);
+-
+-	count = nbox ? nbox : 1;
+-
+-	for (i = 0; i < count; i++) {
+-		if (i < nbox) {
+-			ret = i915_emit_box(dev, &cliprects[i],
+-					    cmd->DR1, cmd->DR4);
+-			if (ret)
+-				return ret;
+-		}
+-
+-		ret = i915_emit_cmds(dev, cmdbuf, cmd->sz / 4);
+-		if (ret)
+-			return ret;
+-	}
+-
+-	i915_emit_breadcrumb(dev);
+-	return 0;
+-}
+-
+-static int i915_dispatch_batchbuffer(struct drm_device *dev,
+-				     drm_i915_batchbuffer_t *batch,
+-				     struct drm_clip_rect *cliprects)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	int nbox = batch->num_cliprects;
+-	int i, count, ret;
+-
+-	if ((batch->start | batch->used) & 0x7) {
+-		DRM_ERROR("alignment");
+-		return -EINVAL;
+-	}
+-
+-	i915_kernel_lost_context(dev);
+-
+-	count = nbox ? nbox : 1;
+-	for (i = 0; i < count; i++) {
+-		if (i < nbox) {
+-			ret = i915_emit_box(dev, &cliprects[i],
+-					    batch->DR1, batch->DR4);
+-			if (ret)
+-				return ret;
+-		}
+-
+-		if (!IS_I830(dev) && !IS_845G(dev)) {
+-			ret = BEGIN_LP_RING(2);
+-			if (ret)
+-				return ret;
+-
+-			if (INTEL_INFO(dev)->gen >= 4) {
+-				OUT_RING(MI_BATCH_BUFFER_START | (2 << 6) | MI_BATCH_NON_SECURE_I965);
+-				OUT_RING(batch->start);
+-			} else {
+-				OUT_RING(MI_BATCH_BUFFER_START | (2 << 6));
+-				OUT_RING(batch->start | MI_BATCH_NON_SECURE);
+-			}
+-		} else {
+-			ret = BEGIN_LP_RING(4);
+-			if (ret)
+-				return ret;
+-
+-			OUT_RING(MI_BATCH_BUFFER);
+-			OUT_RING(batch->start | MI_BATCH_NON_SECURE);
+-			OUT_RING(batch->start + batch->used - 4);
+-			OUT_RING(0);
+-		}
+-		ADVANCE_LP_RING();
+-	}
+-
+-
+-	if (IS_G4X(dev) || IS_GEN5(dev)) {
+-		if (BEGIN_LP_RING(2) == 0) {
+-			OUT_RING(MI_FLUSH | MI_NO_WRITE_FLUSH | MI_INVALIDATE_ISP);
+-			OUT_RING(MI_NOOP);
+-			ADVANCE_LP_RING();
+-		}
+-	}
+-
+-	i915_emit_breadcrumb(dev);
+-	return 0;
+-}
+-
+-static int i915_dispatch_flip(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct drm_i915_master_private *master_priv =
+-		dev->primary->master->driver_priv;
+-	int ret;
+-
+-	if (!master_priv->sarea_priv)
+-		return -EINVAL;
+-
+-	DRM_DEBUG_DRIVER("%s: page=%d pfCurrentPage=%d\n",
+-			  __func__,
+-			 dev_priv->dri1.current_page,
+-			 master_priv->sarea_priv->pf_current_page);
+-
+-	i915_kernel_lost_context(dev);
+-
+-	ret = BEGIN_LP_RING(10);
+-	if (ret)
+-		return ret;
+-
+-	OUT_RING(MI_FLUSH | MI_READ_FLUSH);
+-	OUT_RING(0);
+-
+-	OUT_RING(CMD_OP_DISPLAYBUFFER_INFO | ASYNC_FLIP);
+-	OUT_RING(0);
+-	if (dev_priv->dri1.current_page == 0) {
+-		OUT_RING(dev_priv->dri1.back_offset);
+-		dev_priv->dri1.current_page = 1;
+-	} else {
+-		OUT_RING(dev_priv->dri1.front_offset);
+-		dev_priv->dri1.current_page = 0;
+-	}
+-	OUT_RING(0);
+-
+-	OUT_RING(MI_WAIT_FOR_EVENT | MI_WAIT_FOR_PLANE_A_FLIP);
+-	OUT_RING(0);
+-
+-	ADVANCE_LP_RING();
+-
+-	master_priv->sarea_priv->last_enqueue = dev_priv->dri1.counter++;
+-
+-	if (BEGIN_LP_RING(4) == 0) {
+-		OUT_RING(MI_STORE_DWORD_INDEX);
+-		OUT_RING(I915_BREADCRUMB_INDEX << MI_STORE_DWORD_INDEX_SHIFT);
+-		OUT_RING(dev_priv->dri1.counter);
+-		OUT_RING(0);
+-		ADVANCE_LP_RING();
+-	}
+-
+-	master_priv->sarea_priv->pf_current_page = dev_priv->dri1.current_page;
+-	return 0;
+-}
+-
+-static int i915_quiescent(struct drm_device *dev)
+-{
+-	i915_kernel_lost_context(dev);
+-	return intel_ring_idle(LP_RING(dev->dev_private));
++	return -ENODEV;
+ }
+ 
+ static int i915_flush_ioctl(struct drm_device *dev, void *data,
+ 			    struct drm_file *file_priv)
+ {
+-	int ret;
+-
+-	if (drm_core_check_feature(dev, DRIVER_MODESET))
+-		return -ENODEV;
+-
+-	RING_LOCK_TEST_WITH_RETURN(dev, file_priv);
+-
+-	mutex_lock(&dev->struct_mutex);
+-	ret = i915_quiescent(dev);
+-	mutex_unlock(&dev->struct_mutex);
+-
+-	return ret;
++	return -ENODEV;
+ }
+ 
+ static int i915_batchbuffer(struct drm_device *dev, void *data,
+ 			    struct drm_file *file_priv)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct drm_i915_master_private *master_priv;
+-	drm_i915_sarea_t *sarea_priv;
+-	drm_i915_batchbuffer_t *batch = data;
+-	int ret;
+-	struct drm_clip_rect *cliprects = NULL;
+-
+-	if (drm_core_check_feature(dev, DRIVER_MODESET))
+-		return -ENODEV;
+-
+-	master_priv = dev->primary->master->driver_priv;
+-	sarea_priv = (drm_i915_sarea_t *) master_priv->sarea_priv;
+-
+-	if (!dev_priv->dri1.allow_batchbuffer) {
+-		DRM_ERROR("Batchbuffer ioctl disabled\n");
+-		return -EINVAL;
+-	}
+-
+-	DRM_DEBUG_DRIVER("i915 batchbuffer, start %x used %d cliprects %d\n",
+-			batch->start, batch->used, batch->num_cliprects);
+-
+-	RING_LOCK_TEST_WITH_RETURN(dev, file_priv);
+-
+-	if (batch->num_cliprects < 0)
+-		return -EINVAL;
+-
+-	if (batch->num_cliprects) {
+-		cliprects = kcalloc(batch->num_cliprects,
+-				    sizeof(*cliprects),
+-				    GFP_KERNEL);
+-		if (cliprects == NULL)
+-			return -ENOMEM;
+-
+-		ret = copy_from_user(cliprects, batch->cliprects,
+-				     batch->num_cliprects *
+-				     sizeof(struct drm_clip_rect));
+-		if (ret != 0) {
+-			ret = -EFAULT;
+-			goto fail_free;
+-		}
+-	}
+-
+-	mutex_lock(&dev->struct_mutex);
+-	ret = i915_dispatch_batchbuffer(dev, batch, cliprects);
+-	mutex_unlock(&dev->struct_mutex);
+-
+-	if (sarea_priv)
+-		sarea_priv->last_dispatch = READ_BREADCRUMB(dev_priv);
+-
+-fail_free:
+-	kfree(cliprects);
+-
+-	return ret;
++	return -ENODEV;
+ }
+ 
+ static int i915_cmdbuffer(struct drm_device *dev, void *data,
+ 			  struct drm_file *file_priv)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct drm_i915_master_private *master_priv;
+-	drm_i915_sarea_t *sarea_priv;
+-	drm_i915_cmdbuffer_t *cmdbuf = data;
+-	struct drm_clip_rect *cliprects = NULL;
+-	void *batch_data;
+-	int ret;
+-
+-	DRM_DEBUG_DRIVER("i915 cmdbuffer, buf %p sz %d cliprects %d\n",
+-			cmdbuf->buf, cmdbuf->sz, cmdbuf->num_cliprects);
+-
+-	if (drm_core_check_feature(dev, DRIVER_MODESET))
+-		return -ENODEV;
+-
+-	master_priv = dev->primary->master->driver_priv;
+-	sarea_priv = (drm_i915_sarea_t *) master_priv->sarea_priv;
+-
+-	RING_LOCK_TEST_WITH_RETURN(dev, file_priv);
+-
+-	if (cmdbuf->num_cliprects < 0)
+-		return -EINVAL;
+-
+-	batch_data = kmalloc(cmdbuf->sz, GFP_KERNEL);
+-	if (batch_data == NULL)
+-		return -ENOMEM;
+-
+-	ret = copy_from_user(batch_data, cmdbuf->buf, cmdbuf->sz);
+-	if (ret != 0) {
+-		ret = -EFAULT;
+-		goto fail_batch_free;
+-	}
+-
+-	if (cmdbuf->num_cliprects) {
+-		cliprects = kcalloc(cmdbuf->num_cliprects,
+-				    sizeof(*cliprects), GFP_KERNEL);
+-		if (cliprects == NULL) {
+-			ret = -ENOMEM;
+-			goto fail_batch_free;
+-		}
+-
+-		ret = copy_from_user(cliprects, cmdbuf->cliprects,
+-				     cmdbuf->num_cliprects *
+-				     sizeof(struct drm_clip_rect));
+-		if (ret != 0) {
+-			ret = -EFAULT;
+-			goto fail_clip_free;
+-		}
+-	}
+-
+-	mutex_lock(&dev->struct_mutex);
+-	ret = i915_dispatch_cmdbuffer(dev, cmdbuf, cliprects, batch_data);
+-	mutex_unlock(&dev->struct_mutex);
+-	if (ret) {
+-		DRM_ERROR("i915_dispatch_cmdbuffer failed\n");
+-		goto fail_clip_free;
+-	}
+-
+-	if (sarea_priv)
+-		sarea_priv->last_dispatch = READ_BREADCRUMB(dev_priv);
+-
+-fail_clip_free:
+-	kfree(cliprects);
+-fail_batch_free:
+-	kfree(batch_data);
+-
+-	return ret;
+-}
+-
+-static int i915_emit_irq(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct drm_i915_master_private *master_priv = dev->primary->master->driver_priv;
+-
+-	i915_kernel_lost_context(dev);
+-
+-	DRM_DEBUG_DRIVER("\n");
+-
+-	dev_priv->dri1.counter++;
+-	if (dev_priv->dri1.counter > 0x7FFFFFFFUL)
+-		dev_priv->dri1.counter = 1;
+-	if (master_priv->sarea_priv)
+-		master_priv->sarea_priv->last_enqueue = dev_priv->dri1.counter;
+-
+-	if (BEGIN_LP_RING(4) == 0) {
+-		OUT_RING(MI_STORE_DWORD_INDEX);
+-		OUT_RING(I915_BREADCRUMB_INDEX << MI_STORE_DWORD_INDEX_SHIFT);
+-		OUT_RING(dev_priv->dri1.counter);
+-		OUT_RING(MI_USER_INTERRUPT);
+-		ADVANCE_LP_RING();
+-	}
+-
+-	return dev_priv->dri1.counter;
+-}
+-
+-static int i915_wait_irq(struct drm_device *dev, int irq_nr)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct drm_i915_master_private *master_priv = dev->primary->master->driver_priv;
+-	int ret = 0;
+-	struct intel_engine_cs *ring = LP_RING(dev_priv);
+-
+-	DRM_DEBUG_DRIVER("irq_nr=%d breadcrumb=%d\n", irq_nr,
+-		  READ_BREADCRUMB(dev_priv));
+-
+-	if (READ_BREADCRUMB(dev_priv) >= irq_nr) {
+-		if (master_priv->sarea_priv)
+-			master_priv->sarea_priv->last_dispatch = READ_BREADCRUMB(dev_priv);
+-		return 0;
+-	}
+-
+-	if (master_priv->sarea_priv)
+-		master_priv->sarea_priv->perf_boxes |= I915_BOX_WAIT;
+-
+-	if (ring->irq_get(ring)) {
+-		DRM_WAIT_ON(ret, ring->irq_queue, 3 * HZ,
+-			    READ_BREADCRUMB(dev_priv) >= irq_nr);
+-		ring->irq_put(ring);
+-	} else if (wait_for(READ_BREADCRUMB(dev_priv) >= irq_nr, 3000))
+-		ret = -EBUSY;
+-
+-	if (ret == -EBUSY) {
+-		DRM_ERROR("EBUSY -- rec: %d emitted: %d\n",
+-			  READ_BREADCRUMB(dev_priv), (int)dev_priv->dri1.counter);
+-	}
+-
+-	return ret;
++	return -ENODEV;
+ }
+ 
+-/* Needs the lock as it touches the ring.
+- */
+ static int i915_irq_emit(struct drm_device *dev, void *data,
+ 			 struct drm_file *file_priv)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	drm_i915_irq_emit_t *emit = data;
+-	int result;
+-
+-	if (drm_core_check_feature(dev, DRIVER_MODESET))
+-		return -ENODEV;
+-
+-	if (!dev_priv || !LP_RING(dev_priv)->buffer->virtual_start) {
+-		DRM_ERROR("called with no initialization\n");
+-		return -EINVAL;
+-	}
+-
+-	RING_LOCK_TEST_WITH_RETURN(dev, file_priv);
+-
+-	mutex_lock(&dev->struct_mutex);
+-	result = i915_emit_irq(dev);
+-	mutex_unlock(&dev->struct_mutex);
+-
+-	if (copy_to_user(emit->irq_seq, &result, sizeof(int))) {
+-		DRM_ERROR("copy_to_user\n");
+-		return -EFAULT;
+-	}
+-
+-	return 0;
++	return -ENODEV;
+ }
+ 
+-/* Doesn't need the hardware lock.
+- */
+ static int i915_irq_wait(struct drm_device *dev, void *data,
+ 			 struct drm_file *file_priv)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	drm_i915_irq_wait_t *irqwait = data;
+-
+-	if (drm_core_check_feature(dev, DRIVER_MODESET))
+-		return -ENODEV;
+-
+-	if (!dev_priv) {
+-		DRM_ERROR("called with no initialization\n");
+-		return -EINVAL;
+-	}
+-
+-	return i915_wait_irq(dev, irqwait->irq_seq);
++	return -ENODEV;
+ }
+ 
+ static int i915_vblank_pipe_get(struct drm_device *dev, void *data,
+ 			 struct drm_file *file_priv)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	drm_i915_vblank_pipe_t *pipe = data;
+-
+-	if (drm_core_check_feature(dev, DRIVER_MODESET))
+-		return -ENODEV;
+-
+-	if (!dev_priv) {
+-		DRM_ERROR("called with no initialization\n");
+-		return -EINVAL;
+-	}
+-
+-	pipe->pipe = DRM_I915_VBLANK_PIPE_A | DRM_I915_VBLANK_PIPE_B;
+-
+-	return 0;
++	return -ENODEV;
+ }
+ 
+-/**
+- * Schedule buffer swap at given vertical blank.
+- */
+ static int i915_vblank_swap(struct drm_device *dev, void *data,
+ 		     struct drm_file *file_priv)
+ {
+-	/* The delayed swap mechanism was fundamentally racy, and has been
+-	 * removed.  The model was that the client requested a delayed flip/swap
+-	 * from the kernel, then waited for vblank before continuing to perform
+-	 * rendering.  The problem was that the kernel might wake the client
+-	 * up before it dispatched the vblank swap (since the lock has to be
+-	 * held while touching the ringbuffer), in which case the client would
+-	 * clear and start the next frame before the swap occurred, and
+-	 * flicker would occur in addition to likely missing the vblank.
+-	 *
+-	 * In the absence of this ioctl, userland falls back to a correct path
+-	 * of waiting for a vblank, then dispatching the swap on its own.
+-	 * Context switching to userland and back is plenty fast enough for
+-	 * meeting the requirements of vblank swapping.
+-	 */
+-	return -EINVAL;
++	return -ENODEV;
+ }
+ 
+ static int i915_flip_bufs(struct drm_device *dev, void *data,
+ 			  struct drm_file *file_priv)
+ {
+-	int ret;
+-
+-	if (drm_core_check_feature(dev, DRIVER_MODESET))
+-		return -ENODEV;
+-
+-	DRM_DEBUG_DRIVER("%s\n", __func__);
+-
+-	RING_LOCK_TEST_WITH_RETURN(dev, file_priv);
+-
+-	mutex_lock(&dev->struct_mutex);
+-	ret = i915_dispatch_flip(dev);
+-	mutex_unlock(&dev->struct_mutex);
+-
+-	return ret;
++	return -ENODEV;
+ }
+ 
+ static int i915_getparam(struct drm_device *dev, void *data,
+@@ -941,14 +118,11 @@
+ 
+ 	switch (param->param) {
+ 	case I915_PARAM_IRQ_ACTIVE:
+-		value = dev->pdev->irq ? 1 : 0;
+-		break;
++		return -ENODEV;
+ 	case I915_PARAM_ALLOW_BATCHBUFFER:
+-		value = dev_priv->dri1.allow_batchbuffer ? 1 : 0;
+-		break;
++		return -ENODEV;
+ 	case I915_PARAM_LAST_DISPATCH:
+-		value = READ_BREADCRUMB(dev_priv);
+-		break;
++		return -ENODEV;
+ 	case I915_PARAM_CHIPSET_ID:
+ 		value = dev->pdev->device;
+ 		break;
+@@ -969,13 +143,13 @@
+ 		value = 1;
+ 		break;
+ 	case I915_PARAM_HAS_BSD:
+-		value = intel_ring_initialized(&dev_priv->ring[VCS]);
++		value = intel_engine_initialized(&dev_priv->engine[VCS]);
+ 		break;
+ 	case I915_PARAM_HAS_BLT:
+-		value = intel_ring_initialized(&dev_priv->ring[BCS]);
++		value = intel_engine_initialized(&dev_priv->engine[BCS]);
+ 		break;
+ 	case I915_PARAM_HAS_VEBOX:
+-		value = intel_ring_initialized(&dev_priv->ring[VECS]);
++		value = intel_engine_initialized(&dev_priv->engine[VECS]);
+ 		break;
+ 	case I915_PARAM_HAS_RELAXED_FENCING:
+ 		value = 1;
+@@ -999,13 +173,13 @@
+ 		value = HAS_WT(dev);
+ 		break;
+ 	case I915_PARAM_HAS_ALIASING_PPGTT:
+-		value = dev_priv->mm.aliasing_ppgtt || USES_FULL_PPGTT(dev);
++		value = USES_PPGTT(dev);
+ 		break;
+ 	case I915_PARAM_HAS_WAIT_TIMEOUT:
+ 		value = 1;
+ 		break;
+ 	case I915_PARAM_HAS_SEMAPHORES:
+-		value = i915_semaphore_is_enabled(dev);
++		value = RCS_ENGINE(dev_priv)->semaphore.wait != NULL;
+ 		break;
+ 	case I915_PARAM_HAS_PRIME_VMAP_FLUSH:
+ 		value = 1;
+@@ -1025,6 +199,12 @@
+ 	case I915_PARAM_CMD_PARSER_VERSION:
+ 		value = i915_cmd_parser_get_version();
+ 		break;
++	case I915_PARAM_HAS_COHERENT_PHYS_GTT:
++		value = 1;
++		break;
++	case I915_PARAM_MMAP_VERSION:
++		value = 1;
++		break;
+ 	default:
+ 		DRM_DEBUG("Unknown parameter %d\n", param->param);
+ 		return -EINVAL;
+@@ -1051,12 +231,10 @@
+ 
+ 	switch (param->param) {
+ 	case I915_SETPARAM_USE_MI_BATCHBUFFER_START:
+-		break;
+ 	case I915_SETPARAM_TEX_LRU_LOG_GRANULARITY:
+-		break;
+ 	case I915_SETPARAM_ALLOW_BATCHBUFFER:
+-		dev_priv->dri1.allow_batchbuffer = param->value ? 1 : 0;
+-		break;
++		return -ENODEV;
++
+ 	case I915_SETPARAM_NUM_USED_FENCES:
+ 		if (param->value > dev_priv->num_fence_regs ||
+ 		    param->value < 0)
+@@ -1076,49 +254,7 @@
+ static int i915_set_status_page(struct drm_device *dev, void *data,
+ 				struct drm_file *file_priv)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	drm_i915_hws_addr_t *hws = data;
+-	struct intel_engine_cs *ring;
+-
+-	if (drm_core_check_feature(dev, DRIVER_MODESET))
+-		return -ENODEV;
+-
+-	if (!I915_NEED_GFX_HWS(dev))
+-		return -EINVAL;
+-
+-	if (!dev_priv) {
+-		DRM_ERROR("called with no initialization\n");
+-		return -EINVAL;
+-	}
+-
+-	if (drm_core_check_feature(dev, DRIVER_MODESET)) {
+-		WARN(1, "tried to set status page when mode setting active\n");
+-		return 0;
+-	}
+-
+-	DRM_DEBUG_DRIVER("set status page addr 0x%08x\n", (u32)hws->addr);
+-
+-	ring = LP_RING(dev_priv);
+-	ring->status_page.gfx_addr = hws->addr & (0x1ffff<<12);
+-
+-	dev_priv->dri1.gfx_hws_cpu_addr =
+-		ioremap_wc(dev_priv->gtt.mappable_base + hws->addr, 4096);
+-	if (dev_priv->dri1.gfx_hws_cpu_addr == NULL) {
+-		i915_dma_cleanup(dev);
+-		ring->status_page.gfx_addr = 0;
+-		DRM_ERROR("can not ioremap virtual address for"
+-				" G33 hw status page\n");
+-		return -ENOMEM;
+-	}
+-
+-	memset_io(dev_priv->dri1.gfx_hws_cpu_addr, 0, PAGE_SIZE);
+-	I915_WRITE(HWS_PGA, ring->status_page.gfx_addr);
+-
+-	DRM_DEBUG_DRIVER("load hws HWS_PGA with gfx mem 0x%x\n",
+-			 ring->status_page.gfx_addr);
+-	DRM_DEBUG_DRIVER("load hws at %p\n",
+-			 ring->status_page.page_addr);
+-	return 0;
++	return -ENODEV;
+ }
+ 
+ static int i915_get_bridge_dev(struct drm_device *dev)
+@@ -1273,12 +409,12 @@
+ 		dev->switch_power_state = DRM_SWITCH_POWER_CHANGING;
+ 		/* i915 resume handler doesn't set to D0 */
+ 		pci_set_power_state(dev->pdev, PCI_D0);
+-		i915_resume(dev);
++		i915_resume_legacy(dev);
+ 		dev->switch_power_state = DRM_SWITCH_POWER_ON;
+ 	} else {
+ 		pr_err("switched off\n");
+ 		dev->switch_power_state = DRM_SWITCH_POWER_CHANGING;
+-		i915_suspend(dev, pmm);
++		i915_suspend_legacy(dev, pmm);
+ 		dev->switch_power_state = DRM_SWITCH_POWER_OFF;
+ 	}
+ }
+@@ -1318,44 +454,45 @@
+ 	 * vga_client_register() fails with -ENODEV.
+ 	 */
+ 	ret = vga_client_register(dev->pdev, dev, NULL, i915_vga_set_decode);
+-	if (ret && ret != -ENODEV)
++	if (ret && ret != -ENODEV) {
++		DRM_ERROR("unable to register VGA client\n");
+ 		goto out;
++	}
+ 
+ 	intel_register_dsm_handler();
+ 
+ 	ret = vga_switcheroo_register_client(dev->pdev, &i915_switcheroo_ops, false);
+-	if (ret)
++	if (ret) {
++		DRM_ERROR("unable to register VGA switcheroo\n");
+ 		goto cleanup_vga_client;
++	}
+ 
+ 	/* Initialise stolen first so that we may reserve preallocated
+ 	 * objects for the BIOS to KMS transition.
+ 	 */
+ 	ret = i915_gem_init_stolen(dev);
+-	if (ret)
++	if (ret) {
++		DRM_ERROR("unable to initialise stolen memory\n");
+ 		goto cleanup_vga_switcheroo;
++	}
+ 
+ 	intel_power_domains_init_hw(dev_priv);
+ 
+-	/*
+-	 * We enable some interrupt sources in our postinstall hooks, so mark
+-	 * interrupts as enabled _before_ actually enabling them to avoid
+-	 * special cases in our ordering checks.
+-	 */
+-	dev_priv->pm._irqs_disabled = false;
+-
+-	ret = drm_irq_install(dev, dev->pdev->irq);
+-	if (ret)
++	ret = intel_irq_install(dev_priv);
++	if (ret) {
++		DRM_ERROR("unable to install IRQ\n");
+ 		goto cleanup_gem_stolen;
++	}
+ 
+ 	/* Important: The output setup functions called by modeset_init need
+ 	 * working irqs for e.g. gmbus and dp aux transfers. */
+ 	intel_modeset_init(dev);
+ 
+ 	ret = i915_gem_init(dev);
+-	if (ret)
++	if (ret) {
++		DRM_ERROR("unable to initialise GEM\n");
+ 		goto cleanup_irq;
+-
+-	INIT_WORK(&dev_priv->console_resume_work, intel_console_resume);
++	}
+ 
+ 	intel_modeset_gem_init(dev);
+ 
+@@ -1366,11 +503,13 @@
+ 		return 0;
+ 
+ 	ret = intel_fbdev_init(dev);
+-	if (ret)
++	if (ret) {
++		DRM_ERROR("unable to initialise fbdev\n");
+ 		goto cleanup_gem;
++	}
+ 
+ 	/* Only enable hotplug handling once the fbdev is fully set up. */
+-	intel_hpd_init(dev);
++	intel_hpd_init(dev_priv);
+ 
+ 	/*
+ 	 * Some ports require correctly set-up hpd registers for detection to
+@@ -1382,7 +521,7 @@
+ 	 * scanning against hotplug events. Hence do this first and ignore the
+ 	 * tiny window where we will loose hotplug notifactions.
+ 	 */
+-	intel_fbdev_initial_config(dev);
++	async_schedule(intel_fbdev_initial_config, dev_priv);
+ 
+ 	drm_kms_helper_poll_init(dev);
+ 
+@@ -1390,11 +529,10 @@
+ 
+ cleanup_gem:
+ 	mutex_lock(&dev->struct_mutex);
+-	i915_gem_cleanup_ringbuffer(dev);
+-	i915_gem_context_fini(dev);
++	i915_gem_fini(dev);
+ 	mutex_unlock(&dev->struct_mutex);
+-	WARN_ON(dev_priv->mm.aliasing_ppgtt);
+ cleanup_irq:
++	intel_modeset_cleanup(dev);
+ 	drm_irq_uninstall(dev);
+ cleanup_gem_stolen:
+ 	i915_gem_cleanup_stolen(dev);
+@@ -1406,30 +544,6 @@
+ 	return ret;
+ }
+ 
+-int i915_master_create(struct drm_device *dev, struct drm_master *master)
+-{
+-	struct drm_i915_master_private *master_priv;
+-
+-	master_priv = kzalloc(sizeof(*master_priv), GFP_KERNEL);
+-	if (!master_priv)
+-		return -ENOMEM;
+-
+-	master->driver_priv = master_priv;
+-	return 0;
+-}
+-
+-void i915_master_destroy(struct drm_device *dev, struct drm_master *master)
+-{
+-	struct drm_i915_master_private *master_priv = master->driver_priv;
+-
+-	if (!master_priv)
+-		return;
+-
+-	kfree(master_priv);
+-
+-	master->driver_priv = NULL;
+-}
+-
+ #if IS_ENABLED(CONFIG_FB)
+ static int i915_kick_out_firmware_fb(struct drm_i915_private *dev_priv)
+ {
+@@ -1535,14 +649,14 @@
+ 
+ 	info = (struct intel_device_info *)&dev_priv->info;
+ 
+-	if (IS_VALLEYVIEW(dev))
+-		for_each_pipe(pipe)
++	if (IS_VALLEYVIEW(dev) || INTEL_INFO(dev)->gen == 9)
++		for_each_pipe(dev_priv, pipe)
+ 			info->num_sprites[pipe] = 2;
+ 	else
+-		for_each_pipe(pipe)
++		for_each_pipe(dev_priv, pipe)
+ 			info->num_sprites[pipe] = 1;
+ 
+-	if (i915.disable_display) {
++	if (i915_module.disable_display) {
+ 		DRM_INFO("Display disabled (module parameter)\n");
+ 		info->num_pipes = 0;
+ 	} else if (info->num_pipes > 0 &&
+@@ -1601,6 +715,8 @@
+ 	if (!drm_core_check_feature(dev, DRIVER_MODESET) && !dev->agp)
+ 		return -EINVAL;
+ 
++	BUILD_BUG_ON(I915_NUM_ENGINES >= (1 << I915_NUM_ENGINE_BITS));
++
+ 	dev_priv = kzalloc(sizeof(*dev_priv), GFP_KERNEL);
+ 	if (dev_priv == NULL)
+ 		return -ENOMEM;
+@@ -1608,13 +724,14 @@
+ 	dev->dev_private = dev_priv;
+ 	dev_priv->dev = dev;
+ 
+-	/* copy initial configuration to dev_priv->info */
++	/* Setup the write-once "constant" device info */
+ 	device_info = (struct intel_device_info *)&dev_priv->info;
+-	*device_info = *info;
++	memcpy(device_info, info, sizeof(dev_priv->info));
++	device_info->device_id = dev->pdev->device;
+ 
+ 	spin_lock_init(&dev_priv->irq_lock);
+ 	spin_lock_init(&dev_priv->gpu_error.lock);
+-	spin_lock_init(&dev_priv->backlight_lock);
++	mutex_init(&dev_priv->backlight_lock);
+ 	spin_lock_init(&dev_priv->uncore.lock);
+ 	spin_lock_init(&dev_priv->mm.object_stat_lock);
+ 	spin_lock_init(&dev_priv->mmio_flip_lock);
+@@ -1670,15 +787,17 @@
+ 		goto out_regs;
+ 
+ 	if (drm_core_check_feature(dev, DRIVER_MODESET)) {
+-		ret = i915_kick_out_vgacon(dev_priv);
++		/* WARNING: Apparently we must kick fbdev drivers before vgacon,
++		 * otherwise the vga fbdev driver falls over. */
++		ret = i915_kick_out_firmware_fb(dev_priv);
+ 		if (ret) {
+-			DRM_ERROR("failed to remove conflicting VGA console\n");
++			DRM_ERROR("failed to remove conflicting framebuffer drivers\n");
+ 			goto out_gtt;
+ 		}
+ 
+-		ret = i915_kick_out_firmware_fb(dev_priv);
++		ret = i915_kick_out_vgacon(dev_priv);
+ 		if (ret) {
+-			DRM_ERROR("failed to remove conflicting framebuffer drivers\n");
++			DRM_ERROR("failed to remove conflicting VGA console\n");
+ 			goto out_gtt;
+ 		}
+ 	}
+@@ -1740,7 +859,7 @@
+ 		goto out_freewq;
+ 	}
+ 
+-	intel_irq_init(dev);
++	intel_irq_init(dev_priv);
+ 	intel_uncore_sanitize(dev);
+ 
+ 	/* Try to make sure MCHBAR is enabled before poking at it */
+@@ -1798,12 +917,13 @@
+ 	if (IS_GEN5(dev))
+ 		intel_gpu_ips_init(dev_priv);
+ 
+-	intel_init_runtime_pm(dev_priv);
++	intel_runtime_pm_enable(dev_priv);
++	i915_perf_register(dev);
+ 
+ 	return 0;
+ 
+ out_power_well:
+-	intel_power_domains_remove(dev_priv);
++	intel_power_domains_fini(dev_priv);
+ 	drm_vblank_cleanup(dev);
+ out_gem_unload:
+ 	WARN_ON(unregister_oom_notifier(&dev_priv->mm.oom_notifier));
+@@ -1822,7 +942,9 @@
+ 	arch_phys_wc_del(dev_priv->gtt.mtrr);
+ 	io_mapping_free(dev_priv->gtt.mappable);
+ out_gtt:
+-	dev_priv->gtt.base.cleanup(&dev_priv->gtt.base);
++	mutex_lock(&dev->struct_mutex);
++	i915_global_gtt_cleanup(dev);
++	mutex_unlock(&dev->struct_mutex);
+ out_regs:
+ 	intel_uncore_fini(dev);
+ 	pci_iounmap(dev->pdev, dev_priv->regs);
+@@ -1846,16 +968,11 @@
+ 		return ret;
+ 	}
+ 
+-	intel_fini_runtime_pm(dev_priv);
++	i915_perf_unregister(dev);
++	intel_power_domains_fini(dev_priv);
+ 
+ 	intel_gpu_ips_teardown();
+ 
+-	/* The i915.ko module is still not prepared to be loaded when
+-	 * the power well is not enabled, so just enable it in case
+-	 * we're going to unload/reload. */
+-	intel_display_set_init_power(dev_priv, true);
+-	intel_power_domains_remove(dev_priv);
+-
+ 	i915_teardown_sysfs(dev);
+ 
+ 	WARN_ON(unregister_oom_notifier(&dev_priv->mm.oom_notifier));
+@@ -1866,10 +983,13 @@
+ 
+ 	acpi_video_unregister();
+ 
+-	if (drm_core_check_feature(dev, DRIVER_MODESET)) {
++	if (drm_core_check_feature(dev, DRIVER_MODESET))
+ 		intel_fbdev_fini(dev);
++
++	drm_vblank_cleanup(dev);
++
++	if (drm_core_check_feature(dev, DRIVER_MODESET)) {
+ 		intel_modeset_cleanup(dev);
+-		cancel_work_sync(&dev_priv->console_resume_work);
+ 
+ 		/*
+ 		 * free the memory space allocated for the child device
+@@ -1886,7 +1006,7 @@
+ 	}
+ 
+ 	/* Free error state after interrupts are fully disabled. */
+-	del_timer_sync(&dev_priv->gpu_error.hangcheck_timer);
++	cancel_delayed_work_sync(&dev_priv->gpu_error.hangcheck_work);
+ 	cancel_work_sync(&dev_priv->gpu_error.work);
+ 	i915_destroy_error_state(dev);
+ 
+@@ -1900,20 +1020,10 @@
+ 		flush_workqueue(dev_priv->wq);
+ 
+ 		mutex_lock(&dev->struct_mutex);
+-		i915_gem_cleanup_ringbuffer(dev);
+-		i915_gem_context_fini(dev);
+-		WARN_ON(dev_priv->mm.aliasing_ppgtt);
++		i915_gem_fini(dev);
+ 		mutex_unlock(&dev->struct_mutex);
+-		i915_gem_cleanup_stolen(dev);
+-
+-		if (!I915_NEED_GFX_HWS(dev))
+-			i915_free_hws(dev);
+ 	}
+ 
+-	WARN_ON(!list_empty(&dev_priv->vm_list));
+-
+-	drm_vblank_cleanup(dev);
+-
+ 	intel_teardown_gmbus(dev);
+ 	intel_teardown_mchbar(dev);
+ 
+@@ -1921,7 +1031,10 @@
+ 	destroy_workqueue(dev_priv->wq);
+ 	pm_qos_remove_request(&dev_priv->pm_qos);
+ 
+-	dev_priv->gtt.base.cleanup(&dev_priv->gtt.base);
++	mutex_lock(&dev->struct_mutex);
++	i915_global_gtt_cleanup(dev);
++	i915_gem_cleanup_stolen(dev);
++	mutex_unlock(&dev->struct_mutex);
+ 
+ 	intel_uncore_fini(dev);
+ 	if (dev_priv->regs != NULL)
+@@ -1976,24 +1089,32 @@
+ 	}
+ 
+ 	i915_gem_lastclose(dev);
+-
+-	i915_dma_cleanup(dev);
+ }
+ 
+ void i915_driver_preclose(struct drm_device *dev, struct drm_file *file)
+ {
++	struct drm_i915_private *dev_priv = to_i915(dev);
++	bool was_interruptible;
++
+ 	mutex_lock(&dev->struct_mutex);
++	was_interruptible = dev_priv->mm.interruptible;
++	WARN_ON(!was_interruptible);
++	dev_priv->mm.interruptible = false;
++
+ 	i915_gem_context_close(dev, file);
+ 	i915_gem_release(dev, file);
++
++	dev_priv->mm.interruptible = was_interruptible;
+ 	mutex_unlock(&dev->struct_mutex);
++
++	if (drm_core_check_feature(dev, DRIVER_MODESET))
++		intel_modeset_preclose(dev, file);
+ }
+ 
+ void i915_driver_postclose(struct drm_device *dev, struct drm_file *file)
+ {
+ 	struct drm_i915_file_private *file_priv = file->driver_priv;
+ 
+-	if (file_priv && file_priv->bsd_ring)
+-		file_priv->bsd_ring = NULL;
+ 	kfree(file_priv);
+ }
+ 
+@@ -2048,6 +1169,9 @@
+ 	DRM_IOCTL_DEF_DRV(I915_REG_READ, i915_reg_read_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
+ 	DRM_IOCTL_DEF_DRV(I915_GET_RESET_STATS, i915_get_reset_stats_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
+ 	DRM_IOCTL_DEF_DRV(I915_GEM_USERPTR, i915_gem_userptr_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
++	DRM_IOCTL_DEF_DRV(I915_GEM_CONTEXT_GETPARAM, i915_gem_context_getparam_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
++	DRM_IOCTL_DEF_DRV(I915_GEM_CONTEXT_SETPARAM, i915_gem_context_setparam_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
++	DRM_IOCTL_DEF_DRV(I915_GEM_CONTEXT_DUMP, i915_gem_context_dump_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
+ };
+ 
+ int i915_max_ioctl = ARRAY_SIZE(i915_ioctls);
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_drv.c b/drivers/gpu/drm/i915/i915_drv.c
+--- a/drivers/gpu/drm/i915/i915_drv.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/i915_drv.c	2014-11-20 09:53:37.960762838 -0700
+@@ -356,6 +356,19 @@
+ 	CURSOR_OFFSETS,
+ };
+ 
++static const struct intel_device_info intel_skylake_info = {
++	.is_preliminary = 1,
++	.is_skylake = 1,
++	.gen = 9, .num_pipes = 3,
++	.need_gfx_hws = 1, .has_hotplug = 1,
++	.ring_mask = RENDER_RING | BSD_RING | BLT_RING | VEBOX_RING,
++	.has_llc = 1,
++	.has_ddi = 1,
++	.has_fbc = 1,
++	GEN_DEFAULT_PIPEOFFSETS,
++	IVB_CURSOR_OFFSETS,
++};
++
+ /*
+  * Make sure any device matches here are from most specific to most
+  * general.  For example, since the Quanta match is based on the subsystem
+@@ -392,7 +405,8 @@
+ 	INTEL_BDW_GT12D_IDS(&intel_broadwell_d_info),	\
+ 	INTEL_BDW_GT3M_IDS(&intel_broadwell_gt3m_info),	\
+ 	INTEL_BDW_GT3D_IDS(&intel_broadwell_gt3d_info), \
+-	INTEL_CHV_IDS(&intel_cherryview_info)
++	INTEL_CHV_IDS(&intel_cherryview_info),	\
++	INTEL_SKL_IDS(&intel_skylake_info)
+ 
+ static const struct pci_device_id pciidlist[] = {		/* aka */
+ 	INTEL_PCI_IDS,
+@@ -449,7 +463,7 @@
+ 				dev_priv->pch_type = PCH_LPT;
+ 				DRM_DEBUG_KMS("Found LynxPoint PCH\n");
+ 				WARN_ON(!IS_HASWELL(dev));
+-				WARN_ON(IS_ULT(dev));
++				WARN_ON(IS_HSW_ULT(dev));
+ 			} else if (IS_BROADWELL(dev)) {
+ 				dev_priv->pch_type = PCH_LPT;
+ 				dev_priv->pch_id =
+@@ -460,7 +474,15 @@
+ 				dev_priv->pch_type = PCH_LPT;
+ 				DRM_DEBUG_KMS("Found LynxPoint LP PCH\n");
+ 				WARN_ON(!IS_HASWELL(dev));
+-				WARN_ON(!IS_ULT(dev));
++				WARN_ON(!IS_HSW_ULT(dev));
++			} else if (id == INTEL_PCH_SPT_DEVICE_ID_TYPE) {
++				dev_priv->pch_type = PCH_SPT;
++				DRM_DEBUG_KMS("Found SunrisePoint PCH\n");
++				WARN_ON(!IS_SKYLAKE(dev));
++			} else if (id == INTEL_PCH_SPT_LP_DEVICE_ID_TYPE) {
++				dev_priv->pch_type = PCH_SPT;
++				DRM_DEBUG_KMS("Found SunrisePoint LP PCH\n");
++				WARN_ON(!IS_SKYLAKE(dev));
+ 			} else
+ 				continue;
+ 
+@@ -473,26 +495,6 @@
+ 	pci_dev_put(pch);
+ }
+ 
+-bool i915_semaphore_is_enabled(struct drm_device *dev)
+-{
+-	if (INTEL_INFO(dev)->gen < 6)
+-		return false;
+-
+-	if (i915.semaphores >= 0)
+-		return i915.semaphores;
+-
+-	/* Until we get further testing... */
+-	if (IS_GEN8(dev))
+-		return false;
+-
+-#ifdef CONFIG_INTEL_IOMMU
+-	/* Enable semaphores on SNB when IO remapping is off */
+-	if (INTEL_INFO(dev)->gen == 6 && intel_iommu_gfx_mapped)
+-		return false;
+-#endif
+-
+-	return true;
+-}
+ 
+ void intel_hpd_cancel_work(struct drm_i915_private *dev_priv)
+ {
+@@ -505,7 +507,7 @@
+ 	spin_unlock_irq(&dev_priv->irq_lock);
+ 
+ 	cancel_work_sync(&dev_priv->dig_port_work);
+-	cancel_work_sync(&dev_priv->hotplug_work);
++	cancel_delayed_work_sync(&dev_priv->hotplug_work);
+ 	cancel_delayed_work_sync(&dev_priv->hotplug_reenable_work);
+ }
+ 
+@@ -524,7 +526,11 @@
+ 	drm_modeset_unlock_all(dev);
+ }
+ 
+-static int i915_drm_freeze(struct drm_device *dev)
++static int intel_suspend_complete(struct drm_i915_private *dev_priv);
++static int vlv_resume_prepare(struct drm_i915_private *dev_priv,
++			      bool rpm_resume);
++
++static int i915_drm_suspend(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct drm_crtc *crtc;
+@@ -567,14 +573,14 @@
+ 
+ 		flush_delayed_work(&dev_priv->rps.delayed_resume_work);
+ 
+-		intel_runtime_pm_disable_interrupts(dev);
++		intel_runtime_pm_disable_interrupts(dev_priv);
+ 		intel_hpd_cancel_work(dev_priv);
+ 
+ 		intel_suspend_encoders(dev_priv);
+ 
+ 		intel_suspend_gt_powersave(dev);
+ 
+-		intel_modeset_suspend_hw(dev);
++		intel_suspend_hw(dev);
+ 	}
+ 
+ 	i915_gem_suspend_gtt_mappings(dev);
+@@ -591,9 +597,7 @@
+ 	intel_uncore_forcewake_reset(dev, false);
+ 	intel_opregion_fini(dev);
+ 
+-	console_lock();
+-	intel_fbdev_set_suspend(dev, FBINFO_STATE_SUSPENDED);
+-	console_unlock();
++	intel_fbdev_set_suspend(dev, FBINFO_STATE_SUSPENDED, true);
+ 
+ 	dev_priv->suspend_count++;
+ 
+@@ -602,7 +606,26 @@
+ 	return 0;
+ }
+ 
+-int i915_suspend(struct drm_device *dev, pm_message_t state)
++static int i915_drm_suspend_late(struct drm_device *drm_dev)
++{
++	struct drm_i915_private *dev_priv = drm_dev->dev_private;
++	int ret;
++
++	ret = intel_suspend_complete(dev_priv);
++
++	if (ret) {
++		DRM_ERROR("Suspend complete failed: %d\n", ret);
++
++		return ret;
++	}
++
++	pci_disable_device(drm_dev->pdev);
++	pci_set_power_state(drm_dev->pdev, PCI_D3hot);
++
++	return 0;
++}
++
++int i915_suspend_legacy(struct drm_device *dev, pm_message_t state)
+ {
+ 	int error;
+ 
+@@ -612,58 +635,25 @@
+ 		return -ENODEV;
+ 	}
+ 
+-	if (state.event == PM_EVENT_PRETHAW)
+-		return 0;
+-
++	if (WARN_ON_ONCE(state.event != PM_EVENT_SUSPEND &&
++			 state.event != PM_EVENT_FREEZE))
++		return -EINVAL;
+ 
+ 	if (dev->switch_power_state == DRM_SWITCH_POWER_OFF)
+ 		return 0;
+ 
+-	error = i915_drm_freeze(dev);
++	error = i915_drm_suspend(dev);
+ 	if (error)
+ 		return error;
+ 
+-	if (state.event == PM_EVENT_SUSPEND) {
+-		/* Shut down the device */
+-		pci_disable_device(dev->pdev);
+-		pci_set_power_state(dev->pdev, PCI_D3hot);
+-	}
+-
+-	return 0;
+-}
+-
+-void intel_console_resume(struct work_struct *work)
+-{
+-	struct drm_i915_private *dev_priv =
+-		container_of(work, struct drm_i915_private,
+-			     console_resume_work);
+-	struct drm_device *dev = dev_priv->dev;
+-
+-	console_lock();
+-	intel_fbdev_set_suspend(dev, FBINFO_STATE_RUNNING);
+-	console_unlock();
++	return i915_drm_suspend_late(dev);
+ }
+ 
+-static int i915_drm_thaw_early(struct drm_device *dev)
++static int i915_drm_resume(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
+-		hsw_disable_pc8(dev_priv);
+-
+-	intel_uncore_early_sanitize(dev, true);
+-	intel_uncore_sanitize(dev);
+-	intel_power_domains_init_hw(dev_priv);
+-
+-	return 0;
+-}
+-
+-static int __i915_drm_thaw(struct drm_device *dev, bool restore_gtt_mappings)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-
+-	if (drm_core_check_feature(dev, DRIVER_MODESET) &&
+-	    restore_gtt_mappings) {
++	if (drm_core_check_feature(dev, DRIVER_MODESET)) {
+ 		mutex_lock(&dev->struct_mutex);
+ 		i915_gem_restore_gtt_mappings(dev);
+ 		mutex_unlock(&dev->struct_mutex);
+@@ -684,17 +674,15 @@
+ 		}
+ 		mutex_unlock(&dev->struct_mutex);
+ 
+-		intel_runtime_pm_restore_interrupts(dev);
++		/* We need working interrupts for modeset enabling ... */
++		intel_runtime_pm_enable_interrupts(dev_priv);
+ 
+ 		intel_modeset_init_hw(dev);
+ 
+-		{
+-			unsigned long irqflags;
+-			spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
+-			if (dev_priv->display.hpd_irq_setup)
+-				dev_priv->display.hpd_irq_setup(dev);
+-			spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
+-		}
++		spin_lock_irq(&dev_priv->irq_lock);
++		if (dev_priv->display.hpd_irq_setup)
++			dev_priv->display.hpd_irq_setup(dev);
++		spin_unlock_irq(&dev_priv->irq_lock);
+ 
+ 		intel_dp_mst_resume(dev);
+ 		drm_modeset_lock_all(dev);
+@@ -707,24 +695,14 @@
+ 		 * bother with the tiny race here where we might loose hotplug
+ 		 * notifications.
+ 		 * */
+-		intel_hpd_init(dev);
++		intel_hpd_init(dev_priv);
+ 		/* Config may have changed between suspend and resume */
+ 		drm_helper_hpd_irq_event(dev);
+ 	}
+ 
+ 	intel_opregion_init(dev);
+ 
+-	/*
+-	 * The console lock can be pretty contented on resume due
+-	 * to all the printk activity.  Try to keep it out of the hot
+-	 * path of resume if possible.
+-	 */
+-	if (console_trylock()) {
+-		intel_fbdev_set_suspend(dev, FBINFO_STATE_RUNNING);
+-		console_unlock();
+-	} else {
+-		schedule_work(&dev_priv->console_resume_work);
+-	}
++	intel_fbdev_set_suspend(dev, FBINFO_STATE_RUNNING, false);
+ 
+ 	mutex_lock(&dev_priv->modeset_restore_lock);
+ 	dev_priv->modeset_restore = MODESET_DONE;
+@@ -732,21 +710,15 @@
+ 
+ 	intel_opregion_notify_adapter(dev, PCI_D0);
+ 
+-	return 0;
+-}
+-
+-static int i915_drm_thaw(struct drm_device *dev)
+-{
+-	if (drm_core_check_feature(dev, DRIVER_MODESET))
+-		i915_check_and_clear_faults(dev);
++	drm_kms_helper_poll_enable(dev);
+ 
+-	return __i915_drm_thaw(dev, true);
++	return 0;
+ }
+ 
+-static int i915_resume_early(struct drm_device *dev)
++static int i915_drm_resume_early(struct drm_device *dev)
+ {
+-	if (dev->switch_power_state == DRM_SWITCH_POWER_OFF)
+-		return 0;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	int ret = 0;
+ 
+ 	/*
+ 	 * We have a resume ordering issue with the snd-hda driver also
+@@ -762,33 +734,34 @@
+ 
+ 	pci_set_master(dev->pdev);
+ 
+-	return i915_drm_thaw_early(dev);
++	if (IS_VALLEYVIEW(dev_priv))
++		ret = vlv_resume_prepare(dev_priv, false);
++	if (ret)
++		DRM_ERROR("Resume prepare failed: %d,Continuing resume\n", ret);
++
++	intel_uncore_early_sanitize(dev, true);
++
++	if (IS_HASWELL(dev_priv) || IS_BROADWELL(dev_priv))
++		hsw_disable_pc8(dev_priv);
++
++	intel_uncore_sanitize(dev);
++	intel_power_domains_init_hw(dev_priv);
++
++	return ret;
+ }
+ 
+-int i915_resume(struct drm_device *dev)
++int i915_resume_legacy(struct drm_device *dev)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	int ret;
+ 
+-	/*
+-	 * Platforms with opregion should have sane BIOS, older ones (gen3 and
+-	 * earlier) need to restore the GTT mappings since the BIOS might clear
+-	 * all our scratch PTEs.
+-	 */
+-	ret = __i915_drm_thaw(dev, !dev_priv->opregion.header);
++	if (dev->switch_power_state == DRM_SWITCH_POWER_OFF)
++		return 0;
++
++	ret = i915_drm_resume_early(dev);
+ 	if (ret)
+ 		return ret;
+ 
+-	drm_kms_helper_poll_enable(dev);
+-	return 0;
+-}
+-
+-static int i915_resume_legacy(struct drm_device *dev)
+-{
+-	i915_resume_early(dev);
+-	i915_resume(dev);
+-
+-	return 0;
++	return i915_drm_resume(dev);
+ }
+ 
+ /**
+@@ -809,22 +782,23 @@
+ int i915_reset(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	bool simulated;
+ 	int ret;
+ 
+-	if (!i915.reset)
++	if (!i915_module.reset)
+ 		return 0;
+ 
+ 	mutex_lock(&dev->struct_mutex);
+ 
+-	i915_gem_reset(dev);
+-
+-	simulated = dev_priv->gpu_error.stop_rings != 0;
+-
+ 	ret = intel_gpu_reset(dev);
+ 
++	/* Clear the reset counter. Before anyone else
++	 * can grab the mutex, we will declare whether or
++	 * not the GPU is wedged.
++	 */
++	atomic_inc(&dev_priv->gpu_error.reset_counter);
++
+ 	/* Also reset the gpu hangman. */
+-	if (simulated) {
++	if (dev_priv->gpu_error.stop_rings) {
+ 		DRM_INFO("Simulated gpu hang, resetting stop_rings\n");
+ 		dev_priv->gpu_error.stop_rings = 0;
+ 		if (ret == -ENODEV) {
+@@ -834,6 +808,11 @@
+ 		}
+ 	}
+ 
++	if (i915_stop_ring_allow_warn(dev_priv))
++		pr_notice("drm/i915: Resetting chip after gpu hang\n");
++
++	i915_gem_reset(dev);
++
+ 	if (ret) {
+ 		DRM_ERROR("Failed to reset chip: %i\n", ret);
+ 		mutex_unlock(&dev->struct_mutex);
+@@ -857,7 +836,6 @@
+ 	if (drm_core_check_feature(dev, DRIVER_MODESET) ||
+ 			!dev_priv->ums.mm_suspended) {
+ 		dev_priv->ums.mm_suspended = 0;
+-
+ 		ret = i915_gem_init_hw(dev);
+ 		mutex_unlock(&dev->struct_mutex);
+ 		if (ret) {
+@@ -879,8 +857,6 @@
+ 		 */
+ 		if (INTEL_INFO(dev)->gen > 5)
+ 			intel_reset_gt_powersave(dev);
+-
+-		intel_hpd_init(dev);
+ 	} else {
+ 		mutex_unlock(&dev->struct_mutex);
+ 	}
+@@ -893,7 +869,8 @@
+ 	struct intel_device_info *intel_info =
+ 		(struct intel_device_info *) ent->driver_data;
+ 
+-	if (IS_PRELIMINARY_HW(intel_info) && !i915.preliminary_hw_support) {
++	if (IS_PRELIMINARY_HW(intel_info) &&
++	    !i915_module.preliminary_hw_support) {
+ 		DRM_INFO("This hardware requires preliminary hardware support.\n"
+ 			 "See CONFIG_DRM_I915_PRELIMINARY_HW_SUPPORT, and/or modparam preliminary_hw_support\n");
+ 		return -ENODEV;
+@@ -933,14 +910,13 @@
+ 	if (drm_dev->switch_power_state == DRM_SWITCH_POWER_OFF)
+ 		return 0;
+ 
+-	return i915_drm_freeze(drm_dev);
++	return i915_drm_suspend(drm_dev);
+ }
+ 
+ static int i915_pm_suspend_late(struct device *dev)
+ {
+ 	struct pci_dev *pdev = to_pci_dev(dev);
+ 	struct drm_device *drm_dev = pci_get_drvdata(pdev);
+-	struct drm_i915_private *dev_priv = drm_dev->dev_private;
+ 
+ 	/*
+ 	 * We have a suspedn ordering issue with the snd-hda driver also
+@@ -954,13 +930,7 @@
+ 	if (drm_dev->switch_power_state == DRM_SWITCH_POWER_OFF)
+ 		return 0;
+ 
+-	if (IS_HASWELL(drm_dev) || IS_BROADWELL(drm_dev))
+-		hsw_enable_pc8(dev_priv);
+-
+-	pci_disable_device(pdev);
+-	pci_set_power_state(pdev, PCI_D3hot);
+-
+-	return 0;
++	return i915_drm_suspend_late(drm_dev);
+ }
+ 
+ static int i915_pm_resume_early(struct device *dev)
+@@ -968,77 +938,30 @@
+ 	struct pci_dev *pdev = to_pci_dev(dev);
+ 	struct drm_device *drm_dev = pci_get_drvdata(pdev);
+ 
+-	return i915_resume_early(drm_dev);
+-}
+-
+-static int i915_pm_resume(struct device *dev)
+-{
+-	struct pci_dev *pdev = to_pci_dev(dev);
+-	struct drm_device *drm_dev = pci_get_drvdata(pdev);
+-
+-	return i915_resume(drm_dev);
+-}
+-
+-static int i915_pm_freeze(struct device *dev)
+-{
+-	struct pci_dev *pdev = to_pci_dev(dev);
+-	struct drm_device *drm_dev = pci_get_drvdata(pdev);
+-
+-	if (!drm_dev || !drm_dev->dev_private) {
+-		dev_err(dev, "DRM not initialized, aborting suspend.\n");
+-		return -ENODEV;
+-	}
+-
+-	return i915_drm_freeze(drm_dev);
+-}
+-
+-static int i915_pm_thaw_early(struct device *dev)
+-{
+-	struct pci_dev *pdev = to_pci_dev(dev);
+-	struct drm_device *drm_dev = pci_get_drvdata(pdev);
++	if (drm_dev->switch_power_state == DRM_SWITCH_POWER_OFF)
++		return 0;
+ 
+-	return i915_drm_thaw_early(drm_dev);
++	return i915_drm_resume_early(drm_dev);
+ }
+ 
+-static int i915_pm_thaw(struct device *dev)
++static int i915_pm_resume(struct device *dev)
+ {
+ 	struct pci_dev *pdev = to_pci_dev(dev);
+ 	struct drm_device *drm_dev = pci_get_drvdata(pdev);
+ 
+-	return i915_drm_thaw(drm_dev);
+-}
+-
+-static int i915_pm_poweroff(struct device *dev)
+-{
+-	struct pci_dev *pdev = to_pci_dev(dev);
+-	struct drm_device *drm_dev = pci_get_drvdata(pdev);
++	if (drm_dev->switch_power_state == DRM_SWITCH_POWER_OFF)
++		return 0;
+ 
+-	return i915_drm_freeze(drm_dev);
++	return i915_drm_resume(drm_dev);
+ }
+ 
+-static int hsw_runtime_suspend(struct drm_i915_private *dev_priv)
++static int hsw_suspend_complete(struct drm_i915_private *dev_priv)
+ {
+ 	hsw_enable_pc8(dev_priv);
+ 
+ 	return 0;
+ }
+ 
+-static int snb_runtime_resume(struct drm_i915_private *dev_priv)
+-{
+-	struct drm_device *dev = dev_priv->dev;
+-
+-	intel_init_pch_refclk(dev);
+-
+-	return 0;
+-}
+-
+-static int hsw_runtime_resume(struct drm_i915_private *dev_priv)
+-{
+-	hsw_disable_pc8(dev_priv);
+-
+-	return 0;
+-}
+-
+ /*
+  * Save all Gunit registers that may be lost after a D3 and a subsequent
+  * S0i[R123] transition. The list of registers needing a save/restore is
+@@ -1328,7 +1251,7 @@
+ 	I915_WRITE(VLV_GTLC_PW_STATUS, VLV_GTLC_ALLOWWAKEERR);
+ }
+ 
+-static int vlv_runtime_suspend(struct drm_i915_private *dev_priv)
++static int vlv_suspend_complete(struct drm_i915_private *dev_priv)
+ {
+ 	u32 mask;
+ 	int err;
+@@ -1368,7 +1291,8 @@
+ 	return err;
+ }
+ 
+-static int vlv_runtime_resume(struct drm_i915_private *dev_priv)
++static int vlv_resume_prepare(struct drm_i915_private *dev_priv,
++				bool rpm_resume)
+ {
+ 	struct drm_device *dev = dev_priv->dev;
+ 	int err;
+@@ -1393,8 +1317,10 @@
+ 
+ 	vlv_check_no_gt_access(dev_priv);
+ 
+-	intel_init_clock_gating(dev);
+-	i915_gem_restore_fences(dev);
++	if (rpm_resume) {
++		intel_init_clock_gating(dev);
++		i915_gem_restore_fences(dev);
++	}
+ 
+ 	return ret;
+ }
+@@ -1409,7 +1335,9 @@
+ 	if (WARN_ON_ONCE(!(dev_priv->rps.enabled && intel_enable_rc6(dev))))
+ 		return -ENODEV;
+ 
+-	WARN_ON(!HAS_RUNTIME_PM(dev));
++	if (WARN_ON_ONCE(!HAS_RUNTIME_PM(dev)))
++		return -ENODEV;
++
+ 	assert_force_wake_inactive(dev_priv);
+ 
+ 	DRM_DEBUG_KMS("Suspending device\n");
+@@ -1438,43 +1366,46 @@
+ 	i915_gem_release_all_mmaps(dev_priv);
+ 	mutex_unlock(&dev->struct_mutex);
+ 
+-	/*
+-	 * rps.work can't be rearmed here, since we get here only after making
+-	 * sure the GPU is idle and the RPS freq is set to the minimum. See
+-	 * intel_mark_idle().
+-	 */
+-	cancel_work_sync(&dev_priv->rps.work);
+-	intel_runtime_pm_disable_interrupts(dev);
+-
+-	if (IS_GEN6(dev)) {
+-		ret = 0;
+-	} else if (IS_HASWELL(dev) || IS_BROADWELL(dev)) {
+-		ret = hsw_runtime_suspend(dev_priv);
+-	} else if (IS_VALLEYVIEW(dev)) {
+-		ret = vlv_runtime_suspend(dev_priv);
+-	} else {
+-		ret = -ENODEV;
+-		WARN_ON(1);
+-	}
++	flush_delayed_work(&dev_priv->rps.delayed_resume_work);
++	intel_runtime_pm_disable_interrupts(dev_priv);
++	intel_suspend_gt_powersave(dev);
+ 
++	ret = intel_suspend_complete(dev_priv);
+ 	if (ret) {
+ 		DRM_ERROR("Runtime suspend failed, disabling it (%d)\n", ret);
+-		intel_runtime_pm_restore_interrupts(dev);
++		intel_runtime_pm_enable_interrupts(dev_priv);
+ 
+ 		return ret;
+ 	}
+ 
+-	del_timer_sync(&dev_priv->gpu_error.hangcheck_timer);
++	cancel_delayed_work_sync(&dev_priv->gpu_error.hangcheck_work);
++	intel_uncore_forcewake_reset(dev, false);
+ 	dev_priv->pm.suspended = true;
+ 
+ 	/*
+-	 * current versions of firmware which depend on this opregion
+-	 * notification have repurposed the D1 definition to mean
+-	 * "runtime suspended" vs. what you would normally expect (D3)
+-	 * to distinguish it from notifications that might be sent
+-	 * via the suspend path.
++	 * FIXME: We really should find a document that references the arguments
++	 * used below!
+ 	 */
+-	intel_opregion_notify_adapter(dev, PCI_D1);
++	if (IS_HASWELL(dev)) {
++		/*
++		 * current versions of firmware which depend on this opregion
++		 * notification have repurposed the D1 definition to mean
++		 * "runtime suspended" vs. what you would normally expect (D3)
++		 * to distinguish it from notifications that might be sent via
++		 * the suspend path.
++		 */
++		intel_opregion_notify_adapter(dev, PCI_D1);
++	} else {
++		/*
++		 * On Broadwell, if we use PCI_D1 the PCH DDI ports will stop
++		 * being detected, and the call we do at intel_runtime_resume()
++		 * won't be able to restore them. Since PCI_D3hot matches the
++		 * actual specification and appears to be working, use it. Let's
++		 * assume the other non-Haswell platforms will stay the same as
++		 * Broadwell.
++		 */
++		intel_opregion_notify_adapter(dev, PCI_D3hot);
++	}
+ 
+ 	DRM_DEBUG_KMS("Device suspended\n");
+ 	return 0;
+@@ -1485,25 +1416,22 @@
+ 	struct pci_dev *pdev = to_pci_dev(device);
+ 	struct drm_device *dev = pci_get_drvdata(pdev);
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	int ret;
++	int ret = 0;
+ 
+-	WARN_ON(!HAS_RUNTIME_PM(dev));
++	if (WARN_ON_ONCE(!HAS_RUNTIME_PM(dev)))
++		return -ENODEV;
+ 
+ 	DRM_DEBUG_KMS("Resuming device\n");
+ 
+ 	intel_opregion_notify_adapter(dev, PCI_D0);
+ 	dev_priv->pm.suspended = false;
+ 
+-	if (IS_GEN6(dev)) {
+-		ret = snb_runtime_resume(dev_priv);
+-	} else if (IS_HASWELL(dev) || IS_BROADWELL(dev)) {
+-		ret = hsw_runtime_resume(dev_priv);
+-	} else if (IS_VALLEYVIEW(dev)) {
+-		ret = vlv_runtime_resume(dev_priv);
+-	} else {
+-		WARN_ON(1);
+-		ret = -ENODEV;
+-	}
++	if (IS_GEN6(dev_priv))
++		intel_init_pch_refclk(dev);
++	else if (IS_HASWELL(dev_priv) || IS_BROADWELL(dev_priv))
++		hsw_disable_pc8(dev_priv);
++	else if (IS_VALLEYVIEW(dev_priv))
++		ret = vlv_resume_prepare(dev_priv, true);
+ 
+ 	/*
+ 	 * No point of rolling back things in case of an error, as the best
+@@ -1512,8 +1440,8 @@
+ 	i915_gem_init_swizzling(dev);
+ 	gen6_update_ring_freq(dev);
+ 
+-	intel_runtime_pm_restore_interrupts(dev);
+-	intel_reset_gt_powersave(dev);
++	intel_runtime_pm_enable_interrupts(dev_priv);
++	intel_enable_gt_powersave(dev);
+ 
+ 	if (ret)
+ 		DRM_ERROR("Runtime resume failed, disabling it (%d)\n", ret);
+@@ -1523,17 +1451,60 @@
+ 	return ret;
+ }
+ 
++/*
++ * This function implements common functionality of runtime and system
++ * suspend sequence.
++ */
++static int intel_suspend_complete(struct drm_i915_private *dev_priv)
++{
++	struct drm_device *dev = dev_priv->dev;
++	int ret;
++
++	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
++		ret = hsw_suspend_complete(dev_priv);
++	else if (IS_VALLEYVIEW(dev))
++		ret = vlv_suspend_complete(dev_priv);
++	else
++		ret = 0;
++
++	return ret;
++}
++
+ static const struct dev_pm_ops i915_pm_ops = {
++	/*
++	 * S0ix (via system suspend) and S3 event handlers [PMSG_SUSPEND,
++	 * PMSG_RESUME]
++	 */
+ 	.suspend = i915_pm_suspend,
+ 	.suspend_late = i915_pm_suspend_late,
+ 	.resume_early = i915_pm_resume_early,
+ 	.resume = i915_pm_resume,
+-	.freeze = i915_pm_freeze,
+-	.thaw_early = i915_pm_thaw_early,
+-	.thaw = i915_pm_thaw,
+-	.poweroff = i915_pm_poweroff,
++
++	/*
++	 * S4 event handlers
++	 * @freeze, @freeze_late    : called (1) before creating the
++	 *                            hibernation image [PMSG_FREEZE] and
++	 *                            (2) after rebooting, before restoring
++	 *                            the image [PMSG_QUIESCE]
++	 * @thaw, @thaw_early       : called (1) after creating the hibernation
++	 *                            image, before writing it [PMSG_THAW]
++	 *                            and (2) after failing to create or
++	 *                            restore the image [PMSG_RECOVER]
++	 * @poweroff, @poweroff_late: called after writing the hibernation
++	 *                            image, before rebooting [PMSG_HIBERNATE]
++	 * @restore, @restore_early : called after rebooting and restoring the
++	 *                            hibernation image [PMSG_RESTORE]
++	 */
++	.freeze = i915_pm_suspend,
++	.freeze_late = i915_pm_suspend_late,
++	.thaw_early = i915_pm_resume_early,
++	.thaw = i915_pm_resume,
++	.poweroff = i915_pm_suspend,
++	.poweroff_late = i915_pm_suspend_late,
+ 	.restore_early = i915_pm_resume_early,
+ 	.restore = i915_pm_resume,
++
++	/* S0ix (via runtime suspend) event handlers */
+ 	.runtime_suspend = intel_runtime_suspend,
+ 	.runtime_resume = intel_runtime_resume,
+ };
+@@ -1572,14 +1543,13 @@
+ 	.lastclose = i915_driver_lastclose,
+ 	.preclose = i915_driver_preclose,
+ 	.postclose = i915_driver_postclose,
++	.set_busid = drm_pci_set_busid,
+ 
+ 	/* Used in place of i915_pm_ops for non-DRIVER_MODESET */
+-	.suspend = i915_suspend,
++	.suspend = i915_suspend_legacy,
+ 	.resume = i915_resume_legacy,
+ 
+ 	.device_is_agp = i915_driver_device_is_agp,
+-	.master_create = i915_master_create,
+-	.master_destroy = i915_master_destroy,
+ #if defined(CONFIG_DEBUG_FS)
+ 	.debugfs_init = i915_debugfs_init,
+ 	.debugfs_cleanup = i915_debugfs_cleanup,
+@@ -1627,14 +1597,14 @@
+ 	 * the default behavior.
+ 	 */
+ #if defined(CONFIG_DRM_I915_KMS)
+-	if (i915.modeset != 0)
++	if (i915_module.modeset != 0)
+ 		driver.driver_features |= DRIVER_MODESET;
+ #endif
+-	if (i915.modeset == 1)
++	if (i915_module.modeset == 1)
+ 		driver.driver_features |= DRIVER_MODESET;
+ 
+ #ifdef CONFIG_VGA_CONSOLE
+-	if (vgacon_text_force() && i915.modeset == -1)
++	if (vgacon_text_force() && i915_module.modeset == -1)
+ 		driver.driver_features &= ~DRIVER_MODESET;
+ #endif
+ 
+@@ -1663,6 +1633,8 @@
+ module_init(i915_init);
+ module_exit(i915_exit);
+ 
+-MODULE_AUTHOR(DRIVER_AUTHOR);
++MODULE_AUTHOR("Tungsten Graphics, Inc.");
++MODULE_AUTHOR("Intel Corporation");
++
+ MODULE_DESCRIPTION(DRIVER_DESC);
+ MODULE_LICENSE("GPL and additional rights");
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
+--- a/drivers/gpu/drm/i915/i915_drv.h	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/i915_drv.h	2014-11-21 07:48:24.659922067 -0700
+@@ -35,25 +35,30 @@
+ #include "i915_reg.h"
+ #include "intel_bios.h"
+ #include "intel_ringbuffer.h"
++#include "intel_lrc.h"
+ #include "i915_gem_gtt.h"
+ #include <linux/io-mapping.h>
+ #include <linux/i2c.h>
+ #include <linux/i2c-algo-bit.h>
+ #include <drm/intel-gtt.h>
++#include <drm/drm_legacy.h> /* for struct drm_dma_handle */
++#include <drm/drmP.h>
+ #include <linux/backlight.h>
+ #include <linux/hashtable.h>
+ #include <linux/intel-iommu.h>
+ #include <linux/kref.h>
++#include <linux/perf_event.h>
+ #include <linux/pm_qos.h>
+ 
+ /* General customization:
+  */
+ 
+-#define DRIVER_AUTHOR		"Tungsten Graphics, Inc."
+-
+ #define DRIVER_NAME		"i915"
+ #define DRIVER_DESC		"Intel Graphics"
+-#define DRIVER_DATE		"20140725"
++#define DRIVER_DATE		"20141107"
++
++#undef WARN_ON
++#define WARN_ON(x)		WARN(x, "WARN_ON(" #x ")")
+ 
+ enum pipe {
+ 	INVALID_PIPE = -1,
+@@ -74,6 +79,14 @@
+ };
+ #define transcoder_name(t) ((t) + 'A')
+ 
++/*
++ * This is the maximum (across all platforms) number of planes (primary +
++ * sprites) that can be active at the same time on one pipe.
++ *
++ * This value doesn't count the cursor plane.
++ */
++#define I915_MAX_PLANES	3
++
+ enum plane {
+ 	PLANE_A = 0,
+ 	PLANE_B,
+@@ -162,7 +175,10 @@
+ 	 I915_GEM_DOMAIN_INSTRUCTION | \
+ 	 I915_GEM_DOMAIN_VERTEX)
+ 
+-#define for_each_pipe(p) for ((p) = 0; (p) < INTEL_INFO(dev)->num_pipes; (p)++)
++#define for_each_pipe(__dev_priv, __p) \
++	for ((__p) = 0; (__p) < INTEL_INFO(__dev_priv)->num_pipes; (__p)++)
++#define for_each_plane(pipe, p) \
++	for ((p) = 0; (p) < INTEL_INFO(dev)->num_sprites[(pipe)] + 1; (p)++)
+ #define for_each_sprite(p, s) for ((s) = 0; (s) < INTEL_INFO(dev)->num_sprites[(p)]; (s)++)
+ 
+ #define for_each_crtc(dev, crtc) \
+@@ -171,6 +187,11 @@
+ #define for_each_intel_crtc(dev, intel_crtc) \
+ 	list_for_each_entry(intel_crtc, &dev->mode_config.crtc_list, base.head)
+ 
++#define for_each_intel_encoder(dev, intel_encoder)		\
++	list_for_each_entry(intel_encoder,			\
++			    &(dev)->mode_config.encoder_list,	\
++			    base.head)
++
+ #define for_each_encoder_on_crtc(dev, __crtc, intel_encoder) \
+ 	list_for_each_entry((intel_encoder), &(dev)->mode_config.encoder_list, base.head) \
+ 		if ((intel_encoder)->base.crtc == (__crtc))
+@@ -186,33 +207,59 @@
+ struct drm_i915_private;
+ struct i915_mm_struct;
+ struct i915_mmu_object;
++struct i915_gem_request;
+ 
+ enum intel_dpll_id {
+ 	DPLL_ID_PRIVATE = -1, /* non-shared dpll in use */
+ 	/* real shared dpll ids must be >= 0 */
+ 	DPLL_ID_PCH_PLL_A = 0,
+ 	DPLL_ID_PCH_PLL_B = 1,
++	/* hsw/bdw */
+ 	DPLL_ID_WRPLL1 = 0,
+ 	DPLL_ID_WRPLL2 = 1,
++	/* skl */
++	DPLL_ID_SKL_DPLL1 = 0,
++	DPLL_ID_SKL_DPLL2 = 1,
++	DPLL_ID_SKL_DPLL3 = 2,
+ };
+-#define I915_NUM_PLLS 2
++#define I915_NUM_PLLS 3
+ 
+ struct intel_dpll_hw_state {
++	/* i9xx, pch plls */
+ 	uint32_t dpll;
+ 	uint32_t dpll_md;
+ 	uint32_t fp0;
+ 	uint32_t fp1;
++
++	/* hsw, bdw */
+ 	uint32_t wrpll;
++
++	/* skl */
++	/*
++	 * DPLL_CTRL1 has 6 bits for each each this DPLL. We store those in
++	 * lower part of crtl1 and they get shifted into position when writing
++	 * the register.  This allows us to easily compare the state to share
++	 * the DPLL.
++	 */
++	uint32_t ctrl1;
++	/* HDMI only, 0 when used for DP */
++	uint32_t cfgcr1, cfgcr2;
++};
++
++struct intel_shared_dpll_config {
++	unsigned crtc_mask; /* mask of CRTCs sharing this PLL */
++	struct intel_dpll_hw_state hw_state;
+ };
+ 
+ struct intel_shared_dpll {
+-	int refcount; /* count of number of CRTCs sharing this PLL */
++	struct intel_shared_dpll_config config;
++	struct intel_shared_dpll_config *new_config;
++
+ 	int active; /* count of number of active CRTCs (i.e. DPMS on) */
+ 	bool on; /* is the PLL actually active? Disabled during modeset */
+ 	const char *name;
+ 	/* should match the index in the dev_priv->shared_dplls array */
+ 	enum intel_dpll_id id;
+-	struct intel_dpll_hw_state hw_state;
+ 	/* The mode_set hook is optional and should be used together with the
+ 	 * intel_prepare_shared_dpll function. */
+ 	void (*mode_set)(struct drm_i915_private *dev_priv,
+@@ -226,6 +273,11 @@
+ 			     struct intel_dpll_hw_state *hw_state);
+ };
+ 
++#define SKL_DPLL0 0
++#define SKL_DPLL1 1
++#define SKL_DPLL2 2
++#define SKL_DPLL3 3
++
+ /* Used by dp and fdi links */
+ struct intel_link_m_n {
+ 	uint32_t	tu;
+@@ -254,7 +306,6 @@
+ #define DRIVER_PATCHLEVEL	0
+ 
+ #define WATCH_LISTS	0
+-#define WATCH_GTT	0
+ 
+ struct opregion_header;
+ struct opregion_acpi;
+@@ -277,10 +328,6 @@
+ struct intel_overlay;
+ struct intel_overlay_error_state;
+ 
+-struct drm_i915_master_private {
+-	drm_local_map_t *sarea;
+-	struct _drm_i915_sarea *sarea_priv;
+-};
+ #define I915_FENCE_REG_NONE -1
+ #define I915_MAX_NUM_FENCES 32
+ /* 32 fences + sign bit for FENCE_REG_NONE */
+@@ -316,6 +363,7 @@
+ 	u32 pgtbl_er;
+ 	u32 ier;
+ 	u32 gtier[4];
++	u32 gtimr[4];
+ 	u32 ccid;
+ 	u32 derrmr;
+ 	u32 forcewake;
+@@ -333,23 +381,26 @@
+ 	struct drm_i915_error_object *semaphore_obj;
+ 
+ 	struct drm_i915_error_ring {
++		int id;
+ 		bool valid;
+ 		/* Software tracked state */
+ 		bool waiting;
+ 		int hangcheck_score;
+-		enum intel_ring_hangcheck_action hangcheck_action;
+-		int num_requests;
++		enum intel_engine_hangcheck_action hangcheck_action;
++		int num_batches;
+ 
+ 		/* our own tracking of ring head and tail */
+ 		u32 cpu_ring_head;
+ 		u32 cpu_ring_tail;
+-
+-		u32 semaphore_seqno[I915_NUM_RINGS - 1];
++		u32 interrupts;
++		u32 irq_count;
+ 
+ 		/* Register state */
+ 		u32 tail;
+ 		u32 head;
++		u32 start;
+ 		u32 ctl;
++		u32 mode;
+ 		u32 hws;
+ 		u32 ipeir;
+ 		u32 ipehr;
+@@ -357,25 +408,34 @@
+ 		u32 bbstate;
+ 		u32 instpm;
+ 		u32 instps;
+-		u32 seqno;
++		u32 seqno, request, tag, hangcheck;
++		u32 breadcrumb[I915_NUM_ENGINES];
+ 		u64 bbaddr;
+ 		u64 acthd;
+ 		u32 fault_reg;
+ 		u64 faddr;
+ 		u32 rc_psmi; /* sleep state */
+-		u32 semaphore_mboxes[I915_NUM_RINGS - 1];
++		u32 semaphore_mboxes[I915_NUM_ENGINES];
++		u32 semaphore_sync[I915_NUM_ENGINES];
+ 
+ 		struct drm_i915_error_object {
+ 			int page_count;
++			int unused;
+ 			u32 gtt_offset;
+ 			u32 *pages[0];
+ 		} *ringbuffer, *batchbuffer, *wa_batchbuffer, *ctx, *hws_page;
+ 
+ 		struct drm_i915_error_request {
+ 			long jiffies;
+-			u32 seqno;
++			long pid;
++			u32 batch;
++			u32 head;
+ 			u32 tail;
+-		} *requests;
++			u32 seqno;
++			u32 breadcrumb[I915_NUM_ENGINES];
++			u32 complete;
++			u32 tag;
++		} *batches;
+ 
+ 		struct {
+ 			u32 gfx_mode;
+@@ -387,11 +447,12 @@
+ 
+ 		pid_t pid;
+ 		char comm[TASK_COMM_LEN];
+-	} ring[I915_NUM_RINGS];
++	} ring[I915_NUM_ENGINES];
++
+ 	struct drm_i915_error_buffer {
+ 		u32 size;
+ 		u32 name;
+-		u32 rseqno, wseqno;
++		u32 rseqno[I915_NUM_ENGINES], wseqno, fseqno;
+ 		u32 gtt_offset;
+ 		u32 read_domains;
+ 		u32 write_domain;
+@@ -406,9 +467,11 @@
+ 	} **active_bo, **pinned_bo;
+ 
+ 	u32 *active_bo_count, *pinned_bo_count;
++	u32 vm_count;
+ };
+ 
+ struct intel_connector;
++struct intel_encoder;
+ struct intel_crtc_config;
+ struct intel_plane_config;
+ struct intel_crtc;
+@@ -435,7 +498,7 @@
+ 	 * Returns true on success, false on failure.
+ 	 */
+ 	bool (*find_dpll)(const struct intel_limit *limit,
+-			  struct drm_crtc *crtc,
++			  struct intel_crtc *crtc,
+ 			  int target, int refclk,
+ 			  struct dpll *match_clock,
+ 			  struct dpll *best_clock);
+@@ -451,21 +514,20 @@
+ 				struct intel_crtc_config *);
+ 	void (*get_plane_config)(struct intel_crtc *,
+ 				 struct intel_plane_config *);
+-	int (*crtc_mode_set)(struct drm_crtc *crtc,
+-			     int x, int y,
+-			     struct drm_framebuffer *old_fb);
++	int (*crtc_compute_clock)(struct intel_crtc *crtc);
+ 	void (*crtc_enable)(struct drm_crtc *crtc);
+ 	void (*crtc_disable)(struct drm_crtc *crtc);
+ 	void (*off)(struct drm_crtc *crtc);
+-	void (*write_eld)(struct drm_connector *connector,
+-			  struct drm_crtc *crtc,
+-			  struct drm_display_mode *mode);
++	void (*audio_codec_enable)(struct drm_connector *connector,
++				   struct intel_encoder *encoder,
++				   struct drm_display_mode *mode);
++	void (*audio_codec_disable)(struct intel_encoder *encoder);
+ 	void (*fdi_link_train)(struct drm_crtc *crtc);
+ 	void (*init_clock_gating)(struct drm_device *dev);
+-	int (*queue_flip)(struct drm_device *dev, struct drm_crtc *crtc,
++	int (*queue_flip)(struct i915_gem_request *rq,
++			  struct intel_crtc *crtc,
+ 			  struct drm_framebuffer *fb,
+ 			  struct drm_i915_gem_object *obj,
+-			  struct intel_engine_cs *ring,
+ 			  uint32_t flags);
+ 	void (*update_primary_plane)(struct drm_crtc *crtc,
+ 				     struct drm_framebuffer *fb,
+@@ -477,7 +539,7 @@
+ 	/* display clock increase/decrease */
+ 	/* pll clock increase/decrease */
+ 
+-	int (*setup_backlight)(struct intel_connector *connector);
++	int (*setup_backlight)(struct intel_connector *connector, enum pipe pipe);
+ 	uint32_t (*get_backlight)(struct intel_connector *connector);
+ 	void (*set_backlight)(struct intel_connector *connector,
+ 			      uint32_t level);
+@@ -506,18 +568,32 @@
+ 				uint64_t val, bool trace);
+ };
+ 
++enum {
++	FW_DOMAIN_RENDER = 0,
++	FW_DOMAIN_MEDIA,
++	FW_DOMAIN_BLITTER,
++
++	FW_DOMAIN_COUNT
++};
++
+ struct intel_uncore {
+ 	spinlock_t lock; /** lock is also taken in irq contexts. */
+ 
+ 	struct intel_uncore_funcs funcs;
+ 
+ 	unsigned fifo_count;
+-	unsigned forcewake_count;
+-
+-	unsigned fw_rendercount;
+-	unsigned fw_mediacount;
++	unsigned fw_domains;
+ 
+-	struct timer_list force_wake_timer;
++	struct intel_uncore_forcewake_domain {
++		struct drm_i915_private *i915;
++		int id;
++		unsigned wake_count;
++		struct timer_list timer;
++	} fw_domain[FW_DOMAIN_COUNT];
++#define FORCEWAKE_RENDER	(1 << FW_DOMAIN_RENDER)
++#define FORCEWAKE_MEDIA		(1 << FW_DOMAIN_MEDIA)
++#define FORCEWAKE_BLITTER	(1 << FW_DOMAIN_BLITTER)
++#define FORCEWAKE_ALL		(FORCEWAKE_RENDER | FORCEWAKE_MEDIA | FORCEWAKE_BLITTER)
+ };
+ 
+ #define DEV_INFO_FOR_EACH_FLAG(func, sep) \
+@@ -534,6 +610,7 @@
+ 	func(is_ivybridge) sep \
+ 	func(is_valleyview) sep \
+ 	func(is_haswell) sep \
++	func(is_skylake) sep \
+ 	func(is_preliminary) sep \
+ 	func(has_fbc) sep \
+ 	func(has_pipe_cxsr) sep \
+@@ -551,6 +628,7 @@
+ 
+ struct intel_device_info {
+ 	u32 display_mmio_offset;
++	u16 device_id;
+ 	u8 num_pipes:3;
+ 	u8 num_sprites[I915_MAX_PIPES];
+ 	u8 gen;
+@@ -586,6 +664,11 @@
+ 	/* Time when this context was last blamed for a GPU reset */
+ 	unsigned long guilty_ts;
+ 
++	/* If the contexts causes a second GPU hang within this time,
++	 * it is permanently banned from submitting any more work.
++	 */
++	unsigned long ban_period_seconds;
++
+ 	/* This context is banned to submit more work */
+ 	bool banned;
+ };
+@@ -611,16 +694,18 @@
+  */
+ struct intel_context {
+ 	struct kref ref;
++	struct drm_i915_private *i915;
+ 	int user_handle;
+ 	uint8_t remap_slice;
+ 	struct drm_i915_file_private *file_priv;
+ 	struct i915_ctx_hang_stats hang_stats;
+-	struct i915_address_space *vm;
++	struct i915_hw_ppgtt *ppgtt;
+ 
+-	struct {
+-		struct drm_i915_gem_object *rcs_state;
++	struct intel_engine_context {
++		struct intel_ringbuffer *ring;
++		struct drm_i915_gem_object *state;
+ 		bool initialized;
+-	} legacy_hw_ctx;
++	} ring[I915_NUM_ENGINES];
+ 
+ 	struct list_head link;
+ };
+@@ -635,6 +720,20 @@
+ 	struct drm_mm_node compressed_fb;
+ 	struct drm_mm_node *compressed_llb;
+ 
++	bool false_color;
++
++	/* Tracks whether the HW is actually enabled, not whether the feature is
++	 * possible. */
++	bool enabled;
++
++	/* On gen8 some rings cannont perform fbc clean operation so for now
++	 * we are doing this on SW with mmio.
++	 * This variable works in the opposite information direction
++	 * of ring->fbc_dirty telling software on frontbuffer tracking
++	 * to perform the cache clean on sw side.
++	 */
++	bool need_sw_cache_clean;
++
+ 	struct intel_fbc_work {
+ 		struct delayed_work work;
+ 		struct drm_crtc *crtc;
+@@ -669,6 +768,7 @@
+ 	bool active;
+ 	struct delayed_work work;
+ 	unsigned busy_frontbuffer_bits;
++	struct drm_property *property;
+ };
+ 
+ enum intel_pch {
+@@ -676,6 +776,7 @@
+ 	PCH_IBX,	/* Ibexpeak PCH */
+ 	PCH_CPT,	/* Cougarpoint PCH */
+ 	PCH_LPT,	/* Lynxpoint PCH */
++	PCH_SPT,        /* Sunrisepoint PCH */
+ 	PCH_NOP,
+ };
+ 
+@@ -688,6 +789,7 @@
+ #define QUIRK_LVDS_SSC_DISABLE (1<<1)
+ #define QUIRK_INVERT_BRIGHTNESS (1<<2)
+ #define QUIRK_BACKLIGHT_PRESENT (1<<3)
++#define QUIRK_PIPEB_FORCE (1<<4)
+ 
+ struct intel_fbdev;
+ struct intel_fbc_work;
+@@ -739,7 +841,6 @@
+ 	u32 saveBLC_HIST_CTL;
+ 	u32 saveBLC_PWM_CTL;
+ 	u32 saveBLC_PWM_CTL2;
+-	u32 saveBLC_HIST_CTL_B;
+ 	u32 saveBLC_CPU_PWM_CTL;
+ 	u32 saveBLC_CPU_PWM_CTL2;
+ 	u32 saveFPB0;
+@@ -921,6 +1022,7 @@
+ 	/* work and pm_iir are protected by dev_priv->irq_lock */
+ 	struct work_struct work;
+ 	u32 pm_iir;
++	u32 pm_events;
+ 
+ 	/* Frequencies are stored in potentially platform dependent multiples.
+ 	 * In other words, *_freq needs to be multiplied by X to be interesting.
+@@ -942,13 +1044,15 @@
+ 	u8 rp0_freq;		/* Non-overclocked max frequency. */
+ 	u32 cz_freq;
+ 
+-	u32 ei_interrupt_count;
++	u8 up_threshold; /* Current %busy required to uplock */
++	u8 down_threshold; /* Current %busy required to downclock */
+ 
+ 	int last_adj;
+ 	enum { LOW_POWER, BETWEEN, HIGH_POWER } power;
+ 
+ 	bool enabled;
+ 	struct delayed_work delayed_resume_work;
++	struct list_head clients;
+ 
+ 	/* manual wa residency calculations */
+ 	struct intel_rps_ei up_ei, down_ei;
+@@ -982,7 +1086,6 @@
+ 	int r_t;
+ 
+ 	struct drm_i915_gem_object *pwrctx;
+-	struct drm_i915_gem_object *renderctx;
+ };
+ 
+ struct drm_i915_private;
+@@ -1042,19 +1145,6 @@
+ 	struct i915_power_well *power_wells;
+ };
+ 
+-struct i915_dri1_state {
+-	unsigned allow_batchbuffer : 1;
+-	u32 __iomem *gfx_hws_cpu_addr;
+-
+-	unsigned int cpp;
+-	int back_offset;
+-	int front_offset;
+-	int current_page;
+-	int page_flipping;
+-
+-	uint32_t counter;
+-};
+-
+ struct i915_ums_state {
+ 	/**
+ 	 * Flag if the X Server, and thus DRM, is not currently in
+@@ -1147,6 +1237,7 @@
+ };
+ 
+ struct drm_i915_error_state_buf {
++	struct drm_i915_private *i915;
+ 	unsigned bytes;
+ 	unsigned size;
+ 	int err;
+@@ -1167,7 +1258,7 @@
+ 	/* Hang gpu twice in this window and your context gets banned */
+ #define DRM_I915_CTX_BAN_PERIOD DIV_ROUND_UP(8*DRM_I915_HANGCHECK_PERIOD, 1000)
+ 
+-	struct timer_list hangcheck_timer;
++	struct delayed_work hangcheck_work;
+ 
+ 	/* For reset and error_state handling. */
+ 	spinlock_t lock;
+@@ -1213,7 +1304,7 @@
+ 	/* Userspace knobs for gpu hang simulation;
+ 	 * combines both a ring mask, and extra flags
+ 	 */
+-	u32 stop_rings;
++	unsigned long stop_rings;
+ #define I915_STOP_RING_ALLOW_BAN       (1 << 31)
+ #define I915_STOP_RING_ALLOW_WARN      (1 << 30)
+ 
+@@ -1228,6 +1319,12 @@
+ };
+ 
+ struct ddi_vbt_port_info {
++	/*
++	 * This is an index in the HDMI/DVI DDI buffer translation table.
++	 * The special value HDMI_LEVEL_SHIFT_UNKNOWN means the VBT didn't
++	 * populate this field.
++	 */
++#define HDMI_LEVEL_SHIFT_UNKNOWN	0xff
+ 	uint8_t hdmi_level_shift;
+ 
+ 	uint8_t supports_dvi:1;
+@@ -1318,6 +1415,49 @@
+ 	enum intel_ddb_partitioning partitioning;
+ };
+ 
++struct skl_ddb_entry {
++	uint16_t start, end;	/* in number of blocks, 'end' is exclusive */
++};
++
++static inline uint16_t skl_ddb_entry_size(const struct skl_ddb_entry *entry)
++{
++	return entry->end - entry->start;
++}
++
++static inline bool skl_ddb_entry_equal(const struct skl_ddb_entry *e1,
++				       const struct skl_ddb_entry *e2)
++{
++	if (e1->start == e2->start && e1->end == e2->end)
++		return true;
++
++	return false;
++}
++
++struct skl_ddb_allocation {
++	struct skl_ddb_entry pipe[I915_MAX_PIPES];
++	struct skl_ddb_entry plane[I915_MAX_PIPES][I915_MAX_PLANES];
++	struct skl_ddb_entry cursor[I915_MAX_PIPES];
++};
++
++struct skl_wm_values {
++	bool dirty[I915_MAX_PIPES];
++	struct skl_ddb_allocation ddb;
++	uint32_t wm_linetime[I915_MAX_PIPES];
++	uint32_t plane[I915_MAX_PIPES][I915_MAX_PLANES][8];
++	uint32_t cursor[I915_MAX_PIPES][8];
++	uint32_t plane_trans[I915_MAX_PIPES][I915_MAX_PLANES];
++	uint32_t cursor_trans[I915_MAX_PIPES];
++};
++
++struct skl_wm_level {
++	bool plane_en[I915_MAX_PLANES];
++	bool cursor_en;
++	uint16_t plane_res_b[I915_MAX_PLANES];
++	uint8_t plane_res_l[I915_MAX_PLANES];
++	uint16_t cursor_res_b;
++	uint8_t cursor_res_l;
++};
++
+ /*
+  * This struct helps tracking the state needed for runtime PM, which puts the
+  * device in PCI D3 state. Notice that when this happens, nothing on the
+@@ -1330,7 +1470,7 @@
+  *
+  * Our driver uses the autosuspend delay feature, which means we'll only really
+  * suspend if we stay with zero refcount for a certain amount of time. The
+- * default value is currently very conservative (see intel_init_runtime_pm), but
++ * default value is currently very conservative (see intel_runtime_pm_enable), but
+  * it can be changed with the standard runtime PM files from sysfs.
+  *
+  * The irqs_disabled variable becomes true exactly after we disable the IRQs and
+@@ -1343,7 +1483,7 @@
+  */
+ struct i915_runtime_pm {
+ 	bool suspended;
+-	bool _irqs_disabled;
++	bool irqs_enabled;
+ };
+ 
+ enum intel_pipe_crc_source {
+@@ -1387,6 +1527,28 @@
+ 	unsigned flip_bits;
+ };
+ 
++struct i915_wa_reg {
++	u32 addr;
++	u32 value;
++	/* bitmask representing WA bits */
++	u32 mask;
++};
++
++#define I915_MAX_WA_REGS 16
++
++struct i915_workarounds {
++	struct i915_wa_reg reg[I915_MAX_WA_REGS];
++	u32 count;
++};
++
++enum {
++	__I915_SAMPLE_FREQ_ACT = 0,
++	__I915_SAMPLE_FREQ_REQ,
++	__I915_SAMPLE_INSTDONE_0,
++	__I915_SAMPLE_INSTDONE_63 = __I915_SAMPLE_INSTDONE_0 + 63,
++	__I915_NUM_PMU_SAMPLERS
++};
++
+ struct drm_i915_private {
+ 	struct drm_device *dev;
+ 	struct kmem_cache *slab;
+@@ -1417,11 +1579,11 @@
+ 	wait_queue_head_t gmbus_wait_queue;
+ 
+ 	struct pci_dev *bridge_dev;
+-	struct intel_engine_cs ring[I915_NUM_RINGS];
++	struct intel_engine_cs engine[I915_NUM_ENGINES];
+ 	struct drm_i915_gem_object *semaphore_obj;
+-	uint32_t last_seqno, next_seqno;
++	uint32_t next_seqno;
+ 
+-	drm_dma_handle_t *status_page_dmah;
++	struct drm_dma_handle *status_page_dmah;
+ 	struct resource mch_res;
+ 
+ 	/* protects the irq masks */
+@@ -1438,17 +1600,17 @@
+ 	/* DPIO indirect register protection */
+ 	struct mutex dpio_lock;
+ 
+-	/** Cached value of IMR to avoid reads in updating the bitfield */
++	/** Cached value of IMR/IER to avoid reads in updating the bitfield */
+ 	union {
+ 		u32 irq_mask;
+ 		u32 de_irq_mask[I915_MAX_PIPES];
+ 	};
++	u32 irq_enable;
+ 	u32 gt_irq_mask;
+ 	u32 pm_irq_mask;
+-	u32 pm_rps_events;
+ 	u32 pipestat_irq_mask[I915_MAX_PIPES];
+ 
+-	struct work_struct hotplug_work;
++	struct delayed_work hotplug_work;
+ 	struct {
+ 		unsigned long hpd_last_jiffies;
+ 		int hpd_cnt;
+@@ -1466,21 +1628,27 @@
+ 	struct intel_opregion opregion;
+ 	struct intel_vbt_data vbt;
+ 
++	bool preserve_bios_swizzle;
++
+ 	/* overlay */
+ 	struct intel_overlay *overlay;
+ 
+ 	/* backlight registers and fields in struct intel_panel */
+-	spinlock_t backlight_lock;
++	struct mutex backlight_lock;
+ 
+ 	/* LVDS info */
+ 	bool no_aux_handshake;
+ 
++	/* protects panel power sequencer state */
++	struct mutex pps_mutex;
++
+ 	struct drm_i915_fence_reg fence_regs[I915_MAX_NUM_FENCES]; /* assume 965 */
+ 	int fence_reg_start; /* 4 if userland hasn't ioctl'd us yet */
+ 	int num_fence_regs; /* 8 on pre-965, 16 otherwise */
+ 
+ 	unsigned int fsb_freq, mem_freq, is_ddr3;
+ 	unsigned int vlv_cdclk_freq;
++	unsigned int hpll_freq;
+ 
+ 	/**
+ 	 * wq - Driver workqueue for GEM.
+@@ -1526,6 +1694,8 @@
+ 	struct intel_shared_dpll shared_dplls[I915_NUM_PLLS];
+ 	int dpio_phy_iosf_port[I915_NUM_PHYS_VLV];
+ 
++	struct i915_workarounds workarounds;
++
+ 	/* Reclocking support */
+ 	bool render_reclock_avail;
+ 	bool lvds_downclock_avail;
+@@ -1561,14 +1731,9 @@
+ #ifdef CONFIG_DRM_I915_FBDEV
+ 	/* list of fbdev register on this device */
+ 	struct intel_fbdev *fbdev;
++	struct work_struct fbdev_suspend_work;
+ #endif
+ 
+-	/*
+-	 * The console may be contended at resume, but we don't
+-	 * want it to block on it.
+-	 */
+-	struct work_struct console_resume_work;
+-
+ 	struct drm_property *broadcast_rgb_property;
+ 	struct drm_property *force_audio_property;
+ 
+@@ -1593,9 +1758,25 @@
+ 		uint16_t spr_latency[5];
+ 		/* cursor */
+ 		uint16_t cur_latency[5];
++		/*
++		 * Raw watermark memory latency values
++		 * for SKL for all 8 levels
++		 * in 1us units.
++		 */
++		uint16_t skl_latency[8];
++
++		/*
++		 * The skl_wm_values structure is a bit too big for stack
++		 * allocation, so we keep the staging struct where we store
++		 * intermediate results here instead.
++		 */
++		struct skl_wm_values skl_results;
+ 
+ 		/* current hardware state */
+-		struct ilk_wm_values hw;
++		union {
++			struct ilk_wm_values hw;
++			struct skl_wm_values skl_hw;
++		};
+ 	} wm;
+ 
+ 	struct i915_runtime_pm pm;
+@@ -1614,12 +1795,18 @@
+ 	 */
+ 	struct workqueue_struct *dp_wq;
+ 
+-	/* Old dri1 support infrastructure, beware the dragons ya fools entering
+-	 * here! */
+-	struct i915_dri1_state dri1;
++	uint32_t bios_vgacntr;
++
++	struct {
++		struct pmu base;
++		struct hrtimer timer;
++		u64 enable;
++		u64 instdone;
++		u64 sample[__I915_NUM_PMU_SAMPLERS];
++	} pmu;
++
+ 	/* Old ums support infrastructure, same warning applies. */
+ 	struct i915_ums_state ums;
+-
+ 	/*
+ 	 * NOTE: This is the dri1/ums dungeon, don't add stuff here. Your patch
+ 	 * will be rejected. Instead look for a better place.
+@@ -1632,9 +1819,11 @@
+ }
+ 
+ /* Iterate over initialised rings */
+-#define for_each_ring(ring__, dev_priv__, i__) \
+-	for ((i__) = 0; (i__) < I915_NUM_RINGS; (i__)++) \
+-		if (((ring__) = &(dev_priv__)->ring[(i__)]), intel_ring_initialized((ring__)))
++#define for_each_engine(engine__, dev_priv__, i__) \
++	for ((i__) = 0; (i__) < I915_NUM_ENGINES; (i__)++) \
++		if (((engine__) = &(dev_priv__)->engine[(i__)]), intel_engine_initialized((engine__)))
++
++#define RCS_ENGINE(x) (&__I915__(x)->engine[RCS])
+ 
+ enum hdmi_force_audio {
+ 	HDMI_AUDIO_OFF_DVI = -2,	/* no aux data for HDMI-DVI converter */
+@@ -1699,16 +1888,15 @@
+ 	struct drm_mm_node *stolen;
+ 	struct list_head global_list;
+ 
+-	struct list_head ring_list;
+ 	/** Used in execbuf to temporarily hold a ref */
+ 	struct list_head obj_exec_link;
+ 
+ 	/**
+ 	 * This is set if the object is on the active lists (has pending
+-	 * rendering and so a non-zero seqno), and is not set if it i s on
+-	 * inactive (ready to be unbound) list.
++	 * rendering and so a submitted request), and is not set if it is on
++	 * inactive (ready to be unbound) list. We track activity per engine.
+ 	 */
+-	unsigned int active:1;
++	unsigned int active:I915_NUM_ENGINE_BITS;
+ 
+ 	/**
+ 	 * This is set if the object has been written to since last bound
+@@ -1761,19 +1949,16 @@
+ 	 * Only honoured if hardware has relevant pte bit
+ 	 */
+ 	unsigned long gt_ro:1;
+-
+-	/*
+-	 * Is the GPU currently using a fence to access this buffer,
+-	 */
+-	unsigned int pending_fenced_gpu_access:1;
+-	unsigned int fenced_gpu_access:1;
+-
+ 	unsigned int cache_level:3;
+ 
+-	unsigned int has_aliasing_ppgtt_mapping:1;
+-	unsigned int has_global_gtt_mapping:1;
+ 	unsigned int has_dma_mapping:1;
+ 
++	/**
++	 * This is set if the object is a special page directory used
++	 * for ppGTT.
++	 */
++	unsigned int pde:1;
++
+ 	unsigned int frontbuffer_bits:INTEL_FRONTBUFFER_BITS;
+ 
+ 	struct sg_table *pages;
+@@ -1783,13 +1968,11 @@
+ 	void *dma_buf_vmapping;
+ 	int vmapping_count;
+ 
+-	struct intel_engine_cs *ring;
+-
+-	/** Breadcrumb of last rendering to the buffer. */
+-	uint32_t last_read_seqno;
+-	uint32_t last_write_seqno;
+-	/** Breadcrumb of last fenced GPU access to the buffer. */
+-	uint32_t last_fenced_seqno;
++	/** Breadcrumbs of last rendering to the buffer. */
++	struct {
++		struct i915_gem_request *request;
++		struct list_head engine_link;
++	} last_write, last_read[I915_NUM_ENGINES], last_fence;
+ 
+ 	/** Current tiling stride for the object, if it's tiled. */
+ 	uint32_t stride;
+@@ -1804,10 +1987,10 @@
+ 	unsigned long user_pin_count;
+ 	struct drm_file *pin_filp;
+ 
+-	/** for phy allocated objects */
+-	drm_dma_handle_t *phys_handle;
+-
+ 	union {
++		/** for phy allocated objects */
++		struct drm_dma_handle *phys_handle;
++
+ 		struct i915_gem_userptr {
+ 			uintptr_t ptr;
+ 			unsigned read_only :1;
+@@ -1827,44 +2010,13 @@
+ 		       unsigned frontbuffer_bits);
+ 
+ /**
+- * Request queue structure.
+- *
+- * The request queue allows us to note sequence numbers that have been emitted
+- * and may be associated with active buffers to be retired.
+- *
+- * By keeping this list, we can avoid having to do questionable
+- * sequence-number comparisons on buffer last_rendering_seqnos, and associate
+- * an emission time with seqnos for tracking how far ahead of the GPU we are.
++ * Returns true if seq1 is later than seq2.
+  */
+-struct drm_i915_gem_request {
+-	/** On Which ring this request was generated */
+-	struct intel_engine_cs *ring;
+-
+-	/** GEM sequence number associated with this request. */
+-	uint32_t seqno;
+-
+-	/** Position in the ringbuffer of the start of the request */
+-	u32 head;
+-
+-	/** Position in the ringbuffer of the end of the request */
+-	u32 tail;
+-
+-	/** Context related to this request */
+-	struct intel_context *ctx;
+-
+-	/** Batch buffer related to this request if any */
+-	struct drm_i915_gem_object *batch_obj;
+-
+-	/** Time at which this request was emitted, in jiffies. */
+-	unsigned long emitted_jiffies;
+-
+-	/** global list entry for this request */
+-	struct list_head list;
+-
+-	struct drm_i915_file_private *file_priv;
+-	/** file_priv list entry for this request */
+-	struct list_head client_list;
+-};
++static inline bool
++__i915_seqno_passed(uint32_t seq1, uint32_t seq2)
++{
++	return (int32_t)(seq1 - seq2) >= 0;
++}
+ 
+ struct drm_i915_file_private {
+ 	struct drm_i915_private *dev_priv;
+@@ -1873,12 +2025,13 @@
+ 	struct {
+ 		spinlock_t lock;
+ 		struct list_head request_list;
+-		struct delayed_work idle_work;
+ 	} mm;
+ 	struct idr context_idr;
+ 
+-	atomic_t rps_wait_boost;
+-	struct  intel_engine_cs *bsd_ring;
++	struct list_head rps_boost;
++	struct intel_engine_cs *bsd_engine;
++
++	unsigned rps_boosts;
+ };
+ 
+ /*
+@@ -1971,51 +2124,65 @@
+ 	int count;
+ };
+ 
+-#define INTEL_INFO(dev)	(&to_i915(dev)->info)
++/* Note that the (struct drm_i915_private *) cast is just to shut up gcc. */
++#define __I915__(p) ({ \
++	struct drm_i915_private *__p; \
++	if (__builtin_types_compatible_p(typeof(*p), struct drm_i915_private)) \
++		__p = (struct drm_i915_private *)p; \
++	else if (__builtin_types_compatible_p(typeof(*p), struct drm_device)) \
++		__p = to_i915((struct drm_device *)p); \
++	else \
++		BUILD_BUG(); \
++	__p; \
++})
++#define INTEL_INFO(p) 	(&__I915__(p)->info)
++#define INTEL_DEVID(p)	(INTEL_INFO(p)->device_id)
+ 
+-#define IS_I830(dev)		((dev)->pdev->device == 0x3577)
+-#define IS_845G(dev)		((dev)->pdev->device == 0x2562)
++#define IS_I830(dev)		(INTEL_DEVID(dev) == 0x3577)
++#define IS_845G(dev)		(INTEL_DEVID(dev) == 0x2562)
+ #define IS_I85X(dev)		(INTEL_INFO(dev)->is_i85x)
+-#define IS_I865G(dev)		((dev)->pdev->device == 0x2572)
++#define IS_I865G(dev)		(INTEL_DEVID(dev) == 0x2572)
+ #define IS_I915G(dev)		(INTEL_INFO(dev)->is_i915g)
+-#define IS_I915GM(dev)		((dev)->pdev->device == 0x2592)
+-#define IS_I945G(dev)		((dev)->pdev->device == 0x2772)
++#define IS_I915GM(dev)		(INTEL_DEVID(dev) == 0x2592)
++#define IS_I945G(dev)		(INTEL_DEVID(dev) == 0x2772)
+ #define IS_I945GM(dev)		(INTEL_INFO(dev)->is_i945gm)
+ #define IS_BROADWATER(dev)	(INTEL_INFO(dev)->is_broadwater)
+ #define IS_CRESTLINE(dev)	(INTEL_INFO(dev)->is_crestline)
+-#define IS_GM45(dev)		((dev)->pdev->device == 0x2A42)
++#define IS_GM45(dev)		(INTEL_DEVID(dev) == 0x2A42)
+ #define IS_G4X(dev)		(INTEL_INFO(dev)->is_g4x)
+-#define IS_PINEVIEW_G(dev)	((dev)->pdev->device == 0xa001)
+-#define IS_PINEVIEW_M(dev)	((dev)->pdev->device == 0xa011)
++#define IS_PINEVIEW_G(dev)	(INTEL_DEVID(dev) == 0xa001)
++#define IS_PINEVIEW_M(dev)	(INTEL_DEVID(dev) == 0xa011)
+ #define IS_PINEVIEW(dev)	(INTEL_INFO(dev)->is_pineview)
+ #define IS_G33(dev)		(INTEL_INFO(dev)->is_g33)
+-#define IS_IRONLAKE_M(dev)	((dev)->pdev->device == 0x0046)
++#define IS_IRONLAKE_M(dev)	(INTEL_DEVID(dev) == 0x0046)
+ #define IS_IVYBRIDGE(dev)	(INTEL_INFO(dev)->is_ivybridge)
+-#define IS_IVB_GT1(dev)		((dev)->pdev->device == 0x0156 || \
+-				 (dev)->pdev->device == 0x0152 || \
+-				 (dev)->pdev->device == 0x015a)
+-#define IS_SNB_GT1(dev)		((dev)->pdev->device == 0x0102 || \
+-				 (dev)->pdev->device == 0x0106 || \
+-				 (dev)->pdev->device == 0x010A)
++#define IS_IVB_GT1(dev)		(INTEL_DEVID(dev) == 0x0156 || \
++				 INTEL_DEVID(dev) == 0x0152 || \
++				 INTEL_DEVID(dev) == 0x015a)
++#define IS_SNB_GT1(dev)		(INTEL_DEVID(dev) == 0x0102 || \
++				 INTEL_DEVID(dev) == 0x0106 || \
++				 INTEL_DEVID(dev) == 0x010A)
+ #define IS_VALLEYVIEW(dev)	(INTEL_INFO(dev)->is_valleyview)
+ #define IS_CHERRYVIEW(dev)	(INTEL_INFO(dev)->is_valleyview && IS_GEN8(dev))
+ #define IS_HASWELL(dev)	(INTEL_INFO(dev)->is_haswell)
+ #define IS_BROADWELL(dev)	(!INTEL_INFO(dev)->is_valleyview && IS_GEN8(dev))
++#define IS_SKYLAKE(dev)	(INTEL_INFO(dev)->is_skylake)
+ #define IS_MOBILE(dev)		(INTEL_INFO(dev)->is_mobile)
+ #define IS_HSW_EARLY_SDV(dev)	(IS_HASWELL(dev) && \
+-				 ((dev)->pdev->device & 0xFF00) == 0x0C00)
++				 (INTEL_DEVID(dev) & 0xFF00) == 0x0C00)
+ #define IS_BDW_ULT(dev)		(IS_BROADWELL(dev) && \
+-				 (((dev)->pdev->device & 0xf) == 0x2  || \
+-				 ((dev)->pdev->device & 0xf) == 0x6 || \
+-				 ((dev)->pdev->device & 0xf) == 0xe))
++				 ((INTEL_DEVID(dev) & 0xf) == 0x2  || \
++				 (INTEL_DEVID(dev) & 0xf) == 0x6 || \
++				 (INTEL_DEVID(dev) & 0xf) == 0xe))
++#define IS_BDW_GT3(dev)		(IS_BROADWELL(dev) && \
++				 (INTEL_DEVID(dev) & 0x00F0) == 0x0020)
+ #define IS_HSW_ULT(dev)		(IS_HASWELL(dev) && \
+-				 ((dev)->pdev->device & 0xFF00) == 0x0A00)
+-#define IS_ULT(dev)		(IS_HSW_ULT(dev) || IS_BDW_ULT(dev))
++				 (INTEL_DEVID(dev) & 0xFF00) == 0x0A00)
+ #define IS_HSW_GT3(dev)		(IS_HASWELL(dev) && \
+-				 ((dev)->pdev->device & 0x00F0) == 0x0020)
++				 (INTEL_DEVID(dev) & 0x00F0) == 0x0020)
+ /* ULX machines are also considered ULT. */
+-#define IS_HSW_ULX(dev)		((dev)->pdev->device == 0x0A0E || \
+-				 (dev)->pdev->device == 0x0A1E)
++#define IS_HSW_ULX(dev)		(INTEL_DEVID(dev) == 0x0A0E || \
++				 INTEL_DEVID(dev) == 0x0A1E)
+ #define IS_PRELIMINARY_HW(intel_info) ((intel_info)->is_preliminary)
+ 
+ /*
+@@ -2031,6 +2198,7 @@
+ #define IS_GEN6(dev)	(INTEL_INFO(dev)->gen == 6)
+ #define IS_GEN7(dev)	(INTEL_INFO(dev)->gen == 7)
+ #define IS_GEN8(dev)	(INTEL_INFO(dev)->gen == 8)
++#define IS_GEN9(dev)	(INTEL_INFO(dev)->gen == 9)
+ 
+ #define RENDER_RING		(1<<RCS)
+ #define BSD_RING		(1<<VCS)
+@@ -2043,14 +2211,13 @@
+ #define HAS_VEBOX(dev)		(INTEL_INFO(dev)->ring_mask & VEBOX_RING)
+ #define HAS_LLC(dev)		(INTEL_INFO(dev)->has_llc)
+ #define HAS_WT(dev)		((IS_HASWELL(dev) || IS_BROADWELL(dev)) && \
+-				 to_i915(dev)->ellc_size)
++				 __I915__(dev)->ellc_size)
+ #define I915_NEED_GFX_HWS(dev)	(INTEL_INFO(dev)->need_gfx_hws)
+ 
+-#define HAS_HW_CONTEXTS(dev)	(INTEL_INFO(dev)->gen >= 6)
+-#define HAS_ALIASING_PPGTT(dev)	(INTEL_INFO(dev)->gen >= 6)
+-#define HAS_PPGTT(dev)		(INTEL_INFO(dev)->gen >= 7 && !IS_GEN8(dev))
+-#define USES_PPGTT(dev)		intel_enable_ppgtt(dev, false)
+-#define USES_FULL_PPGTT(dev)	intel_enable_ppgtt(dev, true)
++#define HAS_HW_CONTEXTS(dev)	(INTEL_INFO(dev)->gen >= 5)
++#define HAS_LOGICAL_RING_CONTEXTS(dev)	(INTEL_INFO(dev)->gen >= 8)
++#define USES_PPGTT(dev)		(i915_module.enable_ppgtt)
++#define USES_FULL_PPGTT(dev)	(i915_module.enable_ppgtt == 2)
+ 
+ #define HAS_OVERLAY(dev)		(INTEL_INFO(dev)->has_overlay)
+ #define OVERLAY_NEEDS_PHYSICAL(dev)	(INTEL_INFO(dev)->overlay_needs_physical)
+@@ -2081,13 +2248,15 @@
+ #define HAS_PIPE_CXSR(dev) (INTEL_INFO(dev)->has_pipe_cxsr)
+ #define HAS_FBC(dev) (INTEL_INFO(dev)->has_fbc)
+ 
+-#define HAS_IPS(dev)		(IS_ULT(dev) || IS_BROADWELL(dev))
++#define HAS_IPS(dev)		(IS_HSW_ULT(dev) || IS_BROADWELL(dev))
+ 
+ #define HAS_DDI(dev)		(INTEL_INFO(dev)->has_ddi)
+ #define HAS_FPGA_DBG_UNCLAIMED(dev)	(INTEL_INFO(dev)->has_fpga_dbg)
+ #define HAS_PSR(dev)		(IS_HASWELL(dev) || IS_BROADWELL(dev))
+ #define HAS_RUNTIME_PM(dev)	(IS_GEN6(dev) || IS_HASWELL(dev) || \
+ 				 IS_BROADWELL(dev) || IS_VALLEYVIEW(dev))
++#define HAS_RC6(dev)		(INTEL_INFO(dev)->gen >= 6)
++#define HAS_RC6p(dev)		(INTEL_INFO(dev)->gen == 6 || IS_IVYBRIDGE(dev))
+ 
+ #define INTEL_PCH_DEVICE_ID_MASK		0xff00
+ #define INTEL_PCH_IBX_DEVICE_ID_TYPE		0x3b00
+@@ -2095,8 +2264,11 @@
+ #define INTEL_PCH_PPT_DEVICE_ID_TYPE		0x1e00
+ #define INTEL_PCH_LPT_DEVICE_ID_TYPE		0x8c00
+ #define INTEL_PCH_LPT_LP_DEVICE_ID_TYPE		0x9c00
++#define INTEL_PCH_SPT_DEVICE_ID_TYPE		0xA100
++#define INTEL_PCH_SPT_LP_DEVICE_ID_TYPE		0x9D00
+ 
+-#define INTEL_PCH_TYPE(dev) (to_i915(dev)->pch_type)
++#define INTEL_PCH_TYPE(dev) (__I915__(dev)->pch_type)
++#define HAS_PCH_SPT(dev) (INTEL_PCH_TYPE(dev) == PCH_SPT)
+ #define HAS_PCH_LPT(dev) (INTEL_PCH_TYPE(dev) == PCH_LPT)
+ #define HAS_PCH_CPT(dev) (INTEL_PCH_TYPE(dev) == PCH_CPT)
+ #define HAS_PCH_IBX(dev) (INTEL_PCH_TYPE(dev) == PCH_IBX)
+@@ -2116,13 +2288,13 @@
+ extern const struct drm_ioctl_desc i915_ioctls[];
+ extern int i915_max_ioctl;
+ 
+-extern int i915_suspend(struct drm_device *dev, pm_message_t state);
+-extern int i915_resume(struct drm_device *dev);
++extern int i915_suspend_legacy(struct drm_device *dev, pm_message_t state);
++extern int i915_resume_legacy(struct drm_device *dev);
+ extern int i915_master_create(struct drm_device *dev, struct drm_master *master);
+ extern void i915_master_destroy(struct drm_device *dev, struct drm_master *master);
+ 
+ /* i915_params.c */
+-struct i915_params {
++extern struct i915_module_parameters {
+ 	int modeset;
+ 	int panel_ignore_lid;
+ 	unsigned int powersave;
+@@ -2134,6 +2306,7 @@
+ 	int enable_rc6;
+ 	int enable_fbc;
+ 	int enable_ppgtt;
++	int enable_execlists;
+ 	int enable_psr;
+ 	unsigned int preliminary_hw_support;
+ 	int disable_power_well;
+@@ -2149,12 +2322,9 @@
+ 	bool disable_vtd_wa;
+ 	int use_mmio_flip;
+ 	bool mmio_debug;
+-};
+-extern struct i915_params i915 __read_mostly;
++} i915_module __read_mostly;
+ 
+-				/* i915_dma.c */
+-void i915_update_dri1_breadcrumb(struct drm_device *dev);
+-extern void i915_kernel_lost_context(struct drm_device * dev);
++/* i915_dma.c */
+ extern int i915_driver_load(struct drm_device *, unsigned long flags);
+ extern int i915_driver_unload(struct drm_device *);
+ extern int i915_driver_open(struct drm_device *dev, struct drm_file *file);
+@@ -2168,9 +2338,6 @@
+ extern long i915_compat_ioctl(struct file *filp, unsigned int cmd,
+ 			      unsigned long arg);
+ #endif
+-extern int i915_emit_box(struct drm_device *dev,
+-			 struct drm_clip_rect *box,
+-			 int DR1, int DR4);
+ extern int intel_gpu_reset(struct drm_device *dev);
+ extern int i915_reset(struct drm_device *dev);
+ extern unsigned long i915_chipset_val(struct drm_i915_private *dev_priv);
+@@ -2180,18 +2347,19 @@
+ int vlv_force_gfx_clock(struct drm_i915_private *dev_priv, bool on);
+ void intel_hpd_cancel_work(struct drm_i915_private *dev_priv);
+ 
+-extern void intel_console_resume(struct work_struct *work);
+-
+ /* i915_irq.c */
+ void i915_queue_hangcheck(struct drm_device *dev);
+ __printf(3, 4)
+-void i915_handle_error(struct drm_device *dev, bool wedged,
++void i915_handle_error(struct drm_device *dev,
++		       unsigned flags,
+ 		       const char *fmt, ...);
++#define I915_HANG_RESET 0x1
++#define I915_HANG_SIMULATED 0x2
+ 
+-void gen6_set_pm_mask(struct drm_i915_private *dev_priv, u32 pm_iir,
+-							int new_delay);
+-extern void intel_irq_init(struct drm_device *dev);
+-extern void intel_hpd_init(struct drm_device *dev);
++extern void intel_irq_init(struct drm_i915_private *dev_priv);
++extern void intel_hpd_init(struct drm_i915_private *dev_priv);
++int intel_irq_install(struct drm_i915_private *dev_priv);
++void intel_irq_uninstall(struct drm_i915_private *dev_priv);
+ 
+ extern void intel_uncore_sanitize(struct drm_device *dev);
+ extern void intel_uncore_early_sanitize(struct drm_device *dev,
+@@ -2211,6 +2379,17 @@
+ 
+ void valleyview_enable_display_irqs(struct drm_i915_private *dev_priv);
+ void valleyview_disable_display_irqs(struct drm_i915_private *dev_priv);
++void
++ironlake_enable_display_irq(struct drm_i915_private *dev_priv, u32 mask);
++void
++ironlake_disable_display_irq(struct drm_i915_private *dev_priv, u32 mask);
++void ibx_display_interrupt_update(struct drm_i915_private *dev_priv,
++				  uint32_t interrupt_mask,
++				  uint32_t enabled_irq_mask);
++#define ibx_enable_display_interrupt(dev_priv, bits) \
++	ibx_display_interrupt_update((dev_priv), (bits), (bits))
++#define ibx_disable_display_interrupt(dev_priv, bits) \
++	ibx_display_interrupt_update((dev_priv), (bits), 0)
+ 
+ /* i915_gem.c */
+ int i915_gem_init_ioctl(struct drm_device *dev, void *data,
+@@ -2255,7 +2434,7 @@
+ 			struct drm_file *file_priv);
+ int i915_gem_get_tiling(struct drm_device *dev, void *data,
+ 			struct drm_file *file_priv);
+-int i915_gem_init_userptr(struct drm_device *dev);
++void i915_gem_init_userptr(struct drm_device *dev);
+ int i915_gem_userptr_ioctl(struct drm_device *dev, void *data,
+ 			   struct drm_file *file);
+ int i915_gem_get_aperture_ioctl(struct drm_device *dev, void *data,
+@@ -2263,6 +2442,12 @@
+ int i915_gem_wait_ioctl(struct drm_device *dev, void *data,
+ 			struct drm_file *file_priv);
+ void i915_gem_load(struct drm_device *dev);
++unsigned long i915_gem_shrink(struct drm_i915_private *dev_priv,
++			      long target,
++			      unsigned flags);
++#define I915_SHRINK_PURGEABLE 0x1
++#define I915_SHRINK_UNBOUND 0x2
++#define I915_SHRINK_BOUND 0x4
+ void *i915_gem_object_alloc(struct drm_device *dev);
+ void i915_gem_object_free(struct drm_i915_gem_object *obj);
+ void i915_gem_object_init(struct drm_i915_gem_object *obj,
+@@ -2272,17 +2457,18 @@
+ void i915_init_vm(struct drm_i915_private *dev_priv,
+ 		  struct i915_address_space *vm);
+ void i915_gem_free_object(struct drm_gem_object *obj);
+-void i915_gem_vma_destroy(struct i915_vma *vma);
++void i915_vma_unreserve(struct i915_vma *vma);
+ 
+-#define PIN_MAPPABLE 0x1
+-#define PIN_NONBLOCK 0x2
+-#define PIN_GLOBAL 0x4
+-#define PIN_OFFSET_BIAS 0x8
++#define PIN_OFFSET_FIXED 0x1
++#define PIN_OFFSET_BIAS 0x2
++#define PIN_LOCAL 0x4
++#define PIN_GLOBAL 0x8
++#define PIN_MAPPABLE 0x10
++#define PIN_NONBLOCK 0x20
+ #define PIN_OFFSET_MASK (~4095)
+-int __must_check i915_gem_object_pin(struct drm_i915_gem_object *obj,
+-				     struct i915_address_space *vm,
+-				     uint32_t alignment,
+-				     uint64_t flags);
++int __must_check i915_vma_pin(struct i915_vma *vma,
++			      uint32_t alignment,
++			      uint64_t flags);
+ int __must_check i915_vma_unbind(struct i915_vma *vma);
+ int i915_gem_object_put_pages(struct drm_i915_gem_object *obj);
+ void i915_gem_release_all_mmaps(struct drm_i915_private *dev_priv);
+@@ -2315,22 +2501,12 @@
+ 
+ int __must_check i915_mutex_lock_interruptible(struct drm_device *dev);
+ int i915_gem_object_sync(struct drm_i915_gem_object *obj,
+-			 struct intel_engine_cs *to);
+-void i915_vma_move_to_active(struct i915_vma *vma,
+-			     struct intel_engine_cs *ring);
++			 struct i915_gem_request *rq);
+ int i915_gem_dumb_create(struct drm_file *file_priv,
+ 			 struct drm_device *dev,
+ 			 struct drm_mode_create_dumb *args);
+ int i915_gem_mmap_gtt(struct drm_file *file_priv, struct drm_device *dev,
+ 		      uint32_t handle, uint64_t *offset);
+-/**
+- * Returns true if seq1 is later than seq2.
+- */
+-static inline bool
+-i915_seqno_passed(uint32_t seq1, uint32_t seq2)
+-{
+-	return (int32_t)(seq1 - seq2) >= 0;
+-}
+ 
+ int __must_check i915_gem_get_seqno(struct drm_device *dev, u32 *seqno);
+ int __must_check i915_gem_set_seqno(struct drm_device *dev, u32 seqno);
+@@ -2340,24 +2516,33 @@
+ bool i915_gem_object_pin_fence(struct drm_i915_gem_object *obj);
+ void i915_gem_object_unpin_fence(struct drm_i915_gem_object *obj);
+ 
+-struct drm_i915_gem_request *
+-i915_gem_find_active_request(struct intel_engine_cs *ring);
+-
+ bool i915_gem_retire_requests(struct drm_device *dev);
+-void i915_gem_retire_requests_ring(struct intel_engine_cs *ring);
+-int __must_check i915_gem_check_wedge(struct i915_gpu_error *error,
+-				      bool interruptible);
+-int __must_check i915_gem_check_olr(struct intel_engine_cs *ring, u32 seqno);
++void i915_gem_retire_requests__engine(struct intel_engine_cs *engine);
++
++static inline bool __i915_reset_in_progress(unsigned x)
++{
++	return unlikely(x & I915_RESET_IN_PROGRESS_FLAG);
++}
+ 
+ static inline bool i915_reset_in_progress(struct i915_gpu_error *error)
+ {
+-	return unlikely(atomic_read(&error->reset_counter)
+-			& (I915_RESET_IN_PROGRESS_FLAG | I915_WEDGED));
++	return __i915_reset_in_progress(atomic_read(&error->reset_counter));
++}
++
++static inline bool __i915_terminally_wedged(unsigned x)
++{
++	return unlikely(x & I915_WEDGED);
+ }
+ 
+ static inline bool i915_terminally_wedged(struct i915_gpu_error *error)
+ {
+-	return atomic_read(&error->reset_counter) & I915_WEDGED;
++	return __i915_terminally_wedged(atomic_read(&error->reset_counter));
++}
++
++static inline bool i915_recovery_pending(struct i915_gpu_error *error)
++{
++	unsigned x = atomic_read(&error->reset_counter);
++	return __i915_reset_in_progress(x) && !__i915_terminally_wedged(x);
+ }
+ 
+ static inline u32 i915_reset_count(struct i915_gpu_error *error)
+@@ -2382,19 +2567,10 @@
+ int __must_check i915_gem_object_finish_gpu(struct drm_i915_gem_object *obj);
+ int __must_check i915_gem_init(struct drm_device *dev);
+ int __must_check i915_gem_init_hw(struct drm_device *dev);
+-int i915_gem_l3_remap(struct intel_engine_cs *ring, int slice);
++void i915_gem_fini(struct drm_device *dev);
+ void i915_gem_init_swizzling(struct drm_device *dev);
+-void i915_gem_cleanup_ringbuffer(struct drm_device *dev);
+ int __must_check i915_gpu_idle(struct drm_device *dev);
+ int __must_check i915_gem_suspend(struct drm_device *dev);
+-int __i915_add_request(struct intel_engine_cs *ring,
+-		       struct drm_file *file,
+-		       struct drm_i915_gem_object *batch_obj,
+-		       u32 *seqno);
+-#define i915_add_request(ring, seqno) \
+-	__i915_add_request(ring, NULL, NULL, seqno)
+-int __must_check i915_wait_seqno(struct intel_engine_cs *ring,
+-				 uint32_t seqno);
+ int i915_gem_fault(struct vm_area_struct *vma, struct vm_fault *vmf);
+ int __must_check
+ i915_gem_object_set_to_gtt_domain(struct drm_i915_gem_object *obj,
+@@ -2404,7 +2580,7 @@
+ int __must_check
+ i915_gem_object_pin_to_display_plane(struct drm_i915_gem_object *obj,
+ 				     u32 alignment,
+-				     struct intel_engine_cs *pipelined);
++				     struct i915_gem_request *pipelined);
+ void i915_gem_object_unpin_from_display_plane(struct drm_i915_gem_object *obj);
+ int i915_gem_object_attach_phys(struct drm_i915_gem_object *obj,
+ 				int align);
+@@ -2438,74 +2614,77 @@
+ struct i915_vma *i915_gem_obj_to_vma(struct drm_i915_gem_object *obj,
+ 				     struct i915_address_space *vm);
+ struct i915_vma *
+-i915_gem_obj_lookup_or_create_vma(struct drm_i915_gem_object *obj,
+-				  struct i915_address_space *vm);
++i915_gem_obj_get_vma(struct drm_i915_gem_object *obj,
++		     struct i915_address_space *vm);
+ 
+-struct i915_vma *i915_gem_obj_to_ggtt(struct drm_i915_gem_object *obj);
+ static inline bool i915_gem_obj_is_pinned(struct drm_i915_gem_object *obj) {
+ 	struct i915_vma *vma;
+-	list_for_each_entry(vma, &obj->vma_list, vma_link)
++	list_for_each_entry(vma, &obj->vma_list, obj_link)
+ 		if (vma->pin_count > 0)
+ 			return true;
+ 	return false;
+ }
+ 
+ /* Some GGTT VM helpers */
+-#define obj_to_ggtt(obj) \
+-	(&((struct drm_i915_private *)(obj)->base.dev->dev_private)->gtt.base)
++#define i915_obj_to_ggtt(obj) (&to_i915((obj)->base.dev)->gtt.base)
+ static inline bool i915_is_ggtt(struct i915_address_space *vm)
+ {
+-	struct i915_address_space *ggtt =
+-		&((struct drm_i915_private *)(vm)->dev->dev_private)->gtt.base;
+-	return vm == ggtt;
++	return vm == &to_i915(vm->dev)->gtt.base;
+ }
+ 
+-static inline bool i915_gem_obj_ggtt_bound(struct drm_i915_gem_object *obj)
++struct i915_vma *i915_gem_obj_to_ggtt(struct drm_i915_gem_object *obj);
++static inline struct i915_vma *
++i915_gem_obj_get_ggtt(struct drm_i915_gem_object *obj)
+ {
+-	return i915_gem_obj_bound(obj, obj_to_ggtt(obj));
++	return i915_gem_obj_get_vma(obj, i915_obj_to_ggtt(obj));
+ }
+ 
+-static inline unsigned long
+-i915_gem_obj_ggtt_offset(struct drm_i915_gem_object *obj)
++
++static inline struct i915_hw_ppgtt *
++i915_vm_to_ppgtt(struct i915_address_space *vm)
+ {
+-	return i915_gem_obj_offset(obj, obj_to_ggtt(obj));
++	WARN_ON(i915_is_ggtt(vm));
++
++	return container_of(vm, struct i915_hw_ppgtt, base);
+ }
+ 
+-static inline unsigned long
+-i915_gem_obj_ggtt_size(struct drm_i915_gem_object *obj)
++
++static inline bool i915_gem_obj_ggtt_bound(struct drm_i915_gem_object *obj)
+ {
+-	return i915_gem_obj_size(obj, obj_to_ggtt(obj));
++	return i915_gem_obj_bound(obj, i915_obj_to_ggtt(obj));
+ }
+ 
+-static inline int __must_check
+-i915_gem_obj_ggtt_pin(struct drm_i915_gem_object *obj,
+-		      uint32_t alignment,
+-		      unsigned flags)
++static inline unsigned long
++i915_gem_obj_ggtt_offset(struct drm_i915_gem_object *obj)
+ {
+-	return i915_gem_object_pin(obj, obj_to_ggtt(obj), alignment, flags | PIN_GLOBAL);
++	return i915_gem_obj_offset(obj, i915_obj_to_ggtt(obj));
+ }
+ 
+-static inline int
+-i915_gem_object_ggtt_unbind(struct drm_i915_gem_object *obj)
++static inline unsigned long
++i915_gem_obj_ggtt_size(struct drm_i915_gem_object *obj)
+ {
+-	return i915_vma_unbind(i915_gem_obj_to_ggtt(obj));
++	return i915_gem_obj_size(obj, i915_obj_to_ggtt(obj));
+ }
+ 
++int __must_check i915_gem_object_ggtt_pin(struct drm_i915_gem_object *obj,
++					  uint32_t alignment,
++					  unsigned flags);
+ void i915_gem_object_ggtt_unpin(struct drm_i915_gem_object *obj);
++void i915_gem_object_finish_gtt(struct drm_i915_gem_object *obj);
+ 
+ /* i915_gem_context.c */
+-#define ctx_to_ppgtt(ctx) container_of((ctx)->vm, struct i915_hw_ppgtt, base)
+ int __must_check i915_gem_context_init(struct drm_device *dev);
+ void i915_gem_context_fini(struct drm_device *dev);
+-void i915_gem_context_reset(struct drm_device *dev);
+ int i915_gem_context_open(struct drm_device *dev, struct drm_file *file);
+ int i915_gem_context_enable(struct drm_i915_private *dev_priv);
+ void i915_gem_context_close(struct drm_device *dev, struct drm_file *file);
+-int i915_switch_context(struct intel_engine_cs *ring,
+-			struct intel_context *to);
++int i915_request_switch_context(struct i915_gem_request *rq);
++void i915_request_switch_context__commit(struct i915_gem_request *rq);
+ struct intel_context *
+ i915_gem_context_get(struct drm_i915_file_private *file_priv, u32 id);
+-void i915_gem_context_free(struct kref *ctx_ref);
++void __i915_gem_context_free(struct kref *ctx_ref);
++struct drm_i915_gem_object *
++i915_gem_alloc_context_obj(struct drm_device *dev, size_t size);
+ static inline void i915_gem_context_reference(struct intel_context *ctx)
+ {
+ 	kref_get(&ctx->ref);
+@@ -2513,7 +2692,7 @@
+ 
+ static inline void i915_gem_context_unreference(struct intel_context *ctx)
+ {
+-	kref_put(&ctx->ref, i915_gem_context_free);
++	kref_put(&ctx->ref, __i915_gem_context_free);
+ }
+ 
+ static inline bool i915_gem_context_is_default(const struct intel_context *c)
+@@ -2525,9 +2704,15 @@
+ 				  struct drm_file *file);
+ int i915_gem_context_destroy_ioctl(struct drm_device *dev, void *data,
+ 				   struct drm_file *file);
++int i915_gem_context_getparam_ioctl(struct drm_device *dev, void *data,
++				    struct drm_file *file_priv);
++int i915_gem_context_setparam_ioctl(struct drm_device *dev, void *data,
++				    struct drm_file *file_priv);
++int i915_gem_context_dump_ioctl(struct drm_device *dev, void *data,
++				struct drm_file *file);
+ 
+ /* i915_gem_render_state.c */
+-int i915_gem_render_state_init(struct intel_engine_cs *ring);
++int i915_gem_render_state_init(struct i915_gem_request *rq);
+ /* i915_gem_evict.c */
+ int __must_check i915_gem_evict_something(struct drm_device *dev,
+ 					  struct i915_address_space *vm,
+@@ -2537,6 +2722,9 @@
+ 					  unsigned long start,
+ 					  unsigned long end,
+ 					  unsigned flags);
++int __must_check
++i915_gem_evict_range(struct drm_device *dev, struct i915_address_space *vm,
++		     unsigned long start, unsigned long end);
+ int i915_gem_evict_vm(struct i915_address_space *vm, bool do_idle);
+ int i915_gem_evict_everything(struct drm_device *dev);
+ 
+@@ -2547,6 +2735,164 @@
+ 		intel_gtt_chipset_flush();
+ }
+ 
++/* i915_gem_request.c */
++
++/**
++ * Request queue structure.
++ *
++ * The request queue allows us to note sequence numbers that have been emitted
++ * and may be associated with active buffers to be retired.
++ *
++ * By keeping this list, we can avoid having to do questionable
++ * sequence-number comparisons on buffer last_rendering_seqnos, and associate
++ * an emission time with seqnos for tracking how far ahead of the GPU we are.
++ */
++struct i915_gem_request {
++	struct kref kref;
++
++	/** On which ring/engine/ctx this request was generated */
++	struct drm_i915_private *i915;
++	struct intel_context *ctx;
++	struct intel_engine_cs *engine;
++	struct intel_ringbuffer *ring;
++
++	/** How many GPU resets ago was this request first constructed? */
++	unsigned reset_counter;
++
++	/** GEM sequence number/breadcrumb associated with this request. */
++	u32 seqno;
++	u32 breadcrumb[I915_NUM_ENGINES];
++	u32 semaphore[I915_NUM_ENGINES];
++
++	/** Position in the ringbuffer of the request */
++	u32 head, tail;
++
++	/** Batch buffer and objects related to this request if any */
++	struct i915_vma *batch;
++	struct list_head vmas;
++
++	/** Time at which this request was emitted, in jiffies. */
++	unsigned long emitted_jiffies;
++
++	/** global list entry for this request */
++	struct list_head engine_link;
++	struct list_head breadcrumb_link;
++
++	struct drm_i915_file_private *file_priv;
++	/** file_priv list entry for this request */
++	struct list_head client_list;
++
++	u16 tag;
++	unsigned remap_l3:8;
++	unsigned pending_flush:4;
++	bool outstanding:1;
++	bool has_ctx_switch:1;
++
++	bool completed; /* kept separate for atomicity */
++};
++
++struct i915_gem_request * __must_check __attribute__((nonnull))
++i915_request_create(struct intel_context *ctx,
++		    struct intel_engine_cs *engine);
++
++static inline struct intel_engine_cs *i915_request_engine(struct i915_gem_request *rq)
++{
++	return rq ? rq->engine : NULL;
++}
++
++static inline int i915_request_engine_id(struct i915_gem_request *rq)
++{
++	return rq ? rq->engine->id : -1;
++}
++
++static inline u32 i915_request_seqno(struct i915_gem_request *rq)
++{
++	return rq ? rq->seqno : 0;
++}
++
++bool __i915_request_complete__wa(struct i915_gem_request *rq);
++
++static inline bool
++i915_request_complete(struct i915_gem_request *rq)
++{
++	if (!rq->completed && rq->engine->is_complete(rq)) {
++		trace_i915_gem_request_complete(rq);
++		rq->completed = true;
++	}
++	return rq->completed;
++}
++
++static inline struct i915_gem_request *
++i915_request_get(struct i915_gem_request *rq)
++{
++	if (rq)
++		kref_get(&rq->kref);
++	return rq;
++}
++
++void __i915_request_free(struct kref *kref);
++
++static inline void
++i915_request_put(struct i915_gem_request *rq)
++{
++	if (rq == NULL)
++		return;
++
++	lockdep_assert_held(&rq->i915->dev->struct_mutex);
++	kref_put(&rq->kref, __i915_request_free);
++}
++
++static inline void
++i915_request_put__unlocked(struct i915_gem_request *rq)
++{
++	if (!atomic_add_unless(&rq->kref.refcount, -1, 1)) {
++		struct drm_device *dev = rq->i915->dev;
++
++		mutex_lock(&dev->struct_mutex);
++		if (likely(atomic_dec_and_test(&rq->kref.refcount)))
++			__i915_request_free(&rq->kref);
++		mutex_unlock(&dev->struct_mutex);
++	}
++}
++
++void
++i915_request_add_vma(struct i915_gem_request *rq,
++		     struct i915_vma *vma,
++		     unsigned fenced);
++#define VMA_IS_FENCED 0x1
++#define VMA_HAS_FENCE 0x2
++int __must_check
++i915_request_emit_flush(struct i915_gem_request *rq,
++			unsigned flags);
++int __must_check
++__i915_request_emit_breadcrumb(struct i915_gem_request *rq, int id);
++static inline int __must_check
++i915_request_emit_breadcrumb(struct i915_gem_request *rq)
++{
++	return __i915_request_emit_breadcrumb(rq, rq->engine->id);
++}
++static inline int __must_check
++i915_request_emit_semaphore(struct i915_gem_request *rq, int id)
++{
++	return __i915_request_emit_breadcrumb(rq, id);
++}
++int __must_check
++i915_request_emit_batchbuffer(struct i915_gem_request *rq,
++			      struct i915_vma *batch,
++			      uint64_t start, uint32_t len,
++			      unsigned flags);
++int __must_check
++i915_request_commit(struct i915_gem_request *rq);
++struct i915_gem_request *
++i915_request_get_breadcrumb(struct i915_gem_request *rq);
++int __must_check
++i915_request_wait(struct i915_gem_request *rq);
++int __i915_request_wait(struct i915_gem_request *rq,
++			bool interruptible,
++			s64 *timeout,
++			struct drm_i915_file_private *file);
++void i915_request_retire(struct i915_gem_request *rq);
++
+ /* i915_gem_stolen.c */
+ int i915_gem_init_stolen(struct drm_device *dev);
+ int i915_gem_stolen_setup_compression(struct drm_device *dev, int size, int fb_cpp);
+@@ -2573,13 +2919,6 @@
+ void i915_gem_object_do_bit_17_swizzle(struct drm_i915_gem_object *obj);
+ void i915_gem_object_save_bit_17_swizzle(struct drm_i915_gem_object *obj);
+ 
+-/* i915_gem_debug.c */
+-#if WATCH_LISTS
+-int i915_verify_lists(struct drm_device *dev);
+-#else
+-#define i915_verify_lists(dev) 0
+-#endif
+-
+ /* i915_debugfs.c */
+ int i915_debugfs_init(struct drm_minor *minor);
+ void i915_debugfs_cleanup(struct drm_minor *minor);
+@@ -2595,6 +2934,7 @@
+ int i915_error_state_to_str(struct drm_i915_error_state_buf *estr,
+ 			    const struct i915_error_state_file_priv *error);
+ int i915_error_state_buf_init(struct drm_i915_error_state_buf *eb,
++			      struct drm_i915_private *i915,
+ 			      size_t count, loff_t pos);
+ static inline void i915_error_state_buf_release(
+ 	struct drm_i915_error_state_buf *eb)
+@@ -2609,18 +2949,27 @@
+ void i915_destroy_error_state(struct drm_device *dev);
+ 
+ void i915_get_extra_instdone(struct drm_device *dev, uint32_t *instdone);
+-const char *i915_cache_level_str(int type);
++const char *i915_cache_level_str(struct drm_i915_private *i915, int type);
+ 
+ /* i915_cmd_parser.c */
+ int i915_cmd_parser_get_version(void);
+-int i915_cmd_parser_init_ring(struct intel_engine_cs *ring);
+-void i915_cmd_parser_fini_ring(struct intel_engine_cs *ring);
+-bool i915_needs_cmd_parser(struct intel_engine_cs *ring);
+-int i915_parse_cmds(struct intel_engine_cs *ring,
++int i915_cmd_parser_init_engine(struct intel_engine_cs *engine);
++void i915_cmd_parser_fini_engine(struct intel_engine_cs *engine);
++bool i915_needs_cmd_parser(struct intel_engine_cs *engine);
++int i915_parse_cmds(struct intel_engine_cs *engine,
+ 		    struct drm_i915_gem_object *batch_obj,
+ 		    u32 batch_start_offset,
+ 		    bool is_master);
+ 
++/* i915_perf.c */
++#ifdef CONFIG_PERF_EVENTS
++extern void i915_perf_register(struct drm_device *dev);
++extern void i915_perf_unregister(struct drm_device *dev);
++#else
++static inline void i915_perf_register(struct drm_device *dev) {}
++static inline void i915_perf_unregister(struct drm_device *dev) {}
++#endif
++
+ /* i915_suspend.c */
+ extern int i915_save_state(struct drm_device *dev);
+ extern int i915_restore_state(struct drm_device *dev);
+@@ -2652,7 +3001,6 @@
+ extern void intel_i2c_reset(struct drm_device *dev);
+ 
+ /* intel_opregion.c */
+-struct intel_encoder;
+ #ifdef CONFIG_ACPI
+ extern int intel_opregion_setup(struct drm_device *dev);
+ extern void intel_opregion_init(struct drm_device *dev);
+@@ -2690,7 +3038,6 @@
+ 
+ /* modesetting */
+ extern void intel_modeset_init_hw(struct drm_device *dev);
+-extern void intel_modeset_suspend_hw(struct drm_device *dev);
+ extern void intel_modeset_init(struct drm_device *dev);
+ extern void intel_modeset_gem_init(struct drm_device *dev);
+ extern void intel_modeset_cleanup(struct drm_device *dev);
+@@ -2701,6 +3048,7 @@
+ extern void i915_redisable_vga(struct drm_device *dev);
+ extern void i915_redisable_vga_power_on(struct drm_device *dev);
+ extern bool intel_fbc_enabled(struct drm_device *dev);
++extern void bdw_fbc_sw_flush(struct drm_device *dev, u32 value);
+ extern void intel_disable_fbc(struct drm_device *dev);
+ extern bool ironlake_set_drps(struct drm_device *dev, u8 val);
+ extern void intel_init_pch_refclk(struct drm_device *dev);
+@@ -2712,14 +3060,11 @@
+ extern int intel_trans_dp_port_sel(struct drm_crtc *crtc);
+ extern int intel_enable_rc6(const struct drm_device *dev);
+ 
+-extern bool i915_semaphore_is_enabled(struct drm_device *dev);
+ int i915_reg_read_ioctl(struct drm_device *dev, void *data,
+ 			struct drm_file *file);
+ int i915_get_reset_stats_ioctl(struct drm_device *dev, void *data,
+ 			       struct drm_file *file);
+ 
+-void intel_notify_mmio_flip(struct intel_engine_cs *ring);
+-
+ /* overlay */
+ extern struct intel_overlay_error_state *intel_overlay_capture_error_state(struct drm_device *dev);
+ extern void intel_overlay_print_error_state(struct drm_i915_error_state_buf *e,
+@@ -2734,8 +3079,10 @@
+  * must be set to prevent GT core from power down and stale values being
+  * returned.
+  */
+-void gen6_gt_force_wake_get(struct drm_i915_private *dev_priv, int fw_engine);
+-void gen6_gt_force_wake_put(struct drm_i915_private *dev_priv, int fw_engine);
++void gen6_gt_force_wake_get(struct drm_i915_private *dev_priv,
++			    unsigned fw_domains);
++void gen6_gt_force_wake_put(struct drm_i915_private *dev_priv,
++			    unsigned fw_domains);
+ void assert_force_wake_inactive(struct drm_i915_private *dev_priv);
+ 
+ int sandybridge_pcode_read(struct drm_i915_private *dev_priv, u8 mbox, u32 *val);
+@@ -2767,11 +3114,6 @@
+ int vlv_gpu_freq(struct drm_i915_private *dev_priv, int val);
+ int vlv_freq_opcode(struct drm_i915_private *dev_priv, int val);
+ 
+-#define FORCEWAKE_RENDER	(1 << 0)
+-#define FORCEWAKE_MEDIA		(1 << 1)
+-#define FORCEWAKE_ALL		(FORCEWAKE_RENDER | FORCEWAKE_MEDIA)
+-
+-
+ #define I915_READ8(reg)		dev_priv->uncore.funcs.mmio_readb(dev_priv, (reg), true)
+ #define I915_WRITE8(reg, val)	dev_priv->uncore.funcs.mmio_writeb(dev_priv, (reg), (val), true)
+ 
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_gem.c b/drivers/gpu/drm/i915/i915_gem.c
+--- a/drivers/gpu/drm/i915/i915_gem.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/i915_gem.c	2014-11-21 11:55:17.304093638 -0700
+@@ -31,6 +31,7 @@
+ #include "i915_drv.h"
+ #include "i915_trace.h"
+ #include "intel_drv.h"
++#include <linux/list_sort.h>
+ #include <linux/oom.h>
+ #include <linux/shmem_fs.h>
+ #include <linux/slab.h>
+@@ -38,14 +39,18 @@
+ #include <linux/pci.h>
+ #include <linux/dma-buf.h>
+ 
++#define RQ_BUG_ON(x) BUG_ON(x)
++
+ static void i915_gem_object_flush_gtt_write_domain(struct drm_i915_gem_object *obj);
+ static void i915_gem_object_flush_cpu_write_domain(struct drm_i915_gem_object *obj,
+ 						   bool force);
+ static __must_check int
+ i915_gem_object_wait_rendering(struct drm_i915_gem_object *obj,
+ 			       bool readonly);
+-static void
+-i915_gem_object_retire(struct drm_i915_gem_object *obj);
++static __must_check int
++i915_gem_object_wait_rendering__nonblocking(struct drm_i915_gem_object *obj,
++					    struct drm_i915_file_private *file_priv,
++					    bool readonly);
+ 
+ static void i915_gem_write_fence(struct drm_device *dev, int reg,
+ 				 struct drm_i915_gem_object *obj);
+@@ -60,7 +65,6 @@
+ static int i915_gem_shrinker_oom(struct notifier_block *nb,
+ 				 unsigned long event,
+ 				 void *ptr);
+-static unsigned long i915_gem_purge(struct drm_i915_private *dev_priv, long target);
+ static unsigned long i915_gem_shrink_all(struct drm_i915_private *dev_priv);
+ 
+ static bool cpu_cache_is_coherent(struct drm_device *dev,
+@@ -108,23 +112,127 @@
+ 	spin_unlock(&dev_priv->mm.object_stat_lock);
+ }
+ 
++static void
++i915_gem_object_retire__write(struct drm_i915_gem_object *obj)
++{
++	RQ_BUG_ON(obj->active == 0);
++	RQ_BUG_ON(obj->last_write.request == NULL);
++	RQ_BUG_ON(list_empty(&obj->last_write.engine_link));
++
++	intel_fb_obj_flush(obj, true);
++	list_del_init(&obj->last_write.engine_link);
++	i915_request_put(obj->last_write.request);
++	obj->last_write.request = NULL;
++}
++
++static void
++i915_gem_object_retire__fence(struct drm_i915_gem_object *obj)
++{
++	RQ_BUG_ON(obj->active == 0);
++	RQ_BUG_ON(obj->last_fence.request == NULL);
++	RQ_BUG_ON(list_empty(&obj->last_fence.engine_link));
++
++	list_del_init(&obj->last_fence.engine_link);
++	i915_request_put(obj->last_fence.request);
++	obj->last_fence.request = NULL;
++}
++
++static void
++i915_gem_object_retire__read(struct drm_i915_gem_object *obj,
++			     struct intel_engine_cs *engine)
++{
++	RQ_BUG_ON(obj->active == 0);
++	RQ_BUG_ON(obj->last_read[engine->id].request == NULL);
++	RQ_BUG_ON(list_empty(&obj->last_read[engine->id].engine_link));
++
++	list_del_init(&obj->last_read[engine->id].engine_link);
++	i915_request_put(obj->last_read[engine->id].request);
++	obj->last_read[engine->id].request = NULL;
++
++	if (obj->last_write.request &&
++	    obj->last_write.request->engine == engine)
++		i915_gem_object_retire__write(obj);
++
++	if (obj->last_fence.request &&
++	    obj->last_fence.request->engine == engine)
++		i915_gem_object_retire__fence(obj);
++
++	if (--obj->active)
++		return;
++
++	RQ_BUG_ON(obj->last_write.request);
++	RQ_BUG_ON(obj->last_fence.request);
++
++	drm_gem_object_unreference(&obj->base);
++}
++
++static void
++i915_vma_retire__read(struct i915_vma *vma, int engine)
++{
++	RQ_BUG_ON(vma->active == 0);
++	RQ_BUG_ON(vma->last_read[engine].request == NULL);
++	RQ_BUG_ON(list_empty(&vma->last_read[engine].engine_link));
++
++	list_del_init(&vma->last_read[engine].engine_link);
++	i915_request_put(vma->last_read[engine].request);
++	vma->last_read[engine].request = NULL;
++
++	if (--vma->active)
++		return;
++
++	if (drm_mm_node_allocated(&vma->node)) {
++		RQ_BUG_ON(list_empty(&vma->mm_list));
++		list_move_tail(&vma->mm_list, &vma->vm->inactive_list);
++		if (vma->vm->closed)
++			WARN_ON(i915_vma_unbind(vma));
++	}
++
++	drm_gem_object_unreference(&vma->obj->base);
++	i915_vma_put(vma);
++}
++
++static void
++i915_gem_object_retire(struct drm_i915_gem_object *obj)
++{
++	struct i915_gem_request *rq;
++	int i;
++
++	/* We should only be called from code paths where we know we
++	 * hold both the active reference *and* a user reference.
++	 * Therefore we can safely access the object after retiring as
++	 * we will hold a second reference and not free the object.
++	 */
++
++	rq = obj->last_write.request;
++	if (rq && i915_request_complete(rq))
++		i915_gem_object_retire__write(obj);
++
++	rq = obj->last_fence.request;
++	if (rq && i915_request_complete(rq))
++		i915_gem_object_retire__fence(obj);
++
++	for (i = 0; i < I915_NUM_ENGINES; i++) {
++		rq = obj->last_read[i].request;
++		if (rq && i915_request_complete(rq))
++			i915_gem_object_retire__read(obj, rq->engine);
++	}
++
++	if (!obj->active)
++		i915_gem_retire_requests(obj->base.dev);
++}
++
+ static int
+ i915_gem_wait_for_error(struct i915_gpu_error *error)
+ {
+ 	int ret;
+ 
+-#define EXIT_COND (!i915_reset_in_progress(error) || \
+-		   i915_terminally_wedged(error))
+-	if (EXIT_COND)
+-		return 0;
+-
+ 	/*
+ 	 * Only wait 10 seconds for the gpu reset to complete to avoid hanging
+ 	 * userspace. If it takes that long something really bad is going on and
+ 	 * we should simply try to bail out and fail as gracefully as possible.
+ 	 */
+ 	ret = wait_event_interruptible_timeout(error->reset_queue,
+-					       EXIT_COND,
++					       !i915_recovery_pending(error),
+ 					       10*HZ);
+ 	if (ret == 0) {
+ 		DRM_ERROR("Timed out waiting for the gpu reset to complete\n");
+@@ -132,7 +240,6 @@
+ 	} else if (ret < 0) {
+ 		return ret;
+ 	}
+-#undef EXIT_COND
+ 
+ 	return 0;
+ }
+@@ -146,20 +253,15 @@
+ 	if (ret)
+ 		return ret;
+ 
+-	ret = mutex_lock_interruptible(&dev->struct_mutex);
++	ret = mutex_lock_wrapper(&dev->struct_mutex,
++				 TASK_INTERRUPTIBLE,
++				 _RET_IP_);
+ 	if (ret)
+ 		return ret;
+ 
+-	WARN_ON(i915_verify_lists(dev));
+ 	return 0;
+ }
+ 
+-static inline bool
+-i915_gem_object_is_inactive(struct drm_i915_gem_object *obj)
+-{
+-	return i915_gem_obj_bound_any(obj) && !obj->active;
+-}
+-
+ int
+ i915_gem_init_ioctl(struct drm_device *dev, void *data,
+ 		    struct drm_file *file)
+@@ -187,6 +289,49 @@
+ 	return 0;
+ }
+ 
++static int obj_rank_by_ggtt(void *priv,
++			    struct list_head *A,
++			    struct list_head *B)
++{
++	struct drm_i915_gem_object *a = list_entry(A,typeof(*a), obj_exec_link);
++	struct drm_i915_gem_object *b = list_entry(B,typeof(*b), obj_exec_link);
++
++	return i915_gem_obj_ggtt_offset(a) - i915_gem_obj_ggtt_offset(b);
++}
++
++static u32 __fence_size(struct drm_i915_private *dev_priv, u32 start, u32 end)
++{
++	u32 size = end - start;
++	u32 fence_size;
++
++	if (INTEL_INFO(dev_priv)->gen < 4) {
++		u32 fence_max;
++		u32 fence_next;
++
++		if (IS_GEN3(dev_priv)) {
++			fence_max = I830_FENCE_MAX_SIZE_VAL << 20;
++			fence_next = 1024*1024;
++		} else {
++			fence_max = I830_FENCE_MAX_SIZE_VAL << 19;
++			fence_next = 512*1024;
++		}
++
++		fence_max = min(fence_max, size);
++		fence_size = 0;
++		while (fence_next <= fence_max) {
++			u32 base = ALIGN(start, fence_next);
++			if (base + fence_next > end)
++				break;
++
++			fence_size = fence_next;
++			fence_next <<= 1;
++		}
++	} else
++		fence_size = size;
++
++	return fence_size;
++}
++
+ int
+ i915_gem_get_aperture_ioctl(struct drm_device *dev, void *data,
+ 			    struct drm_file *file)
+@@ -194,55 +339,208 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct drm_i915_gem_get_aperture *args = data;
+ 	struct drm_i915_gem_object *obj;
+-	size_t pinned;
++	struct list_head map_list;
++	const u32 map_limit = dev_priv->gtt.mappable_end;
++	size_t pinned, map_space, map_largest, fence_space, fence_largest;
++	u32 last, size;
++
++	INIT_LIST_HEAD(&map_list);
+ 
+ 	pinned = 0;
++	map_space = map_largest = 0;
++	fence_space = fence_largest = 0;
++
+ 	mutex_lock(&dev->struct_mutex);
+-	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list)
+-		if (i915_gem_obj_is_pinned(obj))
+-			pinned += i915_gem_obj_ggtt_size(obj);
++	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list) {
++		struct i915_vma *vma = i915_gem_obj_to_ggtt(obj);
++
++		if (vma == NULL || !vma->pin_count)
++			continue;
++
++		pinned += vma->node.size;
++
++		if (vma->node.start < map_limit)
++			list_add(&obj->obj_exec_link, &map_list);
++	}
++
++	last = 0;
++	list_sort(NULL, &map_list, obj_rank_by_ggtt);
++	while (!list_empty(&map_list)) {
++		struct i915_vma *vma;
++
++		obj = list_first_entry(&map_list, typeof(*obj), obj_exec_link);
++		list_del_init(&obj->obj_exec_link);
++
++		vma = i915_gem_obj_to_ggtt(obj);
++		if (last == 0)
++			goto skip_first;
++
++		size = vma->node.start - last;
++		if (size > map_largest)
++			map_largest = size;
++		map_space += size;
++
++		size = __fence_size(dev_priv, last, vma->node.start);
++		if (size > fence_largest)
++			fence_largest = size;
++		fence_space += size;
++
++skip_first:
++		last = vma->node.start + vma->node.size;
++	}
++	if (last < map_limit) {
++		size = map_limit - last;
++		if (size > map_largest)
++			map_largest = size;
++		map_space += size;
++
++		size = __fence_size(dev_priv, last, map_limit);
++		if (size > fence_largest)
++			fence_largest = size;
++		fence_space += size;
++	}
+ 	mutex_unlock(&dev->struct_mutex);
+ 
+ 	args->aper_size = dev_priv->gtt.base.total;
+ 	args->aper_available_size = args->aper_size - pinned;
++	args->map_available_size = map_space;
++	args->map_largest_size = map_largest;
++	args->map_total_size = dev_priv->gtt.mappable_end;
++	args->fence_available_size = fence_space;
++	args->fence_largest_size = fence_largest;
+ 
+ 	return 0;
+ }
+ 
+-static void i915_gem_object_detach_phys(struct drm_i915_gem_object *obj)
++static int
++i915_gem_object_get_pages_phys(struct drm_i915_gem_object *obj)
+ {
+-	drm_dma_handle_t *phys = obj->phys_handle;
++	struct address_space *mapping = file_inode(obj->base.filp)->i_mapping;
++	char *vaddr = obj->phys_handle->vaddr;
++	struct sg_table *st;
++	struct scatterlist *sg;
++	int i;
+ 
+-	if (!phys)
+-		return;
++	if (WARN_ON(i915_gem_object_needs_bit17_swizzle(obj)))
++		return -EINVAL;
++
++	for (i = 0; i < obj->base.size / PAGE_SIZE; i++) {
++		struct page *page;
++		char *src;
++
++		page = shmem_read_mapping_page(mapping, i);
++		if (IS_ERR(page))
++			return PTR_ERR(page);
++
++		src = kmap_atomic(page);
++		memcpy(vaddr, src, PAGE_SIZE);
++		drm_clflush_virt_range(vaddr, PAGE_SIZE);
++		kunmap_atomic(src);
++
++		page_cache_release(page);
++		vaddr += PAGE_SIZE;
++	}
++
++	i915_gem_chipset_flush(obj->base.dev);
++
++	st = kmalloc(sizeof(*st), GFP_KERNEL);
++	if (st == NULL)
++		return -ENOMEM;
++
++	if (sg_alloc_table(st, 1, GFP_KERNEL)) {
++		kfree(st);
++		return -ENOMEM;
++	}
++
++	sg = st->sgl;
++	sg->offset = 0;
++	sg->length = obj->base.size;
++
++	sg_dma_address(sg) = obj->phys_handle->busaddr;
++	sg_dma_len(sg) = obj->base.size;
++
++	obj->pages = st;
++	obj->has_dma_mapping = true;
++	return 0;
++}
++
++static void
++i915_gem_object_put_pages_phys(struct drm_i915_gem_object *obj)
++{
++	int ret;
++
++	ret = i915_gem_object_set_to_cpu_domain(obj, true);
++	if (ret) {
++		/* In the event of a disaster, abandon all caches and
++		 * hope for the best.
++		 */
++		WARN_ON(ret != -EIO);
++		obj->base.read_domains = obj->base.write_domain = I915_GEM_DOMAIN_CPU;
++	}
++
++	if (obj->madv == I915_MADV_DONTNEED)
++		obj->dirty = 0;
+ 
+-	if (obj->madv == I915_MADV_WILLNEED) {
++	if (obj->dirty) {
+ 		struct address_space *mapping = file_inode(obj->base.filp)->i_mapping;
+-		char *vaddr = phys->vaddr;
++		char *vaddr = obj->phys_handle->vaddr;
+ 		int i;
+ 
+ 		for (i = 0; i < obj->base.size / PAGE_SIZE; i++) {
+-			struct page *page = shmem_read_mapping_page(mapping, i);
+-			if (!IS_ERR(page)) {
+-				char *dst = kmap_atomic(page);
+-				memcpy(dst, vaddr, PAGE_SIZE);
+-				drm_clflush_virt_range(dst, PAGE_SIZE);
+-				kunmap_atomic(dst);
++			struct page *page;
++			char *dst;
++
++			page = shmem_read_mapping_page(mapping, i);
++			if (IS_ERR(page))
++				continue;
+ 
+-				set_page_dirty(page);
++			dst = kmap_atomic(page);
++			drm_clflush_virt_range(vaddr, PAGE_SIZE);
++			memcpy(dst, vaddr, PAGE_SIZE);
++			kunmap_atomic(dst);
++
++			set_page_dirty(page);
++			if (obj->madv == I915_MADV_WILLNEED)
+ 				mark_page_accessed(page);
+-				page_cache_release(page);
+-			}
++			page_cache_release(page);
+ 			vaddr += PAGE_SIZE;
+ 		}
+-		i915_gem_chipset_flush(obj->base.dev);
++		obj->dirty = 0;
+ 	}
+ 
+-#ifdef CONFIG_X86
+-	set_memory_wb((unsigned long)phys->vaddr, phys->size / PAGE_SIZE);
+-#endif
+-	drm_pci_free(obj->base.dev, phys);
+-	obj->phys_handle = NULL;
++	sg_free_table(obj->pages);
++	kfree(obj->pages);
++
++	obj->has_dma_mapping = false;
++}
++
++static void
++i915_gem_object_release_phys(struct drm_i915_gem_object *obj)
++{
++	drm_pci_free(obj->base.dev, obj->phys_handle);
++}
++
++static const struct drm_i915_gem_object_ops i915_gem_phys_ops = {
++	.get_pages = i915_gem_object_get_pages_phys,
++	.put_pages = i915_gem_object_put_pages_phys,
++	.release = i915_gem_object_release_phys,
++};
++
++static int
++drop_pages(struct drm_i915_gem_object *obj)
++{
++	struct i915_vma *vma, *next;
++	int ret;
++
++	drm_gem_object_reference(&obj->base);
++	list_for_each_entry_safe(vma, next, &obj->vma_list, obj_link)
++		if (i915_vma_unbind(vma))
++			break;
++
++	ret = i915_gem_object_put_pages(obj);
++	drm_gem_object_unreference(&obj->base);
++
++	return ret;
+ }
+ 
+ int
+@@ -250,9 +548,7 @@
+ 			    int align)
+ {
+ 	drm_dma_handle_t *phys;
+-	struct address_space *mapping;
+-	char *vaddr;
+-	int i;
++	int ret;
+ 
+ 	if (obj->phys_handle) {
+ 		if ((unsigned long)obj->phys_handle->vaddr & (align -1))
+@@ -267,41 +563,19 @@
+ 	if (obj->base.filp == NULL)
+ 		return -EINVAL;
+ 
++	ret = drop_pages(obj);
++	if (ret)
++		return ret;
++
+ 	/* create a new object */
+ 	phys = drm_pci_alloc(obj->base.dev, obj->base.size, align);
+ 	if (!phys)
+ 		return -ENOMEM;
+ 
+-	vaddr = phys->vaddr;
+-#ifdef CONFIG_X86
+-	set_memory_wc((unsigned long)vaddr, phys->size / PAGE_SIZE);
+-#endif
+-	mapping = file_inode(obj->base.filp)->i_mapping;
+-	for (i = 0; i < obj->base.size / PAGE_SIZE; i++) {
+-		struct page *page;
+-		char *src;
+-
+-		page = shmem_read_mapping_page(mapping, i);
+-		if (IS_ERR(page)) {
+-#ifdef CONFIG_X86
+-			set_memory_wb((unsigned long)phys->vaddr, phys->size / PAGE_SIZE);
+-#endif
+-			drm_pci_free(obj->base.dev, phys);
+-			return PTR_ERR(page);
+-		}
+-
+-		src = kmap_atomic(page);
+-		memcpy(vaddr, src, PAGE_SIZE);
+-		kunmap_atomic(src);
+-
+-		mark_page_accessed(page);
+-		page_cache_release(page);
+-
+-		vaddr += PAGE_SIZE;
+-	}
+-
+ 	obj->phys_handle = phys;
+-	return 0;
++	obj->ops = &i915_gem_phys_ops;
++
++	return i915_gem_object_get_pages(obj);
+ }
+ 
+ static int
+@@ -312,6 +586,14 @@
+ 	struct drm_device *dev = obj->base.dev;
+ 	void *vaddr = obj->phys_handle->vaddr + args->offset;
+ 	char __user *user_data = to_user_ptr(args->data_ptr);
++	int ret;
++
++	/* We manually control the domain here and pretend that it
++	 * remains coherent i.e. in the GTT domain, like shmem_pwrite.
++	 */
++	ret = i915_gem_object_wait_rendering(obj, false);
++	if (ret)
++		return ret;
+ 
+ 	if (__copy_from_user_inatomic_nocache(vaddr, user_data, args->size)) {
+ 		unsigned long unwritten;
+@@ -327,6 +609,7 @@
+ 			return -EFAULT;
+ 	}
+ 
++	drm_clflush_virt_range(vaddr, args->size);
+ 	i915_gem_chipset_flush(dev);
+ 	return 0;
+ }
+@@ -474,8 +757,6 @@
+ 		ret = i915_gem_object_wait_rendering(obj, true);
+ 		if (ret)
+ 			return ret;
+-
+-		i915_gem_object_retire(obj);
+ 	}
+ 
+ 	ret = i915_gem_object_get_pages(obj);
+@@ -618,7 +899,7 @@
+ 
+ 		mutex_unlock(&dev->struct_mutex);
+ 
+-		if (likely(!i915.prefault_disable) && !prefaulted) {
++		if (likely(!i915_module.prefault_disable) && !prefaulted) {
+ 			ret = fault_in_multipages_writeable(user_data, remain);
+ 			/* Userspace is tricking us, but we've already clobbered
+ 			 * its pages with the prefault and promised to write the
+@@ -695,6 +976,12 @@
+ 		goto out;
+ 	}
+ 
++	ret = i915_gem_object_wait_rendering__nonblocking(obj,
++							  file->driver_priv,
++							  false);
++	if (ret)
++		goto out;
++
+ 	trace_i915_gem_object_pread(obj, args->offset, args->size);
+ 
+ 	ret = i915_gem_shmem_pread(dev, obj, args, file);
+@@ -745,7 +1032,7 @@
+ 	char __user *user_data;
+ 	int page_offset, page_length, ret;
+ 
+-	ret = i915_gem_obj_ggtt_pin(obj, 0, PIN_MAPPABLE | PIN_NONBLOCK);
++	ret = i915_gem_object_ggtt_pin(obj, 0, PIN_MAPPABLE | PIN_NONBLOCK);
+ 	if (ret)
+ 		goto out;
+ 
+@@ -877,6 +1164,14 @@
+ 	int needs_clflush_before = 0;
+ 	struct sg_page_iter sg_iter;
+ 
++	/* prime objects have no backing filp to GEM pread/pwrite
++	 * pages from.
++	 */
++	if (!obj->base.filp) {
++		ret = -EINVAL;
++		goto out;
++	}
++
+ 	user_data = to_user_ptr(args->data_ptr);
+ 	remain = args->size;
+ 
+@@ -891,8 +1186,6 @@
+ 		ret = i915_gem_object_wait_rendering(obj, false);
+ 		if (ret)
+ 			return ret;
+-
+-		i915_gem_object_retire(obj);
+ 	}
+ 	/* Same trick applies to invalidate partially written cachelines read
+ 	 * before writing. */
+@@ -1006,7 +1299,7 @@
+ 		       args->size))
+ 		return -EFAULT;
+ 
+-	if (likely(!i915.prefault_disable)) {
++	if (likely(!i915_module.prefault_disable)) {
+ 		ret = fault_in_multipages_readable(to_user_ptr(args->data_ptr),
+ 						   args->size);
+ 		if (ret)
+@@ -1030,13 +1323,11 @@
+ 		goto out;
+ 	}
+ 
+-	/* prime objects have no backing filp to GEM pread/pwrite
+-	 * pages from.
+-	 */
+-	if (!obj->base.filp) {
+-		ret = -EINVAL;
++	ret = i915_gem_object_wait_rendering__nonblocking(obj,
++							  file->driver_priv,
++							  true);
++	if (ret)
+ 		goto out;
+-	}
+ 
+ 	trace_i915_gem_object_pwrite(obj, args->offset, args->size);
+ 
+@@ -1047,22 +1338,22 @@
+ 	 * pread/pwrite currently are reading and writing from the CPU
+ 	 * perspective, requiring manual detiling by the client.
+ 	 */
+-	if (obj->phys_handle) {
+-		ret = i915_gem_phys_pwrite(obj, args, file);
+-		goto out;
+-	}
+-
+ 	if (obj->tiling_mode == I915_TILING_NONE &&
+-	    obj->base.write_domain != I915_GEM_DOMAIN_CPU &&
+-	    cpu_write_needs_clflush(obj)) {
++	    (obj->base.filp == NULL ||
++	     (obj->base.write_domain != I915_GEM_DOMAIN_CPU &&
++	      cpu_write_needs_clflush(obj)))) {
+ 		ret = i915_gem_gtt_pwrite_fast(dev, obj, args, file);
+ 		/* Note that the gtt paths might fail with non-page-backed user
+ 		 * pointers (e.g. gtt mappings when moving data between
+ 		 * textures). Fallback to the shmem path in that case. */
+ 	}
+ 
+-	if (ret == -EFAULT || ret == -ENOSPC)
+-		ret = i915_gem_shmem_pwrite(dev, obj, args, file);
++	if (ret == -EFAULT || ret == -ENOSPC) {
++		if (obj->phys_handle)
++			ret = i915_gem_phys_pwrite(obj, args, file);
++		else
++			ret = i915_gem_shmem_pwrite(dev, obj, args, file);
++	}
+ 
+ out:
+ 	drm_gem_object_unreference(&obj->base);
+@@ -1071,254 +1362,42 @@
+ 	return ret;
+ }
+ 
+-int
+-i915_gem_check_wedge(struct i915_gpu_error *error,
+-		     bool interruptible)
++/**
++ * Ensures that all rendering to the object has completed and the object is
++ * safe to unbind from the GTT or access from the CPU.
++ */
++static __must_check int
++i915_gem_object_wait_rendering(struct drm_i915_gem_object *obj,
++			       bool readonly)
+ {
+-	if (i915_reset_in_progress(error)) {
+-		/* Non-interruptible callers can't handle -EAGAIN, hence return
+-		 * -EIO unconditionally for these. */
+-		if (!interruptible)
+-			return -EIO;
++	int i, ret;
+ 
+-		/* Recovery complete, but the reset failed ... */
+-		if (i915_terminally_wedged(error))
+-			return -EIO;
++	if (!obj->active)
++		return 0;
++
++	if (readonly) {
++		if (obj->last_write.request) {
++			ret = i915_request_wait(obj->last_write.request);
++			if (ret)
++				return ret;
++		}
++	} else {
++		for (i = 0; i < I915_NUM_ENGINES; i++) {
++			if (obj->last_read[i].request == NULL)
++				continue;
+ 
+-		return -EAGAIN;
++			ret = i915_request_wait(obj->last_read[i].request);
++			if (ret)
++				return ret;
++		}
+ 	}
+ 
++	i915_gem_object_retire(obj);
+ 	return 0;
+ }
+ 
+-/*
+- * Compare seqno against outstanding lazy request. Emit a request if they are
+- * equal.
+- */
+-int
+-i915_gem_check_olr(struct intel_engine_cs *ring, u32 seqno)
+-{
+-	int ret;
+-
+-	BUG_ON(!mutex_is_locked(&ring->dev->struct_mutex));
+-
+-	ret = 0;
+-	if (seqno == ring->outstanding_lazy_seqno)
+-		ret = i915_add_request(ring, NULL);
+-
+-	return ret;
+-}
+-
+-static void fake_irq(unsigned long data)
+-{
+-	wake_up_process((struct task_struct *)data);
+-}
+-
+-static bool missed_irq(struct drm_i915_private *dev_priv,
+-		       struct intel_engine_cs *ring)
+-{
+-	return test_bit(ring->id, &dev_priv->gpu_error.missed_irq_rings);
+-}
+-
+-static bool can_wait_boost(struct drm_i915_file_private *file_priv)
+-{
+-	if (file_priv == NULL)
+-		return true;
+-
+-	return !atomic_xchg(&file_priv->rps_wait_boost, true);
+-}
+-
+-/**
+- * __wait_seqno - wait until execution of seqno has finished
+- * @ring: the ring expected to report seqno
+- * @seqno: duh!
+- * @reset_counter: reset sequence associated with the given seqno
+- * @interruptible: do an interruptible wait (normally yes)
+- * @timeout: in - how long to wait (NULL forever); out - how much time remaining
+- *
+- * Note: It is of utmost importance that the passed in seqno and reset_counter
+- * values have been read by the caller in an smp safe manner. Where read-side
+- * locks are involved, it is sufficient to read the reset_counter before
+- * unlocking the lock that protects the seqno. For lockless tricks, the
+- * reset_counter _must_ be read before, and an appropriate smp_rmb must be
+- * inserted.
+- *
+- * Returns 0 if the seqno was found within the alloted time. Else returns the
+- * errno with remaining time filled in timeout argument.
+- */
+-static int __wait_seqno(struct intel_engine_cs *ring, u32 seqno,
+-			unsigned reset_counter,
+-			bool interruptible,
+-			s64 *timeout,
+-			struct drm_i915_file_private *file_priv)
+-{
+-	struct drm_device *dev = ring->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	const bool irq_test_in_progress =
+-		ACCESS_ONCE(dev_priv->gpu_error.test_irq_rings) & intel_ring_flag(ring);
+-	DEFINE_WAIT(wait);
+-	unsigned long timeout_expire;
+-	s64 before, now;
+-	int ret;
+-
+-	WARN(!intel_irqs_enabled(dev_priv), "IRQs disabled");
+-
+-	if (i915_seqno_passed(ring->get_seqno(ring, true), seqno))
+-		return 0;
+-
+-	timeout_expire = timeout ? jiffies + nsecs_to_jiffies((u64)*timeout) : 0;
+-
+-	if (INTEL_INFO(dev)->gen >= 6 && ring->id == RCS && can_wait_boost(file_priv)) {
+-		gen6_rps_boost(dev_priv);
+-		if (file_priv)
+-			mod_delayed_work(dev_priv->wq,
+-					 &file_priv->mm.idle_work,
+-					 msecs_to_jiffies(100));
+-	}
+-
+-	if (!irq_test_in_progress && WARN_ON(!ring->irq_get(ring)))
+-		return -ENODEV;
+-
+-	/* Record current time in case interrupted by signal, or wedged */
+-	trace_i915_gem_request_wait_begin(ring, seqno);
+-	before = ktime_get_raw_ns();
+-	for (;;) {
+-		struct timer_list timer;
+-
+-		prepare_to_wait(&ring->irq_queue, &wait,
+-				interruptible ? TASK_INTERRUPTIBLE : TASK_UNINTERRUPTIBLE);
+-
+-		/* We need to check whether any gpu reset happened in between
+-		 * the caller grabbing the seqno and now ... */
+-		if (reset_counter != atomic_read(&dev_priv->gpu_error.reset_counter)) {
+-			/* ... but upgrade the -EAGAIN to an -EIO if the gpu
+-			 * is truely gone. */
+-			ret = i915_gem_check_wedge(&dev_priv->gpu_error, interruptible);
+-			if (ret == 0)
+-				ret = -EAGAIN;
+-			break;
+-		}
+-
+-		if (i915_seqno_passed(ring->get_seqno(ring, false), seqno)) {
+-			ret = 0;
+-			break;
+-		}
+-
+-		if (interruptible && signal_pending(current)) {
+-			ret = -ERESTARTSYS;
+-			break;
+-		}
+-
+-		if (timeout && time_after_eq(jiffies, timeout_expire)) {
+-			ret = -ETIME;
+-			break;
+-		}
+-
+-		timer.function = NULL;
+-		if (timeout || missed_irq(dev_priv, ring)) {
+-			unsigned long expire;
+-
+-			setup_timer_on_stack(&timer, fake_irq, (unsigned long)current);
+-			expire = missed_irq(dev_priv, ring) ? jiffies + 1 : timeout_expire;
+-			mod_timer(&timer, expire);
+-		}
+-
+-		io_schedule();
+-
+-		if (timer.function) {
+-			del_singleshot_timer_sync(&timer);
+-			destroy_timer_on_stack(&timer);
+-		}
+-	}
+-	now = ktime_get_raw_ns();
+-	trace_i915_gem_request_wait_end(ring, seqno);
+-
+-	if (!irq_test_in_progress)
+-		ring->irq_put(ring);
+-
+-	finish_wait(&ring->irq_queue, &wait);
+-
+-	if (timeout) {
+-		s64 tres = *timeout - (now - before);
+-
+-		*timeout = tres < 0 ? 0 : tres;
+-	}
+-
+-	return ret;
+-}
+-
+-/**
+- * Waits for a sequence number to be signaled, and cleans up the
+- * request and object lists appropriately for that event.
+- */
+-int
+-i915_wait_seqno(struct intel_engine_cs *ring, uint32_t seqno)
+-{
+-	struct drm_device *dev = ring->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	bool interruptible = dev_priv->mm.interruptible;
+-	int ret;
+-
+-	BUG_ON(!mutex_is_locked(&dev->struct_mutex));
+-	BUG_ON(seqno == 0);
+-
+-	ret = i915_gem_check_wedge(&dev_priv->gpu_error, interruptible);
+-	if (ret)
+-		return ret;
+-
+-	ret = i915_gem_check_olr(ring, seqno);
+-	if (ret)
+-		return ret;
+-
+-	return __wait_seqno(ring, seqno,
+-			    atomic_read(&dev_priv->gpu_error.reset_counter),
+-			    interruptible, NULL, NULL);
+-}
+-
+-static int
+-i915_gem_object_wait_rendering__tail(struct drm_i915_gem_object *obj,
+-				     struct intel_engine_cs *ring)
+-{
+-	if (!obj->active)
+-		return 0;
+-
+-	/* Manually manage the write flush as we may have not yet
+-	 * retired the buffer.
+-	 *
+-	 * Note that the last_write_seqno is always the earlier of
+-	 * the two (read/write) seqno, so if we haved successfully waited,
+-	 * we know we have passed the last write.
+-	 */
+-	obj->last_write_seqno = 0;
+-
+-	return 0;
+-}
+-
+-/**
+- * Ensures that all rendering to the object has completed and the object is
+- * safe to unbind from the GTT or access from the CPU.
+- */
+-static __must_check int
+-i915_gem_object_wait_rendering(struct drm_i915_gem_object *obj,
+-			       bool readonly)
+-{
+-	struct intel_engine_cs *ring = obj->ring;
+-	u32 seqno;
+-	int ret;
+-
+-	seqno = readonly ? obj->last_write_seqno : obj->last_read_seqno;
+-	if (seqno == 0)
+-		return 0;
+-
+-	ret = i915_wait_seqno(ring, seqno);
+-	if (ret)
+-		return ret;
+-
+-	return i915_gem_object_wait_rendering__tail(obj, ring);
+-}
+-
+-/* A nonblocking variant of the above wait. This is a highly dangerous routine
+- * as the object state may change during this call.
++/* A nonblocking variant of the above wait. This is a highly dangerous routine
++ * as the object state may change during this call.
+  */
+ static __must_check int
+ i915_gem_object_wait_rendering__nonblocking(struct drm_i915_gem_object *obj,
+@@ -1327,34 +1406,51 @@
+ {
+ 	struct drm_device *dev = obj->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring = obj->ring;
+-	unsigned reset_counter;
+-	u32 seqno;
+-	int ret;
++	struct i915_gem_request *rq[I915_NUM_ENGINES] = {};
++	int i, n, ret;
+ 
+ 	BUG_ON(!mutex_is_locked(&dev->struct_mutex));
+ 	BUG_ON(!dev_priv->mm.interruptible);
+ 
+-	seqno = readonly ? obj->last_write_seqno : obj->last_read_seqno;
+-	if (seqno == 0)
++	n = 0;
++	if (readonly) {
++		if (obj->last_write.request) {
++			rq[n] = i915_request_get_breadcrumb(obj->last_write.request);
++			if (IS_ERR(rq[n]))
++				return PTR_ERR(rq[n]);
++			n++;
++		}
++	} else {
++		for (i = 0; i < I915_NUM_ENGINES; i++) {
++			if (obj->last_read[i].request == NULL)
++				continue;
++
++			rq[n] = i915_request_get_breadcrumb(obj->last_read[i].request);
++			if (IS_ERR(rq[n])) {
++				ret = PTR_ERR(rq[n]);
++				goto out;
++			}
++			n++;
++		}
++	}
++	if (n == 0)
+ 		return 0;
+ 
+-	ret = i915_gem_check_wedge(&dev_priv->gpu_error, true);
+-	if (ret)
+-		return ret;
++	mutex_unlock(&dev->struct_mutex);
+ 
+-	ret = i915_gem_check_olr(ring, seqno);
+-	if (ret)
+-		return ret;
++	for (i = 0; i < n; i++) {
++		ret = __i915_request_wait(rq[i], true, NULL, file_priv);
++		if (ret)
++			break;
++	}
+ 
+-	reset_counter = atomic_read(&dev_priv->gpu_error.reset_counter);
+-	mutex_unlock(&dev->struct_mutex);
+-	ret = __wait_seqno(ring, seqno, reset_counter, true, NULL, file_priv);
+ 	mutex_lock(&dev->struct_mutex);
+-	if (ret)
+-		return ret;
+ 
+-	return i915_gem_object_wait_rendering__tail(obj, ring);
++out:
++	for (i = 0; i < n; i++)
++		i915_request_put(rq[i]);
++
++	return ret;
+ }
+ 
+ /**
+@@ -1404,18 +1500,10 @@
+ 	if (ret)
+ 		goto unref;
+ 
+-	if (read_domains & I915_GEM_DOMAIN_GTT) {
++	if (read_domains & I915_GEM_DOMAIN_GTT)
+ 		ret = i915_gem_object_set_to_gtt_domain(obj, write_domain != 0);
+-
+-		/* Silently promote "you're not bound, there was nothing to do"
+-		 * to success, since the client was just asking us to
+-		 * make sure everything was done.
+-		 */
+-		if (ret == -EINVAL)
+-			ret = 0;
+-	} else {
++	else
+ 		ret = i915_gem_object_set_to_cpu_domain(obj, write_domain != 0);
+-	}
+ 
+ unref:
+ 	drm_gem_object_unreference(&obj->base);
+@@ -1461,6 +1549,16 @@
+  *
+  * While the mapping holds a reference on the contents of the object, it doesn't
+  * imply a ref on the object itself.
++ *
++ * IMPORTANT:
++ *
++ * DRM driver writers who look a this function as an example for how to do GEM
++ * mmap support, please don't implement mmap support like here. The modern way
++ * to implement DRM mmap support is with an mmap offset ioctl (like
++ * i915_gem_mmap_gtt) and then using the mmap syscall on the DRM fd directly.
++ * That way debug tooling like valgrind will understand what's going on, hiding
++ * the mmap call in a driver private ioctl will break that. The i915 driver only
++ * does cpu mmaps this way because we didn't know better.
+  */
+ int
+ i915_gem_mmap_ioctl(struct drm_device *dev, void *data,
+@@ -1470,6 +1568,12 @@
+ 	struct drm_gem_object *obj;
+ 	unsigned long addr;
+ 
++	if (args->flags & ~(I915_MMAP_WC))
++		return -EINVAL;
++
++	if (args->flags & I915_MMAP_WC && !cpu_has_pat)
++		return -ENODEV;
++
+ 	obj = drm_gem_object_lookup(dev, file, args->handle);
+ 	if (obj == NULL)
+ 		return -ENOENT;
+@@ -1485,6 +1589,19 @@
+ 	addr = vm_mmap(obj->filp, 0, args->size,
+ 		       PROT_READ | PROT_WRITE, MAP_SHARED,
+ 		       args->offset);
++	if (args->flags & I915_MMAP_WC) {
++		struct mm_struct *mm = current->mm;
++		struct vm_area_struct *vma;
++
++		down_write(&mm->mmap_sem);
++		vma = find_vma(mm, addr);
++		if (vma)
++			vma->vm_page_prot =
++				pgprot_writecombine(vm_get_page_prot(vma->vm_flags));
++		else
++			addr = -ENOMEM;
++		up_write(&mm->mmap_sem);
++	}
+ 	drm_gem_object_unreference_unlocked(obj);
+ 	if (IS_ERR((void *)addr))
+ 		return addr;
+@@ -1548,7 +1665,7 @@
+ 	}
+ 
+ 	/* Now bind it into the GTT if needed */
+-	ret = i915_gem_obj_ggtt_pin(obj, 0, PIN_MAPPABLE);
++	ret = i915_gem_object_ggtt_pin(obj, 0, PIN_MAPPABLE);
+ 	if (ret)
+ 		goto unlock;
+ 
+@@ -1564,51 +1681,35 @@
+ 	pfn = dev_priv->gtt.mappable_base + i915_gem_obj_ggtt_offset(obj);
+ 	pfn >>= PAGE_SHIFT;
+ 
+-	if (!obj->fault_mappable) {
+-		unsigned long size = min_t(unsigned long,
+-					   vma->vm_end - vma->vm_start,
+-					   obj->base.size);
+-		int i;
++	ret = remap_io_mapping(vma,
++			       vma->vm_start, pfn, vma->vm_end - vma->vm_start,
++			       dev_priv->gtt.mappable);
++	if (ret)
++		goto unpin;
+ 
+-		for (i = 0; i < size >> PAGE_SHIFT; i++) {
+-			ret = vm_insert_pfn(vma,
+-					    (unsigned long)vma->vm_start + i * PAGE_SIZE,
+-					    pfn + i);
+-			if (ret)
+-				break;
+-		}
++	obj->fault_mappable = true;
+ 
+-		obj->fault_mappable = true;
+-	} else
+-		ret = vm_insert_pfn(vma,
+-				    (unsigned long)vmf->virtual_address,
+-				    pfn + page_offset);
+ unpin:
+ 	i915_gem_object_ggtt_unpin(obj);
+ unlock:
+ 	mutex_unlock(&dev->struct_mutex);
+ out:
+ 	switch (ret) {
+-	case -EIO:
++	case 0:
+ 		/*
+-		 * We eat errors when the gpu is terminally wedged to avoid
+-		 * userspace unduly crashing (gl has no provisions for mmaps to
+-		 * fail). But any other -EIO isn't ours (e.g. swap in failure)
+-		 * and so needs to be reported.
++		 * Success. You may proceed.
+ 		 */
+-		if (!i915_terminally_wedged(&dev_priv->gpu_error)) {
+-			ret = VM_FAULT_SIGBUS;
+-			break;
+-		}
+ 	case -EAGAIN:
+ 		/*
+ 		 * EAGAIN means the gpu is hung and we'll wait for the error
+ 		 * handler to reset everything when re-faulting in
+ 		 * i915_mutex_lock_interruptible.
+ 		 */
+-	case 0:
+ 	case -ERESTARTSYS:
+ 	case -EINTR:
++		/* Signal interruptus. Return to userspace and await
++		 * further instructions.
++		 */
+ 	case -EBUSY:
+ 		/*
+ 		 * EBUSY is ok: this just means that another thread
+@@ -1619,6 +1720,20 @@
+ 	case -ENOMEM:
+ 		ret = VM_FAULT_OOM;
+ 		break;
++	case -EIO:
++		/* The driver should never report an EIO here due to GPU hangs,
++		 * all requests shold have been reset and the object should
++		 * be idle. There should be no impediment for us to mmap the
++		 * object in those cases. It would be nice if we could throw
++		 * a warning in such cases, but...
++		 *
++		 * EIO can also arise from a failure to swap in pages,
++		 * for example. Here, we must report SIGBUS as there is
++		 * no recovery for memory corruption.
++		 *
++		 * And since we can have a wedged GPU and swap failure,
++		 * having a WARN here could be misleading.
++		 */
+ 	case -ENOSPC:
+ 	case -EFAULT:
+ 		ret = VM_FAULT_SIGBUS;
+@@ -1735,7 +1850,11 @@
+ 	 * offsets on purgeable objects by truncating it and marking it purged,
+ 	 * which prevents userspace from ever using that object again.
+ 	 */
+-	i915_gem_purge(dev_priv, obj->base.size >> PAGE_SHIFT);
++	i915_gem_shrink(dev_priv,
++			obj->base.size >> PAGE_SHIFT,
++			I915_SHRINK_BOUND |
++			I915_SHRINK_UNBOUND |
++			I915_SHRINK_PURGEABLE);
+ 	ret = drm_gem_create_mmap_offset(&obj->base);
+ 	if (ret != -ENOSPC)
+ 		goto out;
+@@ -1871,8 +1990,6 @@
+ 	struct sg_page_iter sg_iter;
+ 	int ret;
+ 
+-	BUG_ON(obj->madv == __I915_MADV_PURGED);
+-
+ 	ret = i915_gem_object_set_to_cpu_domain(obj, true);
+ 	if (ret) {
+ 		/* In the event of a disaster, abandon all caches and
+@@ -1918,6 +2035,7 @@
+ 		return -EBUSY;
+ 
+ 	BUG_ON(i915_gem_obj_bound_any(obj));
++	BUG_ON(obj->madv == __I915_MADV_PURGED);
+ 
+ 	/* ->put_pages might need to allocate memory for the bit17 swizzle
+ 	 * array, hence protect them from being reaped by removing them from gtt
+@@ -1932,12 +2050,18 @@
+ 	return 0;
+ }
+ 
+-static unsigned long
+-__i915_gem_shrink(struct drm_i915_private *dev_priv, long target,
+-		  bool purgeable_only)
+-{
+-	struct list_head still_in_list;
+-	struct drm_i915_gem_object *obj;
++unsigned long
++i915_gem_shrink(struct drm_i915_private *dev_priv,
++		long target, unsigned flags)
++{
++	const struct {
++		struct list_head *list;
++		unsigned int bit;
++	} phases[] = {
++		{ &dev_priv->mm.unbound_list, I915_SHRINK_UNBOUND },
++		{ &dev_priv->mm.bound_list, I915_SHRINK_BOUND },
++		{ NULL, 0 },
++	}, *phase;
+ 	unsigned long count = 0;
+ 
+ 	/*
+@@ -1959,62 +2083,56 @@
+ 	 * dev->struct_mutex and so we won't ever be able to observe an
+ 	 * object on the bound_list with a reference count equals 0.
+ 	 */
+-	INIT_LIST_HEAD(&still_in_list);
+-	while (count < target && !list_empty(&dev_priv->mm.unbound_list)) {
+-		obj = list_first_entry(&dev_priv->mm.unbound_list,
+-				       typeof(*obj), global_list);
+-		list_move_tail(&obj->global_list, &still_in_list);
+-
+-		if (!i915_gem_object_is_purgeable(obj) && purgeable_only)
+-			continue;
+-
+-		drm_gem_object_reference(&obj->base);
+-
+-		if (i915_gem_object_put_pages(obj) == 0)
+-			count += obj->base.size >> PAGE_SHIFT;
+-
+-		drm_gem_object_unreference(&obj->base);
+-	}
+-	list_splice(&still_in_list, &dev_priv->mm.unbound_list);
+-
+-	INIT_LIST_HEAD(&still_in_list);
+-	while (count < target && !list_empty(&dev_priv->mm.bound_list)) {
+-		struct i915_vma *vma, *v;
+-
+-		obj = list_first_entry(&dev_priv->mm.bound_list,
+-				       typeof(*obj), global_list);
+-		list_move_tail(&obj->global_list, &still_in_list);
++	for (phase = phases; phase->list; phase++) {
++		struct list_head still_in_list;
+ 
+-		if (!i915_gem_object_is_purgeable(obj) && purgeable_only)
++		if ((flags & phase->bit) == 0)
+ 			continue;
+ 
+-		drm_gem_object_reference(&obj->base);
+-
+-		list_for_each_entry_safe(vma, v, &obj->vma_list, vma_link)
+-			if (i915_vma_unbind(vma))
+-				break;
++		INIT_LIST_HEAD(&still_in_list);
++		while (count < target && !list_empty(phase->list)) {
++			struct list_head vma_list;
++			struct drm_i915_gem_object *obj;
++
++			obj = list_first_entry(phase->list,
++					       typeof(*obj), global_list);
++			list_move_tail(&obj->global_list, &still_in_list);
++
++			if (flags & I915_SHRINK_PURGEABLE &&
++			    !i915_gem_object_is_purgeable(obj))
++				continue;
++
++			drm_gem_object_reference(&obj->base);
++
++			/* For the unbound phase, this should be a no-op! */
++			INIT_LIST_HEAD(&vma_list);
++			while (!list_empty(&obj->vma_list)) {
++				struct i915_vma *vma;
++
++				vma = list_first_entry(&obj->vma_list,
++						       typeof(*vma), obj_link);
++				list_move_tail(&vma->obj_link, &vma_list);
++				if (i915_vma_unbind(vma))
++					break;
++			}
++			list_splice(&vma_list, &obj->vma_list);
+ 
+-		if (i915_gem_object_put_pages(obj) == 0)
+-			count += obj->base.size >> PAGE_SHIFT;
++			if (i915_gem_object_put_pages(obj) == 0)
++				count += obj->base.size >> PAGE_SHIFT;
+ 
+-		drm_gem_object_unreference(&obj->base);
++			drm_gem_object_unreference(&obj->base);
++		}
++		list_splice(&still_in_list, phase->list);
+ 	}
+-	list_splice(&still_in_list, &dev_priv->mm.bound_list);
+ 
+ 	return count;
+ }
+ 
+ static unsigned long
+-i915_gem_purge(struct drm_i915_private *dev_priv, long target)
+-{
+-	return __i915_gem_shrink(dev_priv, target, true);
+-}
+-
+-static unsigned long
+ i915_gem_shrink_all(struct drm_i915_private *dev_priv)
+ {
+-	i915_gem_evict_everything(dev_priv->dev);
+-	return __i915_gem_shrink(dev_priv, LONG_MAX, false);
++	return i915_gem_shrink(dev_priv, LONG_MAX,
++			       I915_SHRINK_BOUND | I915_SHRINK_UNBOUND);
+ }
+ 
+ static int
+@@ -2061,15 +2179,18 @@
+ 	for (i = 0; i < page_count; i++) {
+ 		page = shmem_read_mapping_page_gfp(mapping, i, gfp);
+ 		if (IS_ERR(page)) {
+-			i915_gem_purge(dev_priv, page_count);
++			i915_gem_shrink(dev_priv,
++					page_count,
++					I915_SHRINK_BOUND |
++					I915_SHRINK_UNBOUND |
++					I915_SHRINK_PURGEABLE);
+ 			page = shmem_read_mapping_page_gfp(mapping, i, gfp);
+ 		}
+ 		if (IS_ERR(page)) {
+ 			/* We've tried hard to allocate the memory by reaping
+-			 * our own buffer, now let the real VM do its job and
++			 * our own buffers, now let the real VM do its job and
+ 			 * go down in flames if truly OOM.
+ 			 */
+-			i915_gem_shrink_all(dev_priv);
+ 			page = shmem_read_mapping_page(mapping, i);
+ 			if (IS_ERR(page))
+ 				goto err_pages;
+@@ -2159,399 +2280,69 @@
+ 	return 0;
+ }
+ 
+-static void
+-i915_gem_object_move_to_active(struct drm_i915_gem_object *obj,
+-			       struct intel_engine_cs *ring)
+-{
+-	struct drm_device *dev = obj->base.dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	u32 seqno = intel_ring_get_seqno(ring);
+-
+-	BUG_ON(ring == NULL);
+-	if (obj->ring != ring && obj->last_write_seqno) {
+-		/* Keep the seqno relative to the current ring */
+-		obj->last_write_seqno = seqno;
+-	}
+-	obj->ring = ring;
+-
+-	/* Add a reference if we're newly entering the active list. */
+-	if (!obj->active) {
+-		drm_gem_object_reference(&obj->base);
+-		obj->active = 1;
+-	}
+-
+-	list_move_tail(&obj->ring_list, &ring->active_list);
+-
+-	obj->last_read_seqno = seqno;
+-
+-	if (obj->fenced_gpu_access) {
+-		obj->last_fenced_seqno = seqno;
+-
+-		/* Bump MRU to take account of the delayed flush */
+-		if (obj->fence_reg != I915_FENCE_REG_NONE) {
+-			struct drm_i915_fence_reg *reg;
+-
+-			reg = &dev_priv->fence_regs[obj->fence_reg];
+-			list_move_tail(&reg->lru_list,
+-				       &dev_priv->mm.fence_list);
+-		}
+-	}
+-}
+-
+-void i915_vma_move_to_active(struct i915_vma *vma,
+-			     struct intel_engine_cs *ring)
+-{
+-	list_move_tail(&vma->mm_list, &vma->vm->active_list);
+-	return i915_gem_object_move_to_active(vma->obj, ring);
+-}
+-
+-static void
+-i915_gem_object_move_to_inactive(struct drm_i915_gem_object *obj)
+-{
+-	struct drm_i915_private *dev_priv = obj->base.dev->dev_private;
+-	struct i915_address_space *vm;
+-	struct i915_vma *vma;
+-
+-	BUG_ON(obj->base.write_domain & ~I915_GEM_GPU_DOMAINS);
+-	BUG_ON(!obj->active);
+-
+-	list_for_each_entry(vm, &dev_priv->vm_list, global_link) {
+-		vma = i915_gem_obj_to_vma(obj, vm);
+-		if (vma && !list_empty(&vma->mm_list))
+-			list_move_tail(&vma->mm_list, &vm->inactive_list);
+-	}
+-
+-	intel_fb_obj_flush(obj, true);
+-
+-	list_del_init(&obj->ring_list);
+-	obj->ring = NULL;
+-
+-	obj->last_read_seqno = 0;
+-	obj->last_write_seqno = 0;
+-	obj->base.write_domain = 0;
+-
+-	obj->last_fenced_seqno = 0;
+-	obj->fenced_gpu_access = false;
+-
+-	obj->active = 0;
+-	drm_gem_object_unreference(&obj->base);
+-
+-	WARN_ON(i915_verify_lists(dev));
+-}
+-
+-static void
+-i915_gem_object_retire(struct drm_i915_gem_object *obj)
+-{
+-	struct intel_engine_cs *ring = obj->ring;
+-
+-	if (ring == NULL)
+-		return;
+-
+-	if (i915_seqno_passed(ring->get_seqno(ring, true),
+-			      obj->last_read_seqno))
+-		i915_gem_object_move_to_inactive(obj);
+-}
+-
+-static int
+-i915_gem_init_seqno(struct drm_device *dev, u32 seqno)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring;
+-	int ret, i, j;
+-
+-	/* Carefully retire all requests without writing to the rings */
+-	for_each_ring(ring, dev_priv, i) {
+-		ret = intel_ring_idle(ring);
+-		if (ret)
+-			return ret;
+-	}
+-	i915_gem_retire_requests(dev);
+-
+-	/* Finally reset hw state */
+-	for_each_ring(ring, dev_priv, i) {
+-		intel_ring_init_seqno(ring, seqno);
+-
+-		for (j = 0; j < ARRAY_SIZE(ring->semaphore.sync_seqno); j++)
+-			ring->semaphore.sync_seqno[j] = 0;
+-	}
+-
+-	return 0;
+-}
+-
+ int i915_gem_set_seqno(struct drm_device *dev, u32 seqno)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	int ret;
++	struct intel_engine_cs *signaller, *waiter;
++	int ret, i, j;
+ 
+ 	if (seqno == 0)
+ 		return -EINVAL;
+ 
+-	/* HWS page needs to be set less than what we
+-	 * will inject to ring
+-	 */
+-	ret = i915_gem_init_seqno(dev, seqno - 1);
+-	if (ret)
+-		return ret;
+-
+-	/* Carefully set the last_seqno value so that wrap
+-	 * detection still works
+-	 */
+-	dev_priv->next_seqno = seqno;
+-	dev_priv->last_seqno = seqno - 1;
+-	if (dev_priv->last_seqno == 0)
+-		dev_priv->last_seqno--;
+-
+-	return 0;
+-}
+-
+-int
+-i915_gem_get_seqno(struct drm_device *dev, u32 *seqno)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	if (seqno == dev_priv->next_seqno)
++		return 0;
+ 
+-	/* reserve 0 for non-seqno */
+-	if (dev_priv->next_seqno == 0) {
+-		int ret = i915_gem_init_seqno(dev, 0);
++	do {
++		/* Flush the breadcrumbs */
++		ret = i915_gpu_idle(dev);
+ 		if (ret)
+ 			return ret;
+ 
+-		dev_priv->next_seqno = 1;
+-	}
+-
+-	*seqno = dev_priv->last_seqno = dev_priv->next_seqno++;
+-	return 0;
+-}
+-
+-int __i915_add_request(struct intel_engine_cs *ring,
+-		       struct drm_file *file,
+-		       struct drm_i915_gem_object *obj,
+-		       u32 *out_seqno)
+-{
+-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
+-	struct drm_i915_gem_request *request;
+-	u32 request_ring_position, request_start;
+-	int ret;
+-
+-	request_start = intel_ring_get_tail(ring->buffer);
+-	/*
+-	 * Emit any outstanding flushes - execbuf can fail to emit the flush
+-	 * after having emitted the batchbuffer command. Hence we need to fix
+-	 * things up similar to emitting the lazy request. The difference here
+-	 * is that the flush _must_ happen before the next request, no matter
+-	 * what.
+-	 */
+-	ret = intel_ring_flush_all_caches(ring);
+-	if (ret)
+-		return ret;
+-
+-	request = ring->preallocated_lazy_request;
+-	if (WARN_ON(request == NULL))
+-		return -ENOMEM;
+-
+-	/* Record the position of the start of the request so that
+-	 * should we detect the updated seqno part-way through the
+-	 * GPU processing the request, we never over-estimate the
+-	 * position of the head.
+-	 */
+-	request_ring_position = intel_ring_get_tail(ring->buffer);
+-
+-	ret = ring->add_request(ring);
+-	if (ret)
+-		return ret;
+-
+-	request->seqno = intel_ring_get_seqno(ring);
+-	request->ring = ring;
+-	request->head = request_start;
+-	request->tail = request_ring_position;
+-
+-	/* Whilst this request exists, batch_obj will be on the
+-	 * active_list, and so will hold the active reference. Only when this
+-	 * request is retired will the the batch_obj be moved onto the
+-	 * inactive_list and lose its active reference. Hence we do not need
+-	 * to explicitly hold another reference here.
+-	 */
+-	request->batch_obj = obj;
+-
+-	/* Hold a reference to the current context so that we can inspect
+-	 * it later in case a hangcheck error event fires.
+-	 */
+-	request->ctx = ring->last_context;
+-	if (request->ctx)
+-		i915_gem_context_reference(request->ctx);
+-
+-	request->emitted_jiffies = jiffies;
+-	list_add_tail(&request->list, &ring->request_list);
+-	request->file_priv = NULL;
+-
+-	if (file) {
+-		struct drm_i915_file_private *file_priv = file->driver_priv;
+-
+-		spin_lock(&file_priv->mm.lock);
+-		request->file_priv = file_priv;
+-		list_add_tail(&request->client_list,
+-			      &file_priv->mm.request_list);
+-		spin_unlock(&file_priv->mm.lock);
+-	}
+-
+-	trace_i915_gem_request_add(ring, request->seqno);
+-	ring->outstanding_lazy_seqno = 0;
+-	ring->preallocated_lazy_request = NULL;
+-
+-	if (!dev_priv->ums.mm_suspended) {
+-		i915_queue_hangcheck(ring->dev);
+-
+-		cancel_delayed_work_sync(&dev_priv->mm.idle_work);
+-		queue_delayed_work(dev_priv->wq,
+-				   &dev_priv->mm.retire_work,
+-				   round_jiffies_up_relative(HZ));
+-		intel_mark_busy(dev_priv->dev);
+-	}
+-
+-	if (out_seqno)
+-		*out_seqno = request->seqno;
+-	return 0;
+-}
+-
+-static inline void
+-i915_gem_request_remove_from_client(struct drm_i915_gem_request *request)
+-{
+-	struct drm_i915_file_private *file_priv = request->file_priv;
+-
+-	if (!file_priv)
+-		return;
+-
+-	spin_lock(&file_priv->mm.lock);
+-	list_del(&request->client_list);
+-	request->file_priv = NULL;
+-	spin_unlock(&file_priv->mm.lock);
+-}
+-
+-static bool i915_context_is_banned(struct drm_i915_private *dev_priv,
+-				   const struct intel_context *ctx)
+-{
+-	unsigned long elapsed;
+-
+-	elapsed = get_seconds() - ctx->hang_stats.guilty_ts;
++		if (!i915_gem_retire_requests(dev))
++			return -EIO;
+ 
+-	if (ctx->hang_stats.banned)
+-		return true;
++		/* Update all semaphores to the current value */
++		for_each_engine(signaller, to_i915(dev), i) {
++			struct i915_gem_request *rq;
++
++			if (!signaller->semaphore.signal)
++				continue;
++
++			rq = i915_request_create(signaller->default_context,
++						 signaller);
++			if (IS_ERR(rq))
++				return PTR_ERR(rq);
++
++			for_each_engine(waiter, to_i915(dev), j) {
++				if (signaller == waiter)
++					continue;
++
++				if (!waiter->semaphore.wait)
++					continue;
++
++				ret = i915_request_emit_semaphore(rq, waiter->id);
++				if (ret)
++					break;
++			}
+ 
+-	if (elapsed <= DRM_I915_CTX_BAN_PERIOD) {
+-		if (!i915_gem_context_is_default(ctx)) {
+-			DRM_DEBUG("context hanging too fast, banning!\n");
+-			return true;
+-		} else if (i915_stop_ring_allow_ban(dev_priv)) {
+-			if (i915_stop_ring_allow_warn(dev_priv))
+-				DRM_ERROR("gpu hanging too fast, banning!\n");
+-			return true;
++			if (ret == 0)
++				ret = i915_request_commit(rq);
++			i915_request_put(rq);
++			if (ret)
++				return ret;
+ 		}
+-	}
+-
+-	return false;
+-}
+ 
+-static void i915_set_reset_status(struct drm_i915_private *dev_priv,
+-				  struct intel_context *ctx,
+-				  const bool guilty)
+-{
+-	struct i915_ctx_hang_stats *hs;
+-
+-	if (WARN_ON(!ctx))
+-		return;
+-
+-	hs = &ctx->hang_stats;
+-
+-	if (guilty) {
+-		hs->banned = i915_context_is_banned(dev_priv, ctx);
+-		hs->batch_active++;
+-		hs->guilty_ts = get_seconds();
+-	} else {
+-		hs->batch_pending++;
+-	}
+-}
+-
+-static void i915_gem_free_request(struct drm_i915_gem_request *request)
+-{
+-	list_del(&request->list);
+-	i915_gem_request_remove_from_client(request);
+-
+-	if (request->ctx)
+-		i915_gem_context_unreference(request->ctx);
+-
+-	kfree(request);
+-}
+-
+-struct drm_i915_gem_request *
+-i915_gem_find_active_request(struct intel_engine_cs *ring)
+-{
+-	struct drm_i915_gem_request *request;
+-	u32 completed_seqno;
+-
+-	completed_seqno = ring->get_seqno(ring, false);
+-
+-	list_for_each_entry(request, &ring->request_list, list) {
+-		if (i915_seqno_passed(completed_seqno, request->seqno))
+-			continue;
+-
+-		return request;
+-	}
+-
+-	return NULL;
+-}
+-
+-static void i915_gem_reset_ring_status(struct drm_i915_private *dev_priv,
+-				       struct intel_engine_cs *ring)
+-{
+-	struct drm_i915_gem_request *request;
+-	bool ring_hung;
+-
+-	request = i915_gem_find_active_request(ring);
+-
+-	if (request == NULL)
+-		return;
+-
+-	ring_hung = ring->hangcheck.score >= HANGCHECK_SCORE_RING_HUNG;
+-
+-	i915_set_reset_status(dev_priv, request->ctx, ring_hung);
+-
+-	list_for_each_entry_continue(request, &ring->request_list, list)
+-		i915_set_reset_status(dev_priv, request->ctx, false);
+-}
+-
+-static void i915_gem_reset_ring_cleanup(struct drm_i915_private *dev_priv,
+-					struct intel_engine_cs *ring)
+-{
+-	while (!list_empty(&ring->active_list)) {
+-		struct drm_i915_gem_object *obj;
+-
+-		obj = list_first_entry(&ring->active_list,
+-				       struct drm_i915_gem_object,
+-				       ring_list);
++		/* We can only roll seqno forwards across a wraparound.
++		 * This ship is not for turning!
++		 */
++		if (!__i915_seqno_passed(dev_priv->next_seqno, seqno))
++			break;
+ 
+-		i915_gem_object_move_to_inactive(obj);
+-	}
++		dev_priv->next_seqno += 0x40000000;
++	}while (1);
+ 
+-	/*
+-	 * We must free the requests after all the corresponding objects have
+-	 * been moved off active lists. Which is the same order as the normal
+-	 * retire_requests function does. This is important if object hold
+-	 * implicit references on things like e.g. ppgtt address spaces through
+-	 * the request.
+-	 */
+-	while (!list_empty(&ring->request_list)) {
+-		struct drm_i915_gem_request *request;
+-
+-		request = list_first_entry(&ring->request_list,
+-					   struct drm_i915_gem_request,
+-					   list);
+-
+-		i915_gem_free_request(request);
+-	}
+-
+-	/* These may not have been flush before the reset, do so now */
+-	kfree(ring->preallocated_lazy_request);
+-	ring->preallocated_lazy_request = NULL;
+-	ring->outstanding_lazy_seqno = 0;
++	dev_priv->next_seqno = seqno;
++	return 0;
+ }
+ 
+ void i915_gem_restore_fences(struct drm_device *dev)
+@@ -2578,21 +2369,35 @@
+ void i915_gem_reset(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring;
++	struct intel_engine_cs *engine;
+ 	int i;
+ 
+-	/*
+-	 * Before we free the objects from the requests, we need to inspect
+-	 * them for finding the guilty party. As the requests only borrow
+-	 * their reference to the objects, the inspection must be done first.
+-	 */
+-	for_each_ring(ring, dev_priv, i)
+-		i915_gem_reset_ring_status(dev_priv, ring);
++	for_each_engine(engine, dev_priv, i) {
++		/* Clearing the read list will also clear the write
++		 * and fence lists, 3 birds with one stone.
++		 */
++		while (!list_empty(&engine->read_list)) {
++			struct drm_i915_gem_object *obj;
+ 
+-	for_each_ring(ring, dev_priv, i)
+-		i915_gem_reset_ring_cleanup(dev_priv, ring);
++			obj = list_first_entry(&engine->read_list,
++					       struct drm_i915_gem_object,
++					       last_read[i].engine_link);
+ 
+-	i915_gem_context_reset(dev);
++			i915_gem_object_retire__read(obj, engine);
++		}
++
++		while (!list_empty(&engine->vma_list)) {
++			struct i915_vma *vma;
++
++			vma = list_first_entry(&engine->vma_list,
++					       struct i915_vma,
++					       last_read[i].engine_link);
++
++			i915_vma_retire__read(vma, i);
++		}
++
++		intel_engine_reset(engine);
++	}
+ 
+ 	i915_gem_restore_fences(dev);
+ }
+@@ -2601,87 +2406,117 @@
+  * This function clears the request list as sequence numbers are passed.
+  */
+ void
+-i915_gem_retire_requests_ring(struct intel_engine_cs *ring)
++i915_gem_retire_requests__engine(struct intel_engine_cs *engine)
+ {
+-	uint32_t seqno;
++	if (engine->last_request == NULL)
++		return;
+ 
+-	if (list_empty(&ring->request_list))
++	if (!intel_engine_retire(engine, intel_engine_get_seqno(engine)))
+ 		return;
+ 
+-	WARN_ON(i915_verify_lists(ring->dev));
++	while (!list_empty(&engine->read_list)) {
++		struct drm_i915_gem_object *obj;
++
++		obj = list_first_entry(&engine->read_list,
++				       struct drm_i915_gem_object,
++				       last_read[engine->id].engine_link);
+ 
+-	seqno = ring->get_seqno(ring, true);
++		if (!obj->last_read[engine->id].request->completed)
++			break;
+ 
+-	/* Move any buffers on the active list that are no longer referenced
+-	 * by the ringbuffer to the flushing/inactive lists as appropriate,
+-	 * before we free the context associated with the requests.
+-	 */
+-	while (!list_empty(&ring->active_list)) {
++		i915_gem_object_retire__read(obj, engine);
++	}
++
++	while (!list_empty(&engine->fence_list)) {
+ 		struct drm_i915_gem_object *obj;
+ 
+-		obj = list_first_entry(&ring->active_list,
+-				      struct drm_i915_gem_object,
+-				      ring_list);
++		obj = list_first_entry(&engine->fence_list,
++				       struct drm_i915_gem_object,
++				       last_fence.engine_link);
+ 
+-		if (!i915_seqno_passed(seqno, obj->last_read_seqno))
++		if (!obj->last_fence.request->completed)
+ 			break;
+ 
+-		i915_gem_object_move_to_inactive(obj);
++		i915_gem_object_retire__fence(obj);
+ 	}
+ 
++	while (!list_empty(&engine->write_list)) {
++		struct drm_i915_gem_object *obj;
+ 
+-	while (!list_empty(&ring->request_list)) {
+-		struct drm_i915_gem_request *request;
+-
+-		request = list_first_entry(&ring->request_list,
+-					   struct drm_i915_gem_request,
+-					   list);
++		obj = list_first_entry(&engine->write_list,
++				       struct drm_i915_gem_object,
++				       last_write.engine_link);
+ 
+-		if (!i915_seqno_passed(seqno, request->seqno))
++		if (!obj->last_write.request->completed)
+ 			break;
+ 
+-		trace_i915_gem_request_retire(ring, request->seqno);
+-		/* We know the GPU must have read the request to have
+-		 * sent us the seqno + interrupt, so use the position
+-		 * of tail of the request to update the last known position
+-		 * of the GPU head.
+-		 */
+-		ring->buffer->last_retired_head = request->tail;
+-
+-		i915_gem_free_request(request);
++		i915_gem_object_retire__write(obj);
+ 	}
+ 
+-	if (unlikely(ring->trace_irq_seqno &&
+-		     i915_seqno_passed(seqno, ring->trace_irq_seqno))) {
+-		ring->irq_put(ring);
+-		ring->trace_irq_seqno = 0;
+-	}
++	while (!list_empty(&engine->vma_list)) {
++		struct i915_vma *vma;
++
++		vma = list_first_entry(&engine->vma_list,
++				       struct i915_vma,
++				       last_read[engine->id].engine_link);
++
++		if (!vma->last_read[engine->id].request->completed)
++			break;
+ 
+-	WARN_ON(i915_verify_lists(ring->dev));
++		i915_vma_retire__read(vma, engine->id);
++	}
+ }
+ 
+ bool
+ i915_gem_retire_requests(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring;
++	struct intel_engine_cs *engine;
+ 	bool idle = true;
+ 	int i;
+ 
+-	for_each_ring(ring, dev_priv, i) {
+-		i915_gem_retire_requests_ring(ring);
+-		idle &= list_empty(&ring->request_list);
++	for_each_engine(engine, dev_priv, i) {
++		i915_gem_retire_requests__engine(engine);
++		idle &= engine->last_request == NULL;
+ 	}
+ 
+ 	if (idle)
+ 		mod_delayed_work(dev_priv->wq,
+-				   &dev_priv->mm.idle_work,
+-				   msecs_to_jiffies(100));
++				 &dev_priv->mm.idle_work,
++				 msecs_to_jiffies(100));
+ 
+ 	return idle;
+ }
+ 
+ static void
++i915_gem_flush_requests(struct drm_i915_private *i915)
++{
++	struct intel_engine_cs *engine;
++	int i;
++
++	for_each_engine(engine, i915, i) {
++		struct i915_gem_request *rq;
++		int ret;
++
++		if (engine->last_request == NULL)
++			continue;
++
++		if (engine->last_request->breadcrumb[i])
++			continue;
++
++		rq = i915_request_create(engine->last_request->ctx,
++					 engine);
++		if (IS_ERR(rq))
++			continue;
++
++		if (i915_request_emit_breadcrumb(rq) == 0)
++			ret = i915_request_commit(rq);
++		i915_request_put(rq);
++		(void)ret;
++	}
++}
++
++static void
+ i915_gem_retire_work_handler(struct work_struct *work)
+ {
+ 	struct drm_i915_private *dev_priv =
+@@ -2692,11 +2527,13 @@
+ 	/* Come back later if the device is busy... */
+ 	idle = false;
+ 	if (mutex_trylock(&dev->struct_mutex)) {
++		i915_gem_flush_requests(dev_priv);
+ 		idle = i915_gem_retire_requests(dev);
+ 		mutex_unlock(&dev->struct_mutex);
+ 	}
+ 	if (!idle)
+-		queue_delayed_work(dev_priv->wq, &dev_priv->mm.retire_work,
++		queue_delayed_work(dev_priv->wq,
++				   &dev_priv->mm.retire_work,
+ 				   round_jiffies_up_relative(HZ));
+ }
+ 
+@@ -2717,16 +2554,21 @@
+ static int
+ i915_gem_object_flush_active(struct drm_i915_gem_object *obj)
+ {
+-	int ret;
++	int ret, n;
+ 
+-	if (obj->active) {
+-		ret = i915_gem_check_olr(obj->ring, obj->last_read_seqno);
++	if (!obj->active)
++		return 0;
++
++	for (n = 0; n < I915_NUM_ENGINES; n++) {
++		if (obj->last_read[n].request == NULL)
++			continue;
++
++		ret = i915_request_emit_breadcrumb(obj->last_read[n].request);
+ 		if (ret)
+ 			return ret;
+-
+-		i915_gem_retire_requests_ring(obj->ring);
+ 	}
+ 
++	i915_gem_object_retire(obj);
+ 	return 0;
+ }
+ 
+@@ -2755,13 +2597,13 @@
+ int
+ i915_gem_wait_ioctl(struct drm_device *dev, void *data, struct drm_file *file)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct drm_i915_gem_wait *args = data;
+ 	struct drm_i915_gem_object *obj;
+-	struct intel_engine_cs *ring = NULL;
+-	unsigned reset_counter;
+-	u32 seqno = 0;
+-	int ret = 0;
++	struct i915_gem_request *rq[I915_NUM_ENGINES] = {};
++	int i, n = 0, ret;
++
++	if (args->flags != 0)
++		return -EINVAL;
+ 
+ 	ret = i915_mutex_lock_interruptible(dev);
+ 	if (ret)
+@@ -2773,40 +2615,87 @@
+ 		return -ENOENT;
+ 	}
+ 
+-	/* Need to make sure the object gets inactive eventually. */
+-	ret = i915_gem_object_flush_active(obj);
+-	if (ret)
+-		goto out;
+-
+-	if (obj->active) {
+-		seqno = obj->last_read_seqno;
+-		ring = obj->ring;
+-	}
+-
+-	if (seqno == 0)
+-		 goto out;
+-
+-	/* Do this after OLR check to make sure we make forward progress polling
+-	 * on this IOCTL with a timeout <=0 (like busy ioctl)
++	/* Make sure we make forward progress polling with a timeout <=0
++	 * (like busy ioctl)
+ 	 */
+ 	if (args->timeout_ns <= 0) {
+-		ret = -ETIME;
++		ret = i915_gem_object_flush_active(obj);
++		if (ret == 0)
++			ret = -ETIME;
+ 		goto out;
+ 	}
+ 
+-	drm_gem_object_unreference(&obj->base);
+-	reset_counter = atomic_read(&dev_priv->gpu_error.reset_counter);
+-	mutex_unlock(&dev->struct_mutex);
++	for (i = 0; i < I915_NUM_ENGINES; i++) {
++		if (obj->last_read[i].request == NULL)
++			continue;
+ 
+-	return __wait_seqno(ring, seqno, reset_counter, true, &args->timeout_ns,
+-			    file->driver_priv);
++		rq[n] = i915_request_get_breadcrumb(obj->last_read[i].request);
++		if (IS_ERR(rq[n])) {
++			ret = PTR_ERR(rq[n]);
++			break;
++		}
++		n++;
++	}
+ 
+ out:
+ 	drm_gem_object_unreference(&obj->base);
+ 	mutex_unlock(&dev->struct_mutex);
++
++	for (i = 0; i < n; i++) {
++		if (ret == 0)
++			ret = __i915_request_wait(rq[i], true,
++						  &args->timeout_ns,
++						  file->driver_priv);
++
++		i915_request_put__unlocked(rq[i]);
++	}
++
+ 	return ret;
+ }
+ 
++static int
++__i915_request_sync(struct i915_gem_request *waiter,
++		    struct i915_gem_request *signaller,
++		    bool *retire)
++{
++	int ret;
++
++	if (signaller == NULL || i915_request_complete(signaller))
++		return 0;
++
++	if (waiter == NULL)
++		goto wait;
++
++	/* XXX still true with execlists? */
++	if (waiter->engine == signaller->engine)
++		return 0;
++
++	if (!waiter->engine->semaphore.wait)
++		goto wait;
++
++	/* Try to emit only one wait per request per ring */
++	if (waiter->semaphore[signaller->engine->id] &&
++	    __i915_seqno_passed(waiter->semaphore[signaller->engine->id],
++				signaller->seqno))
++		return 0;
++
++	ret = i915_request_emit_semaphore(signaller, waiter->engine->id);
++	if (ret)
++		goto wait;
++
++	trace_i915_gem_ring_wait(signaller, waiter);
++	if (waiter->engine->semaphore.wait(waiter, signaller))
++		goto wait;
++
++	waiter->pending_flush &= ~I915_COMMAND_BARRIER;
++	waiter->semaphore[signaller->engine->id] = signaller->breadcrumb[waiter->engine->id];
++	return 0;
++
++wait:
++	*retire = true;
++	return i915_request_wait(signaller);
++}
++
+ /**
+  * i915_gem_object_sync - sync an object to a ring.
+  *
+@@ -2821,43 +2710,28 @@
+  */
+ int
+ i915_gem_object_sync(struct drm_i915_gem_object *obj,
+-		     struct intel_engine_cs *to)
++		     struct i915_gem_request *rq)
+ {
+-	struct intel_engine_cs *from = obj->ring;
+-	u32 seqno;
+-	int ret, idx;
++	int ret = 0, i;
++	bool retire = false;
+ 
+-	if (from == NULL || to == from)
+-		return 0;
+-
+-	if (to == NULL || !i915_semaphore_is_enabled(obj->base.dev))
+-		return i915_gem_object_wait_rendering(obj, false);
+-
+-	idx = intel_ring_sync_index(from, to);
+-
+-	seqno = obj->last_read_seqno;
+-	/* Optimization: Avoid semaphore sync when we are sure we already
+-	 * waited for an object with higher seqno */
+-	if (seqno <= from->semaphore.sync_seqno[idx])
+-		return 0;
+-
+-	ret = i915_gem_check_olr(obj->ring, seqno);
+-	if (ret)
+-		return ret;
++	if (obj->base.pending_write_domain == 0) {
++		ret = __i915_request_sync(rq, obj->last_write.request, &retire);
++	} else {
++		for (i = 0; i < I915_NUM_ENGINES; i++) {
++			ret = __i915_request_sync(rq, obj->last_read[i].request, &retire);
++			if (ret)
++				break;
++		}
++	}
+ 
+-	trace_i915_gem_ring_sync_to(from, to, seqno);
+-	ret = to->semaphore.sync_to(to, from, seqno);
+-	if (!ret)
+-		/* We use last_read_seqno because sync_to()
+-		 * might have just caused seqno wrap under
+-		 * the radar.
+-		 */
+-		from->semaphore.sync_seqno[idx] = obj->last_read_seqno;
++	if (retire && obj->active)
++		i915_gem_object_retire(obj);
+ 
+ 	return ret;
+ }
+ 
+-static void i915_gem_object_finish_gtt(struct drm_i915_gem_object *obj)
++void i915_gem_object_finish_gtt(struct drm_i915_gem_object *obj)
+ {
+ 	u32 old_write_domain, old_read_domains;
+ 
+@@ -2881,17 +2755,44 @@
+ 					    old_write_domain);
+ }
+ 
++static __must_check int
++i915_vma_wait_rendering(struct i915_vma *vma)
++{
++	int i, ret;
++
++	if (!vma->active)
++		return 0;
++
++	for (i = 0; i < I915_NUM_ENGINES; i++) {
++		struct i915_gem_request *rq;
++
++		rq = i915_request_get(vma->last_read[i].request);
++		if (rq == NULL)
++			continue;
++
++		/* Waiting on a request, may require emitting a new
++		 * request for this vma - e.g. a context object, or
++		 * it may invoke the shrinker and also complete the
++		 * request.
++		 */
++		ret = i915_request_wait(rq);
++		if (ret == 0 && rq == vma->last_read[i].request)
++			i915_vma_retire__read(vma, i);
++		i915_request_put(rq);
++		if (ret)
++			return ret;
++	}
++
++	return 0;
++}
++
+ int i915_vma_unbind(struct i915_vma *vma)
+ {
+ 	struct drm_i915_gem_object *obj = vma->obj;
+-	struct drm_i915_private *dev_priv = obj->base.dev->dev_private;
+ 	int ret;
+ 
+-	if (list_empty(&vma->vma_link))
+-		return 0;
+-
+ 	if (!drm_mm_node_allocated(&vma->node)) {
+-		i915_gem_vma_destroy(vma);
++		RQ_BUG_ON(!list_empty(&vma->mm_list));
+ 		return 0;
+ 	}
+ 
+@@ -2899,41 +2800,45 @@
+ 		return -EBUSY;
+ 
+ 	BUG_ON(obj->pages == NULL);
++	BUG_ON(obj->madv == __I915_MADV_PURGED);
+ 
+-	ret = i915_gem_object_finish_gpu(obj);
+-	if (ret)
+-		return ret;
+-	/* Continue on if we fail due to EIO, the GPU is hung so we
+-	 * should be safe and we need to cleanup or else we might
+-	 * cause memory corruption through use-after-free.
+-	 */
++	i915_vma_get(vma);
+ 
+-	if (i915_is_ggtt(vma->vm)) {
+-		i915_gem_object_finish_gtt(obj);
++	ret = i915_vma_wait_rendering(vma);
++	if (ret)
++		goto out;
+ 
+-		/* release the fence reg _after_ flushing */
+-		ret = i915_gem_object_put_fence(obj);
+-		if (ret)
+-			return ret;
++	/* Double check that waiting on the rendering did not inadvertently
++	 * pin our vma (for example due to a context switch).
++	 */
++	if (vma->pin_count) {
++		ret = -EBUSY;
++		goto out;
+ 	}
+ 
+-	trace_i915_vma_unbind(vma);
++	/* Whilst waiting we may have invoked the shrinker who gazzumped us
++	 * and freed the vma allocation already.
++	 */
++	if (!drm_mm_node_allocated(&vma->node))
++		goto out;
+ 
+-	vma->unbind_vma(vma);
++	trace_i915_vma_unbind(vma);
++	ret = vma->unbind_vma(vma);
++	if (ret)
++		goto out;
++	RQ_BUG_ON(vma->bound);
+ 
+ 	list_del_init(&vma->mm_list);
+-	/* Avoid an unnecessary call to unbind on rebind. */
+-	if (i915_is_ggtt(vma->vm))
+-		obj->map_and_fenceable = true;
+-
+ 	drm_mm_remove_node(&vma->node);
+-	i915_gem_vma_destroy(vma);
++	RQ_BUG_ON(drm_mm_node_allocated(&vma->node));
+ 
+-	/* Since the unbound list is global, only move to that list if
+-	 * no more VMAs exist. */
+-	if (list_empty(&obj->vma_list)) {
++	i915_vma_put(vma);
++
++	/* The unbound list is global, check if we were the last bound */
++	if (!i915_gem_obj_bound_any(obj)) {
+ 		i915_gem_gtt_finish_object(obj);
+-		list_move_tail(&obj->global_list, &dev_priv->mm.unbound_list);
++		list_move_tail(&obj->global_list,
++			       &to_i915(obj->base.dev)->mm.unbound_list);
+ 	}
+ 
+ 	/* And finally now the object is completely decoupled from this vma,
+@@ -2942,22 +2847,34 @@
+ 	 */
+ 	i915_gem_object_unpin_pages(obj);
+ 
+-	return 0;
++out:
++	i915_vma_put(vma);
++	return ret;
+ }
+ 
+ int i915_gpu_idle(struct drm_device *dev)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring;
+-	int ret, i;
++	struct intel_engine_cs *engine;
++	int i;
+ 
+-	/* Flush everything onto the inactive list. */
+-	for_each_ring(ring, dev_priv, i) {
+-		ret = i915_switch_context(ring, ring->default_context);
+-		if (ret)
+-			return ret;
++	/* Flush everything including contexts onto the inactive list. */
++	for_each_engine(engine, to_i915(dev), i) {
++		struct i915_gem_request *rq;
++		int ret;
++
++		/* Only emit a wait if busy or we need to force a ctx switch */
++		rq = engine->last_request;
++		if (rq == NULL || rq->ctx != engine->default_context) {
++			rq = i915_request_create(engine->default_context,
++						 engine);
++			if (IS_ERR(rq))
++				return PTR_ERR(rq);
++		} else
++			rq = i915_request_get(rq);
++
++		ret = i915_request_wait(rq);
++		i915_request_put(rq);
+ 
+-		ret = intel_ring_idle(ring);
+ 		if (ret)
+ 			return ret;
+ 	}
+@@ -3114,6 +3031,7 @@
+ 	     obj->stride, obj->tiling_mode);
+ 
+ 	switch (INTEL_INFO(dev)->gen) {
++	case 9:
+ 	case 8:
+ 	case 7:
+ 	case 6:
+@@ -3161,15 +3079,16 @@
+ static int
+ i915_gem_object_wait_fence(struct drm_i915_gem_object *obj)
+ {
+-	if (obj->last_fenced_seqno) {
+-		int ret = i915_wait_seqno(obj->ring, obj->last_fenced_seqno);
+-		if (ret)
+-			return ret;
++	int ret;
+ 
+-		obj->last_fenced_seqno = 0;
+-	}
++	if (obj->last_fence.request == NULL)
++		return 0;
++
++	ret = i915_request_wait(obj->last_fence.request);
++	if (ret)
++		return ret;
+ 
+-	obj->fenced_gpu_access = false;
++	i915_gem_object_retire__fence(obj);
+ 	return 0;
+ }
+ 
+@@ -3276,6 +3195,9 @@
+ 			return 0;
+ 		}
+ 	} else if (enable) {
++		if (WARN_ON(!obj->map_and_fenceable))
++			return -EINVAL;
++
+ 		reg = i915_find_fence_reg(dev);
+ 		if (IS_ERR(reg))
+ 			return PTR_ERR(reg);
+@@ -3297,17 +3219,20 @@
+ 	return 0;
+ }
+ 
+-static bool i915_gem_valid_gtt_space(struct drm_device *dev,
+-				     struct drm_mm_node *gtt_space,
++static bool i915_gem_valid_gtt_space(struct i915_vma *vma,
+ 				     unsigned long cache_level)
+ {
++	struct drm_mm_node *gtt_space = &vma->node;
+ 	struct drm_mm_node *other;
+ 
+-	/* On non-LLC machines we have to be careful when putting differing
+-	 * types of snoopable memory together to avoid the prefetcher
+-	 * crossing memory domains and dying.
++	/*
++	 * On some machines we have to be careful when putting differing types
++	 * of snoopable memory together to avoid the prefetcher crossing memory
++	 * domains and dying. During vm initialisation, we decide whether or not
++	 * these constraints apply and set the drm_mm.color_adjust
++	 * appropriately.
+ 	 */
+-	if (HAS_LLC(dev))
++	if (vma->vm->mm.color_adjust == NULL)
+ 		return true;
+ 
+ 	if (!drm_mm_node_allocated(gtt_space))
+@@ -3327,63 +3252,20 @@
+ 	return true;
+ }
+ 
+-static void i915_gem_verify_gtt(struct drm_device *dev)
+-{
+-#if WATCH_GTT
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct drm_i915_gem_object *obj;
+-	int err = 0;
+-
+-	list_for_each_entry(obj, &dev_priv->mm.gtt_list, global_list) {
+-		if (obj->gtt_space == NULL) {
+-			printk(KERN_ERR "object found on GTT list with no space reserved\n");
+-			err++;
+-			continue;
+-		}
+-
+-		if (obj->cache_level != obj->gtt_space->color) {
+-			printk(KERN_ERR "object reserved space [%08lx, %08lx] with wrong color, cache_level=%x, color=%lx\n",
+-			       i915_gem_obj_ggtt_offset(obj),
+-			       i915_gem_obj_ggtt_offset(obj) + i915_gem_obj_ggtt_size(obj),
+-			       obj->cache_level,
+-			       obj->gtt_space->color);
+-			err++;
+-			continue;
+-		}
+-
+-		if (!i915_gem_valid_gtt_space(dev,
+-					      obj->gtt_space,
+-					      obj->cache_level)) {
+-			printk(KERN_ERR "invalid GTT space found at [%08lx, %08lx] - color=%x\n",
+-			       i915_gem_obj_ggtt_offset(obj),
+-			       i915_gem_obj_ggtt_offset(obj) + i915_gem_obj_ggtt_size(obj),
+-			       obj->cache_level);
+-			err++;
+-			continue;
+-		}
+-	}
+-
+-	WARN_ON(err);
+-#endif
+-}
+-
+ /**
+  * Finds free space in the GTT aperture and binds the object there.
+  */
+-static struct i915_vma *
+-i915_gem_object_bind_to_vm(struct drm_i915_gem_object *obj,
+-			   struct i915_address_space *vm,
+-			   unsigned alignment,
+-			   uint64_t flags)
++static int
++i915_vma_reserve(struct i915_vma *vma, unsigned alignment, uint64_t flags)
+ {
++	struct drm_i915_gem_object *obj = vma->obj;
+ 	struct drm_device *dev = obj->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	u32 size, fence_size, fence_alignment, unfenced_alignment;
+ 	unsigned long start =
+ 		flags & PIN_OFFSET_BIAS ? flags & PIN_OFFSET_MASK : 0;
+ 	unsigned long end =
+-		flags & PIN_MAPPABLE ? dev_priv->gtt.mappable_end : vm->total;
+-	struct i915_vma *vma;
++		flags & PIN_MAPPABLE ? dev_priv->gtt.mappable_end : vma->vm->total;
+ 	int ret;
+ 
+ 	fence_size = i915_gem_get_gtt_size(dev,
+@@ -3402,7 +3284,7 @@
+ 						unfenced_alignment;
+ 	if (flags & PIN_MAPPABLE && alignment & (fence_alignment - 1)) {
+ 		DRM_DEBUG("Invalid object alignment requested %u\n", alignment);
+-		return ERR_PTR(-EINVAL);
++		return -EINVAL;
+ 	}
+ 
+ 	size = flags & PIN_MAPPABLE ? fence_size : obj->base.size;
+@@ -3415,38 +3297,64 @@
+ 			  obj->base.size,
+ 			  flags & PIN_MAPPABLE ? "mappable" : "total",
+ 			  end);
+-		return ERR_PTR(-E2BIG);
++		return -E2BIG;
+ 	}
+ 
+ 	ret = i915_gem_object_get_pages(obj);
+ 	if (ret)
+-		return ERR_PTR(ret);
++		return ret;
+ 
+ 	i915_gem_object_pin_pages(obj);
+ 
+-	vma = i915_gem_obj_lookup_or_create_vma(obj, vm);
+-	if (IS_ERR(vma))
+-		goto err_unpin;
+-
++	if (flags & PIN_OFFSET_FIXED) {
++		uint64_t offset = flags & PIN_OFFSET_MASK;
++		if (alignment && offset & (alignment - 1)) {
++			ret = -EINVAL;
++			goto err_unpin;
++		}
++		vma->node.start = offset;
++		vma->node.size = size;
++		vma->node.color = obj->cache_level;
++		ret = drm_mm_reserve_node(&vma->vm->mm, &vma->node);
++		if (ret) {
++			ret = i915_gem_evict_range(dev, vma->vm, start, end);
++			if (ret == 0)
++				ret = drm_mm_reserve_node(&vma->vm->mm, &vma->node);
++		}
++		if (ret)
++			goto err_unpin;
++	} else {
++		unsigned search = DRM_MM_SEARCH_DEFAULT;
++		unsigned create = DRM_MM_CREATE_DEFAULT;
++		if (i915_is_ggtt(vma->vm) &&
++		    (flags & PIN_MAPPABLE) == 0 &&
++		    obj->cache_level != I915_CACHE_NONE) {
++			search = DRM_MM_SEARCH_BELOW;
++			create = DRM_MM_CREATE_TOP;
++		}
+ search_free:
+-	ret = drm_mm_insert_node_in_range_generic(&vm->mm, &vma->node,
+-						  size, alignment,
+-						  obj->cache_level,
+-						  start, end,
+-						  DRM_MM_SEARCH_DEFAULT,
+-						  DRM_MM_CREATE_DEFAULT);
+-	if (ret) {
+-		ret = i915_gem_evict_something(dev, vm, size, alignment,
+-					       obj->cache_level,
+-					       start, end,
+-					       flags);
+-		if (ret == 0)
+-			goto search_free;
++		ret = drm_mm_insert_node_in_range_generic(&vma->vm->mm,
++							  &vma->node,
++							  size, alignment,
++							  obj->cache_level,
++							  start, end,
++							  search, create);
++		if (ret) {
++			ret = i915_gem_evict_something(dev, vma->vm,
++						       size, alignment,
++						       obj->cache_level,
++						       start, end,
++						       flags);
++			if (ret == 0)
++				goto search_free;
+ 
+-		goto err_free_vma;
++			goto err_unpin;
++		}
+ 	}
+-	if (WARN_ON(!i915_gem_valid_gtt_space(dev, &vma->node,
+-					      obj->cache_level))) {
++	i915_vma_get(vma);
++	vma->pin_count++;
++
++	if (WARN_ON(!i915_gem_valid_gtt_space(vma, obj->cache_level))) {
+ 		ret = -EINVAL;
+ 		goto err_remove_node;
+ 	}
+@@ -3455,38 +3363,20 @@
+ 	if (ret)
+ 		goto err_remove_node;
+ 
++	RQ_BUG_ON(!list_empty(&vma->mm_list));
+ 	list_move_tail(&obj->global_list, &dev_priv->mm.bound_list);
+-	list_add_tail(&vma->mm_list, &vm->inactive_list);
++	list_add_tail(&vma->mm_list, &vma->vm->inactive_list);
+ 
+-	if (i915_is_ggtt(vm)) {
+-		bool mappable, fenceable;
+-
+-		fenceable = (vma->node.size == fence_size &&
+-			     (vma->node.start & (fence_alignment - 1)) == 0);
+-
+-		mappable = (vma->node.start + obj->base.size <=
+-			    dev_priv->gtt.mappable_end);
+-
+-		obj->map_and_fenceable = mappable && fenceable;
+-	}
+-
+-	WARN_ON(flags & PIN_MAPPABLE && !obj->map_and_fenceable);
+-
+-	trace_i915_vma_bind(vma, flags);
+-	vma->bind_vma(vma, obj->cache_level,
+-		      flags & (PIN_MAPPABLE | PIN_GLOBAL) ? GLOBAL_BIND : 0);
+-
+-	i915_gem_verify_gtt(dev);
+-	return vma;
++	return 0;
+ 
+ err_remove_node:
++	vma->pin_count--;
++	RQ_BUG_ON(vma->pin_count != 0);
++	i915_vma_put(vma);
+ 	drm_mm_remove_node(&vma->node);
+-err_free_vma:
+-	i915_gem_vma_destroy(vma);
+-	vma = ERR_PTR(ret);
+ err_unpin:
+ 	i915_gem_object_unpin_pages(obj);
+-	return vma;
++	return ret;
+ }
+ 
+ bool
+@@ -3504,7 +3394,7 @@
+ 	 * Stolen memory is always coherent with the GPU as it is explicitly
+ 	 * marked as wc by the system, or the system is cache-coherent.
+ 	 */
+-	if (obj->stolen)
++	if (obj->stolen || obj->phys_handle)
+ 		return false;
+ 
+ 	/* If the GPU is snooping the contents of the CPU cache,
+@@ -3585,14 +3475,10 @@
+ int
+ i915_gem_object_set_to_gtt_domain(struct drm_i915_gem_object *obj, bool write)
+ {
+-	struct drm_i915_private *dev_priv = obj->base.dev->dev_private;
+ 	uint32_t old_write_domain, old_read_domains;
++	struct i915_vma *vma;
+ 	int ret;
+ 
+-	/* Not valid to be called on unbound objects. */
+-	if (!i915_gem_obj_bound_any(obj))
+-		return -EINVAL;
+-
+ 	if (obj->base.write_domain == I915_GEM_DOMAIN_GTT)
+ 		return 0;
+ 
+@@ -3600,7 +3486,18 @@
+ 	if (ret)
+ 		return ret;
+ 
+-	i915_gem_object_retire(obj);
++	/* Flush and acquire obj->pages so that we are coherent through
++	 * direct access in memory with previous cached writes through
++	 * shmemfs and that our cache domain tracking remains valid.
++	 * For example, if the obj->filp was moved to swap without us
++	 * being notified and releasing the pages, we would mistakenly
++	 * continue to assume that the obj remained out of the CPU cached
++	 * domain.
++	 */
++	ret = i915_gem_object_get_pages(obj);
++	if (ret)
++		return ret;
++
+ 	i915_gem_object_flush_cpu_write_domain(obj, false);
+ 
+ 	/* Serialise direct access to this object with the barriers for
+@@ -3619,25 +3516,22 @@
+ 	BUG_ON((obj->base.write_domain & ~I915_GEM_DOMAIN_GTT) != 0);
+ 	obj->base.read_domains |= I915_GEM_DOMAIN_GTT;
+ 	if (write) {
++		intel_fb_obj_invalidate(obj, NULL);
+ 		obj->base.read_domains = I915_GEM_DOMAIN_GTT;
+ 		obj->base.write_domain = I915_GEM_DOMAIN_GTT;
+ 		obj->dirty = 1;
+ 	}
+ 
+-	if (write)
+-		intel_fb_obj_invalidate(obj, NULL);
+-
+ 	trace_i915_gem_object_change_domain(obj,
+ 					    old_read_domains,
+ 					    old_write_domain);
+ 
+ 	/* And bump the LRU for this access */
+-	if (i915_gem_object_is_inactive(obj)) {
+-		struct i915_vma *vma = i915_gem_obj_to_ggtt(obj);
+-		if (vma)
+-			list_move_tail(&vma->mm_list,
+-				       &dev_priv->gtt.base.inactive_list);
+-
++	vma = i915_gem_obj_to_ggtt(obj);
++	if (vma && vma->bound & GLOBAL_BIND && !vma->active) {
++		RQ_BUG_ON(list_empty(&vma->mm_list));
++		list_move_tail(&vma->mm_list,
++			       &to_i915(obj->base.dev)->gtt.base.inactive_list);
+ 	}
+ 
+ 	return 0;
+@@ -3658,8 +3552,8 @@
+ 		return -EBUSY;
+ 	}
+ 
+-	list_for_each_entry_safe(vma, next, &obj->vma_list, vma_link) {
+-		if (!i915_gem_valid_gtt_space(dev, &vma->node, cache_level)) {
++	list_for_each_entry_safe(vma, next, &obj->vma_list, obj_link) {
++		if (!i915_gem_valid_gtt_space(vma, cache_level)) {
+ 			ret = i915_vma_unbind(vma);
+ 			if (ret)
+ 				return ret;
+@@ -3683,13 +3577,12 @@
+ 				return ret;
+ 		}
+ 
+-		list_for_each_entry(vma, &obj->vma_list, vma_link)
++		list_for_each_entry(vma, &obj->vma_list, obj_link)
+ 			if (drm_mm_node_allocated(&vma->node))
+-				vma->bind_vma(vma, cache_level,
+-					      obj->has_global_gtt_mapping ? GLOBAL_BIND : 0);
++				vma->bind_vma(vma, cache_level, REBIND);
+ 	}
+ 
+-	list_for_each_entry(vma, &obj->vma_list, vma_link)
++	list_for_each_entry(vma, &obj->vma_list, obj_link)
+ 		vma->node.color = cache_level;
+ 	obj->cache_level = cache_level;
+ 
+@@ -3702,7 +3595,6 @@
+ 		 * in obj->write_domain and have been skipping the clflushes.
+ 		 * Just set it to the CPU cache for now.
+ 		 */
+-		i915_gem_object_retire(obj);
+ 		WARN_ON(obj->base.write_domain & ~I915_GEM_DOMAIN_CPU);
+ 
+ 		old_read_domains = obj->base.read_domains;
+@@ -3716,7 +3608,6 @@
+ 						    old_write_domain);
+ 	}
+ 
+-	i915_gem_verify_gtt(dev);
+ 	return 0;
+ }
+ 
+@@ -3802,9 +3693,6 @@
+ {
+ 	struct i915_vma *vma;
+ 
+-	if (list_empty(&obj->vma_list))
+-		return false;
+-
+ 	vma = i915_gem_obj_to_ggtt(obj);
+ 	if (!vma)
+ 		return false;
+@@ -3831,17 +3719,15 @@
+ int
+ i915_gem_object_pin_to_display_plane(struct drm_i915_gem_object *obj,
+ 				     u32 alignment,
+-				     struct intel_engine_cs *pipelined)
++				     struct i915_gem_request *pipelined)
+ {
+ 	u32 old_read_domains, old_write_domain;
+ 	bool was_pin_display;
+ 	int ret;
+ 
+-	if (pipelined != obj->ring) {
+-		ret = i915_gem_object_sync(obj, pipelined);
+-		if (ret)
+-			return ret;
+-	}
++	ret = i915_gem_object_sync(obj, pipelined);
++	if (ret)
++		return ret;
+ 
+ 	/* Mark the pin_display early so that we account for the
+ 	 * display coherency whilst setting up the cache domains.
+@@ -3867,7 +3753,7 @@
+ 	 * (e.g. libkms for the bootup splash), we have to ensure that we
+ 	 * always use map_and_fenceable for all scanout buffers.
+ 	 */
+-	ret = i915_gem_obj_ggtt_pin(obj, alignment, PIN_MAPPABLE);
++	ret = i915_gem_object_ggtt_pin(obj, alignment, PIN_MAPPABLE);
+ 	if (ret)
+ 		goto err_unpin_display;
+ 
+@@ -3937,7 +3823,6 @@
+ 	if (ret)
+ 		return ret;
+ 
+-	i915_gem_object_retire(obj);
+ 	i915_gem_object_flush_gtt_write_domain(obj);
+ 
+ 	old_write_domain = obj->base.write_domain;
+@@ -3959,13 +3844,11 @@
+ 	 * need to be invalidated at next use.
+ 	 */
+ 	if (write) {
++		intel_fb_obj_invalidate(obj, NULL);
+ 		obj->base.read_domains = I915_GEM_DOMAIN_CPU;
+ 		obj->base.write_domain = I915_GEM_DOMAIN_CPU;
+ 	}
+ 
+-	if (write)
+-		intel_fb_obj_invalidate(obj, NULL);
+-
+ 	trace_i915_gem_object_change_domain(obj,
+ 					    old_read_domains,
+ 					    old_write_domain);
+@@ -3973,64 +3856,15 @@
+ 	return 0;
+ }
+ 
+-/* Throttle our rendering by waiting until the ring has completed our requests
+- * emitted over 20 msec ago.
+- *
+- * Note that if we were to use the current jiffies each time around the loop,
+- * we wouldn't escape the function with any frames outstanding if the time to
+- * render a frame was over 20ms.
+- *
+- * This should get us reasonable parallelism between CPU and GPU but also
+- * relatively low latency when blocking on a particular request to finish.
+- */
+-static int
+-i915_gem_ring_throttle(struct drm_device *dev, struct drm_file *file)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct drm_i915_file_private *file_priv = file->driver_priv;
+-	unsigned long recent_enough = jiffies - msecs_to_jiffies(20);
+-	struct drm_i915_gem_request *request;
+-	struct intel_engine_cs *ring = NULL;
+-	unsigned reset_counter;
+-	u32 seqno = 0;
+-	int ret;
+-
+-	ret = i915_gem_wait_for_error(&dev_priv->gpu_error);
+-	if (ret)
+-		return ret;
+-
+-	ret = i915_gem_check_wedge(&dev_priv->gpu_error, false);
+-	if (ret)
+-		return ret;
+-
+-	spin_lock(&file_priv->mm.lock);
+-	list_for_each_entry(request, &file_priv->mm.request_list, client_list) {
+-		if (time_after_eq(request->emitted_jiffies, recent_enough))
+-			break;
+-
+-		ring = request->ring;
+-		seqno = request->seqno;
+-	}
+-	reset_counter = atomic_read(&dev_priv->gpu_error.reset_counter);
+-	spin_unlock(&file_priv->mm.lock);
+-
+-	if (seqno == 0)
+-		return 0;
+-
+-	ret = __wait_seqno(ring, seqno, reset_counter, true, NULL, NULL);
+-	if (ret == 0)
+-		queue_delayed_work(dev_priv->wq, &dev_priv->mm.retire_work, 0);
+-
+-	return ret;
+-}
+-
+ static bool
+ i915_vma_misplaced(struct i915_vma *vma, uint32_t alignment, uint64_t flags)
+ {
+ 	struct drm_i915_gem_object *obj = vma->obj;
+ 
+-	if (alignment &&
+-	    vma->node.start & (alignment - 1))
++	if (!drm_mm_node_allocated(&vma->node))
++		return false;
++
++	if (alignment && vma->node.start & (alignment - 1))
+ 		return true;
+ 
+ 	if (flags & PIN_MAPPABLE && !obj->map_and_fenceable)
+@@ -4040,60 +3874,86 @@
+ 	    vma->node.start < (flags & PIN_OFFSET_MASK))
+ 		return true;
+ 
++	if (flags & PIN_OFFSET_FIXED &&
++	    vma->node.start != (flags & PIN_OFFSET_MASK))
++		return true;
++
+ 	return false;
+ }
+ 
+ int
+-i915_gem_object_pin(struct drm_i915_gem_object *obj,
+-		    struct i915_address_space *vm,
+-		    uint32_t alignment,
+-		    uint64_t flags)
++i915_vma_pin(struct i915_vma *vma, uint32_t alignment, uint64_t flags)
++{
++	unsigned bind;
++	int ret;
++
++	if (WARN_ON(flags & (PIN_GLOBAL | PIN_MAPPABLE) &&
++		    !i915_is_ggtt(vma->vm)))
++		return -EINVAL;
++
++	if (WARN_ON((flags & (PIN_MAPPABLE | PIN_GLOBAL)) == PIN_MAPPABLE))
++		return -EINVAL;
++
++	if (WARN_ON(vma->obj->madv == __I915_MADV_PURGED))
++		return -EFAULT;
++
++	RQ_BUG_ON(vma->vm->closed);
++
++	if (i915_vma_misplaced(vma, alignment, flags)) {
++		WARN(vma->pin_count,
++		     "bo is already pinned with incorrect alignment:"
++		     " offset=%lx, req.alignment=%x, req.map_and_fenceable=%d,"
++		     " obj->map_and_fenceable=%d\n",
++		     vma->node.start, alignment,
++		     !!(flags & PIN_MAPPABLE),
++		     vma->obj->map_and_fenceable);
++		ret = i915_vma_unbind(vma);
++		if (ret)
++			return ret;
++	}
++
++	if (!drm_mm_node_allocated(&vma->node)) {
++		ret = i915_vma_reserve(vma, alignment, flags);
++		if (ret)
++			return ret;
++	} else
++		vma->pin_count++;
++
++	bind = 0;
++	if (flags & PIN_GLOBAL)
++		bind |= GLOBAL_BIND;
++	if (flags & PIN_LOCAL)
++		bind |= LOCAL_BIND;
++	ret = vma->bind_vma(vma, vma->obj->cache_level, bind);
++	if (ret) {
++		--vma->pin_count;
++		return ret;
++	}
++
++	if (flags & PIN_MAPPABLE) {
++		WARN_ON(!vma->obj->map_and_fenceable);
++		vma->obj->pin_mappable = true;
++	}
++
++	return 0;
++}
++
++int
++i915_gem_object_ggtt_pin(struct drm_i915_gem_object *obj,
++			 uint32_t alignment,
++			 unsigned flags)
+ {
+-	struct drm_i915_private *dev_priv = obj->base.dev->dev_private;
+ 	struct i915_vma *vma;
+ 	int ret;
+ 
+-	if (WARN_ON(vm == &dev_priv->mm.aliasing_ppgtt->base))
+-		return -ENODEV;
+-
+-	if (WARN_ON(flags & (PIN_GLOBAL | PIN_MAPPABLE) && !i915_is_ggtt(vm)))
+-		return -EINVAL;
+-
+-	vma = i915_gem_obj_to_vma(obj, vm);
+-	if (vma) {
+-		if (WARN_ON(vma->pin_count == DRM_I915_GEM_OBJECT_MAX_PIN_COUNT))
+-			return -EBUSY;
+-
+-		if (i915_vma_misplaced(vma, alignment, flags)) {
+-			WARN(vma->pin_count,
+-			     "bo is already pinned with incorrect alignment:"
+-			     " offset=%lx, req.alignment=%x, req.map_and_fenceable=%d,"
+-			     " obj->map_and_fenceable=%d\n",
+-			     i915_gem_obj_offset(obj, vm), alignment,
+-			     !!(flags & PIN_MAPPABLE),
+-			     obj->map_and_fenceable);
+-			ret = i915_vma_unbind(vma);
+-			if (ret)
+-				return ret;
+-
+-			vma = NULL;
+-		}
+-	}
+-
+-	if (vma == NULL || !drm_mm_node_allocated(&vma->node)) {
+-		vma = i915_gem_object_bind_to_vm(obj, vm, alignment, flags);
+-		if (IS_ERR(vma))
+-			return PTR_ERR(vma);
+-	}
+-
+-	if (flags & PIN_GLOBAL && !obj->has_global_gtt_mapping)
+-		vma->bind_vma(vma, obj->cache_level, GLOBAL_BIND);
++	vma = i915_gem_obj_get_ggtt(obj);
++	if (IS_ERR(vma))
++		return PTR_ERR(vma);
+ 
+-	vma->pin_count++;
+-	if (flags & PIN_MAPPABLE)
+-		obj->pin_mappable |= true;
++	ret = i915_vma_pin(vma, alignment, flags | PIN_GLOBAL);
++	i915_vma_put(vma);
+ 
+-	return 0;
++	return ret;
+ }
+ 
+ void
+@@ -4101,9 +3961,9 @@
+ {
+ 	struct i915_vma *vma = i915_gem_obj_to_ggtt(obj);
+ 
+-	BUG_ON(!vma);
++	BUG_ON(vma == NULL);
+ 	BUG_ON(vma->pin_count == 0);
+-	BUG_ON(!i915_gem_obj_ggtt_bound(obj));
++	BUG_ON(!drm_mm_node_allocated(&vma->node));
+ 
+ 	if (--vma->pin_count == 0)
+ 		obj->pin_mappable = false;
+@@ -4175,7 +4035,9 @@
+ 	}
+ 
+ 	if (obj->user_pin_count == 0) {
+-		ret = i915_gem_obj_ggtt_pin(obj, args->alignment, PIN_MAPPABLE);
++		ret = i915_gem_object_ggtt_pin(obj,
++					       args->alignment,
++					       PIN_MAPPABLE);
+ 		if (ret)
+ 			goto out;
+ 	}
+@@ -4234,7 +4096,7 @@
+ {
+ 	struct drm_i915_gem_busy *args = data;
+ 	struct drm_i915_gem_object *obj;
+-	int ret;
++	int ret, i;
+ 
+ 	ret = i915_mutex_lock_interruptible(dev);
+ 	if (ret)
+@@ -4253,10 +4115,16 @@
+ 	 */
+ 	ret = i915_gem_object_flush_active(obj);
+ 
+-	args->busy = obj->active;
+-	if (obj->ring) {
+-		BUILD_BUG_ON(I915_NUM_RINGS > 16);
+-		args->busy |= intel_ring_flag(obj->ring) << 16;
++	args->busy = 0;
++	if (obj->active) {
++		BUILD_BUG_ON(I915_NUM_ENGINES > 16);
++		args->busy |= 1;
++		for (i = 0; i < I915_NUM_ENGINES; i++)  {
++			if (obj->last_read[i].request == NULL)
++				continue;
++
++			args->busy |= 1 << (16 + i);
++		}
+ 	}
+ 
+ 	drm_gem_object_unreference(&obj->base);
+@@ -4265,11 +4133,58 @@
+ 	return ret;
+ }
+ 
++/* Throttle our rendering by waiting until the ring has completed our requests
++ * emitted over 20 msec ago.
++ *
++ * Note that if we were to use the current jiffies each time around the loop,
++ * we wouldn't escape the function with any frames outstanding if the time to
++ * render a frame was over 20ms.
++ *
++ * This should get us reasonable parallelism between CPU and GPU but also
++ * relatively low latency when blocking on a particular request to finish.
++ */
+ int
+ i915_gem_throttle_ioctl(struct drm_device *dev, void *data,
+-			struct drm_file *file_priv)
++			struct drm_file *file)
+ {
+-	return i915_gem_ring_throttle(dev, file_priv);
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_i915_file_private *file_priv = file->driver_priv;
++	unsigned long recent_enough = jiffies - msecs_to_jiffies(20);
++	struct i915_gem_request *rq, *tmp;
++	int ret;
++
++	ret = i915_gem_wait_for_error(&dev_priv->gpu_error);
++	if (ret)
++		return ret;
++
++	/* used for querying whethering the GPU is wedged by legacy userspace */
++	if (i915_terminally_wedged(&dev_priv->gpu_error))
++		return -EIO;
++
++	spin_lock(&file_priv->mm.lock);
++	rq = NULL;
++	list_for_each_entry(tmp, &file_priv->mm.request_list, client_list) {
++		if (time_after_eq(tmp->emitted_jiffies, recent_enough))
++			break;
++		rq = tmp;
++	}
++	rq = i915_request_get(rq);
++	spin_unlock(&file_priv->mm.lock);
++
++	if (rq != NULL) {
++		if (rq->breadcrumb[rq->engine->id] == 0) {
++			ret = i915_mutex_lock_interruptible(dev);
++			if (ret == 0) {
++				ret = i915_request_emit_breadcrumb(rq);
++				mutex_unlock(&dev->struct_mutex);
++			}
++		}
++		if (ret == 0)
++			ret = __i915_request_wait(rq, true, NULL, NULL);
++		i915_request_put__unlocked(rq);
++	}
++
++	return ret;
+ }
+ 
+ int
+@@ -4283,6 +4198,8 @@
+ 	switch (args->madv) {
+ 	case I915_MADV_DONTNEED:
+ 	case I915_MADV_WILLNEED:
++	case I915_MADV_POPULATE:
++	case I915_MADV_INVALIDATE:
+ 	    break;
+ 	default:
+ 	    return -EINVAL;
+@@ -4298,21 +4215,21 @@
+ 		goto unlock;
+ 	}
+ 
+-	if (i915_gem_obj_is_pinned(obj)) {
+-		ret = -EINVAL;
+-		goto out;
+-	}
+-
+-	if (obj->madv != __I915_MADV_PURGED)
+-		obj->madv = args->madv;
++	if (args->madv == I915_MADV_INVALIDATE) {
++		ret = drop_pages(obj);
++	} else if (args->madv == I915_MADV_POPULATE) {
++		ret = i915_gem_object_get_pages(obj);
++	} else {
++		if (obj->madv != __I915_MADV_PURGED)
++			obj->madv = args->madv;
+ 
+-	/* if the object is no longer attached, discard its backing storage */
+-	if (i915_gem_object_is_purgeable(obj) && obj->pages == NULL)
+-		i915_gem_object_truncate(obj);
++		/* if the object is no longer attached, discard its backing storage */
++		if (obj->pages == NULL && i915_gem_object_is_purgeable(obj))
++			i915_gem_object_truncate(obj);
++	}
+ 
+ 	args->retained = obj->madv != __I915_MADV_PURGED;
+ 
+-out:
+ 	drm_gem_object_unreference(&obj->base);
+ unlock:
+ 	mutex_unlock(&dev->struct_mutex);
+@@ -4322,8 +4239,13 @@
+ void i915_gem_object_init(struct drm_i915_gem_object *obj,
+ 			  const struct drm_i915_gem_object_ops *ops)
+ {
++	int i;
++
+ 	INIT_LIST_HEAD(&obj->global_list);
+-	INIT_LIST_HEAD(&obj->ring_list);
++	INIT_LIST_HEAD(&obj->last_fence.engine_link);
++	INIT_LIST_HEAD(&obj->last_write.engine_link);
++	for (i = 0; i < I915_NUM_ENGINES; i++)
++		INIT_LIST_HEAD(&obj->last_read[i].engine_link);
+ 	INIT_LIST_HEAD(&obj->obj_exec_link);
+ 	INIT_LIST_HEAD(&obj->vma_list);
+ 
+@@ -4331,8 +4253,6 @@
+ 
+ 	obj->fence_reg = I915_FENCE_REG_NONE;
+ 	obj->madv = I915_MADV_WILLNEED;
+-	/* Avoid an unnecessary call to unbind on the first bind. */
+-	obj->map_and_fenceable = true;
+ 
+ 	i915_gem_info_add_obj(obj->base.dev->dev_private, obj->base.size);
+ }
+@@ -4424,30 +4344,26 @@
+ 	struct drm_i915_gem_object *obj = to_intel_bo(gem_obj);
+ 	struct drm_device *dev = obj->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct i915_vma *vma, *next;
++	bool was_interruptible;
+ 
+ 	intel_runtime_pm_get(dev_priv);
+ 
+ 	trace_i915_gem_object_destroy(obj);
+ 
+-	list_for_each_entry_safe(vma, next, &obj->vma_list, vma_link) {
+-		int ret;
+-
+-		vma->pin_count = 0;
+-		ret = i915_vma_unbind(vma);
+-		if (WARN_ON(ret == -ERESTARTSYS)) {
+-			bool was_interruptible;
+-
+-			was_interruptible = dev_priv->mm.interruptible;
+-			dev_priv->mm.interruptible = false;
++	was_interruptible = dev_priv->mm.interruptible;
++	dev_priv->mm.interruptible = false;
++	while (!list_empty(&obj->vma_list)) {
++		struct i915_vma *vma;
+ 
+-			WARN_ON(i915_vma_unbind(vma));
++		vma = list_first_entry(&obj->vma_list, typeof(*vma), obj_link);
++		list_del_init(&vma->obj_link);
+ 
+-			dev_priv->mm.interruptible = was_interruptible;
+-		}
++		vma->pin_count = 0;
++		BUG_ON(i915_vma_unbind(vma));
++		i915_vma_put(vma);
+ 	}
+-
+-	i915_gem_object_detach_phys(obj);
++	dev_priv->mm.interruptible = was_interruptible;
++	BUG_ON(obj->fence_reg != I915_FENCE_REG_NONE);
+ 
+ 	/* Stolen objects don't hold a ref, but do hold pin count. Fix that up
+ 	 * before progressing. */
+@@ -4484,35 +4400,140 @@
+ 				     struct i915_address_space *vm)
+ {
+ 	struct i915_vma *vma;
+-	list_for_each_entry(vma, &obj->vma_list, vma_link)
++	list_for_each_entry(vma, &obj->vma_list, obj_link)
+ 		if (vma->vm == vm)
+ 			return vma;
+ 
+ 	return NULL;
+ }
+ 
+-void i915_gem_vma_destroy(struct i915_vma *vma)
++static void
++i915_gem_cleanup_engines(struct drm_device *dev)
+ {
+-	WARN_ON(vma->node.allocated);
++	int i;
+ 
+-	/* Keep the vma as a placeholder in the execbuffer reservation lists */
+-	if (!list_empty(&vma->exec_list))
+-		return;
++	/* Not the regular for_each_engine so we can cleanup a failed setup */
++	for (i = 0; i < I915_NUM_ENGINES; i++)
++		intel_engine_cleanup(&to_i915(dev)->engine[i]);
++}
++
++static int
++i915_gem_resume_engines(struct drm_device *dev)
++{
++	struct intel_engine_cs *engine;
++	int i, ret;
++
++	for_each_engine(engine, to_i915(dev), i) {
++		ret = intel_engine_resume(engine);
++		if (ret)
++			return ret;
++	}
+ 
+-	list_del(&vma->vma_link);
++	return 0;
++}
++
++static int
++i915_gem_suspend_engines(struct drm_device *dev)
++{
++	struct intel_engine_cs *engine;
++	int i, ret;
++
++	for_each_engine(engine, to_i915(dev), i) {
++		ret = intel_engine_suspend(engine);
++		if (ret)
++			return ret;
++	}
+ 
+-	kfree(vma);
++	return 0;
+ }
+ 
+-static void
+-i915_gem_stop_ringbuffers(struct drm_device *dev)
++static bool
++intel_enable_blt(struct drm_i915_private *dev_priv)
++{
++	if (!HAS_BLT(dev_priv))
++		return false;
++
++	/* The blitter was dysfunctional on early prototypes */
++	if (IS_GEN6(dev_priv) && dev_priv->dev->pdev->revision < 8) {
++		DRM_INFO("BLT not supported on this pre-production hardware;"
++			 " graphics performance will be degraded.\n");
++		return false;
++	}
++
++	return true;
++}
++
++static void stop_unused_ring(struct drm_i915_private *dev_priv, u32 base)
++{
++	I915_WRITE(RING_CTL(base), 0);
++	I915_WRITE(RING_HEAD(base), 0);
++	I915_WRITE(RING_TAIL(base), 0);
++	I915_WRITE(RING_START(base), 0);
++}
++
++static void stop_unused_rings(struct drm_i915_private *dev_priv)
++{
++	if (IS_I830(dev_priv)) {
++		stop_unused_ring(dev_priv, PRB1_BASE);
++		stop_unused_ring(dev_priv, SRB0_BASE);
++		stop_unused_ring(dev_priv, SRB1_BASE);
++		stop_unused_ring(dev_priv, SRB2_BASE);
++		stop_unused_ring(dev_priv, SRB3_BASE);
++	} else if (IS_GEN2(dev_priv)) {
++		stop_unused_ring(dev_priv, SRB0_BASE);
++		stop_unused_ring(dev_priv, SRB1_BASE);
++	} else if (IS_GEN3(dev_priv)) {
++		stop_unused_ring(dev_priv, PRB1_BASE);
++		stop_unused_ring(dev_priv, PRB2_BASE);
++	}
++}
++
++static int i915_gem_setup_engines(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring;
+-	int i;
++	int ret;
++
++	/*
++	 * At least 830 can leave some of the unused rings
++	 * "active" (ie. head != tail) after resume which
++	 * will prevent c3 entry. Makes sure all unused rings
++	 * are totally idle.
++	 */
++	stop_unused_rings(dev_priv);
++
++	ret = intel_init_render_engine(dev_priv);
++	if (ret)
++		goto cleanup;
++
++	if (HAS_BSD(dev_priv)) {
++		ret = intel_init_bsd_engine(dev_priv);
++		if (ret)
++			goto cleanup;
++	}
++
++	if (intel_enable_blt(dev_priv)) {
++		ret = intel_init_blt_engine(dev_priv);
++		if (ret)
++			goto cleanup;
++	}
++
++	if (HAS_VEBOX(dev_priv)) {
++		ret = intel_init_vebox_engine(dev_priv);
++		if (ret)
++			goto cleanup;
++	}
+ 
+-	for_each_ring(ring, dev_priv, i)
+-		intel_stop_ring_buffer(ring);
++	if (HAS_BSD2(dev_priv)) {
++		ret = intel_init_bsd2_engine(dev_priv);
++		if (ret)
++			goto cleanup;
++	}
++
++	return 0;
++
++cleanup:
++	i915_gem_cleanup_engines(dev);
++	return ret;
+ }
+ 
+ int
+@@ -4535,8 +4556,9 @@
+ 	if (!drm_core_check_feature(dev, DRIVER_MODESET))
+ 		i915_gem_evict_everything(dev);
+ 
+-	i915_kernel_lost_context(dev);
+-	i915_gem_stop_ringbuffers(dev);
++	ret = i915_gem_suspend_engines(dev);
++	if (ret)
++		goto err;
+ 
+ 	/* Hack!  Don't let anybody do execbuf while we don't control the chip.
+ 	 * We need to replace this with a semaphore, or something.
+@@ -4546,7 +4568,7 @@
+ 							     DRIVER_MODESET);
+ 	mutex_unlock(&dev->struct_mutex);
+ 
+-	del_timer_sync(&dev_priv->gpu_error.hangcheck_timer);
++	cancel_delayed_work_sync(&dev_priv->gpu_error.hangcheck_work);
+ 	cancel_delayed_work_sync(&dev_priv->mm.retire_work);
+ 	flush_delayed_work(&dev_priv->mm.idle_work);
+ 
+@@ -4557,37 +4579,6 @@
+ 	return ret;
+ }
+ 
+-int i915_gem_l3_remap(struct intel_engine_cs *ring, int slice)
+-{
+-	struct drm_device *dev = ring->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	u32 reg_base = GEN7_L3LOG_BASE + (slice * 0x200);
+-	u32 *remap_info = dev_priv->l3_parity.remap_info[slice];
+-	int i, ret;
+-
+-	if (!HAS_L3_DPF(dev) || !remap_info)
+-		return 0;
+-
+-	ret = intel_ring_begin(ring, GEN7_L3LOG_SIZE / 4 * 3);
+-	if (ret)
+-		return ret;
+-
+-	/*
+-	 * Note: We do not worry about the concurrent register cacheline hang
+-	 * here because no other code should access these registers other than
+-	 * at initialization time.
+-	 */
+-	for (i = 0; i < GEN7_L3LOG_SIZE; i += 4) {
+-		intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
+-		intel_ring_emit(ring, reg_base + i);
+-		intel_ring_emit(ring, remap_info[i/4]);
+-	}
+-
+-	intel_ring_advance(ring);
+-
+-	return ret;
+-}
+-
+ void i915_gem_init_swizzling(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+@@ -4613,80 +4604,11 @@
+ 		BUG();
+ }
+ 
+-static bool
+-intel_enable_blt(struct drm_device *dev)
+-{
+-	if (!HAS_BLT(dev))
+-		return false;
+-
+-	/* The blitter was dysfunctional on early prototypes */
+-	if (IS_GEN6(dev) && dev->pdev->revision < 8) {
+-		DRM_INFO("BLT not supported on this pre-production hardware;"
+-			 " graphics performance will be degraded.\n");
+-		return false;
+-	}
+-
+-	return true;
+-}
+-
+-static int i915_gem_init_rings(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	int ret;
+-
+-	ret = intel_init_render_ring_buffer(dev);
+-	if (ret)
+-		return ret;
+-
+-	if (HAS_BSD(dev)) {
+-		ret = intel_init_bsd_ring_buffer(dev);
+-		if (ret)
+-			goto cleanup_render_ring;
+-	}
+-
+-	if (intel_enable_blt(dev)) {
+-		ret = intel_init_blt_ring_buffer(dev);
+-		if (ret)
+-			goto cleanup_bsd_ring;
+-	}
+-
+-	if (HAS_VEBOX(dev)) {
+-		ret = intel_init_vebox_ring_buffer(dev);
+-		if (ret)
+-			goto cleanup_blt_ring;
+-	}
+-
+-	if (HAS_BSD2(dev)) {
+-		ret = intel_init_bsd2_ring_buffer(dev);
+-		if (ret)
+-			goto cleanup_vebox_ring;
+-	}
+-
+-	ret = i915_gem_set_seqno(dev, ((u32)~0 - 0x1000));
+-	if (ret)
+-		goto cleanup_bsd2_ring;
+-
+-	return 0;
+-
+-cleanup_bsd2_ring:
+-	intel_cleanup_ring_buffer(&dev_priv->ring[VCS2]);
+-cleanup_vebox_ring:
+-	intel_cleanup_ring_buffer(&dev_priv->ring[VECS]);
+-cleanup_blt_ring:
+-	intel_cleanup_ring_buffer(&dev_priv->ring[BCS]);
+-cleanup_bsd_ring:
+-	intel_cleanup_ring_buffer(&dev_priv->ring[VCS]);
+-cleanup_render_ring:
+-	intel_cleanup_ring_buffer(&dev_priv->ring[RCS]);
+-
+-	return ret;
+-}
+-
+ int
+ i915_gem_init_hw(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	int ret, i;
++	int ret;
+ 
+ 	if (INTEL_INFO(dev)->gen < 6 && !intel_enable_gtt())
+ 		return -EIO;
+@@ -4712,25 +4634,11 @@
+ 
+ 	i915_gem_init_swizzling(dev);
+ 
+-	ret = i915_gem_init_rings(dev);
+-	if (ret)
+-		return ret;
+-
+-	for (i = 0; i < NUM_L3_SLICES(dev); i++)
+-		i915_gem_l3_remap(&dev_priv->ring[RCS], i);
+-
+-	/*
+-	 * XXX: Contexts should only be initialized once. Doing a switch to the
+-	 * default context switch however is something we'd like to do after
+-	 * reset or thaw (the latter may not actually be necessary for HW, but
+-	 * goes with our code better). Context switching requires rings (for
+-	 * the do_switch), but before enabling PPGTT. So don't move this.
+-	 */
+-	ret = i915_gem_context_enable(dev_priv);
+-	if (ret && ret != -EIO) {
+-		DRM_ERROR("Context enable failed %d\n", ret);
+-		i915_gem_cleanup_ringbuffer(dev);
+-	}
++	ret = i915_ppgtt_init_hw(dev);
++	if (ret == 0)
++		ret = i915_gem_context_enable(dev_priv);
++	if (ret == 0)
++		ret = i915_gem_resume_engines(dev);
+ 
+ 	return ret;
+ }
+@@ -4753,13 +4661,12 @@
+ 	i915_gem_init_userptr(dev);
+ 	i915_gem_init_global_gtt(dev);
+ 
+-	ret = i915_gem_context_init(dev);
+-	if (ret) {
+-		mutex_unlock(&dev->struct_mutex);
+-		return ret;
+-	}
+-
+-	ret = i915_gem_init_hw(dev);
++	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
++	ret = i915_gem_setup_engines(dev);
++	if (ret == 0)
++		ret = i915_gem_context_init(dev);
++	if (ret == 0)
++		ret = i915_gem_init_hw(dev);
+ 	if (ret == -EIO) {
+ 		/* Allow ring initialisation to fail by marking the GPU as
+ 		 * wedged. But we only want to do this where the GPU is angry,
+@@ -4769,23 +4676,16 @@
+ 		atomic_set_mask(I915_WEDGED, &dev_priv->gpu_error.reset_counter);
+ 		ret = 0;
+ 	}
++	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
+ 	mutex_unlock(&dev->struct_mutex);
+ 
+-	/* Allow hardware batchbuffers unless told otherwise, but not for KMS. */
+-	if (!drm_core_check_feature(dev, DRIVER_MODESET))
+-		dev_priv->dri1.allow_batchbuffer = 1;
+ 	return ret;
+ }
+ 
+-void
+-i915_gem_cleanup_ringbuffer(struct drm_device *dev)
++void i915_gem_fini(struct drm_device *dev)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring;
+-	int i;
+-
+-	for_each_ring(ring, dev_priv, i)
+-		intel_cleanup_ring_buffer(ring);
++	i915_gem_context_fini(dev);
++	i915_gem_cleanup_engines(dev);
+ }
+ 
+ int
+@@ -4804,26 +4704,12 @@
+ 	}
+ 
+ 	mutex_lock(&dev->struct_mutex);
+-	dev_priv->ums.mm_suspended = 0;
+-
+ 	ret = i915_gem_init_hw(dev);
+-	if (ret != 0) {
+-		mutex_unlock(&dev->struct_mutex);
+-		return ret;
+-	}
+-
++	if (ret == 0)
++		ret = drm_irq_install(dev, dev->pdev->irq);
++	if (ret == 0)
++		dev_priv->ums.mm_suspended = 0;
+ 	BUG_ON(!list_empty(&dev_priv->gtt.base.active_list));
+-
+-	ret = drm_irq_install(dev, dev->pdev->irq);
+-	if (ret)
+-		goto cleanup_ringbuffer;
+-	mutex_unlock(&dev->struct_mutex);
+-
+-	return 0;
+-
+-cleanup_ringbuffer:
+-	i915_gem_cleanup_ringbuffer(dev);
+-	dev_priv->ums.mm_suspended = 1;
+ 	mutex_unlock(&dev->struct_mutex);
+ 
+ 	return ret;
+@@ -4857,15 +4743,19 @@
+ }
+ 
+ static void
+-init_ring_lists(struct intel_engine_cs *ring)
++init_null_engine(struct intel_engine_cs *engine)
+ {
+-	INIT_LIST_HEAD(&ring->active_list);
+-	INIT_LIST_HEAD(&ring->request_list);
++	INIT_LIST_HEAD(&engine->read_list);
++	INIT_LIST_HEAD(&engine->write_list);
++	INIT_LIST_HEAD(&engine->fence_list);
++	INIT_LIST_HEAD(&engine->requests);
++	INIT_LIST_HEAD(&engine->rings);
+ }
+ 
+ void i915_init_vm(struct drm_i915_private *dev_priv,
+ 		  struct i915_address_space *vm)
+ {
++	kref_init(&vm->ref);
+ 	if (!i915_is_ggtt(vm))
+ 		drm_mm_init(&vm->mm, vm->start, vm->total);
+ 	vm->dev = dev_priv->dev;
+@@ -4894,8 +4784,8 @@
+ 	INIT_LIST_HEAD(&dev_priv->mm.unbound_list);
+ 	INIT_LIST_HEAD(&dev_priv->mm.bound_list);
+ 	INIT_LIST_HEAD(&dev_priv->mm.fence_list);
+-	for (i = 0; i < I915_NUM_RINGS; i++)
+-		init_ring_lists(&dev_priv->ring[i]);
++	for (i = 0; i < I915_NUM_ENGINES; i++)
++		init_null_engine(&dev_priv->engine[i]);
+ 	for (i = 0; i < I915_MAX_NUM_FENCES; i++)
+ 		INIT_LIST_HEAD(&dev_priv->fence_regs[i].lru_list);
+ 	INIT_DELAYED_WORK(&dev_priv->mm.retire_work,
+@@ -4947,32 +4837,27 @@
+ {
+ 	struct drm_i915_file_private *file_priv = file->driver_priv;
+ 
+-	cancel_delayed_work_sync(&file_priv->mm.idle_work);
+-
+ 	/* Clean up our request list when the client is going away, so that
+ 	 * later retire_requests won't dereference our soon-to-be-gone
+ 	 * file_priv.
+ 	 */
+ 	spin_lock(&file_priv->mm.lock);
+ 	while (!list_empty(&file_priv->mm.request_list)) {
+-		struct drm_i915_gem_request *request;
++		struct i915_gem_request *rq;
+ 
+-		request = list_first_entry(&file_priv->mm.request_list,
+-					   struct drm_i915_gem_request,
+-					   client_list);
+-		list_del(&request->client_list);
+-		request->file_priv = NULL;
++		rq = list_first_entry(&file_priv->mm.request_list,
++				      struct i915_gem_request,
++				      client_list);
++		list_del(&rq->client_list);
++		rq->file_priv = NULL;
+ 	}
+ 	spin_unlock(&file_priv->mm.lock);
+-}
+-
+-static void
+-i915_gem_file_idle_work_handler(struct work_struct *work)
+-{
+-	struct drm_i915_file_private *file_priv =
+-		container_of(work, typeof(*file_priv), mm.idle_work.work);
+ 
+-	atomic_set(&file_priv->rps_wait_boost, false);
++	if (!list_empty(&file_priv->rps_boost)) {
++		mutex_lock(&to_i915(dev)->rps.hw_lock);
++		list_del(&file_priv->rps_boost);
++		mutex_unlock(&to_i915(dev)->rps.hw_lock);
++	}
+ }
+ 
+ int i915_gem_open(struct drm_device *dev, struct drm_file *file)
+@@ -4989,11 +4874,10 @@
+ 	file->driver_priv = file_priv;
+ 	file_priv->dev_priv = dev->dev_private;
+ 	file_priv->file = file;
++	INIT_LIST_HEAD(&file_priv->rps_boost);
+ 
+ 	spin_lock_init(&file_priv->mm.lock);
+ 	INIT_LIST_HEAD(&file_priv->mm.request_list);
+-	INIT_DELAYED_WORK(&file_priv->mm.idle_work,
+-			  i915_gem_file_idle_work_handler);
+ 
+ 	ret = i915_gem_context_open(dev, file);
+ 	if (ret)
+@@ -5002,6 +4886,15 @@
+ 	return ret;
+ }
+ 
++/**
++ * i915_gem_track_fb - update frontbuffer tracking
++ * old: current GEM buffer for the frontbuffer slots
++ * new: new GEM buffer for the frontbuffer slots
++ * frontbuffer_bits: bitmask of frontbuffer slots
++ *
++ * This updates the frontbuffer tracking bits @frontbuffer_bits by clearing them
++ * from @old and setting them in @new. Both @old and @new can be NULL.
++ */
+ void i915_gem_track_fb(struct drm_i915_gem_object *old,
+ 		       struct drm_i915_gem_object *new,
+ 		       unsigned frontbuffer_bits)
+@@ -5053,7 +4946,7 @@
+ 	struct i915_vma *vma;
+ 	int count = 0;
+ 
+-	list_for_each_entry(vma, &obj->vma_list, vma_link)
++	list_for_each_entry(vma, &obj->vma_list, obj_link)
+ 		if (drm_mm_node_allocated(&vma->node))
+ 			count++;
+ 
+@@ -5097,11 +4990,9 @@
+ 	struct drm_i915_private *dev_priv = o->base.dev->dev_private;
+ 	struct i915_vma *vma;
+ 
+-	if (!dev_priv->mm.aliasing_ppgtt ||
+-	    vm == &dev_priv->mm.aliasing_ppgtt->base)
+-		vm = &dev_priv->gtt.base;
++	WARN_ON(vm == &dev_priv->mm.aliasing_ppgtt->base);
+ 
+-	list_for_each_entry(vma, &o->vma_list, vma_link) {
++	list_for_each_entry(vma, &o->vma_list, obj_link) {
+ 		if (vma->vm == vm)
+ 			return vma->node.start;
+ 
+@@ -5116,7 +5007,7 @@
+ {
+ 	struct i915_vma *vma;
+ 
+-	list_for_each_entry(vma, &o->vma_list, vma_link)
++	list_for_each_entry(vma, &o->vma_list, obj_link)
+ 		if (vma->vm == vm && drm_mm_node_allocated(&vma->node))
+ 			return true;
+ 
+@@ -5127,7 +5018,7 @@
+ {
+ 	struct i915_vma *vma;
+ 
+-	list_for_each_entry(vma, &o->vma_list, vma_link)
++	list_for_each_entry(vma, &o->vma_list, obj_link)
+ 		if (drm_mm_node_allocated(&vma->node))
+ 			return true;
+ 
+@@ -5140,13 +5031,11 @@
+ 	struct drm_i915_private *dev_priv = o->base.dev->dev_private;
+ 	struct i915_vma *vma;
+ 
+-	if (!dev_priv->mm.aliasing_ppgtt ||
+-	    vm == &dev_priv->mm.aliasing_ppgtt->base)
+-		vm = &dev_priv->gtt.base;
++	WARN_ON(vm == &dev_priv->mm.aliasing_ppgtt->base);
+ 
+ 	BUG_ON(list_empty(&o->vma_list));
+ 
+-	list_for_each_entry(vma, &o->vma_list, vma_link)
++	list_for_each_entry(vma, &o->vma_list, obj_link)
+ 		if (vma->vm == vm)
+ 			return vma->node.size;
+ 
+@@ -5165,11 +5054,16 @@
+ 	if (!i915_gem_shrinker_lock(dev, &unlock))
+ 		return SHRINK_STOP;
+ 
+-	freed = i915_gem_purge(dev_priv, sc->nr_to_scan);
++	freed = i915_gem_shrink(dev_priv,
++				sc->nr_to_scan,
++				I915_SHRINK_BOUND |
++				I915_SHRINK_UNBOUND |
++				I915_SHRINK_PURGEABLE);
+ 	if (freed < sc->nr_to_scan)
+-		freed += __i915_gem_shrink(dev_priv,
+-					   sc->nr_to_scan - freed,
+-					   false);
++		freed += i915_gem_shrink(dev_priv,
++					 sc->nr_to_scan - freed,
++					 I915_SHRINK_BOUND |
++					 I915_SHRINK_UNBOUND);
+ 	if (unlock)
+ 		mutex_unlock(&dev->struct_mutex);
+ 
+@@ -5184,7 +5078,7 @@
+ 	struct drm_device *dev = dev_priv->dev;
+ 	struct drm_i915_gem_object *obj;
+ 	unsigned long timeout = msecs_to_jiffies(5000) + 1;
+-	unsigned long pinned, bound, unbound, freed;
++	unsigned long pinned, bound, unbound, freed_pages;
+ 	bool was_interruptible;
+ 	bool unlock;
+ 
+@@ -5201,7 +5095,7 @@
+ 	was_interruptible = dev_priv->mm.interruptible;
+ 	dev_priv->mm.interruptible = false;
+ 
+-	freed = i915_gem_shrink_all(dev_priv);
++	freed_pages = i915_gem_shrink_all(dev_priv);
+ 
+ 	dev_priv->mm.interruptible = was_interruptible;
+ 
+@@ -5232,14 +5126,15 @@
+ 	if (unlock)
+ 		mutex_unlock(&dev->struct_mutex);
+ 
+-	pr_info("Purging GPU memory, %lu bytes freed, %lu bytes still pinned.\n",
+-		freed, pinned);
++	if (freed_pages || unbound || bound)
++		pr_info("Purging GPU memory, %lu bytes freed, %lu bytes still pinned.\n",
++			freed_pages << PAGE_SHIFT, pinned);
+ 	if (unbound || bound)
+ 		pr_err("%lu and %lu bytes still available in the "
+ 		       "bound and unbound GPU page lists.\n",
+ 		       bound, unbound);
+ 
+-	*(unsigned long *)ptr += freed;
++	*(unsigned long *)ptr += freed_pages;
+ 	return NOTIFY_DONE;
+ }
+ 
+@@ -5247,14 +5142,8 @@
+ {
+ 	struct i915_vma *vma;
+ 
+-	/* This WARN has probably outlived its usefulness (callers already
+-	 * WARN if they don't find the GGTT vma they expect). When removing,
+-	 * remember to remove the pre-check in is_pin_display() as well */
+-	if (WARN_ON(list_empty(&obj->vma_list)))
+-		return NULL;
+-
+-	vma = list_first_entry(&obj->vma_list, typeof(*vma), vma_link);
+-	if (vma->vm != obj_to_ggtt(obj))
++	vma = list_first_entry(&obj->vma_list, typeof(*vma), obj_link);
++	if (vma->vm != i915_obj_to_ggtt(obj))
+ 		return NULL;
+ 
+ 	return vma;
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_gem_context.c b/drivers/gpu/drm/i915/i915_gem_context.c
+--- a/drivers/gpu/drm/i915/i915_gem_context.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/i915_gem_context.c	2014-11-20 09:53:37.964762837 -0700
+@@ -88,65 +88,7 @@
+ #include <drm/drmP.h>
+ #include <drm/i915_drm.h>
+ #include "i915_drv.h"
+-
+-/* This is a HW constraint. The value below is the largest known requirement
+- * I've seen in a spec to date, and that was a workaround for a non-shipping
+- * part. It should be safe to decrease this, but it's more future proof as is.
+- */
+-#define GEN6_CONTEXT_ALIGN (64<<10)
+-#define GEN7_CONTEXT_ALIGN 4096
+-
+-static void do_ppgtt_cleanup(struct i915_hw_ppgtt *ppgtt)
+-{
+-	struct drm_device *dev = ppgtt->base.dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct i915_address_space *vm = &ppgtt->base;
+-
+-	if (ppgtt == dev_priv->mm.aliasing_ppgtt ||
+-	    (list_empty(&vm->active_list) && list_empty(&vm->inactive_list))) {
+-		ppgtt->base.cleanup(&ppgtt->base);
+-		return;
+-	}
+-
+-	/*
+-	 * Make sure vmas are unbound before we take down the drm_mm
+-	 *
+-	 * FIXME: Proper refcounting should take care of this, this shouldn't be
+-	 * needed at all.
+-	 */
+-	if (!list_empty(&vm->active_list)) {
+-		struct i915_vma *vma;
+-
+-		list_for_each_entry(vma, &vm->active_list, mm_list)
+-			if (WARN_ON(list_empty(&vma->vma_link) ||
+-				    list_is_singular(&vma->vma_link)))
+-				break;
+-
+-		i915_gem_evict_vm(&ppgtt->base, true);
+-	} else {
+-		i915_gem_retire_requests(dev);
+-		i915_gem_evict_vm(&ppgtt->base, false);
+-	}
+-
+-	ppgtt->base.cleanup(&ppgtt->base);
+-}
+-
+-static void ppgtt_release(struct kref *kref)
+-{
+-	struct i915_hw_ppgtt *ppgtt =
+-		container_of(kref, struct i915_hw_ppgtt, ref);
+-
+-	do_ppgtt_cleanup(ppgtt);
+-	kfree(ppgtt);
+-}
+-
+-static size_t get_context_alignment(struct drm_device *dev)
+-{
+-	if (IS_GEN6(dev))
+-		return GEN6_CONTEXT_ALIGN;
+-
+-	return GEN7_CONTEXT_ALIGN;
+-}
++#include "i915_trace.h"
+ 
+ static int get_context_size(struct drm_device *dev)
+ {
+@@ -155,6 +97,9 @@
+ 	u32 reg;
+ 
+ 	switch (INTEL_INFO(dev)->gen) {
++	case 5:
++		ret = ILK_CXT_TOTAL_SIZE;
++		break;
+ 	case 6:
+ 		reg = I915_READ(CXT_SIZE);
+ 		ret = GEN6_CXT_TOTAL_SIZE(reg) * 64;
+@@ -170,39 +115,44 @@
+ 		ret = GEN8_CXT_TOTAL_SIZE;
+ 		break;
+ 	default:
+-		BUG();
++		WARN(1,
++		     "context size not known for gen %d\n",
++		     INTEL_INFO(dev)->gen);
++		ret = -1;
++		break;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+-void i915_gem_context_free(struct kref *ctx_ref)
++void __i915_gem_context_free(struct kref *ctx_ref)
+ {
+-	struct intel_context *ctx = container_of(ctx_ref,
+-						   typeof(*ctx), ref);
+-	struct i915_hw_ppgtt *ppgtt = NULL;
+-
+-	if (ctx->legacy_hw_ctx.rcs_state) {
+-		/* We refcount even the aliasing PPGTT to keep the code symmetric */
+-		if (USES_PPGTT(ctx->legacy_hw_ctx.rcs_state->base.dev))
+-			ppgtt = ctx_to_ppgtt(ctx);
+-	}
+-
+-	if (ppgtt)
+-		kref_put(&ppgtt->ref, ppgtt_release);
+-	if (ctx->legacy_hw_ctx.rcs_state)
+-		drm_gem_object_unreference(&ctx->legacy_hw_ctx.rcs_state->base);
++	struct intel_context *ctx =
++		container_of(ctx_ref, typeof(*ctx), ref);
++	struct intel_engine_cs *engine;
++	int i;
++
++	DRM_DEBUG_DRIVER("HW context %d freed\n", ctx->user_handle);
++	trace_i915_context_free(ctx);
++
++	for_each_engine(engine, ctx->i915, i)
++		engine->free_context(engine, ctx);
++
++	i915_vm_put(&ctx->ppgtt->base);
++
+ 	list_del(&ctx->link);
+ 	kfree(ctx);
+ }
+ 
+-static struct drm_i915_gem_object *
++struct drm_i915_gem_object *
+ i915_gem_alloc_context_obj(struct drm_device *dev, size_t size)
+ {
+ 	struct drm_i915_gem_object *obj;
+ 	int ret;
+ 
+-	obj = i915_gem_alloc_object(dev, size);
++	obj = i915_gem_object_create_stolen(dev, size);
++	if (obj == NULL)
++		obj = i915_gem_alloc_object(dev, size);
+ 	if (obj == NULL)
+ 		return ERR_PTR(-ENOMEM);
+ 
+@@ -226,49 +176,38 @@
+ 	return obj;
+ }
+ 
+-static struct i915_hw_ppgtt *
+-create_vm_for_ctx(struct drm_device *dev, struct intel_context *ctx)
+-{
+-	struct i915_hw_ppgtt *ppgtt;
+-	int ret;
+-
+-	ppgtt = kzalloc(sizeof(*ppgtt), GFP_KERNEL);
+-	if (!ppgtt)
+-		return ERR_PTR(-ENOMEM);
+-
+-	ret = i915_gem_init_ppgtt(dev, ppgtt);
+-	if (ret) {
+-		kfree(ppgtt);
+-		return ERR_PTR(ret);
+-	}
+-
+-	ppgtt->ctx = ctx;
+-	return ppgtt;
+-}
+-
+ static struct intel_context *
+-__create_hw_context(struct drm_device *dev,
+-		  struct drm_i915_file_private *file_priv)
++i915_gem_create_context(struct drm_device *dev,
++			struct drm_i915_file_private *file_priv,
++			bool is_default)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_i915_private *dev_priv = to_i915(dev);
+ 	struct intel_context *ctx;
+ 	int ret;
+ 
++	BUG_ON(!mutex_is_locked(&dev->struct_mutex));
++
+ 	ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);
+ 	if (ctx == NULL)
+ 		return ERR_PTR(-ENOMEM);
+ 
+ 	kref_init(&ctx->ref);
+ 	list_add_tail(&ctx->link, &dev_priv->context_list);
++	ctx->i915 = dev_priv;
+ 
+ 	if (dev_priv->hw_context_size) {
+-		struct drm_i915_gem_object *obj =
+-				i915_gem_alloc_context_obj(dev, dev_priv->hw_context_size);
++		struct drm_i915_gem_object *obj;
++
++		if (is_default) {
++			obj = RCS_ENGINE(dev_priv)->default_context->ring[RCS].state;
++			drm_gem_object_reference(&obj->base);
++		} else
++			obj = i915_gem_alloc_context_obj(dev, dev_priv->hw_context_size);
+ 		if (IS_ERR(obj)) {
+ 			ret = PTR_ERR(obj);
+-			goto err_out;
++			goto err;
+ 		}
+-		ctx->legacy_hw_ctx.rcs_state = obj;
++		ctx->ring[RCS].state = obj;
+ 	}
+ 
+ 	/* Default context will never have a file_priv */
+@@ -276,10 +215,12 @@
+ 		ret = idr_alloc(&file_priv->context_idr, ctx,
+ 				DEFAULT_CONTEXT_HANDLE, 0, GFP_KERNEL);
+ 		if (ret < 0)
+-			goto err_out;
++			goto err;
+ 	} else
+ 		ret = DEFAULT_CONTEXT_HANDLE;
+ 
++	BUG_ON(is_default && ret != DEFAULT_CONTEXT_HANDLE);
++
+ 	ctx->file_priv = file_priv;
+ 	ctx->user_handle = ret;
+ 	/* NB: Mark all slices as needing a remap so that when the context first
+@@ -287,137 +228,42 @@
+ 	 * is no remap info, it will be a NOP. */
+ 	ctx->remap_slice = (1 << NUM_L3_SLICES(dev)) - 1;
+ 
+-	return ctx;
+-
+-err_out:
+-	i915_gem_context_unreference(ctx);
+-	return ERR_PTR(ret);
+-}
+-
+-/**
+- * The default context needs to exist per ring that uses contexts. It stores the
+- * context state of the GPU for applications that don't utilize HW contexts, as
+- * well as an idle case.
+- */
+-static struct intel_context *
+-i915_gem_create_context(struct drm_device *dev,
+-			struct drm_i915_file_private *file_priv,
+-			bool create_vm)
+-{
+-	const bool is_global_default_ctx = file_priv == NULL;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_context *ctx;
+-	int ret = 0;
+-
+-	BUG_ON(!mutex_is_locked(&dev->struct_mutex));
+-
+-	ctx = __create_hw_context(dev, file_priv);
+-	if (IS_ERR(ctx))
+-		return ctx;
+-
+-	if (is_global_default_ctx && ctx->legacy_hw_ctx.rcs_state) {
+-		/* We may need to do things with the shrinker which
+-		 * require us to immediately switch back to the default
+-		 * context. This can cause a problem as pinning the
+-		 * default context also requires GTT space which may not
+-		 * be available. To avoid this we always pin the default
+-		 * context.
+-		 */
+-		ret = i915_gem_obj_ggtt_pin(ctx->legacy_hw_ctx.rcs_state,
+-					    get_context_alignment(dev), 0);
+-		if (ret) {
+-			DRM_DEBUG_DRIVER("Couldn't pin %d\n", ret);
+-			goto err_destroy;
+-		}
+-	}
++	ctx->hang_stats.ban_period_seconds = DRM_I915_CTX_BAN_PERIOD;
+ 
+-	if (create_vm) {
+-		struct i915_hw_ppgtt *ppgtt = create_vm_for_ctx(dev, ctx);
++	if (USES_FULL_PPGTT(dev)) {
++		struct i915_hw_ppgtt *ppgtt = i915_ppgtt_create(dev, file_priv);
+ 
+ 		if (IS_ERR_OR_NULL(ppgtt)) {
+ 			DRM_DEBUG_DRIVER("PPGTT setup failed (%ld)\n",
+ 					 PTR_ERR(ppgtt));
++			idr_remove(&file_priv->context_idr, ctx->user_handle);
+ 			ret = PTR_ERR(ppgtt);
+-			goto err_unpin;
+-		} else
+-			ctx->vm = &ppgtt->base;
++			goto err;
++		}
+ 
+-		/* This case is reserved for the global default context and
+-		 * should only happen once. */
+-		if (is_global_default_ctx) {
+-			if (WARN_ON(dev_priv->mm.aliasing_ppgtt)) {
+-				ret = -EEXIST;
+-				goto err_unpin;
+-			}
++		ctx->ppgtt = ppgtt;
++	}
+ 
+-			dev_priv->mm.aliasing_ppgtt = ppgtt;
+-		}
+-	} else if (USES_PPGTT(dev)) {
+-		/* For platforms which only have aliasing PPGTT, we fake the
+-		 * address space and refcounting. */
+-		ctx->vm = &dev_priv->mm.aliasing_ppgtt->base;
+-		kref_get(&dev_priv->mm.aliasing_ppgtt->ref);
+-	} else
+-		ctx->vm = &dev_priv->gtt.base;
++	trace_i915_context_create(ctx);
+ 
+ 	return ctx;
+ 
+-err_unpin:
+-	if (is_global_default_ctx && ctx->legacy_hw_ctx.rcs_state)
+-		i915_gem_object_ggtt_unpin(ctx->legacy_hw_ctx.rcs_state);
+-err_destroy:
++err:
+ 	i915_gem_context_unreference(ctx);
+ 	return ERR_PTR(ret);
+ }
+ 
+-void i915_gem_context_reset(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	int i;
+-
+-	/* Prevent the hardware from restoring the last context (which hung) on
+-	 * the next switch */
+-	for (i = 0; i < I915_NUM_RINGS; i++) {
+-		struct intel_engine_cs *ring = &dev_priv->ring[i];
+-		struct intel_context *dctx = ring->default_context;
+-		struct intel_context *lctx = ring->last_context;
+-
+-		/* Do a fake switch to the default context */
+-		if (lctx == dctx)
+-			continue;
+-
+-		if (!lctx)
+-			continue;
+-
+-		if (dctx->legacy_hw_ctx.rcs_state && i == RCS) {
+-			WARN_ON(i915_gem_obj_ggtt_pin(dctx->legacy_hw_ctx.rcs_state,
+-						      get_context_alignment(dev), 0));
+-			/* Fake a finish/inactive */
+-			dctx->legacy_hw_ctx.rcs_state->base.write_domain = 0;
+-			dctx->legacy_hw_ctx.rcs_state->active = 0;
+-		}
+-
+-		if (lctx->legacy_hw_ctx.rcs_state && i == RCS)
+-			i915_gem_object_ggtt_unpin(lctx->legacy_hw_ctx.rcs_state);
+-
+-		i915_gem_context_unreference(lctx);
+-		i915_gem_context_reference(dctx);
+-		ring->last_context = dctx;
+-	}
+-}
+-
+ int i915_gem_context_init(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct intel_context *ctx;
+ 	int i;
+ 
+-	/* Init should only be called once per module load. Eventually the
+-	 * restriction on the context_disabled check can be loosened. */
+-	if (WARN_ON(dev_priv->ring[RCS].default_context))
+-		return 0;
+-
+-	if (HAS_HW_CONTEXTS(dev)) {
++	if (RCS_ENGINE(dev_priv)->execlists_enabled) {
++		/* NB: intentionally left blank. We will allocate our own
++		 * backing objects as we need them, thank you very much */
++		dev_priv->hw_context_size = 0;
++	} else if (HAS_HW_CONTEXTS(dev)) {
+ 		dev_priv->hw_context_size = round_up(get_context_size(dev), 4096);
+ 		if (dev_priv->hw_context_size > (1<<20)) {
+ 			DRM_DEBUG_DRIVER("Disabling HW Contexts; invalid size %d\n",
+@@ -426,86 +272,106 @@
+ 		}
+ 	}
+ 
+-	ctx = i915_gem_create_context(dev, NULL, USES_PPGTT(dev));
++	/**
++	 * The default context needs to exist per ring that uses contexts.
++	 * It stores the context state of the GPU for applications that don't
++	 * utilize HW contexts or per-process VM, as well as an idle case.
++	 */
++	ctx = i915_gem_create_context(dev, NULL, false);
+ 	if (IS_ERR(ctx)) {
+ 		DRM_ERROR("Failed to create default global context (error %ld)\n",
+ 			  PTR_ERR(ctx));
+ 		return PTR_ERR(ctx);
+ 	}
+ 
+-	/* NB: RCS will hold a ref for all rings */
+-	for (i = 0; i < I915_NUM_RINGS; i++)
+-		dev_priv->ring[i].default_context = ctx;
++	/* We may need to do things with the shrinker which
++	 * require us to immediately switch back to the default
++	 * context. This can cause a problem as pinning the
++	 * default context also requires GTT space which may not
++	 * be available. To avoid this we always pin the default
++	 * context.
++	 */
++	for (i = 0; i < I915_NUM_ENGINES; i++) {
++		struct intel_engine_cs *engine = &dev_priv->engine[i];
++		struct intel_ringbuffer *ring;
++
++		if (engine->i915 == NULL)
++			continue;
++
++		engine->default_context = ctx;
++		i915_gem_context_reference(ctx);
+ 
+-	DRM_DEBUG_DRIVER("%s context support initialized\n", dev_priv->hw_context_size ? "HW" : "fake");
++		ring = engine->pin_context(engine, ctx);
++		if (IS_ERR(ring)) {
++			DRM_ERROR("Failed to pin global default context (%s)\n",
++				  engine->name);
++			i915_gem_context_unreference(ctx);
++			return PTR_ERR(ring);
++		}
++	}
++	i915_gem_context_unreference(ctx);
++
++	DRM_DEBUG_DRIVER("%s context support initialized\n",
++			 RCS_ENGINE(dev_priv)->execlists_enabled ? "LR" :
++			 dev_priv->hw_context_size ? "HW" : "fake");
+ 	return 0;
+ }
+ 
+ void i915_gem_context_fini(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_context *dctx = dev_priv->ring[RCS].default_context;
++	struct intel_engine_cs *engine;
+ 	int i;
+ 
+-	if (dctx->legacy_hw_ctx.rcs_state) {
+-		/* The only known way to stop the gpu from accessing the hw context is
+-		 * to reset it. Do this as the very last operation to avoid confusing
+-		 * other code, leading to spurious errors. */
++	if (dev_priv->hw_context_size)
++		/* The only known way to stop the gpu from accessing
++		 * the hw context is to reset it. Do this as the very
++		 * last operation to avoid confusing other code,
++		 * leading to spurious errors.
++		 */
+ 		intel_gpu_reset(dev);
+ 
+-		/* When default context is created and switched to, base object refcount
+-		 * will be 2 (+1 from object creation and +1 from do_switch()).
+-		 * i915_gem_context_fini() will be called after gpu_idle() has switched
+-		 * to default context. So we need to unreference the base object once
+-		 * to offset the do_switch part, so that i915_gem_context_unreference()
+-		 * can then free the base object correctly. */
+-		WARN_ON(!dev_priv->ring[RCS].last_context);
+-		if (dev_priv->ring[RCS].last_context == dctx) {
+-			/* Fake switch to NULL context */
+-			WARN_ON(dctx->legacy_hw_ctx.rcs_state->active);
+-			i915_gem_object_ggtt_unpin(dctx->legacy_hw_ctx.rcs_state);
+-			i915_gem_context_unreference(dctx);
+-			dev_priv->ring[RCS].last_context = NULL;
+-		}
+-
+-		i915_gem_object_ggtt_unpin(dctx->legacy_hw_ctx.rcs_state);
+-	}
+-
+-	for (i = 0; i < I915_NUM_RINGS; i++) {
+-		struct intel_engine_cs *ring = &dev_priv->ring[i];
+-
+-		if (ring->last_context)
+-			i915_gem_context_unreference(ring->last_context);
+-
+-		ring->default_context = NULL;
+-		ring->last_context = NULL;
++	for_each_engine(engine, dev_priv, i) {
++		engine->unpin_context(engine, engine->default_context);
++		i915_gem_context_unreference(engine->default_context);
++		engine->default_context = NULL;
+ 	}
+-
+-	i915_gem_context_unreference(dctx);
+ }
+ 
+ int i915_gem_context_enable(struct drm_i915_private *dev_priv)
+ {
+-	struct intel_engine_cs *ring;
++	struct intel_engine_cs *engine;
+ 	int ret, i;
+ 
+-	/* This is the only place the aliasing PPGTT gets enabled, which means
+-	 * it has to happen before we bail on reset */
+-	if (dev_priv->mm.aliasing_ppgtt) {
+-		struct i915_hw_ppgtt *ppgtt = dev_priv->mm.aliasing_ppgtt;
+-		ppgtt->enable(ppgtt);
+-	}
+-
+-	/* FIXME: We should make this work, even in reset */
+-	if (i915_reset_in_progress(&dev_priv->gpu_error))
+-		return 0;
+-
+-	BUG_ON(!dev_priv->ring[RCS].default_context);
++	for_each_engine(engine, dev_priv, i) {
++		struct intel_context *ctx = engine->default_context;
++		struct i915_gem_request *rq;
++
++		if (HAS_L3_DPF(dev_priv))
++			ctx->remap_slice = (1 << NUM_L3_SLICES(dev_priv)) - 1;
++
++		rq = i915_request_create(ctx, engine);
++		if (IS_ERR(rq)) {
++			ret = PTR_ERR(rq);
++			goto err;
++		}
+ 
+-	for_each_ring(ring, dev_priv, i) {
+-		ret = i915_switch_context(ring, ring->default_context);
+-		if (ret)
++		ret = 0;
++		/*
++		 * Workarounds applied in this fn are part of register state context,
++		 * they need to be re-initialized followed by gpu reset, suspend/resume,
++		 * module reload.
++		 */
++		if (engine->init_context)
++			ret = engine->init_context(rq);
++		if (ret == 0)
++			ret = i915_request_commit(rq);
++		i915_request_put(rq);
++		if (ret) {
++err:
++			DRM_ERROR("failed to enabled contexts (%s): %d\n", engine->name, ret);
+ 			return ret;
++		}
+ 	}
+ 
+ 	return 0;
+@@ -515,7 +381,42 @@
+ {
+ 	struct intel_context *ctx = p;
+ 
++	if (ctx->ppgtt) {
++		struct list_head *list;
++		struct i915_vma *vma;
++
++		/* Decouple the remaining vma to keep the next lookup fast */
++		list = &ctx->ppgtt->base.vma_list;
++		while (!list_empty(list)) {
++			vma = list_first_entry(list, typeof(*vma), vm_link);
++			list_del_init(&vma->vm_link);
++			list_del_init(&vma->obj_link);
++			i915_vma_put(vma);
++		}
++
++		/* Drop active references to this vm upon retire */
++		ctx->ppgtt->base.closed = true;
++
++		/* Drop all inactive references (via vma->vm reference) */
++		list = &ctx->ppgtt->base.inactive_list;
++		while (!list_empty(list)) {
++			struct drm_i915_gem_object *obj;
++			int ret;
++
++			vma = list_first_entry(list, typeof(*vma), mm_list);
++			obj = vma->obj;
++
++			drm_gem_object_reference(&obj->base);
++			ret = i915_vma_unbind(vma);
++			drm_gem_object_unreference(&obj->base);
++			if (WARN_ON(ret))
++				break;
++		}
++	}
++
++	ctx->file_priv = NULL;
+ 	i915_gem_context_unreference(ctx);
++
+ 	return 0;
+ }
+ 
+@@ -527,7 +428,7 @@
+ 	idr_init(&file_priv->context_idr);
+ 
+ 	mutex_lock(&dev->struct_mutex);
+-	ctx = i915_gem_create_context(dev, file_priv, USES_FULL_PPGTT(dev));
++	ctx = i915_gem_create_context(dev, file_priv, true);
+ 	mutex_unlock(&dev->struct_mutex);
+ 
+ 	if (IS_ERR(ctx)) {
+@@ -559,136 +460,173 @@
+ }
+ 
+ static inline int
+-mi_set_context(struct intel_engine_cs *ring,
+-	       struct intel_context *new_context,
+-	       u32 hw_flags)
++mi_set_context(struct i915_gem_request *rq,
++	       struct intel_engine_context *new_context,
++	       u32 flags)
+ {
+-	int ret;
++	struct intel_ringbuffer *ring;
++	int len;
+ 
+ 	/* w/a: If Flush TLB Invalidation Mode is enabled, driver must do a TLB
+ 	 * invalidation prior to MI_SET_CONTEXT. On GEN6 we don't set the value
+-	 * explicitly, so we rely on the value at ring init, stored in
++	 * explicitly, so we rely on the value at engine init, stored in
+ 	 * itlb_before_ctx_switch.
+ 	 */
+-	if (IS_GEN6(ring->dev)) {
+-		ret = ring->flush(ring, I915_GEM_GPU_DOMAINS, 0);
++	if (IS_GEN6(rq->i915))
++		rq->pending_flush |= I915_INVALIDATE_CACHES;
++	if ((flags & MI_RESTORE_INHIBIT) == 0) {
++		int ret = i915_request_emit_flush(rq, I915_COMMAND_BARRIER);
+ 		if (ret)
+ 			return ret;
+ 	}
+ 
+-	ret = intel_ring_begin(ring, 6);
+-	if (ret)
+-		return ret;
++	len = 3;
++	switch (INTEL_INFO(rq->i915)->gen) {
++	case 8:
++	case 7:
++	case 5: len += 2;
++		break;
++	}
++
++	ring = intel_ring_begin(rq, len);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+-	/* WaProgramMiArbOnOffAroundMiSetContext:ivb,vlv,hsw,bdw,chv */
+-	if (INTEL_INFO(ring->dev)->gen >= 7)
++	switch (INTEL_INFO(rq->i915)->gen) {
++	case 8:
++	case 7:
++		/* WaProgramMiArbOnOffAroundMiSetContext:ivb,vlv,hsw,bdw,chv */
+ 		intel_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_DISABLE);
+-	else
+-		intel_ring_emit(ring, MI_NOOP);
++		break;
++	case 5:
++		intel_ring_emit(ring, MI_SUSPEND_FLUSH | MI_SUSPEND_FLUSH_EN);
++		break;
++	}
+ 
+-	intel_ring_emit(ring, MI_NOOP);
+ 	intel_ring_emit(ring, MI_SET_CONTEXT);
+-	intel_ring_emit(ring, i915_gem_obj_ggtt_offset(new_context->legacy_hw_ctx.rcs_state) |
++	intel_ring_emit(ring,
++			i915_gem_obj_ggtt_offset(new_context->state) |
+ 			MI_MM_SPACE_GTT |
+-			MI_SAVE_EXT_STATE_EN |
+-			MI_RESTORE_EXT_STATE_EN |
+-			hw_flags);
++			flags);
+ 	/*
+ 	 * w/a: MI_SET_CONTEXT must always be followed by MI_NOOP
+ 	 * WaMiSetContext_Hang:snb,ivb,vlv
+ 	 */
+ 	intel_ring_emit(ring, MI_NOOP);
+ 
+-	if (INTEL_INFO(ring->dev)->gen >= 7)
++	switch (INTEL_INFO(rq->i915)->gen) {
++	case 8:
++	case 7:
+ 		intel_ring_emit(ring, MI_ARB_ON_OFF | MI_ARB_ENABLE);
+-	else
+-		intel_ring_emit(ring, MI_NOOP);
++		break;
++	case 5:
++		intel_ring_emit(ring, MI_SUSPEND_FLUSH);
++		break;
++	}
+ 
+ 	intel_ring_advance(ring);
+ 
+-	return ret;
++	rq->pending_flush &= ~I915_COMMAND_BARRIER;
++	return 0;
+ }
+ 
+-static int do_switch(struct intel_engine_cs *ring,
+-		     struct intel_context *to)
++static int l3_remap(struct i915_gem_request *rq, int slice)
+ {
+-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
+-	struct intel_context *from = ring->last_context;
+-	struct i915_hw_ppgtt *ppgtt = ctx_to_ppgtt(to);
+-	u32 hw_flags = 0;
+-	bool uninitialized = false;
+-	int ret, i;
+-
+-	if (from != NULL && ring == &dev_priv->ring[RCS]) {
+-		BUG_ON(from->legacy_hw_ctx.rcs_state == NULL);
+-		BUG_ON(!i915_gem_obj_is_pinned(from->legacy_hw_ctx.rcs_state));
+-	}
++	const u32 reg_base = GEN7_L3LOG_BASE + (slice * 0x200);
++	const u32 *remap_info;
++	struct intel_ringbuffer *ring;
++	int i;
+ 
+-	if (from == to && !to->remap_slice)
++	remap_info = rq->i915->l3_parity.remap_info[slice];
++	if (remap_info == NULL)
+ 		return 0;
+ 
+-	/* Trying to pin first makes error handling easier. */
+-	if (ring == &dev_priv->ring[RCS]) {
+-		ret = i915_gem_obj_ggtt_pin(to->legacy_hw_ctx.rcs_state,
+-					    get_context_alignment(ring->dev), 0);
+-		if (ret)
+-			return ret;
+-	}
++	ring = intel_ring_begin(rq, GEN7_L3LOG_SIZE / 4 * 3);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+ 	/*
+-	 * Pin can switch back to the default context if we end up calling into
+-	 * evict_everything - as a last ditch gtt defrag effort that also
+-	 * switches to the default context. Hence we need to reload from here.
++	 * Note: We do not worry about the concurrent register cacheline hang
++	 * here because no other code should access these registers other than
++	 * at initialization time.
+ 	 */
+-	from = ring->last_context;
+-
+-	if (USES_FULL_PPGTT(ring->dev)) {
+-		ret = ppgtt->switch_mm(ppgtt, ring, false);
+-		if (ret)
+-			goto unpin_out;
++	for (i = 0; i < GEN7_L3LOG_SIZE; i += 4) {
++		intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
++		intel_ring_emit(ring, reg_base + i);
++		intel_ring_emit(ring, remap_info[i/4]);
+ 	}
+ 
+-	if (ring != &dev_priv->ring[RCS]) {
+-		if (from)
+-			i915_gem_context_unreference(from);
+-		goto done;
+-	}
++	intel_ring_advance(ring);
++	return 0;
++}
+ 
+-	/*
+-	 * Clear this page out of any CPU caches for coherent swap-in/out. Note
+-	 * that thanks to write = false in this call and us not setting any gpu
+-	 * write domains when putting a context object onto the active list
+-	 * (when switching away from it), this won't block.
++/**
++ * i915_request_switch_context() - perform a GPU context switch.
++ * @rq: request and ring/ctx for which we'll execute the context switch
++ *
++ * The context life cycle is simple. The context refcount is incremented and
++ * decremented by 1 and create and destroy. If the context is in use by the GPU,
++ * it will have a refoucnt > 1. This allows us to destroy the context abstract
++ * object while letting the normal object tracking destroy the backing BO.
++ */
++int i915_request_switch_context(struct i915_gem_request *rq)
++{
++	struct intel_context *to = rq->ctx;
++	struct intel_context *from = rq->engine->last_context;
++	int ret, i;
++
++	lockdep_assert_held(&rq->i915->dev->struct_mutex);
++
++	if (from == to && !to->remap_slice)
++		return 0;
++
++	/* With execlists enabled, the ring, vm and logical state are
++	 * interwined and we do not need to explicitly load the mm or
++	 * logical state as it is loaded along with the LRCA.
+ 	 *
+-	 * XXX: We need a real interface to do this instead of trickery.
++	 * But we still want to pin the state (for global usage tracking)
++	 * whilst in use and reload the l3 mapping if it has changed.
+ 	 */
+-	ret = i915_gem_object_set_to_gtt_domain(to->legacy_hw_ctx.rcs_state, false);
+-	if (ret)
+-		goto unpin_out;
++	if (!rq->engine->execlists_enabled) {
++		struct intel_engine_context *ctx = &to->ring[rq->engine->id];
+ 
+-	if (!to->legacy_hw_ctx.rcs_state->has_global_gtt_mapping) {
+-		struct i915_vma *vma = i915_gem_obj_to_vma(to->legacy_hw_ctx.rcs_state,
+-							   &dev_priv->gtt.base);
+-		vma->bind_vma(vma, to->legacy_hw_ctx.rcs_state->cache_level, GLOBAL_BIND);
+-	}
++		if (ctx->state != (from ? from->ring[rq->engine->id].state : NULL)) {
++			u32 flags;
+ 
+-	if (!to->legacy_hw_ctx.initialized || i915_gem_context_is_default(to))
+-		hw_flags |= MI_RESTORE_INHIBIT;
++			flags = 0;
++			if (!ctx->initialized || i915_gem_context_is_default(to))
++				flags |= MI_RESTORE_INHIBIT;
++
++			/* These flags are for resource streamer on HSW+ */
++			if (!IS_HASWELL(rq->i915) && INTEL_INFO(rq->i915)->gen < 8) {
++				if (ctx->initialized)
++					flags |= MI_RESTORE_EXT_STATE_EN;
++				flags |= MI_SAVE_EXT_STATE_EN;
++			}
+ 
+-	ret = mi_set_context(ring, to, hw_flags);
+-	if (ret)
+-		goto unpin_out;
++			trace_i915_gem_ring_switch_context(rq->engine, to, flags);
++			ret = mi_set_context(rq, ctx, flags);
++			if (ret)
++				return ret;
++
++			rq->pending_flush &= ~I915_COMMAND_BARRIER;
++		}
++
++		if (to->ppgtt) {
++			trace_i915_gem_request_switch_mm(rq);
++			ret = to->ppgtt->switch_mm(rq, to->ppgtt);
++			if (ret)
++				return ret;
++		}
++	}
+ 
+ 	for (i = 0; i < MAX_L3_SLICES; i++) {
+-		if (!(to->remap_slice & (1<<i)))
++		if (!(to->remap_slice & (1 << i)))
+ 			continue;
+ 
+-		ret = i915_gem_l3_remap(ring, i);
+ 		/* If it failed, try again next round */
+-		if (ret)
+-			DRM_DEBUG_DRIVER("L3 remapping failed\n");
+-		else
+-			to->remap_slice &= ~(1<<i);
++		if (l3_remap(rq, i) == 0)
++			rq->remap_l3 |= 1 << i;
+ 	}
+ 
+ 	/* The backing object for the context is done after switching to the
+@@ -697,78 +635,36 @@
+ 	 * is a bit suboptimal because the retiring can occur simply after the
+ 	 * MI_SET_CONTEXT instead of when the next seqno has completed.
+ 	 */
+-	if (from != NULL) {
+-		from->legacy_hw_ctx.rcs_state->base.read_domains = I915_GEM_DOMAIN_INSTRUCTION;
+-		i915_vma_move_to_active(i915_gem_obj_to_ggtt(from->legacy_hw_ctx.rcs_state), ring);
+-		/* As long as MI_SET_CONTEXT is serializing, ie. it flushes the
+-		 * whole damn pipeline, we don't need to explicitly mark the
+-		 * object dirty. The only exception is that the context must be
+-		 * correct in case the object gets swapped out. Ideally we'd be
+-		 * able to defer doing this until we know the object would be
+-		 * swapped, but there is no way to do that yet.
+-		 */
+-		from->legacy_hw_ctx.rcs_state->dirty = 1;
+-		BUG_ON(from->legacy_hw_ctx.rcs_state->ring != ring);
+-
+-		/* obj is kept alive until the next request by its active ref */
+-		i915_gem_object_ggtt_unpin(from->legacy_hw_ctx.rcs_state);
+-		i915_gem_context_unreference(from);
+-	}
+-
+-	uninitialized = !to->legacy_hw_ctx.initialized && from == NULL;
+-	to->legacy_hw_ctx.initialized = true;
+-
+-done:
+-	i915_gem_context_reference(to);
+-	ring->last_context = to;
+-
+-	if (uninitialized) {
+-		ret = i915_gem_render_state_init(ring);
+-		if (ret)
+-			DRM_ERROR("init render state: %d\n", ret);
+-	}
++	if (from)
++		rq->engine->add_context(rq, from);
+ 
++	rq->has_ctx_switch = true;
+ 	return 0;
+-
+-unpin_out:
+-	if (ring->id == RCS)
+-		i915_gem_object_ggtt_unpin(to->legacy_hw_ctx.rcs_state);
+-	return ret;
+ }
+ 
+ /**
+- * i915_switch_context() - perform a GPU context switch.
+- * @ring: ring for which we'll execute the context switch
+- * @to: the context to switch to
+- *
+- * The context life cycle is simple. The context refcount is incremented and
+- * decremented by 1 and create and destroy. If the context is in use by the GPU,
+- * it will have a refoucnt > 1. This allows us to destroy the context abstract
+- * object while letting the normal object tracking destroy the backing BO.
++ * i915_request_switch_context__commit() - commit the context sitch
++ * @rq: request for which we have executed the context switch
+  */
+-int i915_switch_context(struct intel_engine_cs *ring,
+-			struct intel_context *to)
++void i915_request_switch_context__commit(struct i915_gem_request *rq)
+ {
+-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
++	lockdep_assert_held(&rq->i915->dev->struct_mutex);
+ 
+-	WARN_ON(!mutex_is_locked(&dev_priv->dev->struct_mutex));
++	if (!rq->has_ctx_switch)
++		return;
+ 
+-	if (to->legacy_hw_ctx.rcs_state == NULL) { /* We have the fake context */
+-		if (to != ring->last_context) {
+-			i915_gem_context_reference(to);
+-			if (ring->last_context)
+-				i915_gem_context_unreference(ring->last_context);
+-			ring->last_context = to;
+-		}
+-		return 0;
+-	}
++	rq->ctx->remap_slice &= ~rq->remap_l3;
++	rq->ctx->ring[rq->engine->id].initialized = true;
+ 
+-	return do_switch(ring, to);
++	rq->has_ctx_switch = false;
+ }
+ 
+-static bool hw_context_enabled(struct drm_device *dev)
++static bool contexts_enabled(struct drm_i915_private *dev_priv)
+ {
+-	return to_i915(dev)->hw_context_size;
++	if (RCS_ENGINE(dev_priv)->execlists_enabled)
++		return true;
++
++	return dev_priv->hw_context_size;
+ }
+ 
+ int i915_gem_context_create_ioctl(struct drm_device *dev, void *data,
+@@ -779,14 +675,14 @@
+ 	struct intel_context *ctx;
+ 	int ret;
+ 
+-	if (!hw_context_enabled(dev))
++	if (!contexts_enabled(to_i915(dev)))
+ 		return -ENODEV;
+ 
+ 	ret = i915_mutex_lock_interruptible(dev);
+ 	if (ret)
+ 		return ret;
+ 
+-	ctx = i915_gem_create_context(dev, file_priv, USES_FULL_PPGTT(dev));
++	ctx = i915_gem_create_context(dev, file_priv, false);
+ 	mutex_unlock(&dev->struct_mutex);
+ 	if (IS_ERR(ctx))
+ 		return PTR_ERR(ctx);
+@@ -819,9 +715,142 @@
+ 	}
+ 
+ 	idr_remove(&ctx->file_priv->context_idr, ctx->user_handle);
+-	i915_gem_context_unreference(ctx);
++	context_idr_cleanup(ctx->user_handle, ctx, NULL);
+ 	mutex_unlock(&dev->struct_mutex);
+ 
+ 	DRM_DEBUG_DRIVER("HW context %d destroyed\n", args->ctx_id);
+ 	return 0;
+ }
++
++int i915_gem_context_getparam_ioctl(struct drm_device *dev, void *data,
++				    struct drm_file *file)
++{
++	struct drm_i915_file_private *file_priv = file->driver_priv;
++	struct drm_i915_gem_context_param *args = data;
++	struct intel_context *ctx;
++	int ret;
++
++	ret = i915_mutex_lock_interruptible(dev);
++	if (ret)
++		return ret;
++
++	ctx = i915_gem_context_get(file_priv, args->ctx_id);
++	if (IS_ERR(ctx)) {
++		mutex_unlock(&dev->struct_mutex);
++		return PTR_ERR(ctx);
++	}
++
++	args->size = 0;
++	switch (args->param) {
++	case I915_CONTEXT_PARAM_BAN_PERIOD:
++		args->value = ctx->hang_stats.ban_period_seconds;
++		break;
++	default:
++		ret = -EINVAL;
++		break;
++	}
++	mutex_unlock(&dev->struct_mutex);
++
++	return ret;
++}
++
++int i915_gem_context_setparam_ioctl(struct drm_device *dev, void *data,
++				    struct drm_file *file)
++{
++	struct drm_i915_file_private *file_priv = file->driver_priv;
++	struct drm_i915_gem_context_param *args = data;
++	struct intel_context *ctx;
++	int ret;
++
++	ret = i915_mutex_lock_interruptible(dev);
++	if (ret)
++		return ret;
++
++	ctx = i915_gem_context_get(file_priv, args->ctx_id);
++	if (IS_ERR(ctx)) {
++		mutex_unlock(&dev->struct_mutex);
++		return PTR_ERR(ctx);
++	}
++
++	switch (args->param) {
++	case I915_CONTEXT_PARAM_BAN_PERIOD:
++		if (args->size)
++			ret = -EINVAL;
++		else if (args->value < ctx->hang_stats.ban_period_seconds &&
++			 !capable(CAP_SYS_ADMIN))
++			ret = -EPERM;
++		else
++			ctx->hang_stats.ban_period_seconds = args->value;
++		break;
++	default:
++		ret = -EINVAL;
++		break;
++	}
++	mutex_unlock(&dev->struct_mutex);
++
++	return ret;
++}
++
++int i915_gem_context_dump_ioctl(struct drm_device *dev, void *data,
++				struct drm_file *file)
++{
++	struct drm_i915_file_private *file_priv = file->driver_priv;
++	struct drm_i915_gem_context_dump *args = data;
++	struct intel_context *ctx;
++	int ret;
++
++	ret = i915_mutex_lock_interruptible(dev);
++	if (ret)
++		return ret;
++
++	ctx = i915_gem_context_get(file_priv, args->ctx_id);
++	if (IS_ERR(ctx)) {
++		mutex_unlock(&dev->struct_mutex);
++		return PTR_ERR(ctx);
++	}
++
++	if (args->size >= ctx->ring[RCS].state->base.size) {
++		struct intel_engine_cs *rcs = RCS_ENGINE(dev);
++		struct drm_i915_gem_object *obj = ctx->ring[RCS].state;
++		struct sg_page_iter sg_iter;
++		struct i915_gem_request *rq;
++		int n = 0;
++
++		rq = i915_request_create(ctx, rcs);
++		if (IS_ERR(rq))
++			ret = PTR_ERR(rq);
++
++		/* Force a restore to itself in order to save the ctx */
++		if (ret == 0)
++			ret = mi_set_context(rq, &ctx->ring[RCS], 0);
++		if (ret == 0)
++			ret = i915_request_wait(rq);
++		i915_request_put(rq);
++		if (ret) {
++			mutex_unlock(&dev->struct_mutex);
++			return ret;
++		}
++
++		for_each_sg_page(obj->pages->sgl, &sg_iter, obj->pages->nents, 0) {
++			struct page *page = sg_page_iter_page(&sg_iter);
++			void *vaddr;
++
++			vaddr = kmap(page);
++			drm_clflush_virt_range(vaddr, PAGE_SIZE);
++			ret = copy_to_user(to_user_ptr(args->ptr + n),
++					   vaddr, PAGE_SIZE);
++			kunmap(page);
++			if (ret) {
++				mutex_unlock(&dev->struct_mutex);
++				return -EFAULT;
++			}
++
++			n += PAGE_SIZE;
++		}
++	}
++
++	args->size = ctx->ring[RCS].state->base.size;
++	mutex_unlock(&dev->struct_mutex);
++
++	return 0;
++}
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_gem_evict.c b/drivers/gpu/drm/i915/i915_gem_evict.c
+--- a/drivers/gpu/drm/i915/i915_gem_evict.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/i915_gem_evict.c	2014-11-20 09:53:37.964762837 -0700
+@@ -39,10 +39,10 @@
+ 	if (vma->pin_count)
+ 		return false;
+ 
+-	if (WARN_ON(!list_empty(&vma->exec_list)))
++	if (WARN_ON(!list_empty(&vma->exec_link)))
+ 		return false;
+ 
+-	list_add(&vma->exec_list, unwind);
++	list_add(&vma->exec_link, unwind);
+ 	return drm_mm_scan_add_block(&vma->node);
+ }
+ 
+@@ -130,11 +130,11 @@
+ 	while (!list_empty(&unwind_list)) {
+ 		vma = list_first_entry(&unwind_list,
+ 				       struct i915_vma,
+-				       exec_list);
++				       exec_link);
+ 		ret = drm_mm_scan_remove_block(&vma->node);
+ 		BUG_ON(ret);
+ 
+-		list_del_init(&vma->exec_list);
++		list_del_init(&vma->exec_link);
+ 	}
+ 
+ 	/* Can we unpin some objects such as idle hw contents,
+@@ -167,24 +167,73 @@
+ 	while (!list_empty(&unwind_list)) {
+ 		vma = list_first_entry(&unwind_list,
+ 				       struct i915_vma,
+-				       exec_list);
+-		if (drm_mm_scan_remove_block(&vma->node)) {
+-			list_move(&vma->exec_list, &eviction_list);
+-			drm_gem_object_reference(&vma->obj->base);
++				       exec_link);
++		list_del_init(&vma->exec_link);
++
++		if (drm_mm_scan_remove_block(&vma->node))
++			list_add(&i915_vma_get(vma)->exec_link,
++				  &eviction_list);
++	}
++
++	/* Unbinding will emit any required flushes */
++	while (!list_empty(&eviction_list)) {
++		vma = list_first_entry(&eviction_list,
++				       struct i915_vma,
++				       exec_link);
++		list_del_init(&vma->exec_link);
++
++		if (ret == 0)
++			ret = i915_vma_unbind(vma);
++
++		i915_vma_put(vma);
++	}
++
++	return ret;
++}
++
++int
++i915_gem_evict_range(struct drm_device *dev, struct i915_address_space *vm,
++		     unsigned long start, unsigned long end)
++{
++	struct drm_mm_node *node;
++	struct list_head eviction_list;
++	int ret = 0;
++
++	INIT_LIST_HEAD(&eviction_list);
++	drm_mm_for_each_node(node, &vm->mm) {
++		struct i915_vma *vma;
++
++		if (node->start + node->size <= start)
+ 			continue;
++		if (node->start >= end)
++			break;
++
++		vma = container_of(node, typeof(*vma), node);
++		if (vma->pin_count) {
++			ret = -EBUSY;
++			break;
++		}
++
++		if (WARN_ON(!list_empty(&vma->exec_link))) {
++			ret = -EINVAL;
++			break;
+ 		}
+-		list_del_init(&vma->exec_list);
++
++		drm_gem_object_reference(&vma->obj->base);
++		list_add(&vma->exec_link, &eviction_list);
+ 	}
+ 
+-	/* Unbinding will emit any required flushes */
+ 	while (!list_empty(&eviction_list)) {
++		struct i915_vma *vma;
+ 		struct drm_gem_object *obj;
++
+ 		vma = list_first_entry(&eviction_list,
+ 				       struct i915_vma,
+-				       exec_list);
++				       exec_link);
+ 
+-		obj =  &vma->obj->base;
+-		list_del_init(&vma->exec_list);
++		obj = &vma->obj->base;
++
++		list_del_init(&vma->exec_link);
+ 		if (ret == 0)
+ 			ret = i915_vma_unbind(vma);
+ 
+@@ -243,18 +292,10 @@
+ i915_gem_evict_everything(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct i915_address_space *vm;
+-	bool lists_empty = true;
++	struct list_head still_in_list;
+ 	int ret;
+ 
+-	list_for_each_entry(vm, &dev_priv->vm_list, global_link) {
+-		lists_empty = (list_empty(&vm->inactive_list) &&
+-			       list_empty(&vm->active_list));
+-		if (!lists_empty)
+-			lists_empty = false;
+-	}
+-
+-	if (lists_empty)
++	if (list_empty(&dev_priv->mm.bound_list))
+ 		return -ENOSPC;
+ 
+ 	trace_i915_gem_evict_everything(dev);
+@@ -270,8 +311,32 @@
+ 	i915_gem_retire_requests(dev);
+ 
+ 	/* Having flushed everything, unbind() should never raise an error */
+-	list_for_each_entry(vm, &dev_priv->vm_list, global_link)
+-		WARN_ON(i915_gem_evict_vm(vm, false));
++	INIT_LIST_HEAD(&still_in_list);
++	while (!list_empty(&dev_priv->mm.bound_list)) {
++		struct list_head vma_list;
++		struct drm_i915_gem_object *obj;
++
++		obj = list_first_entry(&dev_priv->mm.bound_list,
++				       typeof(*obj), global_list);
++		list_move_tail(&obj->global_list, &still_in_list);
++
++		drm_gem_object_reference(&obj->base);
++
++		INIT_LIST_HEAD(&vma_list);
++		while (!list_empty(&obj->vma_list)) {
++			struct i915_vma *vma;
++
++			vma = list_first_entry(&obj->vma_list,
++					       typeof(*vma), obj_link);
++			list_move_tail(&vma->obj_link, &vma_list);
++			if (i915_vma_unbind(vma))
++				break;
++		}
++		list_splice(&vma_list, &obj->vma_list);
++
++		drm_gem_object_unreference(&obj->base);
++	}
++	list_splice(&still_in_list, &dev_priv->mm.bound_list);
+ 
+ 	return 0;
+ }
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_gem_execbuffer.c b/drivers/gpu/drm/i915/i915_gem_execbuffer.c
+--- a/drivers/gpu/drm/i915/i915_gem_execbuffer.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/i915_gem_execbuffer.c	2014-11-20 09:53:37.964762837 -0700
+@@ -35,12 +35,15 @@
+ 
+ #define  __EXEC_OBJECT_HAS_PIN (1<<31)
+ #define  __EXEC_OBJECT_HAS_FENCE (1<<30)
++#define  __EXEC_OBJECT_NEEDS_MAP (1<<29)
+ #define  __EXEC_OBJECT_NEEDS_BIAS (1<<28)
+ 
+ #define BATCH_OFFSET_BIAS (256*1024)
+ 
+ struct eb_vmas {
+ 	struct list_head vmas;
++	struct i915_address_space *vm;
++	struct i915_vma *batch;
+ 	int and;
+ 	union {
+ 		struct i915_vma *lut[0];
+@@ -87,14 +90,33 @@
+ 		memset(eb->buckets, 0, (eb->and+1)*sizeof(struct hlist_head));
+ }
+ 
++static struct i915_vma *
++eb_get_batch(struct eb_vmas *eb)
++{
++	struct i915_vma *vma =
++		list_entry(eb->vmas.prev, typeof(*vma), exec_link);
++
++	/*
++	 * SNA is doing fancy tricks with compressing batch buffers, which leads
++	 * to negative relocation deltas. Usually that works out ok since the
++	 * relocate address is still positive, except when the batch is placed
++	 * very low in the GTT. Ensure this doesn't happen.
++	 *
++	 * Note that actual hangs have only been observed on gen7, but for
++	 * paranoia do it everywhere.
++	 */
++	if ((vma->exec_entry->flags & EXEC_OBJECT_PINNED) == 0)
++		vma->exec_entry->flags |= __EXEC_OBJECT_NEEDS_BIAS;
++
++	return vma;
++}
++
+ static int
+ eb_lookup_vmas(struct eb_vmas *eb,
+ 	       struct drm_i915_gem_exec_object2 *exec,
+ 	       const struct drm_i915_gem_execbuffer2 *args,
+-	       struct i915_address_space *vm,
+ 	       struct drm_file *file)
+ {
+-	struct drm_i915_private *dev_priv = vm->dev->dev_private;
+ 	struct drm_i915_gem_object *obj;
+ 	struct list_head objects;
+ 	int i, ret;
+@@ -129,20 +151,6 @@
+ 	i = 0;
+ 	while (!list_empty(&objects)) {
+ 		struct i915_vma *vma;
+-		struct i915_address_space *bind_vm = vm;
+-
+-		if (exec[i].flags & EXEC_OBJECT_NEEDS_GTT &&
+-		    USES_FULL_PPGTT(vm->dev)) {
+-			ret = -EINVAL;
+-			goto err;
+-		}
+-
+-		/* If we have secure dispatch, or the userspace assures us that
+-		 * they know what they're doing, use the GGTT VM.
+-		 */
+-		if (((args->flags & I915_EXEC_SECURE) &&
+-		    (i == (args->buffer_count - 1))))
+-			bind_vm = &dev_priv->gtt.base;
+ 
+ 		obj = list_first_entry(&objects,
+ 				       struct drm_i915_gem_object,
+@@ -156,7 +164,7 @@
+ 		 * from the (obj, vm) we don't run the risk of creating
+ 		 * duplicated vmas for the same vm.
+ 		 */
+-		vma = i915_gem_obj_lookup_or_create_vma(obj, bind_vm);
++		vma = i915_gem_obj_get_vma(obj, eb->vm);
+ 		if (IS_ERR(vma)) {
+ 			DRM_DEBUG("Failed to lookup VMA\n");
+ 			ret = PTR_ERR(vma);
+@@ -164,7 +172,7 @@
+ 		}
+ 
+ 		/* Transfer ownership from the objects list to the vmas list. */
+-		list_add_tail(&vma->exec_list, &eb->vmas);
++		list_add_tail(&vma->exec_link, &eb->vmas);
+ 		list_del_init(&obj->obj_exec_link);
+ 
+ 		vma->exec_entry = &exec[i];
+@@ -179,6 +187,9 @@
+ 		++i;
+ 	}
+ 
++	/* take note of the batch buffer before we might reorder the lists */
++	eb->batch = eb_get_batch(eb);
++
+ 	return 0;
+ 
+ 
+@@ -221,37 +232,39 @@
+ }
+ 
+ static void
+-i915_gem_execbuffer_unreserve_vma(struct i915_vma *vma)
++__i915_vma_unreserve(struct i915_vma *vma)
+ {
+-	struct drm_i915_gem_exec_object2 *entry;
+-	struct drm_i915_gem_object *obj = vma->obj;
+-
+-	if (!drm_mm_node_allocated(&vma->node))
+-		return;
+-
+-	entry = vma->exec_entry;
++	struct drm_i915_gem_exec_object2 *entry = vma->exec_entry;
+ 
+ 	if (entry->flags & __EXEC_OBJECT_HAS_FENCE)
+-		i915_gem_object_unpin_fence(obj);
++		i915_gem_object_unpin_fence(vma->obj);
+ 
+-	if (entry->flags & __EXEC_OBJECT_HAS_PIN)
++	if (entry->flags & __EXEC_OBJECT_HAS_PIN) {
++		BUG_ON(vma->pin_count == 0);
+ 		vma->pin_count--;
++	}
+ 
+ 	entry->flags &= ~(__EXEC_OBJECT_HAS_FENCE | __EXEC_OBJECT_HAS_PIN);
+ }
+ 
+-static void eb_destroy(struct eb_vmas *eb)
++void
++i915_vma_unreserve(struct i915_vma *vma)
+ {
+-	while (!list_empty(&eb->vmas)) {
+-		struct i915_vma *vma;
+-
+-		vma = list_first_entry(&eb->vmas,
+-				       struct i915_vma,
+-				       exec_list);
+-		list_del_init(&vma->exec_list);
+-		i915_gem_execbuffer_unreserve_vma(vma);
+-		drm_gem_object_unreference(&vma->obj->base);
++	list_del_init(&vma->exec_link);
++	if (vma->exec_entry) {
++		__i915_vma_unreserve(vma);
++		vma->exec_entry = NULL;
+ 	}
++	drm_gem_object_unreference(&vma->obj->base);
++	i915_vma_put(vma);
++}
++
++static void eb_destroy(struct eb_vmas *eb)
++{
++	while (!list_empty(&eb->vmas))
++		i915_vma_unreserve(list_first_entry(&eb->vmas,
++						    struct i915_vma,
++						    exec_link));
+ 	kfree(eb);
+ }
+ 
+@@ -270,7 +283,7 @@
+ {
+ 	struct drm_device *dev = obj->base.dev;
+ 	uint32_t page_offset = offset_in_page(reloc->offset);
+-	uint64_t delta = reloc->delta + target_offset;
++	uint64_t delta = (int)reloc->delta + target_offset;
+ 	char *vaddr;
+ 	int ret;
+ 
+@@ -306,8 +319,8 @@
+ {
+ 	struct drm_device *dev = obj->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	uint64_t delta = reloc->delta + target_offset;
+-	uint32_t __iomem *reloc_entry;
++	uint64_t delta = (int)reloc->delta + target_offset;
++	uint64_t offset;
+ 	void __iomem *reloc_page;
+ 	int ret;
+ 
+@@ -320,25 +333,24 @@
+ 		return ret;
+ 
+ 	/* Map the page containing the relocation we're going to perform.  */
+-	reloc->offset += i915_gem_obj_ggtt_offset(obj);
++	offset = i915_gem_obj_ggtt_offset(obj);
++	offset += reloc->offset;
+ 	reloc_page = io_mapping_map_atomic_wc(dev_priv->gtt.mappable,
+-			reloc->offset & PAGE_MASK);
+-	reloc_entry = (uint32_t __iomem *)
+-		(reloc_page + offset_in_page(reloc->offset));
+-	iowrite32(lower_32_bits(delta), reloc_entry);
++					      offset & PAGE_MASK);
++	iowrite32(lower_32_bits(delta), reloc_page + offset_in_page(offset));
+ 
+ 	if (INTEL_INFO(dev)->gen >= 8) {
+-		reloc_entry += 1;
++		offset += sizeof(uint32_t);
+ 
+-		if (offset_in_page(reloc->offset + sizeof(uint32_t)) == 0) {
++		if (offset_in_page(offset) == 0) {
+ 			io_mapping_unmap_atomic(reloc_page);
+-			reloc_page = io_mapping_map_atomic_wc(
+-					dev_priv->gtt.mappable,
+-					reloc->offset + sizeof(uint32_t));
+-			reloc_entry = reloc_page;
++			reloc_page =
++				io_mapping_map_atomic_wc(dev_priv->gtt.mappable,
++							 offset);
+ 		}
+ 
+-		iowrite32(upper_32_bits(delta), reloc_entry);
++		iowrite32(upper_32_bits(delta),
++			  reloc_page + offset_in_page(offset));
+ 	}
+ 
+ 	io_mapping_unmap_atomic(reloc_page);
+@@ -346,6 +358,24 @@
+ 	return 0;
+ }
+ 
++static bool obj_active(struct drm_i915_gem_object *obj)
++{
++	int i;
++
++	if (!obj->active)
++		return false;
++
++	for (i = 0; i < I915_NUM_ENGINES; i++) {
++		if (obj->last_read[i].request == NULL)
++			continue;
++
++		if (!i915_request_complete(obj->last_read[i].request))
++			return true;
++	}
++
++	return false;
++}
++
+ static int
+ i915_gem_execbuffer_relocate_entry(struct drm_i915_gem_object *obj,
+ 				   struct eb_vmas *eb,
+@@ -371,13 +401,8 @@
+ 	 * pipe_control writes because the gpu doesn't properly redirect them
+ 	 * through the ppgtt for non_secure batchbuffers. */
+ 	if (unlikely(IS_GEN6(dev) &&
+-	    reloc->write_domain == I915_GEM_DOMAIN_INSTRUCTION &&
+-	    !target_i915_obj->has_global_gtt_mapping)) {
+-		struct i915_vma *vma =
+-			list_first_entry(&target_i915_obj->vma_list,
+-					 typeof(*vma), vma_link);
+-		vma->bind_vma(vma, target_i915_obj->cache_level, GLOBAL_BIND);
+-	}
++		     reloc->write_domain == I915_GEM_DOMAIN_INSTRUCTION))
++		target_vma->exec_entry->flags |= EXEC_OBJECT_NEEDS_GTT;
+ 
+ 	/* Validate that the target is in a valid r/w GPU domain */
+ 	if (unlikely(reloc->write_domain & (reloc->write_domain - 1))) {
+@@ -430,20 +455,18 @@
+ 	}
+ 
+ 	/* We can't wait for rendering with pagefaults disabled */
+-	if (obj->active && in_atomic())
++	if (in_atomic() && obj_active(obj))
+ 		return -EFAULT;
+ 
+ 	if (use_cpu_reloc(obj))
+ 		ret = relocate_entry_cpu(obj, reloc, target_offset);
+ 	else
+ 		ret = relocate_entry_gtt(obj, reloc, target_offset);
+-
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* and update the user's relocation entry */
+ 	reloc->presumed_offset = target_offset;
+-
+ 	return 0;
+ }
+ 
+@@ -524,7 +547,7 @@
+ 	 * lockdep complains vehemently.
+ 	 */
+ 	pagefault_disable();
+-	list_for_each_entry(vma, &eb->vmas, exec_list) {
++	list_for_each_entry(vma, &eb->vmas, exec_link) {
+ 		ret = i915_gem_execbuffer_relocate_vma(vma, eb);
+ 		if (ret)
+ 			break;
+@@ -535,56 +558,38 @@
+ }
+ 
+ static int
+-need_reloc_mappable(struct i915_vma *vma)
+-{
+-	struct drm_i915_gem_exec_object2 *entry = vma->exec_entry;
+-	return entry->relocation_count && !use_cpu_reloc(vma->obj) &&
+-		i915_is_ggtt(vma->vm);
+-}
+-
+-static int
+ i915_gem_execbuffer_reserve_vma(struct i915_vma *vma,
+-				struct intel_engine_cs *ring,
++				struct intel_engine_cs *engine,
+ 				bool *need_reloc)
+ {
+ 	struct drm_i915_gem_object *obj = vma->obj;
+ 	struct drm_i915_gem_exec_object2 *entry = vma->exec_entry;
+-	bool has_fenced_gpu_access = INTEL_INFO(ring->dev)->gen < 4;
+-	bool need_fence;
+ 	uint64_t flags;
+ 	int ret;
+ 
+-	flags = 0;
+-
+-	need_fence =
+-		has_fenced_gpu_access &&
+-		entry->flags & EXEC_OBJECT_NEEDS_FENCE &&
+-		obj->tiling_mode != I915_TILING_NONE;
+-	if (need_fence || need_reloc_mappable(vma))
+-		flags |= PIN_MAPPABLE;
+-
++	flags = PIN_LOCAL;
++	if (entry->flags & __EXEC_OBJECT_NEEDS_MAP)
++		flags |= PIN_GLOBAL | PIN_MAPPABLE;
+ 	if (entry->flags & EXEC_OBJECT_NEEDS_GTT)
+ 		flags |= PIN_GLOBAL;
+ 	if (entry->flags & __EXEC_OBJECT_NEEDS_BIAS)
+ 		flags |= BATCH_OFFSET_BIAS | PIN_OFFSET_BIAS;
++	if (entry->flags & EXEC_OBJECT_PINNED)
++		flags |= entry->offset | PIN_OFFSET_FIXED;
+ 
+-	ret = i915_gem_object_pin(obj, vma->vm, entry->alignment, flags);
++	ret = i915_vma_pin(vma, entry->alignment, flags);
+ 	if (ret)
+ 		return ret;
+ 
+ 	entry->flags |= __EXEC_OBJECT_HAS_PIN;
+ 
+-	if (has_fenced_gpu_access) {
+-		if (entry->flags & EXEC_OBJECT_NEEDS_FENCE) {
+-			ret = i915_gem_object_get_fence(obj);
+-			if (ret)
+-				return ret;
+-
+-			if (i915_gem_object_pin_fence(obj))
+-				entry->flags |= __EXEC_OBJECT_HAS_FENCE;
++	if (entry->flags & EXEC_OBJECT_NEEDS_FENCE) {
++		ret = i915_gem_object_get_fence(obj);
++		if (ret)
++			return ret;
+ 
+-			obj->pending_fenced_gpu_access = true;
+-		}
++		if (i915_gem_object_pin_fence(obj))
++			entry->flags |= __EXEC_OBJECT_HAS_FENCE;
+ 	}
+ 
+ 	if (entry->offset != vma->node.start) {
+@@ -601,26 +606,44 @@
+ }
+ 
+ static bool
+-eb_vma_misplaced(struct i915_vma *vma, bool has_fenced_gpu_access)
++need_reloc_mappable(struct i915_vma *vma)
++{
++	struct drm_i915_gem_exec_object2 *entry = vma->exec_entry;
++
++	if (entry->relocation_count == 0)
++		return false;
++
++	if (!i915_is_ggtt(vma->vm))
++		return false;
++
++	/* See also use_cpu_reloc() */
++	if (HAS_LLC(vma->obj->base.dev))
++		return false;
++
++	if (vma->obj->base.write_domain == I915_GEM_DOMAIN_CPU)
++		return false;
++
++	return true;
++}
++
++static bool
++eb_vma_misplaced(struct i915_vma *vma)
+ {
+ 	struct drm_i915_gem_exec_object2 *entry = vma->exec_entry;
+ 	struct drm_i915_gem_object *obj = vma->obj;
+-	bool need_fence, need_mappable;
+-
+-	need_fence =
+-		has_fenced_gpu_access &&
+-		entry->flags & EXEC_OBJECT_NEEDS_FENCE &&
+-		obj->tiling_mode != I915_TILING_NONE;
+-	need_mappable = need_fence || need_reloc_mappable(vma);
+ 
+-	WARN_ON((need_mappable || need_fence) &&
++	WARN_ON(entry->flags & __EXEC_OBJECT_NEEDS_MAP &&
+ 	       !i915_is_ggtt(vma->vm));
+ 
+ 	if (entry->alignment &&
+ 	    vma->node.start & (entry->alignment - 1))
+ 		return true;
+ 
+-	if (need_mappable && !obj->map_and_fenceable)
++	if (entry->flags & EXEC_OBJECT_PINNED &&
++	    vma->node.start != entry->offset)
++		return true;
++
++	if (entry->flags & __EXEC_OBJECT_NEEDS_MAP && !obj->map_and_fenceable)
+ 		return true;
+ 
+ 	if (entry->flags & __EXEC_OBJECT_NEEDS_BIAS &&
+@@ -631,49 +654,44 @@
+ }
+ 
+ static int
+-i915_gem_execbuffer_reserve(struct intel_engine_cs *ring,
+-			    struct list_head *vmas,
++i915_gem_execbuffer_reserve(struct intel_engine_cs *engine,
++			    struct eb_vmas *eb,
+ 			    bool *need_relocs)
+ {
+ 	struct drm_i915_gem_object *obj;
+ 	struct i915_vma *vma;
+-	struct i915_address_space *vm;
+ 	struct list_head ordered_vmas;
+-	bool has_fenced_gpu_access = INTEL_INFO(ring->dev)->gen < 4;
++	bool has_fenced_gpu_access = INTEL_INFO(engine->i915)->gen < 4;
+ 	int retry;
+ 
+-	if (list_empty(vmas))
+-		return 0;
+-
+-	i915_gem_retire_requests_ring(ring);
+-
+-	vm = list_first_entry(vmas, struct i915_vma, exec_list)->vm;
++	i915_gem_retire_requests__engine(engine);
+ 
+ 	INIT_LIST_HEAD(&ordered_vmas);
+-	while (!list_empty(vmas)) {
++	while (!list_empty(&eb->vmas)) {
+ 		struct drm_i915_gem_exec_object2 *entry;
+ 		bool need_fence, need_mappable;
+ 
+-		vma = list_first_entry(vmas, struct i915_vma, exec_list);
++		vma = list_first_entry(&eb->vmas, struct i915_vma, exec_link);
+ 		obj = vma->obj;
+ 		entry = vma->exec_entry;
+ 
++		if (!has_fenced_gpu_access)
++			entry->flags &= ~EXEC_OBJECT_NEEDS_FENCE;
+ 		need_fence =
+-			has_fenced_gpu_access &&
+ 			entry->flags & EXEC_OBJECT_NEEDS_FENCE &&
+ 			obj->tiling_mode != I915_TILING_NONE;
+ 		need_mappable = need_fence || need_reloc_mappable(vma);
+ 
+-		if (need_mappable)
+-			list_move(&vma->exec_list, &ordered_vmas);
+-		else
+-			list_move_tail(&vma->exec_list, &ordered_vmas);
++		if (need_mappable) {
++			entry->flags |= __EXEC_OBJECT_NEEDS_MAP;
++			list_move(&vma->exec_link, &ordered_vmas);
++		} else
++			list_move_tail(&vma->exec_link, &ordered_vmas);
+ 
+ 		obj->base.pending_read_domains = I915_GEM_GPU_DOMAINS & ~I915_GEM_DOMAIN_COMMAND;
+ 		obj->base.pending_write_domain = 0;
+-		obj->pending_fenced_gpu_access = false;
+ 	}
+-	list_splice(&ordered_vmas, vmas);
++	list_splice(&ordered_vmas, &eb->vmas);
+ 
+ 	/* Attempt to pin all of the buffers into the GTT.
+ 	 * This is done in 3 phases:
+@@ -692,24 +710,24 @@
+ 		int ret = 0;
+ 
+ 		/* Unbind any ill-fitting objects or pin. */
+-		list_for_each_entry(vma, vmas, exec_list) {
++		list_for_each_entry(vma, &eb->vmas, exec_link) {
+ 			if (!drm_mm_node_allocated(&vma->node))
+ 				continue;
+ 
+-			if (eb_vma_misplaced(vma, has_fenced_gpu_access))
++			if (eb_vma_misplaced(vma))
+ 				ret = i915_vma_unbind(vma);
+ 			else
+-				ret = i915_gem_execbuffer_reserve_vma(vma, ring, need_relocs);
++				ret = i915_gem_execbuffer_reserve_vma(vma, engine, need_relocs);
+ 			if (ret)
+ 				goto err;
+ 		}
+ 
+ 		/* Bind fresh objects */
+-		list_for_each_entry(vma, vmas, exec_list) {
++		list_for_each_entry(vma, &eb->vmas, exec_link) {
+ 			if (drm_mm_node_allocated(&vma->node))
+ 				continue;
+ 
+-			ret = i915_gem_execbuffer_reserve_vma(vma, ring, need_relocs);
++			ret = i915_gem_execbuffer_reserve_vma(vma, engine, need_relocs);
+ 			if (ret)
+ 				goto err;
+ 		}
+@@ -719,45 +737,37 @@
+ 			return ret;
+ 
+ 		/* Decrement pin count for bound objects */
+-		list_for_each_entry(vma, vmas, exec_list)
+-			i915_gem_execbuffer_unreserve_vma(vma);
++		list_for_each_entry(vma, &eb->vmas, exec_link)
++			__i915_vma_unreserve(vma);
+ 
+-		ret = i915_gem_evict_vm(vm, true);
++		ret = i915_gem_evict_vm(eb->vm, true);
+ 		if (ret)
+ 			return ret;
+ 	} while (1);
+ }
+ 
+ static int
+-i915_gem_execbuffer_relocate_slow(struct drm_device *dev,
++i915_gem_execbuffer_relocate_slow(struct drm_i915_private *i915,
+ 				  struct drm_i915_gem_execbuffer2 *args,
+ 				  struct drm_file *file,
+-				  struct intel_engine_cs *ring,
++				  struct intel_engine_cs *engine,
+ 				  struct eb_vmas *eb,
+ 				  struct drm_i915_gem_exec_object2 *exec)
+ {
+ 	struct drm_i915_gem_relocation_entry *reloc;
+-	struct i915_address_space *vm;
+ 	struct i915_vma *vma;
+ 	bool need_relocs;
+ 	int *reloc_offset;
+ 	int i, total, ret;
+ 	unsigned count = args->buffer_count;
+ 
+-	if (WARN_ON(list_empty(&eb->vmas)))
+-		return 0;
+-
+-	vm = list_first_entry(&eb->vmas, struct i915_vma, exec_list)->vm;
+-
+ 	/* We may process another execbuffer during the unlock... */
+ 	while (!list_empty(&eb->vmas)) {
+-		vma = list_first_entry(&eb->vmas, struct i915_vma, exec_list);
+-		list_del_init(&vma->exec_list);
+-		i915_gem_execbuffer_unreserve_vma(vma);
+-		drm_gem_object_unreference(&vma->obj->base);
++		vma = list_first_entry(&eb->vmas, struct i915_vma, exec_link);
++		i915_vma_unreserve(vma);
+ 	}
+ 
+-	mutex_unlock(&dev->struct_mutex);
++	mutex_unlock(&i915->dev->struct_mutex);
+ 
+ 	total = 0;
+ 	for (i = 0; i < count; i++)
+@@ -768,7 +778,7 @@
+ 	if (reloc == NULL || reloc_offset == NULL) {
+ 		drm_free_large(reloc);
+ 		drm_free_large(reloc_offset);
+-		mutex_lock(&dev->struct_mutex);
++		mutex_lock(&i915->dev->struct_mutex);
+ 		return -ENOMEM;
+ 	}
+ 
+@@ -783,7 +793,7 @@
+ 		if (copy_from_user(reloc+total, user_relocs,
+ 				   exec[i].relocation_count * sizeof(*reloc))) {
+ 			ret = -EFAULT;
+-			mutex_lock(&dev->struct_mutex);
++			mutex_lock(&i915->dev->struct_mutex);
+ 			goto err;
+ 		}
+ 
+@@ -801,7 +811,7 @@
+ 					   &invalid_offset,
+ 					   sizeof(invalid_offset))) {
+ 				ret = -EFAULT;
+-				mutex_lock(&dev->struct_mutex);
++				mutex_lock(&i915->dev->struct_mutex);
+ 				goto err;
+ 			}
+ 		}
+@@ -810,24 +820,24 @@
+ 		total += exec[i].relocation_count;
+ 	}
+ 
+-	ret = i915_mutex_lock_interruptible(dev);
++	ret = i915_mutex_lock_interruptible(i915->dev);
+ 	if (ret) {
+-		mutex_lock(&dev->struct_mutex);
++		mutex_lock(&i915->dev->struct_mutex);
+ 		goto err;
+ 	}
+ 
+ 	/* reacquire the objects */
+ 	eb_reset(eb);
+-	ret = eb_lookup_vmas(eb, exec, args, vm, file);
++	ret = eb_lookup_vmas(eb, exec, args, file);
+ 	if (ret)
+ 		goto err;
+ 
+ 	need_relocs = (args->flags & I915_EXEC_NO_RELOC) == 0;
+-	ret = i915_gem_execbuffer_reserve(ring, &eb->vmas, &need_relocs);
++	ret = i915_gem_execbuffer_reserve(engine, eb, &need_relocs);
+ 	if (ret)
+ 		goto err;
+ 
+-	list_for_each_entry(vma, &eb->vmas, exec_list) {
++	list_for_each_entry(vma, &eb->vmas, exec_link) {
+ 		int offset = vma->exec_entry - exec;
+ 		ret = i915_gem_execbuffer_relocate_vma_slow(vma, eb,
+ 							    reloc + reloc_offset[offset]);
+@@ -848,17 +858,19 @@
+ }
+ 
+ static int
+-i915_gem_execbuffer_move_to_gpu(struct intel_engine_cs *ring,
+-				struct list_head *vmas)
++vmas_move_to_rq(struct eb_vmas *eb,
++		struct i915_gem_request *rq)
+ {
+ 	struct i915_vma *vma;
+ 	uint32_t flush_domains = 0;
+ 	bool flush_chipset = false;
+ 	int ret;
+ 
+-	list_for_each_entry(vma, vmas, exec_list) {
++	/* 1: flush/serialise damage from other sources */
++	list_for_each_entry(vma, &eb->vmas, exec_link) {
+ 		struct drm_i915_gem_object *obj = vma->obj;
+-		ret = i915_gem_object_sync(obj, ring);
++
++		ret = i915_gem_object_sync(obj, rq);
+ 		if (ret)
+ 			return ret;
+ 
+@@ -866,42 +878,65 @@
+ 			flush_chipset |= i915_gem_clflush_object(obj, false);
+ 
+ 		flush_domains |= obj->base.write_domain;
++		if (obj->last_read[rq->engine->id].request == NULL)
++			rq->pending_flush |= I915_INVALIDATE_CACHES;
+ 	}
+ 
+ 	if (flush_chipset)
+-		i915_gem_chipset_flush(ring->dev);
++		i915_gem_chipset_flush(rq->i915->dev);
+ 
+ 	if (flush_domains & I915_GEM_DOMAIN_GTT)
+ 		wmb();
+ 
+-	/* Unconditionally invalidate gpu caches and ensure that we do flush
+-	 * any residual writes from the previous batch.
+-	 */
+-	return intel_ring_invalidate_all_caches(ring);
+-}
++	/* 2: invalidate the caches from this ring after emitting semaphores */
++	ret = i915_request_emit_flush(rq, I915_INVALIDATE_CACHES);
++	if (ret)
++		return ret;
+ 
+-static bool
+-i915_gem_check_execbuffer(struct drm_i915_gem_execbuffer2 *exec)
+-{
+-	if (exec->flags & __I915_EXEC_UNKNOWN_FLAGS)
+-		return false;
++	/* 3: track flushes and objects for this rq */
++	while (!list_empty(&eb->vmas)) {
++		unsigned fenced;
+ 
+-	return ((exec->batch_start_offset | exec->batch_len) & 0x7) == 0;
++		vma = list_first_entry(&eb->vmas, typeof(*vma), exec_link);
++
++		fenced = 0;
++		if (vma->exec_entry->flags & EXEC_OBJECT_NEEDS_FENCE) {
++			fenced |= VMA_IS_FENCED;
++			if (vma->exec_entry->flags & __EXEC_OBJECT_HAS_FENCE)
++				fenced |= VMA_HAS_FENCE;
++		}
++
++		i915_request_add_vma(rq, vma, fenced);
++	}
++
++	return 0;
+ }
+ 
+ static int
+-validate_exec_list(struct drm_i915_gem_exec_object2 *exec,
+-		   int count)
++validate_exec_list(struct drm_i915_private *dev_priv,
++		   struct drm_i915_gem_execbuffer2 *args,
++		   struct drm_i915_gem_exec_object2 *exec)
+ {
+-	int i;
+ 	unsigned relocs_total = 0;
+ 	unsigned relocs_max = UINT_MAX / sizeof(struct drm_i915_gem_relocation_entry);
++	unsigned invalid_flags;
++	int i;
+ 
+-	for (i = 0; i < count; i++) {
++	if (args->flags & __I915_EXEC_UNKNOWN_FLAGS)
++		return -EINVAL;
++
++	if ((args->batch_start_offset | args->batch_len) & 0x7)
++		return -EINVAL;
++
++	invalid_flags = __EXEC_OBJECT_UNKNOWN_FLAGS;
++	if (USES_FULL_PPGTT(dev_priv))
++		invalid_flags |= EXEC_OBJECT_NEEDS_GTT;
++
++	for (i = 0; i < args->buffer_count; i++) {
+ 		char __user *ptr = to_user_ptr(exec[i].relocs_ptr);
+ 		int length; /* limited by fault_in_pages_readable() */
+ 
+-		if (exec[i].flags & __EXEC_OBJECT_UNKNOWN_FLAGS)
++		if (exec[i].flags & invalid_flags)
+ 			return -EINVAL;
+ 
+ 		/* First check for malicious input causing overflow in
+@@ -922,7 +957,7 @@
+ 		if (!access_ok(VERIFY_WRITE, ptr, length))
+ 			return -EFAULT;
+ 
+-		if (likely(!i915.prefault_disable)) {
++		if (likely(!i915_module.prefault_disable)) {
+ 			if (fault_in_multipages_readable(ptr, length))
+ 				return -EFAULT;
+ 		}
+@@ -932,13 +967,14 @@
+ }
+ 
+ static struct intel_context *
+-i915_gem_validate_context(struct drm_device *dev, struct drm_file *file,
+-			  struct intel_engine_cs *ring, const u32 ctx_id)
++i915_gem_validate_context(struct drm_file *file,
++			  struct intel_engine_cs *engine,
++			  const u32 ctx_id)
+ {
+ 	struct intel_context *ctx = NULL;
+ 	struct i915_ctx_hang_stats *hs;
+ 
+-	if (ring->id != RCS && ctx_id != DEFAULT_CONTEXT_HANDLE)
++	if (engine->id != RCS && ctx_id != DEFAULT_CONTEXT_HANDLE)
+ 		return ERR_PTR(-EINVAL);
+ 
+ 	ctx = i915_gem_context_get(file->driver_priv, ctx_id);
+@@ -954,109 +990,153 @@
+ 	return ctx;
+ }
+ 
+-static void
+-i915_gem_execbuffer_move_to_active(struct list_head *vmas,
+-				   struct intel_engine_cs *ring)
++static int
++reset_sol_offsets(struct i915_gem_request *rq)
+ {
+-	struct i915_vma *vma;
+-
+-	list_for_each_entry(vma, vmas, exec_list) {
+-		struct drm_i915_gem_object *obj = vma->obj;
+-		u32 old_read = obj->base.read_domains;
+-		u32 old_write = obj->base.write_domain;
++	struct intel_ringbuffer *ring;
++	int i;
+ 
+-		obj->base.write_domain = obj->base.pending_write_domain;
+-		if (obj->base.write_domain == 0)
+-			obj->base.pending_read_domains |= obj->base.read_domains;
+-		obj->base.read_domains = obj->base.pending_read_domains;
+-		obj->fenced_gpu_access = obj->pending_fenced_gpu_access;
++	if (!IS_GEN7(rq->i915) || rq->engine->id != RCS) {
++		DRM_DEBUG("sol reset is gen7/rcs only\n");
++		return -EINVAL;
++	}
+ 
+-		i915_vma_move_to_active(vma, ring);
+-		if (obj->base.write_domain) {
+-			obj->dirty = 1;
+-			obj->last_write_seqno = intel_ring_get_seqno(ring);
++	ring = intel_ring_begin(rq, 4 * 3);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+-			intel_fb_obj_invalidate(obj, ring);
++	for (i = 0; i < 4; i++) {
++		intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
++		intel_ring_emit(ring, GEN7_SO_WRITE_OFFSET(i));
++		intel_ring_emit(ring, 0);
++	}
+ 
+-			/* update for the implicit flush after a batch */
+-			obj->base.write_domain &= ~I915_GEM_GPU_DOMAINS;
+-		}
++	intel_ring_advance(ring);
++	return 0;
++}
+ 
+-		trace_i915_gem_object_change_domain(obj, old_read, old_write);
++static int
++emit_box(struct i915_gem_request *rq,
++	 struct drm_clip_rect *box,
++	 int DR1, int DR4)
++{
++	struct intel_ringbuffer *ring;
++
++	if (box->y2 <= box->y1 || box->x2 <= box->x1 ||
++	    box->y2 <= 0 || box->x2 <= 0) {
++		DRM_DEBUG("Bad box %d,%d..%d,%d\n",
++			  box->x1, box->y1, box->x2, box->y2);
++		return -EINVAL;
+ 	}
+-}
+ 
+-static void
+-i915_gem_execbuffer_retire_commands(struct drm_device *dev,
+-				    struct drm_file *file,
+-				    struct intel_engine_cs *ring,
+-				    struct drm_i915_gem_object *obj)
+-{
+-	/* Unconditionally force add_request to emit a full flush. */
+-	ring->gpu_caches_dirty = true;
++	if (INTEL_INFO(rq->i915)->gen >= 4) {
++		ring = intel_ring_begin(rq, 4);
++		if (IS_ERR(ring))
++			return PTR_ERR(ring);
+ 
+-	/* Add a breadcrumb for the completion of the batch buffer */
+-	(void)__i915_add_request(ring, file, obj, NULL);
++		intel_ring_emit(ring, GFX_OP_DRAWRECT_INFO_I965);
++	} else {
++		ring = intel_ring_begin(rq, 5);
++		if (IS_ERR(ring))
++			return PTR_ERR(ring);
++
++		intel_ring_emit(ring, GFX_OP_DRAWRECT_INFO);
++		intel_ring_emit(ring, DR1);
++	}
++	intel_ring_emit(ring, (box->x1 & 0xffff) | box->y1 << 16);
++	intel_ring_emit(ring, ((box->x2 - 1) & 0xffff) | (box->y2 - 1) << 16);
++	intel_ring_emit(ring, DR4);
++	intel_ring_advance(ring);
++
++	return 0;
+ }
+ 
+-static int
+-i915_reset_gen7_sol_offsets(struct drm_device *dev,
+-			    struct intel_engine_cs *ring)
++static int set_contants_base(struct i915_gem_request *rq,
++			     struct drm_i915_gem_execbuffer2 *args)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	int ret, i;
++	int mode = args->flags & I915_EXEC_CONSTANTS_MASK;
++	u32 mask = I915_EXEC_CONSTANTS_MASK;
+ 
+-	if (!IS_GEN7(dev) || ring != &dev_priv->ring[RCS]) {
+-		DRM_DEBUG("sol reset is gen7/rcs only\n");
++	switch (mode) {
++	case I915_EXEC_CONSTANTS_REL_GENERAL:
++	case I915_EXEC_CONSTANTS_ABSOLUTE:
++	case I915_EXEC_CONSTANTS_REL_SURFACE:
++		if (mode != 0 && rq->engine->id != RCS) {
++			DRM_DEBUG("non-0 rel constants mode on non-RCS\n");
++			return -EINVAL;
++		}
++
++		if (mode != rq->engine->i915->relative_constants_mode) {
++			if (INTEL_INFO(rq->engine->i915)->gen < 4) {
++				DRM_DEBUG("no rel constants on pre-gen4\n");
++				return -EINVAL;
++			}
++
++			if (INTEL_INFO(rq->engine->i915)->gen > 5 &&
++			    mode == I915_EXEC_CONSTANTS_REL_SURFACE) {
++				DRM_DEBUG("rel surface constants mode invalid on gen5+\n");
++				return -EINVAL;
++			}
++
++			/* The HW changed the meaning on this bit on gen6 */
++			if (INTEL_INFO(rq->i915)->gen >= 6)
++				mask &= ~I915_EXEC_CONSTANTS_REL_SURFACE;
++		}
++		break;
++	default:
++		DRM_DEBUG("execbuf with unknown constants: %d\n", mode);
+ 		return -EINVAL;
+ 	}
+ 
+-	ret = intel_ring_begin(ring, 4 * 3);
+-	if (ret)
+-		return ret;
++	/* XXX INSTPM is per-context not global etc */
++	if (rq->engine->id == RCS && mode != rq->i915->relative_constants_mode) {
++		struct intel_ringbuffer *ring;
++
++		ring = intel_ring_begin(rq, 3);
++		if (IS_ERR(ring))
++			return PTR_ERR(ring);
+ 
+-	for (i = 0; i < 4; i++) {
+ 		intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
+-		intel_ring_emit(ring, GEN7_SO_WRITE_OFFSET(i));
+-		intel_ring_emit(ring, 0);
+-	}
++		intel_ring_emit(ring, INSTPM);
++		intel_ring_emit(ring, mask << 16 | mode);
++		intel_ring_advance(ring);
+ 
+-	intel_ring_advance(ring);
++		rq->i915->relative_constants_mode = mode;
++	}
+ 
+ 	return 0;
+ }
+ 
+ static int
+-legacy_ringbuffer_submission(struct drm_device *dev, struct drm_file *file,
+-			     struct intel_engine_cs *ring,
+-			     struct intel_context *ctx,
+-			     struct drm_i915_gem_execbuffer2 *args,
+-			     struct list_head *vmas,
+-			     struct drm_i915_gem_object *batch_obj,
+-			     u64 exec_start, u32 flags)
++submit_execbuf(struct intel_engine_cs *engine,
++	       struct intel_context *ctx,
++	       struct drm_i915_gem_execbuffer2 *args,
++	       struct eb_vmas *eb,
++	       u32 flags)
+ {
+ 	struct drm_clip_rect *cliprects = NULL;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	u64 exec_len;
+-	int instp_mode;
+-	u32 instp_mask;
+-	int i, ret = 0;
++	struct i915_gem_request *rq = NULL;
++	u64 exec_start;
++	int i, ret;
+ 
+ 	if (args->num_cliprects != 0) {
+-		if (ring != &dev_priv->ring[RCS]) {
++		if (engine->id != RCS) {
+ 			DRM_DEBUG("clip rectangles are only valid with the render ring\n");
+-			return -EINVAL;
++			ret = -EINVAL;
++			goto out;
+ 		}
+ 
+-		if (INTEL_INFO(dev)->gen >= 5) {
++		if (INTEL_INFO(engine->i915)->gen >= 5) {
+ 			DRM_DEBUG("clip rectangles are only valid on pre-gen5\n");
+-			return -EINVAL;
++			ret = -EINVAL;
++			goto out;
+ 		}
+ 
+ 		if (args->num_cliprects > UINT_MAX / sizeof(*cliprects)) {
+ 			DRM_DEBUG("execbuf with %u cliprects\n",
+ 				  args->num_cliprects);
+-			return -EINVAL;
++			ret = -EINVAL;
++			goto out;
+ 		}
+ 
+ 		cliprects = kcalloc(args->num_cliprects,
+@@ -1064,14 +1144,14 @@
+ 				    GFP_KERNEL);
+ 		if (cliprects == NULL) {
+ 			ret = -ENOMEM;
+-			goto error;
++			goto out;
+ 		}
+ 
+ 		if (copy_from_user(cliprects,
+ 				   to_user_ptr(args->cliprects_ptr),
+ 				   sizeof(*cliprects)*args->num_cliprects)) {
+ 			ret = -EFAULT;
+-			goto error;
++			goto out_cliprects;
+ 		}
+ 	} else {
+ 		if (args->DR4 == 0xffffffff) {
+@@ -1081,181 +1161,142 @@
+ 
+ 		if (args->DR1 || args->DR4 || args->cliprects_ptr) {
+ 			DRM_DEBUG("0 cliprects but dirt in cliprects fields\n");
+-			return -EINVAL;
++			ret = -EINVAL;
++			goto out;
+ 		}
+ 	}
+ 
+-	ret = i915_gem_execbuffer_move_to_gpu(ring, vmas);
+-	if (ret)
+-		goto error;
++	rq = i915_request_create(ctx, engine);
++	if (IS_ERR(rq)) {
++		ret = PTR_ERR(rq);
++		goto out_cliprects;
++	}
+ 
+-	ret = i915_switch_context(ring, ctx);
++	ret = vmas_move_to_rq(eb, rq);
+ 	if (ret)
+-		goto error;
+-
+-	instp_mode = args->flags & I915_EXEC_CONSTANTS_MASK;
+-	instp_mask = I915_EXEC_CONSTANTS_MASK;
+-	switch (instp_mode) {
+-	case I915_EXEC_CONSTANTS_REL_GENERAL:
+-	case I915_EXEC_CONSTANTS_ABSOLUTE:
+-	case I915_EXEC_CONSTANTS_REL_SURFACE:
+-		if (instp_mode != 0 && ring != &dev_priv->ring[RCS]) {
+-			DRM_DEBUG("non-0 rel constants mode on non-RCS\n");
+-			ret = -EINVAL;
+-			goto error;
+-		}
+-
+-		if (instp_mode != dev_priv->relative_constants_mode) {
+-			if (INTEL_INFO(dev)->gen < 4) {
+-				DRM_DEBUG("no rel constants on pre-gen4\n");
+-				ret = -EINVAL;
+-				goto error;
+-			}
++		goto out_rq;
+ 
+-			if (INTEL_INFO(dev)->gen > 5 &&
+-			    instp_mode == I915_EXEC_CONSTANTS_REL_SURFACE) {
+-				DRM_DEBUG("rel surface constants mode invalid on gen5+\n");
+-				ret = -EINVAL;
+-				goto error;
+-			}
++	ret = set_contants_base(rq, args);
++	if (ret)
++		goto out_rq;
+ 
+-			/* The HW changed the meaning on this bit on gen6 */
+-			if (INTEL_INFO(dev)->gen >= 6)
+-				instp_mask &= ~I915_EXEC_CONSTANTS_REL_SURFACE;
+-		}
+-		break;
+-	default:
+-		DRM_DEBUG("execbuf with unknown constants: %d\n", instp_mode);
+-		ret = -EINVAL;
+-		goto error;
++	if (args->flags & I915_EXEC_GEN7_SOL_RESET) {
++		ret = reset_sol_offsets(rq);
++		if (ret)
++			goto out_rq;
+ 	}
+ 
+-	if (ring == &dev_priv->ring[RCS] &&
+-			instp_mode != dev_priv->relative_constants_mode) {
+-		ret = intel_ring_begin(ring, 4);
+-		if (ret)
+-			goto error;
++	/* snb/ivb/vlv conflate the "batch in ppgtt" bit with the "non-secure
++	 * batch" bit. Hence we need to pin secure batches into the global gtt.
++	 * hsw should have this fixed, but bdw mucks it up again.
++	 */
++	exec_start = args->batch_start_offset;
++	if (flags & I915_DISPATCH_SECURE && !i915_is_ggtt(eb->vm)) {
++		struct i915_vma *ggtt;
+ 
+-		intel_ring_emit(ring, MI_NOOP);
+-		intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
+-		intel_ring_emit(ring, INSTPM);
+-		intel_ring_emit(ring, instp_mask << 16 | instp_mode);
+-		intel_ring_advance(ring);
++		/*
++		 * So on first glance it looks freaky that we pin the batch here
++		 * outside of the reservation loop. But:
++		 * - The batch is already pinned into the relevant ppgtt, so we
++		 *   already have the backing storage fully allocated.
++		 * - No other BO uses the global gtt (well contexts, but meh),
++		 *   so we don't really have issues with mutliple objects not
++		 *   fitting due to fragmentation.
++		 * So this is actually safe.
++		 */
++		ggtt = i915_gem_obj_get_ggtt(eb->batch->obj);
++		ret = i915_vma_pin(ggtt, 0, PIN_GLOBAL);
++		if (ret) {
++			i915_vma_put(ggtt);
++			goto out_rq;
++		}
+ 
+-		dev_priv->relative_constants_mode = instp_mode;
+-	}
++		drm_gem_object_reference(&eb->batch->obj->base);
++		i915_request_add_vma(rq, ggtt, 0);
+ 
+-	if (args->flags & I915_EXEC_GEN7_SOL_RESET) {
+-		ret = i915_reset_gen7_sol_offsets(dev, ring);
+-		if (ret)
+-			goto error;
+-	}
++		exec_start += ggtt->node.start;
++	} else
++		exec_start += eb->batch->node.start;
+ 
+-	exec_len = args->batch_len;
+ 	if (cliprects) {
+ 		for (i = 0; i < args->num_cliprects; i++) {
+-			ret = i915_emit_box(dev, &cliprects[i],
+-					    args->DR1, args->DR4);
++			ret = emit_box(rq, &cliprects[i],
++				       args->DR1, args->DR4);
+ 			if (ret)
+-				goto error;
++				goto out_rq;
+ 
+-			ret = ring->dispatch_execbuffer(ring,
+-							exec_start, exec_len,
+-							flags);
++			ret = i915_request_emit_batchbuffer(rq, eb->batch,
++							    exec_start,
++							    args->batch_len,
++							    flags);
+ 			if (ret)
+-				goto error;
++				goto out_rq;
+ 		}
+ 	} else {
+-		ret = ring->dispatch_execbuffer(ring,
+-						exec_start, exec_len,
+-						flags);
++		ret = i915_request_emit_batchbuffer(rq, eb->batch,
++						    exec_start,
++						    args->batch_len,
++						    flags);
+ 		if (ret)
+-			return ret;
++			goto out_rq;
+ 	}
+ 
+-	trace_i915_gem_ring_dispatch(ring, intel_ring_get_seqno(ring), flags);
+-
+-	i915_gem_execbuffer_move_to_active(vmas, ring);
+-	i915_gem_execbuffer_retire_commands(dev, file, ring, batch_obj);
++	ret = i915_request_commit(rq);
++	if (ret)
++		goto out_rq;
+ 
+-error:
++out_rq:
++	i915_request_put(rq);
++out_cliprects:
+ 	kfree(cliprects);
++out:
+ 	return ret;
+ }
+ 
+ /**
+  * Find one BSD ring to dispatch the corresponding BSD command.
+- * The Ring ID is returned.
+  */
+-static int gen8_dispatch_bsd_ring(struct drm_device *dev,
+-				  struct drm_file *file)
++static struct intel_engine_cs *
++gen8_select_bsd_engine(struct drm_i915_private *dev_priv,
++		       struct drm_file *file)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct drm_i915_file_private *file_priv = file->driver_priv;
+ 
+-	/* Check whether the file_priv is using one ring */
+-	if (file_priv->bsd_ring)
+-		return file_priv->bsd_ring->id;
+-	else {
+-		/* If no, use the ping-pong mechanism to select one ring */
+-		int ring_id;
++	/* Use the ping-pong mechanism to select one ring for this client */
++	if (file_priv->bsd_engine == NULL) {
++		int id;
+ 
+-		mutex_lock(&dev->struct_mutex);
++		mutex_lock(&dev_priv->dev->struct_mutex);
+ 		if (dev_priv->mm.bsd_ring_dispatch_index == 0) {
+-			ring_id = VCS;
++			id = VCS;
+ 			dev_priv->mm.bsd_ring_dispatch_index = 1;
+ 		} else {
+-			ring_id = VCS2;
++			id = VCS2;
+ 			dev_priv->mm.bsd_ring_dispatch_index = 0;
+ 		}
+-		file_priv->bsd_ring = &dev_priv->ring[ring_id];
+-		mutex_unlock(&dev->struct_mutex);
+-		return ring_id;
++		file_priv->bsd_engine = &dev_priv->engine[id];
++		mutex_unlock(&dev_priv->dev->struct_mutex);
+ 	}
+-}
+-
+-static struct drm_i915_gem_object *
+-eb_get_batch(struct eb_vmas *eb)
+-{
+-	struct i915_vma *vma = list_entry(eb->vmas.prev, typeof(*vma), exec_list);
+ 
+-	/*
+-	 * SNA is doing fancy tricks with compressing batch buffers, which leads
+-	 * to negative relocation deltas. Usually that works out ok since the
+-	 * relocate address is still positive, except when the batch is placed
+-	 * very low in the GTT. Ensure this doesn't happen.
+-	 *
+-	 * Note that actual hangs have only been observed on gen7, but for
+-	 * paranoia do it everywhere.
+-	 */
+-	vma->exec_entry->flags |= __EXEC_OBJECT_NEEDS_BIAS;
+-
+-	return vma->obj;
++	return file_priv->bsd_engine;
+ }
+ 
+ static int
+-i915_gem_do_execbuffer(struct drm_device *dev, void *data,
++i915_gem_do_execbuffer(struct drm_i915_private *dev_priv, void *data,
+ 		       struct drm_file *file,
+ 		       struct drm_i915_gem_execbuffer2 *args,
+ 		       struct drm_i915_gem_exec_object2 *exec)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct eb_vmas *eb;
+-	struct drm_i915_gem_object *batch_obj;
+-	struct intel_engine_cs *ring;
++	struct intel_engine_cs *engine;
+ 	struct intel_context *ctx;
+-	struct i915_address_space *vm;
+ 	const u32 ctx_id = i915_execbuffer2_get_context_id(*args);
+-	u64 exec_start = args->batch_start_offset;
+ 	u32 flags;
+ 	int ret;
+ 	bool need_relocs;
+ 
+-	if (!i915_gem_check_execbuffer(args))
+-		return -EINVAL;
+-
+-	ret = validate_exec_list(exec, args->buffer_count);
+-	if (ret)
++	ret = validate_exec_list(dev_priv, args, exec);
++	if (unlikely(ret))
+ 		return ret;
+ 
+ 	flags = 0;
+@@ -1275,18 +1316,16 @@
+ 	}
+ 
+ 	if ((args->flags & I915_EXEC_RING_MASK) == I915_EXEC_DEFAULT)
+-		ring = &dev_priv->ring[RCS];
++		engine = &dev_priv->engine[RCS];
+ 	else if ((args->flags & I915_EXEC_RING_MASK) == I915_EXEC_BSD) {
+-		if (HAS_BSD2(dev)) {
+-			int ring_id;
+-			ring_id = gen8_dispatch_bsd_ring(dev, file);
+-			ring = &dev_priv->ring[ring_id];
+-		} else
+-			ring = &dev_priv->ring[VCS];
++		if (HAS_BSD2(dev_priv))
++			engine = gen8_select_bsd_engine(dev_priv, file);
++		else
++			engine = &dev_priv->engine[VCS];
+ 	} else
+-		ring = &dev_priv->ring[(args->flags & I915_EXEC_RING_MASK) - 1];
++		engine = &dev_priv->engine[(args->flags & I915_EXEC_RING_MASK) - 1];
+ 
+-	if (!intel_ring_initialized(ring)) {
++	if (!intel_engine_initialized(engine)) {
+ 		DRM_DEBUG("execbuf with invalid ring: %d\n",
+ 			  (int)(args->flags & I915_EXEC_RING_MASK));
+ 		return -EINVAL;
+@@ -1299,48 +1338,42 @@
+ 
+ 	intel_runtime_pm_get(dev_priv);
+ 
+-	ret = i915_mutex_lock_interruptible(dev);
++	ret = i915_mutex_lock_interruptible(dev_priv->dev);
+ 	if (ret)
+ 		goto pre_mutex_err;
+ 
+ 	if (dev_priv->ums.mm_suspended) {
+-		mutex_unlock(&dev->struct_mutex);
++		mutex_unlock(&dev_priv->dev->struct_mutex);
+ 		ret = -EBUSY;
+ 		goto pre_mutex_err;
+ 	}
+ 
+-	ctx = i915_gem_validate_context(dev, file, ring, ctx_id);
++	ctx = i915_gem_validate_context(file, engine, ctx_id);
+ 	if (IS_ERR(ctx)) {
+-		mutex_unlock(&dev->struct_mutex);
++		mutex_unlock(&dev_priv->dev->struct_mutex);
+ 		ret = PTR_ERR(ctx);
+ 		goto pre_mutex_err;
+ 	}
+ 
+ 	i915_gem_context_reference(ctx);
+ 
+-	vm = ctx->vm;
+-	if (!USES_FULL_PPGTT(dev))
+-		vm = &dev_priv->gtt.base;
+-
+ 	eb = eb_create(args);
+ 	if (eb == NULL) {
+ 		i915_gem_context_unreference(ctx);
+-		mutex_unlock(&dev->struct_mutex);
++		mutex_unlock(&dev_priv->dev->struct_mutex);
+ 		ret = -ENOMEM;
+ 		goto pre_mutex_err;
+ 	}
++	eb->vm = ctx->ppgtt ? &ctx->ppgtt->base : &dev_priv->gtt.base;
+ 
+ 	/* Look up object handles */
+-	ret = eb_lookup_vmas(eb, exec, args, vm, file);
++	ret = eb_lookup_vmas(eb, exec, args, file);
+ 	if (ret)
+ 		goto err;
+ 
+-	/* take note of the batch buffer before we might reorder the lists */
+-	batch_obj = eb_get_batch(eb);
+-
+ 	/* Move the objects en-masse into the GTT, evicting if necessary. */
+ 	need_relocs = (args->flags & I915_EXEC_NO_RELOC) == 0;
+-	ret = i915_gem_execbuffer_reserve(ring, &eb->vmas, &need_relocs);
++	ret = i915_gem_execbuffer_reserve(engine, eb, &need_relocs);
+ 	if (ret)
+ 		goto err;
+ 
+@@ -1349,68 +1382,29 @@
+ 		ret = i915_gem_execbuffer_relocate(eb);
+ 	if (ret) {
+ 		if (ret == -EFAULT) {
+-			ret = i915_gem_execbuffer_relocate_slow(dev, args, file, ring,
++			ret = i915_gem_execbuffer_relocate_slow(dev_priv, args, file, engine,
+ 								eb, exec);
+-			BUG_ON(!mutex_is_locked(&dev->struct_mutex));
++			BUG_ON(!mutex_is_locked(&dev_priv->dev->struct_mutex));
+ 		}
+ 		if (ret)
+ 			goto err;
+ 	}
+ 
+ 	/* Set the pending read domains for the batch buffer to COMMAND */
+-	if (batch_obj->base.pending_write_domain) {
++	if (eb->batch->obj->base.pending_write_domain) {
+ 		DRM_DEBUG("Attempting to use self-modifying batch buffer\n");
+ 		ret = -EINVAL;
+ 		goto err;
+ 	}
+-	batch_obj->base.pending_read_domains |= I915_GEM_DOMAIN_COMMAND;
+-
+-	if (i915_needs_cmd_parser(ring)) {
+-		ret = i915_parse_cmds(ring,
+-				      batch_obj,
+-				      args->batch_start_offset,
+-				      file->is_master);
+-		if (ret)
+-			goto err;
+-
+-		/*
+-		 * XXX: Actually do this when enabling batch copy...
+-		 *
+-		 * Set the DISPATCH_SECURE bit to remove the NON_SECURE bit
+-		 * from MI_BATCH_BUFFER_START commands issued in the
+-		 * dispatch_execbuffer implementations. We specifically don't
+-		 * want that set when the command parser is enabled.
+-		 */
+-	}
+-
+-	/* snb/ivb/vlv conflate the "batch in ppgtt" bit with the "non-secure
+-	 * batch" bit. Hence we need to pin secure batches into the global gtt.
+-	 * hsw should have this fixed, but bdw mucks it up again. */
+-	if (flags & I915_DISPATCH_SECURE &&
+-	    !batch_obj->has_global_gtt_mapping) {
+-		/* When we have multiple VMs, we'll need to make sure that we
+-		 * allocate space first */
+-		struct i915_vma *vma = i915_gem_obj_to_ggtt(batch_obj);
+-		BUG_ON(!vma);
+-		vma->bind_vma(vma, batch_obj->cache_level, GLOBAL_BIND);
+-	}
+-
+-	if (flags & I915_DISPATCH_SECURE)
+-		exec_start += i915_gem_obj_ggtt_offset(batch_obj);
+-	else
+-		exec_start += i915_gem_obj_offset(batch_obj, vm);
+-
+-	ret = legacy_ringbuffer_submission(dev, file, ring, ctx,
+-			args, &eb->vmas, batch_obj, exec_start, flags);
+-	if (ret)
+-		goto err;
++	eb->batch->obj->base.pending_read_domains |= I915_GEM_DOMAIN_COMMAND;
+ 
++	ret = submit_execbuf(engine, ctx, args, eb, flags);
+ err:
+ 	/* the request owns the ref now */
+ 	i915_gem_context_unreference(ctx);
+ 	eb_destroy(eb);
+ 
+-	mutex_unlock(&dev->struct_mutex);
++	mutex_unlock(&dev_priv->dev->struct_mutex);
+ 
+ pre_mutex_err:
+ 	/* intel_gpu_busy should also get a ref, so it will free when the device
+@@ -1482,7 +1476,7 @@
+ 	exec2.flags = I915_EXEC_RENDER;
+ 	i915_execbuffer2_set_context_id(exec2, 0);
+ 
+-	ret = i915_gem_do_execbuffer(dev, data, file, &exec2, exec2_list);
++	ret = i915_gem_do_execbuffer(to_i915(dev), data, file, &exec2, exec2_list);
+ 	if (!ret) {
+ 		struct drm_i915_gem_exec_object __user *user_exec_list =
+ 			to_user_ptr(args->buffers_ptr);
+@@ -1546,7 +1540,7 @@
+ 		return -EFAULT;
+ 	}
+ 
+-	ret = i915_gem_do_execbuffer(dev, data, file, args, exec2_list);
++	ret = i915_gem_do_execbuffer(to_i915(dev), data, file, args, exec2_list);
+ 	if (!ret) {
+ 		/* Copy the new buffer offsets back to the user's exec list. */
+ 		struct drm_i915_gem_exec_object2 __user *user_exec_list =
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_gem_gtt.c b/drivers/gpu/drm/i915/i915_gem_gtt.c
+--- a/drivers/gpu/drm/i915/i915_gem_gtt.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/i915_gem_gtt.c	2014-11-20 09:53:37.968762837 -0700
+@@ -33,26 +33,28 @@
+ static void bdw_setup_private_ppat(struct drm_i915_private *dev_priv);
+ static void chv_setup_private_ppat(struct drm_i915_private *dev_priv);
+ 
+-bool intel_enable_ppgtt(struct drm_device *dev, bool full)
++static int sanitize_enable_ppgtt(struct drm_device *dev, int enable_ppgtt)
+ {
+-	if (i915.enable_ppgtt == 0)
+-		return false;
+-
+-	if (i915.enable_ppgtt == 1 && full)
+-		return false;
++	bool has_aliasing_ppgtt;
++	bool has_full_ppgtt;
+ 
+-	return true;
+-}
++	has_aliasing_ppgtt = INTEL_INFO(dev)->gen >= 6;
++	has_full_ppgtt = INTEL_INFO(dev)->gen >= 7;
++	if (IS_GEN8(dev))
++		has_full_ppgtt = false; /* XXX why? */
+ 
+-static int sanitize_enable_ppgtt(struct drm_device *dev, int enable_ppgtt)
+-{
+-	if (enable_ppgtt == 0 || !HAS_ALIASING_PPGTT(dev))
++	/*
++	 * We don't allow disabling PPGTT for gen9+ as it's a requirement for
++	 * execlists, the sole mechanism available to submit work.
++	 */
++	if (INTEL_INFO(dev)->gen < 9 &&
++	    (enable_ppgtt == 0 || !has_aliasing_ppgtt))
+ 		return 0;
+ 
+ 	if (enable_ppgtt == 1)
+ 		return 1;
+ 
+-	if (enable_ppgtt == 2 && HAS_PPGTT(dev))
++	if (enable_ppgtt == 2 && has_full_ppgtt)
+ 		return 2;
+ 
+ #ifdef CONFIG_INTEL_IOMMU
+@@ -70,16 +72,9 @@
+ 		return 0;
+ 	}
+ 
+-	return HAS_ALIASING_PPGTT(dev) ? 1 : 0;
++	return has_aliasing_ppgtt ? 1 : 0;
+ }
+ 
+-
+-static void ppgtt_bind_vma(struct i915_vma *vma,
+-			   enum i915_cache_level cache_level,
+-			   u32 flags);
+-static void ppgtt_unbind_vma(struct i915_vma *vma);
+-static int gen8_ppgtt_enable(struct i915_hw_ppgtt *ppgtt);
+-
+ static inline gen8_gtt_pte_t gen8_pte_encode(dma_addr_t addr,
+ 					     enum i915_cache_level level,
+ 					     bool valid)
+@@ -168,9 +163,6 @@
+ 	gen6_gtt_pte_t pte = valid ? GEN6_PTE_VALID : 0;
+ 	pte |= GEN6_PTE_ADDR_ENCODE(addr);
+ 
+-	/* Mark the page as writeable.  Other platforms don't have a
+-	 * setting for read-only/writable, so this matches that behavior.
+-	 */
+ 	if (!(flags & PTE_READ_ONLY))
+ 		pte |= BYT_PTE_WRITEABLE;
+ 
+@@ -215,38 +207,28 @@
+ }
+ 
+ /* Broadwell Page Directory Pointer Descriptors */
+-static int gen8_write_pdp(struct intel_engine_cs *ring, unsigned entry,
+-			   uint64_t val, bool synchronous)
++static int gen8_write_pdp(struct i915_gem_request *rq, unsigned entry, uint64_t val)
+ {
+-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
+-	int ret;
++	struct intel_ringbuffer *ring;
+ 
+ 	BUG_ON(entry >= 4);
+ 
+-	if (synchronous) {
+-		I915_WRITE(GEN8_RING_PDP_UDW(ring, entry), val >> 32);
+-		I915_WRITE(GEN8_RING_PDP_LDW(ring, entry), (u32)val);
+-		return 0;
+-	}
+-
+-	ret = intel_ring_begin(ring, 6);
+-	if (ret)
+-		return ret;
++	ring = intel_ring_begin(rq, 5);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+-	intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
+-	intel_ring_emit(ring, GEN8_RING_PDP_UDW(ring, entry));
+-	intel_ring_emit(ring, (u32)(val >> 32));
+-	intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
+-	intel_ring_emit(ring, GEN8_RING_PDP_LDW(ring, entry));
+-	intel_ring_emit(ring, (u32)(val));
++	intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(2));
++	intel_ring_emit(ring, GEN8_RING_PDP_UDW(rq->engine, entry));
++	intel_ring_emit(ring, upper_32_bits(val));
++	intel_ring_emit(ring, GEN8_RING_PDP_LDW(rq->engine, entry));
++	intel_ring_emit(ring, lower_32_bits(val));
+ 	intel_ring_advance(ring);
+ 
+ 	return 0;
+ }
+ 
+-static int gen8_mm_switch(struct i915_hw_ppgtt *ppgtt,
+-			  struct intel_engine_cs *ring,
+-			  bool synchronous)
++static int gen8_mm_switch(struct i915_gem_request *rq,
++			  struct i915_hw_ppgtt *ppgtt)
+ {
+ 	int i, ret;
+ 
+@@ -255,7 +237,7 @@
+ 
+ 	for (i = used_pd - 1; i >= 0; i--) {
+ 		dma_addr_t addr = ppgtt->pd_dma_addr[i];
+-		ret = gen8_write_pdp(ring, i, addr, synchronous);
++		ret = gen8_write_pdp(rq, i, addr);
+ 		if (ret)
+ 			return ret;
+ 	}
+@@ -263,10 +245,10 @@
+ 	return 0;
+ }
+ 
+-static void gen8_ppgtt_clear_range(struct i915_address_space *vm,
+-				   uint64_t start,
+-				   uint64_t length,
+-				   bool use_scratch)
++static int gen8_ppgtt_clear_range(struct i915_address_space *vm,
++				  uint64_t start,
++				  uint64_t length,
++				  bool use_scratch)
+ {
+ 	struct i915_hw_ppgtt *ppgtt =
+ 		container_of(vm, struct i915_hw_ppgtt, base);
+@@ -304,12 +286,14 @@
+ 			pde = 0;
+ 		}
+ 	}
++
++	return 0;
+ }
+ 
+-static void gen8_ppgtt_insert_entries(struct i915_address_space *vm,
+-				      struct sg_table *pages,
+-				      uint64_t start,
+-				      enum i915_cache_level cache_level, u32 unused)
++static int gen8_ppgtt_insert_entries(struct i915_address_space *vm,
++				     struct sg_table *pages,
++				     uint64_t start,
++				     enum i915_cache_level cache_level, u32 unused)
+ {
+ 	struct i915_hw_ppgtt *ppgtt =
+ 		container_of(vm, struct i915_hw_ppgtt, base);
+@@ -348,6 +332,8 @@
+ 			drm_clflush_virt_range(pt_vaddr, PAGE_SIZE);
+ 		kunmap_atomic(pt_vaddr);
+ 	}
++
++	return 0;
+ }
+ 
+ static void gen8_free_page_tables(struct page **pt_pages)
+@@ -403,9 +389,6 @@
+ 	struct i915_hw_ppgtt *ppgtt =
+ 		container_of(vm, struct i915_hw_ppgtt, base);
+ 
+-	list_del(&vm->global_link);
+-	drm_mm_takedown(&vm->mm);
+-
+ 	gen8_ppgtt_unmap_pages(ppgtt);
+ 	gen8_ppgtt_free(ppgtt);
+ }
+@@ -615,7 +598,6 @@
+ 		kunmap_atomic(pd_vaddr);
+ 	}
+ 
+-	ppgtt->enable = gen8_ppgtt_enable;
+ 	ppgtt->switch_mm = gen8_mm_switch;
+ 	ppgtt->base.clear_range = gen8_ppgtt_clear_range;
+ 	ppgtt->base.insert_entries = gen8_ppgtt_insert_entries;
+@@ -640,35 +622,19 @@
+ 
+ static void gen6_dump_ppgtt(struct i915_hw_ppgtt *ppgtt, struct seq_file *m)
+ {
+-	struct drm_i915_private *dev_priv = ppgtt->base.dev->dev_private;
+ 	struct i915_address_space *vm = &ppgtt->base;
+-	gen6_gtt_pte_t __iomem *pd_addr;
+ 	gen6_gtt_pte_t scratch_pte;
+-	uint32_t pd_entry;
+ 	int pte, pde;
+ 
+-	scratch_pte = vm->pte_encode(vm->scratch.addr, I915_CACHE_LLC, true, 0);
++	if (ppgtt->state->pages == NULL)
++		return;
+ 
+-	pd_addr = (gen6_gtt_pte_t __iomem *)dev_priv->gtt.gsm +
+-		ppgtt->pd_offset / sizeof(gen6_gtt_pte_t);
++	scratch_pte = vm->pte_encode(vm->scratch.addr, I915_CACHE_LLC, true, 0);
+ 
+-	seq_printf(m, "  VM %p (pd_offset %x-%x):\n", vm,
+-		   ppgtt->pd_offset, ppgtt->pd_offset + ppgtt->num_pd_entries);
+ 	for (pde = 0; pde < ppgtt->num_pd_entries; pde++) {
+-		u32 expected;
+ 		gen6_gtt_pte_t *pt_vaddr;
+-		dma_addr_t pt_addr = ppgtt->pt_dma_addr[pde];
+-		pd_entry = readl(pd_addr + pde);
+-		expected = (GEN6_PDE_ADDR_ENCODE(pt_addr) | GEN6_PDE_VALID);
+-
+-		if (pd_entry != expected)
+-			seq_printf(m, "\tPDE #%d mismatch: Actual PDE: %x Expected PDE: %x\n",
+-				   pde,
+-				   pd_entry,
+-				   expected);
+-		seq_printf(m, "\tPDE: %x\n", pd_entry);
+ 
+-		pt_vaddr = kmap_atomic(ppgtt->pt_pages[pde]);
++		pt_vaddr = kmap_atomic(i915_gem_object_get_page(ppgtt->state, pde));
+ 		for (pte = 0; pte < I915_PPGTT_PT_ENTRIES; pte+=4) {
+ 			unsigned long va =
+ 				(pde * PAGE_SIZE * I915_PPGTT_PT_ENTRIES) +
+@@ -694,183 +660,80 @@
+ 	}
+ }
+ 
+-static void gen6_write_pdes(struct i915_hw_ppgtt *ppgtt)
+-{
+-	struct drm_i915_private *dev_priv = ppgtt->base.dev->dev_private;
+-	gen6_gtt_pte_t __iomem *pd_addr;
+-	uint32_t pd_entry;
+-	int i;
+-
+-	WARN_ON(ppgtt->pd_offset & 0x3f);
+-	pd_addr = (gen6_gtt_pte_t __iomem*)dev_priv->gtt.gsm +
+-		ppgtt->pd_offset / sizeof(gen6_gtt_pte_t);
+-	for (i = 0; i < ppgtt->num_pd_entries; i++) {
+-		dma_addr_t pt_addr;
+-
+-		pt_addr = ppgtt->pt_dma_addr[i];
+-		pd_entry = GEN6_PDE_ADDR_ENCODE(pt_addr);
+-		pd_entry |= GEN6_PDE_VALID;
+-
+-		writel(pd_entry, pd_addr + i);
+-	}
+-	readl(pd_addr);
+-}
+-
+ static uint32_t get_pd_offset(struct i915_hw_ppgtt *ppgtt)
+ {
+-	BUG_ON(ppgtt->pd_offset & 0x3f);
+-
+-	return (ppgtt->pd_offset / 64) << 16;
+-}
+-
+-static int hsw_mm_switch(struct i915_hw_ppgtt *ppgtt,
+-			 struct intel_engine_cs *ring,
+-			 bool synchronous)
+-{
+-	struct drm_device *dev = ppgtt->base.dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	int ret;
+-
+-	/* If we're in reset, we can assume the GPU is sufficiently idle to
+-	 * manually frob these bits. Ideally we could use the ring functions,
+-	 * except our error handling makes it quite difficult (can't use
+-	 * intel_ring_begin, ring->flush, or intel_ring_advance)
+-	 *
+-	 * FIXME: We should try not to special case reset
+-	 */
+-	if (synchronous ||
+-	    i915_reset_in_progress(&dev_priv->gpu_error)) {
+-		WARN_ON(ppgtt != dev_priv->mm.aliasing_ppgtt);
+-		I915_WRITE(RING_PP_DIR_DCLV(ring), PP_DIR_DCLV_2G);
+-		I915_WRITE(RING_PP_DIR_BASE(ring), get_pd_offset(ppgtt));
+-		POSTING_READ(RING_PP_DIR_BASE(ring));
+-		return 0;
+-	}
+-
+-	/* NB: TLBs must be flushed and invalidated before a switch */
+-	ret = ring->flush(ring, I915_GEM_GPU_DOMAINS, I915_GEM_GPU_DOMAINS);
+-	if (ret)
+-		return ret;
+-
+-	ret = intel_ring_begin(ring, 6);
+-	if (ret)
+-		return ret;
+-
+-	intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(2));
+-	intel_ring_emit(ring, RING_PP_DIR_DCLV(ring));
+-	intel_ring_emit(ring, PP_DIR_DCLV_2G);
+-	intel_ring_emit(ring, RING_PP_DIR_BASE(ring));
+-	intel_ring_emit(ring, get_pd_offset(ppgtt));
+-	intel_ring_emit(ring, MI_NOOP);
+-	intel_ring_advance(ring);
+-
+-	return 0;
++	uint64_t offset = i915_gem_obj_to_ggtt(ppgtt->state)->node.start / PAGE_SIZE;
++	return (offset * sizeof(gen6_gtt_pte_t) / 64) << 16;
+ }
+ 
+-static int gen7_mm_switch(struct i915_hw_ppgtt *ppgtt,
+-			  struct intel_engine_cs *ring,
+-			  bool synchronous)
++static int gen7_mm_switch(struct i915_gem_request *rq,
++			  struct i915_hw_ppgtt *ppgtt)
+ {
+-	struct drm_device *dev = ppgtt->base.dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_ringbuffer *ring;
+ 	int ret;
+ 
+-	/* If we're in reset, we can assume the GPU is sufficiently idle to
+-	 * manually frob these bits. Ideally we could use the ring functions,
+-	 * except our error handling makes it quite difficult (can't use
+-	 * intel_ring_begin, ring->flush, or intel_ring_advance)
+-	 *
+-	 * FIXME: We should try not to special case reset
+-	 */
+-	if (synchronous ||
+-	    i915_reset_in_progress(&dev_priv->gpu_error)) {
+-		WARN_ON(ppgtt != dev_priv->mm.aliasing_ppgtt);
+-		I915_WRITE(RING_PP_DIR_DCLV(ring), PP_DIR_DCLV_2G);
+-		I915_WRITE(RING_PP_DIR_BASE(ring), get_pd_offset(ppgtt));
+-		POSTING_READ(RING_PP_DIR_BASE(ring));
+-		return 0;
+-	}
+-
+-	/* NB: TLBs must be flushed and invalidated before a switch */
+-	ret = ring->flush(ring, I915_GEM_GPU_DOMAINS, I915_GEM_GPU_DOMAINS);
++	rq->pending_flush = ~0; /* XXX force the flush */
++	ret = i915_request_emit_flush(rq, I915_COMMAND_BARRIER);
+ 	if (ret)
+ 		return ret;
+ 
+-	ret = intel_ring_begin(ring, 6);
+-	if (ret)
+-		return ret;
++	ring = intel_ring_begin(rq, 5);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+ 	intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(2));
+-	intel_ring_emit(ring, RING_PP_DIR_DCLV(ring));
++	intel_ring_emit(ring, RING_PP_DIR_DCLV(rq->engine));
+ 	intel_ring_emit(ring, PP_DIR_DCLV_2G);
+-	intel_ring_emit(ring, RING_PP_DIR_BASE(ring));
++	intel_ring_emit(ring, RING_PP_DIR_BASE(rq->engine));
+ 	intel_ring_emit(ring, get_pd_offset(ppgtt));
+-	intel_ring_emit(ring, MI_NOOP);
+ 	intel_ring_advance(ring);
+ 
+-	/* XXX: RCS is the only one to auto invalidate the TLBs? */
+-	if (ring->id != RCS) {
+-		ret = ring->flush(ring, I915_GEM_GPU_DOMAINS, I915_GEM_GPU_DOMAINS);
+-		if (ret)
+-			return ret;
+-	}
++	rq->pending_flush |= I915_INVALIDATE_CACHES;
+ 
+ 	return 0;
+ }
+ 
+-static int gen6_mm_switch(struct i915_hw_ppgtt *ppgtt,
+-			  struct intel_engine_cs *ring,
+-			  bool synchronous)
++static int gen6_mm_switch(struct i915_gem_request *rq,
++			  struct i915_hw_ppgtt *ppgtt)
+ {
+-	struct drm_device *dev = ppgtt->base.dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-
+-	if (!synchronous)
+-		return 0;
+-
+-	I915_WRITE(RING_PP_DIR_DCLV(ring), PP_DIR_DCLV_2G);
+-	I915_WRITE(RING_PP_DIR_BASE(ring), get_pd_offset(ppgtt));
+-
+-	POSTING_READ(RING_PP_DIR_DCLV(ring));
+-
+-	return 0;
++	return -ENODEV;
+ }
+ 
+-static int gen8_ppgtt_enable(struct i915_hw_ppgtt *ppgtt)
++static int gen8_ppgtt_enable(struct drm_device *dev)
+ {
+-	struct drm_device *dev = ppgtt->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring;
+-	int j, ret;
++	struct intel_engine_cs *engine;
++	int j;
+ 
+-	for_each_ring(ring, dev_priv, j) {
+-		I915_WRITE(RING_MODE_GEN7(ring),
+-			   _MASKED_BIT_ENABLE(GFX_PPGTT_ENABLE));
++	for_each_engine(engine, dev_priv, j) {
++		I915_WRITE(RING_MODE_GEN7(engine),
++				_MASKED_BIT_ENABLE(GFX_PPGTT_ENABLE));
+ 
+-		/* We promise to do a switch later with FULL PPGTT. If this is
+-		 * aliasing, this is the one and only switch we'll do */
+-		if (USES_FULL_PPGTT(dev))
+-			continue;
++		if (dev_priv->mm.aliasing_ppgtt) {
++			struct i915_gem_request *rq;
++			int ret;
+ 
+-		ret = ppgtt->switch_mm(ppgtt, ring, true);
+-		if (ret)
+-			goto err_out;
++			rq = i915_request_create(engine->default_context,
++						 engine);
++			if (IS_ERR(rq))
++				return PTR_ERR(rq);
++
++			ret = gen8_mm_switch(rq, dev_priv->mm.aliasing_ppgtt);
++			if (ret == 0)
++				ret = i915_request_commit(rq);
++			i915_request_put(rq);
++			if (ret)
++				return ret;
++		}
+ 	}
+ 
+ 	return 0;
+-
+-err_out:
+-	for_each_ring(ring, dev_priv, j)
+-		I915_WRITE(RING_MODE_GEN7(ring),
+-			   _MASKED_BIT_DISABLE(GFX_PPGTT_ENABLE));
+-	return ret;
+ }
+ 
+-static int gen7_ppgtt_enable(struct i915_hw_ppgtt *ppgtt)
++static int gen7_ppgtt_enable(struct drm_device *dev)
+ {
+-	struct drm_device *dev = ppgtt->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring;
++	struct intel_engine_cs *engine;
+ 	uint32_t ecochk, ecobits;
+ 	int i;
+ 
+@@ -886,31 +749,26 @@
+ 	}
+ 	I915_WRITE(GAM_ECOCHK, ecochk);
+ 
+-	for_each_ring(ring, dev_priv, i) {
+-		int ret;
++	for_each_engine(engine, dev_priv, i) {
+ 		/* GFX_MODE is per-ring on gen7+ */
+-		I915_WRITE(RING_MODE_GEN7(ring),
++		I915_WRITE(RING_MODE_GEN7(engine),
+ 			   _MASKED_BIT_ENABLE(GFX_PPGTT_ENABLE));
+ 
+-		/* We promise to do a switch later with FULL PPGTT. If this is
+-		 * aliasing, this is the one and only switch we'll do */
+-		if (USES_FULL_PPGTT(dev))
+-			continue;
++		if (dev_priv->mm.aliasing_ppgtt) {
++			I915_WRITE(RING_PP_DIR_DCLV(engine), PP_DIR_DCLV_2G);
++			I915_WRITE(RING_PP_DIR_BASE(engine), get_pd_offset(dev_priv->mm.aliasing_ppgtt));
++		}
+ 
+-		ret = ppgtt->switch_mm(ppgtt, ring, true);
+-		if (ret)
+-			return ret;
+ 	}
+-
++	POSTING_READ(RING_PP_DIR_DCLV(RCS_ENGINE(dev_priv)));
+ 	return 0;
+ }
+ 
+-static int gen6_ppgtt_enable(struct i915_hw_ppgtt *ppgtt)
++static int gen6_ppgtt_enable(struct drm_device *dev)
+ {
+-	struct drm_device *dev = ppgtt->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring;
+ 	uint32_t ecochk, gab_ctl, ecobits;
++	struct intel_engine_cs *engine;
+ 	int i;
+ 
+ 	ecobits = I915_READ(GAC_ECO_BITS);
+@@ -925,20 +783,27 @@
+ 
+ 	I915_WRITE(GFX_MODE, _MASKED_BIT_ENABLE(GFX_PPGTT_ENABLE));
+ 
+-	for_each_ring(ring, dev_priv, i) {
+-		int ret = ppgtt->switch_mm(ppgtt, ring, true);
+-		if (ret)
+-			return ret;
+-	}
++	for_each_engine(engine, dev_priv, i) {
++		if (dev_priv->mm.aliasing_ppgtt) {
++			I915_WRITE(RING_PP_DIR_DCLV(engine), PP_DIR_DCLV_2G);
++			I915_WRITE(RING_PP_DIR_BASE(engine), get_pd_offset(dev_priv->mm.aliasing_ppgtt));
++		}
+ 
++	}
++	POSTING_READ(RING_PP_DIR_DCLV(RCS_ENGINE(dev_priv)));
+ 	return 0;
+ }
+ 
+ /* PPGTT support for Sandybdrige/Gen6 and later */
+-static void gen6_ppgtt_clear_range(struct i915_address_space *vm,
+-				   uint64_t start,
+-				   uint64_t length,
+-				   bool use_scratch)
++static inline void *kmap_pt(struct i915_hw_ppgtt *ppgtt, int pt)
++{
++	return kmap_atomic(i915_gem_object_get_page(ppgtt->state, pt));
++}
++
++static int gen6_ppgtt_clear_range(struct i915_address_space *vm,
++				  uint64_t start,
++				  uint64_t length,
++				  bool use_scratch)
+ {
+ 	struct i915_hw_ppgtt *ppgtt =
+ 		container_of(vm, struct i915_hw_ppgtt, base);
+@@ -948,6 +813,11 @@
+ 	unsigned act_pt = first_entry / I915_PPGTT_PT_ENTRIES;
+ 	unsigned first_pte = first_entry % I915_PPGTT_PT_ENTRIES;
+ 	unsigned last_pte, i;
++	int ret;
++
++	ret = i915_gem_object_get_pages(ppgtt->state);
++	if (ret)
++		return ret;
+ 
+ 	scratch_pte = vm->pte_encode(vm->scratch.addr, I915_CACHE_LLC, true, 0);
+ 
+@@ -956,7 +826,7 @@
+ 		if (last_pte > I915_PPGTT_PT_ENTRIES)
+ 			last_pte = I915_PPGTT_PT_ENTRIES;
+ 
+-		pt_vaddr = kmap_atomic(ppgtt->pt_pages[act_pt]);
++		pt_vaddr = kmap_pt(ppgtt, act_pt);
+ 
+ 		for (i = first_pte; i < last_pte; i++)
+ 			pt_vaddr[i] = scratch_pte;
+@@ -967,12 +837,14 @@
+ 		first_pte = 0;
+ 		act_pt++;
+ 	}
++
++	return 0;
+ }
+ 
+-static void gen6_ppgtt_insert_entries(struct i915_address_space *vm,
+-				      struct sg_table *pages,
+-				      uint64_t start,
+-				      enum i915_cache_level cache_level, u32 flags)
++static int gen6_ppgtt_insert_entries(struct i915_address_space *vm,
++				     struct sg_table *pages, uint64_t start,
++				     enum i915_cache_level cache_level,
++				     u32 flags)
+ {
+ 	struct i915_hw_ppgtt *ppgtt =
+ 		container_of(vm, struct i915_hw_ppgtt, base);
+@@ -981,11 +853,16 @@
+ 	unsigned act_pt = first_entry / I915_PPGTT_PT_ENTRIES;
+ 	unsigned act_pte = first_entry % I915_PPGTT_PT_ENTRIES;
+ 	struct sg_page_iter sg_iter;
++	int ret;
++
++	ret = i915_gem_object_get_pages(ppgtt->state);
++	if (ret)
++		return ret;
+ 
+ 	pt_vaddr = NULL;
+ 	for_each_sg_page(pages->sgl, &sg_iter, pages->nents, 0) {
+ 		if (pt_vaddr == NULL)
+-			pt_vaddr = kmap_atomic(ppgtt->pt_pages[act_pt]);
++			pt_vaddr = kmap_pt(ppgtt, act_pt);
+ 
+ 		pt_vaddr[act_pte] =
+ 			vm->pte_encode(sg_page_iter_dma_address(&sg_iter),
+@@ -1000,28 +877,8 @@
+ 	}
+ 	if (pt_vaddr)
+ 		kunmap_atomic(pt_vaddr);
+-}
+-
+-static void gen6_ppgtt_unmap_pages(struct i915_hw_ppgtt *ppgtt)
+-{
+-	int i;
+-
+-	if (ppgtt->pt_dma_addr) {
+-		for (i = 0; i < ppgtt->num_pd_entries; i++)
+-			pci_unmap_page(ppgtt->base.dev->pdev,
+-				       ppgtt->pt_dma_addr[i],
+-				       4096, PCI_DMA_BIDIRECTIONAL);
+-	}
+-}
+-
+-static void gen6_ppgtt_free(struct i915_hw_ppgtt *ppgtt)
+-{
+-	int i;
+ 
+-	kfree(ppgtt->pt_dma_addr);
+-	for (i = 0; i < ppgtt->num_pd_entries; i++)
+-		__free_page(ppgtt->pt_pages[i]);
+-	kfree(ppgtt->pt_pages);
++	return 0;
+ }
+ 
+ static void gen6_ppgtt_cleanup(struct i915_address_space *vm)
+@@ -1029,117 +886,23 @@
+ 	struct i915_hw_ppgtt *ppgtt =
+ 		container_of(vm, struct i915_hw_ppgtt, base);
+ 
+-	list_del(&vm->global_link);
+-	drm_mm_takedown(&ppgtt->base.mm);
+-	drm_mm_remove_node(&ppgtt->node);
+-
+-	gen6_ppgtt_unmap_pages(ppgtt);
+-	gen6_ppgtt_free(ppgtt);
++	drm_gem_object_unreference(&ppgtt->state->base);
+ }
+ 
+-static int gen6_ppgtt_allocate_page_directories(struct i915_hw_ppgtt *ppgtt)
++static int gen6_ppgtt_alloc(struct i915_hw_ppgtt *ppgtt)
+ {
+-	struct drm_device *dev = ppgtt->base.dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	bool retried = false;
+-	int ret;
+-
+ 	/* PPGTT PDEs reside in the GGTT and consists of 512 entries. The
+ 	 * allocator works in address space sizes, so it's multiplied by page
+ 	 * size. We allocate at the top of the GTT to avoid fragmentation.
+ 	 */
+-	BUG_ON(!drm_mm_initialized(&dev_priv->gtt.base.mm));
+-alloc:
+-	ret = drm_mm_insert_node_in_range_generic(&dev_priv->gtt.base.mm,
+-						  &ppgtt->node, GEN6_PD_SIZE,
+-						  GEN6_PD_ALIGN, 0,
+-						  0, dev_priv->gtt.base.total,
+-						  DRM_MM_TOPDOWN);
+-	if (ret == -ENOSPC && !retried) {
+-		ret = i915_gem_evict_something(dev, &dev_priv->gtt.base,
+-					       GEN6_PD_SIZE, GEN6_PD_ALIGN,
+-					       I915_CACHE_NONE,
+-					       0, dev_priv->gtt.base.total,
+-					       0);
+-		if (ret)
+-			return ret;
+-
+-		retried = true;
+-		goto alloc;
+-	}
+-
+-	if (ppgtt->node.start < dev_priv->gtt.mappable_end)
+-		DRM_DEBUG("Forced to use aperture for PDEs\n");
+-
+-	ppgtt->num_pd_entries = GEN6_PPGTT_PD_ENTRIES;
+-	return ret;
+-}
+-
+-static int gen6_ppgtt_allocate_page_tables(struct i915_hw_ppgtt *ppgtt)
+-{
+-	int i;
+-
+-	ppgtt->pt_pages = kcalloc(ppgtt->num_pd_entries, sizeof(struct page *),
+-				  GFP_KERNEL);
+-
+-	if (!ppgtt->pt_pages)
++	ppgtt->state = i915_gem_alloc_object(ppgtt->base.dev, GEN6_PD_SIZE);
++	if (ppgtt->state == NULL)
+ 		return -ENOMEM;
+ 
+-	for (i = 0; i < ppgtt->num_pd_entries; i++) {
+-		ppgtt->pt_pages[i] = alloc_page(GFP_KERNEL);
+-		if (!ppgtt->pt_pages[i]) {
+-			gen6_ppgtt_free(ppgtt);
+-			return -ENOMEM;
+-		}
+-	}
+-
+-	return 0;
+-}
+-
+-static int gen6_ppgtt_alloc(struct i915_hw_ppgtt *ppgtt)
+-{
+-	int ret;
+-
+-	ret = gen6_ppgtt_allocate_page_directories(ppgtt);
+-	if (ret)
+-		return ret;
+-
+-	ret = gen6_ppgtt_allocate_page_tables(ppgtt);
+-	if (ret) {
+-		drm_mm_remove_node(&ppgtt->node);
+-		return ret;
+-	}
+-
+-	ppgtt->pt_dma_addr = kcalloc(ppgtt->num_pd_entries, sizeof(dma_addr_t),
+-				     GFP_KERNEL);
+-	if (!ppgtt->pt_dma_addr) {
+-		drm_mm_remove_node(&ppgtt->node);
+-		gen6_ppgtt_free(ppgtt);
+-		return -ENOMEM;
+-	}
+-
+-	return 0;
+-}
+-
+-static int gen6_ppgtt_setup_page_tables(struct i915_hw_ppgtt *ppgtt)
+-{
+-	struct drm_device *dev = ppgtt->base.dev;
+-	int i;
+-
+-	for (i = 0; i < ppgtt->num_pd_entries; i++) {
+-		dma_addr_t pt_addr;
+-
+-		pt_addr = pci_map_page(dev->pdev, ppgtt->pt_pages[i], 0, 4096,
+-				       PCI_DMA_BIDIRECTIONAL);
+-
+-		if (pci_dma_mapping_error(dev->pdev, pt_addr)) {
+-			gen6_ppgtt_unmap_pages(ppgtt);
+-			return -EIO;
+-		}
+-
+-		ppgtt->pt_dma_addr[i] = pt_addr;
+-	}
++	ppgtt->state->pde = true;
+ 
++	ppgtt->num_pd_entries = GEN6_PPGTT_PD_ENTRIES;
++	ppgtt->alignment = GEN6_PD_ALIGN;
+ 	return 0;
+ }
+ 
+@@ -1151,13 +914,8 @@
+ 
+ 	ppgtt->base.pte_encode = dev_priv->gtt.base.pte_encode;
+ 	if (IS_GEN6(dev)) {
+-		ppgtt->enable = gen6_ppgtt_enable;
+ 		ppgtt->switch_mm = gen6_mm_switch;
+-	} else if (IS_HASWELL(dev)) {
+-		ppgtt->enable = gen7_ppgtt_enable;
+-		ppgtt->switch_mm = hsw_mm_switch;
+ 	} else if (IS_GEN7(dev)) {
+-		ppgtt->enable = gen7_ppgtt_enable;
+ 		ppgtt->switch_mm = gen7_mm_switch;
+ 	} else
+ 		BUG();
+@@ -1166,12 +924,6 @@
+ 	if (ret)
+ 		return ret;
+ 
+-	ret = gen6_ppgtt_setup_page_tables(ppgtt);
+-	if (ret) {
+-		gen6_ppgtt_free(ppgtt);
+-		return ret;
+-	}
+-
+ 	ppgtt->base.clear_range = gen6_ppgtt_clear_range;
+ 	ppgtt->base.insert_entries = gen6_ppgtt_insert_entries;
+ 	ppgtt->base.cleanup = gen6_ppgtt_cleanup;
+@@ -1179,68 +931,199 @@
+ 	ppgtt->base.total =  ppgtt->num_pd_entries * I915_PPGTT_PT_ENTRIES * PAGE_SIZE;
+ 	ppgtt->debug_dump = gen6_dump_ppgtt;
+ 
+-	ppgtt->pd_offset =
+-		ppgtt->node.start / PAGE_SIZE * sizeof(gen6_gtt_pte_t);
++	return ppgtt->base.clear_range(&ppgtt->base,
++				       0, ppgtt->base.total,
++				       true);
++}
+ 
+-	ppgtt->base.clear_range(&ppgtt->base, 0, ppgtt->base.total, true);
++static void i915_ggtt_flush(struct drm_i915_private *dev_priv)
++{
++	if (INTEL_INFO(dev_priv->dev)->gen < 6) {
++		intel_gtt_chipset_flush();
++	} else {
++		I915_WRITE(GFX_FLSH_CNTL_GEN6, GFX_FLSH_CNTL_EN);
++		POSTING_READ(GFX_FLSH_CNTL_GEN6);
++	}
++}
++
++static int pde_bind_vma(struct i915_vma *vma,
++			enum i915_cache_level cache_level,
++			u32 flags)
++{
++	struct sg_table *pages = vma->obj->pages;
++	struct drm_i915_private *i915 = to_i915(vma->vm->dev);
++	gen6_gtt_pte_t __iomem *pd_addr;
++	struct sg_page_iter sg_iter;
++
++	if (WARN_ON(flags != GLOBAL_BIND))
++		return -EINVAL;
+ 
+-	DRM_DEBUG_DRIVER("Allocated pde space (%ldM) at GTT entry: %lx\n",
+-			 ppgtt->node.size >> 20,
+-			 ppgtt->node.start / PAGE_SIZE);
++	if (WARN_ON(vma->node.start & 0x3f))
++		return -EINVAL;
+ 
++	if (vma->bound & GLOBAL_BIND)
++		return 0;
++
++	pd_addr = (gen6_gtt_pte_t __iomem*)i915->gtt.gsm;
++	pd_addr += vma->node.start >> PAGE_SHIFT;
++	for_each_sg_page(pages->sgl, &sg_iter, pages->nents, 0) {
++		dma_addr_t addr = (sg_dma_address(sg_iter.sg) +
++				   (sg_iter.sg_pgoffset << PAGE_SHIFT));
++		writel(GEN6_PDE_ADDR_ENCODE(addr) | GEN6_PDE_VALID, pd_addr++);
++	}
++	i915_ggtt_flush(i915);
++
++	/* Skips a second call when pinning */
++	vma->bound = GLOBAL_BIND;
++	return 0;
++}
++
++static int pde_unbind_vma(struct i915_vma *vma)
++{
++	vma->bound = 0;
+ 	return 0;
+ }
+ 
+-int i915_gem_init_ppgtt(struct drm_device *dev, struct i915_hw_ppgtt *ppgtt)
++static int __hw_ppgtt_init(struct drm_device *dev, struct i915_hw_ppgtt *ppgtt)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	int ret = 0;
+ 
+ 	ppgtt->base.dev = dev;
+ 	ppgtt->base.scratch = dev_priv->gtt.base.scratch;
++	INIT_LIST_HEAD(&ppgtt->base.vma_list);
+ 
+ 	if (INTEL_INFO(dev)->gen < 8)
+-		ret = gen6_ppgtt_init(ppgtt);
+-	else if (IS_GEN8(dev))
+-		ret = gen8_ppgtt_init(ppgtt, dev_priv->gtt.base.total);
++		return gen6_ppgtt_init(ppgtt);
++	else if (IS_GEN8(dev) || IS_GEN9(dev))
++		return gen8_ppgtt_init(ppgtt, dev_priv->gtt.base.total);
+ 	else
+ 		BUG();
++}
++int i915_ppgtt_init(struct drm_device *dev, struct i915_hw_ppgtt *ppgtt)
++{
++	int ret;
+ 
+-	if (!ret) {
+-		struct drm_i915_private *dev_priv = dev->dev_private;
+-		kref_init(&ppgtt->ref);
+-		drm_mm_init(&ppgtt->base.mm, ppgtt->base.start,
+-			    ppgtt->base.total);
+-		i915_init_vm(dev_priv, &ppgtt->base);
+-		if (INTEL_INFO(dev)->gen < 8) {
+-			gen6_write_pdes(ppgtt);
+-			DRM_DEBUG("Adding PPGTT at offset %x\n",
+-				  ppgtt->pd_offset << 10);
+-		}
++	ret = __hw_ppgtt_init(dev, ppgtt);
++	if (ret)
++		return ret;
++
++	drm_mm_init(&ppgtt->base.mm, ppgtt->base.start, ppgtt->base.total);
++	i915_init_vm(to_i915(dev), &ppgtt->base);
++
++	return 0;
++}
++
++int i915_ppgtt_init_hw(struct drm_device *dev)
++{
++	int ret;
++
++	if (!USES_PPGTT(dev))
++		return 0;
++
++	/* In the case of execlists, PPGTT is enabled by the context descriptor
++	 * and the PDPs are contained within the context itself.  We don't
++	 * need to do anything here. */
++	if (RCS_ENGINE(dev)->execlists_enabled)
++		return 0;
++
++	if (IS_GEN6(dev))
++		ret = gen6_ppgtt_enable(dev);
++	else if (IS_GEN7(dev))
++		ret = gen7_ppgtt_enable(dev);
++	else if (INTEL_INFO(dev)->gen >= 8)
++		ret = gen8_ppgtt_enable(dev);
++	else {
++		WARN_ON(1);
++		ret = -ENODEV;
+ 	}
+ 
+ 	return ret;
+ }
+ 
+-static void
++struct i915_hw_ppgtt *
++i915_ppgtt_create(struct drm_device *dev, struct drm_i915_file_private *fpriv)
++{
++	struct i915_hw_ppgtt *ppgtt;
++	int ret;
++
++	ppgtt = kzalloc(sizeof(*ppgtt), GFP_KERNEL);
++	if (!ppgtt)
++		return ERR_PTR(-ENOMEM);
++
++	ret = i915_ppgtt_init(dev, ppgtt);
++	if (ret) {
++		kfree(ppgtt);
++		return ERR_PTR(ret);
++	}
++
++	ppgtt->file_priv = fpriv;
++
++	trace_i915_vm_create(&ppgtt->base);
++
++	return ppgtt;
++}
++
++void __i915_vm_free(struct kref *kref)
++{
++	struct i915_address_space *vm =
++		container_of(kref, struct i915_address_space, ref);
++
++	DRM_DEBUG_DRIVER("VM %p freed\n", vm);
++	trace_i915_vm_free(vm);
++
++	/* vmas should already be unbound */
++	WARN_ON(!list_empty(&vm->vma_list));
++	WARN_ON(!list_empty(&vm->active_list));
++	WARN_ON(!list_empty(&vm->inactive_list));
++
++	list_del(&vm->global_link);
++	drm_mm_takedown(&vm->mm);
++
++	vm->cleanup(vm);
++	kfree(vm);
++}
++
++static int
+ ppgtt_bind_vma(struct i915_vma *vma,
+ 	       enum i915_cache_level cache_level,
+ 	       u32 flags)
+ {
++	int ret;
++
++	if (WARN_ON(flags & GLOBAL_BIND))
++		return -EINVAL;
++
+ 	/* Currently applicable only to VLV */
+ 	if (vma->obj->gt_ro)
+ 		flags |= PTE_READ_ONLY;
+ 
+-	vma->vm->insert_entries(vma->vm, vma->obj->pages, vma->node.start,
+-				cache_level, flags);
++	if (flags == vma->bound)
++		return 0;
++
++	if (flags == REBIND) {
++		flags &= ~REBIND;
++		flags |= LOCAL_BIND;
++	}
++
++	ret = vma->vm->insert_entries(vma->vm,
++				      vma->obj->pages, vma->node.start,
++				      cache_level, flags);
++	if (ret)
++		return ret;
++
++	vma->vm->dirty = true;
++	vma->bound = flags;
++	return 0;
+ }
+ 
+-static void ppgtt_unbind_vma(struct i915_vma *vma)
++static int ppgtt_unbind_vma(struct i915_vma *vma)
+ {
+ 	vma->vm->clear_range(vma->vm,
+ 			     vma->node.start,
+ 			     vma->obj->base.size,
+ 			     true);
++	vma->bound = 0;
++	return 0;
+ }
+ 
+ extern int intel_iommu_gfx_mapped;
+@@ -1284,18 +1167,18 @@
+ void i915_check_and_clear_faults(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring;
++	struct intel_engine_cs *engine;
+ 	int i;
+ 
+ 	if (INTEL_INFO(dev)->gen < 6)
+ 		return;
+ 
+-	for_each_ring(ring, dev_priv, i) {
++	for_each_engine(engine, dev_priv, i) {
+ 		u32 fault_reg;
+-		fault_reg = I915_READ(RING_FAULT_REG(ring));
++		fault_reg = I915_READ(RING_FAULT_REG(engine));
+ 		if (fault_reg & RING_FAULT_VALID) {
+ 			DRM_DEBUG_DRIVER("Unexpected fault\n"
+-					 "\tAddr: 0x%08lx\\n"
++					 "\tAddr: 0x%08lx\n"
+ 					 "\tAddress space: %s\n"
+ 					 "\tSource ID: %d\n"
+ 					 "\tType: %d\n",
+@@ -1303,21 +1186,11 @@
+ 					 fault_reg & RING_FAULT_GTTSEL_MASK ? "GGTT" : "PPGTT",
+ 					 RING_FAULT_SRCID(fault_reg),
+ 					 RING_FAULT_FAULT_TYPE(fault_reg));
+-			I915_WRITE(RING_FAULT_REG(ring),
++			I915_WRITE(RING_FAULT_REG(engine),
+ 				   fault_reg & ~RING_FAULT_VALID);
+ 		}
+ 	}
+-	POSTING_READ(RING_FAULT_REG(&dev_priv->ring[RCS]));
+-}
+-
+-static void i915_ggtt_flush(struct drm_i915_private *dev_priv)
+-{
+-	if (INTEL_INFO(dev_priv->dev)->gen < 6) {
+-		intel_gtt_chipset_flush();
+-	} else {
+-		I915_WRITE(GFX_FLSH_CNTL_GEN6, GFX_FLSH_CNTL_EN);
+-		POSTING_READ(GFX_FLSH_CNTL_GEN6);
+-	}
++	POSTING_READ(RING_FAULT_REG(RCS_ENGINE(dev_priv)));
+ }
+ 
+ void i915_gem_suspend_gtt_mappings(struct drm_device *dev)
+@@ -1344,7 +1217,6 @@
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct drm_i915_gem_object *obj;
+-	struct i915_address_space *vm;
+ 
+ 	i915_check_and_clear_faults(dev);
+ 
+@@ -1361,12 +1233,7 @@
+ 			continue;
+ 
+ 		i915_gem_clflush_object(obj, obj->pin_display);
+-		/* The bind_vma code tries to be smart about tracking mappings.
+-		 * Unfortunately above, we've just wiped out the mappings
+-		 * without telling our object about it. So we need to fake it.
+-		 */
+-		obj->has_global_gtt_mapping = 0;
+-		vma->bind_vma(vma, obj->cache_level, GLOBAL_BIND);
++		vma->bind_vma(vma, obj->cache_level, REBIND);
+ 	}
+ 
+ 
+@@ -1379,17 +1246,6 @@
+ 		return;
+ 	}
+ 
+-	list_for_each_entry(vm, &dev_priv->vm_list, global_link) {
+-		/* TODO: Perhaps it shouldn't be gen6 specific */
+-		if (i915_is_ggtt(vm)) {
+-			if (dev_priv->mm.aliasing_ppgtt)
+-				gen6_write_pdes(dev_priv->mm.aliasing_ppgtt);
+-			continue;
+-		}
+-
+-		gen6_write_pdes(container_of(vm, struct i915_hw_ppgtt, base));
+-	}
+-
+ 	i915_ggtt_flush(dev_priv);
+ }
+ 
+@@ -1408,18 +1264,18 @@
+ 
+ static inline void gen8_set_pte(void __iomem *addr, gen8_gtt_pte_t pte)
+ {
+-#ifdef writeq
++#if defined(writeq)
+ 	writeq(pte, addr);
+ #else
+-	iowrite32((u32)pte, addr);
+-	iowrite32(pte >> 32, addr + 4);
++	iowrite32(lower_32_bits(pte), addr);
++	iowrite32(upper_32_bits(pte), addr + 4);
+ #endif
+ }
+ 
+-static void gen8_ggtt_insert_entries(struct i915_address_space *vm,
+-				     struct sg_table *st,
+-				     uint64_t start,
+-				     enum i915_cache_level level, u32 unused)
++static int gen8_ggtt_insert_entries(struct i915_address_space *vm,
++				    struct sg_table *st,
++				    uint64_t start,
++				    enum i915_cache_level level, u32 unused)
+ {
+ 	struct drm_i915_private *dev_priv = vm->dev->dev_private;
+ 	unsigned first_entry = start >> PAGE_SHIFT;
+@@ -1454,6 +1310,8 @@
+ 	 */
+ 	I915_WRITE(GFX_FLSH_CNTL_GEN6, GFX_FLSH_CNTL_EN);
+ 	POSTING_READ(GFX_FLSH_CNTL_GEN6);
++
++	return 0;
+ }
+ 
+ /*
+@@ -1462,10 +1320,10 @@
+  * within the global GTT as well as accessible by the GPU through the GMADR
+  * mapped BAR (dev_priv->mm.gtt->gtt).
+  */
+-static void gen6_ggtt_insert_entries(struct i915_address_space *vm,
+-				     struct sg_table *st,
+-				     uint64_t start,
+-				     enum i915_cache_level level, u32 flags)
++static int gen6_ggtt_insert_entries(struct i915_address_space *vm,
++				    struct sg_table *st,
++				    uint64_t start,
++				    enum i915_cache_level level, u32 flags)
+ {
+ 	struct drm_i915_private *dev_priv = vm->dev->dev_private;
+ 	unsigned first_entry = start >> PAGE_SHIFT;
+@@ -1498,12 +1356,14 @@
+ 	 */
+ 	I915_WRITE(GFX_FLSH_CNTL_GEN6, GFX_FLSH_CNTL_EN);
+ 	POSTING_READ(GFX_FLSH_CNTL_GEN6);
++
++	return 0;
+ }
+ 
+-static void gen8_ggtt_clear_range(struct i915_address_space *vm,
+-				  uint64_t start,
+-				  uint64_t length,
+-				  bool use_scratch)
++static int gen8_ggtt_clear_range(struct i915_address_space *vm,
++				 uint64_t start,
++				 uint64_t length,
++				 bool use_scratch)
+ {
+ 	struct drm_i915_private *dev_priv = vm->dev->dev_private;
+ 	unsigned first_entry = start >> PAGE_SHIFT;
+@@ -1524,12 +1384,14 @@
+ 	for (i = 0; i < num_entries; i++)
+ 		gen8_set_pte(&gtt_base[i], scratch_pte);
+ 	readl(gtt_base);
++
++	return 0;
+ }
+ 
+-static void gen6_ggtt_clear_range(struct i915_address_space *vm,
+-				  uint64_t start,
+-				  uint64_t length,
+-				  bool use_scratch)
++static int gen6_ggtt_clear_range(struct i915_address_space *vm,
++				 uint64_t start,
++				 uint64_t length,
++				 bool use_scratch)
+ {
+ 	struct drm_i915_private *dev_priv = vm->dev->dev_private;
+ 	unsigned first_entry = start >> PAGE_SHIFT;
+@@ -1549,49 +1411,117 @@
+ 	for (i = 0; i < num_entries; i++)
+ 		iowrite32(scratch_pte, &gtt_base[i]);
+ 	readl(gtt_base);
++
++	return 0;
+ }
+ 
++static void mark_global_obj(struct i915_vma *vma)
++{
++	struct drm_i915_gem_object *obj = vma->obj;
++	bool mappable, fenceable;
++	u32 fence_size, fence_alignment;
++
++	fence_size = i915_gem_get_gtt_size(obj->base.dev,
++					   obj->base.size,
++					   obj->tiling_mode);
++	fence_alignment = i915_gem_get_gtt_alignment(obj->base.dev,
++						     obj->base.size,
++						     obj->tiling_mode,
++						     true);
++
++	fenceable = (vma->node.size == fence_size &&
++		     (vma->node.start & (fence_alignment - 1)) == 0);
++
++	mappable = (vma->node.start + obj->base.size <=
++		    to_i915(obj->base.dev)->gtt.mappable_end);
+ 
+-static void i915_ggtt_bind_vma(struct i915_vma *vma,
+-			       enum i915_cache_level cache_level,
+-			       u32 unused)
++	obj->map_and_fenceable = mappable && fenceable;
++}
++
++static int i915_ggtt_bind_vma(struct i915_vma *vma,
++			      enum i915_cache_level cache_level,
++			      u32 unused)
+ {
+ 	const unsigned long entry = vma->node.start >> PAGE_SHIFT;
+ 	unsigned int flags = (cache_level == I915_CACHE_NONE) ?
+ 		AGP_USER_MEMORY : AGP_USER_CACHED_MEMORY;
+ 
++	if (vma->bound && (flags & REBIND) == 0)
++		return 0;
++
+ 	BUG_ON(!i915_is_ggtt(vma->vm));
+ 	intel_gtt_insert_sg_entries(vma->obj->pages, entry, flags);
+-	vma->obj->has_global_gtt_mapping = 1;
++	vma->bound = GLOBAL_BIND;
++	vma->vm->dirty = true;
++
++	mark_global_obj(vma);
++
++	return 0;
+ }
+ 
+-static void i915_ggtt_clear_range(struct i915_address_space *vm,
+-				  uint64_t start,
+-				  uint64_t length,
+-				  bool unused)
++static int i915_ggtt_clear_range(struct i915_address_space *vm,
++				 uint64_t start,
++				 uint64_t length,
++				 bool unused)
+ {
+ 	unsigned first_entry = start >> PAGE_SHIFT;
+ 	unsigned num_entries = length >> PAGE_SHIFT;
++
+ 	intel_gtt_clear_range(first_entry, num_entries);
++	return 0;
++}
++
++static int nop_clear_range(struct i915_address_space *vm,
++			   uint64_t start,
++			   uint64_t length,
++			   bool use_scratch)
++{
++	return 0;
+ }
+ 
+-static void i915_ggtt_unbind_vma(struct i915_vma *vma)
++static int i915_ggtt_unbind_vma(struct i915_vma *vma)
+ {
++	struct drm_i915_gem_object *obj = vma->obj;
+ 	const unsigned int first = vma->node.start >> PAGE_SHIFT;
+-	const unsigned int size = vma->obj->base.size >> PAGE_SHIFT;
++	const unsigned int size = obj->base.size >> PAGE_SHIFT;
++	int ret;
+ 
+ 	BUG_ON(!i915_is_ggtt(vma->vm));
+-	vma->obj->has_global_gtt_mapping = 0;
++	BUG_ON(vma->bound == 0);
++
++	i915_gem_object_finish_gtt(obj);
++
++	/* release the fence reg _after_ flushing */
++	ret = i915_gem_object_put_fence(obj);
++	if (WARN_ON(ret)) /* should be idle already */
++		return ret;
++
++	obj->map_and_fenceable = false;
++
+ 	intel_gtt_clear_range(first, size);
++	vma->bound = 0;
++
++	return 0;
+ }
+ 
+-static void ggtt_bind_vma(struct i915_vma *vma,
+-			  enum i915_cache_level cache_level,
+-			  u32 flags)
++static int ggtt_bind_vma(struct i915_vma *vma,
++			 enum i915_cache_level cache_level,
++			 u32 flags)
+ {
+ 	struct drm_device *dev = vma->vm->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct drm_i915_gem_object *obj = vma->obj;
++	int ret;
++
++	if (flags == REBIND) {
++		flags = vma->bound;
++		vma->bound = 0;
++	}
++
++	if (!dev_priv->mm.aliasing_ppgtt) {
++		flags |= GLOBAL_BIND;
++		flags &= ~LOCAL_BIND;
++	}
+ 
+ 	/* Currently applicable only to VLV */
+ 	if (obj->gt_ro)
+@@ -1608,50 +1538,69 @@
+ 	 * "gtt mappable", SNB errata, or if specified via special execbuf
+ 	 * flags. At all other times, the GPU will use the aliasing PPGTT.
+ 	 */
+-	if (!dev_priv->mm.aliasing_ppgtt || flags & GLOBAL_BIND) {
+-		if (!obj->has_global_gtt_mapping ||
+-		    (cache_level != obj->cache_level)) {
+-			vma->vm->insert_entries(vma->vm, obj->pages,
+-						vma->node.start,
+-						cache_level, flags);
+-			obj->has_global_gtt_mapping = 1;
+-		}
++	if ((flags & ~vma->bound) & GLOBAL_BIND) {
++		ret = vma->vm->insert_entries(vma->vm, obj->pages,
++					      vma->node.start,
++					      cache_level, flags);
++		if (ret)
++			return ret;
++
++		vma->bound |= GLOBAL_BIND;
++		vma->vm->dirty = true;
++		mark_global_obj(vma);
+ 	}
+ 
+-	if (dev_priv->mm.aliasing_ppgtt &&
+-	    (!obj->has_aliasing_ppgtt_mapping ||
+-	     (cache_level != obj->cache_level))) {
++	if ((flags & ~vma->bound) & LOCAL_BIND) {
+ 		struct i915_hw_ppgtt *appgtt = dev_priv->mm.aliasing_ppgtt;
+-		appgtt->base.insert_entries(&appgtt->base,
+-					    vma->obj->pages,
+-					    vma->node.start,
+-					    cache_level, flags);
+-		vma->obj->has_aliasing_ppgtt_mapping = 1;
++
++		ret = appgtt->base.insert_entries(&appgtt->base,
++						  obj->pages,
++						  vma->node.start,
++						  cache_level, flags);
++		if (ret)
++			return ret;
++
++		vma->bound |= LOCAL_BIND;
++		vma->vm->dirty = true;
+ 	}
++
++	vma->bound |= flags;
++	return 0;
+ }
+ 
+-static void ggtt_unbind_vma(struct i915_vma *vma)
++static int ggtt_unbind_vma(struct i915_vma *vma)
+ {
+ 	struct drm_device *dev = vma->vm->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct drm_i915_gem_object *obj = vma->obj;
++	int ret;
++
++	if (vma->bound & GLOBAL_BIND) {
++		i915_gem_object_finish_gtt(obj);
++
++		/* release the fence reg _after_ flushing */
++		ret = i915_gem_object_put_fence(obj);
++		if (WARN_ON(ret)) /* should be idle already */
++			return ret;
++
++		obj->map_and_fenceable = false;
+ 
+-	if (obj->has_global_gtt_mapping) {
+ 		vma->vm->clear_range(vma->vm,
+ 				     vma->node.start,
+ 				     obj->base.size,
+ 				     true);
+-		obj->has_global_gtt_mapping = 0;
+ 	}
+ 
+-	if (obj->has_aliasing_ppgtt_mapping) {
++	if (vma->bound & LOCAL_BIND) {
+ 		struct i915_hw_ppgtt *appgtt = dev_priv->mm.aliasing_ppgtt;
+ 		appgtt->base.clear_range(&appgtt->base,
+ 					 vma->node.start,
+ 					 obj->base.size,
+ 					 true);
+-		obj->has_aliasing_ppgtt_mapping = 0;
+ 	}
++
++	vma->bound = 0;
++	return 0;
+ }
+ 
+ void i915_gem_gtt_finish_object(struct drm_i915_gem_object *obj)
+@@ -1687,10 +1636,10 @@
+ 	}
+ }
+ 
+-void i915_gem_setup_global_gtt(struct drm_device *dev,
+-			       unsigned long start,
+-			       unsigned long mappable_end,
+-			       unsigned long end)
++int i915_gem_setup_global_gtt(struct drm_device *dev,
++			      unsigned long start,
++			      unsigned long mappable_end,
++			      unsigned long end)
+ {
+ 	/* Let GEM Manage all of the aperture.
+ 	 *
+@@ -1702,45 +1651,71 @@
+ 	 * of the aperture.
+ 	 */
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct i915_address_space *ggtt_vm = &dev_priv->gtt.base;
++	struct i915_address_space *ggtt = &dev_priv->gtt.base;
+ 	struct drm_mm_node *entry;
+ 	struct drm_i915_gem_object *obj;
+ 	unsigned long hole_start, hole_end;
++	int ret;
+ 
+ 	BUG_ON(mappable_end > end);
+ 
+ 	/* Subtract the guard page ... */
+-	drm_mm_init(&ggtt_vm->mm, start, end - start - PAGE_SIZE);
++	drm_mm_init(&ggtt->mm, start, end - start - PAGE_SIZE);
+ 	if (!HAS_LLC(dev))
+ 		dev_priv->gtt.base.mm.color_adjust = i915_gtt_color_adjust;
+ 
+ 	/* Mark any preallocated objects as occupied */
+ 	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list) {
+-		struct i915_vma *vma = i915_gem_obj_to_vma(obj, ggtt_vm);
+-		int ret;
++		struct i915_vma *vma = i915_gem_obj_to_ggtt(obj);
++
++		WARN_ON(drm_mm_node_allocated(&vma->node));
++
+ 		DRM_DEBUG_KMS("reserving preallocated space: %lx + %zx\n",
+-			      i915_gem_obj_ggtt_offset(obj), obj->base.size);
++			      vma->node.start, obj->base.size);
++		ret = drm_mm_reserve_node(&ggtt->mm, &vma->node);
++		if (ret) {
++			DRM_DEBUG_KMS("Reservation failed: %i\n", ret);
++			return ret;
++		}
++		vma->bound |= GLOBAL_BIND;
++		mark_global_obj(vma);
++	}
+ 
+-		WARN_ON(i915_gem_obj_ggtt_bound(obj));
+-		ret = drm_mm_reserve_node(&ggtt_vm->mm, &vma->node);
+-		if (ret)
+-			DRM_DEBUG_KMS("Reservation failed\n");
+-		obj->has_global_gtt_mapping = 1;
++	ggtt->start = start;
++	ggtt->total = end - start;
++
++	if (USES_PPGTT(dev) && !USES_FULL_PPGTT(dev)) {
++		struct i915_hw_ppgtt *ppgtt;
++
++		ppgtt = kzalloc(sizeof(*ppgtt), GFP_KERNEL);
++		if (!ppgtt)
++			return -ENOMEM;
++
++		ret = __hw_ppgtt_init(dev, ppgtt);
++		if (ret== 0 && ppgtt->state)
++			ret = i915_gem_object_ggtt_pin(ppgtt->state,
++						       ppgtt->alignment,
++						       0);
++		if (ret == 0)
++			dev_priv->mm.aliasing_ppgtt = ppgtt;
+ 	}
+ 
+-	dev_priv->gtt.base.start = start;
+-	dev_priv->gtt.base.total = end - start;
++	if (USES_FULL_PPGTT(dev) || dev_priv->mm.aliasing_ppgtt)
++		ggtt->clear_range = nop_clear_range;
+ 
+ 	/* Clear any non-preallocated blocks */
+-	drm_mm_for_each_hole(entry, &ggtt_vm->mm, hole_start, hole_end) {
++	drm_mm_for_each_hole(entry, &ggtt->mm, hole_start, hole_end) {
+ 		DRM_DEBUG_KMS("clearing unused GTT space: [%lx, %lx]\n",
+ 			      hole_start, hole_end);
+-		ggtt_vm->clear_range(ggtt_vm, hole_start,
+-				     hole_end - hole_start, true);
++		ggtt->clear_range(ggtt,
++				  hole_start, hole_end - hole_start,
++				  true);
+ 	}
+ 
+ 	/* And finally clear the reserved guard page */
+-	ggtt_vm->clear_range(ggtt_vm, end - PAGE_SIZE, PAGE_SIZE, true);
++	ggtt->clear_range(ggtt, end - PAGE_SIZE, PAGE_SIZE, true);
++
++	return 0;
+ }
+ 
+ void i915_gem_init_global_gtt(struct drm_device *dev)
+@@ -1754,6 +1729,27 @@
+ 	i915_gem_setup_global_gtt(dev, 0, mappable_size, gtt_size);
+ }
+ 
++void i915_global_gtt_cleanup(struct drm_device *dev)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct i915_address_space *vm = &dev_priv->gtt.base;
++
++	lockdep_assert_held(&dev->struct_mutex);
++
++	if (dev_priv->mm.aliasing_ppgtt) {
++		struct i915_hw_ppgtt *ppgtt = dev_priv->mm.aliasing_ppgtt;
++
++		ppgtt->base.cleanup(&ppgtt->base);
++	}
++
++	if (drm_mm_initialized(&vm->mm)) {
++		drm_mm_takedown(&vm->mm);
++		list_del(&vm->global_link);
++	}
++
++	vm->cleanup(vm);
++}
++
+ static int setup_scratch_page(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+@@ -1763,7 +1759,6 @@
+ 	page = alloc_page(GFP_KERNEL | GFP_DMA32 | __GFP_ZERO);
+ 	if (page == NULL)
+ 		return -ENOMEM;
+-	get_page(page);
+ 	set_pages_uc(page, 1);
+ 
+ #ifdef CONFIG_INTEL_IOMMU
+@@ -1788,7 +1783,6 @@
+ 	set_pages_wb(page, 1);
+ 	pci_unmap_page(dev->pdev, dev_priv->gtt.base.scratch.addr,
+ 		       PAGE_SIZE, PCI_DMA_BIDIRECTIONAL);
+-	put_page(page);
+ 	__free_page(page);
+ }
+ 
+@@ -1858,6 +1852,18 @@
+ 		return (gmch_ctrl - 0x17 + 9) << 22;
+ }
+ 
++static size_t gen9_get_stolen_size(u16 gen9_gmch_ctl)
++{
++	gen9_gmch_ctl >>= BDW_GMCH_GMS_SHIFT;
++	gen9_gmch_ctl &= BDW_GMCH_GMS_MASK;
++
++	if (gen9_gmch_ctl < 0xf0)
++		return gen9_gmch_ctl << 25; /* 32 MB units */
++	else
++		/* 4MB increments starting at 0xf0 for 4MB */
++		return (gen9_gmch_ctl - 0xf0 + 1) << 22;
++}
++
+ static int ggtt_probe_common(struct drm_device *dev,
+ 			     size_t gtt_size)
+ {
+@@ -1901,6 +1907,22 @@
+ 	      GEN8_PPAT(6, GEN8_PPAT_WB | GEN8_PPAT_LLCELLC | GEN8_PPAT_AGE(2)) |
+ 	      GEN8_PPAT(7, GEN8_PPAT_WB | GEN8_PPAT_LLCELLC | GEN8_PPAT_AGE(3));
+ 
++	if (!USES_PPGTT(dev_priv->dev))
++		/* Spec: "For GGTT, there is NO pat_sel[2:0] from the entry,
++		 * so RTL will always use the value corresponding to
++		 * pat_sel = 000".
++		 * So let's disable cache for GGTT to avoid screen corruptions.
++		 * MOCS still can be used though.
++		 * - System agent ggtt writes (i.e. cpu gtt mmaps) already work
++		 * before this patch, i.e. the same uncached + snooping access
++		 * like on gen6/7 seems to be in effect.
++		 * - So this just fixes blitter/render access. Again it looks
++		 * like it's not just uncached access, but uncached + snooping.
++		 * So we can still hold onto all our assumptions wrt cpu
++		 * clflushing on LLC machines.
++		 */
++		pat = GEN8_PPAT(0, GEN8_PPAT_UC);
++
+ 	/* XXX: spec defines this as 2 distinct registers. It's unclear if a 64b
+ 	 * write would work. */
+ 	I915_WRITE(GEN8_PRIVATE_PAT, pat);
+@@ -1917,9 +1939,17 @@
+ 	 * Only the snoop bit has meaning for CHV, the rest is
+ 	 * ignored.
+ 	 *
+-	 * Note that the harware enforces snooping for all page
+-	 * table accesses. The snoop bit is actually ignored for
+-	 * PDEs.
++	 * The hardware will never snoop for certain types of accesses:
++	 * - CPU GTT (GMADR->GGTT->no snoop->memory)
++	 * - PPGTT page tables
++	 * - some other special cycles
++	 *
++	 * As with BDW, we also need to consider the following for GT accesses:
++	 * "For GGTT, there is NO pat_sel[2:0] from the entry,
++	 * so RTL will always use the value corresponding to
++	 * pat_sel = 000".
++	 * Which means we must set the snoop bit in PAT entry 0
++	 * in order to keep the global status page working.
+ 	 */
+ 	pat = GEN8_PPAT(0, CHV_PPAT_SNOOP) |
+ 	      GEN8_PPAT(1, 0) |
+@@ -1954,7 +1984,10 @@
+ 
+ 	pci_read_config_word(dev->pdev, SNB_GMCH_CTRL, &snb_gmch_ctl);
+ 
+-	if (IS_CHERRYVIEW(dev)) {
++	if (INTEL_INFO(dev)->gen >= 9) {
++		*stolen = gen9_get_stolen_size(snb_gmch_ctl);
++		gtt_size = gen8_get_total_gtt_size(snb_gmch_ctl);
++	} else if (IS_CHERRYVIEW(dev)) {
+ 		*stolen = chv_get_stolen_size(snb_gmch_ctl);
+ 		gtt_size = chv_get_total_gtt_size(snb_gmch_ctl);
+ 	} else {
+@@ -2022,10 +2055,6 @@
+ 
+ 	struct i915_gtt *gtt = container_of(vm, struct i915_gtt, base);
+ 
+-	if (drm_mm_initialized(&vm->mm)) {
+-		drm_mm_takedown(&vm->mm);
+-		list_del(&vm->global_link);
+-	}
+ 	iounmap(gtt->gsm);
+ 	teardown_scratch_page(vm->dev);
+ }
+@@ -2058,10 +2087,6 @@
+ 
+ static void i915_gmch_remove(struct i915_address_space *vm)
+ {
+-	if (drm_mm_initialized(&vm->mm)) {
+-		drm_mm_takedown(&vm->mm);
+-		list_del(&vm->global_link);
+-	}
+ 	intel_gmch_remove();
+ }
+ 
+@@ -2071,6 +2096,9 @@
+ 	struct i915_gtt *gtt = &dev_priv->gtt;
+ 	int ret;
+ 
++	INIT_LIST_HEAD(&gtt->base.vma_list);
++	kref_init(&gtt->base.ref);
++
+ 	if (INTEL_INFO(dev)->gen <= 5) {
+ 		gtt->gtt_probe = i915_gmch_probe;
+ 		gtt->base.cleanup = i915_gmch_remove;
+@@ -2114,30 +2142,45 @@
+ 	 * do this now so that we can print out any log messages once rather
+ 	 * than every time we check intel_enable_ppgtt().
+ 	 */
+-	i915.enable_ppgtt = sanitize_enable_ppgtt(dev, i915.enable_ppgtt);
+-	DRM_DEBUG_DRIVER("ppgtt mode: %i\n", i915.enable_ppgtt);
++	i915_module.enable_ppgtt =
++	       	sanitize_enable_ppgtt(dev, i915_module.enable_ppgtt);
++	DRM_DEBUG_DRIVER("ppgtt mode: %i\n", i915_module.enable_ppgtt);
+ 
+ 	return 0;
+ }
+ 
+-static struct i915_vma *__i915_gem_vma_create(struct drm_i915_gem_object *obj,
+-					      struct i915_address_space *vm)
++static struct i915_vma *__i915_vma_create(struct drm_i915_gem_object *obj,
++					  struct i915_address_space *vm)
+ {
+-	struct i915_vma *vma = kzalloc(sizeof(*vma), GFP_KERNEL);
++	struct i915_vma *vma;
++	int n;
++
++	vma = kzalloc(sizeof(*vma), GFP_KERNEL);
+ 	if (vma == NULL)
+ 		return ERR_PTR(-ENOMEM);
+ 
+-	INIT_LIST_HEAD(&vma->vma_link);
+-	INIT_LIST_HEAD(&vma->mm_list);
+-	INIT_LIST_HEAD(&vma->exec_list);
+-	vma->vm = vm;
++	kref_init(&vma->kref);
++	vma->vm = i915_vm_get(vm);
+ 	vma->obj = obj;
++	list_add(&vma->vm_link, &vm->vma_list);
++
++	for (n = 0; n < I915_NUM_ENGINES; n++)
++		INIT_LIST_HEAD(&vma->last_read[n].engine_link);
++
++	INIT_LIST_HEAD(&vma->mm_list);
++	INIT_LIST_HEAD(&vma->exec_link);
+ 
+ 	switch (INTEL_INFO(vm->dev)->gen) {
++	case 9:
+ 	case 8:
+ 	case 7:
+ 	case 6:
+-		if (i915_is_ggtt(vm)) {
++		if (obj->pde) {
++			if (WARN_ON(!i915_is_ggtt(vm)))
++				return ERR_PTR(-EINVAL);
++			vma->unbind_vma = pde_unbind_vma;
++			vma->bind_vma = pde_bind_vma;
++		} else if (i915_is_ggtt(vm)) {
+ 			vma->unbind_vma = ggtt_unbind_vma;
+ 			vma->bind_vma = ggtt_bind_vma;
+ 		} else {
+@@ -2159,22 +2202,37 @@
+ 
+ 	/* Keep GGTT vmas first to make debug easier */
+ 	if (i915_is_ggtt(vm))
+-		list_add(&vma->vma_link, &obj->vma_list);
++		list_add(&vma->obj_link, &obj->vma_list);
+ 	else
+-		list_add_tail(&vma->vma_link, &obj->vma_list);
++		list_add_tail(&vma->obj_link, &obj->vma_list);
+ 
+ 	return vma;
+ }
+ 
++void __i915_vma_free(struct kref *kref)
++{
++	struct i915_vma *vma = container_of(kref, typeof(*vma), kref);
++
++	WARN_ON(drm_mm_node_allocated(&vma->node));
++	WARN_ON(vma->bound);
++	WARN_ON(!list_empty(&vma->mm_list));
++	WARN_ON(!list_empty(&vma->exec_link));
++	WARN_ON(!list_empty(&vma->obj_link));
++
++	list_del(&vma->vm_link);
++	i915_vm_put(vma->vm);
++	kfree(vma);
++}
++
+ struct i915_vma *
+-i915_gem_obj_lookup_or_create_vma(struct drm_i915_gem_object *obj,
+-				  struct i915_address_space *vm)
++i915_gem_obj_get_vma(struct drm_i915_gem_object *obj,
++		     struct i915_address_space *vm)
+ {
+ 	struct i915_vma *vma;
+ 
+ 	vma = i915_gem_obj_to_vma(obj, vm);
+-	if (!vma)
+-		vma = __i915_gem_vma_create(obj, vm);
++	if (vma == NULL)
++		vma = __i915_vma_create(obj, vm);
+ 
+-	return vma;
++	return i915_vma_get(vma);
+ }
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_gem_gtt.h b/drivers/gpu/drm/i915/i915_gem_gtt.h
+--- a/drivers/gpu/drm/i915/i915_gem_gtt.h	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/i915_gem_gtt.h	2014-11-20 09:53:37.968762837 -0700
+@@ -34,6 +34,8 @@
+ #ifndef __I915_GEM_GTT_H__
+ #define __I915_GEM_GTT_H__
+ 
++struct drm_i915_file_private;
++
+ typedef uint32_t gen6_gtt_pte_t;
+ typedef uint64_t gen8_gtt_pte_t;
+ typedef gen8_gtt_pte_t gen8_ppgtt_pde_t;
+@@ -117,17 +119,20 @@
+  * will always be <= an objects lifetime. So object refcounting should cover us.
+  */
+ struct i915_vma {
++	struct kref kref;
+ 	struct drm_mm_node node;
+ 	struct drm_i915_gem_object *obj;
+ 	struct i915_address_space *vm;
+ 
++	struct list_head vm_link;
++
+ 	/** This object's place on the active/inactive lists */
+ 	struct list_head mm_list;
+ 
+-	struct list_head vma_link; /* Link in the object's VMA list */
++	struct list_head obj_link; /* Link in the object's VMA list */
+ 
+ 	/** This vma's place in the batchbuffer or on the eviction list */
+-	struct list_head exec_list;
++	struct list_head exec_link;
+ 
+ 	/**
+ 	 * Used for performing relocations during execbuffer insertion.
+@@ -136,43 +141,55 @@
+ 	unsigned long exec_handle;
+ 	struct drm_i915_gem_exec_object2 *exec_entry;
+ 
+-	/**
+-	 * How many users have pinned this object in GTT space. The following
+-	 * users can each hold at most one reference: pwrite/pread, pin_ioctl
+-	 * (via user_pin_count), execbuffer (objects are not allowed multiple
+-	 * times for the same batchbuffer), and the framebuffer code. When
+-	 * switching/pageflipping, the framebuffer code has at most two buffers
+-	 * pinned per crtc.
+-	 *
+-	 * In the worst case this is 1 + 1 + 1 + 2*2 = 7. That would fit into 3
+-	 * bits with absolutely no headroom. So use 4 bits. */
+-	unsigned int pin_count:4;
+-#define DRM_I915_GEM_OBJECT_MAX_PIN_COUNT 0xf
++	struct {
++		struct i915_gem_request *request;
++		struct list_head engine_link;
++	} last_read[I915_NUM_ENGINES];
++
++	unsigned int pin_count;
++
++	/** Flags and address space this VMA is bound to */
++	unsigned int bound:4;
++#define LOCAL_BIND	(1<<0)
++#define GLOBAL_BIND	(1<<1)
++#define PTE_READ_ONLY	(1<<2)
++#define REBIND		(1<<3)
++	unsigned int active:I915_NUM_ENGINE_BITS;
++	unsigned int exec_read:1;
++	unsigned int exec_write:1;
++	unsigned int exec_fence:2;
+ 
+ 	/** Unmap an object from an address space. This usually consists of
+ 	 * setting the valid PTE entries to a reserved scratch page. */
+-	void (*unbind_vma)(struct i915_vma *vma);
++	int (*unbind_vma)(struct i915_vma *vma);
+ 	/* Map an object into an address space with the given cache flags. */
+-#define GLOBAL_BIND (1<<0)
+-#define PTE_READ_ONLY (1<<1)
+-	void (*bind_vma)(struct i915_vma *vma,
+-			 enum i915_cache_level cache_level,
+-			 u32 flags);
++	int (*bind_vma)(struct i915_vma *vma,
++			enum i915_cache_level cache_level,
++			unsigned flags);
+ };
+ 
+ struct i915_address_space {
++	struct kref ref;
+ 	struct drm_mm mm;
+ 	struct drm_device *dev;
+ 	struct list_head global_link;
+ 	unsigned long start;		/* Start offset always 0 for dri2 */
+ 	size_t total;		/* size addr space maps (ex. 2GB for ggtt) */
+ 
++	bool dirty;
++	bool closed;
++
+ 	struct {
+ 		dma_addr_t addr;
+ 		struct page *page;
+ 	} scratch;
+ 
+ 	/**
++	 * List of all allocated vma.
++	 */
++	struct list_head vma_list;
++
++	/**
+ 	 * List of objects currently involved in rendering.
+ 	 *
+ 	 * Includes buffers having the contents of their GPU caches
+@@ -199,14 +216,15 @@
+ 	gen6_gtt_pte_t (*pte_encode)(dma_addr_t addr,
+ 				     enum i915_cache_level level,
+ 				     bool valid, u32 flags); /* Create a valid PTE */
+-	void (*clear_range)(struct i915_address_space *vm,
+-			    uint64_t start,
+-			    uint64_t length,
+-			    bool use_scratch);
+-	void (*insert_entries)(struct i915_address_space *vm,
+-			       struct sg_table *st,
+-			       uint64_t start,
+-			       enum i915_cache_level cache_level, u32 flags);
++	int (*clear_range)(struct i915_address_space *vm,
++			   uint64_t start,
++			   uint64_t length,
++			   bool use_scratch);
++	int (*insert_entries)(struct i915_address_space *vm,
++			      struct sg_table *st,
++			      uint64_t start,
++			      enum i915_cache_level cache_level,
++			      u32 flags);
+ 	void (*cleanup)(struct i915_address_space *vm);
+ };
+ 
+@@ -240,40 +258,70 @@
+ 
+ struct i915_hw_ppgtt {
+ 	struct i915_address_space base;
+-	struct kref ref;
+-	struct drm_mm_node node;
++	struct drm_i915_gem_object *state;
++	unsigned alignment;
+ 	unsigned num_pd_entries;
+ 	unsigned num_pd_pages; /* gen8+ */
+ 	union {
+-		struct page **pt_pages;
+ 		struct page **gen8_pt_pages[GEN8_LEGACY_PDPS];
+ 	};
+ 	struct page *pd_pages;
+ 	union {
+-		uint32_t pd_offset;
+ 		dma_addr_t pd_dma_addr[GEN8_LEGACY_PDPS];
+ 	};
+ 	union {
+-		dma_addr_t *pt_dma_addr;
+ 		dma_addr_t *gen8_pt_dma_addr[4];
+ 	};
+ 
+-	struct intel_context *ctx;
++	struct drm_i915_file_private *file_priv;
+ 
+ 	int (*enable)(struct i915_hw_ppgtt *ppgtt);
+-	int (*switch_mm)(struct i915_hw_ppgtt *ppgtt,
+-			 struct intel_engine_cs *ring,
+-			 bool synchronous);
++	int (*switch_mm)(struct i915_gem_request *rq,
++			 struct i915_hw_ppgtt *ppgtt);
+ 	void (*debug_dump)(struct i915_hw_ppgtt *ppgtt, struct seq_file *m);
+ };
+ 
++void __i915_vma_free(struct kref *kref);
++
++static inline struct i915_vma *
++i915_vma_get(struct i915_vma *vma)
++{
++	if (vma)
++		kref_get(&vma->kref);
++	return vma;
++}
++
++static inline void
++i915_vma_put(struct i915_vma *vma)
++{
++	if (vma)
++		kref_put(&vma->kref, __i915_vma_free);
++}
++
+ int i915_gem_gtt_init(struct drm_device *dev);
+ void i915_gem_init_global_gtt(struct drm_device *dev);
+-void i915_gem_setup_global_gtt(struct drm_device *dev, unsigned long start,
+-			       unsigned long mappable_end, unsigned long end);
+-
+-bool intel_enable_ppgtt(struct drm_device *dev, bool full);
+-int i915_gem_init_ppgtt(struct drm_device *dev, struct i915_hw_ppgtt *ppgtt);
++int i915_gem_setup_global_gtt(struct drm_device *dev, unsigned long start,
++			      unsigned long mappable_end, unsigned long end);
++void i915_global_gtt_cleanup(struct drm_device *dev);
++
++
++int i915_ppgtt_init(struct drm_device *dev, struct i915_hw_ppgtt *ppgtt);
++int i915_ppgtt_init_hw(struct drm_device *dev);
++struct i915_hw_ppgtt *i915_ppgtt_create(struct drm_device *dev,
++					struct drm_i915_file_private *fpriv);
++void __i915_vm_free(struct kref *kref);
++static inline struct i915_address_space *
++i915_vm_get(struct i915_address_space *vm)
++{
++	if (vm)
++		kref_get(&vm->ref);
++	return vm;
++}
++static inline void i915_vm_put(struct i915_address_space *vm)
++{
++	if (vm)
++		kref_put(&vm->ref, __i915_vm_free);
++}
+ 
+ void i915_check_and_clear_faults(struct drm_device *dev);
+ void i915_gem_suspend_gtt_mappings(struct drm_device *dev);
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_gem_render_state.c b/drivers/gpu/drm/i915/i915_gem_render_state.c
+--- a/drivers/gpu/drm/i915/i915_gem_render_state.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/i915_gem_render_state.c	2014-11-20 09:53:37.968762837 -0700
+@@ -33,10 +33,11 @@
+ 	struct drm_i915_gem_object *obj;
+ 	u64 ggtt_offset;
+ 	int gen;
++	unsigned batch_length;
+ };
+ 
+ static const struct intel_renderstate_rodata *
+-render_state_get_rodata(struct drm_device *dev, const int gen)
++render_state_get_rodata(const int gen)
+ {
+ 	switch (gen) {
+ 	case 6:
+@@ -45,28 +46,30 @@
+ 		return &gen7_null_state;
+ 	case 8:
+ 		return &gen8_null_state;
++	case 9:
++		return &gen9_null_state;
+ 	}
+ 
+ 	return NULL;
+ }
+ 
+-static int render_state_init(struct render_state *so, struct drm_device *dev)
++static int render_state_init(struct render_state *so, struct i915_gem_request *rq)
+ {
+ 	int ret;
+ 
+-	so->gen = INTEL_INFO(dev)->gen;
+-	so->rodata = render_state_get_rodata(dev, so->gen);
++	so->gen = INTEL_INFO(rq->i915)->gen;
++	so->rodata = render_state_get_rodata(so->gen);
+ 	if (so->rodata == NULL)
+ 		return 0;
+ 
+ 	if (so->rodata->batch_items * 4 > 4096)
+ 		return -EINVAL;
+ 
+-	so->obj = i915_gem_alloc_object(dev, 4096);
++	so->obj = i915_gem_alloc_object(rq->i915->dev, 4096);
+ 	if (so->obj == NULL)
+ 		return -ENOMEM;
+ 
+-	ret = i915_gem_obj_ggtt_pin(so->obj, 4096, 0);
++	ret = i915_gem_object_ggtt_pin(so->obj, 4096, 0);
+ 	if (ret)
+ 		goto free_gem;
+ 
+@@ -93,9 +96,13 @@
+ 	page = sg_page(so->obj->pages->sgl);
+ 	d = kmap(page);
+ 
++	so->batch_length = 0;
+ 	while (i < rodata->batch_items) {
+ 		u32 s = rodata->batch[i];
+ 
++		if (so->batch_length == 0 && s == MI_BATCH_BUFFER_END)
++			so->batch_length = sizeof(u32) * ALIGN(i, 2);
++
+ 		if (i * 4  == rodata->reloc[reloc_index]) {
+ 			u64 r = s + so->ggtt_offset;
+ 			s = lower_32_bits(r);
+@@ -115,10 +122,6 @@
+ 	}
+ 	kunmap(page);
+ 
+-	ret = i915_gem_object_set_to_gtt_domain(so->obj, false);
+-	if (ret)
+-		return ret;
+-
+ 	if (rodata->reloc[reloc_index] != -1) {
+ 		DRM_ERROR("only %d relocs resolved\n", reloc_index);
+ 		return -EINVAL;
+@@ -133,15 +136,15 @@
+ 	drm_gem_object_unreference(&so->obj->base);
+ }
+ 
+-int i915_gem_render_state_init(struct intel_engine_cs *ring)
++int i915_gem_render_state_init(struct i915_gem_request *rq)
+ {
+ 	struct render_state so;
+ 	int ret;
+ 
+-	if (WARN_ON(ring->id != RCS))
++	if (WARN_ON(rq->engine->id != RCS))
+ 		return -ENOENT;
+ 
+-	ret = render_state_init(&so, ring->dev);
++	ret = render_state_init(&so, rq);
+ 	if (ret)
+ 		return ret;
+ 
+@@ -152,16 +155,20 @@
+ 	if (ret)
+ 		goto out;
+ 
+-	ret = ring->dispatch_execbuffer(ring,
+-					so.ggtt_offset,
+-					so.rodata->batch_items * 4,
+-					I915_DISPATCH_SECURE);
++	if (i915_gem_clflush_object(so.obj, false))
++		i915_gem_chipset_flush(rq->i915->dev);
++
++	ret = i915_request_emit_batchbuffer(rq, NULL,
++					    so.ggtt_offset,
++					    so.batch_length,
++					    I915_DISPATCH_SECURE);
+ 	if (ret)
+ 		goto out;
+ 
+-	i915_vma_move_to_active(i915_gem_obj_to_ggtt(so.obj), ring);
++	so.obj->base.pending_read_domains = I915_GEM_DOMAIN_COMMAND;
++	drm_gem_object_reference(&so.obj->base);
++	i915_request_add_vma(rq, i915_gem_obj_get_ggtt(so.obj), 0);
+ 
+-	ret = __i915_add_request(ring, NULL, so.obj, NULL);
+ 	/* __i915_add_request moves object to inactive if it fails */
+ out:
+ 	render_state_fini(&so);
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_gem_request.c b/drivers/gpu/drm/i915/i915_gem_request.c
+--- a/drivers/gpu/drm/i915/i915_gem_request.c	1969-12-31 17:00:00.000000000 -0700
++++ b/drivers/gpu/drm/i915/i915_gem_request.c	2014-11-20 09:53:37.968762837 -0700
+@@ -0,0 +1,733 @@
++/*
++ * Copyright  2014 Intel Corporation
++ *
++ * Permission is hereby granted, free of charge, to any person obtaining a
++ * copy of this software and associated documentation files (the "Software"),
++ * to deal in the Software without restriction, including without limitation
++ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
++ * and/or sell copies of the Software, and to permit persons to whom the
++ * Software is furnished to do so, subject to the following conditions:
++ *
++ * The above copyright notice and this permission notice (including the next
++ * paragraph) shall be included in all copies or substantial portions of the
++ * Software.
++ *
++ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
++ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
++ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
++ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
++ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
++ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
++ * IN THE SOFTWARE.
++ *
++ */
++
++#include <drm/drmP.h>
++#include "i915_drv.h"
++#include <drm/i915_drm.h>
++#include "i915_trace.h"
++#include "intel_drv.h"
++
++static bool check_reset(struct i915_gem_request *rq)
++{
++	unsigned reset = atomic_read(&rq->i915->gpu_error.reset_counter);
++	return likely(reset == rq->reset_counter);
++}
++
++static u32
++next_seqno(struct drm_i915_private *i915)
++{
++	/* reserve 0 for non-seqno */
++	if (++i915->next_seqno == 0)
++		++i915->next_seqno;
++	return i915->next_seqno;
++}
++
++struct i915_gem_request *
++i915_request_create(struct intel_context *ctx,
++		    struct intel_engine_cs *engine)
++{
++	struct intel_ringbuffer *ring;
++	struct i915_gem_request *rq;
++	int ret, n;
++
++	lockdep_assert_held(&engine->i915->dev->struct_mutex);
++
++	/* Pin first in case we need to recurse */
++	ring = engine->pin_context(engine, ctx);
++	if (IS_ERR(ring))
++		return ERR_CAST(ring);
++
++	rq = kzalloc(sizeof(*rq), GFP_KERNEL);
++	if (rq == NULL) {
++		ret = -ENOMEM;
++		goto err;
++	}
++
++	kref_init(&rq->kref);
++	INIT_LIST_HEAD(&rq->vmas);
++	INIT_LIST_HEAD(&rq->breadcrumb_link);
++
++	rq->i915 = engine->i915;
++	rq->ring = ring;
++	rq->engine = engine;
++
++	rq->reset_counter = atomic_read(&rq->i915->gpu_error.reset_counter);
++	if (rq->reset_counter & (I915_RESET_IN_PROGRESS_FLAG | I915_WEDGED)) {
++		ret = rq->reset_counter & I915_WEDGED ? -EIO : -EAGAIN;
++		goto err_rq;
++	}
++
++	rq->seqno = next_seqno(rq->i915);
++	memcpy(rq->semaphore, engine->semaphore.sync, sizeof(rq->semaphore));
++	for (n = 0; n < ARRAY_SIZE(rq->semaphore); n++)
++		if (__i915_seqno_passed(rq->semaphore[n], rq->seqno))
++			rq->semaphore[n] = 0;
++	rq->head = ring->tail;
++	rq->outstanding = true;
++	rq->pending_flush = ring->pending_flush;
++
++	rq->ctx = ctx;
++	i915_gem_context_reference(rq->ctx);
++
++	ret = i915_request_switch_context(rq);
++	if (ret)
++		goto err_ctx;
++
++	return rq;
++
++err_ctx:
++	i915_gem_context_unreference(ctx);
++err_rq:
++	kfree(rq);
++err:
++	engine->unpin_context(engine, ctx);
++	return ERR_PTR(ret);
++}
++
++void
++i915_request_add_vma(struct i915_gem_request *rq,
++		     struct i915_vma *vma,
++		     unsigned fenced)
++{
++	struct drm_i915_gem_object *obj = vma->obj;
++	u32 old_read = obj->base.read_domains;
++	u32 old_write = obj->base.write_domain;
++
++	lockdep_assert_held(&rq->i915->dev->struct_mutex);
++	BUG_ON(!drm_mm_node_allocated(&vma->node));
++
++	obj->base.write_domain = obj->base.pending_write_domain;
++	if (obj->base.write_domain == 0)
++		obj->base.pending_read_domains |= obj->base.read_domains;
++	obj->base.read_domains = obj->base.pending_read_domains;
++
++	obj->base.pending_read_domains = 0;
++	obj->base.pending_write_domain = 0;
++
++	trace_i915_gem_object_change_domain(obj, old_read, old_write);
++	list_move_tail(&vma->exec_link, &rq->vmas);
++
++	if (obj->base.read_domains) {
++		vma->exec_read = 1;
++		vma->exec_fence = fenced;
++		vma->exec_write = !!(obj->base.write_domain & I915_GEM_GPU_DOMAINS);
++
++		if (vma->exec_write) {
++			rq->pending_flush |= I915_FLUSH_CACHES;
++			intel_fb_obj_invalidate(obj, rq);
++		}
++	} else
++		vma->exec_read = 0;
++
++	/* update for the implicit flush after the rq */
++	obj->base.write_domain &= ~I915_GEM_GPU_DOMAINS;
++}
++
++int
++i915_request_emit_flush(struct i915_gem_request *rq,
++			unsigned flags)
++{
++	struct intel_engine_cs *engine = rq->engine;
++	int ret;
++
++	lockdep_assert_held(&rq->i915->dev->struct_mutex);
++
++	if ((flags & rq->pending_flush) == 0)
++		return 0;
++
++	trace_i915_gem_request_emit_flush(rq);
++	ret = engine->emit_flush(rq, rq->pending_flush);
++	if (ret)
++		return ret;
++
++	rq->pending_flush = 0;
++	return 0;
++}
++
++int
++__i915_request_emit_breadcrumb(struct i915_gem_request *rq, int id)
++{
++	struct intel_engine_cs *engine = rq->engine;
++	u32 seqno;
++	int ret;
++
++	lockdep_assert_held(&rq->i915->dev->struct_mutex);
++
++	if (rq->breadcrumb[id])
++		return 0;
++
++	if (rq->outstanding) {
++		ret = i915_request_emit_flush(rq, I915_FLUSH_CACHES | I915_COMMAND_BARRIER);
++		if (ret)
++			return ret;
++
++		trace_i915_gem_request_emit_breadcrumb(rq);
++		if (id == engine->id)
++			ret = engine->emit_breadcrumb(rq);
++		else
++			ret = engine->semaphore.signal(rq, id);
++		if (ret)
++			return ret;
++
++		seqno = rq->seqno;
++	} else if (engine->breadcrumb[id] == 0 ||
++		   __i915_seqno_passed(rq->seqno, engine->breadcrumb[id])) {
++		struct i915_gem_request *tmp;
++
++		tmp = i915_request_create(engine->last_context, engine);
++		if (IS_ERR(tmp))
++			return PTR_ERR(tmp);
++
++		/* Masquerade as a continuation of the earlier request */
++		tmp->reset_counter = rq->reset_counter;
++
++		ret = __i915_request_emit_breadcrumb(tmp, id);
++		if (ret == 0 && id != engine->id) {
++			/* semaphores are unstable across a wrap */
++			if (tmp->seqno < engine->breadcrumb[id])
++				ret = i915_request_wait(tmp);
++		}
++		if (ret == 0)
++			ret = i915_request_commit(tmp);
++
++		i915_request_put(tmp);
++		if (ret)
++			return ret;
++
++		seqno = tmp->seqno;
++	} else
++		seqno = engine->breadcrumb[id];
++
++	rq->breadcrumb[id] = seqno;
++	return 0;
++}
++
++int
++i915_request_emit_batchbuffer(struct i915_gem_request *rq,
++			      struct i915_vma *batch,
++			      uint64_t start, uint32_t len,
++			      unsigned flags)
++{
++	struct intel_engine_cs *engine = rq->engine;
++	int ret;
++
++	lockdep_assert_held(&rq->i915->dev->struct_mutex);
++
++	trace_i915_gem_request_emit_batch(rq);
++	rq->batch = batch;
++
++	ret = engine->emit_batchbuffer(rq, start, len, flags);
++	if (ret)
++		return ret;
++
++	/* We track the associated batch vma for debugging and error capture.
++	 * Whilst this request exists, the batch obj will be on the active_list,
++	 * and so will hold the active reference. Only when this request is
++	 * retired will the the batch be moved onto the inactive_list and lose
++	 * its active reference. Hence we do not need to explicitly hold
++	 * another reference here.
++	 */
++	rq->pending_flush |= I915_COMMAND_BARRIER;
++	return 0;
++}
++
++/* Track the batches submitted by clients for throttling */
++static void
++add_to_client(struct i915_gem_request *rq)
++{
++	struct drm_i915_file_private *file_priv = rq->ctx->file_priv;
++
++	if (file_priv) {
++		spin_lock(&file_priv->mm.lock);
++		list_add_tail(&rq->client_list,
++			      &file_priv->mm.request_list);
++		rq->file_priv = file_priv;
++		rq->emitted_jiffies = jiffies;
++		spin_unlock(&file_priv->mm.lock);
++	}
++}
++
++static void
++remove_from_client(struct i915_gem_request *rq)
++{
++	struct drm_i915_file_private *file_priv = rq->file_priv;
++
++	if (!file_priv)
++		return;
++
++	spin_lock(&file_priv->mm.lock);
++	if (rq->file_priv) {
++		list_del(&rq->client_list);
++		rq->file_priv = NULL;
++	}
++	spin_unlock(&file_priv->mm.lock);
++}
++
++/* Activity tracking on the object so that we can serialise CPU access to
++ * the object's memory with the GPU.
++ */
++static void
++add_to_obj(struct i915_gem_request *rq, struct i915_vma *vma)
++{
++	struct drm_i915_gem_object *obj = vma->obj;
++	struct intel_engine_cs *engine = rq->engine;
++
++	if (!vma->exec_read)
++		return;
++
++	if (vma->last_read[engine->id].request == NULL && vma->active++ == 0) {
++		drm_gem_object_reference(&obj->base);
++		i915_vma_get(vma);
++	}
++
++	i915_request_put(vma->last_read[engine->id].request);
++	vma->last_read[engine->id].request = i915_request_get(rq);
++
++	list_move_tail(&vma->last_read[engine->id].engine_link,
++		       &engine->vma_list);
++
++	/* Add a reference if we're newly entering the active list. */
++	if (obj->last_read[engine->id].request == NULL && obj->active++ == 0)
++		drm_gem_object_reference(&obj->base);
++
++	if (vma->exec_write) {
++		obj->dirty = 1;
++		i915_request_put(obj->last_write.request);
++		obj->last_write.request = i915_request_get(rq);
++		list_move_tail(&obj->last_write.engine_link,
++			       &engine->write_list);
++
++		if (obj->active > 1) {
++			int i;
++
++			for (i = 0; i < I915_NUM_ENGINES; i++) {
++				if (obj->last_read[i].request == NULL)
++					continue;
++
++				list_del_init(&obj->last_read[i].engine_link);
++				i915_request_put(obj->last_read[i].request);
++				obj->last_read[i].request = NULL;
++			}
++
++			obj->active = 1;
++		}
++	}
++
++	if (vma->exec_fence & VMA_IS_FENCED) {
++		i915_request_put(obj->last_fence.request);
++		obj->last_fence.request = i915_request_get(rq);
++		list_move_tail(&obj->last_fence.engine_link,
++			       &engine->fence_list);
++		if (vma->exec_fence & VMA_HAS_FENCE)
++			list_move_tail(&rq->i915->fence_regs[obj->fence_reg].lru_list,
++					&rq->i915->mm.fence_list);
++	}
++
++	i915_request_put(obj->last_read[engine->id].request);
++	obj->last_read[engine->id].request = i915_request_get(rq);
++	list_move_tail(&obj->last_read[engine->id].engine_link,
++		       &engine->read_list);
++
++	BUG_ON(!drm_mm_node_allocated(&vma->node));
++	list_move_tail(&vma->mm_list, &vma->vm->active_list);
++}
++
++static bool leave_breadcrumb(struct i915_gem_request *rq)
++{
++	if (rq->breadcrumb[rq->engine->id])
++		return false;
++
++	/* Auto-report HEAD every 4k to make sure that we can always wait on
++	 * some available ring space in the future. This also caps the
++	 * latency of future waits for missed breadcrumbs.
++	 */
++	if (__intel_ring_space(rq->ring->tail, rq->ring->breadcrumb_tail,
++			       rq->ring->size, 0) >= PAGE_SIZE)
++		return true;
++
++	return false;
++}
++
++static bool simulated_hang(struct intel_engine_cs *engine)
++{
++	return test_and_clear_bit(engine->id,
++				  &engine->i915->gpu_error.stop_rings);
++}
++
++int i915_request_commit(struct i915_gem_request *rq)
++{
++	int ret, n;
++
++	lockdep_assert_held(&rq->i915->dev->struct_mutex);
++
++	if (!rq->outstanding)
++		return 0;
++
++	if (rq->head == rq->ring->tail) {
++		rq->completed = true;
++		return 0;
++	}
++
++	if (simulated_hang(rq->engine))
++		i915_handle_error(rq->i915->dev,
++				  I915_HANG_RESET | I915_HANG_SIMULATED,
++				  "Simulated hang");
++
++	if (!check_reset(rq))
++		return rq->i915->mm.interruptible ? -EAGAIN : -EIO;
++
++	if (leave_breadcrumb(rq)) {
++		ret = i915_request_emit_breadcrumb(rq);
++		if (ret)
++			return ret;
++	}
++
++	/* TAIL must be aligned to a qword */
++	if ((rq->ring->tail / sizeof (uint32_t)) & 1) {
++		intel_ring_emit(rq->ring, MI_NOOP);
++		intel_ring_advance(rq->ring);
++	}
++	rq->tail = rq->ring->tail;
++
++	trace_i915_gem_request_commit(rq);
++	ret = rq->engine->add_request(rq);
++	if (ret)
++		return ret;
++
++	i915_request_get(rq);
++
++	rq->outstanding = false;
++	if (rq->breadcrumb[rq->engine->id]) {
++		list_add_tail(&rq->breadcrumb_link, &rq->ring->breadcrumbs);
++		rq->ring->breadcrumb_tail = rq->tail;
++	}
++
++	memcpy(rq->engine->semaphore.sync,
++	       rq->semaphore,
++	       sizeof(rq->semaphore));
++	for (n = 0; n < ARRAY_SIZE(rq->breadcrumb); n++)
++		if (rq->breadcrumb[n])
++			rq->engine->breadcrumb[n] = rq->breadcrumb[n];
++
++	rq->ring->pending_flush = rq->pending_flush;
++
++	if (rq->batch) {
++		add_to_client(rq);
++		rq->batch->vm->dirty = false;
++
++		i915_queue_hangcheck(rq->i915->dev);
++	}
++
++	rq->engine->last_request = rq;
++	intel_mark_busy(rq->i915->dev);
++
++	cancel_delayed_work_sync(&rq->i915->mm.idle_work);
++	mod_delayed_work(rq->i915->wq,
++			 &rq->i915->mm.retire_work,
++			 round_jiffies_up_relative(HZ));
++
++	while (!list_empty(&rq->vmas)) {
++		struct i915_vma *vma =
++			list_first_entry(&rq->vmas,
++					 typeof(*vma),
++					 exec_link);
++
++		add_to_obj(rq, vma);
++		i915_vma_unreserve(vma);
++	}
++
++	i915_request_switch_context__commit(rq);
++
++	if (rq->engine->last_context) {
++		rq->engine->unpin_context(rq->engine, rq->engine->last_context);
++		i915_gem_context_unreference(rq->engine->last_context);
++	}
++
++	rq->engine->last_request = rq;
++	rq->engine->last_context = rq->ctx;
++	return 0;
++}
++
++static void fake_irq(unsigned long data)
++{
++	wake_up_process((struct task_struct *)data);
++}
++
++static bool missed_irq(struct i915_gem_request *rq)
++{
++	return test_bit(rq->engine->id, &rq->i915->gpu_error.missed_irq_rings);
++}
++
++bool __i915_request_complete__wa(struct i915_gem_request *rq)
++{
++	struct drm_i915_private *dev_priv = rq->i915;
++	unsigned head, tail;
++
++	if (i915_request_complete(rq))
++		return true;
++
++	/* With execlists, we rely on interrupts to track request completion */
++	if (rq->engine->execlists_enabled)
++		return false;
++
++	/* As we may not emit a breadcrumb with every request, we
++	 * often have unflushed requests. In the event of an emergency,
++	 * just assume that if the RING_HEAD has reached the tail, then
++	 * the request is complete. However, note that the RING_HEAD
++	 * advances before the instruction completes, so this is quite lax,
++	 * and should only be used carefully. To compensate, we only treat
++	 * it as completed if the request flushed.
++	 *
++	 * As we treat this as only an advisory completion, we forgo
++	 * marking the request as actually complete.
++	 */
++	head = __intel_ring_space(I915_READ_HEAD(rq->engine) & HEAD_ADDR,
++				  rq->ring->tail, rq->ring->size,
++				  rq->pending_flush & I915_COMMAND_BARRIER ? 8 : 0);
++	tail = __intel_ring_space(rq->tail,
++				  rq->ring->tail, rq->ring->size, 0);
++	return head >= tail;
++}
++
++/**
++ * __i915_request_wait - wait until execution of request has finished
++ * @request: the request to wait upon
++ * @interruptible: do an interruptible wait (normally yes)
++ * @timeout_ns: in - how long to wait (NULL forever); out - how much time remaining
++ *
++ * Returns 0 if the request was completed within the alloted time. Else returns the
++ * errno with remaining time filled in timeout argument.
++ */
++int __i915_request_wait(struct i915_gem_request *rq,
++			bool interruptible,
++			s64 *timeout_ns,
++			struct drm_i915_file_private *file_priv)
++{
++	const bool irq_test_in_progress =
++		ACCESS_ONCE(rq->i915->gpu_error.test_irq_rings) & intel_engine_flag(rq->engine);
++	DEFINE_WAIT(wait);
++	unsigned long timeout_expire;
++	unsigned long before, now;
++	int ret = 0;
++
++	WARN(!intel_irqs_enabled(rq->i915), "IRQs disabled");
++
++	if (i915_request_complete(rq))
++		return 0;
++
++	timeout_expire = timeout_ns ? jiffies + nsecs_to_jiffies((u64)*timeout_ns) : 0;
++
++	if (rq->engine->id == RCS && INTEL_INFO(rq->i915)->gen >= 6)
++		gen6_rps_boost(rq->i915, file_priv);
++
++	if (!irq_test_in_progress) {
++		if (WARN_ON(!intel_irqs_enabled(rq->i915)))
++			return -ENODEV;
++
++		rq->engine->irq_get(rq->engine);
++	}
++
++	/* Record current time in case interrupted by signal, or wedged */
++	trace_i915_gem_request_wait_begin(rq);
++	before = jiffies;
++	for (;;) {
++		struct timer_list timer;
++
++		prepare_to_wait(&rq->engine->irq_queue, &wait,
++				interruptible ? TASK_INTERRUPTIBLE : TASK_UNINTERRUPTIBLE);
++
++		if (!check_reset(rq))
++			break;
++
++		rq->engine->irq_barrier(rq->engine);
++
++		if (i915_request_complete(rq))
++			break;
++
++		if (timeout_ns && time_after_eq(jiffies, timeout_expire)) {
++			ret = -ETIME;
++			break;
++		}
++
++		if (interruptible && signal_pending(current)) {
++			ret = -ERESTARTSYS;
++			break;
++		}
++
++		/* Paranoid kick of hangcheck so that we never wait forever */
++		i915_queue_hangcheck(rq->i915->dev);
++
++		timer.function = NULL;
++		if (timeout_ns || missed_irq(rq)) {
++			unsigned long expire;
++
++			setup_timer_on_stack(&timer, fake_irq, (unsigned long)current);
++			expire = missed_irq(rq) ? jiffies + 1 : timeout_expire;
++			mod_timer(&timer, expire);
++		}
++
++		io_schedule();
++
++		if (timer.function) {
++			del_singleshot_timer_sync(&timer);
++			destroy_timer_on_stack(&timer);
++		}
++	}
++	now = jiffies;
++	trace_i915_gem_request_wait_end(rq);
++
++	if (!irq_test_in_progress)
++		rq->engine->irq_put(rq->engine);
++
++	finish_wait(&rq->engine->irq_queue, &wait);
++
++	if (timeout_ns) {
++		s64 tres = *timeout_ns - jiffies_to_nsecs(now - before);
++		*timeout_ns = tres <= 0 ? 0 : tres;
++	}
++
++	return ret;
++}
++
++struct i915_gem_request *
++i915_request_get_breadcrumb(struct i915_gem_request *rq)
++{
++	struct list_head *list;
++	u32 seqno;
++	int ret;
++
++	if (i915_request_complete(rq))
++		return i915_request_get(rq);
++
++	/* Writes are only coherent from the cpu (in the general case) when
++	 * the interrupt following the write to memory is complete. That is
++	 * when the breadcrumb after the write request is complete.
++	 *
++	 * Reads are only complete when then command streamer barrier is
++	 * passed.
++	 *
++	 * In both cases, the CPU needs to wait upon the subsequent breadcrumb,
++	 * which ensures that all pending flushes have been emitted and are
++	 * complete, before reporting that the request is finished and
++	 * the CPU's view of memory is coherent with the GPU.
++	 */
++
++	ret = i915_request_emit_breadcrumb(rq);
++	if (ret)
++		return ERR_PTR(ret);
++
++	ret = i915_request_commit(rq);
++	if (ret)
++		return ERR_PTR(ret);
++
++	if (!list_empty(&rq->breadcrumb_link))
++		return i915_request_get(rq);
++
++	seqno = rq->breadcrumb[rq->engine->id];
++	list = &rq->ring->breadcrumbs;
++	list_for_each_entry_reverse(rq, list, breadcrumb_link) {
++		if (rq->seqno == seqno)
++			return i915_request_get(rq);
++	}
++
++	return ERR_PTR(-EIO);
++}
++
++int
++i915_request_wait(struct i915_gem_request *rq)
++{
++	int ret;
++
++	lockdep_assert_held(&rq->i915->dev->struct_mutex);
++
++	rq = i915_request_get_breadcrumb(rq);
++	if (IS_ERR(rq))
++		return PTR_ERR(rq);
++
++	ret = __i915_request_wait(rq, rq->i915->mm.interruptible,
++				  NULL, NULL);
++	i915_request_put(rq);
++
++	return ret;
++}
++
++void
++i915_request_retire(struct i915_gem_request *rq)
++{
++	lockdep_assert_held(&rq->i915->dev->struct_mutex);
++
++	if (!rq->completed) {
++		trace_i915_gem_request_complete(rq);
++		rq->completed = true;
++	}
++	trace_i915_gem_request_retire(rq);
++
++	/* We know the GPU must have read the request to have
++	 * sent us the seqno + interrupt, we can use the position
++	 * of tail of the request to update the last known position
++	 * of the GPU head.
++	 */
++	if (!list_empty(&rq->breadcrumb_link))
++		rq->ring->retired_head = rq->tail;
++
++	rq->batch = NULL;
++
++	/* We need to protect against simultaneous hangcheck/capture */
++	spin_lock(&rq->engine->lock);
++	if (rq->engine->last_request == rq)
++		rq->engine->last_request = NULL;
++	list_del(&rq->engine_link);
++	spin_unlock(&rq->engine->lock);
++
++	list_del(&rq->breadcrumb_link);
++	remove_from_client(rq);
++
++	i915_request_put(rq);
++}
++
++void
++__i915_request_free(struct kref *kref)
++{
++	struct i915_gem_request *rq = container_of(kref, struct i915_gem_request, kref);
++
++	lockdep_assert_held(&rq->i915->dev->struct_mutex);
++
++	if (rq->outstanding) {
++		/* Rollback this partial transaction as we never committed
++		 * the request to the hardware queue.
++		 */
++		rq->ring->tail = rq->head;
++		rq->ring->space = intel_ring_space(rq->ring);
++
++		while (!list_empty(&rq->vmas))
++			i915_vma_unreserve(list_first_entry(&rq->vmas,
++							    struct i915_vma,
++							    exec_link));
++
++		rq->engine->unpin_context(rq->engine, rq->ctx);
++		i915_gem_context_unreference(rq->ctx);
++	}
++
++	kfree(rq);
++}
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_gem_stolen.c b/drivers/gpu/drm/i915/i915_gem_stolen.c
+--- a/drivers/gpu/drm/i915/i915_gem_stolen.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/i915_gem_stolen.c	2014-11-20 09:53:37.968762837 -0700
+@@ -30,6 +30,9 @@
+ #include <drm/i915_drm.h>
+ #include "i915_drv.h"
+ 
++#define KB(x) ((x) * 1024)
++#define MB(x) (KB(x) * 1024)
++
+ /*
+  * The BIOS typically reserves some of the system's memory for the exclusive
+  * use of the integrated graphics. This memory is no longer available for
+@@ -51,24 +54,90 @@
+ 	/* Almost universally we can find the Graphics Base of Stolen Memory
+ 	 * at offset 0x5c in the igfx configuration space. On a few (desktop)
+ 	 * machines this is also mirrored in the bridge device at different
+-	 * locations, or in the MCHBAR. On gen2, the layout is again slightly
+-	 * different with the Graphics Segment immediately following Top of
+-	 * Memory (or Top of Usable DRAM). Note it appears that TOUD is only
+-	 * reported by 865g, so we just use the top of memory as determined
+-	 * by the e820 probe.
++	 * locations, or in the MCHBAR.
++	 *
++	 * On 865 we just check the TOUD register.
++	 *
++	 * On 830/845/85x the stolen memory base isn't available in any
++	 * register. We need to calculate it as TOM-TSEG_SIZE-stolen_size.
+ 	 *
+-	 * XXX However gen2 requires an unavailable symbol.
+ 	 */
+ 	base = 0;
+ 	if (INTEL_INFO(dev)->gen >= 3) {
+ 		/* Read Graphics Base of Stolen Memory directly */
+ 		pci_read_config_dword(dev->pdev, 0x5c, &base);
+ 		base &= ~((1<<20) - 1);
+-	} else { /* GEN2 */
+-#if 0
+-		/* Stolen is immediately above Top of Memory */
+-		base = max_low_pfn_mapped << PAGE_SHIFT;
+-#endif
++	} else if (IS_I865G(dev)) {
++		u16 toud = 0;
++
++		/*
++		 * FIXME is the graphics stolen memory region
++		 * always at TOUD? Ie. is it always the last
++		 * one to be allocated by the BIOS?
++		 */
++		pci_bus_read_config_word(dev->pdev->bus, PCI_DEVFN(0, 0),
++					 I865_TOUD, &toud);
++
++		base = toud << 16;
++	} else if (IS_I85X(dev)) {
++		u32 tseg_size = 0;
++		u32 tom;
++		u8 tmp;
++
++		pci_bus_read_config_byte(dev->pdev->bus, PCI_DEVFN(0, 0), I85X_ESMRAMC, &tmp);
++
++		if (tmp & TSEG_ENABLE)
++			tseg_size = MB(1);
++
++		pci_bus_read_config_byte(dev->pdev->bus, PCI_DEVFN(0, 1),
++					 I85X_DRB3, &tmp);
++		tom = tmp * MB(32);
++
++		base = tom - tseg_size - dev_priv->gtt.stolen_size;
++	} if (IS_845G(dev)) {
++		u32 tseg_size = 0;
++		u32 tom;
++		u8 tmp;
++
++		pci_bus_read_config_byte(dev->pdev->bus, PCI_DEVFN(0, 0),
++					 I845_ESMRAMC, &tmp);
++
++		if (tmp & TSEG_ENABLE) {
++			switch (tmp & I845_TSEG_SIZE_MASK) {
++			case I845_TSEG_SIZE_512K:
++				tseg_size = KB(512);
++				break;
++			case I845_TSEG_SIZE_1M:
++				tseg_size = MB(1);
++				break;
++			}
++		}
++
++		pci_bus_read_config_byte(dev->pdev->bus, PCI_DEVFN(0, 0),
++					 I830_DRB3, &tmp);
++		tom = tmp * MB(32);
++
++		base = tom - tseg_size - dev_priv->gtt.stolen_size;
++	} else if (IS_I830(dev)) {
++		u32 tseg_size = 0;
++		u32 tom;
++		u8 tmp;
++
++		pci_bus_read_config_byte(dev->pdev->bus, PCI_DEVFN(0, 0),
++					 I830_ESMRAMC, &tmp);
++
++		if (tmp & TSEG_ENABLE) {
++			if (tmp & I830_TSEG_SIZE_1M)
++				tseg_size = MB(1);
++			else
++				tseg_size = KB(512);
++		}
++
++		pci_bus_read_config_byte(dev->pdev->bus, PCI_DEVFN(0, 0),
++					 I830_DRB3, &tmp);
++		tom = tmp * MB(32);
++
++		base = tom - tseg_size - dev_priv->gtt.stolen_size;
+ 	}
+ 
+ 	if (base == 0)
+@@ -289,6 +358,7 @@
+ int i915_gem_init_stolen(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
++	u32 tmp;
+ 	int bios_reserved = 0;
+ 
+ #ifdef CONFIG_INTEL_IOMMU
+@@ -308,8 +378,16 @@
+ 	DRM_DEBUG_KMS("found %zd bytes of stolen memory at %08lx\n",
+ 		      dev_priv->gtt.stolen_size, dev_priv->mm.stolen_base);
+ 
+-	if (IS_VALLEYVIEW(dev))
+-		bios_reserved = 1024*1024; /* top 1M on VLV/BYT */
++	if (INTEL_INFO(dev)->gen >= 8) {
++		tmp = I915_READ(GEN7_BIOS_RESERVED);
++		tmp >>= GEN8_BIOS_RESERVED_SHIFT;
++		tmp &= GEN8_BIOS_RESERVED_MASK;
++		bios_reserved = (1024*1024) << tmp;
++	} else if (IS_GEN7(dev)) {
++		tmp = I915_READ(GEN7_BIOS_RESERVED);
++		bios_reserved = tmp & GEN7_BIOS_RESERVED_256K ?
++			256*1024 : 1024*1024;
++	}
+ 
+ 	if (WARN_ON(bios_reserved > dev_priv->gtt.stolen_size))
+ 		return 0;
+@@ -417,18 +495,28 @@
+ 	return NULL;
+ }
+ 
+-struct drm_i915_gem_object *
+-i915_gem_object_create_stolen(struct drm_device *dev, u32 size)
++static bool mark_free(struct drm_i915_gem_object *obj, struct list_head *unwind)
++{
++	if (obj->stolen == NULL)
++		return false;
++
++	if (obj->madv != I915_MADV_DONTNEED)
++		return false;
++
++	if (i915_gem_obj_is_pinned(obj))
++		return false;
++
++	list_add(&obj->obj_exec_link, unwind);
++	return drm_mm_scan_add_block(obj->stolen);
++}
++
++static struct drm_mm_node *stolen_alloc(struct drm_i915_private *dev_priv, u32 size)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct drm_i915_gem_object *obj;
+ 	struct drm_mm_node *stolen;
++	struct drm_i915_gem_object *obj;
++	struct list_head unwind, evict;
+ 	int ret;
+ 
+-	if (!drm_mm_initialized(&dev_priv->mm.stolen))
+-		return NULL;
+-
+-	DRM_DEBUG_KMS("creating stolen object: size=%x\n", size);
+ 	if (size == 0)
+ 		return NULL;
+ 
+@@ -438,11 +526,99 @@
+ 
+ 	ret = drm_mm_insert_node(&dev_priv->mm.stolen, stolen, size,
+ 				 4096, DRM_MM_SEARCH_DEFAULT);
+-	if (ret) {
+-		kfree(stolen);
+-		return NULL;
++	if (ret == 0)
++		return stolen;
++
++	/* No more stolen memory available, or too fragmented.
++	 * Try evicting purgeable objects and search again.
++	 */
++
++	drm_mm_init_scan(&dev_priv->mm.stolen, size, 4096, 0);
++	INIT_LIST_HEAD(&unwind);
++
++	list_for_each_entry(obj, &dev_priv->mm.unbound_list, global_list)
++		if (mark_free(obj, &unwind))
++			goto found;
++
++	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list)
++		if (mark_free(obj, &unwind))
++			goto found;
++
++found:
++	INIT_LIST_HEAD(&evict);
++	while (!list_empty(&unwind)) {
++		obj = list_first_entry(&unwind,
++				       struct drm_i915_gem_object,
++				       obj_exec_link);
++		list_del_init(&obj->obj_exec_link);
++
++		if (drm_mm_scan_remove_block(obj->stolen)) {
++			list_add(&obj->obj_exec_link, &evict);
++			drm_gem_object_reference(&obj->base);
++		}
++	}
++
++	ret = 0;
++	while (!list_empty(&evict)) {
++		obj = list_first_entry(&evict,
++				       struct drm_i915_gem_object,
++				       obj_exec_link);
++		list_del_init(&obj->obj_exec_link);
++
++		if (ret == 0) {
++			struct i915_vma *vma, *vma_next;
++
++			list_for_each_entry_safe(vma, vma_next,
++						 &obj->vma_list,
++						 obj_link)
++				if (i915_vma_unbind(vma))
++					break;
++
++			/* Stolen pins its pages to prevent the
++			 * normal shrinker from processing stolen
++			 * objects.
++			 */
++			i915_gem_object_unpin_pages(obj);
++
++			ret = i915_gem_object_put_pages(obj);
++			if (ret == 0) {
++				i915_gem_object_release_stolen(obj);
++				obj->madv = __I915_MADV_PURGED;
++			} else
++				i915_gem_object_pin_pages(obj);
++		}
++
++		drm_gem_object_unreference(&obj->base);
+ 	}
+ 
++	if (ret == 0)
++		ret = drm_mm_insert_node(&dev_priv->mm.stolen, stolen, size,
++					 4096, DRM_MM_SEARCH_DEFAULT);
++	if (ret == 0)
++		return stolen;
++
++	kfree(stolen);
++	return NULL;
++}
++
++struct drm_i915_gem_object *
++i915_gem_object_create_stolen(struct drm_device *dev, u32 size)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_i915_gem_object *obj;
++	struct drm_mm_node *stolen;
++
++	lockdep_assert_held(&dev->struct_mutex);
++
++	if (!drm_mm_initialized(&dev_priv->mm.stolen))
++		return NULL;
++
++	DRM_DEBUG_KMS("creating stolen object: size=%x\n", size);
++
++	stolen = stolen_alloc(dev_priv, size);
++	if (stolen == NULL)
++		return NULL;
++
+ 	obj = _i915_gem_object_create_stolen(dev, stolen);
+ 	if (obj)
+ 		return obj;
+@@ -503,7 +679,7 @@
+ 	if (gtt_offset == I915_GTT_OFFSET_NONE)
+ 		return obj;
+ 
+-	vma = i915_gem_obj_lookup_or_create_vma(obj, ggtt);
++	vma = i915_gem_obj_get_vma(obj, ggtt);
+ 	if (IS_ERR(vma)) {
+ 		ret = PTR_ERR(vma);
+ 		goto err_out;
+@@ -524,7 +700,7 @@
+ 		}
+ 	}
+ 
+-	obj->has_global_gtt_mapping = 1;
++	vma->bound |= GLOBAL_BIND;
+ 
+ 	list_add_tail(&obj->global_list, &dev_priv->mm.bound_list);
+ 	list_add_tail(&vma->mm_list, &ggtt->inactive_list);
+@@ -533,7 +709,7 @@
+ 	return obj;
+ 
+ err_vma:
+-	i915_gem_vma_destroy(vma);
++	i915_vma_put(vma);
+ err_out:
+ 	drm_mm_remove_node(stolen);
+ 	kfree(stolen);
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_gem_tiling.c b/drivers/gpu/drm/i915/i915_gem_tiling.c
+--- a/drivers/gpu/drm/i915/i915_gem_tiling.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/i915_gem_tiling.c	2014-11-20 09:53:37.968762837 -0700
+@@ -91,26 +91,44 @@
+ 	uint32_t swizzle_x = I915_BIT_6_SWIZZLE_UNKNOWN;
+ 	uint32_t swizzle_y = I915_BIT_6_SWIZZLE_UNKNOWN;
+ 
+-	if (IS_VALLEYVIEW(dev)) {
++	if (INTEL_INFO(dev)->gen >= 8 || IS_VALLEYVIEW(dev)) {
++		/*
++		 * On BDW+, swizzling is not used. We leave the CPU memory
++		 * controller in charge of optimizing memory accesses without
++		 * the extra address manipulation GPU side.
++		 *
++		 * VLV and CHV don't have GPU swizzling.
++		 */
+ 		swizzle_x = I915_BIT_6_SWIZZLE_NONE;
+ 		swizzle_y = I915_BIT_6_SWIZZLE_NONE;
+ 	} else if (INTEL_INFO(dev)->gen >= 6) {
+-		uint32_t dimm_c0, dimm_c1;
+-		dimm_c0 = I915_READ(MAD_DIMM_C0);
+-		dimm_c1 = I915_READ(MAD_DIMM_C1);
+-		dimm_c0 &= MAD_DIMM_A_SIZE_MASK | MAD_DIMM_B_SIZE_MASK;
+-		dimm_c1 &= MAD_DIMM_A_SIZE_MASK | MAD_DIMM_B_SIZE_MASK;
+-		/* Enable swizzling when the channels are populated with
+-		 * identically sized dimms. We don't need to check the 3rd
+-		 * channel because no cpu with gpu attached ships in that
+-		 * configuration. Also, swizzling only makes sense for 2
+-		 * channels anyway. */
+-		if (dimm_c0 == dimm_c1) {
+-			swizzle_x = I915_BIT_6_SWIZZLE_9_10;
+-			swizzle_y = I915_BIT_6_SWIZZLE_9;
++		if (dev_priv->preserve_bios_swizzle) {
++			if (I915_READ(DISP_ARB_CTL) &
++			    DISP_TILE_SURFACE_SWIZZLING) {
++				swizzle_x = I915_BIT_6_SWIZZLE_9_10;
++				swizzle_y = I915_BIT_6_SWIZZLE_9;
++			} else {
++				swizzle_x = I915_BIT_6_SWIZZLE_NONE;
++				swizzle_y = I915_BIT_6_SWIZZLE_NONE;
++			}
+ 		} else {
+-			swizzle_x = I915_BIT_6_SWIZZLE_NONE;
+-			swizzle_y = I915_BIT_6_SWIZZLE_NONE;
++			uint32_t dimm_c0, dimm_c1;
++			dimm_c0 = I915_READ(MAD_DIMM_C0);
++			dimm_c1 = I915_READ(MAD_DIMM_C1);
++			dimm_c0 &= MAD_DIMM_A_SIZE_MASK | MAD_DIMM_B_SIZE_MASK;
++			dimm_c1 &= MAD_DIMM_A_SIZE_MASK | MAD_DIMM_B_SIZE_MASK;
++			/* Enable swizzling when the channels are populated
++			 * with identically sized dimms. We don't need to check
++			 * the 3rd channel because no cpu with gpu attached
++			 * ships in that configuration. Also, swizzling only
++			 * makes sense for 2 channels anyway. */
++			if (dimm_c0 == dimm_c1) {
++				swizzle_x = I915_BIT_6_SWIZZLE_9_10;
++				swizzle_y = I915_BIT_6_SWIZZLE_9;
++			} else {
++				swizzle_x = I915_BIT_6_SWIZZLE_NONE;
++				swizzle_y = I915_BIT_6_SWIZZLE_NONE;
++			}
+ 		}
+ 	} else if (IS_GEN5(dev)) {
+ 		/* On Ironlake whatever DRAM config, GPU always do
+@@ -357,26 +375,12 @@
+ 		 * has to also include the unfenced register the GPU uses
+ 		 * whilst executing a fenced command for an untiled object.
+ 		 */
+-
+-		obj->map_and_fenceable =
+-			!i915_gem_obj_ggtt_bound(obj) ||
+-			(i915_gem_obj_ggtt_offset(obj) +
+-			 obj->base.size <= dev_priv->gtt.mappable_end &&
+-			 i915_gem_object_fence_ok(obj, args->tiling_mode));
+-
+-		/* Rebind if we need a change of alignment */
+-		if (!obj->map_and_fenceable) {
+-			u32 unfenced_align =
+-				i915_gem_get_gtt_alignment(dev, obj->base.size,
+-							    args->tiling_mode,
+-							    false);
+-			if (i915_gem_obj_ggtt_offset(obj) & (unfenced_align - 1))
+-				ret = i915_gem_object_ggtt_unbind(obj);
+-		}
+-
++		if (obj->map_and_fenceable &&
++		    !i915_gem_object_fence_ok(obj, args->tiling_mode))
++			ret = i915_vma_unbind(i915_gem_obj_to_ggtt(obj));
+ 		if (ret == 0) {
+ 			obj->fence_dirty =
+-				obj->fenced_gpu_access ||
++				obj->last_fence.request ||
+ 				obj->fence_reg != I915_FENCE_REG_NONE;
+ 
+ 			obj->tiling_mode = args->tiling_mode;
+@@ -440,6 +444,7 @@
+ 	}
+ 
+ 	/* Hide bit 17 from the user -- see comment in i915_gem_set_tiling */
++	args->phys_swizzle_mode = args->swizzle_mode;
+ 	if (args->swizzle_mode == I915_BIT_6_SWIZZLE_9_17)
+ 		args->swizzle_mode = I915_BIT_6_SWIZZLE_9;
+ 	if (args->swizzle_mode == I915_BIT_6_SWIZZLE_9_10_17)
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_gem_userptr.c b/drivers/gpu/drm/i915/i915_gem_userptr.c
+--- a/drivers/gpu/drm/i915/i915_gem_userptr.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/i915_gem_userptr.c	2014-11-20 09:53:37.968762837 -0700
+@@ -79,7 +79,7 @@
+ 		was_interruptible = dev_priv->mm.interruptible;
+ 		dev_priv->mm.interruptible = false;
+ 
+-		list_for_each_entry_safe(vma, tmp, &obj->vma_list, vma_link) {
++		list_for_each_entry_safe(vma, tmp, &obj->vma_list, obj_link) {
+ 			int ret = i915_vma_unbind(vma);
+ 			WARN_ON(ret && ret != -EIO);
+ 		}
+@@ -293,15 +293,23 @@
+ static struct i915_mmu_notifier *
+ i915_mmu_notifier_find(struct i915_mm_struct *mm)
+ {
+-	if (mm->mn == NULL) {
+-		down_write(&mm->mm->mmap_sem);
+-		mutex_lock(&to_i915(mm->dev)->mm_lock);
+-		if (mm->mn == NULL)
+-			mm->mn = i915_mmu_notifier_create(mm->mm);
+-		mutex_unlock(&to_i915(mm->dev)->mm_lock);
+-		up_write(&mm->mm->mmap_sem);
++	struct i915_mmu_notifier *mn = mm->mn;
++
++	mn = mm->mn;
++	if (mn)
++		return mn;
++
++	down_write(&mm->mm->mmap_sem);
++	mutex_lock(&to_i915(mm->dev)->mm_lock);
++	if ((mn = mm->mn) == NULL) {
++		mn = i915_mmu_notifier_create(mm->mm);
++		if (!IS_ERR(mn))
++			mm->mn = mn;
+ 	}
+-	return mm->mn;
++	mutex_unlock(&to_i915(mm->dev)->mm_lock);
++	up_write(&mm->mm->mmap_sem);
++
++	return mn;
+ }
+ 
+ static int
+@@ -681,16 +689,15 @@
+ static void
+ i915_gem_userptr_put_pages(struct drm_i915_gem_object *obj)
+ {
+-	struct scatterlist *sg;
+-	int i;
++	struct sg_page_iter sg_iter;
+ 
+ 	BUG_ON(obj->userptr.work != NULL);
+ 
+ 	if (obj->madv != I915_MADV_WILLNEED)
+ 		obj->dirty = 0;
+ 
+-	for_each_sg(obj->pages->sgl, sg, obj->pages->nents, i) {
+-		struct page *page = sg_page(sg);
++	for_each_sg_page(obj->pages->sgl, &sg_iter, obj->pages->nents, 0) {
++		struct page *page = sg_page_iter_page(&sg_iter);
+ 
+ 		if (obj->dirty)
+ 			set_page_dirty(page);
+@@ -821,11 +828,10 @@
+ 	return 0;
+ }
+ 
+-int
++void
+ i915_gem_init_userptr(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = to_i915(dev);
+ 	mutex_init(&dev_priv->mm_lock);
+ 	hash_init(dev_priv->mm_structs);
+-	return 0;
+ }
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_gpu_error.c b/drivers/gpu/drm/i915/i915_gpu_error.c
+--- a/drivers/gpu/drm/i915/i915_gpu_error.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/i915_gpu_error.c	2014-11-20 09:53:37.968762837 -0700
+@@ -28,6 +28,7 @@
+  */
+ 
+ #include <generated/utsrelease.h>
++#include <linux/zlib.h>
+ #include "i915_drv.h"
+ 
+ static const char *yesno(int v)
+@@ -192,15 +193,18 @@
+ 				struct drm_i915_error_buffer *err,
+ 				int count)
+ {
+-	err_printf(m, "%s [%d]:\n", name, count);
++	int n;
+ 
++	err_printf(m, "  %s [%d]:\n", name, count);
+ 	while (count--) {
+-		err_printf(m, "  %08x %8u %02x %02x %x %x",
++		err_printf(m, "    %08x %8u %02x %02x [",
+ 			   err->gtt_offset,
+ 			   err->size,
+ 			   err->read_domains,
+-			   err->write_domain,
+-			   err->rseqno, err->wseqno);
++			   err->write_domain);
++		for (n = 0; n < ARRAY_SIZE(err->rseqno); n++)
++			err_printf(m, " %x", err->rseqno[n]);
++		err_printf(m, " ] %x %x ", err->wseqno, err->fseqno);
+ 		err_puts(m, pin_flag(err->pinned));
+ 		err_puts(m, tiling_flag(err->tiling));
+ 		err_puts(m, dirty_flag(err->dirty));
+@@ -208,7 +212,7 @@
+ 		err_puts(m, err->userptr ? " userptr" : "");
+ 		err_puts(m, err->ring != -1 ? " " : "");
+ 		err_puts(m, ring_str(err->ring));
+-		err_puts(m, i915_cache_level_str(err->cache_level));
++		err_puts(m, i915_cache_level_str(m->i915, err->cache_level));
+ 
+ 		if (err->name)
+ 			err_printf(m, " (name: %d)", err->name);
+@@ -220,11 +224,13 @@
+ 	}
+ }
+ 
+-static const char *hangcheck_action_to_str(enum intel_ring_hangcheck_action a)
++static const char *hangcheck_action_to_str(enum intel_engine_hangcheck_action a)
+ {
+ 	switch (a) {
+ 	case HANGCHECK_IDLE:
+ 		return "idle";
++	case HANGCHECK_IDLE_WAITERS:
++		return "idle (with waiters)";
+ 	case HANGCHECK_WAIT:
+ 		return "wait";
+ 	case HANGCHECK_ACTIVE:
+@@ -244,13 +250,19 @@
+ 				  struct drm_device *dev,
+ 				  struct drm_i915_error_ring *ring)
+ {
++	int n;
++
+ 	if (!ring->valid)
+ 		return;
+ 
+-	err_printf(m, "  HEAD: 0x%08x\n", ring->head);
+-	err_printf(m, "  TAIL: 0x%08x\n", ring->tail);
+-	err_printf(m, "  CTL: 0x%08x\n", ring->ctl);
+-	err_printf(m, "  HWS: 0x%08x\n", ring->hws);
++	err_printf(m, "%s command stream:\n", ring_str(ring->id));
++
++	err_printf(m, "  START: 0x%08x\n", ring->start);
++	err_printf(m, "  HEAD:  0x%08x\n", ring->head);
++	err_printf(m, "  TAIL:  0x%08x\n", ring->tail);
++	err_printf(m, "  CTL:   0x%08x\n", ring->ctl);
++	err_printf(m, "  MODE:  0x%08x [idle? %d]\n", ring->mode, !!(ring->mode & MODE_IDLE));
++	err_printf(m, "  HWS:   0x%08x\n", ring->hws);
+ 	err_printf(m, "  ACTHD: 0x%08x %08x\n", (u32)(ring->acthd>>32), (u32)ring->acthd);
+ 	err_printf(m, "  IPEIR: 0x%08x\n", ring->ipeir);
+ 	err_printf(m, "  IPEHR: 0x%08x\n", ring->ipehr);
+@@ -266,17 +278,13 @@
+ 	if (INTEL_INFO(dev)->gen >= 6) {
+ 		err_printf(m, "  RC PSMI: 0x%08x\n", ring->rc_psmi);
+ 		err_printf(m, "  FAULT_REG: 0x%08x\n", ring->fault_reg);
+-		err_printf(m, "  SYNC_0: 0x%08x [last synced 0x%08x]\n",
+-			   ring->semaphore_mboxes[0],
+-			   ring->semaphore_seqno[0]);
+-		err_printf(m, "  SYNC_1: 0x%08x [last synced 0x%08x]\n",
+-			   ring->semaphore_mboxes[1],
+-			   ring->semaphore_seqno[1]);
+-		if (HAS_VEBOX(dev)) {
+-			err_printf(m, "  SYNC_2: 0x%08x [last synced 0x%08x]\n",
+-				   ring->semaphore_mboxes[2],
+-				   ring->semaphore_seqno[2]);
+-		}
++		err_printf(m, "  SYNC_0: 0x%08x\n",
++			   ring->semaphore_mboxes[0]);
++		err_printf(m, "  SYNC_1: 0x%08x\n",
++			   ring->semaphore_mboxes[1]);
++		if (HAS_VEBOX(dev))
++			err_printf(m, "  SYNC_2: 0x%08x\n",
++				   ring->semaphore_mboxes[2]);
+ 	}
+ 	if (USES_PPGTT(dev)) {
+ 		err_printf(m, "  GFX_MODE: 0x%08x\n", ring->vm_info.gfx_mode);
+@@ -291,8 +299,20 @@
+ 				   ring->vm_info.pp_dir_base);
+ 		}
+ 	}
+-	err_printf(m, "  seqno: 0x%08x\n", ring->seqno);
+-	err_printf(m, "  waiting: %s\n", yesno(ring->waiting));
++	err_printf(m, "  tag: 0x%04x\n", ring->tag);
++	err_printf(m, "  seqno: 0x%08x [hangcheck 0x%08x, breadcrumb 0x%08x, request 0x%08x]\n",
++		   ring->seqno, ring->hangcheck, ring->breadcrumb[ring->id], ring->request);
++	err_printf(m, "  sem.signal: [");
++	for (n = 0; n < ARRAY_SIZE(ring->breadcrumb); n++)
++		err_printf(m, " %s%08x", n == ring->id ? "*" : "", ring->breadcrumb[n]);
++	err_printf(m, " ]\n");
++	err_printf(m, "  sem.waited: [");
++	for (n = 0; n < ARRAY_SIZE(ring->semaphore_sync); n++)
++		err_printf(m, " %s%08x", n == ring->id ? "*" : "", ring->semaphore_sync[n]);
++	err_printf(m, " ]\n");
++	err_printf(m, "  waiting: %s [irq count %d]\n",
++		   yesno(ring->waiting), ring->irq_count);
++	err_printf(m, "  interrupts: %d\n", ring->interrupts);
+ 	err_printf(m, "  ring->head: 0x%08x\n", ring->cpu_ring_head);
+ 	err_printf(m, "  ring->tail: 0x%08x\n", ring->cpu_ring_tail);
+ 	err_printf(m, "  hangcheck: %s [%d]\n",
+@@ -309,18 +329,44 @@
+ 	va_end(args);
+ }
+ 
++static bool
++ascii85_encode(uint32_t in, char *out)
++{
++	int i;
++
++	if (in == 0)
++		return false;
++
++	out[5] = '\0';
++	for (i = 5; i--; ) {
++		int digit = in % 85;
++		out[i] = digit + 33;
++		in /= 85;
++	}
++
++	return true;
++}
++
+ static void print_error_obj(struct drm_i915_error_state_buf *m,
+ 			    struct drm_i915_error_object *obj)
+ {
+-	int page, offset, elt;
++	char out[6];
++	int page;
+ 
+-	for (page = offset = 0; page < obj->page_count; page++) {
+-		for (elt = 0; elt < PAGE_SIZE/4; elt++) {
+-			err_printf(m, "%08x :  %08x\n", offset,
+-				   obj->pages[page][elt]);
+-			offset += 4;
++	err_puts(m, ":"); /* indicate compressed data */
++	for (page = 0; page < obj->page_count; page++) {
++		int i, len = PAGE_SIZE;
++		if (page == obj->page_count - 1)
++			len -= obj->unused;
++		len = (len + 3) / 4;
++		for (i = 0; i < len; i++) {
++			if (ascii85_encode(obj->pages[page][i], out))
++				err_puts(m, out);
++			else
++				err_puts(m, "z");
+ 		}
+ 	}
++	err_puts(m, "\n");
+ }
+ 
+ int i915_error_state_to_str(struct drm_i915_error_state_buf *m,
+@@ -330,8 +376,8 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct drm_i915_error_state *error = error_priv->error;
+ 	struct drm_i915_error_object *obj;
+-	int i, j, offset, elt;
+ 	int max_hangcheck_score;
++	int i, j;
+ 
+ 	if (!error) {
+ 		err_printf(m, "no error state collected\n");
+@@ -362,11 +408,16 @@
+ 	err_printf(m, "EIR: 0x%08x\n", error->eir);
+ 	err_printf(m, "IER: 0x%08x\n", error->ier);
+ 	if (INTEL_INFO(dev)->gen >= 8) {
+-		for (i = 0; i < 4; i++)
++		for (i = 0; i < 4; i++) {
+ 			err_printf(m, "GTIER gt %d: 0x%08x\n", i,
+ 				   error->gtier[i]);
+-	} else if (HAS_PCH_SPLIT(dev) || IS_VALLEYVIEW(dev))
++			err_printf(m, "GTIMR gt %d: 0x%08x\n", i,
++				   error->gtimr[i]);
++		}
++	} else if (HAS_PCH_SPLIT(dev) || IS_VALLEYVIEW(dev)) {
+ 		err_printf(m, "GTIER: 0x%08x\n", error->gtier[0]);
++		err_printf(m, "GTIMR: 0x%08x\n", error->gtimr[0]);
++	}
+ 	err_printf(m, "PGTBL_ER: 0x%08x\n", error->pgtbl_er);
+ 	err_printf(m, "FORCEWAKE: 0x%08x\n", error->forcewake);
+ 	err_printf(m, "DERRMR: 0x%08x\n", error->derrmr);
+@@ -388,94 +439,82 @@
+ 	if (INTEL_INFO(dev)->gen == 7)
+ 		err_printf(m, "ERR_INT: 0x%08x\n", error->err_int);
+ 
+-	for (i = 0; i < ARRAY_SIZE(error->ring); i++) {
+-		err_printf(m, "%s command stream:\n", ring_str(i));
++	for (i = 0; i < ARRAY_SIZE(error->ring); i++)
+ 		i915_ring_error_state(m, dev, &error->ring[i]);
+-	}
+ 
+-	if (error->active_bo)
++	for (i = 0; i < error->vm_count; i++) {
++		err_printf(m, "vm[%d]\n", i);
++
+ 		print_error_buffers(m, "Active",
+-				    error->active_bo[0],
+-				    error->active_bo_count[0]);
++				    error->active_bo[i],
++				    error->active_bo_count[i]);
+ 
+-	if (error->pinned_bo)
+ 		print_error_buffers(m, "Pinned",
+-				    error->pinned_bo[0],
+-				    error->pinned_bo_count[0]);
++				    error->pinned_bo[i],
++				    error->pinned_bo_count[i]);
++	}
+ 
+ 	for (i = 0; i < ARRAY_SIZE(error->ring); i++) {
+-		obj = error->ring[i].batchbuffer;
++		const struct drm_i915_error_ring *ering = &error->ring[i];
++		const char *name = dev_priv->engine[ering->id].name;
++
++		obj = ering->batchbuffer;
+ 		if (obj) {
+-			err_puts(m, dev_priv->ring[i].name);
+-			if (error->ring[i].pid != -1)
++			err_puts(m, name);
++			if (ering->pid != -1)
+ 				err_printf(m, " (submitted by %s [%d])",
+-					   error->ring[i].comm,
+-					   error->ring[i].pid);
++					   ering->comm, ering->pid);
+ 			err_printf(m, " --- gtt_offset = 0x%08x\n",
+ 				   obj->gtt_offset);
+ 			print_error_obj(m, obj);
+ 		}
+ 
+-		obj = error->ring[i].wa_batchbuffer;
++		obj = ering->wa_batchbuffer;
+ 		if (obj) {
+ 			err_printf(m, "%s (w/a) --- gtt_offset = 0x%08x\n",
+-				   dev_priv->ring[i].name, obj->gtt_offset);
++				   name, obj->gtt_offset);
+ 			print_error_obj(m, obj);
+ 		}
+ 
+-		if (error->ring[i].num_requests) {
+-			err_printf(m, "%s --- %d requests\n",
+-				   dev_priv->ring[i].name,
+-				   error->ring[i].num_requests);
+-			for (j = 0; j < error->ring[i].num_requests; j++) {
+-				err_printf(m, "  seqno 0x%08x, emitted %ld, tail 0x%08x\n",
+-					   error->ring[i].requests[j].seqno,
+-					   error->ring[i].requests[j].jiffies,
+-					   error->ring[i].requests[j].tail);
++		if (ering->num_batches) {
++			err_printf(m, "%s --- %d batches\n",
++				   name, ering->num_batches);
++			for (j = 0; j < ering->num_batches; j++) {
++				err_printf(m, "  pid %ld, seqno 0x%08x, tag 0x%04x, emitted %dms ago (at %ld jiffies), head 0x%08x, tail 0x%08x, batch 0x%08x, complete? %d\n",
++					   ering->batches[j].pid,
++					   ering->batches[j].seqno,
++					   ering->batches[j].tag,
++					   jiffies_to_usecs(jiffies - ering->batches[j].jiffies) / 1000,
++					   ering->batches[j].jiffies,
++					   ering->batches[j].head,
++					   ering->batches[j].tail,
++					   ering->batches[j].batch,
++					   ering->batches[j].complete);
+ 			}
+ 		}
+ 
+-		if ((obj = error->ring[i].ringbuffer)) {
++		if ((obj = ering->ringbuffer)) {
+ 			err_printf(m, "%s --- ringbuffer = 0x%08x\n",
+-				   dev_priv->ring[i].name,
+-				   obj->gtt_offset);
++				   name, obj->gtt_offset);
+ 			print_error_obj(m, obj);
+ 		}
+ 
+-		if ((obj = error->ring[i].hws_page)) {
++		if ((obj = ering->hws_page)) {
+ 			err_printf(m, "%s --- HW Status = 0x%08x\n",
+-				   dev_priv->ring[i].name,
+-				   obj->gtt_offset);
+-			offset = 0;
+-			for (elt = 0; elt < PAGE_SIZE/16; elt += 4) {
+-				err_printf(m, "[%04x] %08x %08x %08x %08x\n",
+-					   offset,
+-					   obj->pages[0][elt],
+-					   obj->pages[0][elt+1],
+-					   obj->pages[0][elt+2],
+-					   obj->pages[0][elt+3]);
+-					offset += 16;
+-			}
++				   name, obj->gtt_offset);
++			print_error_obj(m, obj);
+ 		}
+ 
+ 		if ((obj = error->ring[i].ctx)) {
+ 			err_printf(m, "%s --- HW Context = 0x%08x\n",
+-				   dev_priv->ring[i].name,
+-				   obj->gtt_offset);
++				   name, obj->gtt_offset);
+ 			print_error_obj(m, obj);
+ 		}
+ 	}
+ 
+ 	if ((obj = error->semaphore_obj)) {
+ 		err_printf(m, "Semaphore page = 0x%08x\n", obj->gtt_offset);
+-		for (elt = 0; elt < PAGE_SIZE/16; elt += 4) {
+-			err_printf(m, "[%04x] %08x %08x %08x %08x\n",
+-				   elt * 4,
+-				   obj->pages[0][elt],
+-				   obj->pages[0][elt+1],
+-				   obj->pages[0][elt+2],
+-				   obj->pages[0][elt+3]);
+-		}
++		print_error_obj(m, obj);
+ 	}
+ 
+ 	if (error->overlay)
+@@ -492,9 +531,11 @@
+ }
+ 
+ int i915_error_state_buf_init(struct drm_i915_error_state_buf *ebuf,
++			      struct drm_i915_private *i915,
+ 			      size_t count, loff_t pos)
+ {
+ 	memset(ebuf, 0, sizeof(*ebuf));
++	ebuf->i915 = i915;
+ 
+ 	/* We need to have enough room to store any i915_error_state printf
+ 	 * so that we can move it to start position.
+@@ -529,7 +570,7 @@
+ 		return;
+ 
+ 	for (page = 0; page < obj->page_count; page++)
+-		kfree(obj->pages[page]);
++		free_page((unsigned long)obj->pages[page]);
+ 
+ 	kfree(obj);
+ }
+@@ -545,7 +586,7 @@
+ 		i915_error_object_free(error->ring[i].ringbuffer);
+ 		i915_error_object_free(error->ring[i].hws_page);
+ 		i915_error_object_free(error->ring[i].ctx);
+-		kfree(error->ring[i].requests);
++		kfree(error->ring[i].batches);
+ 	}
+ 
+ 	i915_error_object_free(error->semaphore_obj);
+@@ -555,102 +596,157 @@
+ 	kfree(error);
+ }
+ 
++static int compress_page(struct z_stream_s *zstream,
++			 void *src,
++			 struct drm_i915_error_object *dst)
++{
++	zstream->next_in = src;
++	zstream->avail_in = PAGE_SIZE;
++
++	do {
++		if (zstream->avail_out == 0) {
++			zstream->next_out = (void *)__get_free_page(GFP_ATOMIC);
++			if (zstream->next_out == NULL)
++				return -ENOMEM;
++
++			dst->pages[dst->page_count++] = (void *)zstream->next_out;
++			zstream->avail_out = PAGE_SIZE;
++		}
++
++		if (zlib_deflate(zstream, Z_SYNC_FLUSH) != Z_OK)
++			return -EIO;
++
++#if 0
++		if (zstream->total_out > zstream->total_in)
++			return -E2BIG;
++#endif
++	} while (zstream->avail_in);
++
++	return 0;
++}
++
+ static struct drm_i915_error_object *
+-i915_error_object_create_sized(struct drm_i915_private *dev_priv,
+-			       struct drm_i915_gem_object *src,
+-			       struct i915_address_space *vm,
+-			       const int num_pages)
++i915_error_object_create(struct drm_i915_private *dev_priv,
++			 struct i915_vma *vma)
+ {
++	struct drm_i915_gem_object *src;
+ 	struct drm_i915_error_object *dst;
+-	int i;
++	int num_pages;
++	bool use_ggtt;
+ 	u32 reloc_offset;
++	struct z_stream_s zstream;
++
++	if (vma == NULL)
++		return NULL;
+ 
+-	if (src == NULL || src->pages == NULL)
++	src = vma->obj;
++	if (src->pages == NULL)
+ 		return NULL;
+ 
+-	dst = kmalloc(sizeof(*dst) + num_pages * sizeof(u32 *), GFP_ATOMIC);
++	num_pages = src->base.size >> PAGE_SHIFT;
++
++	dst = kmalloc(sizeof(*dst) + (10 * num_pages * sizeof(u32 *) >> 3), GFP_ATOMIC);
+ 	if (dst == NULL)
+ 		return NULL;
+ 
+-	reloc_offset = dst->gtt_offset = i915_gem_obj_offset(src, vm);
+-	for (i = 0; i < num_pages; i++) {
+-		unsigned long flags;
+-		void *d;
++	dst->gtt_offset = vma->node.start;
++	dst->page_count = 0;
++	dst->unused = 0;
++
++	memset(&zstream, 0, sizeof(zstream));
++	zstream.workspace = kmalloc(zlib_deflate_workspacesize(MAX_WBITS, MAX_MEM_LEVEL),
++				    GFP_ATOMIC);
++	if (zstream.workspace == NULL ||
++	    zlib_deflateInit(&zstream, Z_DEFAULT_COMPRESSION) != Z_OK) {
++		kfree(dst);
++		return NULL;
++	}
+ 
+-		d = kmalloc(PAGE_SIZE, GFP_ATOMIC);
+-		if (d == NULL)
+-			goto unwind;
++	reloc_offset = dst->gtt_offset;
++	use_ggtt = (src->cache_level == I915_CACHE_NONE &&
++		    vma->bound & GLOBAL_BIND &&
++		    reloc_offset + num_pages * PAGE_SIZE <= dev_priv->gtt.mappable_end);
++
++	/* Cannot access stolen address directly, try to use the aperture */
++	if (src->stolen && !use_ggtt)
++		goto unwind;
++
++	/* Cannot access snooped pages through the aperture */
++	if (use_ggtt && src->cache_level != I915_CACHE_NONE && !HAS_LLC(dev_priv))
++		goto unwind;
+ 
+-		local_irq_save(flags);
+-		if (src->cache_level == I915_CACHE_NONE &&
+-		    reloc_offset < dev_priv->gtt.mappable_end &&
+-		    src->has_global_gtt_mapping &&
+-		    i915_is_ggtt(vm)) {
+-			void __iomem *s;
++	if (!use_ggtt)
++		reloc_offset = 0;
++
++	while (num_pages--) {
++		unsigned long flags;
++		void *s;
++		int ret;
+ 
++		local_irq_save(flags);
++		if (use_ggtt) {
+ 			/* Simply ignore tiling or any overlapping fence.
+ 			 * It's part of the error state, and this hopefully
+ 			 * captures what the GPU read.
+ 			 */
+-
+-			s = io_mapping_map_atomic_wc(dev_priv->gtt.mappable,
+-						     reloc_offset);
+-			memcpy_fromio(d, s, PAGE_SIZE);
++			s = (void *__force)
++				io_mapping_map_atomic_wc(dev_priv->gtt.mappable,
++							 reloc_offset);
++			ret = compress_page(&zstream, s, dst);
+ 			io_mapping_unmap_atomic(s);
+-		} else if (src->stolen) {
+-			unsigned long offset;
+-
+-			offset = dev_priv->mm.stolen_base;
+-			offset += src->stolen->start;
+-			offset += i << PAGE_SHIFT;
+-
+-			memcpy_fromio(d, (void __iomem *) offset, PAGE_SIZE);
+ 		} else {
+-			struct page *page;
+-			void *s;
+-
+-			page = i915_gem_object_get_page(src, i);
+-
+-			drm_clflush_pages(&page, 1);
+-
+-			s = kmap_atomic(page);
+-			memcpy(d, s, PAGE_SIZE);
++			s = kmap_atomic(i915_gem_object_get_page(src, reloc_offset >> PAGE_SHIFT));
++			if (!HAS_LLC(dev_priv))
++				drm_clflush_virt_range(s, PAGE_SIZE);
++			ret = compress_page(&zstream, s, dst);
+ 			kunmap_atomic(s);
+-
+-			drm_clflush_pages(&page, 1);
+ 		}
+ 		local_irq_restore(flags);
+-
+-		dst->pages[i] = d;
+-
++		if (ret)
++			goto unwind;
+ 		reloc_offset += PAGE_SIZE;
+ 	}
+-	dst->page_count = num_pages;
++	zlib_deflate(&zstream, Z_FINISH);
++	dst->unused = zstream.avail_out;
++	zlib_deflateEnd(&zstream);
++	kfree(zstream.workspace);
+ 
+ 	return dst;
+ 
+ unwind:
+-	while (i--)
+-		kfree(dst->pages[i]);
++	while (dst->page_count--)
++		free_page((unsigned long)dst->pages[dst->page_count]);
++	zlib_deflateEnd(&zstream);
++	kfree(zstream.workspace);
+ 	kfree(dst);
+ 	return NULL;
+ }
+-#define i915_error_object_create(dev_priv, src, vm) \
+-	i915_error_object_create_sized((dev_priv), (src), (vm), \
+-				       (src)->base.size>>PAGE_SHIFT)
+-
+-#define i915_error_ggtt_object_create(dev_priv, src) \
+-	i915_error_object_create_sized((dev_priv), (src), &(dev_priv)->gtt.base, \
+-				       (src)->base.size>>PAGE_SHIFT)
++
++static inline struct drm_i915_error_object *
++i915_error_ggtt_object_create(struct drm_i915_private *i915,
++			      struct drm_i915_gem_object *src)
++{
++	if (src == NULL)
++		return NULL;
++
++	return i915_error_object_create(i915,
++					i915_gem_obj_to_vma(src,
++							    &i915->gtt.base));
++}
+ 
+ static void capture_bo(struct drm_i915_error_buffer *err,
+-		       struct drm_i915_gem_object *obj)
++		       struct i915_vma *vma)
+ {
++	struct drm_i915_gem_object *obj = vma->obj;
++	int n;
++
+ 	err->size = obj->base.size;
+ 	err->name = obj->base.name;
+-	err->rseqno = obj->last_read_seqno;
+-	err->wseqno = obj->last_write_seqno;
+-	err->gtt_offset = i915_gem_obj_ggtt_offset(obj);
++	for (n = 0; n < ARRAY_SIZE(obj->last_read); n++)
++		err->rseqno[n] = i915_request_seqno(obj->last_read[n].request);
++	err->wseqno = i915_request_seqno(obj->last_write.request);
++	err->fseqno = i915_request_seqno(obj->last_fence.request);
++	err->gtt_offset = vma->node.start;
+ 	err->read_domains = obj->base.read_domains;
+ 	err->write_domain = obj->base.write_domain;
+ 	err->fence_reg = obj->fence_reg;
+@@ -663,7 +759,7 @@
+ 	err->dirty = obj->dirty;
+ 	err->purgeable = obj->madv != I915_MADV_WILLNEED;
+ 	err->userptr = obj->userptr.mm != NULL;
+-	err->ring = obj->ring ? obj->ring->id : -1;
++	err->ring = i915_request_engine_id(obj->last_write.request);
+ 	err->cache_level = obj->cache_level;
+ }
+ 
+@@ -674,7 +770,7 @@
+ 	int i = 0;
+ 
+ 	list_for_each_entry(vma, head, mm_list) {
+-		capture_bo(err++, vma->obj);
++		capture_bo(err++, vma);
+ 		if (++i == count)
+ 			break;
+ 	}
+@@ -683,21 +779,27 @@
+ }
+ 
+ static u32 capture_pinned_bo(struct drm_i915_error_buffer *err,
+-			     int count, struct list_head *head)
++			     int count, struct list_head *head,
++			     struct i915_address_space *vm)
+ {
+ 	struct drm_i915_gem_object *obj;
+-	int i = 0;
++	struct drm_i915_error_buffer * const first = err;
++	struct drm_i915_error_buffer * const last = err + count;
+ 
+ 	list_for_each_entry(obj, head, global_list) {
+-		if (!i915_gem_obj_is_pinned(obj))
+-			continue;
++		struct i915_vma *vma;
+ 
+-		capture_bo(err++, obj);
+-		if (++i == count)
++		if (err == last)
+ 			break;
++
++		list_for_each_entry(vma, &obj->vma_list, obj_link)
++			if (vma->vm == vm && vma->pin_count > 0) {
++				capture_bo(err++, vma);
++				break;
++			}
+ 	}
+ 
+-	return i;
++	return err - first;
+ }
+ 
+ /* Generate a semi-unique error code. The code is not meant to have meaning, The
+@@ -721,7 +823,7 @@
+ 	 * synchronization commands which almost always appear in the case
+ 	 * strictly a client bug. Use instdone to differentiate those some.
+ 	 */
+-	for (i = 0; i < I915_NUM_RINGS; i++) {
++	for (i = 0; i < I915_NUM_ENGINES; i++) {
+ 		if (error->ring[i].hangcheck_action == HANGCHECK_HUNG) {
+ 			if (ring_id)
+ 				*ring_id = i;
+@@ -741,6 +843,7 @@
+ 
+ 	/* Fences */
+ 	switch (INTEL_INFO(dev)->gen) {
++	case 9:
+ 	case 8:
+ 	case 7:
+ 	case 6:
+@@ -769,83 +872,77 @@
+ 
+ static void gen8_record_semaphore_state(struct drm_i915_private *dev_priv,
+ 					struct drm_i915_error_state *error,
+-					struct intel_engine_cs *ring,
++					struct intel_engine_cs *engine,
+ 					struct drm_i915_error_ring *ering)
+ {
+ 	struct intel_engine_cs *to;
++	u32 *mbox;
+ 	int i;
+ 
+-	if (!i915_semaphore_is_enabled(dev_priv->dev))
++	if (dev_priv->semaphore_obj == NULL)
+ 		return;
+ 
+-	if (!error->semaphore_obj)
++	if (error->semaphore_obj == NULL)
+ 		error->semaphore_obj =
+-			i915_error_object_create(dev_priv,
+-						 dev_priv->semaphore_obj,
+-						 &dev_priv->gtt.base);
+-
+-	for_each_ring(to, dev_priv, i) {
+-		int idx;
+-		u16 signal_offset;
+-		u32 *tmp;
++			i915_error_ggtt_object_create(dev_priv,
++						      dev_priv->semaphore_obj);
++	if (error->semaphore_obj == NULL)
++		return;
+ 
+-		if (ring == to)
++	mbox = error->semaphore_obj->pages[0];
++	for_each_engine(to, dev_priv, i) {
++		if (engine == to)
+ 			continue;
+ 
+-		signal_offset = (GEN8_SIGNAL_OFFSET(ring, i) & (PAGE_SIZE - 1))
+-				/ 4;
+-		tmp = error->semaphore_obj->pages[0];
+-		idx = intel_ring_sync_index(ring, to);
+-
+-		ering->semaphore_mboxes[idx] = tmp[signal_offset];
+-		ering->semaphore_seqno[idx] = ring->semaphore.sync_seqno[idx];
++		ering->semaphore_mboxes[i] =
++			mbox[GEN8_SEMAPHORE_OFFSET(dev_priv,
++						   engine->id,
++						   i) & (PAGE_SIZE - 1) / 4];
+ 	}
+ }
+ 
+ static void gen6_record_semaphore_state(struct drm_i915_private *dev_priv,
+-					struct intel_engine_cs *ring,
++					struct intel_engine_cs *engine,
+ 					struct drm_i915_error_ring *ering)
+ {
+-	ering->semaphore_mboxes[0] = I915_READ(RING_SYNC_0(ring->mmio_base));
+-	ering->semaphore_mboxes[1] = I915_READ(RING_SYNC_1(ring->mmio_base));
+-	ering->semaphore_seqno[0] = ring->semaphore.sync_seqno[0];
+-	ering->semaphore_seqno[1] = ring->semaphore.sync_seqno[1];
+-
++	ering->semaphore_mboxes[0] = I915_READ(RING_SYNC_0(engine->mmio_base));
++	ering->semaphore_mboxes[1] = I915_READ(RING_SYNC_1(engine->mmio_base));
+ 	if (HAS_VEBOX(dev_priv->dev)) {
+ 		ering->semaphore_mboxes[2] =
+-			I915_READ(RING_SYNC_2(ring->mmio_base));
+-		ering->semaphore_seqno[2] = ring->semaphore.sync_seqno[2];
++			I915_READ(RING_SYNC_2(engine->mmio_base));
+ 	}
+ }
+ 
+ static void i915_record_ring_state(struct drm_device *dev,
+ 				   struct drm_i915_error_state *error,
+-				   struct intel_engine_cs *ring,
++				   struct intel_engine_cs *engine,
++				   struct i915_gem_request *rq,
+ 				   struct drm_i915_error_ring *ering)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_ringbuffer *ring;
+ 
+ 	if (INTEL_INFO(dev)->gen >= 6) {
+-		ering->rc_psmi = I915_READ(ring->mmio_base + 0x50);
+-		ering->fault_reg = I915_READ(RING_FAULT_REG(ring));
++		ering->rc_psmi = I915_READ(engine->mmio_base + 0x50);
++		ering->fault_reg = I915_READ(RING_FAULT_REG(engine));
+ 		if (INTEL_INFO(dev)->gen >= 8)
+-			gen8_record_semaphore_state(dev_priv, error, ring, ering);
++			gen8_record_semaphore_state(dev_priv, error, engine, ering);
+ 		else
+-			gen6_record_semaphore_state(dev_priv, ring, ering);
++			gen6_record_semaphore_state(dev_priv, engine, ering);
+ 	}
+ 
+ 	if (INTEL_INFO(dev)->gen >= 4) {
+-		ering->faddr = I915_READ(RING_DMA_FADD(ring->mmio_base));
+-		ering->ipeir = I915_READ(RING_IPEIR(ring->mmio_base));
+-		ering->ipehr = I915_READ(RING_IPEHR(ring->mmio_base));
+-		ering->instdone = I915_READ(RING_INSTDONE(ring->mmio_base));
+-		ering->instps = I915_READ(RING_INSTPS(ring->mmio_base));
+-		ering->bbaddr = I915_READ(RING_BBADDR(ring->mmio_base));
++		ering->faddr = I915_READ(RING_DMA_FADD(engine->mmio_base));
++		ering->ipeir = I915_READ(RING_IPEIR(engine->mmio_base));
++		ering->ipehr = I915_READ(RING_IPEHR(engine->mmio_base));
++		ering->instdone = I915_READ(RING_INSTDONE(engine->mmio_base));
++		ering->instps = I915_READ(RING_INSTPS(engine->mmio_base));
++		ering->bbaddr = I915_READ(RING_BBADDR(engine->mmio_base));
+ 		if (INTEL_INFO(dev)->gen >= 8) {
+-			ering->faddr |= (u64) I915_READ(RING_DMA_FADD_UDW(ring->mmio_base)) << 32;
+-			ering->bbaddr |= (u64) I915_READ(RING_BBADDR_UDW(ring->mmio_base)) << 32;
++			ering->faddr |= (u64) I915_READ(RING_DMA_FADD_UDW(engine->mmio_base)) << 32;
++			ering->bbaddr |= (u64) I915_READ(RING_BBADDR_UDW(engine->mmio_base)) << 32;
+ 		}
+-		ering->bbstate = I915_READ(RING_BBSTATE(ring->mmio_base));
++		ering->bbstate = I915_READ(RING_BBSTATE(engine->mmio_base));
+ 	} else {
+ 		ering->faddr = I915_READ(DMA_FADD_I8XX);
+ 		ering->ipeir = I915_READ(IPEIR);
+@@ -853,19 +950,29 @@
+ 		ering->instdone = I915_READ(INSTDONE);
+ 	}
+ 
+-	ering->waiting = waitqueue_active(&ring->irq_queue);
+-	ering->instpm = I915_READ(RING_INSTPM(ring->mmio_base));
+-	ering->seqno = ring->get_seqno(ring, false);
+-	ering->acthd = intel_ring_get_active_head(ring);
+-	ering->head = I915_READ_HEAD(ring);
+-	ering->tail = I915_READ_TAIL(ring);
+-	ering->ctl = I915_READ_CTL(ring);
++	ering->waiting = waitqueue_active(&engine->irq_queue);
++	ering->instpm = I915_READ(RING_INSTPM(engine->mmio_base));
++	ering->acthd = intel_engine_get_active_head(engine);
++	ering->seqno = intel_engine_get_seqno(engine);
++	ering->request = engine->last_request ? engine->last_request->seqno : 0;
++	ering->hangcheck = engine->hangcheck.seqno;
++	memcpy(ering->breadcrumb, engine->breadcrumb, sizeof(ering->breadcrumb));
++	memcpy(ering->semaphore_sync, engine->semaphore.sync, sizeof(ering->semaphore_sync));
++	ering->tag = engine->tag;
++	ering->interrupts = atomic_read(&engine->interrupts);
++	ering->irq_count = engine->irq_refcount;
++	ering->start = I915_READ_START(engine);
++	ering->head = I915_READ_HEAD(engine);
++	ering->tail = I915_READ_TAIL(engine);
++	ering->ctl = I915_READ_CTL(engine);
++	if (!IS_GEN2(dev_priv))
++		ering->mode = I915_READ_MODE(engine);
+ 
+ 	if (I915_NEED_GFX_HWS(dev)) {
+ 		int mmio;
+ 
+ 		if (IS_GEN7(dev)) {
+-			switch (ring->id) {
++			switch (engine->id) {
+ 			default:
+ 			case RCS:
+ 				mmio = RENDER_HWS_PGA_GEN7;
+@@ -880,59 +987,68 @@
+ 				mmio = VEBOX_HWS_PGA_GEN7;
+ 				break;
+ 			}
+-		} else if (IS_GEN6(ring->dev)) {
+-			mmio = RING_HWS_PGA_GEN6(ring->mmio_base);
++		} else if (IS_GEN6(engine->i915)) {
++			mmio = RING_HWS_PGA_GEN6(engine->mmio_base);
+ 		} else {
+ 			/* XXX: gen8 returns to sanity */
+-			mmio = RING_HWS_PGA(ring->mmio_base);
++			mmio = RING_HWS_PGA(engine->mmio_base);
+ 		}
+ 
+ 		ering->hws = I915_READ(mmio);
+ 	}
+ 
+-	ering->cpu_ring_head = ring->buffer->head;
+-	ering->cpu_ring_tail = ring->buffer->tail;
++	ring = rq ? rq->ctx->ring[engine->id].ring : engine->default_context->ring[engine->id].ring;
++	if (ring) {
++		ering->cpu_ring_head = ring->head;
++		ering->cpu_ring_tail = ring->tail;
++		ering->ringbuffer =
++			i915_error_ggtt_object_create(dev_priv, ring->obj);
++	}
+ 
+-	ering->hangcheck_score = ring->hangcheck.score;
+-	ering->hangcheck_action = ring->hangcheck.action;
++	ering->hws_page =
++		i915_error_ggtt_object_create(dev_priv,
++					      engine->status_page.obj);
++
++	ering->hangcheck_score = engine->hangcheck.score;
++	ering->hangcheck_action = engine->hangcheck.action;
+ 
+ 	if (USES_PPGTT(dev)) {
+ 		int i;
+ 
+-		ering->vm_info.gfx_mode = I915_READ(RING_MODE_GEN7(ring));
++		ering->vm_info.gfx_mode = I915_READ(RING_MODE_GEN7(engine));
+ 
+ 		switch (INTEL_INFO(dev)->gen) {
++		case 9:
+ 		case 8:
+ 			for (i = 0; i < 4; i++) {
+ 				ering->vm_info.pdp[i] =
+-					I915_READ(GEN8_RING_PDP_UDW(ring, i));
++					I915_READ(GEN8_RING_PDP_UDW(engine, i));
+ 				ering->vm_info.pdp[i] <<= 32;
+ 				ering->vm_info.pdp[i] |=
+-					I915_READ(GEN8_RING_PDP_LDW(ring, i));
++					I915_READ(GEN8_RING_PDP_LDW(engine, i));
+ 			}
+ 			break;
+ 		case 7:
+ 			ering->vm_info.pp_dir_base =
+-				I915_READ(RING_PP_DIR_BASE(ring));
++				I915_READ(RING_PP_DIR_BASE(engine));
+ 			break;
+ 		case 6:
+ 			ering->vm_info.pp_dir_base =
+-				I915_READ(RING_PP_DIR_BASE_READ(ring));
++				I915_READ(RING_PP_DIR_BASE_READ(engine));
+ 			break;
+ 		}
+ 	}
+ }
+ 
+-
+-static void i915_gem_record_active_context(struct intel_engine_cs *ring,
++static void i915_gem_record_active_context(struct intel_engine_cs *engine,
+ 					   struct drm_i915_error_state *error,
+ 					   struct drm_i915_error_ring *ering)
+ {
+-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
++	struct drm_i915_private *dev_priv = engine->i915;
+ 	struct drm_i915_gem_object *obj;
+ 
+ 	/* Currently render ring is the only HW context user */
+-	if (ring->id != RCS || !error->ccid)
++	if (engine->id != RCS || !error->ccid)
+ 		return;
+ 
+ 	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list) {
+@@ -950,45 +1066,40 @@
+ 				  struct drm_i915_error_state *error)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct drm_i915_gem_request *request;
++	struct i915_gem_request *rq;
+ 	int i, count;
+ 
+-	for (i = 0; i < I915_NUM_RINGS; i++) {
+-		struct intel_engine_cs *ring = &dev_priv->ring[i];
++	for (i = 0; i < I915_NUM_ENGINES; i++) {
++		struct intel_engine_cs *engine = &dev_priv->engine[i];
+ 
+ 		error->ring[i].pid = -1;
+ 
+-		if (ring->dev == NULL)
++		if (engine->i915 == NULL)
+ 			continue;
+ 
+ 		error->ring[i].valid = true;
++		error->ring[i].id = i;
+ 
+-		i915_record_ring_state(dev, error, ring, &error->ring[i]);
+-
+-		request = i915_gem_find_active_request(ring);
+-		if (request) {
++		spin_lock(&engine->lock);
++		rq = intel_engine_find_active_batch(engine);
++		if (rq) {
+ 			/* We need to copy these to an anonymous buffer
+ 			 * as the simplest method to avoid being overwritten
+ 			 * by userspace.
+ 			 */
+ 			error->ring[i].batchbuffer =
+-				i915_error_object_create(dev_priv,
+-							 request->batch_obj,
+-							 request->ctx ?
+-							 request->ctx->vm :
+-							 &dev_priv->gtt.base);
++				i915_error_object_create(dev_priv, rq->batch);
+ 
+-			if (HAS_BROKEN_CS_TLB(dev_priv->dev) &&
+-			    ring->scratch.obj)
++			if (HAS_BROKEN_CS_TLB(dev_priv->dev))
+ 				error->ring[i].wa_batchbuffer =
+ 					i915_error_ggtt_object_create(dev_priv,
+-							     ring->scratch.obj);
++							     engine->scratch.obj);
+ 
+-			if (request->file_priv) {
++			if (rq->file_priv) {
+ 				struct task_struct *task;
+ 
+ 				rcu_read_lock();
+-				task = pid_task(request->file_priv->file->pid,
++				task = pid_task(rq->file_priv->file->pid,
+ 						PIDTYPE_PID);
+ 				if (task) {
+ 					strcpy(error->ring[i].comm, task->comm);
+@@ -998,37 +1109,50 @@
+ 			}
+ 		}
+ 
+-		error->ring[i].ringbuffer =
+-			i915_error_ggtt_object_create(dev_priv, ring->buffer->obj);
++		i915_record_ring_state(dev, error, engine, rq, &error->ring[i]);
+ 
+-		if (ring->status_page.obj)
+-			error->ring[i].hws_page =
+-				i915_error_ggtt_object_create(dev_priv, ring->status_page.obj);
+-
+-		i915_gem_record_active_context(ring, error, &error->ring[i]);
++		i915_gem_record_active_context(engine, error, &error->ring[i]);
+ 
+ 		count = 0;
+-		list_for_each_entry(request, &ring->request_list, list)
+-			count++;
++		list_for_each_entry(rq, &engine->requests, engine_link)
++			count += rq->batch != NULL;
+ 
+-		error->ring[i].num_requests = count;
+-		error->ring[i].requests =
+-			kcalloc(count, sizeof(*error->ring[i].requests),
++		error->ring[i].num_batches = count;
++		error->ring[i].batches =
++			kcalloc(count, sizeof(struct drm_i915_error_request),
+ 				GFP_ATOMIC);
+-		if (error->ring[i].requests == NULL) {
+-			error->ring[i].num_requests = 0;
++		if (error->ring[i].batches == NULL) {
++			error->ring[i].num_batches = 0;
+ 			continue;
+ 		}
+ 
+ 		count = 0;
+-		list_for_each_entry(request, &ring->request_list, list) {
++		list_for_each_entry(rq, &engine->requests, engine_link) {
+ 			struct drm_i915_error_request *erq;
++			struct task_struct *task;
++
++			if (rq->batch == NULL)
++				continue;
++
++			if (count == error->ring[i].num_batches)
++				break;
+ 
+-			erq = &error->ring[i].requests[count++];
+-			erq->seqno = request->seqno;
+-			erq->jiffies = request->emitted_jiffies;
+-			erq->tail = request->tail;
++			erq = &error->ring[i].batches[count++];
++			erq->seqno = rq->seqno;
++			erq->jiffies = rq->emitted_jiffies;
++			erq->head = rq->head;
++			erq->tail = rq->tail;
++			erq->batch = rq->batch->node.start;
++			memcpy(erq->breadcrumb, rq->breadcrumb, sizeof(rq->breadcrumb));
++			erq->complete = i915_request_complete(rq);
++			erq->tag = rq->tag;
++
++			rcu_read_lock();
++			task = rq->file_priv ? pid_task(rq->file_priv->file->pid, PIDTYPE_PID) : NULL;
++			erq->pid = task ? task->pid : 0;
++			rcu_read_unlock();
+ 		}
++		spin_unlock(&engine->lock);
+ 	}
+ }
+ 
+@@ -1049,9 +1173,14 @@
+ 	list_for_each_entry(vma, &vm->active_list, mm_list)
+ 		i++;
+ 	error->active_bo_count[ndx] = i;
+-	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list)
+-		if (i915_gem_obj_is_pinned(obj))
+-			i++;
++
++	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list) {
++		list_for_each_entry(vma, &obj->vma_list, obj_link)
++			if (vma->vm == vm && vma->pin_count > 0) {
++				i++;
++				break;
++			}
++	}
+ 	error->pinned_bo_count[ndx] = i - error->active_bo_count[ndx];
+ 
+ 	if (i) {
+@@ -1070,7 +1199,7 @@
+ 		error->pinned_bo_count[ndx] =
+ 			capture_pinned_bo(pinned_bo,
+ 					  error->pinned_bo_count[ndx],
+-					  &dev_priv->mm.bound_list);
++					  &dev_priv->mm.bound_list, vm);
+ 	error->active_bo[ndx] = active_bo;
+ 	error->pinned_bo[ndx] = pinned_bo;
+ }
+@@ -1091,8 +1220,25 @@
+ 	error->pinned_bo_count = kcalloc(cnt, sizeof(*error->pinned_bo_count),
+ 					 GFP_ATOMIC);
+ 
+-	list_for_each_entry(vm, &dev_priv->vm_list, global_link)
+-		i915_gem_capture_vm(dev_priv, error, vm, i++);
++	if (error->active_bo == NULL ||
++	    error->pinned_bo == NULL ||
++	    error->active_bo_count == NULL ||
++	    error->pinned_bo_count == NULL) {
++		kfree(error->active_bo);
++		kfree(error->active_bo_count);
++		kfree(error->pinned_bo);
++		kfree(error->pinned_bo_count);
++
++		error->active_bo = NULL;
++		error->active_bo_count = NULL;
++		error->pinned_bo = NULL;
++		error->pinned_bo_count = NULL;
++	} else {
++		list_for_each_entry(vm, &dev_priv->vm_list, global_link)
++			i915_gem_capture_vm(dev_priv, error, vm, i++);
++
++		error->vm_count = cnt;
++	}
+ }
+ 
+ /* Capture all registers which don't fit into another category. */
+@@ -1113,6 +1259,7 @@
+ 	/* 1: Registers specific to a single generation */
+ 	if (IS_VALLEYVIEW(dev)) {
+ 		error->gtier[0] = I915_READ(GTIER);
++		error->gtimr[0] = I915_READ(GTIMR);
+ 		error->ier = I915_READ(VLV_IER);
+ 		error->forcewake = I915_READ(FORCEWAKE_VLV);
+ 	}
+@@ -1148,11 +1295,14 @@
+ 
+ 	if (INTEL_INFO(dev)->gen >= 8) {
+ 		error->ier = I915_READ(GEN8_DE_MISC_IER);
+-		for (i = 0; i < 4; i++)
++		for (i = 0; i < 4; i++) {
+ 			error->gtier[i] = I915_READ(GEN8_GT_IER(i));
++			error->gtimr[i] = I915_READ(GEN8_GT_IMR(i));
++		}
+ 	} else if (HAS_PCH_SPLIT(dev)) {
+ 		error->ier = I915_READ(DEIER);
+ 		error->gtier[0] = I915_READ(GTIER);
++		error->gtimr[0] = I915_READ(GTIMR);
+ 	} else if (IS_GEN2(dev)) {
+ 		error->ier = I915_READ16(IER);
+ 	} else if (!IS_VALLEYVIEW(dev)) {
+@@ -1176,7 +1326,8 @@
+ 	ecode = i915_error_generate_code(dev_priv, error, &ring_id);
+ 
+ 	len = scnprintf(error->error_msg, sizeof(error->error_msg),
+-			"GPU HANG: ecode %d:0x%08x", ring_id, ecode);
++			"GPU HANG: ecode %d:%d:0x%08x",
++			INTEL_INFO(dev)->gen, ring_id, ecode);
+ 
+ 	if (ring_id != -1 && error->ring[ring_id].pid != -1)
+ 		len += scnprintf(error->error_msg + len,
+@@ -1264,13 +1415,12 @@
+ 			  struct i915_error_state_file_priv *error_priv)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	unsigned long flags;
+ 
+-	spin_lock_irqsave(&dev_priv->gpu_error.lock, flags);
++	spin_lock_irq(&dev_priv->gpu_error.lock);
+ 	error_priv->error = dev_priv->gpu_error.first_error;
+ 	if (error_priv->error)
+ 		kref_get(&error_priv->error->ref);
+-	spin_unlock_irqrestore(&dev_priv->gpu_error.lock, flags);
++	spin_unlock_irq(&dev_priv->gpu_error.lock);
+ 
+ }
+ 
+@@ -1284,22 +1434,21 @@
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct drm_i915_error_state *error;
+-	unsigned long flags;
+ 
+-	spin_lock_irqsave(&dev_priv->gpu_error.lock, flags);
++	spin_lock_irq(&dev_priv->gpu_error.lock);
+ 	error = dev_priv->gpu_error.first_error;
+ 	dev_priv->gpu_error.first_error = NULL;
+-	spin_unlock_irqrestore(&dev_priv->gpu_error.lock, flags);
++	spin_unlock_irq(&dev_priv->gpu_error.lock);
+ 
+ 	if (error)
+ 		kref_put(&error->ref, i915_error_state_free);
+ }
+ 
+-const char *i915_cache_level_str(int type)
++const char *i915_cache_level_str(struct drm_i915_private *i915, int type)
+ {
+ 	switch (type) {
+ 	case I915_CACHE_NONE: return " uncached";
+-	case I915_CACHE_LLC: return " snooped or LLC";
++	case I915_CACHE_LLC: return HAS_LLC(i915) ? " LLC" : " snooped";
+ 	case I915_CACHE_L3_LLC: return " L3+LLC";
+ 	case I915_CACHE_WT: return " WT";
+ 	default: return "";
+@@ -1327,6 +1476,7 @@
+ 		WARN_ONCE(1, "Unsupported platform\n");
+ 	case 7:
+ 	case 8:
++	case 9:
+ 		instdone[0] = I915_READ(GEN7_INSTDONE_1);
+ 		instdone[1] = I915_READ(GEN7_SC_INSTDONE);
+ 		instdone[2] = I915_READ(GEN7_SAMPLER_INSTDONE);
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_ioc32.c b/drivers/gpu/drm/i915/i915_ioc32.c
+--- a/drivers/gpu/drm/i915/i915_ioc32.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/i915_ioc32.c	2014-11-20 09:53:37.968762837 -0700
+@@ -189,7 +189,6 @@
+ 	[DRM_I915_ALLOC] = compat_i915_alloc
+ };
+ 
+-#ifdef CONFIG_COMPAT
+ /**
+  * Called whenever a 32-bit process running under a 64-bit kernel
+  * performs an ioctl on /dev/dri/card<n>.
+@@ -218,4 +217,3 @@
+ 
+ 	return ret;
+ }
+-#endif
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_irq.c b/drivers/gpu/drm/i915/i915_irq.c
+--- a/drivers/gpu/drm/i915/i915_irq.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/i915_irq.c	2014-11-20 09:53:37.972762837 -0700
+@@ -37,6 +37,28 @@
+ #include "i915_trace.h"
+ #include "intel_drv.h"
+ 
++/**
++ * DOC: interrupt handling
++ *
++ * These functions provide the basic support for enabling and disabling the
++ * interrupt handling support. There's a lot more functionality in i915_irq.c
++ * and related files, but that will be described in separate chapters.
++ */
++
++#define __raw_i915_read8(dev_priv__, reg__) readb((dev_priv__)->regs + (reg__))
++#define __raw_i915_write8(dev_priv__, reg__, val__) writeb(val__, (dev_priv__)->regs + (reg__))
++
++#define __raw_i915_read16(dev_priv__, reg__) readw((dev_priv__)->regs + (reg__))
++#define __raw_i915_write16(dev_priv__, reg__, val__) writew(val__, (dev_priv__)->regs + (reg__))
++
++#define __raw_i915_read32(dev_priv__, reg__) readl((dev_priv__)->regs + (reg__))
++#define __raw_i915_write32(dev_priv__, reg__, val__) writel(val__, (dev_priv__)->regs + (reg__))
++
++#define __raw_i915_read64(dev_priv__, reg__) readq((dev_priv__)->regs + (reg__))
++#define __raw_i915_write64(dev_priv__, reg__, val__) writeq(val__, (dev_priv__)->regs + (reg__))
++
++#define __raw_i915_posting_read(dev_priv__, reg__) (void)__raw_i915_read32(dev_priv__, reg__)
++
+ static const u32 hpd_ibx[] = {
+ 	[HPD_CRT] = SDE_CRT_HOTPLUG,
+ 	[HPD_SDVO_B] = SDE_SDVOB_HOTPLUG,
+@@ -118,20 +140,22 @@
+ 
+ #define GEN8_IRQ_INIT_NDX(type, which, imr_val, ier_val) do { \
+ 	GEN5_ASSERT_IIR_IS_ZERO(GEN8_##type##_IIR(which)); \
+-	I915_WRITE(GEN8_##type##_IMR(which), (imr_val)); \
+ 	I915_WRITE(GEN8_##type##_IER(which), (ier_val)); \
+-	POSTING_READ(GEN8_##type##_IER(which)); \
++	I915_WRITE(GEN8_##type##_IMR(which), (imr_val)); \
++	POSTING_READ(GEN8_##type##_IMR(which)); \
+ } while (0)
+ 
+ #define GEN5_IRQ_INIT(type, imr_val, ier_val) do { \
+ 	GEN5_ASSERT_IIR_IS_ZERO(type##IIR); \
+-	I915_WRITE(type##IMR, (imr_val)); \
+ 	I915_WRITE(type##IER, (ier_val)); \
+-	POSTING_READ(type##IER); \
++	I915_WRITE(type##IMR, (imr_val)); \
++	POSTING_READ(type##IMR); \
+ } while (0)
+ 
++static void gen6_rps_irq_handler(struct drm_i915_private *dev_priv, u32 pm_iir);
++
+ /* For display hotplug interrupt */
+-static void
++void
+ ironlake_enable_display_irq(struct drm_i915_private *dev_priv, u32 mask)
+ {
+ 	assert_spin_locked(&dev_priv->irq_lock);
+@@ -146,12 +170,12 @@
+ 	}
+ }
+ 
+-static void
++void
+ ironlake_disable_display_irq(struct drm_i915_private *dev_priv, u32 mask)
+ {
+ 	assert_spin_locked(&dev_priv->irq_lock);
+ 
+-	if (!intel_irqs_enabled(dev_priv))
++	if (WARN_ON(!intel_irqs_enabled(dev_priv)))
+ 		return;
+ 
+ 	if ((dev_priv->irq_mask & mask) != mask) {
+@@ -192,71 +216,28 @@
+ 	ilk_update_gt_irq(dev_priv, mask, 0);
+ }
+ 
+-/**
+-  * snb_update_pm_irq - update GEN6_PMIMR
+-  * @dev_priv: driver private
+-  * @interrupt_mask: mask of interrupt bits to update
+-  * @enabled_irq_mask: mask of interrupt bits to enable
+-  */
+-static void snb_update_pm_irq(struct drm_i915_private *dev_priv,
+-			      uint32_t interrupt_mask,
+-			      uint32_t enabled_irq_mask)
++static u32 gen6_pm_iir(struct drm_i915_private *dev_priv)
+ {
+-	uint32_t new_val;
+-
+-	assert_spin_locked(&dev_priv->irq_lock);
+-
+-	if (WARN_ON(!intel_irqs_enabled(dev_priv)))
+-		return;
+-
+-	new_val = dev_priv->pm_irq_mask;
+-	new_val &= ~interrupt_mask;
+-	new_val |= (~enabled_irq_mask & interrupt_mask);
+-
+-	if (new_val != dev_priv->pm_irq_mask) {
+-		dev_priv->pm_irq_mask = new_val;
+-		I915_WRITE(GEN6_PMIMR, dev_priv->pm_irq_mask);
+-		POSTING_READ(GEN6_PMIMR);
+-	}
+-}
+-
+-void gen6_enable_pm_irq(struct drm_i915_private *dev_priv, uint32_t mask)
+-{
+-	snb_update_pm_irq(dev_priv, mask, mask);
++	return INTEL_INFO(dev_priv)->gen >= 8 ? GEN8_GT_IIR(2) : GEN6_PMIIR;
+ }
+ 
+-void gen6_disable_pm_irq(struct drm_i915_private *dev_priv, uint32_t mask)
++static u32 gen6_pm_imr(struct drm_i915_private *dev_priv)
+ {
+-	snb_update_pm_irq(dev_priv, mask, 0);
++	return INTEL_INFO(dev_priv)->gen >= 8 ? GEN8_GT_IMR(2) : GEN6_PMIMR;
+ }
+ 
+-static bool ivb_can_enable_err_int(struct drm_device *dev)
++static u32 gen6_pm_ier(struct drm_i915_private *dev_priv)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_crtc *crtc;
+-	enum pipe pipe;
+-
+-	assert_spin_locked(&dev_priv->irq_lock);
+-
+-	for_each_pipe(pipe) {
+-		crtc = to_intel_crtc(dev_priv->pipe_to_crtc_mapping[pipe]);
+-
+-		if (crtc->cpu_fifo_underrun_disabled)
+-			return false;
+-	}
+-
+-	return true;
++	return INTEL_INFO(dev_priv)->gen >= 8 ? GEN8_GT_IER(2) : GEN6_PMIER;
+ }
+ 
+ /**
+-  * bdw_update_pm_irq - update GT interrupt 2
++  * snb_update_pm_irq - update GEN6_PMIMR
+   * @dev_priv: driver private
+   * @interrupt_mask: mask of interrupt bits to update
+   * @enabled_irq_mask: mask of interrupt bits to enable
+-  *
+-  * Copied from the snb function, updated with relevant register offsets
+   */
+-static void bdw_update_pm_irq(struct drm_i915_private *dev_priv,
++static void snb_update_pm_irq(struct drm_i915_private *dev_priv,
+ 			      uint32_t interrupt_mask,
+ 			      uint32_t enabled_irq_mask)
+ {
+@@ -273,135 +254,50 @@
+ 
+ 	if (new_val != dev_priv->pm_irq_mask) {
+ 		dev_priv->pm_irq_mask = new_val;
+-		I915_WRITE(GEN8_GT_IMR(2), dev_priv->pm_irq_mask);
+-		POSTING_READ(GEN8_GT_IMR(2));
+-	}
+-}
+-
+-void gen8_enable_pm_irq(struct drm_i915_private *dev_priv, uint32_t mask)
+-{
+-	bdw_update_pm_irq(dev_priv, mask, mask);
+-}
+-
+-void gen8_disable_pm_irq(struct drm_i915_private *dev_priv, uint32_t mask)
+-{
+-	bdw_update_pm_irq(dev_priv, mask, 0);
+-}
+-
+-static bool cpt_can_enable_serr_int(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	enum pipe pipe;
+-	struct intel_crtc *crtc;
+-
+-	assert_spin_locked(&dev_priv->irq_lock);
+-
+-	for_each_pipe(pipe) {
+-		crtc = to_intel_crtc(dev_priv->pipe_to_crtc_mapping[pipe]);
+-
+-		if (crtc->pch_fifo_underrun_disabled)
+-			return false;
++		__raw_i915_write32(dev_priv, gen6_pm_imr(dev_priv), dev_priv->pm_irq_mask);
++		__raw_i915_posting_read(dev_priv, gen6_pm_imr(dev_priv));
+ 	}
+-
+-	return true;
+ }
+ 
+-void i9xx_check_fifo_underruns(struct drm_device *dev)
++void gen6_enable_pm_irq(struct drm_i915_private *dev_priv, uint32_t mask)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_crtc *crtc;
+-	unsigned long flags;
+-
+-	spin_lock_irqsave(&dev_priv->irq_lock, flags);
+-
+-	for_each_intel_crtc(dev, crtc) {
+-		u32 reg = PIPESTAT(crtc->pipe);
+-		u32 pipestat;
+-
+-		if (crtc->cpu_fifo_underrun_disabled)
+-			continue;
+-
+-		pipestat = I915_READ(reg) & 0xffff0000;
+-		if ((pipestat & PIPE_FIFO_UNDERRUN_STATUS) == 0)
+-			continue;
+-
+-		I915_WRITE(reg, pipestat | PIPE_FIFO_UNDERRUN_STATUS);
+-		POSTING_READ(reg);
+-
+-		DRM_ERROR("pipe %c underrun\n", pipe_name(crtc->pipe));
+-	}
+-
+-	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
++	snb_update_pm_irq(dev_priv, mask, mask);
+ }
+ 
+-static void i9xx_set_fifo_underrun_reporting(struct drm_device *dev,
+-					     enum pipe pipe,
+-					     bool enable, bool old)
++void gen6_disable_pm_irq(struct drm_i915_private *dev_priv, uint32_t mask)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	u32 reg = PIPESTAT(pipe);
+-	u32 pipestat = I915_READ(reg) & 0xffff0000;
+-
+-	assert_spin_locked(&dev_priv->irq_lock);
+-
+-	if (enable) {
+-		I915_WRITE(reg, pipestat | PIPE_FIFO_UNDERRUN_STATUS);
+-		POSTING_READ(reg);
+-	} else {
+-		if (old && pipestat & PIPE_FIFO_UNDERRUN_STATUS)
+-			DRM_ERROR("pipe %c underrun\n", pipe_name(pipe));
+-	}
++	snb_update_pm_irq(dev_priv, mask, 0);
+ }
+ 
+-static void ironlake_set_fifo_underrun_reporting(struct drm_device *dev,
+-						 enum pipe pipe, bool enable)
++void gen6_enable_rps_interrupts(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	uint32_t bit = (pipe == PIPE_A) ? DE_PIPEA_FIFO_UNDERRUN :
+-					  DE_PIPEB_FIFO_UNDERRUN;
+ 
+-	if (enable)
+-		ironlake_enable_display_irq(dev_priv, bit);
+-	else
+-		ironlake_disable_display_irq(dev_priv, bit);
++	spin_lock_irq(&dev_priv->irq_lock);
++	WARN_ON(dev_priv->rps.pm_iir);
++	gen6_enable_pm_irq(dev_priv, dev_priv->rps.pm_events);
++	I915_WRITE(gen6_pm_iir(dev_priv), dev_priv->rps.pm_events);
++	spin_unlock_irq(&dev_priv->irq_lock);
+ }
+ 
+-static void ivybridge_set_fifo_underrun_reporting(struct drm_device *dev,
+-						  enum pipe pipe,
+-						  bool enable, bool old)
++void gen6_disable_rps_interrupts(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	if (enable) {
+-		I915_WRITE(GEN7_ERR_INT, ERR_INT_FIFO_UNDERRUN(pipe));
+ 
+-		if (!ivb_can_enable_err_int(dev))
+-			return;
++	I915_WRITE(GEN6_PMINTRMSK, INTEL_INFO(dev_priv)->gen >= 8 ?
++		   ~GEN8_PMINTR_REDIRECT_TO_NON_DISP : ~0);
++	I915_WRITE(gen6_pm_ier(dev_priv), I915_READ(gen6_pm_ier(dev_priv)) &
++				~dev_priv->rps.pm_events);
++	/* Complete PM interrupt masking here doesn't race with the rps work
++	 * item again unmasking PM interrupts because that is using a different
++	 * register (PMIMR) to mask PM interrupts. The only risk is in leaving
++	 * stale bits in PMIIR and PMIMR which gen6_enable_rps will clean up. */
+ 
+-		ironlake_enable_display_irq(dev_priv, DE_ERR_INT_IVB);
+-	} else {
+-		ironlake_disable_display_irq(dev_priv, DE_ERR_INT_IVB);
++	spin_lock_irq(&dev_priv->irq_lock);
++	dev_priv->rps.pm_iir = 0;
++	spin_unlock_irq(&dev_priv->irq_lock);
+ 
+-		if (old &&
+-		    I915_READ(GEN7_ERR_INT) & ERR_INT_FIFO_UNDERRUN(pipe)) {
+-			DRM_ERROR("uncleared fifo underrun on pipe %c\n",
+-				  pipe_name(pipe));
+-		}
+-	}
+-}
+-
+-static void broadwell_set_fifo_underrun_reporting(struct drm_device *dev,
+-						  enum pipe pipe, bool enable)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-
+-	assert_spin_locked(&dev_priv->irq_lock);
+-
+-	if (enable)
+-		dev_priv->de_irq_mask[pipe] &= ~GEN8_PIPE_FIFO_UNDERRUN;
+-	else
+-		dev_priv->de_irq_mask[pipe] |= GEN8_PIPE_FIFO_UNDERRUN;
+-	I915_WRITE(GEN8_DE_PIPE_IMR(pipe), dev_priv->de_irq_mask[pipe]);
+-	POSTING_READ(GEN8_DE_PIPE_IMR(pipe));
++	I915_WRITE(gen6_pm_iir(dev_priv), dev_priv->rps.pm_events);
+ }
+ 
+ /**
+@@ -410,9 +306,9 @@
+  * @interrupt_mask: mask of interrupt bits to update
+  * @enabled_irq_mask: mask of interrupt bits to enable
+  */
+-static void ibx_display_interrupt_update(struct drm_i915_private *dev_priv,
+-					 uint32_t interrupt_mask,
+-					 uint32_t enabled_irq_mask)
++void ibx_display_interrupt_update(struct drm_i915_private *dev_priv,
++				  uint32_t interrupt_mask,
++				  uint32_t enabled_irq_mask)
+ {
+ 	uint32_t sdeimr = I915_READ(SDEIMR);
+ 	sdeimr &= ~interrupt_mask;
+@@ -423,164 +319,9 @@
+ 	if (WARN_ON(!intel_irqs_enabled(dev_priv)))
+ 		return;
+ 
+-	I915_WRITE(SDEIMR, sdeimr);
+-	POSTING_READ(SDEIMR);
+-}
+-#define ibx_enable_display_interrupt(dev_priv, bits) \
+-	ibx_display_interrupt_update((dev_priv), (bits), (bits))
+-#define ibx_disable_display_interrupt(dev_priv, bits) \
+-	ibx_display_interrupt_update((dev_priv), (bits), 0)
+-
+-static void ibx_set_fifo_underrun_reporting(struct drm_device *dev,
+-					    enum transcoder pch_transcoder,
+-					    bool enable)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	uint32_t bit = (pch_transcoder == TRANSCODER_A) ?
+-		       SDE_TRANSA_FIFO_UNDER : SDE_TRANSB_FIFO_UNDER;
+-
+-	if (enable)
+-		ibx_enable_display_interrupt(dev_priv, bit);
+-	else
+-		ibx_disable_display_interrupt(dev_priv, bit);
+-}
+-
+-static void cpt_set_fifo_underrun_reporting(struct drm_device *dev,
+-					    enum transcoder pch_transcoder,
+-					    bool enable, bool old)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-
+-	if (enable) {
+-		I915_WRITE(SERR_INT,
+-			   SERR_INT_TRANS_FIFO_UNDERRUN(pch_transcoder));
+-
+-		if (!cpt_can_enable_serr_int(dev))
+-			return;
+-
+-		ibx_enable_display_interrupt(dev_priv, SDE_ERROR_CPT);
+-	} else {
+-		ibx_disable_display_interrupt(dev_priv, SDE_ERROR_CPT);
+-
+-		if (old && I915_READ(SERR_INT) &
+-		    SERR_INT_TRANS_FIFO_UNDERRUN(pch_transcoder)) {
+-			DRM_ERROR("uncleared pch fifo underrun on pch transcoder %c\n",
+-				  transcoder_name(pch_transcoder));
+-		}
+-	}
+-}
+-
+-/**
+- * intel_set_cpu_fifo_underrun_reporting - enable/disable FIFO underrun messages
+- * @dev: drm device
+- * @pipe: pipe
+- * @enable: true if we want to report FIFO underrun errors, false otherwise
+- *
+- * This function makes us disable or enable CPU fifo underruns for a specific
+- * pipe. Notice that on some Gens (e.g. IVB, HSW), disabling FIFO underrun
+- * reporting for one pipe may also disable all the other CPU error interruts for
+- * the other pipes, due to the fact that there's just one interrupt mask/enable
+- * bit for all the pipes.
+- *
+- * Returns the previous state of underrun reporting.
+- */
+-static bool __intel_set_cpu_fifo_underrun_reporting(struct drm_device *dev,
+-						    enum pipe pipe, bool enable)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct drm_crtc *crtc = dev_priv->pipe_to_crtc_mapping[pipe];
+-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+-	bool old;
+-
+-	assert_spin_locked(&dev_priv->irq_lock);
+-
+-	old = !intel_crtc->cpu_fifo_underrun_disabled;
+-	intel_crtc->cpu_fifo_underrun_disabled = !enable;
+-
+-	if (INTEL_INFO(dev)->gen < 5 || IS_VALLEYVIEW(dev))
+-		i9xx_set_fifo_underrun_reporting(dev, pipe, enable, old);
+-	else if (IS_GEN5(dev) || IS_GEN6(dev))
+-		ironlake_set_fifo_underrun_reporting(dev, pipe, enable);
+-	else if (IS_GEN7(dev))
+-		ivybridge_set_fifo_underrun_reporting(dev, pipe, enable, old);
+-	else if (IS_GEN8(dev))
+-		broadwell_set_fifo_underrun_reporting(dev, pipe, enable);
+-
+-	return old;
+-}
+-
+-bool intel_set_cpu_fifo_underrun_reporting(struct drm_device *dev,
+-					   enum pipe pipe, bool enable)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	unsigned long flags;
+-	bool ret;
+-
+-	spin_lock_irqsave(&dev_priv->irq_lock, flags);
+-	ret = __intel_set_cpu_fifo_underrun_reporting(dev, pipe, enable);
+-	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
+-
+-	return ret;
++	__raw_i915_write32(dev_priv, SDEIMR, sdeimr);
+ }
+ 
+-static bool __cpu_fifo_underrun_reporting_enabled(struct drm_device *dev,
+-						  enum pipe pipe)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct drm_crtc *crtc = dev_priv->pipe_to_crtc_mapping[pipe];
+-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+-
+-	return !intel_crtc->cpu_fifo_underrun_disabled;
+-}
+-
+-/**
+- * intel_set_pch_fifo_underrun_reporting - enable/disable FIFO underrun messages
+- * @dev: drm device
+- * @pch_transcoder: the PCH transcoder (same as pipe on IVB and older)
+- * @enable: true if we want to report FIFO underrun errors, false otherwise
+- *
+- * This function makes us disable or enable PCH fifo underruns for a specific
+- * PCH transcoder. Notice that on some PCHs (e.g. CPT/PPT), disabling FIFO
+- * underrun reporting for one transcoder may also disable all the other PCH
+- * error interruts for the other transcoders, due to the fact that there's just
+- * one interrupt mask/enable bit for all the transcoders.
+- *
+- * Returns the previous state of underrun reporting.
+- */
+-bool intel_set_pch_fifo_underrun_reporting(struct drm_device *dev,
+-					   enum transcoder pch_transcoder,
+-					   bool enable)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct drm_crtc *crtc = dev_priv->pipe_to_crtc_mapping[pch_transcoder];
+-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+-	unsigned long flags;
+-	bool old;
+-
+-	/*
+-	 * NOTE: Pre-LPT has a fixed cpu pipe -> pch transcoder mapping, but LPT
+-	 * has only one pch transcoder A that all pipes can use. To avoid racy
+-	 * pch transcoder -> pipe lookups from interrupt code simply store the
+-	 * underrun statistics in crtc A. Since we never expose this anywhere
+-	 * nor use it outside of the fifo underrun code here using the "wrong"
+-	 * crtc on LPT won't cause issues.
+-	 */
+-
+-	spin_lock_irqsave(&dev_priv->irq_lock, flags);
+-
+-	old = !intel_crtc->pch_fifo_underrun_disabled;
+-	intel_crtc->pch_fifo_underrun_disabled = !enable;
+-
+-	if (HAS_PCH_IBX(dev))
+-		ibx_set_fifo_underrun_reporting(dev, pch_transcoder, enable);
+-	else
+-		cpt_set_fifo_underrun_reporting(dev, pch_transcoder, enable, old);
+-
+-	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
+-	return old;
+-}
+-
+-
+ static void
+ __i915_enable_pipestat(struct drm_i915_private *dev_priv, enum pipe pipe,
+ 		       u32 enable_mask, u32 status_mask)
+@@ -589,6 +330,7 @@
+ 	u32 pipestat = I915_READ(reg) & PIPESTAT_INT_ENABLE_MASK;
+ 
+ 	assert_spin_locked(&dev_priv->irq_lock);
++	WARN_ON(!intel_irqs_enabled(dev_priv));
+ 
+ 	if (WARN_ONCE(enable_mask & ~PIPESTAT_INT_ENABLE_MASK ||
+ 		      status_mask & ~PIPESTAT_INT_STATUS_MASK,
+@@ -615,6 +357,7 @@
+ 	u32 pipestat = I915_READ(reg) & PIPESTAT_INT_ENABLE_MASK;
+ 
+ 	assert_spin_locked(&dev_priv->irq_lock);
++	WARN_ON(!intel_irqs_enabled(dev_priv));
+ 
+ 	if (WARN_ONCE(enable_mask & ~PIPESTAT_INT_ENABLE_MASK ||
+ 		      status_mask & ~PIPESTAT_INT_STATUS_MASK,
+@@ -694,19 +437,18 @@
+ static void i915_enable_asle_pipestat(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	unsigned long irqflags;
+ 
+ 	if (!dev_priv->opregion.asle || !IS_MOBILE(dev))
+ 		return;
+ 
+-	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
++	spin_lock_irq(&dev_priv->irq_lock);
+ 
+ 	i915_enable_pipestat(dev_priv, PIPE_B, PIPE_LEGACY_BLC_EVENT_STATUS);
+ 	if (INTEL_INFO(dev)->gen >= 4)
+ 		i915_enable_pipestat(dev_priv, PIPE_A,
+ 				     PIPE_LEGACY_BLC_EVENT_STATUS);
+ 
+-	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
++	spin_unlock_irq(&dev_priv->irq_lock);
+ }
+ 
+ /**
+@@ -1020,7 +762,7 @@
+ 
+ 	/* In vblank? */
+ 	if (in_vbl)
+-		ret |= DRM_SCANOUTPOS_INVBL;
++		ret |= DRM_SCANOUTPOS_IN_VBLANK;
+ 
+ 	return ret;
+ }
+@@ -1094,46 +836,42 @@
+ {
+ 	struct drm_i915_private *dev_priv =
+ 		container_of(work, struct drm_i915_private, dig_port_work);
+-	unsigned long irqflags;
+-	u32 long_port_mask, short_port_mask;
+-	struct intel_digital_port *intel_dig_port;
+-	int i, ret;
+-	u32 old_bits = 0;
++	u32 long_mask, valid_mask, old_bits = 0;
++	int i;
+ 
+-	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
+-	long_port_mask = dev_priv->long_hpd_port_mask;
++	spin_lock_irq(&dev_priv->irq_lock);
++
++	long_mask = dev_priv->long_hpd_port_mask;
+ 	dev_priv->long_hpd_port_mask = 0;
+-	short_port_mask = dev_priv->short_hpd_port_mask;
++
++	valid_mask = dev_priv->short_hpd_port_mask;
+ 	dev_priv->short_hpd_port_mask = 0;
+-	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
++
++	valid_mask |= long_mask;
++	valid_mask &= ~dev_priv->hpd_event_bits;
++	spin_unlock_irq(&dev_priv->irq_lock);
+ 
+ 	for (i = 0; i < I915_MAX_PORTS; i++) {
+-		bool valid = false;
+-		bool long_hpd = false;
+-		intel_dig_port = dev_priv->hpd_irq_port[i];
+-		if (!intel_dig_port || !intel_dig_port->hpd_pulse)
++		struct intel_digital_port *port;
++
++		port = dev_priv->hpd_irq_port[i];
++		if (!port || !port->hpd_pulse)
+ 			continue;
+ 
+-		if (long_port_mask & (1 << i))  {
+-			valid = true;
+-			long_hpd = true;
+-		} else if (short_port_mask & (1 << i))
+-			valid = true;
+-
+-		if (valid) {
+-			ret = intel_dig_port->hpd_pulse(intel_dig_port, long_hpd);
+-			if (ret == true) {
+-				/* if we get true fallback to old school hpd */
+-				old_bits |= (1 << intel_dig_port->base.hpd_pin);
+-			}
+-		}
++		if (valid_mask & (1 << i) &&
++		    port->hpd_pulse(port, long_mask & (1 << i)))
++			/* unhandled, fallback to old school hpd */
++			old_bits |= 1 << port->base.hpd_pin;
+ 	}
+ 
+ 	if (old_bits) {
+-		spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
++		spin_lock_irq(&dev_priv->irq_lock);
+ 		dev_priv->hpd_event_bits |= old_bits;
+-		spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
+-		schedule_work(&dev_priv->hotplug_work);
++		spin_unlock_irq(&dev_priv->irq_lock);
++
++		mod_delayed_work(system_wq,
++				 &dev_priv->hotplug_work,
++				 msecs_to_jiffies(50));
+ 	}
+ }
+ 
+@@ -1145,13 +883,12 @@
+ static void i915_hotplug_work_func(struct work_struct *work)
+ {
+ 	struct drm_i915_private *dev_priv =
+-		container_of(work, struct drm_i915_private, hotplug_work);
++		container_of(work, struct drm_i915_private, hotplug_work.work);
+ 	struct drm_device *dev = dev_priv->dev;
+ 	struct drm_mode_config *mode_config = &dev->mode_config;
+ 	struct intel_connector *intel_connector;
+ 	struct intel_encoder *intel_encoder;
+ 	struct drm_connector *connector;
+-	unsigned long irqflags;
+ 	bool hpd_disabled = false;
+ 	bool changed = false;
+ 	u32 hpd_event_bits;
+@@ -1159,7 +896,7 @@
+ 	mutex_lock(&mode_config->mutex);
+ 	DRM_DEBUG_KMS("running encoder hotplug functions\n");
+ 
+-	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
++	spin_lock_irq(&dev_priv->irq_lock);
+ 
+ 	hpd_event_bits = dev_priv->hpd_event_bits;
+ 	dev_priv->hpd_event_bits = 0;
+@@ -1193,7 +930,7 @@
+ 				 msecs_to_jiffies(I915_REENABLE_HOTPLUG_DELAY));
+ 	}
+ 
+-	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
++	spin_unlock_irq(&dev_priv->irq_lock);
+ 
+ 	list_for_each_entry(connector, &mode_config->connector_list, head) {
+ 		intel_connector = to_intel_connector(connector);
+@@ -1253,143 +990,72 @@
+ }
+ 
+ static void notify_ring(struct drm_device *dev,
+-			struct intel_engine_cs *ring)
++			struct intel_engine_cs *engine)
+ {
+-	if (!intel_ring_initialized(ring))
++	if (!intel_engine_initialized(engine))
+ 		return;
+ 
+-	trace_i915_gem_request_complete(ring);
+-
+-	if (drm_core_check_feature(dev, DRIVER_MODESET))
+-		intel_notify_mmio_flip(ring);
++	trace_i915_gem_ring_complete(engine);
++	atomic_inc(&engine->interrupts);
+ 
+-	wake_up_all(&ring->irq_queue);
+-	i915_queue_hangcheck(dev);
++	wake_up_all(&engine->irq_queue);
+ }
+ 
+-static u32 vlv_c0_residency(struct drm_i915_private *dev_priv,
+-			    struct intel_rps_ei *rps_ei)
++static void vlv_c0_read(struct drm_i915_private *dev_priv,
++			struct intel_rps_ei *ei)
+ {
+-	u32 cz_ts, cz_freq_khz;
+-	u32 render_count, media_count;
+-	u32 elapsed_render, elapsed_media, elapsed_time;
+-	u32 residency = 0;
+-
+-	cz_ts = vlv_punit_read(dev_priv, PUNIT_REG_CZ_TIMESTAMP);
+-	cz_freq_khz = DIV_ROUND_CLOSEST(dev_priv->mem_freq * 1000, 4);
+-
+-	render_count = I915_READ(VLV_RENDER_C0_COUNT_REG);
+-	media_count = I915_READ(VLV_MEDIA_C0_COUNT_REG);
+-
+-	if (rps_ei->cz_clock == 0) {
+-		rps_ei->cz_clock = cz_ts;
+-		rps_ei->render_c0 = render_count;
+-		rps_ei->media_c0 = media_count;
+-
+-		return dev_priv->rps.cur_freq;
+-	}
+-
+-	elapsed_time = cz_ts - rps_ei->cz_clock;
+-	rps_ei->cz_clock = cz_ts;
+-
+-	elapsed_render = render_count - rps_ei->render_c0;
+-	rps_ei->render_c0 = render_count;
+-
+-	elapsed_media = media_count - rps_ei->media_c0;
+-	rps_ei->media_c0 = media_count;
+-
+-	/* Convert all the counters into common unit of milli sec */
+-	elapsed_time /= VLV_CZ_CLOCK_TO_MILLI_SEC;
+-	elapsed_render /=  cz_freq_khz;
+-	elapsed_media /= cz_freq_khz;
+-
+-	/*
+-	 * Calculate overall C0 residency percentage
+-	 * only if elapsed time is non zero
+-	 */
+-	if (elapsed_time) {
+-		residency =
+-			((max(elapsed_render, elapsed_media) * 100)
+-				/ elapsed_time);
+-	}
+-
+-	return residency;
++	ei->cz_clock = vlv_punit_read(dev_priv, PUNIT_REG_CZ_TIMESTAMP);
++	ei->render_c0 = I915_READ(VLV_RENDER_C0_COUNT);
++	ei->media_c0 = I915_READ(VLV_MEDIA_C0_COUNT);
+ }
+ 
+-/**
+- * vlv_calc_delay_from_C0_counters - Increase/Decrease freq based on GPU
+- * busy-ness calculated from C0 counters of render & media power wells
+- * @dev_priv: DRM device private
+- *
+- */
+-static u32 vlv_calc_delay_from_C0_counters(struct drm_i915_private *dev_priv)
++static bool vlv_c0_above(struct drm_i915_private *dev_priv,
++			 const struct intel_rps_ei *old,
++			 const struct intel_rps_ei *now,
++			 int threshold)
+ {
+-	u32 residency_C0_up = 0, residency_C0_down = 0;
+-	u8 new_delay, adj;
+-
+-	dev_priv->rps.ei_interrupt_count++;
+-
+-	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
++	u64 time = now->cz_clock - old->cz_clock;
++	u64 c0 = max(now->render_c0 - old->render_c0,
++		     now->media_c0 - old->media_c0);
+ 
++	c0 *= 100 * VLV_CZ_CLOCK_TO_MILLI_SEC * 4 / 1000;
++	time *= threshold * dev_priv->mem_freq;
++	return c0 >= time;
++}
+ 
+-	if (dev_priv->rps.up_ei.cz_clock == 0) {
+-		vlv_c0_residency(dev_priv, &dev_priv->rps.up_ei);
+-		vlv_c0_residency(dev_priv, &dev_priv->rps.down_ei);
+-		return dev_priv->rps.cur_freq;
+-	}
++void gen6_rps_reset_ei(struct drm_i915_private *dev_priv)
++{
++	vlv_c0_read(dev_priv, &dev_priv->rps.down_ei);
++	dev_priv->rps.up_ei = dev_priv->rps.down_ei;
++}
+ 
++static u32 vlv_wa_c0_ei(struct drm_i915_private *dev_priv, u32 pm_iir)
++{
++	struct intel_rps_ei now;
++	u32 events = 0;
+ 
+-	/*
+-	 * To down throttle, C0 residency should be less than down threshold
+-	 * for continous EI intervals. So calculate down EI counters
+-	 * once in VLV_INT_COUNT_FOR_DOWN_EI
+-	 */
+-	if (dev_priv->rps.ei_interrupt_count == VLV_INT_COUNT_FOR_DOWN_EI) {
++	if ((pm_iir & (GEN6_PM_RP_DOWN_EI_EXPIRED | GEN6_PM_RP_UP_EI_EXPIRED)) == 0)
++		return 0;
+ 
+-		dev_priv->rps.ei_interrupt_count = 0;
++	vlv_c0_read(dev_priv, &now);
+ 
+-		residency_C0_down = vlv_c0_residency(dev_priv,
+-						     &dev_priv->rps.down_ei);
+-	} else {
+-		residency_C0_up = vlv_c0_residency(dev_priv,
+-						   &dev_priv->rps.up_ei);
++	if (pm_iir & GEN6_PM_RP_DOWN_EI_EXPIRED) {
++		if (!vlv_c0_above(dev_priv,
++				  &dev_priv->rps.down_ei, &now,
++				  dev_priv->rps.down_threshold))
++			events |= GEN6_PM_RP_DOWN_THRESHOLD;
++		dev_priv->rps.down_ei = now;
+ 	}
+ 
+-	new_delay = dev_priv->rps.cur_freq;
+-
+-	adj = dev_priv->rps.last_adj;
+-	/* C0 residency is greater than UP threshold. Increase Frequency */
+-	if (residency_C0_up >= VLV_RP_UP_EI_THRESHOLD) {
+-		if (adj > 0)
+-			adj *= 2;
+-		else
+-			adj = 1;
+-
+-		if (dev_priv->rps.cur_freq < dev_priv->rps.max_freq_softlimit)
+-			new_delay = dev_priv->rps.cur_freq + adj;
+-
+-		/*
+-		 * For better performance, jump directly
+-		 * to RPe if we're below it.
+-		 */
+-		if (new_delay < dev_priv->rps.efficient_freq)
+-			new_delay = dev_priv->rps.efficient_freq;
+-
+-	} else if (!dev_priv->rps.ei_interrupt_count &&
+-			(residency_C0_down < VLV_RP_DOWN_EI_THRESHOLD)) {
+-		if (adj < 0)
+-			adj *= 2;
+-		else
+-			adj = -1;
+-		/*
+-		 * This means, C0 residency is less than down threshold over
+-		 * a period of VLV_INT_COUNT_FOR_DOWN_EI. So, reduce the freq
+-		 */
+-		if (dev_priv->rps.cur_freq > dev_priv->rps.min_freq_softlimit)
+-			new_delay = dev_priv->rps.cur_freq + adj;
++	if (pm_iir & GEN6_PM_RP_UP_EI_EXPIRED) {
++		if (vlv_c0_above(dev_priv,
++				 &dev_priv->rps.up_ei, &now,
++				 dev_priv->rps.up_threshold))
++			events |= GEN6_PM_RP_UP_THRESHOLD;
++		dev_priv->rps.up_ei = now;
+ 	}
+ 
+-	return new_delay;
++	return events;
+ }
+ 
+ static void gen6_pm_rps_work(struct work_struct *work)
+@@ -1402,58 +1068,55 @@
+ 	spin_lock_irq(&dev_priv->irq_lock);
+ 	pm_iir = dev_priv->rps.pm_iir;
+ 	dev_priv->rps.pm_iir = 0;
+-	if (INTEL_INFO(dev_priv->dev)->gen >= 8)
+-		gen8_enable_pm_irq(dev_priv, dev_priv->pm_rps_events);
+-	else {
+-		/* Make sure not to corrupt PMIMR state used by ringbuffer */
+-		gen6_enable_pm_irq(dev_priv, dev_priv->pm_rps_events);
+-	}
++	/* Make sure not to corrupt PMIMR state used by ringbuffer on GEN6 */
++	gen6_enable_pm_irq(dev_priv, dev_priv->rps.pm_events);
+ 	spin_unlock_irq(&dev_priv->irq_lock);
+ 
+ 	/* Make sure we didn't queue anything we're not going to process. */
+-	WARN_ON(pm_iir & ~dev_priv->pm_rps_events);
++	WARN_ON(pm_iir & ~dev_priv->rps.pm_events);
+ 
+-	if ((pm_iir & dev_priv->pm_rps_events) == 0)
++	if ((pm_iir & dev_priv->rps.pm_events) == 0)
+ 		return;
+ 
+ 	mutex_lock(&dev_priv->rps.hw_lock);
+ 
++	pm_iir |= vlv_wa_c0_ei(dev_priv, pm_iir);
++
+ 	adj = dev_priv->rps.last_adj;
++	new_delay = dev_priv->rps.cur_freq;
+ 	if (pm_iir & GEN6_PM_RP_UP_THRESHOLD) {
+ 		if (adj > 0)
+ 			adj *= 2;
+-		else {
+-			/* CHV needs even encode values */
+-			adj = IS_CHERRYVIEW(dev_priv->dev) ? 2 : 1;
+-		}
+-		new_delay = dev_priv->rps.cur_freq + adj;
+-
++		else
++			adj = 1;
+ 		/*
+ 		 * For better performance, jump directly
+ 		 * to RPe if we're below it.
+ 		 */
+-		if (new_delay < dev_priv->rps.efficient_freq)
++		if (new_delay < dev_priv->rps.efficient_freq - adj) {
+ 			new_delay = dev_priv->rps.efficient_freq;
++			adj = 0;
++		}
+ 	} else if (pm_iir & GEN6_PM_RP_DOWN_TIMEOUT) {
+ 		if (dev_priv->rps.cur_freq > dev_priv->rps.efficient_freq)
+ 			new_delay = dev_priv->rps.efficient_freq;
+ 		else
+ 			new_delay = dev_priv->rps.min_freq_softlimit;
+ 		adj = 0;
+-	} else if (pm_iir & GEN6_PM_RP_UP_EI_EXPIRED) {
+-		new_delay = vlv_calc_delay_from_C0_counters(dev_priv);
+ 	} else if (pm_iir & GEN6_PM_RP_DOWN_THRESHOLD) {
+ 		if (adj < 0)
+ 			adj *= 2;
+-		else {
+-			/* CHV needs even encode values */
+-			adj = IS_CHERRYVIEW(dev_priv->dev) ? -2 : -1;
+-		}
+-		new_delay = dev_priv->rps.cur_freq + adj;
++		else
++			adj = -1;
+ 	} else { /* unknown event */
+-		new_delay = dev_priv->rps.cur_freq;
++		adj = 0;
+ 	}
+ 
++	/* CHV needs even encode values */
++	if (IS_CHERRYVIEW(dev_priv->dev))
++		adj <<= 1;
++	new_delay += adj;
++
+ 	/* sysfs frequency interfaces may have snuck in while servicing the
+ 	 * interrupt
+ 	 */
+@@ -1488,7 +1151,6 @@
+ 	u32 error_status, row, bank, subbank;
+ 	char *parity_event[6];
+ 	uint32_t misccpctl;
+-	unsigned long flags;
+ 	uint8_t slice = 0;
+ 
+ 	/* We must turn off DOP level clock gating to access the L3 registers.
+@@ -1547,9 +1209,9 @@
+ 
+ out:
+ 	WARN_ON(dev_priv->l3_parity.which_slice);
+-	spin_lock_irqsave(&dev_priv->irq_lock, flags);
++	spin_lock_irq(&dev_priv->irq_lock);
+ 	gen5_enable_gt_irq(dev_priv, GT_PARITY_ERROR(dev_priv->dev));
+-	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
++	spin_unlock_irq(&dev_priv->irq_lock);
+ 
+ 	mutex_unlock(&dev_priv->dev->struct_mutex);
+ }
+@@ -1581,9 +1243,9 @@
+ {
+ 	if (gt_iir &
+ 	    (GT_RENDER_USER_INTERRUPT | GT_RENDER_PIPECTL_NOTIFY_INTERRUPT))
+-		notify_ring(dev, &dev_priv->ring[RCS]);
++		notify_ring(dev, &dev_priv->engine[RCS]);
+ 	if (gt_iir & ILK_BSD_USER_INTERRUPT)
+-		notify_ring(dev, &dev_priv->ring[VCS]);
++		notify_ring(dev, &dev_priv->engine[VCS]);
+ }
+ 
+ static void snb_gt_irq_handler(struct drm_device *dev,
+@@ -1593,40 +1255,28 @@
+ 
+ 	if (gt_iir &
+ 	    (GT_RENDER_USER_INTERRUPT | GT_RENDER_PIPECTL_NOTIFY_INTERRUPT))
+-		notify_ring(dev, &dev_priv->ring[RCS]);
++		notify_ring(dev, &dev_priv->engine[RCS]);
+ 	if (gt_iir & GT_BSD_USER_INTERRUPT)
+-		notify_ring(dev, &dev_priv->ring[VCS]);
++		notify_ring(dev, &dev_priv->engine[VCS]);
+ 	if (gt_iir & GT_BLT_USER_INTERRUPT)
+-		notify_ring(dev, &dev_priv->ring[BCS]);
++		notify_ring(dev, &dev_priv->engine[BCS]);
+ 
+ 	if (gt_iir & (GT_BLT_CS_ERROR_INTERRUPT |
+ 		      GT_BSD_CS_ERROR_INTERRUPT |
+ 		      GT_RENDER_CS_MASTER_ERROR_INTERRUPT)) {
+-		i915_handle_error(dev, false, "GT error interrupt 0x%08x",
+-				  gt_iir);
++		i915_handle_error(dev, 0,
++				  "GT error interrupt 0x%08x", gt_iir);
+ 	}
+ 
+ 	if (gt_iir & GT_PARITY_ERROR(dev))
+ 		ivybridge_parity_error_irq_handler(dev, gt_iir);
+ }
+ 
+-static void gen8_rps_irq_handler(struct drm_i915_private *dev_priv, u32 pm_iir)
+-{
+-	if ((pm_iir & dev_priv->pm_rps_events) == 0)
+-		return;
+-
+-	spin_lock(&dev_priv->irq_lock);
+-	dev_priv->rps.pm_iir |= pm_iir & dev_priv->pm_rps_events;
+-	gen8_disable_pm_irq(dev_priv, pm_iir & dev_priv->pm_rps_events);
+-	spin_unlock(&dev_priv->irq_lock);
+-
+-	queue_work(dev_priv->wq, &dev_priv->rps.work);
+-}
+-
+ static irqreturn_t gen8_gt_irq_handler(struct drm_device *dev,
+ 				       struct drm_i915_private *dev_priv,
+ 				       u32 master_ctl)
+ {
++	struct intel_engine_cs *engine;
+ 	u32 rcs, bcs, vcs;
+ 	uint32_t tmp = 0;
+ 	irqreturn_t ret = IRQ_NONE;
+@@ -1636,12 +1286,20 @@
+ 		if (tmp) {
+ 			I915_WRITE(GEN8_GT_IIR(0), tmp);
+ 			ret = IRQ_HANDLED;
++
+ 			rcs = tmp >> GEN8_RCS_IRQ_SHIFT;
+-			bcs = tmp >> GEN8_BCS_IRQ_SHIFT;
++			engine = &dev_priv->engine[RCS];
+ 			if (rcs & GT_RENDER_USER_INTERRUPT)
+-				notify_ring(dev, &dev_priv->ring[RCS]);
++				notify_ring(dev, engine);
++			if (rcs & GT_CONTEXT_SWITCH_INTERRUPT)
++				intel_execlists_irq_handler(engine);
++
++			bcs = tmp >> GEN8_BCS_IRQ_SHIFT;
++			engine = &dev_priv->engine[BCS];
+ 			if (bcs & GT_RENDER_USER_INTERRUPT)
+-				notify_ring(dev, &dev_priv->ring[BCS]);
++				notify_ring(dev, engine);
++			if (bcs & GT_CONTEXT_SWITCH_INTERRUPT)
++				intel_execlists_irq_handler(engine);
+ 		} else
+ 			DRM_ERROR("The master control interrupt lied (GT0)!\n");
+ 	}
+@@ -1651,23 +1309,31 @@
+ 		if (tmp) {
+ 			I915_WRITE(GEN8_GT_IIR(1), tmp);
+ 			ret = IRQ_HANDLED;
++
+ 			vcs = tmp >> GEN8_VCS1_IRQ_SHIFT;
++			engine = &dev_priv->engine[VCS];
+ 			if (vcs & GT_RENDER_USER_INTERRUPT)
+-				notify_ring(dev, &dev_priv->ring[VCS]);
++				notify_ring(dev, engine);
++			if (vcs & GT_CONTEXT_SWITCH_INTERRUPT)
++				intel_execlists_irq_handler(engine);
++
+ 			vcs = tmp >> GEN8_VCS2_IRQ_SHIFT;
++			engine = &dev_priv->engine[VCS2];
+ 			if (vcs & GT_RENDER_USER_INTERRUPT)
+-				notify_ring(dev, &dev_priv->ring[VCS2]);
++				notify_ring(dev, engine);
++			if (vcs & GT_CONTEXT_SWITCH_INTERRUPT)
++				intel_execlists_irq_handler(engine);
+ 		} else
+ 			DRM_ERROR("The master control interrupt lied (GT1)!\n");
+ 	}
+ 
+ 	if (master_ctl & GEN8_GT_PM_IRQ) {
+ 		tmp = I915_READ(GEN8_GT_IIR(2));
+-		if (tmp & dev_priv->pm_rps_events) {
++		if (tmp & dev_priv->rps.pm_events) {
+ 			I915_WRITE(GEN8_GT_IIR(2),
+-				   tmp & dev_priv->pm_rps_events);
++				   tmp & dev_priv->rps.pm_events);
+ 			ret = IRQ_HANDLED;
+-			gen8_rps_irq_handler(dev_priv, tmp);
++			gen6_rps_irq_handler(dev_priv, tmp);
+ 		} else
+ 			DRM_ERROR("The master control interrupt lied (PM)!\n");
+ 	}
+@@ -1677,9 +1343,13 @@
+ 		if (tmp) {
+ 			I915_WRITE(GEN8_GT_IIR(3), tmp);
+ 			ret = IRQ_HANDLED;
++
+ 			vcs = tmp >> GEN8_VECS_IRQ_SHIFT;
++			engine = &dev_priv->engine[VECS];
+ 			if (vcs & GT_RENDER_USER_INTERRUPT)
+-				notify_ring(dev, &dev_priv->ring[VECS]);
++				notify_ring(dev, engine);
++			if (vcs & GT_CONTEXT_SWITCH_INTERRUPT)
++				intel_execlists_irq_handler(engine);
+ 		} else
+ 			DRM_ERROR("The master control interrupt lied (GT3)!\n");
+ 	}
+@@ -1690,7 +1360,7 @@
+ #define HPD_STORM_DETECT_PERIOD 1000
+ #define HPD_STORM_THRESHOLD 5
+ 
+-static int ilk_port_to_hotplug_shift(enum port port)
++static int pch_port_to_hotplug_shift(enum port port)
+ {
+ 	switch (port) {
+ 	case PORT_A:
+@@ -1706,7 +1376,7 @@
+ 	}
+ }
+ 
+-static int g4x_port_to_hotplug_shift(enum port port)
++static int i915_port_to_hotplug_shift(enum port port)
+ {
+ 	switch (port) {
+ 	case PORT_A:
+@@ -1764,15 +1434,17 @@
+ 		if (port && dev_priv->hpd_irq_port[port]) {
+ 			bool long_hpd;
+ 
+-			if (IS_G4X(dev)) {
+-				dig_shift = g4x_port_to_hotplug_shift(port);
+-				long_hpd = (hotplug_trigger >> dig_shift) & PORTB_HOTPLUG_LONG_DETECT;
+-			} else {
+-				dig_shift = ilk_port_to_hotplug_shift(port);
++			if (HAS_PCH_SPLIT(dev)) {
++				dig_shift = pch_port_to_hotplug_shift(port);
+ 				long_hpd = (dig_hotplug_reg >> dig_shift) & PORTB_HOTPLUG_LONG_DETECT;
++			} else {
++				dig_shift = i915_port_to_hotplug_shift(port);
++				long_hpd = (hotplug_trigger >> dig_shift) & PORTB_HOTPLUG_LONG_DETECT;
+ 			}
+ 
+-			DRM_DEBUG_DRIVER("digital hpd port %d %d\n", port, long_hpd);
++			DRM_DEBUG_DRIVER("digital hpd port %c - %s\n",
++					 port_name(port),
++					 long_hpd ? "long" : "short");
+ 			/* for long HPD pulses we want to have the digital queue happen,
+ 			   but we still want HPD storm detection to function. */
+ 			if (long_hpd) {
+@@ -1843,7 +1515,9 @@
+ 	if (queue_dig)
+ 		queue_work(dev_priv->dp_wq, &dev_priv->dig_port_work);
+ 	if (queue_hp)
+-		schedule_work(&dev_priv->hotplug_work);
++		mod_delayed_work(system_wq,
++				 &dev_priv->hotplug_work,
++				 msecs_to_jiffies(50));
+ }
+ 
+ static void gmbus_irq_handler(struct drm_device *dev)
+@@ -1918,7 +1592,7 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+ 	display_pipe_crc_irq_handler(dev, pipe,
+-				     I915_READ(PIPE_CRC_RES_1_IVB(pipe)),
++				     __raw_i915_read32(dev_priv, PIPE_CRC_RES_1_IVB(pipe)),
+ 				     0, 0, 0, 0);
+ }
+ 
+@@ -1927,11 +1601,11 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+ 	display_pipe_crc_irq_handler(dev, pipe,
+-				     I915_READ(PIPE_CRC_RES_1_IVB(pipe)),
+-				     I915_READ(PIPE_CRC_RES_2_IVB(pipe)),
+-				     I915_READ(PIPE_CRC_RES_3_IVB(pipe)),
+-				     I915_READ(PIPE_CRC_RES_4_IVB(pipe)),
+-				     I915_READ(PIPE_CRC_RES_5_IVB(pipe)));
++				     __raw_i915_read32(dev_priv, PIPE_CRC_RES_1_IVB(pipe)),
++				     __raw_i915_read32(dev_priv, PIPE_CRC_RES_2_IVB(pipe)),
++				     __raw_i915_read32(dev_priv, PIPE_CRC_RES_3_IVB(pipe)),
++				     __raw_i915_read32(dev_priv, PIPE_CRC_RES_4_IVB(pipe)),
++				     __raw_i915_read32(dev_priv, PIPE_CRC_RES_5_IVB(pipe)));
+ }
+ 
+ static void i9xx_pipe_crc_irq_handler(struct drm_device *dev, enum pipe pipe)
+@@ -1961,21 +1635,37 @@
+  * the work queue. */
+ static void gen6_rps_irq_handler(struct drm_i915_private *dev_priv, u32 pm_iir)
+ {
+-	if (pm_iir & dev_priv->pm_rps_events) {
++	/* TODO: RPS on GEN9 is not supported yet. */
++	if (WARN_ONCE(INTEL_INFO(dev_priv)->gen == 9,
++		      "GEN9: unexpected RPS IRQ\n"))
++		return;
++
++	if (pm_iir & dev_priv->rps.pm_events) {
++		u32 events;
++
+ 		spin_lock(&dev_priv->irq_lock);
+-		dev_priv->rps.pm_iir |= pm_iir & dev_priv->pm_rps_events;
+-		gen6_disable_pm_irq(dev_priv, pm_iir & dev_priv->pm_rps_events);
++		events = pm_iir & dev_priv->rps.pm_events;
++		dev_priv->rps.pm_iir |= events;
++		if ((dev_priv->pm_irq_mask & events) != events) {
++			dev_priv->pm_irq_mask |= events;
++			__raw_i915_write32(dev_priv,
++					   GEN6_PMIMR,
++					   dev_priv->pm_irq_mask);
++		}
+ 		spin_unlock(&dev_priv->irq_lock);
+ 
+ 		queue_work(dev_priv->wq, &dev_priv->rps.work);
+ 	}
+ 
++	if (INTEL_INFO(dev_priv)->gen >= 8)
++		return;
++
+ 	if (HAS_VEBOX(dev_priv->dev)) {
+ 		if (pm_iir & PM_VEBOX_USER_INTERRUPT)
+-			notify_ring(dev_priv->dev, &dev_priv->ring[VECS]);
++			notify_ring(dev_priv->dev, &dev_priv->engine[VECS]);
+ 
+ 		if (pm_iir & PM_VEBOX_CS_ERROR_INTERRUPT) {
+-			i915_handle_error(dev_priv->dev, false,
++			i915_handle_error(dev_priv->dev, 0,
+ 					  "VEBOX CS error interrupt 0x%08x",
+ 					  pm_iir);
+ 		}
+@@ -1984,14 +1674,9 @@
+ 
+ static bool intel_pipe_handle_vblank(struct drm_device *dev, enum pipe pipe)
+ {
+-	struct intel_crtc *crtc;
+-
+ 	if (!drm_handle_vblank(dev, pipe))
+ 		return false;
+ 
+-	crtc = to_intel_crtc(intel_get_crtc_for_pipe(dev, pipe));
+-	wake_up(&crtc->vbl_wait);
+-
+ 	return true;
+ }
+ 
+@@ -2002,7 +1687,7 @@
+ 	int pipe;
+ 
+ 	spin_lock(&dev_priv->irq_lock);
+-	for_each_pipe(pipe) {
++	for_each_pipe(dev_priv, pipe) {
+ 		int reg;
+ 		u32 mask, iir_bit = 0;
+ 
+@@ -2013,9 +1698,9 @@
+ 		 * we need to be careful that we only handle what we want to
+ 		 * handle.
+ 		 */
+-		mask = 0;
+-		if (__cpu_fifo_underrun_reporting_enabled(dev, pipe))
+-			mask |= PIPE_FIFO_UNDERRUN_STATUS;
++
++		/* fifo underruns are filterered in the underrun handler. */
++		mask = PIPE_FIFO_UNDERRUN_STATUS;
+ 
+ 		switch (pipe) {
+ 		case PIPE_A:
+@@ -2047,9 +1732,10 @@
+ 	}
+ 	spin_unlock(&dev_priv->irq_lock);
+ 
+-	for_each_pipe(pipe) {
+-		if (pipe_stats[pipe] & PIPE_START_VBLANK_INTERRUPT_STATUS)
+-			intel_pipe_handle_vblank(dev, pipe);
++	for_each_pipe(dev_priv, pipe) {
++		if (pipe_stats[pipe] & PIPE_START_VBLANK_INTERRUPT_STATUS &&
++		    intel_pipe_handle_vblank(dev, pipe))
++			intel_check_page_flip(dev, pipe);
+ 
+ 		if (pipe_stats[pipe] & PLANE_FLIP_DONE_INT_STATUS_VLV) {
+ 			intel_prepare_page_flip(dev, pipe);
+@@ -2059,9 +1745,8 @@
+ 		if (pipe_stats[pipe] & PIPE_CRC_DONE_INTERRUPT_STATUS)
+ 			i9xx_pipe_crc_irq_handler(dev, pipe);
+ 
+-		if (pipe_stats[pipe] & PIPE_FIFO_UNDERRUN_STATUS &&
+-		    intel_set_cpu_fifo_underrun_reporting(dev, pipe, false))
+-			DRM_ERROR("pipe %c underrun\n", pipe_name(pipe));
++		if (pipe_stats[pipe] & PIPE_FIFO_UNDERRUN_STATUS)
++			intel_cpu_fifo_underrun_irq_handler(dev_priv, pipe);
+ 	}
+ 
+ 	if (pipe_stats[0] & PIPE_GMBUS_INTERRUPT_STATUS)
+@@ -2216,7 +1901,7 @@
+ 		DRM_ERROR("PCH poison interrupt\n");
+ 
+ 	if (pch_iir & SDE_FDI_MASK)
+-		for_each_pipe(pipe)
++		for_each_pipe(dev_priv, pipe)
+ 			DRM_DEBUG_DRIVER("  pipe %c FDI IIR: 0x%08x\n",
+ 					 pipe_name(pipe),
+ 					 I915_READ(FDI_RX_IIR(pipe)));
+@@ -2228,14 +1913,10 @@
+ 		DRM_DEBUG_DRIVER("PCH transcoder CRC error interrupt\n");
+ 
+ 	if (pch_iir & SDE_TRANSA_FIFO_UNDER)
+-		if (intel_set_pch_fifo_underrun_reporting(dev, TRANSCODER_A,
+-							  false))
+-			DRM_ERROR("PCH transcoder A FIFO underrun\n");
++		intel_pch_fifo_underrun_irq_handler(dev_priv, TRANSCODER_A);
+ 
+ 	if (pch_iir & SDE_TRANSB_FIFO_UNDER)
+-		if (intel_set_pch_fifo_underrun_reporting(dev, TRANSCODER_B,
+-							  false))
+-			DRM_ERROR("PCH transcoder B FIFO underrun\n");
++		intel_pch_fifo_underrun_irq_handler(dev_priv, TRANSCODER_B);
+ }
+ 
+ static void ivb_err_int_handler(struct drm_device *dev)
+@@ -2247,13 +1928,9 @@
+ 	if (err_int & ERR_INT_POISON)
+ 		DRM_ERROR("Poison interrupt\n");
+ 
+-	for_each_pipe(pipe) {
+-		if (err_int & ERR_INT_FIFO_UNDERRUN(pipe)) {
+-			if (intel_set_cpu_fifo_underrun_reporting(dev, pipe,
+-								  false))
+-				DRM_ERROR("Pipe %c FIFO underrun\n",
+-					  pipe_name(pipe));
+-		}
++	for_each_pipe(dev_priv, pipe) {
++		if (err_int & ERR_INT_FIFO_UNDERRUN(pipe))
++			intel_cpu_fifo_underrun_irq_handler(dev_priv, pipe);
+ 
+ 		if (err_int & ERR_INT_PIPE_CRC_DONE(pipe)) {
+ 			if (IS_IVYBRIDGE(dev))
+@@ -2275,19 +1952,13 @@
+ 		DRM_ERROR("PCH poison interrupt\n");
+ 
+ 	if (serr_int & SERR_INT_TRANS_A_FIFO_UNDERRUN)
+-		if (intel_set_pch_fifo_underrun_reporting(dev, TRANSCODER_A,
+-							  false))
+-			DRM_ERROR("PCH transcoder A FIFO underrun\n");
++		intel_pch_fifo_underrun_irq_handler(dev_priv, TRANSCODER_A);
+ 
+ 	if (serr_int & SERR_INT_TRANS_B_FIFO_UNDERRUN)
+-		if (intel_set_pch_fifo_underrun_reporting(dev, TRANSCODER_B,
+-							  false))
+-			DRM_ERROR("PCH transcoder B FIFO underrun\n");
++		intel_pch_fifo_underrun_irq_handler(dev_priv, TRANSCODER_B);
+ 
+ 	if (serr_int & SERR_INT_TRANS_C_FIFO_UNDERRUN)
+-		if (intel_set_pch_fifo_underrun_reporting(dev, TRANSCODER_C,
+-							  false))
+-			DRM_ERROR("PCH transcoder C FIFO underrun\n");
++		intel_pch_fifo_underrun_irq_handler(dev_priv, TRANSCODER_C);
+ 
+ 	I915_WRITE(SERR_INT, serr_int);
+ }
+@@ -2299,8 +1970,8 @@
+ 	u32 hotplug_trigger = pch_iir & SDE_HOTPLUG_MASK_CPT;
+ 	u32 dig_hotplug_reg;
+ 
+-	dig_hotplug_reg = I915_READ(PCH_PORT_HOTPLUG);
+-	I915_WRITE(PCH_PORT_HOTPLUG, dig_hotplug_reg);
++	dig_hotplug_reg = __raw_i915_read32(dev_priv, PCH_PORT_HOTPLUG);
++	__raw_i915_write32(dev_priv, PCH_PORT_HOTPLUG, dig_hotplug_reg);
+ 
+ 	intel_hpd_irq_handler(dev, hotplug_trigger, dig_hotplug_reg, hpd_cpt);
+ 
+@@ -2324,7 +1995,7 @@
+ 		DRM_DEBUG_DRIVER("Audio CP change interrupt\n");
+ 
+ 	if (pch_iir & SDE_FDI_MASK_CPT)
+-		for_each_pipe(pipe)
++		for_each_pipe(dev_priv, pipe)
+ 			DRM_DEBUG_DRIVER("  pipe %c FDI IIR: 0x%08x\n",
+ 					 pipe_name(pipe),
+ 					 I915_READ(FDI_RX_IIR(pipe)));
+@@ -2347,14 +2018,13 @@
+ 	if (de_iir & DE_POISON)
+ 		DRM_ERROR("Poison interrupt\n");
+ 
+-	for_each_pipe(pipe) {
+-		if (de_iir & DE_PIPE_VBLANK(pipe))
+-			intel_pipe_handle_vblank(dev, pipe);
++	for_each_pipe(dev_priv, pipe) {
++		if (de_iir & DE_PIPE_VBLANK(pipe) &&
++		    intel_pipe_handle_vblank(dev, pipe))
++			intel_check_page_flip(dev, pipe);
+ 
+ 		if (de_iir & DE_PIPE_FIFO_UNDERRUN(pipe))
+-			if (intel_set_cpu_fifo_underrun_reporting(dev, pipe, false))
+-				DRM_ERROR("Pipe %c FIFO underrun\n",
+-					  pipe_name(pipe));
++			intel_cpu_fifo_underrun_irq_handler(dev_priv, pipe);
+ 
+ 		if (de_iir & DE_PIPE_CRC_DONE(pipe))
+ 			i9xx_pipe_crc_irq_handler(dev, pipe);
+@@ -2368,7 +2038,7 @@
+ 
+ 	/* check event from PCH */
+ 	if (de_iir & DE_PCH_EVENT) {
+-		u32 pch_iir = I915_READ(SDEIIR);
++		u32 pch_iir = __raw_i915_read32(dev_priv, SDEIIR);
+ 
+ 		if (HAS_PCH_CPT(dev))
+ 			cpt_irq_handler(dev, pch_iir);
+@@ -2376,7 +2046,7 @@
+ 			ibx_irq_handler(dev, pch_iir);
+ 
+ 		/* should clear PCH hotplug event before clear CPU irq */
+-		I915_WRITE(SDEIIR, pch_iir);
++		__raw_i915_write32(dev_priv, SDEIIR, pch_iir);
+ 	}
+ 
+ 	if (IS_GEN5(dev) && de_iir & DE_PCU_EVENT)
+@@ -2397,9 +2067,10 @@
+ 	if (de_iir & DE_GSE_IVB)
+ 		intel_opregion_asle_intr(dev);
+ 
+-	for_each_pipe(pipe) {
+-		if (de_iir & (DE_PIPE_VBLANK_IVB(pipe)))
+-			intel_pipe_handle_vblank(dev, pipe);
++	for_each_pipe(dev_priv, pipe) {
++		if (de_iir & (DE_PIPE_VBLANK_IVB(pipe)) &&
++		    intel_pipe_handle_vblank(dev, pipe))
++			intel_check_page_flip(dev, pipe);
+ 
+ 		/* plane/pipes map 1:1 on ilk+ */
+ 		if (de_iir & DE_PLANE_FLIP_DONE_IVB(pipe)) {
+@@ -2410,12 +2081,12 @@
+ 
+ 	/* check event from PCH */
+ 	if (!HAS_PCH_NOP(dev) && (de_iir & DE_PCH_EVENT_IVB)) {
+-		u32 pch_iir = I915_READ(SDEIIR);
++		u32 pch_iir = __raw_i915_read32(dev_priv, SDEIIR);
+ 
+ 		cpt_irq_handler(dev, pch_iir);
+ 
+ 		/* clear PCH hotplug event before clear CPU irq */
+-		I915_WRITE(SDEIIR, pch_iir);
++		__raw_i915_write32(dev_priv, SDEIIR, pch_iir);
+ 	}
+ }
+ 
+@@ -2431,7 +2102,7 @@
+ {
+ 	struct drm_device *dev = arg;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	u32 de_iir, gt_iir, de_ier, sde_ier = 0;
++	u32 de_iir, gt_iir;
+ 	irqreturn_t ret = IRQ_NONE;
+ 
+ 	/* We get interrupts on unclaimed registers, so check for this before we
+@@ -2439,26 +2110,21 @@
+ 	intel_uncore_check_errors(dev);
+ 
+ 	/* disable master interrupt before clearing iir  */
+-	de_ier = I915_READ(DEIER);
+-	I915_WRITE(DEIER, de_ier & ~DE_MASTER_IRQ_CONTROL);
+-	POSTING_READ(DEIER);
++	__raw_i915_write32(dev_priv, DEIER, dev_priv->irq_enable & ~DE_MASTER_IRQ_CONTROL);
+ 
+ 	/* Disable south interrupts. We'll only write to SDEIIR once, so further
+ 	 * interrupts will will be stored on its back queue, and then we'll be
+ 	 * able to process them after we restore SDEIER (as soon as we restore
+ 	 * it, we'll get an interrupt if SDEIIR still has something to process
+ 	 * due to its back queue). */
+-	if (!HAS_PCH_NOP(dev)) {
+-		sde_ier = I915_READ(SDEIER);
+-		I915_WRITE(SDEIER, 0);
+-		POSTING_READ(SDEIER);
+-	}
++	if (!HAS_PCH_NOP(dev))
++		__raw_i915_write32(dev_priv, SDEIER, 0);
+ 
+ 	/* Find, clear, then process each source of interrupt */
+ 
+-	gt_iir = I915_READ(GTIIR);
++	gt_iir = __raw_i915_read32(dev_priv, GTIIR);
+ 	if (gt_iir) {
+-		I915_WRITE(GTIIR, gt_iir);
++		__raw_i915_write32(dev_priv, GTIIR, gt_iir);
+ 		ret = IRQ_HANDLED;
+ 		if (INTEL_INFO(dev)->gen >= 6)
+ 			snb_gt_irq_handler(dev, dev_priv, gt_iir);
+@@ -2466,9 +2132,9 @@
+ 			ilk_gt_irq_handler(dev, dev_priv, gt_iir);
+ 	}
+ 
+-	de_iir = I915_READ(DEIIR);
++	de_iir = __raw_i915_read32(dev_priv, DEIIR);
+ 	if (de_iir) {
+-		I915_WRITE(DEIIR, de_iir);
++		__raw_i915_write32(dev_priv, DEIIR, de_iir);
+ 		ret = IRQ_HANDLED;
+ 		if (INTEL_INFO(dev)->gen >= 7)
+ 			ivb_display_irq_handler(dev, de_iir);
+@@ -2477,20 +2143,18 @@
+ 	}
+ 
+ 	if (INTEL_INFO(dev)->gen >= 6) {
+-		u32 pm_iir = I915_READ(GEN6_PMIIR);
++		u32 pm_iir = __raw_i915_read32(dev_priv, GEN6_PMIIR);
+ 		if (pm_iir) {
+-			I915_WRITE(GEN6_PMIIR, pm_iir);
++			__raw_i915_write32(dev_priv, GEN6_PMIIR, pm_iir);
+ 			ret = IRQ_HANDLED;
+ 			gen6_rps_irq_handler(dev_priv, pm_iir);
+ 		}
+ 	}
+ 
+-	I915_WRITE(DEIER, de_ier);
+-	POSTING_READ(DEIER);
+-	if (!HAS_PCH_NOP(dev)) {
+-		I915_WRITE(SDEIER, sde_ier);
+-		POSTING_READ(SDEIER);
+-	}
++	__raw_i915_write32(dev_priv, DEIER, dev_priv->irq_enable);
++	if (!HAS_PCH_NOP(dev))
++		__raw_i915_write32(dev_priv, SDEIER, ~0);
++	__raw_i915_posting_read(dev_priv, DEIER);
+ 
+ 	return ret;
+ }
+@@ -2503,6 +2167,11 @@
+ 	irqreturn_t ret = IRQ_NONE;
+ 	uint32_t tmp = 0;
+ 	enum pipe pipe;
++	u32 aux_mask = GEN8_AUX_CHANNEL_A;
++
++	if (IS_GEN9(dev))
++		aux_mask |=  GEN9_AUX_CHANNEL_B | GEN9_AUX_CHANNEL_C |
++			GEN9_AUX_CHANNEL_D;
+ 
+ 	master_ctl = I915_READ(GEN8_MASTER_IRQ);
+ 	master_ctl &= ~GEN8_MASTER_IRQ_CONTROL;
+@@ -2535,7 +2204,8 @@
+ 		if (tmp) {
+ 			I915_WRITE(GEN8_DE_PORT_IIR, tmp);
+ 			ret = IRQ_HANDLED;
+-			if (tmp & GEN8_AUX_CHANNEL_A)
++
++			if (tmp & aux_mask)
+ 				dp_aux_irq_handler(dev);
+ 			else
+ 				DRM_ERROR("Unexpected DE Port interrupt\n");
+@@ -2544,8 +2214,8 @@
+ 			DRM_ERROR("The master control interrupt lied (DE PORT)!\n");
+ 	}
+ 
+-	for_each_pipe(pipe) {
+-		uint32_t pipe_iir;
++	for_each_pipe(dev_priv, pipe) {
++		uint32_t pipe_iir, flip_done = 0, fault_errors = 0;
+ 
+ 		if (!(master_ctl & GEN8_DE_PIPE_IRQ(pipe)))
+ 			continue;
+@@ -2554,10 +2224,17 @@
+ 		if (pipe_iir) {
+ 			ret = IRQ_HANDLED;
+ 			I915_WRITE(GEN8_DE_PIPE_IIR(pipe), pipe_iir);
+-			if (pipe_iir & GEN8_PIPE_VBLANK)
+-				intel_pipe_handle_vblank(dev, pipe);
+ 
+-			if (pipe_iir & GEN8_PIPE_PRIMARY_FLIP_DONE) {
++			if (pipe_iir & GEN8_PIPE_VBLANK &&
++			    intel_pipe_handle_vblank(dev, pipe))
++				intel_check_page_flip(dev, pipe);
++
++			if (IS_GEN9(dev))
++				flip_done = pipe_iir & GEN9_PIPE_PLANE1_FLIP_DONE;
++			else
++				flip_done = pipe_iir & GEN8_PIPE_PRIMARY_FLIP_DONE;
++
++			if (flip_done) {
+ 				intel_prepare_page_flip(dev, pipe);
+ 				intel_finish_page_flip_plane(dev, pipe);
+ 			}
+@@ -2565,18 +2242,20 @@
+ 			if (pipe_iir & GEN8_PIPE_CDCLK_CRC_DONE)
+ 				hsw_pipe_crc_irq_handler(dev, pipe);
+ 
+-			if (pipe_iir & GEN8_PIPE_FIFO_UNDERRUN) {
+-				if (intel_set_cpu_fifo_underrun_reporting(dev, pipe,
+-									  false))
+-					DRM_ERROR("Pipe %c FIFO underrun\n",
+-						  pipe_name(pipe));
+-			}
++			if (pipe_iir & GEN8_PIPE_FIFO_UNDERRUN)
++				intel_cpu_fifo_underrun_irq_handler(dev_priv,
++								    pipe);
++
++
++			if (IS_GEN9(dev))
++				fault_errors = pipe_iir & GEN9_DE_PIPE_IRQ_FAULT_ERRORS;
++			else
++				fault_errors = pipe_iir & GEN8_DE_PIPE_IRQ_FAULT_ERRORS;
+ 
+-			if (pipe_iir & GEN8_DE_PIPE_IRQ_FAULT_ERRORS) {
++			if (fault_errors)
+ 				DRM_ERROR("Fault errors on pipe %c\n: 0x%08x",
+ 					  pipe_name(pipe),
+ 					  pipe_iir & GEN8_DE_PIPE_IRQ_FAULT_ERRORS);
+-			}
+ 		} else
+ 			DRM_ERROR("The master control interrupt lied (DE PIPE)!\n");
+ 	}
+@@ -2606,7 +2285,7 @@
+ static void i915_error_wake_up(struct drm_i915_private *dev_priv,
+ 			       bool reset_completed)
+ {
+-	struct intel_engine_cs *ring;
++	struct intel_engine_cs *engine;
+ 	int i;
+ 
+ 	/*
+@@ -2617,8 +2296,8 @@
+ 	 */
+ 
+ 	/* Wake up __wait_seqno, potentially holding dev->struct_mutex. */
+-	for_each_ring(ring, dev_priv, i)
+-		wake_up_all(&ring->irq_queue);
++	for_each_engine(engine, dev_priv, i)
++		wake_up_all(&engine->irq_queue);
+ 
+ 	/* Wake up intel_crtc_wait_for_pending_flips, holding crtc->mutex. */
+ 	wake_up_all(&dev_priv->pending_flip_queue);
+@@ -2662,7 +2341,7 @@
+ 	 * the reset in-progress bit is only ever set by code outside of this
+ 	 * work we don't need to worry about any other races.
+ 	 */
+-	if (i915_reset_in_progress(error) && !i915_terminally_wedged(error)) {
++	if (i915_recovery_pending(error)) {
+ 		DRM_DEBUG_DRIVER("resetting chip\n");
+ 		kobject_uevent_env(&dev->primary->kdev->kobj, KOBJ_CHANGE,
+ 				   reset_event);
+@@ -2698,9 +2377,7 @@
+ 			 * updates before
+ 			 * the counter increment.
+ 			 */
+-			smp_mb__before_atomic();
+-			atomic_inc(&dev_priv->gpu_error.reset_counter);
+-
++			smp_mb__after_atomic();
+ 			kobject_uevent_env(&dev->primary->kdev->kobj,
+ 					   KOBJ_CHANGE, reset_done_event);
+ 		} else {
+@@ -2763,7 +2440,7 @@
+ 
+ 	if (eir & I915_ERROR_MEMORY_REFRESH) {
+ 		pr_err("memory refresh error:\n");
+-		for_each_pipe(pipe)
++		for_each_pipe(dev_priv, pipe)
+ 			pr_err("pipe %c stat: 0x%08x\n",
+ 			       pipe_name(pipe), I915_READ(PIPESTAT(pipe)));
+ 		/* pipestat has already been acked */
+@@ -2817,7 +2494,7 @@
+  * so userspace knows something bad happened (should trigger collection
+  * of a ring dump etc.).
+  */
+-void i915_handle_error(struct drm_device *dev, bool wedged,
++void i915_handle_error(struct drm_device *dev, unsigned flags,
+ 		       const char *fmt, ...)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+@@ -2828,10 +2505,11 @@
+ 	vscnprintf(error_msg, sizeof(error_msg), fmt, args);
+ 	va_end(args);
+ 
+-	i915_capture_error_state(dev, wedged, error_msg);
++	if ((flags & I915_HANG_SIMULATED) == 0)
++		i915_capture_error_state(dev, flags, error_msg);
+ 	i915_report_and_clear_eir(dev);
+ 
+-	if (wedged) {
++	if (flags & I915_HANG_RESET) {
+ 		atomic_set_mask(I915_RESET_IN_PROGRESS_FLAG,
+ 				&dev_priv->gpu_error.reset_counter);
+ 
+@@ -2860,52 +2538,6 @@
+ 	schedule_work(&dev_priv->gpu_error.work);
+ }
+ 
+-static void __always_unused i915_pageflip_stall_check(struct drm_device *dev, int pipe)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct drm_crtc *crtc = dev_priv->pipe_to_crtc_mapping[pipe];
+-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+-	struct drm_i915_gem_object *obj;
+-	struct intel_unpin_work *work;
+-	unsigned long flags;
+-	bool stall_detected;
+-
+-	/* Ignore early vblank irqs */
+-	if (intel_crtc == NULL)
+-		return;
+-
+-	spin_lock_irqsave(&dev->event_lock, flags);
+-	work = intel_crtc->unpin_work;
+-
+-	if (work == NULL ||
+-	    atomic_read(&work->pending) >= INTEL_FLIP_COMPLETE ||
+-	    !work->enable_stall_check) {
+-		/* Either the pending flip IRQ arrived, or we're too early. Don't check */
+-		spin_unlock_irqrestore(&dev->event_lock, flags);
+-		return;
+-	}
+-
+-	/* Potential stall - if we see that the flip has happened, assume a missed interrupt */
+-	obj = work->pending_flip_obj;
+-	if (INTEL_INFO(dev)->gen >= 4) {
+-		int dspsurf = DSPSURF(intel_crtc->plane);
+-		stall_detected = I915_HI_DISPBASE(I915_READ(dspsurf)) ==
+-					i915_gem_obj_ggtt_offset(obj);
+-	} else {
+-		int dspaddr = DSPADDR(intel_crtc->plane);
+-		stall_detected = I915_READ(dspaddr) == (i915_gem_obj_ggtt_offset(obj) +
+-							crtc->y * crtc->primary->fb->pitches[0] +
+-							crtc->x * crtc->primary->fb->bits_per_pixel/8);
+-	}
+-
+-	spin_unlock_irqrestore(&dev->event_lock, flags);
+-
+-	if (stall_detected) {
+-		DRM_DEBUG_DRIVER("Pageflip stall detected\n");
+-		intel_prepare_page_flip(dev, intel_crtc->plane);
+-	}
+-}
+-
+ /* Called from drm generic code, passed 'crtc' which
+  * we use as a pipe index
+  */
+@@ -3031,24 +2663,24 @@
+ 	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
+ }
+ 
+-static u32
+-ring_last_seqno(struct intel_engine_cs *ring)
+-{
+-	return list_entry(ring->request_list.prev,
+-			  struct drm_i915_gem_request, list)->seqno;
+-}
+-
+ static bool
+-ring_idle(struct intel_engine_cs *ring, u32 seqno)
++engine_idle(struct intel_engine_cs *engine)
+ {
+-	return (list_empty(&ring->request_list) ||
+-		i915_seqno_passed(seqno, ring_last_seqno(ring)));
++	bool ret = true;
++
++	spin_lock(&engine->lock);
++	if (engine->last_request &&
++	    !__i915_request_complete__wa(engine->last_request))
++		ret = engine->is_idle(engine);
++	spin_unlock(&engine->lock);
++
++	return ret;
+ }
+ 
+ static bool
+-ipehr_is_semaphore_wait(struct drm_device *dev, u32 ipehr)
++ipehr_is_semaphore_wait(struct drm_i915_private *i915, u32 ipehr)
+ {
+-	if (INTEL_INFO(dev)->gen >= 8) {
++	if (INTEL_INFO(i915)->gen >= 8) {
+ 		return (ipehr >> 23) == 0x1c;
+ 	} else {
+ 		ipehr &= ~MI_SEMAPHORE_SYNC_MASK;
+@@ -3058,48 +2690,54 @@
+ }
+ 
+ static struct intel_engine_cs *
+-semaphore_wait_to_signaller_ring(struct intel_engine_cs *ring, u32 ipehr, u64 offset)
++semaphore_wait_to_signaller_engine(struct intel_engine_cs *engine, u32 ipehr, u64 offset)
+ {
+-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
++	struct drm_i915_private *dev_priv = engine->i915;
+ 	struct intel_engine_cs *signaller;
+ 	int i;
+ 
+ 	if (INTEL_INFO(dev_priv->dev)->gen >= 8) {
+-		for_each_ring(signaller, dev_priv, i) {
+-			if (ring == signaller)
++		for_each_engine(signaller, dev_priv, i) {
++			if (engine == signaller)
+ 				continue;
+ 
+-			if (offset == signaller->semaphore.signal_ggtt[ring->id])
++			if (offset == GEN8_SEMAPHORE_OFFSET(dev_priv, signaller->id, engine->id))
+ 				return signaller;
+ 		}
+ 	} else {
+ 		u32 sync_bits = ipehr & MI_SEMAPHORE_SYNC_MASK;
+ 
+-		for_each_ring(signaller, dev_priv, i) {
+-			if(ring == signaller)
++		for_each_engine(signaller, dev_priv, i) {
++			if(engine == signaller)
+ 				continue;
+ 
+-			if (sync_bits == signaller->semaphore.mbox.wait[ring->id])
++			if (sync_bits == signaller->semaphore.mbox.wait[engine->id])
+ 				return signaller;
+ 		}
+ 	}
+ 
+ 	DRM_ERROR("No signaller ring found for ring %i, ipehr 0x%08x, offset 0x%016llx\n",
+-		  ring->id, ipehr, offset);
++		  engine->id, ipehr, offset);
+ 
+ 	return NULL;
+ }
+ 
+ static struct intel_engine_cs *
+-semaphore_waits_for(struct intel_engine_cs *ring, u32 *seqno)
++semaphore_waits_for(struct intel_engine_cs *engine, u32 *seqno)
+ {
+-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
++	struct drm_i915_private *dev_priv = engine->i915;
++	struct intel_ringbuffer *ring;
+ 	u32 cmd, ipehr, head;
+ 	u64 offset = 0;
+ 	int i, backwards;
+ 
+-	ipehr = I915_READ(RING_IPEHR(ring->mmio_base));
+-	if (!ipehr_is_semaphore_wait(ring->dev, ipehr))
++	ipehr = I915_READ(RING_IPEHR(engine->mmio_base));
++	if (!ipehr_is_semaphore_wait(engine->i915, ipehr))
++		return NULL;
++
++	/* XXX execlists */
++	ring = engine->default_context->ring[RCS].ring;
++	if (ring == NULL)
+ 		return NULL;
+ 
+ 	/*
+@@ -3110,19 +2748,19 @@
+ 	 * point at at batch, and semaphores are always emitted into the
+ 	 * ringbuffer itself.
+ 	 */
+-	head = I915_READ_HEAD(ring) & HEAD_ADDR;
+-	backwards = (INTEL_INFO(ring->dev)->gen >= 8) ? 5 : 4;
++	head = I915_READ_HEAD(engine) & HEAD_ADDR;
++	backwards = (INTEL_INFO(dev_priv)->gen >= 8) ? 5 : 4;
+ 
+ 	for (i = backwards; i; --i) {
+ 		/*
+ 		 * Be paranoid and presume the hw has gone off into the wild -
+-		 * our ring is smaller than what the hardware (and hence
++		 * our engine is smaller than what the hardware (and hence
+ 		 * HEAD_ADDR) allows. Also handles wrap-around.
+ 		 */
+-		head &= ring->buffer->size - 1;
++		head &= ring->size - 1;
+ 
+ 		/* This here seems to blow up */
+-		cmd = ioread32(ring->buffer->virtual_start + head);
++		cmd = ioread32(ring->virtual_start + head);
+ 		if (cmd == ipehr)
+ 			break;
+ 
+@@ -3132,32 +2770,37 @@
+ 	if (!i)
+ 		return NULL;
+ 
+-	*seqno = ioread32(ring->buffer->virtual_start + head + 4) + 1;
+-	if (INTEL_INFO(ring->dev)->gen >= 8) {
+-		offset = ioread32(ring->buffer->virtual_start + head + 12);
++	*seqno = ioread32(ring->virtual_start + head + 4) + 1;
++	if (INTEL_INFO(dev_priv)->gen >= 8) {
++		offset = ioread32(ring->virtual_start + head + 12);
+ 		offset <<= 32;
+-		offset = ioread32(ring->buffer->virtual_start + head + 8);
++		offset = ioread32(ring->virtual_start + head + 8);
+ 	}
+-	return semaphore_wait_to_signaller_ring(ring, ipehr, offset);
++	return semaphore_wait_to_signaller_engine(engine, ipehr, offset);
+ }
+ 
+-static int semaphore_passed(struct intel_engine_cs *ring)
++static int semaphore_passed(struct intel_engine_cs *engine)
+ {
+-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
++	struct drm_i915_private *dev_priv = engine->i915;
+ 	struct intel_engine_cs *signaller;
++	struct i915_gem_request *rq;
+ 	u32 seqno;
+ 
+-	ring->hangcheck.deadlock++;
++	engine->hangcheck.deadlock++;
+ 
+-	signaller = semaphore_waits_for(ring, &seqno);
++	if (engine->semaphore.wait == NULL)
++		return -1;
++
++	signaller = semaphore_waits_for(engine, &seqno);
+ 	if (signaller == NULL)
+ 		return -1;
+ 
+ 	/* Prevent pathological recursion due to driver bugs */
+-	if (signaller->hangcheck.deadlock >= I915_NUM_RINGS)
++	if (signaller->hangcheck.deadlock >= I915_NUM_ENGINES)
+ 		return -1;
+ 
+-	if (i915_seqno_passed(signaller->get_seqno(signaller, false), seqno))
++	rq = intel_engine_seqno_to_request(signaller, seqno);
++	if (rq == NULL || i915_request_complete(rq))
+ 		return 1;
+ 
+ 	/* cursory check for an unkickable deadlock */
+@@ -3170,30 +2813,29 @@
+ 
+ static void semaphore_clear_deadlocks(struct drm_i915_private *dev_priv)
+ {
+-	struct intel_engine_cs *ring;
++	struct intel_engine_cs *engine;
+ 	int i;
+ 
+-	for_each_ring(ring, dev_priv, i)
+-		ring->hangcheck.deadlock = 0;
++	for_each_engine(engine, dev_priv, i)
++		engine->hangcheck.deadlock = 0;
+ }
+ 
+-static enum intel_ring_hangcheck_action
+-ring_stuck(struct intel_engine_cs *ring, u64 acthd)
++static enum intel_engine_hangcheck_action
++engine_stuck(struct intel_engine_cs *engine, u64 acthd)
+ {
+-	struct drm_device *dev = ring->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_i915_private *dev_priv = engine->i915;
+ 	u32 tmp;
+ 
+-	if (acthd != ring->hangcheck.acthd) {
+-		if (acthd > ring->hangcheck.max_acthd) {
+-			ring->hangcheck.max_acthd = acthd;
++	if (acthd != engine->hangcheck.acthd) {
++		if (acthd > engine->hangcheck.max_acthd) {
++			engine->hangcheck.max_acthd = acthd;
+ 			return HANGCHECK_ACTIVE;
+ 		}
+ 
+ 		return HANGCHECK_ACTIVE_LOOP;
+ 	}
+ 
+-	if (IS_GEN2(dev))
++	if (IS_GEN2(dev_priv))
+ 		return HANGCHECK_HUNG;
+ 
+ 	/* Is the chip hanging on a WAIT_FOR_EVENT?
+@@ -3201,24 +2843,24 @@
+ 	 * and break the hang. This should work on
+ 	 * all but the second generation chipsets.
+ 	 */
+-	tmp = I915_READ_CTL(ring);
++	tmp = I915_READ_CTL(engine);
+ 	if (tmp & RING_WAIT) {
+-		i915_handle_error(dev, false,
++		i915_handle_error(dev_priv->dev, 0,
+ 				  "Kicking stuck wait on %s",
+-				  ring->name);
+-		I915_WRITE_CTL(ring, tmp);
++				  engine->name);
++		I915_WRITE_CTL(engine, tmp);
+ 		return HANGCHECK_KICK;
+ 	}
+ 
+-	if (INTEL_INFO(dev)->gen >= 6 && tmp & RING_WAIT_SEMAPHORE) {
+-		switch (semaphore_passed(ring)) {
++	if (INTEL_INFO(dev_priv)->gen >= 6 && tmp & RING_WAIT_SEMAPHORE) {
++		switch (semaphore_passed(engine)) {
+ 		default:
+ 			return HANGCHECK_HUNG;
+ 		case 1:
+-			i915_handle_error(dev, false,
++			i915_handle_error(dev_priv->dev, 0,
+ 					  "Kicking stuck semaphore on %s",
+-					  ring->name);
+-			I915_WRITE_CTL(ring, tmp);
++					  engine->name);
++			I915_WRITE_CTL(engine, tmp);
+ 			return HANGCHECK_KICK;
+ 		case 0:
+ 			return HANGCHECK_WAIT;
+@@ -3230,135 +2872,172 @@
+ 
+ /**
+  * This is called when the chip hasn't reported back with completed
+- * batchbuffers in a long time. We keep track per ring seqno progress and
++ * batchbuffers in a long time. We keep track per engine seqno progress and
+  * if there are no progress, hangcheck score for that ring is increased.
+  * Further, acthd is inspected to see if the ring is stuck. On stuck case
+  * we kick the ring. If we see no progress on three subsequent calls
+  * we assume chip is wedged and try to fix it by resetting the chip.
+  */
+-static void i915_hangcheck_elapsed(unsigned long data)
++static void i915_hangcheck_elapsed(struct work_struct *work)
+ {
+-	struct drm_device *dev = (struct drm_device *)data;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring;
++	struct drm_i915_private *dev_priv =
++		container_of(work, typeof(*dev_priv),
++			     gpu_error.hangcheck_work.work);
++	struct intel_engine_cs *engine;
++	bool stuck[I915_NUM_ENGINES] = { 0 };
++	int busy_count = 0, hung = 0;
+ 	int i;
+-	int busy_count = 0, rings_hung = 0;
+-	bool stuck[I915_NUM_RINGS] = { 0 };
+ #define BUSY 1
+ #define KICK 5
+ #define HUNG 20
+ 
+-	if (!i915.enable_hangcheck)
++	if (!i915_module.enable_hangcheck)
+ 		return;
+ 
+-	for_each_ring(ring, dev_priv, i) {
++	for_each_engine(engine, dev_priv, i) {
+ 		u64 acthd;
+ 		u32 seqno;
++		u32 interrupts;
+ 		bool busy = true;
+ 
+ 		semaphore_clear_deadlocks(dev_priv);
+ 
+-		seqno = ring->get_seqno(ring, false);
+-		acthd = intel_ring_get_active_head(ring);
++		acthd = intel_engine_get_active_head(engine);
++		seqno = intel_engine_get_seqno(engine);
++		interrupts = atomic_read(&engine->interrupts);
++
++		if (engine_idle(engine)) {
++			if (waitqueue_active(&engine->irq_queue)) {
++				/* Issue a wake-up to catch stuck h/w. */
++				if (engine->hangcheck.action == HANGCHECK_IDLE_WAITERS &&
++						engine->hangcheck.interrupts == interrupts &&
++						!test_and_set_bit(engine->id, &dev_priv->gpu_error.missed_irq_rings)) {
++					if (!(dev_priv->gpu_error.test_irq_rings & intel_engine_flag(engine)))
++						DRM_ERROR("Hangcheck timer elapsed... %s idle\n",
++								engine->name);
++					else
++						DRM_INFO("Fake missed irq on %s\n",
++								engine->name);
++					wake_up_all(&engine->irq_queue);
++				}
+ 
+-		if (ring->hangcheck.seqno == seqno) {
+-			if (ring_idle(ring, seqno)) {
+-				ring->hangcheck.action = HANGCHECK_IDLE;
+-
+-				if (waitqueue_active(&ring->irq_queue)) {
+-					/* Issue a wake-up to catch stuck h/w. */
+-					if (!test_and_set_bit(ring->id, &dev_priv->gpu_error.missed_irq_rings)) {
+-						if (!(dev_priv->gpu_error.test_irq_rings & intel_ring_flag(ring)))
+-							DRM_ERROR("Hangcheck timer elapsed... %s idle\n",
+-								  ring->name);
+-						else
+-							DRM_INFO("Fake missed irq on %s\n",
+-								 ring->name);
+-						wake_up_all(&ring->irq_queue);
+-					}
+-					/* Safeguard against driver failure */
+-					ring->hangcheck.score += BUSY;
+-				} else
+-					busy = false;
++				/* Safeguard against driver failure */
++				engine->hangcheck.score += BUSY;
++				engine->hangcheck.action = HANGCHECK_IDLE_WAITERS;
+ 			} else {
+-				/* We always increment the hangcheck score
+-				 * if the ring is busy and still processing
+-				 * the same request, so that no single request
+-				 * can run indefinitely (such as a chain of
+-				 * batches). The only time we do not increment
+-				 * the hangcheck score on this ring, if this
+-				 * ring is in a legitimate wait for another
+-				 * ring. In that case the waiting ring is a
+-				 * victim and we want to be sure we catch the
+-				 * right culprit. Then every time we do kick
+-				 * the ring, add a small increment to the
+-				 * score so that we can catch a batch that is
+-				 * being repeatedly kicked and so responsible
+-				 * for stalling the machine.
+-				 */
+-				ring->hangcheck.action = ring_stuck(ring,
+-								    acthd);
+-
+-				switch (ring->hangcheck.action) {
++				busy = false;
++				engine->hangcheck.action = HANGCHECK_IDLE;
++			}
++		} else if (engine->hangcheck.seqno == seqno) {
++			/* We always increment the hangcheck score
++			 * if the ring is busy and still processing
++			 * the same request, so that no single request
++			 * can run indefinitely (such as a chain of
++			 * batches). The only time we do not increment
++			 * the hangcheck score on this ring, if this
++			 * ring is in a legitimate wait for another
++			 * ring. In that case the waiting ring is a
++			 * victim and we want to be sure we catch the
++			 * right culprit. Then every time we do kick
++			 * the ring, add a small increment to the
++			 * score so that we can catch a batch that is
++			 * being repeatedly kicked and so responsible
++			 * for stalling the machine.
++			 */
++			engine->hangcheck.action = engine_stuck(engine, acthd);
++			switch (engine->hangcheck.action) {
+ 				case HANGCHECK_IDLE:
++				case HANGCHECK_IDLE_WAITERS:
+ 				case HANGCHECK_WAIT:
+ 				case HANGCHECK_ACTIVE:
+ 					break;
+ 				case HANGCHECK_ACTIVE_LOOP:
+-					ring->hangcheck.score += BUSY;
++					engine->hangcheck.score += BUSY;
+ 					break;
+ 				case HANGCHECK_KICK:
+-					ring->hangcheck.score += KICK;
++					engine->hangcheck.score += KICK;
+ 					break;
+ 				case HANGCHECK_HUNG:
+-					ring->hangcheck.score += HUNG;
++					engine->hangcheck.score += HUNG;
+ 					stuck[i] = true;
+ 					break;
+-				}
+ 			}
+ 		} else {
+-			ring->hangcheck.action = HANGCHECK_ACTIVE;
++			engine->hangcheck.action = HANGCHECK_ACTIVE;
+ 
+ 			/* Gradually reduce the count so that we catch DoS
+ 			 * attempts across multiple batches.
+ 			 */
+-			if (ring->hangcheck.score > 0)
+-				ring->hangcheck.score--;
++			if (engine->hangcheck.score > 0)
++				engine->hangcheck.score--;
+ 
+-			ring->hangcheck.acthd = ring->hangcheck.max_acthd = 0;
++			engine->hangcheck.acthd = engine->hangcheck.max_acthd = 0;
+ 		}
+ 
+-		ring->hangcheck.seqno = seqno;
+-		ring->hangcheck.acthd = acthd;
++		engine->hangcheck.interrupts = interrupts;
++		engine->hangcheck.seqno = seqno;
++		engine->hangcheck.acthd = acthd;
+ 		busy_count += busy;
++
++		hung += engine->hangcheck.score >= HANGCHECK_SCORE_RING_HUNG;
+ 	}
+ 
+-	for_each_ring(ring, dev_priv, i) {
+-		if (ring->hangcheck.score >= HANGCHECK_SCORE_RING_HUNG) {
+-			DRM_INFO("%s on %s\n",
+-				 stuck[i] ? "stuck" : "no progress",
+-				 ring->name);
+-			rings_hung++;
++	if (hung) {
++		char msg[512];
++		int rings_stall, rings_stuck, len;
++
++		len = rings_stall = rings_stuck = 0;
++		for_each_engine(engine, dev_priv, i) {
++			if (engine->hangcheck.score >= HANGCHECK_SCORE_RING_HUNG &&
++			    stuck[i]) {
++				if (rings_stuck == 0)
++					len += snprintf(msg + len,
++							sizeof(msg)-len,
++							"Stuck on");
++				len += snprintf(msg + len, sizeof(msg)-len,
++						" %s,", engine->name);
++				rings_stuck++;
++			}
+ 		}
+-	}
++		if (rings_stuck)
++			msg[--len] = '\0';
++
++		for_each_engine(engine, dev_priv, i) {
++			if (engine->hangcheck.score >= HANGCHECK_SCORE_RING_HUNG &&
++			    !stuck[i]) {
++				if (rings_stall == 0) {
++					if (rings_stuck)
++						len += snprintf(msg + len,
++								sizeof(msg)-len,
++								"; ");
++					len += snprintf(msg + len,
++							sizeof(msg)-len,
++							"No progress on");
++				}
++				len += snprintf(msg + len, sizeof(msg)-len,
++						" %s,", engine->name);
++				rings_stall++;
++			}
++		}
++		if (rings_stall)
++			msg[--len] = '\0';
+ 
+-	if (rings_hung)
+-		return i915_handle_error(dev, true, "Ring hung");
++		return i915_handle_error(dev_priv->dev, I915_HANG_RESET, msg);
++	}
+ 
+ 	if (busy_count)
+ 		/* Reset timer case chip hangs without another request
+ 		 * being added */
+-		i915_queue_hangcheck(dev);
++		i915_queue_hangcheck(dev_priv->dev);
+ }
+ 
+ void i915_queue_hangcheck(struct drm_device *dev)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	if (!i915.enable_hangcheck)
++	if (!i915_module.enable_hangcheck)
+ 		return;
+ 
+-	mod_timer(&dev_priv->gpu_error.hangcheck_timer,
+-		  round_jiffies_up(jiffies + DRM_I915_HANGCHECK_JIFFIES));
++	schedule_delayed_work(&to_i915(dev)->gpu_error.hangcheck_work,
++			      round_jiffies_up_relative(DRM_I915_HANGCHECK_JIFFIES));
+ }
+ 
+ static void ibx_irq_reset(struct drm_device *dev)
+@@ -3409,7 +3088,7 @@
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	I915_WRITE(HWSTAM, 0xffffffff);
++	I915_WRITE(HWSTAM, ~0);
+ 
+ 	GEN5_IRQ_RESET(DE);
+ 	if (IS_GEN7(dev))
+@@ -3420,10 +3099,22 @@
+ 	ibx_irq_reset(dev);
+ }
+ 
++static void vlv_display_irq_reset(struct drm_i915_private *dev_priv)
++{
++	enum pipe pipe;
++
++	I915_WRITE(PORT_HOTPLUG_EN, 0);
++	I915_WRITE(PORT_HOTPLUG_STAT, I915_READ(PORT_HOTPLUG_STAT));
++
++	for_each_pipe(dev_priv, pipe)
++		I915_WRITE(PIPESTAT(pipe), 0xffff);
++
++	GEN5_IRQ_RESET(VLV_);
++}
++
+ static void valleyview_irq_preinstall(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	int pipe;
+ 
+ 	/* VLV magic */
+ 	I915_WRITE(VLV_IMR, 0);
+@@ -3431,22 +3122,11 @@
+ 	I915_WRITE(RING_IMR(GEN6_BSD_RING_BASE), 0);
+ 	I915_WRITE(RING_IMR(BLT_RING_BASE), 0);
+ 
+-	/* and GT */
+-	I915_WRITE(GTIIR, I915_READ(GTIIR));
+-	I915_WRITE(GTIIR, I915_READ(GTIIR));
+-
+ 	gen5_gt_irq_reset(dev);
+ 
+-	I915_WRITE(DPINVGTT, 0xff);
++	I915_WRITE(DPINVGTT, DPINVGTT_STATUS_MASK);
+ 
+-	I915_WRITE(PORT_HOTPLUG_EN, 0);
+-	I915_WRITE(PORT_HOTPLUG_STAT, I915_READ(PORT_HOTPLUG_STAT));
+-	for_each_pipe(pipe)
+-		I915_WRITE(PIPESTAT(pipe), 0xffff);
+-	I915_WRITE(VLV_IIR, 0xffffffff);
+-	I915_WRITE(VLV_IMR, 0xffffffff);
+-	I915_WRITE(VLV_IER, 0x0);
+-	POSTING_READ(VLV_IER);
++	vlv_display_irq_reset(dev_priv);
+ }
+ 
+ static void gen8_gt_irq_reset(struct drm_i915_private *dev_priv)
+@@ -3467,9 +3147,9 @@
+ 
+ 	gen8_gt_irq_reset(dev_priv);
+ 
+-	for_each_pipe(pipe)
+-		if (intel_display_power_enabled(dev_priv,
+-						POWER_DOMAIN_PIPE(pipe)))
++	for_each_pipe(dev_priv, pipe)
++		if (intel_display_power_is_enabled(dev_priv,
++						   POWER_DOMAIN_PIPE(pipe)))
+ 			GEN8_IRQ_RESET_NDX(DE_PIPE, pipe);
+ 
+ 	GEN5_IRQ_RESET(GEN8_DE_PORT_);
+@@ -3481,20 +3161,19 @@
+ 
+ void gen8_irq_power_well_post_enable(struct drm_i915_private *dev_priv)
+ {
+-	unsigned long irqflags;
++	uint32_t extra_ier = GEN8_PIPE_VBLANK | GEN8_PIPE_FIFO_UNDERRUN;
+ 
+-	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
++	spin_lock_irq(&dev_priv->irq_lock);
+ 	GEN8_IRQ_INIT_NDX(DE_PIPE, PIPE_B, dev_priv->de_irq_mask[PIPE_B],
+-			  ~dev_priv->de_irq_mask[PIPE_B]);
++			  ~dev_priv->de_irq_mask[PIPE_B] | extra_ier);
+ 	GEN8_IRQ_INIT_NDX(DE_PIPE, PIPE_C, dev_priv->de_irq_mask[PIPE_C],
+-			  ~dev_priv->de_irq_mask[PIPE_C]);
+-	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
++			  ~dev_priv->de_irq_mask[PIPE_C] | extra_ier);
++	spin_unlock_irq(&dev_priv->irq_lock);
+ }
+ 
+ static void cherryview_irq_preinstall(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	int pipe;
+ 
+ 	I915_WRITE(GEN8_MASTER_IRQ, 0);
+ 	POSTING_READ(GEN8_MASTER_IRQ);
+@@ -3503,37 +3182,25 @@
+ 
+ 	GEN5_IRQ_RESET(GEN8_PCU_);
+ 
+-	POSTING_READ(GEN8_PCU_IIR);
+-
+ 	I915_WRITE(DPINVGTT, DPINVGTT_STATUS_MASK_CHV);
+ 
+-	I915_WRITE(PORT_HOTPLUG_EN, 0);
+-	I915_WRITE(PORT_HOTPLUG_STAT, I915_READ(PORT_HOTPLUG_STAT));
+-
+-	for_each_pipe(pipe)
+-		I915_WRITE(PIPESTAT(pipe), 0xffff);
+-
+-	I915_WRITE(VLV_IMR, 0xffffffff);
+-	I915_WRITE(VLV_IER, 0x0);
+-	I915_WRITE(VLV_IIR, 0xffffffff);
+-	POSTING_READ(VLV_IIR);
++	vlv_display_irq_reset(dev_priv);
+ }
+ 
+ static void ibx_hpd_irq_setup(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct drm_mode_config *mode_config = &dev->mode_config;
+ 	struct intel_encoder *intel_encoder;
+ 	u32 hotplug_irqs, hotplug, enabled_irqs = 0;
+ 
+ 	if (HAS_PCH_IBX(dev)) {
+ 		hotplug_irqs = SDE_HOTPLUG_MASK;
+-		list_for_each_entry(intel_encoder, &mode_config->encoder_list, base.head)
++		for_each_intel_encoder(dev, intel_encoder)
+ 			if (dev_priv->hpd_stats[intel_encoder->hpd_pin].hpd_mark == HPD_ENABLED)
+ 				enabled_irqs |= hpd_ibx[intel_encoder->hpd_pin];
+ 	} else {
+ 		hotplug_irqs = SDE_HOTPLUG_MASK_CPT;
+-		list_for_each_entry(intel_encoder, &mode_config->encoder_list, base.head)
++		for_each_intel_encoder(dev, intel_encoder)
+ 			if (dev_priv->hpd_stats[intel_encoder->hpd_pin].hpd_mark == HPD_ENABLED)
+ 				enabled_irqs |= hpd_cpt[intel_encoder->hpd_pin];
+ 	}
+@@ -3596,7 +3263,7 @@
+ 	GEN5_IRQ_INIT(GT, dev_priv->gt_irq_mask, gt_irqs);
+ 
+ 	if (INTEL_INFO(dev)->gen >= 6) {
+-		pm_irqs |= dev_priv->pm_rps_events;
++		pm_irqs |= dev_priv->rps.pm_events;
+ 
+ 		if (HAS_VEBOX(dev))
+ 			pm_irqs |= PM_VEBOX_USER_INTERRUPT;
+@@ -3608,7 +3275,6 @@
+ 
+ static int ironlake_irq_postinstall(struct drm_device *dev)
+ {
+-	unsigned long irqflags;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	u32 display_mask, extra_mask;
+ 
+@@ -3630,8 +3296,9 @@
+ 	}
+ 
+ 	dev_priv->irq_mask = ~display_mask;
++	dev_priv->irq_enable = display_mask | extra_mask;
+ 
+-	I915_WRITE(HWSTAM, 0xeffe);
++	I915_WRITE(HWSTAM, ~0);
+ 
+ 	ibx_irq_pre_postinstall(dev);
+ 
+@@ -3647,9 +3314,9 @@
+ 		 * spinlocking not required here for correctness since interrupt
+ 		 * setup is guaranteed to run in single-threaded context. But we
+ 		 * need it to make the assert_spin_locked happy. */
+-		spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
++		spin_lock_irq(&dev_priv->irq_lock);
+ 		ironlake_enable_display_irq(dev_priv, DE_PCU_EVENT);
+-		spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
++		spin_unlock_irq(&dev_priv->irq_lock);
+ 	}
+ 
+ 	return 0;
+@@ -3659,45 +3326,51 @@
+ {
+ 	u32 pipestat_mask;
+ 	u32 iir_mask;
++	enum pipe pipe;
+ 
+ 	pipestat_mask = PIPESTAT_INT_STATUS_MASK |
+ 			PIPE_FIFO_UNDERRUN_STATUS;
+ 
+-	I915_WRITE(PIPESTAT(PIPE_A), pipestat_mask);
+-	I915_WRITE(PIPESTAT(PIPE_B), pipestat_mask);
++	for_each_pipe(dev_priv, pipe)
++		I915_WRITE(PIPESTAT(pipe), pipestat_mask);
+ 	POSTING_READ(PIPESTAT(PIPE_A));
+ 
+ 	pipestat_mask = PLANE_FLIP_DONE_INT_STATUS_VLV |
+ 			PIPE_CRC_DONE_INTERRUPT_STATUS;
+ 
+-	i915_enable_pipestat(dev_priv, PIPE_A, pipestat_mask |
+-					       PIPE_GMBUS_INTERRUPT_STATUS);
+-	i915_enable_pipestat(dev_priv, PIPE_B, pipestat_mask);
++	i915_enable_pipestat(dev_priv, PIPE_A, PIPE_GMBUS_INTERRUPT_STATUS);
++	for_each_pipe(dev_priv, pipe)
++		      i915_enable_pipestat(dev_priv, pipe, pipestat_mask);
+ 
+ 	iir_mask = I915_DISPLAY_PORT_INTERRUPT |
+ 		   I915_DISPLAY_PIPE_A_EVENT_INTERRUPT |
+ 		   I915_DISPLAY_PIPE_B_EVENT_INTERRUPT;
++	if (IS_CHERRYVIEW(dev_priv))
++		iir_mask |= I915_DISPLAY_PIPE_C_EVENT_INTERRUPT;
+ 	dev_priv->irq_mask &= ~iir_mask;
+ 
+ 	I915_WRITE(VLV_IIR, iir_mask);
+ 	I915_WRITE(VLV_IIR, iir_mask);
+-	I915_WRITE(VLV_IMR, dev_priv->irq_mask);
+ 	I915_WRITE(VLV_IER, ~dev_priv->irq_mask);
+-	POSTING_READ(VLV_IER);
++	I915_WRITE(VLV_IMR, dev_priv->irq_mask);
++	POSTING_READ(VLV_IMR);
+ }
+ 
+ static void valleyview_display_irqs_uninstall(struct drm_i915_private *dev_priv)
+ {
+ 	u32 pipestat_mask;
+ 	u32 iir_mask;
++	enum pipe pipe;
+ 
+ 	iir_mask = I915_DISPLAY_PORT_INTERRUPT |
+ 		   I915_DISPLAY_PIPE_A_EVENT_INTERRUPT |
+ 		   I915_DISPLAY_PIPE_B_EVENT_INTERRUPT;
++	if (IS_CHERRYVIEW(dev_priv))
++		iir_mask |= I915_DISPLAY_PIPE_C_EVENT_INTERRUPT;
+ 
+ 	dev_priv->irq_mask |= iir_mask;
+-	I915_WRITE(VLV_IER, ~dev_priv->irq_mask);
+ 	I915_WRITE(VLV_IMR, dev_priv->irq_mask);
++	I915_WRITE(VLV_IER, ~dev_priv->irq_mask);
+ 	I915_WRITE(VLV_IIR, iir_mask);
+ 	I915_WRITE(VLV_IIR, iir_mask);
+ 	POSTING_READ(VLV_IIR);
+@@ -3705,14 +3378,15 @@
+ 	pipestat_mask = PLANE_FLIP_DONE_INT_STATUS_VLV |
+ 			PIPE_CRC_DONE_INTERRUPT_STATUS;
+ 
+-	i915_disable_pipestat(dev_priv, PIPE_A, pipestat_mask |
+-					        PIPE_GMBUS_INTERRUPT_STATUS);
+-	i915_disable_pipestat(dev_priv, PIPE_B, pipestat_mask);
++	i915_disable_pipestat(dev_priv, PIPE_A, PIPE_GMBUS_INTERRUPT_STATUS);
++	for_each_pipe(dev_priv, pipe)
++		i915_disable_pipestat(dev_priv, pipe, pipestat_mask);
+ 
+ 	pipestat_mask = PIPESTAT_INT_STATUS_MASK |
+ 			PIPE_FIFO_UNDERRUN_STATUS;
+-	I915_WRITE(PIPESTAT(PIPE_A), pipestat_mask);
+-	I915_WRITE(PIPESTAT(PIPE_B), pipestat_mask);
++
++	for_each_pipe(dev_priv, pipe)
++		I915_WRITE(PIPESTAT(pipe), pipestat_mask);
+ 	POSTING_READ(PIPESTAT(PIPE_A));
+ }
+ 
+@@ -3725,7 +3399,7 @@
+ 
+ 	dev_priv->display_irqs_enabled = true;
+ 
+-	if (dev_priv->dev->irq_enabled)
++	if (intel_irqs_enabled(dev_priv))
+ 		valleyview_display_irqs_install(dev_priv);
+ }
+ 
+@@ -3738,34 +3412,36 @@
+ 
+ 	dev_priv->display_irqs_enabled = false;
+ 
+-	if (dev_priv->dev->irq_enabled)
++	if (intel_irqs_enabled(dev_priv))
+ 		valleyview_display_irqs_uninstall(dev_priv);
+ }
+ 
+-static int valleyview_irq_postinstall(struct drm_device *dev)
++static void vlv_display_irq_postinstall(struct drm_i915_private *dev_priv)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	unsigned long irqflags;
+-
+ 	dev_priv->irq_mask = ~0;
+ 
+ 	I915_WRITE(PORT_HOTPLUG_EN, 0);
+ 	POSTING_READ(PORT_HOTPLUG_EN);
+ 
+-	I915_WRITE(VLV_IMR, dev_priv->irq_mask);
+-	I915_WRITE(VLV_IER, ~dev_priv->irq_mask);
+ 	I915_WRITE(VLV_IIR, 0xffffffff);
+-	POSTING_READ(VLV_IER);
++	I915_WRITE(VLV_IIR, 0xffffffff);
++	I915_WRITE(VLV_IER, ~dev_priv->irq_mask);
++	I915_WRITE(VLV_IMR, dev_priv->irq_mask);
++	POSTING_READ(VLV_IMR);
+ 
+ 	/* Interrupt setup is already guaranteed to be single-threaded, this is
+ 	 * just to make the assert_spin_locked check happy. */
+-	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
++	spin_lock_irq(&dev_priv->irq_lock);
+ 	if (dev_priv->display_irqs_enabled)
+ 		valleyview_display_irqs_install(dev_priv);
+-	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
++	spin_unlock_irq(&dev_priv->irq_lock);
++}
+ 
+-	I915_WRITE(VLV_IIR, 0xffffffff);
+-	I915_WRITE(VLV_IIR, 0xffffffff);
++static int valleyview_irq_postinstall(struct drm_device *dev)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++
++	vlv_display_irq_postinstall(dev_priv);
+ 
+ 	gen5_gt_irq_postinstall(dev);
+ 
+@@ -3782,46 +3458,60 @@
+ 
+ static void gen8_gt_irq_postinstall(struct drm_i915_private *dev_priv)
+ {
+-	int i;
+-
+ 	/* These are interrupts we'll toggle with the ring mask register */
+ 	uint32_t gt_interrupts[] = {
+ 		GT_RENDER_USER_INTERRUPT << GEN8_RCS_IRQ_SHIFT |
++			GT_CONTEXT_SWITCH_INTERRUPT << GEN8_RCS_IRQ_SHIFT |
+ 			GT_RENDER_L3_PARITY_ERROR_INTERRUPT |
+-			GT_RENDER_USER_INTERRUPT << GEN8_BCS_IRQ_SHIFT,
++			GT_RENDER_USER_INTERRUPT << GEN8_BCS_IRQ_SHIFT |
++			GT_CONTEXT_SWITCH_INTERRUPT << GEN8_BCS_IRQ_SHIFT,
+ 		GT_RENDER_USER_INTERRUPT << GEN8_VCS1_IRQ_SHIFT |
+-			GT_RENDER_USER_INTERRUPT << GEN8_VCS2_IRQ_SHIFT,
++			GT_CONTEXT_SWITCH_INTERRUPT << GEN8_VCS1_IRQ_SHIFT |
++			GT_RENDER_USER_INTERRUPT << GEN8_VCS2_IRQ_SHIFT |
++			GT_CONTEXT_SWITCH_INTERRUPT << GEN8_VCS2_IRQ_SHIFT,
+ 		0,
+-		GT_RENDER_USER_INTERRUPT << GEN8_VECS_IRQ_SHIFT
++		GT_RENDER_USER_INTERRUPT << GEN8_VECS_IRQ_SHIFT |
++			GT_CONTEXT_SWITCH_INTERRUPT << GEN8_VECS_IRQ_SHIFT
+ 		};
+ 
+-	for (i = 0; i < ARRAY_SIZE(gt_interrupts); i++)
+-		GEN8_IRQ_INIT_NDX(GT, i, ~gt_interrupts[i], gt_interrupts[i]);
+-
+ 	dev_priv->pm_irq_mask = 0xffffffff;
++	GEN8_IRQ_INIT_NDX(GT, 0, ~gt_interrupts[0], gt_interrupts[0]);
++	GEN8_IRQ_INIT_NDX(GT, 1, ~gt_interrupts[1], gt_interrupts[1]);
++	GEN8_IRQ_INIT_NDX(GT, 2, dev_priv->pm_irq_mask, dev_priv->rps.pm_events);
++	GEN8_IRQ_INIT_NDX(GT, 3, ~gt_interrupts[3], gt_interrupts[3]);
+ }
+ 
+ static void gen8_de_irq_postinstall(struct drm_i915_private *dev_priv)
+ {
+-	struct drm_device *dev = dev_priv->dev;
+-	uint32_t de_pipe_masked = GEN8_PIPE_PRIMARY_FLIP_DONE |
+-		GEN8_PIPE_CDCLK_CRC_DONE |
+-		GEN8_DE_PIPE_IRQ_FAULT_ERRORS;
+-	uint32_t de_pipe_enables = de_pipe_masked | GEN8_PIPE_VBLANK |
+-		GEN8_PIPE_FIFO_UNDERRUN;
++	uint32_t de_pipe_masked = GEN8_PIPE_CDCLK_CRC_DONE;
++	uint32_t de_pipe_enables;
+ 	int pipe;
++	u32 aux_en = GEN8_AUX_CHANNEL_A;
++
++	if (IS_GEN9(dev_priv)) {
++		de_pipe_masked |= GEN9_PIPE_PLANE1_FLIP_DONE |
++				  GEN9_DE_PIPE_IRQ_FAULT_ERRORS;
++		aux_en |= GEN9_AUX_CHANNEL_B | GEN9_AUX_CHANNEL_C |
++			GEN9_AUX_CHANNEL_D;
++	} else
++		de_pipe_masked |= GEN8_PIPE_PRIMARY_FLIP_DONE |
++				  GEN8_DE_PIPE_IRQ_FAULT_ERRORS;
++
++	de_pipe_enables = de_pipe_masked | GEN8_PIPE_VBLANK |
++					   GEN8_PIPE_FIFO_UNDERRUN;
++
+ 	dev_priv->de_irq_mask[PIPE_A] = ~de_pipe_masked;
+ 	dev_priv->de_irq_mask[PIPE_B] = ~de_pipe_masked;
+ 	dev_priv->de_irq_mask[PIPE_C] = ~de_pipe_masked;
+ 
+-	for_each_pipe(pipe)
+-		if (intel_display_power_enabled(dev_priv,
++	for_each_pipe(dev_priv, pipe)
++		if (intel_display_power_is_enabled(dev_priv,
+ 				POWER_DOMAIN_PIPE(pipe)))
+ 			GEN8_IRQ_INIT_NDX(DE_PIPE, pipe,
+ 					  dev_priv->de_irq_mask[pipe],
+ 					  de_pipe_enables);
+ 
+-	GEN5_IRQ_INIT(GEN8_DE_PORT_, ~GEN8_AUX_CHANNEL_A, GEN8_AUX_CHANNEL_A);
++	GEN5_IRQ_INIT(GEN8_DE_PORT_, ~aux_en, aux_en);
+ }
+ 
+ static int gen8_irq_postinstall(struct drm_device *dev)
+@@ -3844,33 +3534,8 @@
+ static int cherryview_irq_postinstall(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	u32 enable_mask = I915_DISPLAY_PORT_INTERRUPT |
+-		I915_DISPLAY_PIPE_A_EVENT_INTERRUPT |
+-		I915_DISPLAY_PIPE_B_EVENT_INTERRUPT |
+-		I915_DISPLAY_PIPE_C_EVENT_INTERRUPT;
+-	u32 pipestat_enable = PLANE_FLIP_DONE_INT_STATUS_VLV |
+-		PIPE_CRC_DONE_INTERRUPT_STATUS;
+-	unsigned long irqflags;
+-	int pipe;
+-
+-	/*
+-	 * Leave vblank interrupts masked initially.  enable/disable will
+-	 * toggle them based on usage.
+-	 */
+-	dev_priv->irq_mask = ~enable_mask;
+-
+-	for_each_pipe(pipe)
+-		I915_WRITE(PIPESTAT(pipe), 0xffff);
+-
+-	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
+-	i915_enable_pipestat(dev_priv, PIPE_A, PIPE_GMBUS_INTERRUPT_STATUS);
+-	for_each_pipe(pipe)
+-		i915_enable_pipestat(dev_priv, pipe, pipestat_enable);
+-	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
+ 
+-	I915_WRITE(VLV_IIR, 0xffffffff);
+-	I915_WRITE(VLV_IMR, dev_priv->irq_mask);
+-	I915_WRITE(VLV_IER, enable_mask);
++	vlv_display_irq_postinstall(dev_priv);
+ 
+ 	gen8_gt_irq_postinstall(dev_priv);
+ 
+@@ -3890,41 +3555,39 @@
+ 	gen8_irq_reset(dev);
+ }
+ 
++static void vlv_display_irq_uninstall(struct drm_i915_private *dev_priv)
++{
++	/* Interrupt setup is already guaranteed to be single-threaded, this is
++	 * just to make the assert_spin_locked check happy. */
++	spin_lock_irq(&dev_priv->irq_lock);
++	if (dev_priv->display_irqs_enabled)
++		valleyview_display_irqs_uninstall(dev_priv);
++	spin_unlock_irq(&dev_priv->irq_lock);
++
++	vlv_display_irq_reset(dev_priv);
++
++	dev_priv->irq_mask = 0;
++}
++
+ static void valleyview_irq_uninstall(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	unsigned long irqflags;
+-	int pipe;
+ 
+ 	if (!dev_priv)
+ 		return;
+ 
+ 	I915_WRITE(VLV_MASTER_IER, 0);
+ 
+-	for_each_pipe(pipe)
+-		I915_WRITE(PIPESTAT(pipe), 0xffff);
+-
+-	I915_WRITE(HWSTAM, 0xffffffff);
+-	I915_WRITE(PORT_HOTPLUG_EN, 0);
+-	I915_WRITE(PORT_HOTPLUG_STAT, I915_READ(PORT_HOTPLUG_STAT));
+-
+-	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
+-	if (dev_priv->display_irqs_enabled)
+-		valleyview_display_irqs_uninstall(dev_priv);
+-	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
++	gen5_gt_irq_reset(dev);
+ 
+-	dev_priv->irq_mask = 0;
++	I915_WRITE(HWSTAM, ~0);
+ 
+-	I915_WRITE(VLV_IIR, 0xffffffff);
+-	I915_WRITE(VLV_IMR, 0xffffffff);
+-	I915_WRITE(VLV_IER, 0x0);
+-	POSTING_READ(VLV_IER);
++	vlv_display_irq_uninstall(dev_priv);
+ }
+ 
+ static void cherryview_irq_uninstall(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	int pipe;
+ 
+ 	if (!dev_priv)
+ 		return;
+@@ -3932,44 +3595,11 @@
+ 	I915_WRITE(GEN8_MASTER_IRQ, 0);
+ 	POSTING_READ(GEN8_MASTER_IRQ);
+ 
+-#define GEN8_IRQ_FINI_NDX(type, which)				\
+-do {								\
+-	I915_WRITE(GEN8_##type##_IMR(which), 0xffffffff);	\
+-	I915_WRITE(GEN8_##type##_IER(which), 0);		\
+-	I915_WRITE(GEN8_##type##_IIR(which), 0xffffffff);	\
+-	POSTING_READ(GEN8_##type##_IIR(which));			\
+-	I915_WRITE(GEN8_##type##_IIR(which), 0xffffffff);	\
+-} while (0)
+-
+-#define GEN8_IRQ_FINI(type)				\
+-do {							\
+-	I915_WRITE(GEN8_##type##_IMR, 0xffffffff);	\
+-	I915_WRITE(GEN8_##type##_IER, 0);		\
+-	I915_WRITE(GEN8_##type##_IIR, 0xffffffff);	\
+-	POSTING_READ(GEN8_##type##_IIR);		\
+-	I915_WRITE(GEN8_##type##_IIR, 0xffffffff);	\
+-} while (0)
+-
+-	GEN8_IRQ_FINI_NDX(GT, 0);
+-	GEN8_IRQ_FINI_NDX(GT, 1);
+-	GEN8_IRQ_FINI_NDX(GT, 2);
+-	GEN8_IRQ_FINI_NDX(GT, 3);
+-
+-	GEN8_IRQ_FINI(PCU);
+-
+-#undef GEN8_IRQ_FINI
+-#undef GEN8_IRQ_FINI_NDX
+-
+-	I915_WRITE(PORT_HOTPLUG_EN, 0);
+-	I915_WRITE(PORT_HOTPLUG_STAT, I915_READ(PORT_HOTPLUG_STAT));
++	gen8_gt_irq_reset(dev_priv);
+ 
+-	for_each_pipe(pipe)
+-		I915_WRITE(PIPESTAT(pipe), 0xffff);
++	GEN5_IRQ_RESET(GEN8_PCU_);
+ 
+-	I915_WRITE(VLV_IMR, 0xffffffff);
+-	I915_WRITE(VLV_IER, 0x0);
+-	I915_WRITE(VLV_IIR, 0xffffffff);
+-	POSTING_READ(VLV_IIR);
++	vlv_display_irq_uninstall(dev_priv);
+ }
+ 
+ static void ironlake_irq_uninstall(struct drm_device *dev)
+@@ -3987,7 +3617,7 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	int pipe;
+ 
+-	for_each_pipe(pipe)
++	for_each_pipe(dev_priv, pipe)
+ 		I915_WRITE(PIPESTAT(pipe), 0);
+ 	I915_WRITE16(IMR, 0xffff);
+ 	I915_WRITE16(IER, 0x0);
+@@ -3997,7 +3627,6 @@
+ static int i8xx_irq_postinstall(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	unsigned long irqflags;
+ 
+ 	I915_WRITE16(EMR,
+ 		     ~(I915_ERROR_PAGE_TABLE | I915_ERROR_MEMORY_REFRESH));
+@@ -4020,10 +3649,10 @@
+ 
+ 	/* Interrupt setup is already guaranteed to be single-threaded, this is
+ 	 * just to make the assert_spin_locked check happy. */
+-	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
++	spin_lock_irq(&dev_priv->irq_lock);
+ 	i915_enable_pipestat(dev_priv, PIPE_A, PIPE_CRC_DONE_INTERRUPT_STATUS);
+ 	i915_enable_pipestat(dev_priv, PIPE_B, PIPE_CRC_DONE_INTERRUPT_STATUS);
+-	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
++	spin_unlock_irq(&dev_priv->irq_lock);
+ 
+ 	return 0;
+ }
+@@ -4041,7 +3670,7 @@
+ 		return false;
+ 
+ 	if ((iir & flip_pending) == 0)
+-		return false;
++		goto check_page_flip;
+ 
+ 	intel_prepare_page_flip(dev, plane);
+ 
+@@ -4052,11 +3681,14 @@
+ 	 * an interrupt per se, we watch for the change at vblank.
+ 	 */
+ 	if (I915_READ16(ISR) & flip_pending)
+-		return false;
++		goto check_page_flip;
+ 
+ 	intel_finish_page_flip(dev, pipe);
+-
+ 	return true;
++
++check_page_flip:
++	intel_check_page_flip(dev, pipe);
++	return false;
+ }
+ 
+ static irqreturn_t i8xx_irq_handler(int irq, void *arg)
+@@ -4065,7 +3697,6 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	u16 iir, new_iir;
+ 	u32 pipe_stats[2];
+-	unsigned long irqflags;
+ 	int pipe;
+ 	u16 flip_mask =
+ 		I915_DISPLAY_PLANE_A_FLIP_PENDING_INTERRUPT |
+@@ -4081,13 +3712,13 @@
+ 		 * It doesn't set the bit in iir again, but it still produces
+ 		 * interrupts (for non-MSI).
+ 		 */
+-		spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
++		spin_lock(&dev_priv->irq_lock);
+ 		if (iir & I915_RENDER_COMMAND_PARSER_ERROR_INTERRUPT)
+-			i915_handle_error(dev, false,
++			i915_handle_error(dev, 0,
+ 					  "Command parser error, iir 0x%08x",
+ 					  iir);
+ 
+-		for_each_pipe(pipe) {
++		for_each_pipe(dev_priv, pipe) {
+ 			int reg = PIPESTAT(pipe);
+ 			pipe_stats[pipe] = I915_READ(reg);
+ 
+@@ -4097,17 +3728,15 @@
+ 			if (pipe_stats[pipe] & 0x8000ffff)
+ 				I915_WRITE(reg, pipe_stats[pipe]);
+ 		}
+-		spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
++		spin_unlock(&dev_priv->irq_lock);
+ 
+ 		I915_WRITE16(IIR, iir & ~flip_mask);
+ 		new_iir = I915_READ16(IIR); /* Flush posted writes */
+ 
+-		i915_update_dri1_breadcrumb(dev);
+-
+ 		if (iir & I915_USER_INTERRUPT)
+-			notify_ring(dev, &dev_priv->ring[RCS]);
++			notify_ring(dev, &dev_priv->engine[RCS]);
+ 
+-		for_each_pipe(pipe) {
++		for_each_pipe(dev_priv, pipe) {
+ 			int plane = pipe;
+ 			if (HAS_FBC(dev))
+ 				plane = !plane;
+@@ -4119,9 +3748,9 @@
+ 			if (pipe_stats[pipe] & PIPE_CRC_DONE_INTERRUPT_STATUS)
+ 				i9xx_pipe_crc_irq_handler(dev, pipe);
+ 
+-			if (pipe_stats[pipe] & PIPE_FIFO_UNDERRUN_STATUS &&
+-			    intel_set_cpu_fifo_underrun_reporting(dev, pipe, false))
+-				DRM_ERROR("pipe %c underrun\n", pipe_name(pipe));
++			if (pipe_stats[pipe] & PIPE_FIFO_UNDERRUN_STATUS)
++				intel_cpu_fifo_underrun_irq_handler(dev_priv,
++								    pipe);
+ 		}
+ 
+ 		iir = new_iir;
+@@ -4135,7 +3764,7 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	int pipe;
+ 
+-	for_each_pipe(pipe) {
++	for_each_pipe(dev_priv, pipe) {
+ 		/* Clear enable bits; then clear status bits */
+ 		I915_WRITE(PIPESTAT(pipe), 0);
+ 		I915_WRITE(PIPESTAT(pipe), I915_READ(PIPESTAT(pipe)));
+@@ -4155,8 +3784,8 @@
+ 		I915_WRITE(PORT_HOTPLUG_STAT, I915_READ(PORT_HOTPLUG_STAT));
+ 	}
+ 
+-	I915_WRITE16(HWSTAM, 0xeffe);
+-	for_each_pipe(pipe)
++	I915_WRITE16(HWSTAM, ~0);
++	for_each_pipe(dev_priv, pipe)
+ 		I915_WRITE(PIPESTAT(pipe), 0);
+ 	I915_WRITE(IMR, 0xffffffff);
+ 	I915_WRITE(IER, 0x0);
+@@ -4167,7 +3796,6 @@
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	u32 enable_mask;
+-	unsigned long irqflags;
+ 
+ 	I915_WRITE(EMR, ~(I915_ERROR_PAGE_TABLE | I915_ERROR_MEMORY_REFRESH));
+ 
+@@ -4205,10 +3833,10 @@
+ 
+ 	/* Interrupt setup is already guaranteed to be single-threaded, this is
+ 	 * just to make the assert_spin_locked check happy. */
+-	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
++	spin_lock_irq(&dev_priv->irq_lock);
+ 	i915_enable_pipestat(dev_priv, PIPE_A, PIPE_CRC_DONE_INTERRUPT_STATUS);
+ 	i915_enable_pipestat(dev_priv, PIPE_B, PIPE_CRC_DONE_INTERRUPT_STATUS);
+-	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
++	spin_unlock_irq(&dev_priv->irq_lock);
+ 
+ 	return 0;
+ }
+@@ -4226,7 +3854,7 @@
+ 		return false;
+ 
+ 	if ((iir & flip_pending) == 0)
+-		return false;
++		goto check_page_flip;
+ 
+ 	intel_prepare_page_flip(dev, plane);
+ 
+@@ -4237,11 +3865,14 @@
+ 	 * an interrupt per se, we watch for the change at vblank.
+ 	 */
+ 	if (I915_READ(ISR) & flip_pending)
+-		return false;
++		goto check_page_flip;
+ 
+ 	intel_finish_page_flip(dev, pipe);
+-
+ 	return true;
++
++check_page_flip:
++	intel_check_page_flip(dev, pipe);
++	return false;
+ }
+ 
+ static irqreturn_t i915_irq_handler(int irq, void *arg)
+@@ -4249,7 +3880,6 @@
+ 	struct drm_device *dev = arg;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	u32 iir, new_iir, pipe_stats[I915_MAX_PIPES];
+-	unsigned long irqflags;
+ 	u32 flip_mask =
+ 		I915_DISPLAY_PLANE_A_FLIP_PENDING_INTERRUPT |
+ 		I915_DISPLAY_PLANE_B_FLIP_PENDING_INTERRUPT;
+@@ -4265,13 +3895,13 @@
+ 		 * It doesn't set the bit in iir again, but it still produces
+ 		 * interrupts (for non-MSI).
+ 		 */
+-		spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
++		spin_lock(&dev_priv->irq_lock);
+ 		if (iir & I915_RENDER_COMMAND_PARSER_ERROR_INTERRUPT)
+-			i915_handle_error(dev, false,
++			i915_handle_error(dev, 0,
+ 					  "Command parser error, iir 0x%08x",
+ 					  iir);
+ 
+-		for_each_pipe(pipe) {
++		for_each_pipe(dev_priv, pipe) {
+ 			int reg = PIPESTAT(pipe);
+ 			pipe_stats[pipe] = I915_READ(reg);
+ 
+@@ -4281,7 +3911,7 @@
+ 				irq_received = true;
+ 			}
+ 		}
+-		spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
++		spin_unlock(&dev_priv->irq_lock);
+ 
+ 		if (!irq_received)
+ 			break;
+@@ -4295,9 +3925,9 @@
+ 		new_iir = I915_READ(IIR); /* Flush posted writes */
+ 
+ 		if (iir & I915_USER_INTERRUPT)
+-			notify_ring(dev, &dev_priv->ring[RCS]);
++			notify_ring(dev, &dev_priv->engine[RCS]);
+ 
+-		for_each_pipe(pipe) {
++		for_each_pipe(dev_priv, pipe) {
+ 			int plane = pipe;
+ 			if (HAS_FBC(dev))
+ 				plane = !plane;
+@@ -4312,9 +3942,9 @@
+ 			if (pipe_stats[pipe] & PIPE_CRC_DONE_INTERRUPT_STATUS)
+ 				i9xx_pipe_crc_irq_handler(dev, pipe);
+ 
+-			if (pipe_stats[pipe] & PIPE_FIFO_UNDERRUN_STATUS &&
+-			    intel_set_cpu_fifo_underrun_reporting(dev, pipe, false))
+-				DRM_ERROR("pipe %c underrun\n", pipe_name(pipe));
++			if (pipe_stats[pipe] & PIPE_FIFO_UNDERRUN_STATUS)
++				intel_cpu_fifo_underrun_irq_handler(dev_priv,
++								    pipe);
+ 		}
+ 
+ 		if (blc_event || (iir & I915_ASLE_INTERRUPT))
+@@ -4339,8 +3969,6 @@
+ 		iir = new_iir;
+ 	} while (iir & ~flip_mask);
+ 
+-	i915_update_dri1_breadcrumb(dev);
+-
+ 	return ret;
+ }
+ 
+@@ -4354,8 +3982,8 @@
+ 		I915_WRITE(PORT_HOTPLUG_STAT, I915_READ(PORT_HOTPLUG_STAT));
+ 	}
+ 
+-	I915_WRITE16(HWSTAM, 0xffff);
+-	for_each_pipe(pipe) {
++	I915_WRITE16(HWSTAM, ~0);
++	for_each_pipe(dev_priv, pipe) {
+ 		/* Clear enable bits; then clear status bits */
+ 		I915_WRITE(PIPESTAT(pipe), 0);
+ 		I915_WRITE(PIPESTAT(pipe), I915_READ(PIPESTAT(pipe)));
+@@ -4374,8 +4002,8 @@
+ 	I915_WRITE(PORT_HOTPLUG_EN, 0);
+ 	I915_WRITE(PORT_HOTPLUG_STAT, I915_READ(PORT_HOTPLUG_STAT));
+ 
+-	I915_WRITE(HWSTAM, 0xeffe);
+-	for_each_pipe(pipe)
++	I915_WRITE(HWSTAM, ~0);
++	for_each_pipe(dev_priv, pipe)
+ 		I915_WRITE(PIPESTAT(pipe), 0);
+ 	I915_WRITE(IMR, 0xffffffff);
+ 	I915_WRITE(IER, 0x0);
+@@ -4387,7 +4015,6 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	u32 enable_mask;
+ 	u32 error_mask;
+-	unsigned long irqflags;
+ 
+ 	/* Unmask the interrupts that we always want on. */
+ 	dev_priv->irq_mask = ~(I915_ASLE_INTERRUPT |
+@@ -4408,11 +4035,11 @@
+ 
+ 	/* Interrupt setup is already guaranteed to be single-threaded, this is
+ 	 * just to make the assert_spin_locked check happy. */
+-	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
++	spin_lock_irq(&dev_priv->irq_lock);
+ 	i915_enable_pipestat(dev_priv, PIPE_A, PIPE_GMBUS_INTERRUPT_STATUS);
+ 	i915_enable_pipestat(dev_priv, PIPE_A, PIPE_CRC_DONE_INTERRUPT_STATUS);
+ 	i915_enable_pipestat(dev_priv, PIPE_B, PIPE_CRC_DONE_INTERRUPT_STATUS);
+-	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
++	spin_unlock_irq(&dev_priv->irq_lock);
+ 
+ 	/*
+ 	 * Enable some error detection, note the instruction error mask
+@@ -4444,7 +4071,6 @@
+ static void i915_hpd_irq_setup(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct drm_mode_config *mode_config = &dev->mode_config;
+ 	struct intel_encoder *intel_encoder;
+ 	u32 hotplug_en;
+ 
+@@ -4455,7 +4081,7 @@
+ 		hotplug_en &= ~HOTPLUG_INT_EN_MASK;
+ 		/* Note HDMI and DP share hotplug bits */
+ 		/* enable bits are the same for all generations */
+-		list_for_each_entry(intel_encoder, &mode_config->encoder_list, base.head)
++		for_each_intel_encoder(dev, intel_encoder)
+ 			if (dev_priv->hpd_stats[intel_encoder->hpd_pin].hpd_mark == HPD_ENABLED)
+ 				hotplug_en |= hpd_mask_i915[intel_encoder->hpd_pin];
+ 		/* Programming the CRT detection parameters tends
+@@ -4478,7 +4104,6 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	u32 iir, new_iir;
+ 	u32 pipe_stats[I915_MAX_PIPES];
+-	unsigned long irqflags;
+ 	int ret = IRQ_NONE, pipe;
+ 	u32 flip_mask =
+ 		I915_DISPLAY_PLANE_A_FLIP_PENDING_INTERRUPT |
+@@ -4495,13 +4120,13 @@
+ 		 * It doesn't set the bit in iir again, but it still produces
+ 		 * interrupts (for non-MSI).
+ 		 */
+-		spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
++		spin_lock(&dev_priv->irq_lock);
+ 		if (iir & I915_RENDER_COMMAND_PARSER_ERROR_INTERRUPT)
+-			i915_handle_error(dev, false,
++			i915_handle_error(dev, 0,
+ 					  "Command parser error, iir 0x%08x",
+ 					  iir);
+ 
+-		for_each_pipe(pipe) {
++		for_each_pipe(dev_priv, pipe) {
+ 			int reg = PIPESTAT(pipe);
+ 			pipe_stats[pipe] = I915_READ(reg);
+ 
+@@ -4513,7 +4138,7 @@
+ 				irq_received = true;
+ 			}
+ 		}
+-		spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
++		spin_unlock(&dev_priv->irq_lock);
+ 
+ 		if (!irq_received)
+ 			break;
+@@ -4528,11 +4153,11 @@
+ 		new_iir = I915_READ(IIR); /* Flush posted writes */
+ 
+ 		if (iir & I915_USER_INTERRUPT)
+-			notify_ring(dev, &dev_priv->ring[RCS]);
++			notify_ring(dev, &dev_priv->engine[RCS]);
+ 		if (iir & I915_BSD_USER_INTERRUPT)
+-			notify_ring(dev, &dev_priv->ring[VCS]);
++			notify_ring(dev, &dev_priv->engine[VCS]);
+ 
+-		for_each_pipe(pipe) {
++		for_each_pipe(dev_priv, pipe) {
+ 			if (pipe_stats[pipe] & PIPE_START_VBLANK_INTERRUPT_STATUS &&
+ 			    i915_handle_vblank(dev, pipe, pipe, iir))
+ 				flip_mask &= ~DISPLAY_PLANE_FLIP_PENDING(pipe);
+@@ -4543,9 +4168,8 @@
+ 			if (pipe_stats[pipe] & PIPE_CRC_DONE_INTERRUPT_STATUS)
+ 				i9xx_pipe_crc_irq_handler(dev, pipe);
+ 
+-			if (pipe_stats[pipe] & PIPE_FIFO_UNDERRUN_STATUS &&
+-			    intel_set_cpu_fifo_underrun_reporting(dev, pipe, false))
+-				DRM_ERROR("pipe %c underrun\n", pipe_name(pipe));
++			if (pipe_stats[pipe] & PIPE_FIFO_UNDERRUN_STATUS)
++				intel_cpu_fifo_underrun_irq_handler(dev_priv, pipe);
+ 		}
+ 
+ 		if (blc_event || (iir & I915_ASLE_INTERRUPT))
+@@ -4572,8 +4196,6 @@
+ 		iir = new_iir;
+ 	}
+ 
+-	i915_update_dri1_breadcrumb(dev);
+-
+ 	return ret;
+ }
+ 
+@@ -4588,31 +4210,30 @@
+ 	I915_WRITE(PORT_HOTPLUG_EN, 0);
+ 	I915_WRITE(PORT_HOTPLUG_STAT, I915_READ(PORT_HOTPLUG_STAT));
+ 
+-	I915_WRITE(HWSTAM, 0xffffffff);
+-	for_each_pipe(pipe)
++	I915_WRITE(HWSTAM, ~0);
++	for_each_pipe(dev_priv, pipe)
+ 		I915_WRITE(PIPESTAT(pipe), 0);
+ 	I915_WRITE(IMR, 0xffffffff);
+ 	I915_WRITE(IER, 0x0);
+ 
+-	for_each_pipe(pipe)
++	for_each_pipe(dev_priv, pipe)
+ 		I915_WRITE(PIPESTAT(pipe),
+ 			   I915_READ(PIPESTAT(pipe)) & 0x8000ffff);
+ 	I915_WRITE(IIR, I915_READ(IIR));
+ }
+ 
+-static void intel_hpd_irq_reenable(struct work_struct *work)
++static void intel_hpd_irq_reenable_work(struct work_struct *work)
+ {
+ 	struct drm_i915_private *dev_priv =
+ 		container_of(work, typeof(*dev_priv),
+ 			     hotplug_reenable_work.work);
+ 	struct drm_device *dev = dev_priv->dev;
+ 	struct drm_mode_config *mode_config = &dev->mode_config;
+-	unsigned long irqflags;
+ 	int i;
+ 
+ 	intel_runtime_pm_get(dev_priv);
+ 
+-	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
++	spin_lock_irq(&dev_priv->irq_lock);
+ 	for (i = (HPD_NONE + 1); i < HPD_NUM_PINS; i++) {
+ 		struct drm_connector *connector;
+ 
+@@ -4636,43 +4257,46 @@
+ 	}
+ 	if (dev_priv->display.hpd_irq_setup)
+ 		dev_priv->display.hpd_irq_setup(dev);
+-	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
++	spin_unlock_irq(&dev_priv->irq_lock);
+ 
+ 	intel_runtime_pm_put(dev_priv);
+ }
+ 
+-void intel_irq_init(struct drm_device *dev)
++/**
++ * intel_irq_init - initializes irq support
++ * @dev_priv: i915 device instance
++ *
++ * This function initializes all the irq support including work items, timers
++ * and all the vtables. It does not setup the interrupt itself though.
++ */
++void intel_irq_init(struct drm_i915_private *dev_priv)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_device *dev = dev_priv->dev;
+ 
+-	INIT_WORK(&dev_priv->hotplug_work, i915_hotplug_work_func);
++	INIT_DELAYED_WORK(&dev_priv->hotplug_work, i915_hotplug_work_func);
+ 	INIT_WORK(&dev_priv->dig_port_work, i915_digport_work_func);
+ 	INIT_WORK(&dev_priv->gpu_error.work, i915_error_work_func);
+ 	INIT_WORK(&dev_priv->rps.work, gen6_pm_rps_work);
+ 	INIT_WORK(&dev_priv->l3_parity.error_work, ivybridge_parity_work);
+ 
+ 	/* Let's track the enabled rps events */
+-	if (IS_VALLEYVIEW(dev))
+-		/* WaGsvRC0ResidenncyMethod:VLV */
+-		dev_priv->pm_rps_events = GEN6_PM_RP_UP_EI_EXPIRED;
++	if (IS_VALLEYVIEW(dev_priv) && !IS_CHERRYVIEW(dev_priv))
++		/* WaGsvRC0ResidencyMethod:vlv */
++		dev_priv->rps.pm_events = GEN6_PM_RP_DOWN_EI_EXPIRED | GEN6_PM_RP_UP_EI_EXPIRED;
+ 	else
+-		dev_priv->pm_rps_events = GEN6_PM_RPS_EVENTS;
++		dev_priv->rps.pm_events = GEN6_PM_RPS_EVENTS;
+ 
+-	setup_timer(&dev_priv->gpu_error.hangcheck_timer,
+-		    i915_hangcheck_elapsed,
+-		    (unsigned long) dev);
++	INIT_DELAYED_WORK(&dev_priv->gpu_error.hangcheck_work,
++			  i915_hangcheck_elapsed);
+ 	INIT_DELAYED_WORK(&dev_priv->hotplug_reenable_work,
+-			  intel_hpd_irq_reenable);
++			  intel_hpd_irq_reenable_work);
+ 
+ 	pm_qos_add_request(&dev_priv->pm_qos, PM_QOS_CPU_DMA_LATENCY, PM_QOS_DEFAULT_VALUE);
+ 
+-	/* Haven't installed the IRQ handler yet */
+-	dev_priv->pm._irqs_disabled = true;
+-
+-	if (IS_GEN2(dev)) {
++	if (IS_GEN2(dev_priv)) {
+ 		dev->max_vblank_count = 0;
+ 		dev->driver->get_vblank_counter = i8xx_get_vblank_counter;
+-	} else if (IS_G4X(dev) || INTEL_INFO(dev)->gen >= 5) {
++	} else if (IS_G4X(dev_priv) || INTEL_INFO(dev_priv)->gen >= 5) {
+ 		dev->max_vblank_count = 0xffffffff; /* full 32 bit counter */
+ 		dev->driver->get_vblank_counter = gm45_get_vblank_counter;
+ 	} else {
+@@ -4680,12 +4304,20 @@
+ 		dev->max_vblank_count = 0xffffff; /* only 24 bits of frame count */
+ 	}
+ 
++	/*
++	 * Opt out of the vblank disable timer on everything except gen2.
++	 * Gen2 doesn't have a hardware frame counter and so depends on
++	 * vblank interrupts to produce sane vblank seuquence numbers.
++	 */
++	if (!IS_GEN2(dev_priv))
++		dev->vblank_disable_immediate = true;
++
+ 	if (drm_core_check_feature(dev, DRIVER_MODESET)) {
+ 		dev->driver->get_vblank_timestamp = i915_get_vblank_timestamp;
+ 		dev->driver->get_scanout_position = i915_get_crtc_scanoutpos;
+ 	}
+ 
+-	if (IS_CHERRYVIEW(dev)) {
++	if (IS_CHERRYVIEW(dev_priv)) {
+ 		dev->driver->irq_handler = cherryview_irq_handler;
+ 		dev->driver->irq_preinstall = cherryview_irq_preinstall;
+ 		dev->driver->irq_postinstall = cherryview_irq_postinstall;
+@@ -4693,7 +4325,7 @@
+ 		dev->driver->enable_vblank = valleyview_enable_vblank;
+ 		dev->driver->disable_vblank = valleyview_disable_vblank;
+ 		dev_priv->display.hpd_irq_setup = i915_hpd_irq_setup;
+-	} else if (IS_VALLEYVIEW(dev)) {
++	} else if (IS_VALLEYVIEW(dev_priv)) {
+ 		dev->driver->irq_handler = valleyview_irq_handler;
+ 		dev->driver->irq_preinstall = valleyview_irq_preinstall;
+ 		dev->driver->irq_postinstall = valleyview_irq_postinstall;
+@@ -4701,7 +4333,7 @@
+ 		dev->driver->enable_vblank = valleyview_enable_vblank;
+ 		dev->driver->disable_vblank = valleyview_disable_vblank;
+ 		dev_priv->display.hpd_irq_setup = i915_hpd_irq_setup;
+-	} else if (IS_GEN8(dev)) {
++	} else if (INTEL_INFO(dev_priv)->gen >= 8) {
+ 		dev->driver->irq_handler = gen8_irq_handler;
+ 		dev->driver->irq_preinstall = gen8_irq_reset;
+ 		dev->driver->irq_postinstall = gen8_irq_postinstall;
+@@ -4718,12 +4350,12 @@
+ 		dev->driver->disable_vblank = ironlake_disable_vblank;
+ 		dev_priv->display.hpd_irq_setup = ibx_hpd_irq_setup;
+ 	} else {
+-		if (INTEL_INFO(dev)->gen == 2) {
++		if (INTEL_INFO(dev_priv)->gen == 2) {
+ 			dev->driver->irq_preinstall = i8xx_irq_preinstall;
+ 			dev->driver->irq_postinstall = i8xx_irq_postinstall;
+ 			dev->driver->irq_handler = i8xx_irq_handler;
+ 			dev->driver->irq_uninstall = i8xx_irq_uninstall;
+-		} else if (INTEL_INFO(dev)->gen == 3) {
++		} else if (INTEL_INFO(dev_priv)->gen == 3) {
+ 			dev->driver->irq_preinstall = i915_irq_preinstall;
+ 			dev->driver->irq_postinstall = i915_irq_postinstall;
+ 			dev->driver->irq_uninstall = i915_irq_uninstall;
+@@ -4741,12 +4373,23 @@
+ 	}
+ }
+ 
+-void intel_hpd_init(struct drm_device *dev)
++/**
++ * intel_hpd_init - initializes and enables hpd support
++ * @dev_priv: i915 device instance
++ *
++ * This function enables the hotplug support. It requires that interrupts have
++ * already been enabled with intel_irq_init_hw(). From this point on hotplug and
++ * poll request can run concurrently to other code, so locking rules must be
++ * obeyed.
++ *
++ * This is a separate step from interrupt enabling to simplify the locking rules
++ * in the driver load and resume code.
++ */
++void intel_hpd_init(struct drm_i915_private *dev_priv)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_device *dev = dev_priv->dev;
+ 	struct drm_mode_config *mode_config = &dev->mode_config;
+ 	struct drm_connector *connector;
+-	unsigned long irqflags;
+ 	int i;
+ 
+ 	for (i = 1; i < HPD_NUM_PINS; i++) {
+@@ -4764,27 +4407,72 @@
+ 
+ 	/* Interrupt setup is already guaranteed to be single-threaded, this is
+ 	 * just to make the assert_spin_locked checks happy. */
+-	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
++	spin_lock_irq(&dev_priv->irq_lock);
+ 	if (dev_priv->display.hpd_irq_setup)
+ 		dev_priv->display.hpd_irq_setup(dev);
+-	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
++	spin_unlock_irq(&dev_priv->irq_lock);
+ }
+ 
+-/* Disable interrupts so we can allow runtime PM. */
+-void intel_runtime_pm_disable_interrupts(struct drm_device *dev)
++/**
++ * intel_irq_install - enables the hardware interrupt
++ * @dev_priv: i915 device instance
++ *
++ * This function enables the hardware interrupt handling, but leaves the hotplug
++ * handling still disabled. It is called after intel_irq_init().
++ *
++ * In the driver load and resume code we need working interrupts in a few places
++ * but don't want to deal with the hassle of concurrent probe and hotplug
++ * workers. Hence the split into this two-stage approach.
++ */
++int intel_irq_install(struct drm_i915_private *dev_priv)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	/*
++	 * We enable some interrupt sources in our postinstall hooks, so mark
++	 * interrupts as enabled _before_ actually enabling them to avoid
++	 * special cases in our ordering checks.
++	 */
++	dev_priv->pm.irqs_enabled = true;
+ 
+-	dev->driver->irq_uninstall(dev);
+-	dev_priv->pm._irqs_disabled = true;
++	return drm_irq_install(dev_priv->dev, dev_priv->dev->pdev->irq);
+ }
+ 
+-/* Restore interrupts so we can recover from runtime PM. */
+-void intel_runtime_pm_restore_interrupts(struct drm_device *dev)
++/**
++ * intel_irq_uninstall - finilizes all irq handling
++ * @dev_priv: i915 device instance
++ *
++ * This stops interrupt and hotplug handling and unregisters and frees all
++ * resources acquired in the init functions.
++ */
++void intel_irq_uninstall(struct drm_i915_private *dev_priv)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	drm_irq_uninstall(dev_priv->dev);
++	intel_hpd_cancel_work(dev_priv);
++	dev_priv->pm.irqs_enabled = false;
++}
++
++/**
++ * intel_runtime_pm_disable_interrupts - runtime interrupt disabling
++ * @dev_priv: i915 device instance
++ *
++ * This function is used to disable interrupts at runtime, both in the runtime
++ * pm and the system suspend/resume code.
++ */
++void intel_runtime_pm_disable_interrupts(struct drm_i915_private *dev_priv)
++{
++	dev_priv->dev->driver->irq_uninstall(dev_priv->dev);
++	dev_priv->pm.irqs_enabled = false;
++}
+ 
+-	dev_priv->pm._irqs_disabled = false;
+-	dev->driver->irq_preinstall(dev);
+-	dev->driver->irq_postinstall(dev);
++/**
++ * intel_runtime_pm_enable_interrupts - runtime interrupt enabling
++ * @dev_priv: i915 device instance
++ *
++ * This function is used to enable interrupts at runtime, both in the runtime
++ * pm and the system suspend/resume code.
++ */
++void intel_runtime_pm_enable_interrupts(struct drm_i915_private *dev_priv)
++{
++	dev_priv->pm.irqs_enabled = true;
++	dev_priv->dev->driver->irq_preinstall(dev_priv->dev);
++	dev_priv->dev->driver->irq_postinstall(dev_priv->dev);
+ }
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_params.c b/drivers/gpu/drm/i915/i915_params.c
+--- a/drivers/gpu/drm/i915/i915_params.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/i915_params.c	2014-11-21 08:30:58.502638009 -0700
+@@ -24,7 +24,7 @@
+ 
+ #include "i915_drv.h"
+ 
+-struct i915_params i915 __read_mostly = {
++struct i915_module_parameters i915_module __read_mostly = {
+ 	.modeset = -1,
+ 	.panel_ignore_lid = 1,
+ 	.powersave = 1,
+@@ -35,6 +35,7 @@
+ 	.vbt_sdvo_panel_type = -1,
+ 	.enable_rc6 = -1,
+ 	.enable_fbc = -1,
++	.enable_execlists = 0,
+ 	.enable_hangcheck = true,
+ 	.enable_ppgtt = -1,
+ 	.enable_psr = 0,
+@@ -52,26 +53,26 @@
+ 	.mmio_debug = 0,
+ };
+ 
+-module_param_named(modeset, i915.modeset, int, 0400);
++module_param_named(modeset, i915_module.modeset, int, 0400);
+ MODULE_PARM_DESC(modeset,
+ 	"Use kernel modesetting [KMS] (0=DRM_I915_KMS from .config, "
+ 	"1=on, -1=force vga console preference [default])");
+ 
+-module_param_named(panel_ignore_lid, i915.panel_ignore_lid, int, 0600);
++module_param_named(panel_ignore_lid, i915_module.panel_ignore_lid, int, 0600);
+ MODULE_PARM_DESC(panel_ignore_lid,
+ 	"Override lid status (0=autodetect, 1=autodetect disabled [default], "
+ 	"-1=force lid closed, -2=force lid open)");
+ 
+-module_param_named(powersave, i915.powersave, int, 0600);
++module_param_named(powersave, i915_module.powersave, int, 0600);
+ MODULE_PARM_DESC(powersave,
+ 	"Enable powersavings, fbc, downclocking, etc. (default: true)");
+ 
+-module_param_named(semaphores, i915.semaphores, int, 0400);
++module_param_named(semaphores, i915_module.semaphores, int, 0400);
+ MODULE_PARM_DESC(semaphores,
+ 	"Use semaphores for inter-ring sync "
+ 	"(default: -1 (use per-chip defaults))");
+ 
+-module_param_named(enable_rc6, i915.enable_rc6, int, 0400);
++module_param_named(enable_rc6, i915_module.enable_rc6, int, 0400);
+ MODULE_PARM_DESC(enable_rc6,
+ 	"Enable power-saving render C-state 6. "
+ 	"Different stages can be selected via bitmask values "
+@@ -79,69 +80,74 @@
+ 	"For example, 3 would enable rc6 and deep rc6, and 7 would enable everything. "
+ 	"default: -1 (use per-chip default)");
+ 
+-module_param_named(enable_fbc, i915.enable_fbc, int, 0600);
++module_param_named(enable_fbc, i915_module.enable_fbc, int, 0600);
+ MODULE_PARM_DESC(enable_fbc,
+ 	"Enable frame buffer compression for power savings "
+ 	"(default: -1 (use per-chip default))");
+ 
+-module_param_named(lvds_downclock, i915.lvds_downclock, int, 0400);
++module_param_named(lvds_downclock, i915_module.lvds_downclock, int, 0400);
+ MODULE_PARM_DESC(lvds_downclock,
+ 	"Use panel (LVDS/eDP) downclocking for power savings "
+ 	"(default: false)");
+ 
+-module_param_named(lvds_channel_mode, i915.lvds_channel_mode, int, 0600);
++module_param_named(lvds_channel_mode, i915_module.lvds_channel_mode, int, 0600);
+ MODULE_PARM_DESC(lvds_channel_mode,
+ 	 "Specify LVDS channel mode "
+ 	 "(0=probe BIOS [default], 1=single-channel, 2=dual-channel)");
+ 
+-module_param_named(lvds_use_ssc, i915.panel_use_ssc, int, 0600);
++module_param_named(lvds_use_ssc, i915_module.panel_use_ssc, int, 0600);
+ MODULE_PARM_DESC(lvds_use_ssc,
+ 	"Use Spread Spectrum Clock with panels [LVDS/eDP] "
+ 	"(default: auto from VBT)");
+ 
+-module_param_named(vbt_sdvo_panel_type, i915.vbt_sdvo_panel_type, int, 0600);
++module_param_named(vbt_sdvo_panel_type, i915_module.vbt_sdvo_panel_type, int, 0600);
+ MODULE_PARM_DESC(vbt_sdvo_panel_type,
+ 	"Override/Ignore selection of SDVO panel mode in the VBT "
+ 	"(-2=ignore, -1=auto [default], index in VBT BIOS table)");
+ 
+-module_param_named(reset, i915.reset, bool, 0600);
++module_param_named(reset, i915_module.reset, bool, 0600);
+ MODULE_PARM_DESC(reset, "Attempt GPU resets (default: true)");
+ 
+-module_param_named(enable_hangcheck, i915.enable_hangcheck, bool, 0644);
++module_param_named(enable_hangcheck, i915_module.enable_hangcheck, bool, 0644);
+ MODULE_PARM_DESC(enable_hangcheck,
+ 	"Periodically check GPU activity for detecting hangs. "
+ 	"WARNING: Disabling this can cause system wide hangs. "
+ 	"(default: true)");
+ 
+-module_param_named(enable_ppgtt, i915.enable_ppgtt, int, 0400);
++module_param_named(enable_ppgtt, i915_module.enable_ppgtt, int, 0400);
+ MODULE_PARM_DESC(enable_ppgtt,
+ 	"Override PPGTT usage. "
+ 	"(-1=auto [default], 0=disabled, 1=aliasing, 2=full)");
+ 
+-module_param_named(enable_psr, i915.enable_psr, int, 0600);
++module_param_named(enable_execlists, i915_module.enable_execlists, int, 0400);
++MODULE_PARM_DESC(enable_execlists,
++	"Override execlists usage. "
++	"(-1=auto, 0=disabled [default], 1=enabled)");
++
++module_param_named(enable_psr, i915_module.enable_psr, int, 0600);
+ MODULE_PARM_DESC(enable_psr, "Enable PSR (default: false)");
+ 
+-module_param_named(preliminary_hw_support, i915.preliminary_hw_support, int, 0600);
++module_param_named(preliminary_hw_support, i915_module.preliminary_hw_support, int, 0600);
+ MODULE_PARM_DESC(preliminary_hw_support,
+ 	"Enable preliminary hardware support.");
+ 
+-module_param_named(disable_power_well, i915.disable_power_well, int, 0600);
++module_param_named(disable_power_well, i915_module.disable_power_well, int, 0600);
+ MODULE_PARM_DESC(disable_power_well,
+ 	"Disable the power well when possible (default: true)");
+ 
+-module_param_named(enable_ips, i915.enable_ips, int, 0600);
++module_param_named(enable_ips, i915_module.enable_ips, int, 0600);
+ MODULE_PARM_DESC(enable_ips, "Enable IPS (default: true)");
+ 
+-module_param_named(fastboot, i915.fastboot, bool, 0600);
++module_param_named(fastboot, i915_module.fastboot, bool, 0600);
+ MODULE_PARM_DESC(fastboot,
+ 	"Try to skip unnecessary mode sets at boot time (default: false)");
+ 
+-module_param_named(prefault_disable, i915.prefault_disable, bool, 0600);
++module_param_named(prefault_disable, i915_module.prefault_disable, bool, 0600);
+ MODULE_PARM_DESC(prefault_disable,
+ 	"Disable page prefaulting for pread/pwrite/reloc (default:false). "
+ 	"For developers only.");
+ 
+-module_param_named(invert_brightness, i915.invert_brightness, int, 0600);
++module_param_named(invert_brightness, i915_module.invert_brightness, int, 0600);
+ MODULE_PARM_DESC(invert_brightness,
+ 	"Invert backlight brightness "
+ 	"(-1 force normal, 0 machine defaults, 1 force inversion), please "
+@@ -149,21 +155,21 @@
+ 	"to dri-devel@lists.freedesktop.org, if your machine needs it. "
+ 	"It will then be included in an upcoming module version.");
+ 
+-module_param_named(disable_display, i915.disable_display, bool, 0600);
++module_param_named(disable_display, i915_module.disable_display, bool, 0600);
+ MODULE_PARM_DESC(disable_display, "Disable display (default: false)");
+ 
+-module_param_named(disable_vtd_wa, i915.disable_vtd_wa, bool, 0600);
++module_param_named(disable_vtd_wa, i915_module.disable_vtd_wa, bool, 0600);
+ MODULE_PARM_DESC(disable_vtd_wa, "Disable all VT-d workarounds (default: false)");
+ 
+-module_param_named(enable_cmd_parser, i915.enable_cmd_parser, int, 0600);
++module_param_named(enable_cmd_parser, i915_module.enable_cmd_parser, int, 0600);
+ MODULE_PARM_DESC(enable_cmd_parser,
+ 		 "Enable command parsing (1=enabled [default], 0=disabled)");
+ 
+-module_param_named(use_mmio_flip, i915.use_mmio_flip, int, 0600);
++module_param_named(use_mmio_flip, i915_module.use_mmio_flip, int, 0600);
+ MODULE_PARM_DESC(use_mmio_flip,
+ 		 "use MMIO flips (-1=never, 0=driver discretion [default], 1=always)");
+ 
+-module_param_named(mmio_debug, i915.mmio_debug, bool, 0600);
++module_param_named(mmio_debug, i915_module.mmio_debug, bool, 0600);
+ MODULE_PARM_DESC(mmio_debug,
+ 	"Enable the MMIO debug code (default: false). This may negatively "
+ 	"affect performance.");
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_perf.c b/drivers/gpu/drm/i915/i915_perf.c
+--- a/drivers/gpu/drm/i915/i915_perf.c	1969-12-31 17:00:00.000000000 -0700
++++ b/drivers/gpu/drm/i915/i915_perf.c	2014-11-20 09:53:37.972762837 -0700
+@@ -0,0 +1,567 @@
++#include <linux/perf_event.h>
++#include <linux/pm_runtime.h>
++
++#include "i915_drv.h"
++#include "intel_ringbuffer.h"
++
++#define FREQUENCY 200
++#define PERIOD max_t(u64, 10000, NSEC_PER_SEC / FREQUENCY)
++
++#define RING_MASK 0xffffffff
++#define RING_MAX 32
++
++#define INSTDONE_ENABLE 0x8
++
++static bool gpu_active(struct drm_i915_private *i915)
++{
++	struct intel_engine_cs *engine;
++	int i;
++
++	if (!pm_runtime_active(&i915->dev->pdev->dev))
++		return false;
++
++	for_each_engine(engine, i915, i) {
++		if (engine->last_request == NULL)
++			continue;
++
++		if (!i915_request_complete(engine->last_request))
++			return true;
++	}
++
++	return false;
++}
++
++static void engines_sample(struct drm_i915_private *dev_priv)
++{
++	struct intel_engine_cs *engine;
++	int i;
++
++	if ((dev_priv->pmu.enable & RING_MASK) == 0)
++		return;
++
++	if (!gpu_active(dev_priv))
++		return;
++
++	if (dev_priv->info.gen >= 6)
++		gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
++
++	for_each_engine(engine, dev_priv, i) {
++		u32 head, tail, ctrl;
++
++		if ((dev_priv->pmu.enable & (0x7 << (4*i))) == 0)
++			continue;
++
++		if (engine->last_request == NULL)
++			continue;
++
++		head = I915_READ_NOTRACE(RING_HEAD((engine)->mmio_base));
++		tail = I915_READ_NOTRACE(RING_TAIL((engine)->mmio_base));
++		ctrl = I915_READ_NOTRACE(RING_CTL((engine)->mmio_base));
++
++		if ((head ^ tail) & HEAD_ADDR)
++			engine->pmu_sample[I915_SAMPLE_BUSY] += PERIOD;
++
++		if (ctrl & ((dev_priv->info.gen == 2) ? RING_WAIT_I8XX : RING_WAIT))
++			engine->pmu_sample[I915_SAMPLE_WAIT] += PERIOD;
++
++		if (ctrl & RING_WAIT_SEMAPHORE)
++			engine->pmu_sample[I915_SAMPLE_SEMA] += PERIOD;
++	}
++
++	if (dev_priv->pmu.enable & INSTDONE_ENABLE) {
++		u64 instdone;
++
++		if (dev_priv->info.gen < 4) {
++			instdone = I915_READ_NOTRACE(INSTDONE);
++		} else if (dev_priv->info.gen < 7) {
++			instdone  = I915_READ_NOTRACE(INSTDONE_I965);
++			instdone |= (u64)I915_READ_NOTRACE(INSTDONE1) << 32;
++		} else {
++			instdone  = I915_READ_NOTRACE(GEN7_INSTDONE_1);
++			instdone |= (u64)(I915_READ_NOTRACE(GEN7_SC_INSTDONE) & 0xff) << 32;
++			instdone |= (u64)(I915_READ_NOTRACE(GEN7_SAMPLER_INSTDONE) & 0xff) << 40;
++			instdone |= (u64)(I915_READ_NOTRACE(GEN7_ROW_INSTDONE) & 0xff) << 48;
++		}
++
++		for (instdone = ~instdone & dev_priv->pmu.instdone, i = 0;
++		     instdone;
++		     instdone >>= 1, i++) {
++			if ((instdone & 1) == 0)
++				continue;
++
++			dev_priv->pmu.sample[__I915_SAMPLE_INSTDONE_0 + i] += PERIOD;
++		}
++	}
++
++	if (dev_priv->info.gen >= 6)
++		gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
++}
++
++static void frequency_sample(struct drm_i915_private *dev_priv)
++{
++	if (dev_priv->pmu.enable & ((u64)1 << I915_PERF_ACTUAL_FREQUENCY)) {
++		u64 val;
++
++		if (gpu_active(dev_priv)) {
++			if (dev_priv->info.is_valleyview) {
++				mutex_lock(&dev_priv->rps.hw_lock);
++				val = vlv_punit_read(dev_priv, PUNIT_REG_GPU_FREQ_STS);
++				mutex_unlock(&dev_priv->rps.hw_lock);
++				val = vlv_gpu_freq(dev_priv, (val >> 8) & 0xff);
++			} else {
++				val = I915_READ_NOTRACE(GEN6_RPSTAT1);
++				if (dev_priv->info.is_haswell)
++					val = (val & HSW_CAGF_MASK) >> HSW_CAGF_SHIFT;
++				else
++					val = (val & GEN6_CAGF_MASK) >> GEN6_CAGF_SHIFT;
++				val *= GT_FREQUENCY_MULTIPLIER;
++			}
++		} else {
++			val = dev_priv->rps.cur_freq; /* minor white lie to save power */
++			if (dev_priv->info.is_valleyview)
++				val = vlv_gpu_freq(dev_priv, val);
++			else
++				val *= GT_FREQUENCY_MULTIPLIER;
++		}
++
++		dev_priv->pmu.sample[__I915_SAMPLE_FREQ_ACT] += val * PERIOD;
++	}
++
++	if (dev_priv->pmu.enable & ((u64)1 << I915_PERF_REQUESTED_FREQUENCY)) {
++		u64 val = dev_priv->rps.cur_freq;
++		if (dev_priv->info.is_valleyview)
++			val = vlv_gpu_freq(dev_priv, val);
++		else
++			val *= GT_FREQUENCY_MULTIPLIER;
++		dev_priv->pmu.sample[__I915_SAMPLE_FREQ_REQ] += val * PERIOD;
++	}
++}
++
++static enum hrtimer_restart i915_sample(struct hrtimer *hrtimer)
++{
++	struct drm_i915_private *i915 =
++		container_of(hrtimer, struct drm_i915_private, pmu.timer);
++
++	if (i915->pmu.enable == 0)
++		return HRTIMER_NORESTART;
++
++	engines_sample(i915);
++	frequency_sample(i915);
++
++	hrtimer_forward_now(hrtimer, ns_to_ktime(PERIOD));
++	return HRTIMER_RESTART;
++}
++
++static void i915_perf_event_destroy(struct perf_event *event)
++{
++	WARN_ON(event->parent);
++}
++
++static int engine_event_init(struct perf_event *event)
++{
++	struct drm_i915_private *i915 =
++		container_of(event->pmu, typeof(*i915), pmu.base);
++	int engine = event->attr.config >> 2;
++	int sample = event->attr.config & 3;
++
++	switch (sample) {
++	case I915_SAMPLE_BUSY:
++	case I915_SAMPLE_WAIT:
++		break;
++	case I915_SAMPLE_SEMA:
++		if (i915->info.gen < 6)
++			return -ENODEV;
++		break;
++	default:
++		return -ENOENT;
++	}
++
++	if (engine >= I915_NUM_ENGINES)
++		return -ENOENT;
++
++	if (!intel_engine_initialized(&i915->engine[engine]))
++		return -ENODEV;
++
++	return 0;
++}
++
++static enum hrtimer_restart hrtimer_sample(struct hrtimer *hrtimer)
++{
++	struct pt_regs *regs;
++	struct perf_sample_data data;
++	struct perf_event *event;
++	u64 period;
++
++	event = container_of(hrtimer, struct perf_event, hw.hrtimer);
++	if (event->state != PERF_EVENT_STATE_ACTIVE)
++		return HRTIMER_NORESTART;
++
++	event->pmu->read(event);
++
++	perf_sample_data_init(&data, 0, event->hw.last_period);
++	regs = get_irq_regs();
++
++	perf_event_overflow(event, &data, NULL);
++
++	period = max_t(u64, 10000, event->hw.sample_period);
++	hrtimer_forward_now(hrtimer, ns_to_ktime(period));
++	return HRTIMER_RESTART;
++}
++
++static void init_hrtimer(struct perf_event *event)
++{
++	struct hw_perf_event *hwc = &event->hw;
++
++	printk(KERN_ERR "%s %d, is-sampling-event? %d\n", __func__, (int)event->attr.config, is_sampling_event(event));
++
++	if (!is_sampling_event(event))
++		return;
++
++	hrtimer_init(&hwc->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
++	hwc->hrtimer.function = hrtimer_sample;
++
++	if (event->attr.freq) {
++		long freq = event->attr.sample_freq;
++
++		event->attr.sample_period = NSEC_PER_SEC / freq;
++		hwc->sample_period = event->attr.sample_period;
++		local64_set(&hwc->period_left, hwc->sample_period);
++		hwc->last_period = hwc->sample_period;
++		event->attr.freq = 0;
++	}
++}
++
++static int i915_perf_event_init(struct perf_event *event)
++{
++	struct drm_i915_private *i915 =
++		container_of(event->pmu, typeof(*i915), pmu.base);
++	int ret;
++
++	/* XXX ideally only want pid == -1 && cpu == -1 */
++
++	if (event->attr.type != event->pmu->type)
++		return -ENOENT;
++
++	if (has_branch_stack(event))
++		return -EOPNOTSUPP;
++
++	ret = 0;
++	if (event->attr.config < RING_MAX) {
++		ret = engine_event_init(event);
++	} else switch (event->attr.config) {
++	case I915_PERF_ACTUAL_FREQUENCY:
++	case I915_PERF_REQUESTED_FREQUENCY:
++	case I915_PERF_ENERGY:
++	case I915_PERF_RC6_RESIDENCY:
++	case I915_PERF_RC6p_RESIDENCY:
++	case I915_PERF_RC6pp_RESIDENCY:
++		if (i915->info.gen < 6)
++			ret = -ENODEV;
++		break;
++	case I915_PERF_STATISTIC_0...I915_PERF_STATISTIC_8:
++		if (i915->info.gen < 4)
++			ret = -ENODEV;
++		break;
++	case I915_PERF_INSTDONE_0...I915_PERF_INSTDONE_63:
++		if (i915->info.gen < 4 &&
++		    event->attr.config - I915_PERF_INSTDONE_0 >= 32) {
++			ret = -ENODEV;
++			break;
++		}
++	}
++	if (ret)
++		return ret;
++
++	if (!event->parent) {
++		event->destroy = i915_perf_event_destroy;
++	}
++
++	init_hrtimer(event);
++
++	return 0;
++}
++
++static inline bool is_instdone_event(struct perf_event *event)
++{
++	return (event->attr.config >= I915_PERF_INSTDONE_0 &&
++		event->attr.config <= I915_PERF_INSTDONE_63);
++}
++
++static void i915_perf_timer_start(struct perf_event *event)
++{
++	struct hw_perf_event *hwc = &event->hw;
++	s64 period;
++
++	if (!is_sampling_event(event))
++		return;
++
++	period = local64_read(&hwc->period_left);
++	if (period) {
++		if (period < 0)
++			period = 10000;
++
++		local64_set(&hwc->period_left, 0);
++	} else {
++		period = max_t(u64, 10000, hwc->sample_period);
++	}
++
++	__hrtimer_start_range_ns(&hwc->hrtimer,
++				 ns_to_ktime(period), 0,
++				 HRTIMER_MODE_REL_PINNED, 0);
++}
++
++static void i915_perf_timer_cancel(struct perf_event *event)
++{
++	struct hw_perf_event *hwc = &event->hw;
++
++	if (!is_sampling_event(event))
++		return;
++
++	local64_set(&hwc->period_left,
++		    ktime_to_ns(hrtimer_get_remaining(&hwc->hrtimer)));
++	hrtimer_cancel(&hwc->hrtimer);
++}
++
++static void i915_perf_enable(struct perf_event *event)
++{
++	struct drm_i915_private *i915 =
++		container_of(event->pmu, typeof(*i915), pmu.base);
++	u64 mask;
++
++	if (i915->pmu.enable == 0)
++		__hrtimer_start_range_ns(&i915->pmu.timer,
++					 ns_to_ktime(PERIOD), 0,
++					 HRTIMER_MODE_REL_PINNED, 0);
++
++	if (is_instdone_event(event)) {
++		i915->pmu.instdone |= (u64)1 << (event->attr.config - I915_PERF_INSTDONE_0);
++		mask = INSTDONE_ENABLE;
++	} else
++		mask = (u64)1 << event->attr.config;
++
++	i915->pmu.enable |= mask;
++
++	i915_perf_timer_start(event);
++}
++
++static void i915_perf_disable(struct perf_event *event)
++{
++	struct drm_i915_private *i915 =
++		container_of(event->pmu, typeof(*i915), pmu.base);
++	u64 mask;
++
++	if (is_instdone_event(event)) {
++		i915->pmu.instdone &= ~((u64)1 << (event->attr.config - I915_PERF_INSTDONE_0));
++		mask = i915->pmu.instdone == 0 ? INSTDONE_ENABLE : 0;
++	} else
++		mask = (u64)1 << event->attr.config;
++
++	i915->pmu.enable &= ~mask;
++
++	i915_perf_timer_cancel(event);
++}
++
++static int i915_perf_event_add(struct perf_event *event, int flags)
++{
++	struct hw_perf_event *hwc = &event->hw;
++
++	printk(KERN_ERR "%s %d\n", __func__, (int)event->attr.config);
++
++	if (flags & PERF_EF_START)
++		i915_perf_enable(event);
++
++	hwc->state = !(flags & PERF_EF_START);
++
++	return 0;
++}
++
++static void i915_perf_event_del(struct perf_event *event, int flags)
++{
++	printk(KERN_ERR "%s %d\n", __func__, (int)event->attr.config);
++
++	i915_perf_disable(event);
++}
++
++static void i915_perf_event_start(struct perf_event *event, int flags)
++{
++	printk(KERN_ERR "%s %d\n", __func__, (int)event->attr.config);
++
++	i915_perf_enable(event);
++}
++
++static void i915_perf_event_stop(struct perf_event *event, int flags)
++{
++	printk(KERN_ERR "%s %d\n", __func__, (int)event->attr.config);
++
++	i915_perf_disable(event);
++}
++
++static u64 read_energy_uJ(struct drm_i915_private *dev_priv)
++{
++	u64 power;
++	u32 units;
++
++	if (dev_priv->info.gen < 6)
++		return 0;
++
++	rdmsrl(MSR_RAPL_POWER_UNIT, power);
++	power = (power & 0x1f00) >> 8;
++	units = 1000000 / (1 << power); /* convert to uJ */
++	power = I915_READ_NOTRACE(MCH_SECP_NRG_STTS);
++	power *= units;
++
++	return power;
++}
++
++static inline u64 calc_residency(struct drm_i915_private *dev_priv, const u32 reg)
++{
++	if (dev_priv->info.gen >= 6) {
++		u64 val, units = 128, div = 100000;
++		if (dev_priv->info.is_valleyview) {
++			u32 clock;
++
++			clock = I915_READ_NOTRACE(VLV_CLK_CTL2) >> CLK_CTL2_CZCOUNT_30NS_SHIFT;
++			if (clock) {
++				units = DIV_ROUND_UP(30 * 1000, clock);
++				if (I915_READ_NOTRACE(VLV_COUNTER_CONTROL) & VLV_COUNT_RANGE_HIGH)
++					units <<= 8;
++			} else
++				units = 0;
++
++			div *= 1000;
++		}
++		val = I915_READ_NOTRACE(reg);
++		val *= units;
++		return DIV_ROUND_UP_ULL(val, div);
++	} else
++		return 0;
++}
++
++static inline u64 read_statistic(struct drm_i915_private *dev_priv,
++				 const int statistic)
++{
++	const u32 reg = 0x2310 + 8 *statistic;
++	u32 high, low;
++
++	do {
++		high = I915_READ_NOTRACE(reg + 4);
++		low = I915_READ_NOTRACE(reg);
++	} while (high != I915_READ_NOTRACE(reg + 4));
++
++	return (u64)high << 32 | low;
++}
++
++static u64 count_interrupts(struct drm_i915_private *i915)
++{
++	/* open-coded kstat_irqs() */
++	struct irq_desc *desc = irq_to_desc(i915->dev->pdev->irq);
++	u64 sum = 0;
++	int cpu;
++
++	if (!desc || !desc->kstat_irqs)
++		return 0;
++
++	for_each_possible_cpu(cpu)
++		sum += *per_cpu_ptr(desc->kstat_irqs, cpu);
++
++	return sum;
++}
++
++static void i915_perf_event_read(struct perf_event *event)
++{
++	struct drm_i915_private *i915 =
++		container_of(event->pmu, typeof(*i915), pmu.base);
++	u64 val = 0;
++
++	if (event->attr.config < 32) {
++		int engine = event->attr.config >> 2;
++		int sample = event->attr.config & 3;
++		val = i915->engine[engine].pmu_sample[sample];
++	} else switch (event->attr.config) {
++	case I915_PERF_ACTUAL_FREQUENCY:
++		val = i915->pmu.sample[__I915_SAMPLE_FREQ_ACT];
++		break;
++	case I915_PERF_REQUESTED_FREQUENCY:
++		val = i915->pmu.sample[__I915_SAMPLE_FREQ_REQ];
++		break;
++	case I915_PERF_ENERGY:
++		val = read_energy_uJ(i915);
++		break;
++	case I915_PERF_INTERRUPTS:
++		val = count_interrupts(i915);
++		break;
++
++	case I915_PERF_RC6_RESIDENCY:
++		if (!pm_runtime_active(&i915->dev->pdev->dev))
++			return;
++
++		val = calc_residency(i915, i915->info.is_valleyview ? VLV_GT_RENDER_RC6 : GEN6_GT_GFX_RC6);
++		break;
++
++	case I915_PERF_RC6p_RESIDENCY:
++		if (!pm_runtime_active(&i915->dev->pdev->dev))
++			return;
++
++		if (!i915->info.is_valleyview)
++			val = calc_residency(i915, GEN6_GT_GFX_RC6p);
++		break;
++
++	case I915_PERF_RC6pp_RESIDENCY:
++		if (!pm_runtime_active(&i915->dev->pdev->dev))
++			return;
++
++		if (!i915->info.is_valleyview)
++			val = calc_residency(i915, GEN6_GT_GFX_RC6pp);
++		break;
++
++	case I915_PERF_STATISTIC_0...I915_PERF_STATISTIC_8:
++		if (!pm_runtime_active(&i915->dev->pdev->dev))
++			return;
++
++		val = read_statistic(i915, event->attr.config - I915_PERF_STATISTIC_0);
++		break;
++
++	case I915_PERF_INSTDONE_0...I915_PERF_INSTDONE_63:
++		val = i915->pmu.sample[event->attr.config - I915_PERF_INSTDONE_0 + __I915_SAMPLE_INSTDONE_0];
++		break;
++	}
++
++	local64_set(&event->count, val);
++}
++
++static int i915_perf_event_event_idx(struct perf_event *event)
++{
++	return 0;
++}
++
++void i915_perf_register(struct drm_device *dev)
++{
++	struct drm_i915_private *i915 = to_i915(dev);
++
++	i915->pmu.base.task_ctx_nr	= perf_sw_context;
++	i915->pmu.base.event_init	= i915_perf_event_init;
++	i915->pmu.base.add		= i915_perf_event_add;
++	i915->pmu.base.del		= i915_perf_event_del;
++	i915->pmu.base.start		= i915_perf_event_start;
++	i915->pmu.base.stop		= i915_perf_event_stop;
++	i915->pmu.base.read		= i915_perf_event_read;
++	i915->pmu.base.event_idx	= i915_perf_event_event_idx;
++
++	hrtimer_init(&i915->pmu.timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
++	i915->pmu.timer.function = i915_sample;
++	i915->pmu.enable = 0;
++
++	if (perf_pmu_register(&i915->pmu.base, "i915", -1))
++		i915->pmu.base.event_init = NULL;
++}
++
++void i915_perf_unregister(struct drm_device *dev)
++{
++	struct drm_i915_private *i915 = to_i915(dev);
++
++	if (i915->pmu.base.event_init == NULL)
++		return;
++
++	perf_pmu_unregister(&i915->pmu.base);
++	i915->pmu.base.event_init = NULL;
++}
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_reg.h b/drivers/gpu/drm/i915/i915_reg.h
+--- a/drivers/gpu/drm/i915/i915_reg.h	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/i915_reg.h	2014-11-20 09:53:37.972762837 -0700
+@@ -26,8 +26,8 @@
+ #define _I915_REG_H_
+ 
+ #define _PIPE(pipe, a, b) ((a) + (pipe)*((b)-(a)))
++#define _PLANE(plane, a, b) _PIPE(plane, a, b)
+ #define _TRANSCODER(tran, a, b) ((a) + (tran)*((b)-(a)))
+-
+ #define _PORT(port, a, b) ((a) + (port)*((b)-(a)))
+ #define _PIPE3(pipe, a, b, c) ((pipe) == PIPE_A ? (a) : \
+ 			       (pipe) == PIPE_B ? (b) : (c))
+@@ -143,6 +143,14 @@
+ #define GAB_CTL				0x24000
+ #define   GAB_CTL_CONT_AFTER_PAGEFAULT	(1<<8)
+ 
++#define GEN7_BIOS_RESERVED		0x1082C0
++#define GEN7_BIOS_RESERVED_1M		(0 << 5)
++#define GEN7_BIOS_RESERVED_256K		(1 << 5)
++#define GEN8_BIOS_RESERVED_SHIFT       7
++#define GEN7_BIOS_RESERVED_MASK        0x1
++#define GEN8_BIOS_RESERVED_MASK        0x3
++
++
+ /* VGA stuff */
+ 
+ #define VGA_ST01_MDA 0x3ba
+@@ -232,6 +240,7 @@
+ #define MI_LOAD_SCAN_LINES_INCL MI_INSTR(0x12, 0)
+ #define MI_DISPLAY_FLIP		MI_INSTR(0x14, 2)
+ #define MI_DISPLAY_FLIP_I915	MI_INSTR(0x14, 1)
++#define   MI_DISPLAY_FLIP_ASYNC	(1 << 22)
+ #define   MI_DISPLAY_FLIP_PLANE(n) ((n) << 20)
+ /* IVB has funny definitions for which plane to flip. */
+ #define   MI_DISPLAY_FLIP_IVB_PLANE_A  (0 << 19)
+@@ -240,6 +249,21 @@
+ #define   MI_DISPLAY_FLIP_IVB_SPRITE_B (3 << 19)
+ #define   MI_DISPLAY_FLIP_IVB_PLANE_C  (4 << 19)
+ #define   MI_DISPLAY_FLIP_IVB_SPRITE_C (5 << 19)
++/* SKL ones */
++#define   MI_DISPLAY_FLIP_SKL_PLANE_1_A	(0 << 8)
++#define   MI_DISPLAY_FLIP_SKL_PLANE_1_B	(1 << 8)
++#define   MI_DISPLAY_FLIP_SKL_PLANE_1_C	(2 << 8)
++#define   MI_DISPLAY_FLIP_SKL_PLANE_2_A	(4 << 8)
++#define   MI_DISPLAY_FLIP_SKL_PLANE_2_B	(5 << 8)
++#define   MI_DISPLAY_FLIP_SKL_PLANE_2_C	(6 << 8)
++#define   MI_DISPLAY_FLIP_SKL_PLANE_3_A	(7 << 8)
++#define   MI_DISPLAY_FLIP_SKL_PLANE_3_B	(8 << 8)
++#define   MI_DISPLAY_FLIP_SKL_PLANE_3_C	(9 << 8)
++/* These go in the bottom of the base address value */
++#define   MI_DISPLAY_FLIP_TYPE_SYNC    (0 << 0)
++#define   MI_DISPLAY_FLIP_TYPE_ASYNC   (1 << 0)
++#define   MI_DISPLAY_FLIP_TYPE_STEREO  (2 << 0)
++
+ #define MI_SEMAPHORE_MBOX	MI_INSTR(0x16, 1) /* gen6, gen7 */
+ #define   MI_SEMAPHORE_GLOBAL_GTT    (1<<22)
+ #define   MI_SEMAPHORE_UPDATE	    (1<<21)
+@@ -272,6 +296,7 @@
+ #define   MI_SEMAPHORE_POLL		(1<<15)
+ #define   MI_SEMAPHORE_SAD_GTE_SDD	(1<<12)
+ #define MI_STORE_DWORD_IMM	MI_INSTR(0x20, 1)
++#define MI_STORE_DWORD_IMM_GEN8	MI_INSTR(0x20, 2)
+ #define   MI_MEM_VIRTUAL	(1 << 22) /* 965+ only */
+ #define MI_STORE_DWORD_INDEX	MI_INSTR(0x21, 1)
+ #define   MI_STORE_DWORD_INDEX_SHIFT 2
+@@ -282,10 +307,11 @@
+  *   address/value pairs. Don't overdue it, though, x <= 2^4 must hold!
+  */
+ #define MI_LOAD_REGISTER_IMM(x)	MI_INSTR(0x22, 2*(x)-1)
++#define   MI_LRI_FORCE_POSTED		(1<<12)
+ #define MI_STORE_REGISTER_MEM(x) MI_INSTR(0x24, 2*(x)-1)
+ #define MI_STORE_REGISTER_MEM_GEN8(x) MI_INSTR(0x24, 3*(x)-1)
+ #define   MI_SRM_LRM_GLOBAL_GTT		(1<<22)
+-#define MI_FLUSH_DW		MI_INSTR(0x26, 1) /* for GEN6 */
++#define MI_FLUSH_DW		MI_INSTR(0x26, 0) /* for GEN6 */
+ #define   MI_FLUSH_DW_STORE_INDEX	(1<<21)
+ #define   MI_INVALIDATE_TLB		(1<<18)
+ #define   MI_FLUSH_DW_OP_STOREDW	(1<<14)
+@@ -304,6 +330,8 @@
+ #define   MI_BATCH_GTT		    (2<<6) /* aliased with (1<<7) on gen4 */
+ #define MI_BATCH_BUFFER_START_GEN8	MI_INSTR(0x31, 1)
+ 
++#define MI_PREDICATE_SRC0	(0x2400)
++#define MI_PREDICATE_SRC1	(0x2408)
+ 
+ #define MI_PREDICATE_RESULT_2	(0x2214)
+ #define  LOWER_SLICE_ENABLED	(1<<0)
+@@ -501,10 +529,26 @@
+ #define BUNIT_REG_BISOC				0x11
+ 
+ #define PUNIT_REG_DSPFREQ			0x36
++#define   DSPFREQSTAT_SHIFT_CHV			24
++#define   DSPFREQSTAT_MASK_CHV			(0x1f << DSPFREQSTAT_SHIFT_CHV)
++#define   DSPFREQGUAR_SHIFT_CHV			8
++#define   DSPFREQGUAR_MASK_CHV			(0x1f << DSPFREQGUAR_SHIFT_CHV)
+ #define   DSPFREQSTAT_SHIFT			30
+ #define   DSPFREQSTAT_MASK			(0x3 << DSPFREQSTAT_SHIFT)
+ #define   DSPFREQGUAR_SHIFT			14
+ #define   DSPFREQGUAR_MASK			(0x3 << DSPFREQGUAR_SHIFT)
++#define   _DP_SSC(val, pipe)			((val) << (2 * (pipe)))
++#define   DP_SSC_MASK(pipe)			_DP_SSC(0x3, (pipe))
++#define   DP_SSC_PWR_ON(pipe)			_DP_SSC(0x0, (pipe))
++#define   DP_SSC_CLK_GATE(pipe)			_DP_SSC(0x1, (pipe))
++#define   DP_SSC_RESET(pipe)			_DP_SSC(0x2, (pipe))
++#define   DP_SSC_PWR_GATE(pipe)			_DP_SSC(0x3, (pipe))
++#define   _DP_SSS(val, pipe)			((val) << (2 * (pipe) + 16))
++#define   DP_SSS_MASK(pipe)			_DP_SSS(0x3, (pipe))
++#define   DP_SSS_PWR_ON(pipe)			_DP_SSS(0x0, (pipe))
++#define   DP_SSS_CLK_GATE(pipe)			_DP_SSS(0x1, (pipe))
++#define   DP_SSS_RESET(pipe)			_DP_SSS(0x2, (pipe))
++#define   DP_SSS_PWR_GATE(pipe)			_DP_SSS(0x3, (pipe))
+ 
+ /* See the PUNIT HAS v0.8 for the below bits */
+ enum punit_power_well {
+@@ -518,6 +562,11 @@
+ 	PUNIT_POWER_WELL_DPIO_TX_C_LANES_23	= 9,
+ 	PUNIT_POWER_WELL_DPIO_RX0		= 10,
+ 	PUNIT_POWER_WELL_DPIO_RX1		= 11,
++	PUNIT_POWER_WELL_DPIO_CMN_D		= 12,
++	/* FIXME: guesswork below */
++	PUNIT_POWER_WELL_DPIO_TX_D_LANES_01	= 13,
++	PUNIT_POWER_WELL_DPIO_TX_D_LANES_23	= 14,
++	PUNIT_POWER_WELL_DPIO_RX2		= 15,
+ 
+ 	PUNIT_POWER_WELL_NUM,
+ };
+@@ -562,9 +611,6 @@
+ #define   FB_FMAX_VMIN_FREQ_LO_MASK		0xf8000000
+ 
+ #define VLV_CZ_CLOCK_TO_MILLI_SEC		100000
+-#define VLV_RP_UP_EI_THRESHOLD			90
+-#define VLV_RP_DOWN_EI_THRESHOLD		70
+-#define VLV_INT_COUNT_FOR_DOWN_EI		5
+ 
+ /* vlv2 north clock has */
+ #define CCK_FUSE_REG				0x8
+@@ -641,7 +687,7 @@
+  * need to be accessed during AUX communication,
+  *
+  * Generally the common lane corresponds to the pipe and
+- * the spline (PCS/TX) correponds to the port.
++ * the spline (PCS/TX) corresponds to the port.
+  *
+  * For dual channel PHY (VLV/CHV):
+  *
+@@ -765,6 +811,8 @@
+ #define _VLV_PCS_DW0_CH1		0x8400
+ #define   DPIO_PCS_TX_LANE2_RESET	(1<<16)
+ #define   DPIO_PCS_TX_LANE1_RESET	(1<<7)
++#define   DPIO_LEFT_TXFIFO_RST_MASTER2	(1<<4)
++#define   DPIO_RIGHT_TXFIFO_RST_MASTER2	(1<<3)
+ #define VLV_PCS_DW0(ch) _PORT(ch, _VLV_PCS_DW0_CH0, _VLV_PCS_DW0_CH1)
+ 
+ #define _VLV_PCS01_DW0_CH0		0x200
+@@ -805,12 +853,31 @@
+ 
+ #define _VLV_PCS_DW9_CH0		0x8224
+ #define _VLV_PCS_DW9_CH1		0x8424
++#define   DPIO_PCS_TX2MARGIN_MASK	(0x7<<13)
++#define   DPIO_PCS_TX2MARGIN_000	(0<<13)
++#define   DPIO_PCS_TX2MARGIN_101	(1<<13)
++#define   DPIO_PCS_TX1MARGIN_MASK	(0x7<<10)
++#define   DPIO_PCS_TX1MARGIN_000	(0<<10)
++#define   DPIO_PCS_TX1MARGIN_101	(1<<10)
+ #define	VLV_PCS_DW9(ch) _PORT(ch, _VLV_PCS_DW9_CH0, _VLV_PCS_DW9_CH1)
+ 
++#define _VLV_PCS01_DW9_CH0		0x224
++#define _VLV_PCS23_DW9_CH0		0x424
++#define _VLV_PCS01_DW9_CH1		0x2624
++#define _VLV_PCS23_DW9_CH1		0x2824
++#define VLV_PCS01_DW9(ch) _PORT(ch, _VLV_PCS01_DW9_CH0, _VLV_PCS01_DW9_CH1)
++#define VLV_PCS23_DW9(ch) _PORT(ch, _VLV_PCS23_DW9_CH0, _VLV_PCS23_DW9_CH1)
++
+ #define _CHV_PCS_DW10_CH0		0x8228
+ #define _CHV_PCS_DW10_CH1		0x8428
+ #define   DPIO_PCS_SWING_CALC_TX0_TX2	(1<<30)
+ #define   DPIO_PCS_SWING_CALC_TX1_TX3	(1<<31)
++#define   DPIO_PCS_TX2DEEMP_MASK	(0xf<<24)
++#define   DPIO_PCS_TX2DEEMP_9P5		(0<<24)
++#define   DPIO_PCS_TX2DEEMP_6P0		(2<<24)
++#define   DPIO_PCS_TX1DEEMP_MASK	(0xf<<16)
++#define   DPIO_PCS_TX1DEEMP_9P5		(0<<16)
++#define   DPIO_PCS_TX1DEEMP_6P0		(2<<16)
+ #define CHV_PCS_DW10(ch) _PORT(ch, _CHV_PCS_DW10_CH0, _CHV_PCS_DW10_CH1)
+ 
+ #define _VLV_PCS01_DW10_CH0		0x0228
+@@ -822,8 +889,18 @@
+ 
+ #define _VLV_PCS_DW11_CH0		0x822c
+ #define _VLV_PCS_DW11_CH1		0x842c
++#define   DPIO_LANEDESKEW_STRAP_OVRD	(1<<3)
++#define   DPIO_LEFT_TXFIFO_RST_MASTER	(1<<1)
++#define   DPIO_RIGHT_TXFIFO_RST_MASTER	(1<<0)
+ #define VLV_PCS_DW11(ch) _PORT(ch, _VLV_PCS_DW11_CH0, _VLV_PCS_DW11_CH1)
+ 
++#define _VLV_PCS01_DW11_CH0		0x022c
++#define _VLV_PCS23_DW11_CH0		0x042c
++#define _VLV_PCS01_DW11_CH1		0x262c
++#define _VLV_PCS23_DW11_CH1		0x282c
++#define VLV_PCS01_DW11(ch) _PORT(ch, _VLV_PCS01_DW11_CH0, _VLV_PCS01_DW11_CH1)
++#define VLV_PCS23_DW11(ch) _PORT(ch, _VLV_PCS23_DW11_CH0, _VLV_PCS23_DW11_CH1)
++
+ #define _VLV_PCS_DW12_CH0		0x8230
+ #define _VLV_PCS_DW12_CH1		0x8430
+ #define VLV_PCS_DW12(ch) _PORT(ch, _VLV_PCS_DW12_CH0, _VLV_PCS_DW12_CH1)
+@@ -838,8 +915,8 @@
+ 
+ #define _VLV_TX_DW2_CH0			0x8288
+ #define _VLV_TX_DW2_CH1			0x8488
+-#define   DPIO_SWING_MARGIN_SHIFT	16
+-#define   DPIO_SWING_MARGIN_MASK	(0xff << DPIO_SWING_MARGIN_SHIFT)
++#define   DPIO_SWING_MARGIN000_SHIFT	16
++#define   DPIO_SWING_MARGIN000_MASK	(0xff << DPIO_SWING_MARGIN000_SHIFT)
+ #define   DPIO_UNIQ_TRANS_SCALE_SHIFT	8
+ #define VLV_TX_DW2(ch) _PORT(ch, _VLV_TX_DW2_CH0, _VLV_TX_DW2_CH1)
+ 
+@@ -847,12 +924,16 @@
+ #define _VLV_TX_DW3_CH1			0x848c
+ /* The following bit for CHV phy */
+ #define   DPIO_TX_UNIQ_TRANS_SCALE_EN	(1<<27)
++#define   DPIO_SWING_MARGIN101_SHIFT	16
++#define   DPIO_SWING_MARGIN101_MASK	(0xff << DPIO_SWING_MARGIN101_SHIFT)
+ #define VLV_TX_DW3(ch) _PORT(ch, _VLV_TX_DW3_CH0, _VLV_TX_DW3_CH1)
+ 
+ #define _VLV_TX_DW4_CH0			0x8290
+ #define _VLV_TX_DW4_CH1			0x8490
+ #define   DPIO_SWING_DEEMPH9P5_SHIFT	24
+ #define   DPIO_SWING_DEEMPH9P5_MASK	(0xff << DPIO_SWING_DEEMPH9P5_SHIFT)
++#define   DPIO_SWING_DEEMPH6P0_SHIFT	16
++#define   DPIO_SWING_DEEMPH6P0_MASK	(0xff << DPIO_SWING_DEEMPH6P0_SHIFT)
+ #define VLV_TX_DW4(ch) _PORT(ch, _VLV_TX_DW4_CH0, _VLV_TX_DW4_CH1)
+ 
+ #define _VLV_TX3_DW4_CH0		0x690
+@@ -1003,6 +1084,13 @@
+ #define   PGTBL_ADDRESS_LO_MASK	0xfffff000 /* bits [31:12] */
+ #define   PGTBL_ADDRESS_HI_MASK	0x000000f0 /* bits [35:32] (gen4) */
+ #define PGTBL_ER	0x02024
++#define PRB0_BASE (0x2030-0x30)
++#define PRB1_BASE (0x2040-0x30) /* 830,gen3 */
++#define PRB2_BASE (0x2050-0x30) /* gen3 */
++#define SRB0_BASE (0x2100-0x30) /* gen2 */
++#define SRB1_BASE (0x2110-0x30) /* gen2 */
++#define SRB2_BASE (0x2120-0x30) /* 830 */
++#define SRB3_BASE (0x2130-0x30) /* 830 */
+ #define RENDER_RING_BASE	0x02000
+ #define BSD_RING_BASE		0x04000
+ #define GEN6_BSD_RING_BASE	0x12000
+@@ -1064,6 +1152,7 @@
+ #define RING_ACTHD_UDW(base)	((base)+0x5c)
+ #define RING_NOPID(base)	((base)+0x94)
+ #define RING_IMR(base)		((base)+0xa8)
++#define RING_HWSTAM(base)	((base)+0x98)
+ #define RING_TIMESTAMP(base)	((base)+0x358)
+ #define   TAIL_ADDR		0x001FFFF8
+ #define   HEAD_WRAP_COUNT	0xFFE00000
+@@ -1248,6 +1337,10 @@
+ #define   INSTPM_TLB_INVALIDATE	(1<<9)
+ #define   INSTPM_SYNC_FLUSH	(1<<5)
+ #define ACTHD	        0x020c8
++#define MEM_MODE	0x020cc
++#define   MEM_DISPLAY_B_TRICKLE_FEED_DISABLE (1<<3) /* 830 only */
++#define   MEM_DISPLAY_A_TRICKLE_FEED_DISABLE (1<<2) /* 830/845 only */
++#define   MEM_DISPLAY_TRICKLE_FEED_DISABLE (1<<2) /* 85x only */
+ #define FW_BLC		0x020d8
+ #define FW_BLC2		0x020dc
+ #define FW_BLC_SELF	0x020e0 /* 915+ only */
+@@ -1380,6 +1473,7 @@
+ #define GT_BSD_CS_ERROR_INTERRUPT		(1 << 15)
+ #define GT_BSD_USER_INTERRUPT			(1 << 12)
+ #define GT_RENDER_L3_PARITY_ERROR_INTERRUPT_S1	(1 << 11) /* hsw+; rsvd on snb, ivb, vlv */
++#define GT_CONTEXT_SWITCH_INTERRUPT		(1 <<  8)
+ #define GT_RENDER_L3_PARITY_ERROR_INTERRUPT	(1 <<  5) /* !snb */
+ #define GT_RENDER_PIPECTL_NOTIFY_INTERRUPT	(1 <<  4)
+ #define GT_RENDER_CS_MASTER_ERROR_INTERRUPT	(1 <<  3)
+@@ -1519,6 +1613,7 @@
+ /* Framebuffer compression for Ironlake */
+ #define ILK_DPFC_CB_BASE	0x43200
+ #define ILK_DPFC_CONTROL	0x43208
++#define   FBC_CTL_FALSE_COLOR	(1<<10)
+ /* The bit 28-8 is reserved */
+ #define   DPFC_RESERVED		(0x1FFFFF00)
+ #define ILK_DPFC_RECOMP_CTL	0x4320c
+@@ -1675,12 +1770,9 @@
+ #define DPIO_PHY_STATUS			(VLV_DISPLAY_BASE + 0x6240)
+ #define   DPLL_PORTD_READY_MASK		(0xf)
+ #define DISPLAY_PHY_CONTROL (VLV_DISPLAY_BASE + 0x60100)
+-#define   PHY_COM_LANE_RESET_DEASSERT(phy, val) \
+-				((phy == DPIO_PHY0) ? (val | 1) : (val | 2))
+-#define   PHY_COM_LANE_RESET_ASSERT(phy, val) \
+-				((phy == DPIO_PHY0) ? (val & ~1) : (val & ~2))
++#define   PHY_COM_LANE_RESET_DEASSERT(phy) (1 << (phy))
+ #define DISPLAY_PHY_STATUS (VLV_DISPLAY_BASE + 0x60104)
+-#define   PHY_POWERGOOD(phy)	((phy == DPIO_PHY0) ? (1<<31) : (1<<30))
++#define   PHY_POWERGOOD(phy)	(((phy) == DPIO_PHY0) ? (1<<31) : (1<<30))
+ 
+ /*
+  * The i830 generation, in LVDS mode, defines P1 as the bit number set within
+@@ -2261,6 +2353,7 @@
+  *   doesn't need saving on GT1
+  */
+ #define CXT_SIZE		0x21a0
++#define ILK_CXT_TOTAL_SIZE		(1 * PAGE_SIZE)
+ #define GEN6_CXT_POWER_SIZE(cxt_reg)	((cxt_reg >> 24) & 0x3f)
+ #define GEN6_CXT_RING_SIZE(cxt_reg)	((cxt_reg >> 18) & 0x3f)
+ #define GEN6_CXT_RENDER_SIZE(cxt_reg)	((cxt_reg >> 12) & 0x3f)
+@@ -2397,6 +2490,7 @@
+ #define _PIPEASRC	0x6001c
+ #define _BCLRPAT_A	0x60020
+ #define _VSYNCSHIFT_A	0x60028
++#define _PIPE_MULT_A	0x6002c
+ 
+ /* Pipe B timing regs */
+ #define _HTOTAL_B	0x61000
+@@ -2408,6 +2502,7 @@
+ #define _PIPEBSRC	0x6101c
+ #define _BCLRPAT_B	0x61020
+ #define _VSYNCSHIFT_B	0x61028
++#define _PIPE_MULT_B	0x6102c
+ 
+ #define TRANSCODER_A_OFFSET 0x60000
+ #define TRANSCODER_B_OFFSET 0x61000
+@@ -2428,6 +2523,7 @@
+ #define BCLRPAT(trans) _TRANSCODER2(trans, _BCLRPAT_A)
+ #define VSYNCSHIFT(trans) _TRANSCODER2(trans, _VSYNCSHIFT_A)
+ #define PIPESRC(trans) _TRANSCODER2(trans, _PIPEASRC)
++#define PIPE_MULT(trans) _TRANSCODER2(trans, _PIPE_MULT_A)
+ 
+ /* HSW+ eDP PSR registers */
+ #define EDP_PSR_BASE(dev)                       (IS_HASWELL(dev) ? 0x64800 : 0x6f800)
+@@ -2457,9 +2553,7 @@
+ 
+ #define EDP_PSR_AUX_CTL(dev)			(EDP_PSR_BASE(dev) + 0x10)
+ #define EDP_PSR_AUX_DATA1(dev)			(EDP_PSR_BASE(dev) + 0x14)
+-#define   EDP_PSR_DPCD_COMMAND		0x80060000
+ #define EDP_PSR_AUX_DATA2(dev)			(EDP_PSR_BASE(dev) + 0x18)
+-#define   EDP_PSR_DPCD_NORMAL_OPERATION	(1<<24)
+ #define EDP_PSR_AUX_DATA3(dev)			(EDP_PSR_BASE(dev) + 0x1c)
+ #define EDP_PSR_AUX_DATA4(dev)			(EDP_PSR_BASE(dev) + 0x20)
+ #define EDP_PSR_AUX_DATA5(dev)			(EDP_PSR_BASE(dev) + 0x24)
+@@ -3476,6 +3570,8 @@
+ #define   DP_LINK_TRAIN_OFF		(3 << 28)
+ #define   DP_LINK_TRAIN_MASK		(3 << 28)
+ #define   DP_LINK_TRAIN_SHIFT		28
++#define   DP_LINK_TRAIN_PAT_3_CHV	(1 << 14)
++#define   DP_LINK_TRAIN_MASK_CHV	((3 << 28)|(1<<14))
+ 
+ /* CPT Link training mode */
+ #define   DP_LINK_TRAIN_PAT_1_CPT	(0 << 8)
+@@ -3594,6 +3690,7 @@
+ #define   DP_AUX_CH_CTL_PRECHARGE_TEST	    (1 << 11)
+ #define   DP_AUX_CH_CTL_BIT_CLOCK_2X_MASK    (0x7ff)
+ #define   DP_AUX_CH_CTL_BIT_CLOCK_2X_SHIFT   0
++#define   DP_AUX_CH_CTL_SYNC_PULSE_SKL(c)   ((c) - 1)
+ 
+ /*
+  * Computing GMCH M and N values for the Display Port link
+@@ -3732,7 +3829,6 @@
+ #define   PIPE_VSYNC_INTERRUPT_STATUS		(1UL<<9)
+ #define   PIPE_DISPLAY_LINE_COMPARE_STATUS	(1UL<<8)
+ #define   PIPE_DPST_EVENT_STATUS		(1UL<<7)
+-#define   PIPE_LEGACY_BLC_EVENT_STATUS		(1UL<<6)
+ #define   PIPE_A_PSR_STATUS_VLV			(1UL<<6)
+ #define   PIPE_LEGACY_BLC_EVENT_STATUS		(1UL<<6)
+ #define   PIPE_ODD_FIELD_INTERRUPT_STATUS	(1UL<<5)
+@@ -3842,73 +3938,152 @@
+ #define   DSPARB_BEND_SHIFT	9 /* on 855 */
+ #define   DSPARB_AEND_SHIFT	0
+ 
++/* pnv/gen4/g4x/vlv/chv */
+ #define DSPFW1			(dev_priv->info.display_mmio_offset + 0x70034)
+-#define   DSPFW_SR_SHIFT	23
+-#define   DSPFW_SR_MASK		(0x1ff<<23)
+-#define   DSPFW_CURSORB_SHIFT	16
+-#define   DSPFW_CURSORB_MASK	(0x3f<<16)
+-#define   DSPFW_PLANEB_SHIFT	8
+-#define   DSPFW_PLANEB_MASK	(0x7f<<8)
+-#define   DSPFW_PLANEA_MASK	(0x7f)
++#define   DSPFW_SR_SHIFT		23
++#define   DSPFW_SR_MASK			(0x1ff<<23)
++#define   DSPFW_CURSORB_SHIFT		16
++#define   DSPFW_CURSORB_MASK		(0x3f<<16)
++#define   DSPFW_PLANEB_SHIFT		8
++#define   DSPFW_PLANEB_MASK		(0x7f<<8)
++#define   DSPFW_PLANEB_MASK_VLV		(0xff<<8) /* vlv/chv */
++#define   DSPFW_PLANEA_SHIFT		0
++#define   DSPFW_PLANEA_MASK		(0x7f<<0)
++#define   DSPFW_PLANEA_MASK_VLV		(0xff<<0) /* vlv/chv */
+ #define DSPFW2			(dev_priv->info.display_mmio_offset + 0x70038)
+-#define   DSPFW_CURSORA_MASK	0x00003f00
+-#define   DSPFW_CURSORA_SHIFT	8
+-#define   DSPFW_PLANEC_MASK	(0x7f)
++#define   DSPFW_FBC_SR_EN		(1<<31)	  /* g4x */
++#define   DSPFW_FBC_SR_SHIFT		28
++#define   DSPFW_FBC_SR_MASK		(0x7<<28) /* g4x */
++#define   DSPFW_FBC_HPLL_SR_SHIFT	24
++#define   DSPFW_FBC_HPLL_SR_MASK	(0xf<<24) /* g4x */
++#define   DSPFW_SPRITEB_SHIFT		(16)
++#define   DSPFW_SPRITEB_MASK		(0x7f<<16) /* g4x */
++#define   DSPFW_SPRITEB_MASK_VLV	(0xff<<16) /* vlv/chv */
++#define   DSPFW_CURSORA_SHIFT		8
++#define   DSPFW_CURSORA_MASK		(0x3f<<8)
++#define   DSPFW_PLANEC_SHIFT_OLD	0
++#define   DSPFW_PLANEC_MASK_OLD		(0x7f<<0) /* pre-gen4 sprite C */
++#define   DSPFW_SPRITEA_SHIFT		0
++#define   DSPFW_SPRITEA_MASK		(0x7f<<0) /* g4x */
++#define   DSPFW_SPRITEA_MASK_VLV	(0xff<<0) /* vlv/chv */
+ #define DSPFW3			(dev_priv->info.display_mmio_offset + 0x7003c)
+-#define   DSPFW_HPLL_SR_EN	(1<<31)
+-#define   DSPFW_CURSOR_SR_SHIFT	24
++#define   DSPFW_HPLL_SR_EN		(1<<31)
+ #define   PINEVIEW_SELF_REFRESH_EN	(1<<30)
++#define   DSPFW_CURSOR_SR_SHIFT		24
+ #define   DSPFW_CURSOR_SR_MASK		(0x3f<<24)
+ #define   DSPFW_HPLL_CURSOR_SHIFT	16
+ #define   DSPFW_HPLL_CURSOR_MASK	(0x3f<<16)
+-#define   DSPFW_HPLL_SR_MASK		(0x1ff)
+-#define DSPFW4			(dev_priv->info.display_mmio_offset + 0x70070)
+-#define DSPFW7			(dev_priv->info.display_mmio_offset + 0x7007c)
++#define   DSPFW_HPLL_SR_SHIFT		0
++#define   DSPFW_HPLL_SR_MASK		(0x1ff<<0)
++
++/* vlv/chv */
++#define DSPFW4			(VLV_DISPLAY_BASE + 0x70070)
++#define   DSPFW_SPRITEB_WM1_SHIFT	16
++#define   DSPFW_SPRITEB_WM1_MASK	(0xff<<16)
++#define   DSPFW_CURSORA_WM1_SHIFT	8
++#define   DSPFW_CURSORA_WM1_MASK	(0x3f<<8)
++#define   DSPFW_SPRITEA_WM1_SHIFT	0
++#define   DSPFW_SPRITEA_WM1_MASK	(0xff<<0)
++#define DSPFW5			(VLV_DISPLAY_BASE + 0x70074)
++#define   DSPFW_PLANEB_WM1_SHIFT	24
++#define   DSPFW_PLANEB_WM1_MASK		(0xff<<24)
++#define   DSPFW_PLANEA_WM1_SHIFT	16
++#define   DSPFW_PLANEA_WM1_MASK		(0xff<<16)
++#define   DSPFW_CURSORB_WM1_SHIFT	8
++#define   DSPFW_CURSORB_WM1_MASK	(0x3f<<8)
++#define   DSPFW_CURSOR_SR_WM1_SHIFT	0
++#define   DSPFW_CURSOR_SR_WM1_MASK	(0x3f<<0)
++#define DSPFW6			(VLV_DISPLAY_BASE + 0x70078)
++#define   DSPFW_SR_WM1_SHIFT		0
++#define   DSPFW_SR_WM1_MASK		(0x1ff<<0)
++#define DSPFW7			(VLV_DISPLAY_BASE + 0x7007c)
++#define DSPFW7_CHV		(VLV_DISPLAY_BASE + 0x700b4) /* wtf #1? */
++#define   DSPFW_SPRITED_WM1_SHIFT	24
++#define   DSPFW_SPRITED_WM1_MASK	(0xff<<24)
++#define   DSPFW_SPRITED_SHIFT		16
++#define   DSPFW_SPRITED_MASK		(0xff<<16)
++#define   DSPFW_SPRITEC_WM1_SHIFT	8
++#define   DSPFW_SPRITEC_WM1_MASK	(0xff<<8)
++#define   DSPFW_SPRITEC_SHIFT		0
++#define   DSPFW_SPRITEC_MASK		(0xff<<0)
++#define DSPFW8_CHV		(VLV_DISPLAY_BASE + 0x700b8)
++#define   DSPFW_SPRITEF_WM1_SHIFT	24
++#define   DSPFW_SPRITEF_WM1_MASK	(0xff<<24)
++#define   DSPFW_SPRITEF_SHIFT		16
++#define   DSPFW_SPRITEF_MASK		(0xff<<16)
++#define   DSPFW_SPRITEE_WM1_SHIFT	8
++#define   DSPFW_SPRITEE_WM1_MASK	(0xff<<8)
++#define   DSPFW_SPRITEE_SHIFT		0
++#define   DSPFW_SPRITEE_MASK		(0xff<<0)
++#define DSPFW9_CHV		(VLV_DISPLAY_BASE + 0x7007c) /* wtf #2? */
++#define   DSPFW_PLANEC_WM1_SHIFT	24
++#define   DSPFW_PLANEC_WM1_MASK		(0xff<<24)
++#define   DSPFW_PLANEC_SHIFT		16
++#define   DSPFW_PLANEC_MASK		(0xff<<16)
++#define   DSPFW_CURSORC_WM1_SHIFT	8
++#define   DSPFW_CURSORC_WM1_MASK	(0x3f<<16)
++#define   DSPFW_CURSORC_SHIFT		0
++#define   DSPFW_CURSORC_MASK		(0x3f<<0)
++
++/* vlv/chv high order bits */
++#define DSPHOWM			(VLV_DISPLAY_BASE + 0x70064)
++#define   DSPFW_SR_HI_SHIFT		24
++#define   DSPFW_SR_HI_MASK		(1<<24)
++#define   DSPFW_SPRITEF_HI_SHIFT	23
++#define   DSPFW_SPRITEF_HI_MASK		(1<<23)
++#define   DSPFW_SPRITEE_HI_SHIFT	22
++#define   DSPFW_SPRITEE_HI_MASK		(1<<22)
++#define   DSPFW_PLANEC_HI_SHIFT		21
++#define   DSPFW_PLANEC_HI_MASK		(1<<21)
++#define   DSPFW_SPRITED_HI_SHIFT	20
++#define   DSPFW_SPRITED_HI_MASK		(1<<20)
++#define   DSPFW_SPRITEC_HI_SHIFT	16
++#define   DSPFW_SPRITEC_HI_MASK		(1<<16)
++#define   DSPFW_PLANEB_HI_SHIFT		12
++#define   DSPFW_PLANEB_HI_MASK		(1<<12)
++#define   DSPFW_SPRITEB_HI_SHIFT	8
++#define   DSPFW_SPRITEB_HI_MASK		(1<<8)
++#define   DSPFW_SPRITEA_HI_SHIFT	4
++#define   DSPFW_SPRITEA_HI_MASK		(1<<4)
++#define   DSPFW_PLANEA_HI_SHIFT		0
++#define   DSPFW_PLANEA_HI_MASK		(1<<0)
++#define DSPHOWM1		(VLV_DISPLAY_BASE + 0x70068)
++#define   DSPFW_SR_WM1_HI_SHIFT		24
++#define   DSPFW_SR_WM1_HI_MASK		(1<<24)
++#define   DSPFW_SPRITEF_WM1_HI_SHIFT	23
++#define   DSPFW_SPRITEF_WM1_HI_MASK	(1<<23)
++#define   DSPFW_SPRITEE_WM1_HI_SHIFT	22
++#define   DSPFW_SPRITEE_WM1_HI_MASK	(1<<22)
++#define   DSPFW_PLANEC_WM1_HI_SHIFT	21
++#define   DSPFW_PLANEC_WM1_HI_MASK	(1<<21)
++#define   DSPFW_SPRITED_WM1_HI_SHIFT	20
++#define   DSPFW_SPRITED_WM1_HI_MASK	(1<<20)
++#define   DSPFW_SPRITEC_WM1_HI_SHIFT	16
++#define   DSPFW_SPRITEC_WM1_HI_MASK	(1<<16)
++#define   DSPFW_PLANEB_WM1_HI_SHIFT	12
++#define   DSPFW_PLANEB_WM1_HI_MASK	(1<<12)
++#define   DSPFW_SPRITEB_WM1_HI_SHIFT	8
++#define   DSPFW_SPRITEB_WM1_HI_MASK	(1<<8)
++#define   DSPFW_SPRITEA_WM1_HI_SHIFT	4
++#define   DSPFW_SPRITEA_WM1_HI_MASK	(1<<4)
++#define   DSPFW_PLANEA_WM1_HI_SHIFT	0
++#define   DSPFW_PLANEA_WM1_HI_MASK	(1<<0)
+ 
+ /* drain latency register values*/
++#define DRAIN_LATENCY_PRECISION_16	16
+ #define DRAIN_LATENCY_PRECISION_32	32
+ #define DRAIN_LATENCY_PRECISION_64	64
+-#define VLV_DDL1			(VLV_DISPLAY_BASE + 0x70050)
+-#define DDL_CURSORA_PRECISION_64	(1<<31)
+-#define DDL_CURSORA_PRECISION_32	(0<<31)
+-#define DDL_CURSORA_SHIFT		24
+-#define DDL_SPRITEB_PRECISION_64	(1<<23)
+-#define DDL_SPRITEB_PRECISION_32	(0<<23)
+-#define DDL_SPRITEB_SHIFT		16
+-#define DDL_SPRITEA_PRECISION_64	(1<<15)
+-#define DDL_SPRITEA_PRECISION_32	(0<<15)
+-#define DDL_SPRITEA_SHIFT		8
+-#define DDL_PLANEA_PRECISION_64		(1<<7)
+-#define DDL_PLANEA_PRECISION_32		(0<<7)
+-#define DDL_PLANEA_SHIFT		0
+-
+-#define VLV_DDL2			(VLV_DISPLAY_BASE + 0x70054)
+-#define DDL_CURSORB_PRECISION_64	(1<<31)
+-#define DDL_CURSORB_PRECISION_32	(0<<31)
+-#define DDL_CURSORB_SHIFT		24
+-#define DDL_SPRITED_PRECISION_64	(1<<23)
+-#define DDL_SPRITED_PRECISION_32	(0<<23)
+-#define DDL_SPRITED_SHIFT		16
+-#define DDL_SPRITEC_PRECISION_64	(1<<15)
+-#define DDL_SPRITEC_PRECISION_32	(0<<15)
+-#define DDL_SPRITEC_SHIFT		8
+-#define DDL_PLANEB_PRECISION_64		(1<<7)
+-#define DDL_PLANEB_PRECISION_32		(0<<7)
+-#define DDL_PLANEB_SHIFT		0
+-
+-#define VLV_DDL3			(VLV_DISPLAY_BASE + 0x70058)
+-#define DDL_CURSORC_PRECISION_64	(1<<31)
+-#define DDL_CURSORC_PRECISION_32	(0<<31)
+-#define DDL_CURSORC_SHIFT		24
+-#define DDL_SPRITEF_PRECISION_64	(1<<23)
+-#define DDL_SPRITEF_PRECISION_32	(0<<23)
+-#define DDL_SPRITEF_SHIFT		16
+-#define DDL_SPRITEE_PRECISION_64	(1<<15)
+-#define DDL_SPRITEE_PRECISION_32	(0<<15)
+-#define DDL_SPRITEE_SHIFT		8
+-#define DDL_PLANEC_PRECISION_64		(1<<7)
+-#define DDL_PLANEC_PRECISION_32		(0<<7)
+-#define DDL_PLANEC_SHIFT		0
++#define VLV_DDL(pipe)			(VLV_DISPLAY_BASE + 0x70050 + 4 * (pipe))
++#define DDL_CURSOR_PRECISION_HIGH	(1<<31)
++#define DDL_CURSOR_PRECISION_LOW	(0<<31)
++#define DDL_CURSOR_SHIFT		24
++#define DDL_SPRITE_PRECISION_HIGH(sprite)	(1<<(15+8*(sprite)))
++#define DDL_SPRITE_PRECISION_LOW(sprite)	(0<<(15+8*(sprite)))
++#define DDL_SPRITE_SHIFT(sprite)	(8+8*(sprite))
++#define DDL_PLANE_PRECISION_HIGH	(1<<7)
++#define DDL_PLANE_PRECISION_LOW		(0<<7)
++#define DDL_PLANE_SHIFT			0
++#define DRAIN_LATENCY_MASK		0x7f
+ 
+ /* FIFO watermark sizes etc */
+ #define G4X_FIFO_LINE_SIZE	64
+@@ -3943,6 +4118,41 @@
+ #define I965_CURSOR_MAX_WM	32
+ #define I965_CURSOR_DFT_WM	8
+ 
++/* Watermark register definitions for SKL */
++#define CUR_WM_A_0		0x70140
++#define CUR_WM_B_0		0x71140
++#define PLANE_WM_1_A_0		0x70240
++#define PLANE_WM_1_B_0		0x71240
++#define PLANE_WM_2_A_0		0x70340
++#define PLANE_WM_2_B_0		0x71340
++#define PLANE_WM_TRANS_1_A_0	0x70268
++#define PLANE_WM_TRANS_1_B_0	0x71268
++#define PLANE_WM_TRANS_2_A_0	0x70368
++#define PLANE_WM_TRANS_2_B_0	0x71368
++#define CUR_WM_TRANS_A_0	0x70168
++#define CUR_WM_TRANS_B_0	0x71168
++#define   PLANE_WM_EN		(1 << 31)
++#define   PLANE_WM_LINES_SHIFT	14
++#define   PLANE_WM_LINES_MASK	0x1f
++#define   PLANE_WM_BLOCKS_MASK	0x3ff
++
++#define CUR_WM_0(pipe) _PIPE(pipe, CUR_WM_A_0, CUR_WM_B_0)
++#define CUR_WM(pipe, level) (CUR_WM_0(pipe) + ((4) * (level)))
++#define CUR_WM_TRANS(pipe) _PIPE(pipe, CUR_WM_TRANS_A_0, CUR_WM_TRANS_B_0)
++
++#define _PLANE_WM_1(pipe) _PIPE(pipe, PLANE_WM_1_A_0, PLANE_WM_1_B_0)
++#define _PLANE_WM_2(pipe) _PIPE(pipe, PLANE_WM_2_A_0, PLANE_WM_2_B_0)
++#define _PLANE_WM_BASE(pipe, plane)	\
++			_PLANE(plane, _PLANE_WM_1(pipe), _PLANE_WM_2(pipe))
++#define PLANE_WM(pipe, plane, level)	\
++			(_PLANE_WM_BASE(pipe, plane) + ((4) * (level)))
++#define _PLANE_WM_TRANS_1(pipe)	\
++			_PIPE(pipe, PLANE_WM_TRANS_1_A_0, PLANE_WM_TRANS_1_B_0)
++#define _PLANE_WM_TRANS_2(pipe)	\
++			_PIPE(pipe, PLANE_WM_TRANS_2_A_0, PLANE_WM_TRANS_2_B_0)
++#define PLANE_WM_TRANS(pipe, plane)	\
++		_PLANE(plane, _PLANE_WM_TRANS_1(pipe), _PLANE_WM_TRANS_2(pipe))
++
+ /* define the Watermark register on Ironlake */
+ #define WM0_PIPEA_ILK		0x45100
+ #define  WM0_PIPE_PLANE_MASK	(0xffff<<16)
+@@ -4026,7 +4236,8 @@
+ /* Old style CUR*CNTR flags (desktop 8xx) */
+ #define   CURSOR_ENABLE		0x80000000
+ #define   CURSOR_GAMMA_ENABLE	0x40000000
+-#define   CURSOR_STRIDE_MASK	0x30000000
++#define   CURSOR_STRIDE_SHIFT	28
++#define   CURSOR_STRIDE(x)	((ffs(x)-9) << CURSOR_STRIDE_SHIFT) /* 256,512,1k,2k */
+ #define   CURSOR_PIPE_CSC_ENABLE (1<<24)
+ #define   CURSOR_FORMAT_SHIFT	24
+ #define   CURSOR_FORMAT_MASK	(0x07 << CURSOR_FORMAT_SHIFT)
+@@ -4048,6 +4259,7 @@
+ #define   MCURSOR_PIPE_A	0x00
+ #define   MCURSOR_PIPE_B	(1 << 28)
+ #define   MCURSOR_GAMMA_ENABLE  (1 << 26)
++#define   CURSOR_ROTATE_180	(1<<15)
+ #define   CURSOR_TRICKLE_FEED_DISABLE	(1 << 14)
+ #define _CURABASE		0x70084
+ #define _CURAPOS		0x70088
+@@ -4111,8 +4323,11 @@
+ #define   DISPPLANE_NO_LINE_DOUBLE		0
+ #define   DISPPLANE_STEREO_POLARITY_FIRST	0
+ #define   DISPPLANE_STEREO_POLARITY_SECOND	(1<<18)
++#define   DISPPLANE_ALPHA_PREMULTIPLY		(1<<16) /* CHV pipe B */
++#define   DISPPLANE_ROTATE_180			(1<<15)
+ #define   DISPPLANE_TRICKLE_FEED_DISABLE	(1<<14) /* Ironlake */
+ #define   DISPPLANE_TILED			(1<<10)
++#define   DISPPLANE_MIRROR			(1<<8) /* CHV pipe B */
+ #define _DSPAADDR				0x70184
+ #define _DSPASTRIDE				0x70188
+ #define _DSPAPOS				0x7018C /* reserved */
+@@ -4133,6 +4348,24 @@
+ #define DSPOFFSET(plane) _PIPE2(plane, _DSPAOFFSET)
+ #define DSPSURFLIVE(plane) _PIPE2(plane, _DSPASURFLIVE)
+ 
++/* CHV pipe B blender and primary plane */
++#define _CHV_BLEND_A		0x60a00
++#define   CHV_BLEND_LEGACY		(0<<30)
++#define   CHV_BLEND_ANDROID		(1<<30)
++#define   CHV_BLEND_MPO			(2<<30)
++#define   CHV_BLEND_MASK		(3<<30)
++#define _CHV_CANVAS_A		0x60a04
++#define _PRIMPOS_A		0x60a08
++#define _PRIMSIZE_A		0x60a0c
++#define _PRIMCNSTALPHA_A	0x60a10
++#define   PRIM_CONST_ALPHA_ENABLE	(1<<31)
++
++#define CHV_BLEND(pipe) _TRANSCODER2(pipe, _CHV_BLEND_A)
++#define CHV_CANVAS(pipe) _TRANSCODER2(pipe, _CHV_CANVAS_A)
++#define PRIMPOS(plane) _TRANSCODER2(plane, _PRIMPOS_A)
++#define PRIMSIZE(plane) _TRANSCODER2(plane, _PRIMSIZE_A)
++#define PRIMCNSTALPHA(plane) _TRANSCODER2(plane, _PRIMCNSTALPHA_A)
++
+ /* Display/Sprite base address macros */
+ #define DISP_BASEADDR_MASK	(0xfffff000)
+ #define I915_LO_DISPBASE(val)	(val & ~DISP_BASEADDR_MASK)
+@@ -4195,6 +4428,7 @@
+ #define   DVS_YUV_ORDER_UYVY	(1<<16)
+ #define   DVS_YUV_ORDER_YVYU	(2<<16)
+ #define   DVS_YUV_ORDER_VYUY	(3<<16)
++#define   DVS_ROTATE_180	(1<<15)
+ #define   DVS_DEST_KEY		(1<<2)
+ #define   DVS_TRICKLE_FEED_DISABLE (1<<14)
+ #define   DVS_TILED		(1<<10)
+@@ -4265,6 +4499,7 @@
+ #define   SPRITE_YUV_ORDER_UYVY		(1<<16)
+ #define   SPRITE_YUV_ORDER_YVYU		(2<<16)
+ #define   SPRITE_YUV_ORDER_VYUY		(3<<16)
++#define   SPRITE_ROTATE_180		(1<<15)
+ #define   SPRITE_TRICKLE_FEED_DISABLE	(1<<14)
+ #define   SPRITE_INT_GAMMA_ENABLE	(1<<13)
+ #define   SPRITE_TILED			(1<<10)
+@@ -4332,13 +4567,16 @@
+ #define   SP_FORMAT_RGBA1010102		(9<<26)
+ #define   SP_FORMAT_RGBX8888		(0xe<<26)
+ #define   SP_FORMAT_RGBA8888		(0xf<<26)
++#define   SP_ALPHA_PREMULTIPLY		(1<<23) /* CHV pipe B */
+ #define   SP_SOURCE_KEY			(1<<22)
+ #define   SP_YUV_BYTE_ORDER_MASK	(3<<16)
+ #define   SP_YUV_ORDER_YUYV		(0<<16)
+ #define   SP_YUV_ORDER_UYVY		(1<<16)
+ #define   SP_YUV_ORDER_YVYU		(2<<16)
+ #define   SP_YUV_ORDER_VYUY		(3<<16)
++#define   SP_ROTATE_180			(1<<15)
+ #define   SP_TILED			(1<<10)
++#define   SP_MIRROR			(1<<8) /* CHV pipe B */
+ #define _SPALINOFF		(VLV_DISPLAY_BASE + 0x72184)
+ #define _SPASTRIDE		(VLV_DISPLAY_BASE + 0x72188)
+ #define _SPAPOS			(VLV_DISPLAY_BASE + 0x7218c)
+@@ -4349,6 +4587,7 @@
+ #define _SPAKEYMAXVAL		(VLV_DISPLAY_BASE + 0x721a0)
+ #define _SPATILEOFF		(VLV_DISPLAY_BASE + 0x721a4)
+ #define _SPACONSTALPHA		(VLV_DISPLAY_BASE + 0x721a8)
++#define   SP_CONST_ALPHA_ENABLE		(1<<31)
+ #define _SPAGAMC		(VLV_DISPLAY_BASE + 0x721f4)
+ 
+ #define _SPBCNTR		(VLV_DISPLAY_BASE + 0x72280)
+@@ -4377,6 +4616,195 @@
+ #define SPCONSTALPHA(pipe, plane) _PIPE(pipe * 2 + plane, _SPACONSTALPHA, _SPBCONSTALPHA)
+ #define SPGAMC(pipe, plane) _PIPE(pipe * 2 + plane, _SPAGAMC, _SPBGAMC)
+ 
++/*
++ * CHV pipe B sprite CSC
++ *
++ * |cr|   |c0 c1 c2|   |cr + cr_ioff|   |cr_ooff|
++ * |yg| = |c3 c4 c5| x |yg + yg_ioff| + |yg_ooff|
++ * |cb|   |c6 c7 c8|   |cb + cr_ioff|   |cb_ooff|
++ */
++#define SPCSCYGOFF(sprite)	(VLV_DISPLAY_BASE + 0x6d900 + (sprite) * 0x1000)
++#define SPCSCCBOFF(sprite)	(VLV_DISPLAY_BASE + 0x6d904 + (sprite) * 0x1000)
++#define SPCSCCROFF(sprite)	(VLV_DISPLAY_BASE + 0x6d908 + (sprite) * 0x1000)
++#define  SPCSC_OOFF(x)		(((x) & 0x7ff) << 16) /* s11 */
++#define  SPCSC_IOFF(x)		(((x) & 0x7ff) << 0) /* s11 */
++
++#define SPCSCC01(sprite)	(VLV_DISPLAY_BASE + 0x6d90c + (sprite) * 0x1000)
++#define SPCSCC23(sprite)	(VLV_DISPLAY_BASE + 0x6d910 + (sprite) * 0x1000)
++#define SPCSCC45(sprite)	(VLV_DISPLAY_BASE + 0x6d914 + (sprite) * 0x1000)
++#define SPCSCC67(sprite)	(VLV_DISPLAY_BASE + 0x6d918 + (sprite) * 0x1000)
++#define SPCSCC8(sprite)		(VLV_DISPLAY_BASE + 0x6d91c + (sprite) * 0x1000)
++#define  SPCSC_C1(x)		(((x) & 0x7fff) << 16) /* s3.12 */
++#define  SPCSC_C0(x)		(((x) & 0x7fff) << 0) /* s3.12 */
++
++#define SPCSCYGICLAMP(sprite)	(VLV_DISPLAY_BASE + 0x6d920 + (sprite) * 0x1000)
++#define SPCSCCBICLAMP(sprite)	(VLV_DISPLAY_BASE + 0x6d924 + (sprite) * 0x1000)
++#define SPCSCCRICLAMP(sprite)	(VLV_DISPLAY_BASE + 0x6d928 + (sprite) * 0x1000)
++#define  SPCSC_IMAX(x)		(((x) & 0x7ff) << 16) /* s11 */
++#define  SPCSC_IMIN(x)		(((x) & 0x7ff) << 0) /* s11 */
++
++#define SPCSCYGOCLAMP(sprite)	(VLV_DISPLAY_BASE + 0x6d92c + (sprite) * 0x1000)
++#define SPCSCCBOCLAMP(sprite)	(VLV_DISPLAY_BASE + 0x6d930 + (sprite) * 0x1000)
++#define SPCSCCROCLAMP(sprite)	(VLV_DISPLAY_BASE + 0x6d934 + (sprite) * 0x1000)
++#define  SPCSC_OMAX(x)		((x) << 16) /* u10 */
++#define  SPCSC_OMIN(x)		((x) << 0) /* u10 */
++
++/* Skylake plane registers */
++
++#define _PLANE_CTL_1_A				0x70180
++#define _PLANE_CTL_2_A				0x70280
++#define _PLANE_CTL_3_A				0x70380
++#define   PLANE_CTL_ENABLE			(1 << 31)
++#define   PLANE_CTL_PIPE_GAMMA_ENABLE		(1 << 30)
++#define   PLANE_CTL_FORMAT_MASK			(0xf << 24)
++#define   PLANE_CTL_FORMAT_YUV422		(  0 << 24)
++#define   PLANE_CTL_FORMAT_NV12			(  1 << 24)
++#define   PLANE_CTL_FORMAT_XRGB_2101010		(  2 << 24)
++#define   PLANE_CTL_FORMAT_XRGB_8888		(  4 << 24)
++#define   PLANE_CTL_FORMAT_XRGB_16161616F	(  6 << 24)
++#define   PLANE_CTL_FORMAT_AYUV			(  8 << 24)
++#define   PLANE_CTL_FORMAT_INDEXED		( 12 << 24)
++#define   PLANE_CTL_FORMAT_RGB_565		( 14 << 24)
++#define   PLANE_CTL_PIPE_CSC_ENABLE		(1 << 23)
++#define   PLANE_CTL_KEY_ENABLE_MASK		(0x3 << 21)
++#define   PLANE_CTL_KEY_ENABLE_SOURCE		(  1 << 21)
++#define   PLANE_CTL_KEY_ENABLE_DESTINATION	(  2 << 21)
++#define   PLANE_CTL_ORDER_BGRX			(0 << 20)
++#define   PLANE_CTL_ORDER_RGBX			(1 << 20)
++#define   PLANE_CTL_YUV422_ORDER_MASK		(0x3 << 16)
++#define   PLANE_CTL_YUV422_YUYV			(  0 << 16)
++#define   PLANE_CTL_YUV422_UYVY			(  1 << 16)
++#define   PLANE_CTL_YUV422_YVYU			(  2 << 16)
++#define   PLANE_CTL_YUV422_VYUY			(  3 << 16)
++#define   PLANE_CTL_DECOMPRESSION_ENABLE	(1 << 15)
++#define   PLANE_CTL_TRICKLE_FEED_DISABLE	(1 << 14)
++#define   PLANE_CTL_PLANE_GAMMA_DISABLE		(1 << 13)
++#define   PLANE_CTL_TILED_MASK			(0x7 << 10)
++#define   PLANE_CTL_TILED_LINEAR		(  0 << 10)
++#define   PLANE_CTL_TILED_X			(  1 << 10)
++#define   PLANE_CTL_TILED_Y			(  4 << 10)
++#define   PLANE_CTL_TILED_YF			(  5 << 10)
++#define   PLANE_CTL_ALPHA_MASK			(0x3 << 4)
++#define   PLANE_CTL_ALPHA_DISABLE		(  0 << 4)
++#define   PLANE_CTL_ALPHA_SW_PREMULTIPLY	(  2 << 4)
++#define   PLANE_CTL_ALPHA_HW_PREMULTIPLY	(  3 << 4)
++#define   PLANE_CTL_ROTATE_MASK			0x3
++#define   PLANE_CTL_ROTATE_0			0x0
++#define   PLANE_CTL_ROTATE_180			0x2
++#define _PLANE_STRIDE_1_A			0x70188
++#define _PLANE_STRIDE_2_A			0x70288
++#define _PLANE_STRIDE_3_A			0x70388
++#define _PLANE_POS_1_A				0x7018c
++#define _PLANE_POS_2_A				0x7028c
++#define _PLANE_POS_3_A				0x7038c
++#define _PLANE_SIZE_1_A				0x70190
++#define _PLANE_SIZE_2_A				0x70290
++#define _PLANE_SIZE_3_A				0x70390
++#define _PLANE_SURF_1_A				0x7019c
++#define _PLANE_SURF_2_A				0x7029c
++#define _PLANE_SURF_3_A				0x7039c
++#define _PLANE_OFFSET_1_A			0x701a4
++#define _PLANE_OFFSET_2_A			0x702a4
++#define _PLANE_OFFSET_3_A			0x703a4
++#define _PLANE_KEYVAL_1_A			0x70194
++#define _PLANE_KEYVAL_2_A			0x70294
++#define _PLANE_KEYMSK_1_A			0x70198
++#define _PLANE_KEYMSK_2_A			0x70298
++#define _PLANE_KEYMAX_1_A			0x701a0
++#define _PLANE_KEYMAX_2_A			0x702a0
++#define _PLANE_BUF_CFG_1_A			0x7027c
++#define _PLANE_BUF_CFG_2_A			0x7037c
++
++#define _PLANE_CTL_1_B				0x71180
++#define _PLANE_CTL_2_B				0x71280
++#define _PLANE_CTL_3_B				0x71380
++#define _PLANE_CTL_1(pipe)	_PIPE(pipe, _PLANE_CTL_1_A, _PLANE_CTL_1_B)
++#define _PLANE_CTL_2(pipe)	_PIPE(pipe, _PLANE_CTL_2_A, _PLANE_CTL_2_B)
++#define _PLANE_CTL_3(pipe)	_PIPE(pipe, _PLANE_CTL_3_A, _PLANE_CTL_3_B)
++#define PLANE_CTL(pipe, plane)	\
++	_PLANE(plane, _PLANE_CTL_1(pipe), _PLANE_CTL_2(pipe))
++
++#define _PLANE_STRIDE_1_B			0x71188
++#define _PLANE_STRIDE_2_B			0x71288
++#define _PLANE_STRIDE_3_B			0x71388
++#define _PLANE_STRIDE_1(pipe)	\
++	_PIPE(pipe, _PLANE_STRIDE_1_A, _PLANE_STRIDE_1_B)
++#define _PLANE_STRIDE_2(pipe)	\
++	_PIPE(pipe, _PLANE_STRIDE_2_A, _PLANE_STRIDE_2_B)
++#define _PLANE_STRIDE_3(pipe)	\
++	_PIPE(pipe, _PLANE_STRIDE_3_A, _PLANE_STRIDE_3_B)
++#define PLANE_STRIDE(pipe, plane)	\
++	_PLANE(plane, _PLANE_STRIDE_1(pipe), _PLANE_STRIDE_2(pipe))
++
++#define _PLANE_POS_1_B				0x7118c
++#define _PLANE_POS_2_B				0x7128c
++#define _PLANE_POS_3_B				0x7138c
++#define _PLANE_POS_1(pipe)	_PIPE(pipe, _PLANE_POS_1_A, _PLANE_POS_1_B)
++#define _PLANE_POS_2(pipe)	_PIPE(pipe, _PLANE_POS_2_A, _PLANE_POS_2_B)
++#define _PLANE_POS_3(pipe)	_PIPE(pipe, _PLANE_POS_3_A, _PLANE_POS_3_B)
++#define PLANE_POS(pipe, plane)	\
++	_PLANE(plane, _PLANE_POS_1(pipe), _PLANE_POS_2(pipe))
++
++#define _PLANE_SIZE_1_B				0x71190
++#define _PLANE_SIZE_2_B				0x71290
++#define _PLANE_SIZE_3_B				0x71390
++#define _PLANE_SIZE_1(pipe)	_PIPE(pipe, _PLANE_SIZE_1_A, _PLANE_SIZE_1_B)
++#define _PLANE_SIZE_2(pipe)	_PIPE(pipe, _PLANE_SIZE_2_A, _PLANE_SIZE_2_B)
++#define _PLANE_SIZE_3(pipe)	_PIPE(pipe, _PLANE_SIZE_3_A, _PLANE_SIZE_3_B)
++#define PLANE_SIZE(pipe, plane)	\
++	_PLANE(plane, _PLANE_SIZE_1(pipe), _PLANE_SIZE_2(pipe))
++
++#define _PLANE_SURF_1_B				0x7119c
++#define _PLANE_SURF_2_B				0x7129c
++#define _PLANE_SURF_3_B				0x7139c
++#define _PLANE_SURF_1(pipe)	_PIPE(pipe, _PLANE_SURF_1_A, _PLANE_SURF_1_B)
++#define _PLANE_SURF_2(pipe)	_PIPE(pipe, _PLANE_SURF_2_A, _PLANE_SURF_2_B)
++#define _PLANE_SURF_3(pipe)	_PIPE(pipe, _PLANE_SURF_3_A, _PLANE_SURF_3_B)
++#define PLANE_SURF(pipe, plane)	\
++	_PLANE(plane, _PLANE_SURF_1(pipe), _PLANE_SURF_2(pipe))
++
++#define _PLANE_OFFSET_1_B			0x711a4
++#define _PLANE_OFFSET_2_B			0x712a4
++#define _PLANE_OFFSET_1(pipe) _PIPE(pipe, _PLANE_OFFSET_1_A, _PLANE_OFFSET_1_B)
++#define _PLANE_OFFSET_2(pipe) _PIPE(pipe, _PLANE_OFFSET_2_A, _PLANE_OFFSET_2_B)
++#define PLANE_OFFSET(pipe, plane)	\
++	_PLANE(plane, _PLANE_OFFSET_1(pipe), _PLANE_OFFSET_2(pipe))
++
++#define _PLANE_KEYVAL_1_B			0x71194
++#define _PLANE_KEYVAL_2_B			0x71294
++#define _PLANE_KEYVAL_1(pipe) _PIPE(pipe, _PLANE_KEYVAL_1_A, _PLANE_KEYVAL_1_B)
++#define _PLANE_KEYVAL_2(pipe) _PIPE(pipe, _PLANE_KEYVAL_2_A, _PLANE_KEYVAL_2_B)
++#define PLANE_KEYVAL(pipe, plane)	\
++	_PLANE(plane, _PLANE_KEYVAL_1(pipe), _PLANE_KEYVAL_2(pipe))
++
++#define _PLANE_KEYMSK_1_B			0x71198
++#define _PLANE_KEYMSK_2_B			0x71298
++#define _PLANE_KEYMSK_1(pipe) _PIPE(pipe, _PLANE_KEYMSK_1_A, _PLANE_KEYMSK_1_B)
++#define _PLANE_KEYMSK_2(pipe) _PIPE(pipe, _PLANE_KEYMSK_2_A, _PLANE_KEYMSK_2_B)
++#define PLANE_KEYMSK(pipe, plane)	\
++	_PLANE(plane, _PLANE_KEYMSK_1(pipe), _PLANE_KEYMSK_2(pipe))
++
++#define _PLANE_KEYMAX_1_B			0x711a0
++#define _PLANE_KEYMAX_2_B			0x712a0
++#define _PLANE_KEYMAX_1(pipe) _PIPE(pipe, _PLANE_KEYMAX_1_A, _PLANE_KEYMAX_1_B)
++#define _PLANE_KEYMAX_2(pipe) _PIPE(pipe, _PLANE_KEYMAX_2_A, _PLANE_KEYMAX_2_B)
++#define PLANE_KEYMAX(pipe, plane)	\
++	_PLANE(plane, _PLANE_KEYMAX_1(pipe), _PLANE_KEYMAX_2(pipe))
++
++#define _PLANE_BUF_CFG_1_B			0x7127c
++#define _PLANE_BUF_CFG_2_B			0x7137c
++#define _PLANE_BUF_CFG_1(pipe)	\
++	_PIPE(pipe, _PLANE_BUF_CFG_1_A, _PLANE_BUF_CFG_1_B)
++#define _PLANE_BUF_CFG_2(pipe)	\
++	_PIPE(pipe, _PLANE_BUF_CFG_2_A, _PLANE_BUF_CFG_2_B)
++#define PLANE_BUF_CFG(pipe, plane)	\
++	_PLANE(plane, _PLANE_BUF_CFG_1(pipe), _PLANE_BUF_CFG_2(pipe))
++
++/* SKL new cursor registers */
++#define _CUR_BUF_CFG_A				0x7017c
++#define _CUR_BUF_CFG_B				0x7117c
++#define CUR_BUF_CFG(pipe)	_PIPE(pipe, _CUR_BUF_CFG_A, _CUR_BUF_CFG_B)
++
+ /* VBIOS regs */
+ #define VGACNTRL		0x71400
+ # define VGA_DISP_DISABLE			(1 << 31)
+@@ -4492,6 +4920,18 @@
+ #define PF_VSCALE(pipe)		_PIPE(pipe, _PFA_VSCALE, _PFB_VSCALE)
+ #define PF_HSCALE(pipe)		_PIPE(pipe, _PFA_HSCALE, _PFB_HSCALE)
+ 
++#define _PSA_CTL		0x68180
++#define _PSB_CTL		0x68980
++#define PS_ENABLE		(1<<31)
++#define _PSA_WIN_SZ		0x68174
++#define _PSB_WIN_SZ		0x68974
++#define _PSA_WIN_POS		0x68170
++#define _PSB_WIN_POS		0x68970
++
++#define PS_CTL(pipe)		_PIPE(pipe, _PSA_CTL, _PSB_CTL)
++#define PS_WIN_SZ(pipe)		_PIPE(pipe, _PSA_WIN_SZ, _PSB_WIN_SZ)
++#define PS_WIN_POS(pipe)	_PIPE(pipe, _PSA_WIN_POS, _PSB_WIN_POS)
++
+ /* legacy palette */
+ #define _LGC_PALETTE_A           0x4a000
+ #define _LGC_PALETTE_B           0x4a800
+@@ -4613,16 +5053,32 @@
+ #define  GEN8_PIPE_SCAN_LINE_EVENT	(1 << 2)
+ #define  GEN8_PIPE_VSYNC		(1 << 1)
+ #define  GEN8_PIPE_VBLANK		(1 << 0)
++#define  GEN9_PIPE_CURSOR_FAULT		(1 << 11)
++#define  GEN9_PIPE_PLANE3_FAULT		(1 << 9)
++#define  GEN9_PIPE_PLANE2_FAULT		(1 << 8)
++#define  GEN9_PIPE_PLANE1_FAULT		(1 << 7)
++#define  GEN9_PIPE_PLANE3_FLIP_DONE	(1 << 5)
++#define  GEN9_PIPE_PLANE2_FLIP_DONE	(1 << 4)
++#define  GEN9_PIPE_PLANE1_FLIP_DONE	(1 << 3)
++#define  GEN9_PIPE_PLANE_FLIP_DONE(p)	(1 << (3 + p))
+ #define GEN8_DE_PIPE_IRQ_FAULT_ERRORS \
+ 	(GEN8_PIPE_CURSOR_FAULT | \
+ 	 GEN8_PIPE_SPRITE_FAULT | \
+ 	 GEN8_PIPE_PRIMARY_FAULT)
++#define GEN9_DE_PIPE_IRQ_FAULT_ERRORS \
++	(GEN9_PIPE_CURSOR_FAULT | \
++	 GEN9_PIPE_PLANE3_FAULT | \
++	 GEN9_PIPE_PLANE2_FAULT | \
++	 GEN9_PIPE_PLANE1_FAULT)
+ 
+ #define GEN8_DE_PORT_ISR 0x44440
+ #define GEN8_DE_PORT_IMR 0x44444
+ #define GEN8_DE_PORT_IIR 0x44448
+ #define GEN8_DE_PORT_IER 0x4444c
+ #define  GEN8_PORT_DP_A_HOTPLUG		(1 << 3)
++#define  GEN9_AUX_CHANNEL_D		(1 << 27)
++#define  GEN9_AUX_CHANNEL_C		(1 << 26)
++#define  GEN9_AUX_CHANNEL_B		(1 << 25)
+ #define  GEN8_AUX_CHANNEL_A		(1 << 0)
+ 
+ #define GEN8_DE_MISC_ISR 0x44460
+@@ -4706,6 +5162,8 @@
+ /* GEN8 chicken */
+ #define HDC_CHICKEN0				0x7300
+ #define  HDC_FORCE_NON_COHERENT			(1<<4)
++#define  HDC_DONOT_FETCH_MEM_WHEN_MASKED	(1<<11)
++#define  HDC_FENCE_DEST_SLM_DISABLE		(1<<14)
+ 
+ /* WaCatErrorRejectionIssue */
+ #define GEN7_SQ_CHICKEN_MBCUNIT_CONFIG		0x9030
+@@ -5246,8 +5704,7 @@
+ #define PIPEA_PP_STATUS         (VLV_DISPLAY_BASE + 0x61200)
+ #define PIPEA_PP_CONTROL        (VLV_DISPLAY_BASE + 0x61204)
+ #define PIPEA_PP_ON_DELAYS      (VLV_DISPLAY_BASE + 0x61208)
+-#define  PANEL_PORT_SELECT_DPB_VLV	(1 << 30)
+-#define  PANEL_PORT_SELECT_DPC_VLV	(2 << 30)
++#define  PANEL_PORT_SELECT_VLV(port)	((port) << 30)
+ #define PIPEA_PP_OFF_DELAYS     (VLV_DISPLAY_BASE + 0x6120c)
+ #define PIPEA_PP_DIVISOR        (VLV_DISPLAY_BASE + 0x61210)
+ 
+@@ -5407,8 +5864,13 @@
+ #define   VLV_GTLC_ALLOWWAKEERR			(1 << 1)
+ #define   VLV_GTLC_PW_MEDIA_STATUS_MASK		(1 << 5)
+ #define   VLV_GTLC_PW_RENDER_STATUS_MASK	(1 << 7)
+-#define VLV_GTLC_SURVIVABILITY_REG              0x130098
+ #define  FORCEWAKE_MT				0xa188 /* multi-threaded */
++#define  FORCEWAKE_MEDIA_GEN9			0xa270
++#define  FORCEWAKE_RENDER_GEN9			0xa278
++#define  FORCEWAKE_BLITTER_GEN9			0xa188
++#define  FORCEWAKE_ACK_MEDIA_GEN9		0x0D88
++#define  FORCEWAKE_ACK_RENDER_GEN9		0x0D84
++#define  FORCEWAKE_ACK_BLITTER_GEN9		0x130044
+ #define   FORCEWAKE_KERNEL			0x1
+ #define   FORCEWAKE_USER			0x2
+ #define  FORCEWAKE_MT_ACK			0x130040
+@@ -5545,12 +6007,6 @@
+ 						 GEN6_PM_RP_DOWN_THRESHOLD | \
+ 						 GEN6_PM_RP_DOWN_TIMEOUT)
+ 
+-#define CHV_CZ_CLOCK_FREQ_MODE_200			200
+-#define CHV_CZ_CLOCK_FREQ_MODE_267			267
+-#define CHV_CZ_CLOCK_FREQ_MODE_320			320
+-#define CHV_CZ_CLOCK_FREQ_MODE_333			333
+-#define CHV_CZ_CLOCK_FREQ_MODE_400			400
+-
+ #define GEN7_GT_SCRATCH_BASE			0x4F100
+ #define GEN7_GT_SCRATCH_REG_NUM			8
+ 
+@@ -5571,8 +6027,8 @@
+ 
+ #define GEN6_GT_GFX_RC6p			0x13810C
+ #define GEN6_GT_GFX_RC6pp			0x138110
+-#define VLV_RENDER_C0_COUNT_REG		0x138118
+-#define VLV_MEDIA_C0_COUNT_REG			0x13811C
++#define VLV_RENDER_C0_COUNT			0x138118
++#define VLV_MEDIA_C0_COUNT			0x13811C
+ 
+ #define GEN6_PCODE_MAILBOX			0x138124
+ #define   GEN6_PCODE_READY			(1<<31)
+@@ -5589,6 +6045,13 @@
+ #define GEN6_PCODE_DATA				0x138128
+ #define   GEN6_PCODE_FREQ_IA_RATIO_SHIFT	8
+ #define   GEN6_PCODE_FREQ_RING_RATIO_SHIFT	16
++#define GEN6_PCODE_DATA1			0x13812C
++
++#define   GEN9_PCODE_READ_MEM_LATENCY		0x6
++#define   GEN9_MEM_LATENCY_LEVEL_MASK		0xFF
++#define   GEN9_MEM_LATENCY_LEVEL_1_5_SHIFT	8
++#define   GEN9_MEM_LATENCY_LEVEL_2_6_SHIFT	16
++#define   GEN9_MEM_LATENCY_LEVEL_3_7_SHIFT	24
+ 
+ #define GEN6_GT_CORE_STATUS		0x138060
+ #define   GEN6_CORE_CPD_STATE_MASK	(7<<4)
+@@ -5626,6 +6089,9 @@
+ #define   GEN7_SINGLE_SUBSCAN_DISPATCH_ENABLE	(1<<10)
+ #define   GEN7_PSD_SINGLE_PORT_DISPATCH_ENABLE	(1<<3)
+ 
++#define GEN9_HALF_SLICE_CHICKEN5	0xe188
++#define   GEN9_DG_MIRROR_FIX_ENABLE	(1<<5)
++
+ #define GEN8_ROW_CHICKEN		0xe4f0
+ #define   PARTIAL_INSTRUCTION_SHOOTDOWN_DISABLE	(1<<8)
+ #define   STALL_DOP_GATING_DISABLE		(1<<5)
+@@ -5641,57 +6107,58 @@
+ #define   GEN8_CENTROID_PIXEL_OPT_DIS	(1<<8)
+ #define   GEN8_SAMPLER_POWER_BYPASS_DIS	(1<<1)
+ 
++/* Audio */
+ #define G4X_AUD_VID_DID			(dev_priv->info.display_mmio_offset + 0x62020)
+-#define INTEL_AUDIO_DEVCL		0x808629FB
+-#define INTEL_AUDIO_DEVBLC		0x80862801
+-#define INTEL_AUDIO_DEVCTG		0x80862802
++#define   INTEL_AUDIO_DEVCL		0x808629FB
++#define   INTEL_AUDIO_DEVBLC		0x80862801
++#define   INTEL_AUDIO_DEVCTG		0x80862802
+ 
+ #define G4X_AUD_CNTL_ST			0x620B4
+-#define G4X_ELDV_DEVCL_DEVBLC		(1 << 13)
+-#define G4X_ELDV_DEVCTG			(1 << 14)
+-#define G4X_ELD_ADDR			(0xf << 5)
+-#define G4X_ELD_ACK			(1 << 4)
++#define   G4X_ELDV_DEVCL_DEVBLC		(1 << 13)
++#define   G4X_ELDV_DEVCTG		(1 << 14)
++#define   G4X_ELD_ADDR_MASK		(0xf << 5)
++#define   G4X_ELD_ACK			(1 << 4)
+ #define G4X_HDMIW_HDMIEDID		0x6210C
+ 
+-#define IBX_HDMIW_HDMIEDID_A		0xE2050
+-#define IBX_HDMIW_HDMIEDID_B		0xE2150
++#define _IBX_HDMIW_HDMIEDID_A		0xE2050
++#define _IBX_HDMIW_HDMIEDID_B		0xE2150
+ #define IBX_HDMIW_HDMIEDID(pipe) _PIPE(pipe, \
+-					IBX_HDMIW_HDMIEDID_A, \
+-					IBX_HDMIW_HDMIEDID_B)
+-#define IBX_AUD_CNTL_ST_A		0xE20B4
+-#define IBX_AUD_CNTL_ST_B		0xE21B4
++					_IBX_HDMIW_HDMIEDID_A, \
++					_IBX_HDMIW_HDMIEDID_B)
++#define _IBX_AUD_CNTL_ST_A		0xE20B4
++#define _IBX_AUD_CNTL_ST_B		0xE21B4
+ #define IBX_AUD_CNTL_ST(pipe) _PIPE(pipe, \
+-					IBX_AUD_CNTL_ST_A, \
+-					IBX_AUD_CNTL_ST_B)
+-#define IBX_ELD_BUFFER_SIZE		(0x1f << 10)
+-#define IBX_ELD_ADDRESS			(0x1f << 5)
+-#define IBX_ELD_ACK			(1 << 4)
++					_IBX_AUD_CNTL_ST_A, \
++					_IBX_AUD_CNTL_ST_B)
++#define   IBX_ELD_BUFFER_SIZE_MASK	(0x1f << 10)
++#define   IBX_ELD_ADDRESS_MASK		(0x1f << 5)
++#define   IBX_ELD_ACK			(1 << 4)
+ #define IBX_AUD_CNTL_ST2		0xE20C0
+-#define IBX_ELD_VALIDB			(1 << 0)
+-#define IBX_CP_READYB			(1 << 1)
++#define   IBX_CP_READY(port)		((1 << 1) << (((port) - 1) * 4))
++#define   IBX_ELD_VALID(port)		((1 << 0) << (((port) - 1) * 4))
+ 
+-#define CPT_HDMIW_HDMIEDID_A		0xE5050
+-#define CPT_HDMIW_HDMIEDID_B		0xE5150
++#define _CPT_HDMIW_HDMIEDID_A		0xE5050
++#define _CPT_HDMIW_HDMIEDID_B		0xE5150
+ #define CPT_HDMIW_HDMIEDID(pipe) _PIPE(pipe, \
+-					CPT_HDMIW_HDMIEDID_A, \
+-					CPT_HDMIW_HDMIEDID_B)
+-#define CPT_AUD_CNTL_ST_A		0xE50B4
+-#define CPT_AUD_CNTL_ST_B		0xE51B4
++					_CPT_HDMIW_HDMIEDID_A, \
++					_CPT_HDMIW_HDMIEDID_B)
++#define _CPT_AUD_CNTL_ST_A		0xE50B4
++#define _CPT_AUD_CNTL_ST_B		0xE51B4
+ #define CPT_AUD_CNTL_ST(pipe) _PIPE(pipe, \
+-					CPT_AUD_CNTL_ST_A, \
+-					CPT_AUD_CNTL_ST_B)
++					_CPT_AUD_CNTL_ST_A, \
++					_CPT_AUD_CNTL_ST_B)
+ #define CPT_AUD_CNTRL_ST2		0xE50C0
+ 
+-#define VLV_HDMIW_HDMIEDID_A		(VLV_DISPLAY_BASE + 0x62050)
+-#define VLV_HDMIW_HDMIEDID_B		(VLV_DISPLAY_BASE + 0x62150)
++#define _VLV_HDMIW_HDMIEDID_A		(VLV_DISPLAY_BASE + 0x62050)
++#define _VLV_HDMIW_HDMIEDID_B		(VLV_DISPLAY_BASE + 0x62150)
+ #define VLV_HDMIW_HDMIEDID(pipe) _PIPE(pipe, \
+-					VLV_HDMIW_HDMIEDID_A, \
+-					VLV_HDMIW_HDMIEDID_B)
+-#define VLV_AUD_CNTL_ST_A		(VLV_DISPLAY_BASE + 0x620B4)
+-#define VLV_AUD_CNTL_ST_B		(VLV_DISPLAY_BASE + 0x621B4)
++					_VLV_HDMIW_HDMIEDID_A, \
++					_VLV_HDMIW_HDMIEDID_B)
++#define _VLV_AUD_CNTL_ST_A		(VLV_DISPLAY_BASE + 0x620B4)
++#define _VLV_AUD_CNTL_ST_B		(VLV_DISPLAY_BASE + 0x621B4)
+ #define VLV_AUD_CNTL_ST(pipe) _PIPE(pipe, \
+-					VLV_AUD_CNTL_ST_A, \
+-					VLV_AUD_CNTL_ST_B)
++					_VLV_AUD_CNTL_ST_A, \
++					_VLV_AUD_CNTL_ST_B)
+ #define VLV_AUD_CNTL_ST2		(VLV_DISPLAY_BASE + 0x620C0)
+ 
+ /* These are the 4 32-bit write offset registers for each stream
+@@ -5700,28 +6167,28 @@
+  */
+ #define GEN7_SO_WRITE_OFFSET(n)		(0x5280 + (n) * 4)
+ 
+-#define IBX_AUD_CONFIG_A			0xe2000
+-#define IBX_AUD_CONFIG_B			0xe2100
++#define _IBX_AUD_CONFIG_A		0xe2000
++#define _IBX_AUD_CONFIG_B		0xe2100
+ #define IBX_AUD_CFG(pipe) _PIPE(pipe, \
+-					IBX_AUD_CONFIG_A, \
+-					IBX_AUD_CONFIG_B)
+-#define CPT_AUD_CONFIG_A			0xe5000
+-#define CPT_AUD_CONFIG_B			0xe5100
++					_IBX_AUD_CONFIG_A, \
++					_IBX_AUD_CONFIG_B)
++#define _CPT_AUD_CONFIG_A		0xe5000
++#define _CPT_AUD_CONFIG_B		0xe5100
+ #define CPT_AUD_CFG(pipe) _PIPE(pipe, \
+-					CPT_AUD_CONFIG_A, \
+-					CPT_AUD_CONFIG_B)
+-#define VLV_AUD_CONFIG_A		(VLV_DISPLAY_BASE + 0x62000)
+-#define VLV_AUD_CONFIG_B		(VLV_DISPLAY_BASE + 0x62100)
++					_CPT_AUD_CONFIG_A, \
++					_CPT_AUD_CONFIG_B)
++#define _VLV_AUD_CONFIG_A		(VLV_DISPLAY_BASE + 0x62000)
++#define _VLV_AUD_CONFIG_B		(VLV_DISPLAY_BASE + 0x62100)
+ #define VLV_AUD_CFG(pipe) _PIPE(pipe, \
+-					VLV_AUD_CONFIG_A, \
+-					VLV_AUD_CONFIG_B)
++					_VLV_AUD_CONFIG_A, \
++					_VLV_AUD_CONFIG_B)
+ 
+ #define   AUD_CONFIG_N_VALUE_INDEX		(1 << 29)
+ #define   AUD_CONFIG_N_PROG_ENABLE		(1 << 28)
+ #define   AUD_CONFIG_UPPER_N_SHIFT		20
+-#define   AUD_CONFIG_UPPER_N_VALUE		(0xff << 20)
++#define   AUD_CONFIG_UPPER_N_MASK		(0xff << 20)
+ #define   AUD_CONFIG_LOWER_N_SHIFT		4
+-#define   AUD_CONFIG_LOWER_N_VALUE		(0xfff << 4)
++#define   AUD_CONFIG_LOWER_N_MASK		(0xfff << 4)
+ #define   AUD_CONFIG_PIXEL_CLOCK_HDMI_SHIFT	16
+ #define   AUD_CONFIG_PIXEL_CLOCK_HDMI_MASK	(0xf << 16)
+ #define   AUD_CONFIG_PIXEL_CLOCK_HDMI_25175	(0 << 16)
+@@ -5737,52 +6204,44 @@
+ #define   AUD_CONFIG_DISABLE_NCTS		(1 << 3)
+ 
+ /* HSW Audio */
+-#define   HSW_AUD_CONFIG_A		0x65000 /* Audio Configuration Transcoder A */
+-#define   HSW_AUD_CONFIG_B		0x65100 /* Audio Configuration Transcoder B */
+-#define   HSW_AUD_CFG(pipe) _PIPE(pipe, \
+-					HSW_AUD_CONFIG_A, \
+-					HSW_AUD_CONFIG_B)
+-
+-#define   HSW_AUD_MISC_CTRL_A		0x65010 /* Audio Misc Control Convert 1 */
+-#define   HSW_AUD_MISC_CTRL_B		0x65110 /* Audio Misc Control Convert 2 */
+-#define   HSW_AUD_MISC_CTRL(pipe) _PIPE(pipe, \
+-					HSW_AUD_MISC_CTRL_A, \
+-					HSW_AUD_MISC_CTRL_B)
+-
+-#define   HSW_AUD_DIP_ELD_CTRL_ST_A	0x650b4 /* Audio DIP and ELD Control State Transcoder A */
+-#define   HSW_AUD_DIP_ELD_CTRL_ST_B	0x651b4 /* Audio DIP and ELD Control State Transcoder B */
+-#define   HSW_AUD_DIP_ELD_CTRL(pipe) _PIPE(pipe, \
+-					HSW_AUD_DIP_ELD_CTRL_ST_A, \
+-					HSW_AUD_DIP_ELD_CTRL_ST_B)
++#define _HSW_AUD_CONFIG_A		0x65000
++#define _HSW_AUD_CONFIG_B		0x65100
++#define HSW_AUD_CFG(pipe) _PIPE(pipe, \
++					_HSW_AUD_CONFIG_A, \
++					_HSW_AUD_CONFIG_B)
++
++#define _HSW_AUD_MISC_CTRL_A		0x65010
++#define _HSW_AUD_MISC_CTRL_B		0x65110
++#define HSW_AUD_MISC_CTRL(pipe) _PIPE(pipe, \
++					_HSW_AUD_MISC_CTRL_A, \
++					_HSW_AUD_MISC_CTRL_B)
++
++#define _HSW_AUD_DIP_ELD_CTRL_ST_A	0x650b4
++#define _HSW_AUD_DIP_ELD_CTRL_ST_B	0x651b4
++#define HSW_AUD_DIP_ELD_CTRL(pipe) _PIPE(pipe, \
++					_HSW_AUD_DIP_ELD_CTRL_ST_A, \
++					_HSW_AUD_DIP_ELD_CTRL_ST_B)
+ 
+ /* Audio Digital Converter */
+-#define   HSW_AUD_DIG_CNVT_1		0x65080 /* Audio Converter 1 */
+-#define   HSW_AUD_DIG_CNVT_2		0x65180 /* Audio Converter 1 */
+-#define   AUD_DIG_CNVT(pipe) _PIPE(pipe, \
+-					HSW_AUD_DIG_CNVT_1, \
+-					HSW_AUD_DIG_CNVT_2)
+-#define   DIP_PORT_SEL_MASK		0x3
+-
+-#define   HSW_AUD_EDID_DATA_A		0x65050
+-#define   HSW_AUD_EDID_DATA_B		0x65150
+-#define   HSW_AUD_EDID_DATA(pipe) _PIPE(pipe, \
+-					HSW_AUD_EDID_DATA_A, \
+-					HSW_AUD_EDID_DATA_B)
+-
+-#define   HSW_AUD_PIPE_CONV_CFG		0x6507c /* Audio pipe and converter configs */
+-#define   HSW_AUD_PIN_ELD_CP_VLD	0x650c0 /* Audio ELD and CP Ready Status */
+-#define   AUDIO_INACTIVE_C		(1<<11)
+-#define   AUDIO_INACTIVE_B		(1<<7)
+-#define   AUDIO_INACTIVE_A		(1<<3)
+-#define   AUDIO_OUTPUT_ENABLE_A		(1<<2)
+-#define   AUDIO_OUTPUT_ENABLE_B		(1<<6)
+-#define   AUDIO_OUTPUT_ENABLE_C		(1<<10)
+-#define   AUDIO_ELD_VALID_A		(1<<0)
+-#define   AUDIO_ELD_VALID_B		(1<<4)
+-#define   AUDIO_ELD_VALID_C		(1<<8)
+-#define   AUDIO_CP_READY_A		(1<<1)
+-#define   AUDIO_CP_READY_B		(1<<5)
+-#define   AUDIO_CP_READY_C		(1<<9)
++#define _HSW_AUD_DIG_CNVT_1		0x65080
++#define _HSW_AUD_DIG_CNVT_2		0x65180
++#define AUD_DIG_CNVT(pipe) _PIPE(pipe, \
++					_HSW_AUD_DIG_CNVT_1, \
++					_HSW_AUD_DIG_CNVT_2)
++#define DIP_PORT_SEL_MASK		0x3
++
++#define _HSW_AUD_EDID_DATA_A		0x65050
++#define _HSW_AUD_EDID_DATA_B		0x65150
++#define HSW_AUD_EDID_DATA(pipe) _PIPE(pipe, \
++					_HSW_AUD_EDID_DATA_A, \
++					_HSW_AUD_EDID_DATA_B)
++
++#define HSW_AUD_PIPE_CONV_CFG		0x6507c
++#define HSW_AUD_PIN_ELD_CP_VLD		0x650c0
++#define   AUDIO_INACTIVE(trans)		((1 << 3) << ((trans) * 4))
++#define   AUDIO_OUTPUT_ENABLE(trans)	((1 << 2) << ((trans) * 4))
++#define   AUDIO_CP_READY(trans)		((1 << 1) << ((trans) * 4))
++#define   AUDIO_ELD_VALID(trans)	((1 << 0) << ((trans) * 4))
+ 
+ /* HSW Power Wells */
+ #define HSW_PWR_WELL_BIOS			0x45400 /* CTL1 */
+@@ -5866,15 +6325,7 @@
+ #define DDI_BUF_CTL_B				0x64100
+ #define DDI_BUF_CTL(port) _PORT(port, DDI_BUF_CTL_A, DDI_BUF_CTL_B)
+ #define  DDI_BUF_CTL_ENABLE			(1<<31)
+-#define  DDI_BUF_EMP_400MV_0DB_HSW		(0<<24)   /* Sel0 */
+-#define  DDI_BUF_EMP_400MV_3_5DB_HSW		(1<<24)   /* Sel1 */
+-#define  DDI_BUF_EMP_400MV_6DB_HSW		(2<<24)   /* Sel2 */
+-#define  DDI_BUF_EMP_400MV_9_5DB_HSW		(3<<24)   /* Sel3 */
+-#define  DDI_BUF_EMP_600MV_0DB_HSW		(4<<24)   /* Sel4 */
+-#define  DDI_BUF_EMP_600MV_3_5DB_HSW		(5<<24)   /* Sel5 */
+-#define  DDI_BUF_EMP_600MV_6DB_HSW		(6<<24)   /* Sel6 */
+-#define  DDI_BUF_EMP_800MV_0DB_HSW		(7<<24)   /* Sel7 */
+-#define  DDI_BUF_EMP_800MV_3_5DB_HSW		(8<<24)   /* Sel8 */
++#define  DDI_BUF_TRANS_SELECT(n)	((n) << 24)
+ #define  DDI_BUF_EMP_MASK			(0xf<<24)
+ #define  DDI_BUF_PORT_REVERSAL			(1<<16)
+ #define  DDI_BUF_IS_IDLE			(1<<7)
+@@ -6008,6 +6459,83 @@
+ #define  LCPLL_CD_SOURCE_FCLK		(1<<21)
+ #define  LCPLL_CD_SOURCE_FCLK_DONE	(1<<19)
+ 
++/*
++ * SKL Clocks
++ */
++
++/* CDCLK_CTL */
++#define CDCLK_CTL			0x46000
++#define  CDCLK_FREQ_SEL_MASK		(3<<26)
++#define  CDCLK_FREQ_450_432		(0<<26)
++#define  CDCLK_FREQ_540			(1<<26)
++#define  CDCLK_FREQ_337_308		(2<<26)
++#define  CDCLK_FREQ_675_617		(3<<26)
++#define  CDCLK_FREQ_DECIMAL_MASK	(0x7ff)
++
++/* LCPLL_CTL */
++#define LCPLL1_CTL		0x46010
++#define LCPLL2_CTL		0x46014
++#define  LCPLL_PLL_ENABLE	(1<<31)
++
++/* DPLL control1 */
++#define DPLL_CTRL1		0x6C058
++#define  DPLL_CTRL1_HDMI_MODE(id)		(1<<((id)*6+5))
++#define  DPLL_CTRL1_SSC(id)			(1<<((id)*6+4))
++#define  DPLL_CRTL1_LINK_RATE_MASK(id)		(7<<((id)*6+1))
++#define  DPLL_CRTL1_LINK_RATE_SHIFT(id)		((id)*6+1)
++#define  DPLL_CRTL1_LINK_RATE(linkrate, id)	((linkrate)<<((id)*6+1))
++#define  DPLL_CTRL1_OVERRIDE(id)		(1<<((id)*6))
++#define  DPLL_CRTL1_LINK_RATE_2700		0
++#define  DPLL_CRTL1_LINK_RATE_1350		1
++#define  DPLL_CRTL1_LINK_RATE_810		2
++#define  DPLL_CRTL1_LINK_RATE_1620		3
++#define  DPLL_CRTL1_LINK_RATE_1080		4
++#define  DPLL_CRTL1_LINK_RATE_2160		5
++
++/* DPLL control2 */
++#define DPLL_CTRL2				0x6C05C
++#define  DPLL_CTRL2_DDI_CLK_OFF(port)		(1<<(port+15))
++#define  DPLL_CTRL2_DDI_CLK_SEL_MASK(port)	(3<<((port)*3+1))
++#define  DPLL_CTRL2_DDI_CLK_SEL_SHIFT(port)    ((port)*3+1)
++#define  DPLL_CTRL2_DDI_CLK_SEL(clk, port)	(clk<<((port)*3+1))
++#define  DPLL_CTRL2_DDI_SEL_OVERRIDE(port)     (1<<((port)*3))
++
++/* DPLL Status */
++#define DPLL_STATUS	0x6C060
++#define  DPLL_LOCK(id) (1<<((id)*8))
++
++/* DPLL cfg */
++#define DPLL1_CFGCR1	0x6C040
++#define DPLL2_CFGCR1	0x6C048
++#define DPLL3_CFGCR1	0x6C050
++#define  DPLL_CFGCR1_FREQ_ENABLE	(1<<31)
++#define  DPLL_CFGCR1_DCO_FRACTION_MASK	(0x7fff<<9)
++#define  DPLL_CFGCR1_DCO_FRACTION(x)	(x<<9)
++#define  DPLL_CFGCR1_DCO_INTEGER_MASK	(0x1ff)
++
++#define DPLL1_CFGCR2	0x6C044
++#define DPLL2_CFGCR2	0x6C04C
++#define DPLL3_CFGCR2	0x6C054
++#define  DPLL_CFGCR2_QDIV_RATIO_MASK	(0xff<<8)
++#define  DPLL_CFGCR2_QDIV_RATIO(x)	(x<<8)
++#define  DPLL_CFGCR2_QDIV_MODE(x)	(x<<7)
++#define  DPLL_CFGCR2_KDIV_MASK		(3<<5)
++#define  DPLL_CFGCR2_KDIV(x)		(x<<5)
++#define  DPLL_CFGCR2_KDIV_5 (0<<5)
++#define  DPLL_CFGCR2_KDIV_2 (1<<5)
++#define  DPLL_CFGCR2_KDIV_3 (2<<5)
++#define  DPLL_CFGCR2_KDIV_1 (3<<5)
++#define  DPLL_CFGCR2_PDIV_MASK		(7<<2)
++#define  DPLL_CFGCR2_PDIV(x)		(x<<2)
++#define  DPLL_CFGCR2_PDIV_1 (0<<2)
++#define  DPLL_CFGCR2_PDIV_2 (1<<2)
++#define  DPLL_CFGCR2_PDIV_3 (2<<2)
++#define  DPLL_CFGCR2_PDIV_7 (4<<2)
++#define  DPLL_CFGCR2_CENTRAL_FREQ_MASK	(3)
++
++#define GET_CFG_CR1_REG(id) (DPLL1_CFGCR1 + (id - SKL_DPLL1) * 8)
++#define GET_CFG_CR2_REG(id) (DPLL1_CFGCR2 + (id - SKL_DPLL1) * 8)
++
+ /* Please see hsw_read_dcomp() and hsw_write_dcomp() before using this register,
+  * since on HSW we can't write to it using I915_WRITE. */
+ #define D_COMP_HSW			(MCHBAR_MIRROR_BASE_SNB + 0x5F0C)
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_suspend.c b/drivers/gpu/drm/i915/i915_suspend.c
+--- a/drivers/gpu/drm/i915/i915_suspend.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/i915_suspend.c	2014-11-20 09:53:37.976762837 -0700
+@@ -203,34 +203,19 @@
+ 		i915_save_display_reg(dev);
+ 
+ 	/* LVDS state */
+-	if (HAS_PCH_SPLIT(dev)) {
+-		dev_priv->regfile.savePP_CONTROL = I915_READ(PCH_PP_CONTROL);
+-		if (HAS_PCH_IBX(dev) || HAS_PCH_CPT(dev))
+-			dev_priv->regfile.saveLVDS = I915_READ(PCH_LVDS);
+-	} else if (IS_VALLEYVIEW(dev)) {
+-		dev_priv->regfile.savePP_CONTROL = I915_READ(PP_CONTROL);
+-		dev_priv->regfile.savePFIT_PGM_RATIOS = I915_READ(PFIT_PGM_RATIOS);
+-
+-		dev_priv->regfile.saveBLC_HIST_CTL =
+-			I915_READ(VLV_BLC_HIST_CTL(PIPE_A));
+-		dev_priv->regfile.saveBLC_HIST_CTL_B =
+-			I915_READ(VLV_BLC_HIST_CTL(PIPE_B));
+-	} else {
+-		dev_priv->regfile.savePP_CONTROL = I915_READ(PP_CONTROL);
+-		dev_priv->regfile.savePFIT_PGM_RATIOS = I915_READ(PFIT_PGM_RATIOS);
+-		dev_priv->regfile.saveBLC_HIST_CTL = I915_READ(BLC_HIST_CTL);
+-		if (IS_MOBILE(dev) && !IS_I830(dev))
+-			dev_priv->regfile.saveLVDS = I915_READ(LVDS);
+-	}
+-
+-	if (!IS_I830(dev) && !IS_845G(dev) && !HAS_PCH_SPLIT(dev))
+-		dev_priv->regfile.savePFIT_CONTROL = I915_READ(PFIT_CONTROL);
++	if (HAS_PCH_IBX(dev) || HAS_PCH_CPT(dev))
++		dev_priv->regfile.saveLVDS = I915_READ(PCH_LVDS);
++	else if (INTEL_INFO(dev)->gen <= 4 && IS_MOBILE(dev) && !IS_I830(dev))
++		dev_priv->regfile.saveLVDS = I915_READ(LVDS);
+ 
++	/* Panel power sequencer */
+ 	if (HAS_PCH_SPLIT(dev)) {
++		dev_priv->regfile.savePP_CONTROL = I915_READ(PCH_PP_CONTROL);
+ 		dev_priv->regfile.savePP_ON_DELAYS = I915_READ(PCH_PP_ON_DELAYS);
+ 		dev_priv->regfile.savePP_OFF_DELAYS = I915_READ(PCH_PP_OFF_DELAYS);
+ 		dev_priv->regfile.savePP_DIVISOR = I915_READ(PCH_PP_DIVISOR);
+-	} else {
++	} else if (!IS_VALLEYVIEW(dev)) {
++		dev_priv->regfile.savePP_CONTROL = I915_READ(PP_CONTROL);
+ 		dev_priv->regfile.savePP_ON_DELAYS = I915_READ(PP_ON_DELAYS);
+ 		dev_priv->regfile.savePP_OFF_DELAYS = I915_READ(PP_OFF_DELAYS);
+ 		dev_priv->regfile.savePP_DIVISOR = I915_READ(PP_DIVISOR);
+@@ -259,29 +244,19 @@
+ 	if (drm_core_check_feature(dev, DRIVER_MODESET))
+ 		mask = ~LVDS_PORT_EN;
+ 
++	/* LVDS state */
+ 	if (HAS_PCH_IBX(dev) || HAS_PCH_CPT(dev))
+ 		I915_WRITE(PCH_LVDS, dev_priv->regfile.saveLVDS & mask);
+ 	else if (INTEL_INFO(dev)->gen <= 4 && IS_MOBILE(dev) && !IS_I830(dev))
+ 		I915_WRITE(LVDS, dev_priv->regfile.saveLVDS & mask);
+ 
+-	if (!IS_I830(dev) && !IS_845G(dev) && !HAS_PCH_SPLIT(dev))
+-		I915_WRITE(PFIT_CONTROL, dev_priv->regfile.savePFIT_CONTROL);
+-
++	/* Panel power sequencer */
+ 	if (HAS_PCH_SPLIT(dev)) {
+ 		I915_WRITE(PCH_PP_ON_DELAYS, dev_priv->regfile.savePP_ON_DELAYS);
+ 		I915_WRITE(PCH_PP_OFF_DELAYS, dev_priv->regfile.savePP_OFF_DELAYS);
+ 		I915_WRITE(PCH_PP_DIVISOR, dev_priv->regfile.savePP_DIVISOR);
+ 		I915_WRITE(PCH_PP_CONTROL, dev_priv->regfile.savePP_CONTROL);
+-		I915_WRITE(RSTDBYCTL,
+-			   dev_priv->regfile.saveMCHBAR_RENDER_STANDBY);
+-	} else if (IS_VALLEYVIEW(dev)) {
+-		I915_WRITE(VLV_BLC_HIST_CTL(PIPE_A),
+-			   dev_priv->regfile.saveBLC_HIST_CTL);
+-		I915_WRITE(VLV_BLC_HIST_CTL(PIPE_B),
+-			   dev_priv->regfile.saveBLC_HIST_CTL);
+-	} else {
+-		I915_WRITE(PFIT_PGM_RATIOS, dev_priv->regfile.savePFIT_PGM_RATIOS);
+-		I915_WRITE(BLC_HIST_CTL, dev_priv->regfile.saveBLC_HIST_CTL);
++	} else if (!IS_VALLEYVIEW(dev)) {
+ 		I915_WRITE(PP_ON_DELAYS, dev_priv->regfile.savePP_ON_DELAYS);
+ 		I915_WRITE(PP_OFF_DELAYS, dev_priv->regfile.savePP_OFF_DELAYS);
+ 		I915_WRITE(PP_DIVISOR, dev_priv->regfile.savePP_DIVISOR);
+@@ -368,6 +343,8 @@
+ 			I915_WRITE(_FDI_RXA_IMR, dev_priv->regfile.saveFDI_RXA_IMR);
+ 			I915_WRITE(_FDI_RXB_IMR, dev_priv->regfile.saveFDI_RXB_IMR);
+ 			I915_WRITE(PCH_PORT_HOTPLUG, dev_priv->regfile.savePCH_PORT_HOTPLUG);
++			I915_WRITE(RSTDBYCTL,
++				   dev_priv->regfile.saveMCHBAR_RENDER_STANDBY);
+ 		} else {
+ 			I915_WRITE(IER, dev_priv->regfile.saveIER);
+ 			I915_WRITE(IMR, dev_priv->regfile.saveIMR);
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_sysfs.c b/drivers/gpu/drm/i915/i915_sysfs.c
+--- a/drivers/gpu/drm/i915/i915_sysfs.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/i915_sysfs.c	2014-11-20 09:53:37.976762837 -0700
+@@ -139,8 +139,6 @@
+ static struct attribute *rc6_attrs[] = {
+ 	&dev_attr_rc6_enable.attr,
+ 	&dev_attr_rc6_residency_ms.attr,
+-	&dev_attr_rc6p_residency_ms.attr,
+-	&dev_attr_rc6pp_residency_ms.attr,
+ 	NULL
+ };
+ 
+@@ -148,6 +146,17 @@
+ 	.name = power_group_name,
+ 	.attrs =  rc6_attrs
+ };
++
++static struct attribute *rc6p_attrs[] = {
++	&dev_attr_rc6p_residency_ms.attr,
++	&dev_attr_rc6pp_residency_ms.attr,
++	NULL
++};
++
++static struct attribute_group rc6p_attr_group = {
++	.name = power_group_name,
++	.attrs =  rc6p_attrs
++};
+ #endif
+ 
+ static int l3_access_valid(struct drm_device *dev, loff_t offset)
+@@ -540,7 +549,7 @@
+ 
+ 	memset(&error_priv, 0, sizeof(error_priv));
+ 
+-	ret = i915_error_state_buf_init(&error_str, count, off);
++	ret = i915_error_state_buf_init(&error_str, to_i915(dev), count, off);
+ 	if (ret)
+ 		return ret;
+ 
+@@ -595,12 +604,18 @@
+ 	int ret;
+ 
+ #ifdef CONFIG_PM
+-	if (INTEL_INFO(dev)->gen >= 6) {
++	if (HAS_RC6(dev)) {
+ 		ret = sysfs_merge_group(&dev->primary->kdev->kobj,
+ 					&rc6_attr_group);
+ 		if (ret)
+ 			DRM_ERROR("RC6 residency sysfs setup failed\n");
+ 	}
++	if (HAS_RC6p(dev)) {
++		ret = sysfs_merge_group(&dev->primary->kdev->kobj,
++					&rc6p_attr_group);
++		if (ret)
++			DRM_ERROR("RC6p residency sysfs setup failed\n");
++	}
+ #endif
+ 	if (HAS_L3_DPF(dev)) {
+ 		ret = device_create_bin_file(dev->primary->kdev, &dpf_attrs);
+@@ -640,5 +655,6 @@
+ 	device_remove_bin_file(dev->primary->kdev,  &dpf_attrs);
+ #ifdef CONFIG_PM
+ 	sysfs_unmerge_group(&dev->primary->kdev->kobj, &rc6_attr_group);
++	sysfs_unmerge_group(&dev->primary->kdev->kobj, &rc6p_attr_group);
+ #endif
+ }
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_trace.h b/drivers/gpu/drm/i915/i915_trace.h
+--- a/drivers/gpu/drm/i915/i915_trace.h	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/i915_trace.h	2014-11-20 09:53:37.976762837 -0700
+@@ -156,10 +156,12 @@
+ 		      __entry->obj, __entry->offset, __entry->size, __entry->vm)
+ );
+ 
+-TRACE_EVENT(i915_gem_object_change_domain,
++TRACE_EVENT_CONDITION(i915_gem_object_change_domain,
+ 	    TP_PROTO(struct drm_i915_gem_object *obj, u32 old_read, u32 old_write),
+ 	    TP_ARGS(obj, old_read, old_write),
+ 
++	    TP_CONDITION((old_read ^ obj->base.read_domains) | (old_write ^ obj->base.write_domain)),
++
+ 	    TP_STRUCT__entry(
+ 			     __field(struct drm_i915_gem_object *, obj)
+ 			     __field(u32, read_domains)
+@@ -325,11 +327,10 @@
+ 	    TP_printk("dev=%d, vm=%p", __entry->dev, __entry->vm)
+ );
+ 
+-TRACE_EVENT(i915_gem_ring_sync_to,
+-	    TP_PROTO(struct intel_engine_cs *from,
+-		     struct intel_engine_cs *to,
+-		     u32 seqno),
+-	    TP_ARGS(from, to, seqno),
++TRACE_EVENT(i915_gem_ring_wait,
++	    TP_PROTO(struct i915_gem_request *waiter,
++		     struct i915_gem_request *signaller),
++	    TP_ARGS(waiter, signaller),
+ 
+ 	    TP_STRUCT__entry(
+ 			     __field(u32, dev)
+@@ -339,18 +340,40 @@
+ 			     ),
+ 
+ 	    TP_fast_assign(
+-			   __entry->dev = from->dev->primary->index;
+-			   __entry->sync_from = from->id;
+-			   __entry->sync_to = to->id;
+-			   __entry->seqno = seqno;
++			   __entry->dev = waiter->i915->dev->primary->index;
++			   __entry->sync_from = waiter->engine->id;
++			   __entry->sync_to = signaller->engine->id;
++			   __entry->seqno = signaller->breadcrumb[waiter->engine->id];
+ 			   ),
+ 
+-	    TP_printk("dev=%u, sync-from=%u, sync-to=%u, seqno=%u",
++	    TP_printk("dev=%u, sync-from=%u, sync-to=%u, seqno=%x",
+ 		      __entry->dev,
+ 		      __entry->sync_from, __entry->sync_to,
+ 		      __entry->seqno)
+ );
+ 
++TRACE_EVENT(i915_gem_ring_switch_context,
++	    TP_PROTO(struct intel_engine_cs *engine, struct intel_context *ctx, u32 flags),
++	    TP_ARGS(engine, ctx, flags),
++
++	    TP_STRUCT__entry(
++			     __field(u32, dev)
++			     __field(u32, ring)
++			     __field(u32, ctx)
++			     __field(u32, flags)
++			     ),
++
++	    TP_fast_assign(
++			   __entry->dev = engine->i915->dev->primary->index;
++			   __entry->ring = engine->id;
++			   __entry->ctx = ctx->file_priv ? ctx->user_handle : -1;
++			   __entry->flags = flags;
++			   ),
++
++	    TP_printk("dev=%u, ring=%u, ctx=%d, flags=0x%08x",
++		      __entry->dev, __entry->ring, __entry->ctx, __entry->flags)
++);
++
+ TRACE_EVENT(i915_gem_ring_dispatch,
+ 	    TP_PROTO(struct intel_engine_cs *ring, u32 seqno, u32 flags),
+ 	    TP_ARGS(ring, seqno, flags),
+@@ -363,66 +386,84 @@
+ 			     ),
+ 
+ 	    TP_fast_assign(
+-			   __entry->dev = ring->dev->primary->index;
++			   __entry->dev = ring->i915->dev->primary->index;
+ 			   __entry->ring = ring->id;
+ 			   __entry->seqno = seqno;
+ 			   __entry->flags = flags;
+ 			   i915_trace_irq_get(ring, seqno);
+ 			   ),
+ 
+-	    TP_printk("dev=%u, ring=%u, seqno=%u, flags=%x",
++	    TP_printk("dev=%u, ring=%u, seqno=%x, flags=%x",
+ 		      __entry->dev, __entry->ring, __entry->seqno, __entry->flags)
+ );
+ 
+-TRACE_EVENT(i915_gem_ring_flush,
+-	    TP_PROTO(struct intel_engine_cs *ring, u32 invalidate, u32 flush),
+-	    TP_ARGS(ring, invalidate, flush),
++TRACE_EVENT(intel_ringbuffer_begin,
++	    TP_PROTO(struct intel_ringbuffer *ring, int need),
++	    TP_ARGS(ring, need),
+ 
+ 	    TP_STRUCT__entry(
+ 			     __field(u32, dev)
+ 			     __field(u32, ring)
+-			     __field(u32, invalidate)
+-			     __field(u32, flush)
++			     __field(u32, need)
++			     __field(u32, space)
+ 			     ),
+ 
+ 	    TP_fast_assign(
+-			   __entry->dev = ring->dev->primary->index;
+-			   __entry->ring = ring->id;
+-			   __entry->invalidate = invalidate;
+-			   __entry->flush = flush;
++			   __entry->dev = ring->engine->i915->dev->primary->index;
++			   __entry->ring = ring->engine->id;
++			   __entry->need = need;
++			   __entry->space = intel_ring_space(ring);
+ 			   ),
+ 
+-	    TP_printk("dev=%u, ring=%x, invalidate=%04x, flush=%04x",
+-		      __entry->dev, __entry->ring,
+-		      __entry->invalidate, __entry->flush)
++	    TP_printk("dev=%u, ring=%u, need=%u, space=%u",
++		      __entry->dev, __entry->ring, __entry->need, __entry->space)
+ );
+ 
+-DECLARE_EVENT_CLASS(i915_gem_request,
+-	    TP_PROTO(struct intel_engine_cs *ring, u32 seqno),
+-	    TP_ARGS(ring, seqno),
++TRACE_EVENT(intel_ringbuffer_wait,
++	    TP_PROTO(struct intel_ringbuffer *ring, int need),
++	    TP_ARGS(ring, need),
+ 
+ 	    TP_STRUCT__entry(
+ 			     __field(u32, dev)
+ 			     __field(u32, ring)
+-			     __field(u32, seqno)
++			     __field(u32, need)
++			     __field(u32, space)
+ 			     ),
+ 
+ 	    TP_fast_assign(
+-			   __entry->dev = ring->dev->primary->index;
+-			   __entry->ring = ring->id;
+-			   __entry->seqno = seqno;
++			   __entry->dev = ring->engine->i915->dev->primary->index;
++			   __entry->ring = ring->engine->id;
++			   __entry->need = need;
++			   __entry->space = intel_ring_space(ring);
+ 			   ),
+ 
+-	    TP_printk("dev=%u, ring=%u, seqno=%u",
+-		      __entry->dev, __entry->ring, __entry->seqno)
++	    TP_printk("dev=%u, ring=%u, need=%u, space=%u",
++		      __entry->dev, __entry->ring, __entry->need, __entry->space)
+ );
+ 
+-DEFINE_EVENT(i915_gem_request, i915_gem_request_add,
+-	    TP_PROTO(struct intel_engine_cs *ring, u32 seqno),
+-	    TP_ARGS(ring, seqno)
++TRACE_EVENT(intel_ringbuffer_wrap,
++	    TP_PROTO(struct intel_ringbuffer *ring, int rem),
++	    TP_ARGS(ring, rem),
++
++	    TP_STRUCT__entry(
++			     __field(u32, dev)
++			     __field(u32, ring)
++			     __field(u32, rem)
++			     __field(u32, size)
++			     ),
++
++	    TP_fast_assign(
++			   __entry->dev = ring->engine->i915->dev->primary->index;
++			   __entry->ring = ring->engine->id;
++			   __entry->rem = rem;
++			   __entry->size = ring->effective_size;
++			   ),
++
++	    TP_printk("dev=%u, ring=%u, rem=%u, size=%u",
++		      __entry->dev, __entry->ring, __entry->rem, __entry->size)
+ );
+ 
+-TRACE_EVENT(i915_gem_request_complete,
++TRACE_EVENT(i915_gem_ring_complete,
+ 	    TP_PROTO(struct intel_engine_cs *ring),
+ 	    TP_ARGS(ring),
+ 
+@@ -433,23 +474,80 @@
+ 			     ),
+ 
+ 	    TP_fast_assign(
+-			   __entry->dev = ring->dev->primary->index;
++			   __entry->dev = ring->i915->dev->primary->index;
+ 			   __entry->ring = ring->id;
+-			   __entry->seqno = ring->get_seqno(ring, false);
++			   __entry->seqno = intel_engine_get_seqno(ring);
++			   ),
++
++	    TP_printk("dev=%u, ring=%u, seqno=%x",
++		      __entry->dev, __entry->ring, __entry->seqno)
++);
++
++DECLARE_EVENT_CLASS(i915_gem_request,
++	    TP_PROTO(struct i915_gem_request *rq),
++	    TP_ARGS(rq),
++
++	    TP_STRUCT__entry(
++			     __field(u32, dev)
++			     __field(u32, ring)
++			     __field(u32, seqno)
++			     ),
++
++	    TP_fast_assign(
++			   __entry->dev = rq->i915->dev->primary->index;
++			   __entry->ring = rq->engine->id;
++			   __entry->seqno = rq->seqno;
+ 			   ),
+ 
+-	    TP_printk("dev=%u, ring=%u, seqno=%u",
++	    TP_printk("dev=%u, ring=%u, seqno=%x",
+ 		      __entry->dev, __entry->ring, __entry->seqno)
+ );
+ 
++/**
++ * DOC: switch_mm tracepoint
++ *
++ * This tracepoint allows tracking of the mm switch, which is an important point
++ * in the lifetime of the vm in the legacy submission path. This tracepoint is
++ * called only if full ppgtt is enabled.
++ */
++DEFINE_EVENT(i915_gem_request, i915_gem_request_switch_mm,
++	    TP_PROTO(struct i915_gem_request *rq),
++	    TP_ARGS(rq)
++);
++
++DEFINE_EVENT(i915_gem_request, i915_gem_request_emit_flush,
++	    TP_PROTO(struct i915_gem_request *rq),
++	    TP_ARGS(rq)
++);
++
++DEFINE_EVENT(i915_gem_request, i915_gem_request_emit_batch,
++	    TP_PROTO(struct i915_gem_request *rq),
++	    TP_ARGS(rq)
++);
++
++DEFINE_EVENT(i915_gem_request, i915_gem_request_emit_breadcrumb,
++	    TP_PROTO(struct i915_gem_request *rq),
++	    TP_ARGS(rq)
++);
++
++DEFINE_EVENT(i915_gem_request, i915_gem_request_commit,
++	    TP_PROTO(struct i915_gem_request *rq),
++	    TP_ARGS(rq)
++);
++
++DEFINE_EVENT(i915_gem_request, i915_gem_request_complete,
++	    TP_PROTO(struct i915_gem_request *rq),
++	    TP_ARGS(rq)
++);
++
+ DEFINE_EVENT(i915_gem_request, i915_gem_request_retire,
+-	    TP_PROTO(struct intel_engine_cs *ring, u32 seqno),
+-	    TP_ARGS(ring, seqno)
++	    TP_PROTO(struct i915_gem_request *rq),
++	    TP_ARGS(rq)
+ );
+ 
+ TRACE_EVENT(i915_gem_request_wait_begin,
+-	    TP_PROTO(struct intel_engine_cs *ring, u32 seqno),
+-	    TP_ARGS(ring, seqno),
++	    TP_PROTO(struct i915_gem_request *rq),
++	    TP_ARGS(rq),
+ 
+ 	    TP_STRUCT__entry(
+ 			     __field(u32, dev)
+@@ -465,47 +563,38 @@
+ 	     * less desirable.
+ 	     */
+ 	    TP_fast_assign(
+-			   __entry->dev = ring->dev->primary->index;
+-			   __entry->ring = ring->id;
+-			   __entry->seqno = seqno;
+-			   __entry->blocking = mutex_is_locked(&ring->dev->struct_mutex);
++			   __entry->dev = rq->i915->dev->primary->index;
++			   __entry->ring = rq->engine->id;
++			   __entry->seqno = rq->seqno;
++			   __entry->blocking = mutex_is_locked(&rq->i915->dev->struct_mutex);
+ 			   ),
+ 
+-	    TP_printk("dev=%u, ring=%u, seqno=%u, blocking=%s",
++	    TP_printk("dev=%u, ring=%u, seqno=%x, blocking?=%s",
+ 		      __entry->dev, __entry->ring, __entry->seqno,
+ 		      __entry->blocking ?  "yes (NB)" : "no")
+ );
+ 
+-DEFINE_EVENT(i915_gem_request, i915_gem_request_wait_end,
+-	    TP_PROTO(struct intel_engine_cs *ring, u32 seqno),
+-	    TP_ARGS(ring, seqno)
+-);
+-
+-DECLARE_EVENT_CLASS(i915_ring,
+-	    TP_PROTO(struct intel_engine_cs *ring),
+-	    TP_ARGS(ring),
++TRACE_EVENT(i915_gem_request_wait_end,
++	    TP_PROTO(struct i915_gem_request *rq),
++	    TP_ARGS(rq),
+ 
+ 	    TP_STRUCT__entry(
+ 			     __field(u32, dev)
+ 			     __field(u32, ring)
++			     __field(u32, seqno)
++			     __field(bool, completed)
+ 			     ),
+ 
+ 	    TP_fast_assign(
+-			   __entry->dev = ring->dev->primary->index;
+-			   __entry->ring = ring->id;
++			   __entry->dev = rq->i915->dev->primary->index;
++			   __entry->ring = rq->engine->id;
++			   __entry->seqno = rq->seqno;
++			   __entry->completed = rq->completed;
+ 			   ),
+ 
+-	    TP_printk("dev=%u, ring=%u", __entry->dev, __entry->ring)
+-);
+-
+-DEFINE_EVENT(i915_ring, i915_ring_wait_begin,
+-	    TP_PROTO(struct intel_engine_cs *ring),
+-	    TP_ARGS(ring)
+-);
+-
+-DEFINE_EVENT(i915_ring, i915_ring_wait_end,
+-	    TP_PROTO(struct intel_engine_cs *ring),
+-	    TP_ARGS(ring)
++	    TP_printk("dev=%u, ring=%u, seqno=%x, completed=%s",
++		      __entry->dev, __entry->ring, __entry->seqno,
++		      __entry->completed ?  "yes" : "no")
+ );
+ 
+ TRACE_EVENT(i915_flip_request,
+@@ -587,6 +676,80 @@
+ 	    TP_printk("new_freq=%u", __entry->freq)
+ );
+ 
++/**
++ * DOC: i915_vm_create and i915_vm_free tracepoints
++ *
++ * With full ppgtt enabled each process using drm will allocate at least one
++ * translation table. With these traces it is possible to keep track of the
++ * allocation and of the lifetime of the tables; this can be used during
++ * testing/debug to verify that we are not leaking ppgtts.
++ * These traces identify the ppgtt through the vm pointer, which is also printed
++ * by the i915_vma_bind and i915_vma_unbind tracepoints.
++ */
++DECLARE_EVENT_CLASS(i915_vm,
++	TP_PROTO(struct i915_address_space *vm),
++	TP_ARGS(vm),
++
++	TP_STRUCT__entry(
++			__field(struct i915_address_space *, vm)
++			__field(u32, dev)
++	),
++
++	TP_fast_assign(
++			__entry->vm = vm;
++			__entry->dev = vm->dev->primary->index;
++	),
++
++	TP_printk("dev=%u, vm=%p", __entry->dev, __entry->vm)
++)
++
++DEFINE_EVENT(i915_vm, i915_vm_create,
++	TP_PROTO(struct i915_address_space *vm),
++	TP_ARGS(vm)
++);
++
++DEFINE_EVENT(i915_vm, i915_vm_free,
++	TP_PROTO(struct i915_address_space *vm),
++	TP_ARGS(vm)
++);
++
++/**
++ * DOC: i915_context_create and i915_context_free tracepoints
++ *
++ * These tracepoints are used to track creation and deletion of contexts.
++ * If full ppgtt is enabled, they also print the address of the vm assigned to
++ * the context.
++ */
++DECLARE_EVENT_CLASS(i915_context,
++	TP_PROTO(struct intel_context *ctx),
++	TP_ARGS(ctx),
++
++	TP_STRUCT__entry(
++			__field(u32, dev)
++			__field(struct intel_context *, ctx)
++			__field(struct i915_address_space *, vm)
++	),
++
++	TP_fast_assign(
++			__entry->ctx = ctx;
++			__entry->vm = ctx->ppgtt ? &ctx->ppgtt->base : NULL;
++			__entry->dev = ctx->file_priv->dev_priv->dev->primary->index;
++	),
++
++	TP_printk("dev=%u, ctx=%p, ctx_vm=%p",
++		  __entry->dev, __entry->ctx, __entry->vm)
++)
++
++DEFINE_EVENT(i915_context, i915_context_create,
++	TP_PROTO(struct intel_context *ctx),
++	TP_ARGS(ctx)
++);
++
++DEFINE_EVENT(i915_context, i915_context_free,
++	TP_PROTO(struct intel_context *ctx),
++	TP_ARGS(ctx)
++);
++
+ #endif /* _I915_TRACE_H_ */
+ 
+ /* This part must be outside protection */
+diff -urN -x arch a/drivers/gpu/drm/i915/i915_ums.c b/drivers/gpu/drm/i915/i915_ums.c
+--- a/drivers/gpu/drm/i915/i915_ums.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/i915_ums.c	2014-11-20 09:53:37.976762837 -0700
+@@ -270,6 +270,12 @@
+ 	}
+ 	/* FIXME: regfile.save TV & SDVO state */
+ 
++	/* Panel fitter */
++	if (!IS_I830(dev) && !IS_845G(dev) && !HAS_PCH_SPLIT(dev)) {
++		dev_priv->regfile.savePFIT_CONTROL = I915_READ(PFIT_CONTROL);
++		dev_priv->regfile.savePFIT_PGM_RATIOS = I915_READ(PFIT_PGM_RATIOS);
++	}
++
+ 	/* Backlight */
+ 	if (INTEL_INFO(dev)->gen <= 4)
+ 		pci_read_config_byte(dev->pdev, PCI_LBPC,
+@@ -284,6 +290,7 @@
+ 		dev_priv->regfile.saveBLC_PWM_CTL = I915_READ(BLC_PWM_CTL);
+ 		if (INTEL_INFO(dev)->gen >= 4)
+ 			dev_priv->regfile.saveBLC_PWM_CTL2 = I915_READ(BLC_PWM_CTL2);
++		dev_priv->regfile.saveBLC_HIST_CTL = I915_READ(BLC_HIST_CTL);
+ 	}
+ 
+ 	return;
+@@ -313,6 +320,13 @@
+ 		if (INTEL_INFO(dev)->gen >= 4)
+ 			I915_WRITE(BLC_PWM_CTL2, dev_priv->regfile.saveBLC_PWM_CTL2);
+ 		I915_WRITE(BLC_PWM_CTL, dev_priv->regfile.saveBLC_PWM_CTL);
++		I915_WRITE(BLC_HIST_CTL, dev_priv->regfile.saveBLC_HIST_CTL);
++	}
++
++	/* Panel fitter */
++	if (!IS_I830(dev) && !IS_845G(dev) && !HAS_PCH_SPLIT(dev)) {
++		I915_WRITE(PFIT_PGM_RATIOS, dev_priv->regfile.savePFIT_PGM_RATIOS);
++		I915_WRITE(PFIT_CONTROL, dev_priv->regfile.savePFIT_CONTROL);
+ 	}
+ 
+ 	/* Display port ratios (must be done before clock is set) */
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_audio.c b/drivers/gpu/drm/i915/intel_audio.c
+--- a/drivers/gpu/drm/i915/intel_audio.c	1969-12-31 17:00:00.000000000 -0700
++++ b/drivers/gpu/drm/i915/intel_audio.c	2014-11-20 09:53:37.976762837 -0700
+@@ -0,0 +1,462 @@
++/*
++ * Copyright  2014 Intel Corporation
++ *
++ * Permission is hereby granted, free of charge, to any person obtaining a
++ * copy of this software and associated documentation files (the "Software"),
++ * to deal in the Software without restriction, including without limitation
++ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
++ * and/or sell copies of the Software, and to permit persons to whom the
++ * Software is furnished to do so, subject to the following conditions:
++ *
++ * The above copyright notice and this permission notice (including the next
++ * paragraph) shall be included in all copies or substantial portions of the
++ * Software.
++ *
++ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
++ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
++ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
++ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
++ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
++ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
++ * DEALINGS IN THE SOFTWARE.
++ */
++
++#include <linux/kernel.h>
++
++#include <drm/drmP.h>
++#include <drm/drm_edid.h>
++#include "intel_drv.h"
++#include "i915_drv.h"
++
++/**
++ * DOC: High Definition Audio over HDMI and Display Port
++ *
++ * The graphics and audio drivers together support High Definition Audio over
++ * HDMI and Display Port. The audio programming sequences are divided into audio
++ * codec and controller enable and disable sequences. The graphics driver
++ * handles the audio codec sequences, while the audio driver handles the audio
++ * controller sequences.
++ *
++ * The disable sequences must be performed before disabling the transcoder or
++ * port. The enable sequences may only be performed after enabling the
++ * transcoder and port, and after completed link training.
++ *
++ * The codec and controller sequences could be done either parallel or serial,
++ * but generally the ELDV/PD change in the codec sequence indicates to the audio
++ * driver that the controller sequence should start. Indeed, most of the
++ * co-operation between the graphics and audio drivers is handled via audio
++ * related registers. (The notable exception is the power management, not
++ * covered here.)
++ */
++
++static const struct {
++	int clock;
++	u32 config;
++} hdmi_audio_clock[] = {
++	{ DIV_ROUND_UP(25200 * 1000, 1001), AUD_CONFIG_PIXEL_CLOCK_HDMI_25175 },
++	{ 25200, AUD_CONFIG_PIXEL_CLOCK_HDMI_25200 }, /* default per bspec */
++	{ 27000, AUD_CONFIG_PIXEL_CLOCK_HDMI_27000 },
++	{ 27000 * 1001 / 1000, AUD_CONFIG_PIXEL_CLOCK_HDMI_27027 },
++	{ 54000, AUD_CONFIG_PIXEL_CLOCK_HDMI_54000 },
++	{ 54000 * 1001 / 1000, AUD_CONFIG_PIXEL_CLOCK_HDMI_54054 },
++	{ DIV_ROUND_UP(74250 * 1000, 1001), AUD_CONFIG_PIXEL_CLOCK_HDMI_74176 },
++	{ 74250, AUD_CONFIG_PIXEL_CLOCK_HDMI_74250 },
++	{ DIV_ROUND_UP(148500 * 1000, 1001), AUD_CONFIG_PIXEL_CLOCK_HDMI_148352 },
++	{ 148500, AUD_CONFIG_PIXEL_CLOCK_HDMI_148500 },
++};
++
++/* get AUD_CONFIG_PIXEL_CLOCK_HDMI_* value for mode */
++static u32 audio_config_hdmi_pixel_clock(struct drm_display_mode *mode)
++{
++	int i;
++
++	for (i = 0; i < ARRAY_SIZE(hdmi_audio_clock); i++) {
++		if (mode->clock == hdmi_audio_clock[i].clock)
++			break;
++	}
++
++	if (i == ARRAY_SIZE(hdmi_audio_clock)) {
++		DRM_DEBUG_KMS("HDMI audio pixel clock setting for %d not found, falling back to defaults\n", mode->clock);
++		i = 1;
++	}
++
++	DRM_DEBUG_KMS("Configuring HDMI audio for pixel clock %d (0x%08x)\n",
++		      hdmi_audio_clock[i].clock,
++		      hdmi_audio_clock[i].config);
++
++	return hdmi_audio_clock[i].config;
++}
++
++static bool intel_eld_uptodate(struct drm_connector *connector,
++			       int reg_eldv, uint32_t bits_eldv,
++			       int reg_elda, uint32_t bits_elda,
++			       int reg_edid)
++{
++	struct drm_i915_private *dev_priv = connector->dev->dev_private;
++	uint8_t *eld = connector->eld;
++	uint32_t tmp;
++	int i;
++
++	tmp = I915_READ(reg_eldv);
++	tmp &= bits_eldv;
++
++	if (!tmp)
++		return false;
++
++	tmp = I915_READ(reg_elda);
++	tmp &= ~bits_elda;
++	I915_WRITE(reg_elda, tmp);
++
++	for (i = 0; i < drm_eld_size(eld) / 4; i++)
++		if (I915_READ(reg_edid) != *((uint32_t *)eld + i))
++			return false;
++
++	return true;
++}
++
++static void g4x_audio_codec_disable(struct intel_encoder *encoder)
++{
++	struct drm_i915_private *dev_priv = encoder->base.dev->dev_private;
++	uint32_t eldv, tmp;
++
++	DRM_DEBUG_KMS("Disable audio codec\n");
++
++	tmp = I915_READ(G4X_AUD_VID_DID);
++	if (tmp == INTEL_AUDIO_DEVBLC || tmp == INTEL_AUDIO_DEVCL)
++		eldv = G4X_ELDV_DEVCL_DEVBLC;
++	else
++		eldv = G4X_ELDV_DEVCTG;
++
++	/* Invalidate ELD */
++	tmp = I915_READ(G4X_AUD_CNTL_ST);
++	tmp &= ~eldv;
++	I915_WRITE(G4X_AUD_CNTL_ST, tmp);
++}
++
++static void g4x_audio_codec_enable(struct drm_connector *connector,
++				   struct intel_encoder *encoder,
++				   struct drm_display_mode *mode)
++{
++	struct drm_i915_private *dev_priv = connector->dev->dev_private;
++	uint8_t *eld = connector->eld;
++	uint32_t eldv;
++	uint32_t tmp;
++	int len, i;
++
++	DRM_DEBUG_KMS("Enable audio codec, %u bytes ELD\n", eld[2]);
++
++	tmp = I915_READ(G4X_AUD_VID_DID);
++	if (tmp == INTEL_AUDIO_DEVBLC || tmp == INTEL_AUDIO_DEVCL)
++		eldv = G4X_ELDV_DEVCL_DEVBLC;
++	else
++		eldv = G4X_ELDV_DEVCTG;
++
++	if (intel_eld_uptodate(connector,
++			       G4X_AUD_CNTL_ST, eldv,
++			       G4X_AUD_CNTL_ST, G4X_ELD_ADDR_MASK,
++			       G4X_HDMIW_HDMIEDID))
++		return;
++
++	tmp = I915_READ(G4X_AUD_CNTL_ST);
++	tmp &= ~(eldv | G4X_ELD_ADDR_MASK);
++	len = (tmp >> 9) & 0x1f;		/* ELD buffer size */
++	I915_WRITE(G4X_AUD_CNTL_ST, tmp);
++
++	len = min(drm_eld_size(eld) / 4, len);
++	DRM_DEBUG_DRIVER("ELD size %d\n", len);
++	for (i = 0; i < len; i++)
++		I915_WRITE(G4X_HDMIW_HDMIEDID, *((uint32_t *)eld + i));
++
++	tmp = I915_READ(G4X_AUD_CNTL_ST);
++	tmp |= eldv;
++	I915_WRITE(G4X_AUD_CNTL_ST, tmp);
++}
++
++static void hsw_audio_codec_disable(struct intel_encoder *encoder)
++{
++	struct drm_i915_private *dev_priv = encoder->base.dev->dev_private;
++	struct intel_crtc *intel_crtc = to_intel_crtc(encoder->base.crtc);
++	enum pipe pipe = intel_crtc->pipe;
++	uint32_t tmp;
++
++	DRM_DEBUG_KMS("Disable audio codec on pipe %c\n", pipe_name(pipe));
++
++	/* Disable timestamps */
++	tmp = I915_READ(HSW_AUD_CFG(pipe));
++	tmp &= ~AUD_CONFIG_N_VALUE_INDEX;
++	tmp |= AUD_CONFIG_N_PROG_ENABLE;
++	tmp &= ~AUD_CONFIG_UPPER_N_MASK;
++	tmp &= ~AUD_CONFIG_LOWER_N_MASK;
++	if (intel_pipe_has_type(intel_crtc, INTEL_OUTPUT_DISPLAYPORT))
++		tmp |= AUD_CONFIG_N_VALUE_INDEX;
++	I915_WRITE(HSW_AUD_CFG(pipe), tmp);
++
++	/* Invalidate ELD */
++	tmp = I915_READ(HSW_AUD_PIN_ELD_CP_VLD);
++	tmp &= ~AUDIO_ELD_VALID(pipe);
++	I915_WRITE(HSW_AUD_PIN_ELD_CP_VLD, tmp);
++}
++
++static void hsw_audio_codec_enable(struct drm_connector *connector,
++				   struct intel_encoder *encoder,
++				   struct drm_display_mode *mode)
++{
++	struct drm_i915_private *dev_priv = connector->dev->dev_private;
++	struct intel_crtc *intel_crtc = to_intel_crtc(encoder->base.crtc);
++	enum pipe pipe = intel_crtc->pipe;
++	const uint8_t *eld = connector->eld;
++	uint32_t tmp;
++	int len, i;
++
++	DRM_DEBUG_KMS("Enable audio codec on pipe %c, %u bytes ELD\n",
++		      pipe_name(pipe), drm_eld_size(eld));
++
++	/* Enable audio presence detect, invalidate ELD */
++	tmp = I915_READ(HSW_AUD_PIN_ELD_CP_VLD);
++	tmp |= AUDIO_OUTPUT_ENABLE(pipe);
++	tmp &= ~AUDIO_ELD_VALID(pipe);
++	I915_WRITE(HSW_AUD_PIN_ELD_CP_VLD, tmp);
++
++	/*
++	 * FIXME: We're supposed to wait for vblank here, but we have vblanks
++	 * disabled during the mode set. The proper fix would be to push the
++	 * rest of the setup into a vblank work item, queued here, but the
++	 * infrastructure is not there yet.
++	 */
++
++	/* Reset ELD write address */
++	tmp = I915_READ(HSW_AUD_DIP_ELD_CTRL(pipe));
++	tmp &= ~IBX_ELD_ADDRESS_MASK;
++	I915_WRITE(HSW_AUD_DIP_ELD_CTRL(pipe), tmp);
++
++	/* Up to 84 bytes of hw ELD buffer */
++	len = min(drm_eld_size(eld), 84);
++	for (i = 0; i < len / 4; i++)
++		I915_WRITE(HSW_AUD_EDID_DATA(pipe), *((uint32_t *)eld + i));
++
++	/* ELD valid */
++	tmp = I915_READ(HSW_AUD_PIN_ELD_CP_VLD);
++	tmp |= AUDIO_ELD_VALID(pipe);
++	I915_WRITE(HSW_AUD_PIN_ELD_CP_VLD, tmp);
++
++	/* Enable timestamps */
++	tmp = I915_READ(HSW_AUD_CFG(pipe));
++	tmp &= ~AUD_CONFIG_N_VALUE_INDEX;
++	tmp &= ~AUD_CONFIG_N_PROG_ENABLE;
++	tmp &= ~AUD_CONFIG_PIXEL_CLOCK_HDMI_MASK;
++	if (intel_pipe_has_type(intel_crtc, INTEL_OUTPUT_DISPLAYPORT))
++		tmp |= AUD_CONFIG_N_VALUE_INDEX;
++	else
++		tmp |= audio_config_hdmi_pixel_clock(mode);
++	I915_WRITE(HSW_AUD_CFG(pipe), tmp);
++}
++
++static void ilk_audio_codec_disable(struct intel_encoder *encoder)
++{
++	struct drm_i915_private *dev_priv = encoder->base.dev->dev_private;
++	struct intel_crtc *intel_crtc = to_intel_crtc(encoder->base.crtc);
++	struct intel_digital_port *intel_dig_port =
++		enc_to_dig_port(&encoder->base);
++	enum port port = intel_dig_port->port;
++	enum pipe pipe = intel_crtc->pipe;
++	uint32_t tmp, eldv;
++	int aud_config;
++	int aud_cntrl_st2;
++
++	DRM_DEBUG_KMS("Disable audio codec on port %c, pipe %c\n",
++		      port_name(port), pipe_name(pipe));
++
++	if (HAS_PCH_IBX(dev_priv->dev)) {
++		aud_config = IBX_AUD_CFG(pipe);
++		aud_cntrl_st2 = IBX_AUD_CNTL_ST2;
++	} else if (IS_VALLEYVIEW(dev_priv)) {
++		aud_config = VLV_AUD_CFG(pipe);
++		aud_cntrl_st2 = VLV_AUD_CNTL_ST2;
++	} else {
++		aud_config = CPT_AUD_CFG(pipe);
++		aud_cntrl_st2 = CPT_AUD_CNTRL_ST2;
++	}
++
++	/* Disable timestamps */
++	tmp = I915_READ(aud_config);
++	tmp &= ~AUD_CONFIG_N_VALUE_INDEX;
++	tmp |= AUD_CONFIG_N_PROG_ENABLE;
++	tmp &= ~AUD_CONFIG_UPPER_N_MASK;
++	tmp &= ~AUD_CONFIG_LOWER_N_MASK;
++	if (intel_pipe_has_type(intel_crtc, INTEL_OUTPUT_DISPLAYPORT))
++		tmp |= AUD_CONFIG_N_VALUE_INDEX;
++	I915_WRITE(aud_config, tmp);
++
++	if (WARN_ON(!port)) {
++		eldv = IBX_ELD_VALID(PORT_B) | IBX_ELD_VALID(PORT_C) |
++			IBX_ELD_VALID(PORT_D);
++	} else {
++		eldv = IBX_ELD_VALID(port);
++	}
++
++	/* Invalidate ELD */
++	tmp = I915_READ(aud_cntrl_st2);
++	tmp &= ~eldv;
++	I915_WRITE(aud_cntrl_st2, tmp);
++}
++
++static void ilk_audio_codec_enable(struct drm_connector *connector,
++				   struct intel_encoder *encoder,
++				   struct drm_display_mode *mode)
++{
++	struct drm_i915_private *dev_priv = connector->dev->dev_private;
++	struct intel_crtc *intel_crtc = to_intel_crtc(encoder->base.crtc);
++	struct intel_digital_port *intel_dig_port =
++		enc_to_dig_port(&encoder->base);
++	enum port port = intel_dig_port->port;
++	enum pipe pipe = intel_crtc->pipe;
++	uint8_t *eld = connector->eld;
++	uint32_t eldv;
++	uint32_t tmp;
++	int len, i;
++	int hdmiw_hdmiedid;
++	int aud_config;
++	int aud_cntl_st;
++	int aud_cntrl_st2;
++
++	DRM_DEBUG_KMS("Enable audio codec on port %c, pipe %c, %u bytes ELD\n",
++		      port_name(port), pipe_name(pipe), drm_eld_size(eld));
++
++	/*
++	 * FIXME: We're supposed to wait for vblank here, but we have vblanks
++	 * disabled during the mode set. The proper fix would be to push the
++	 * rest of the setup into a vblank work item, queued here, but the
++	 * infrastructure is not there yet.
++	 */
++
++	if (HAS_PCH_IBX(connector->dev)) {
++		hdmiw_hdmiedid = IBX_HDMIW_HDMIEDID(pipe);
++		aud_config = IBX_AUD_CFG(pipe);
++		aud_cntl_st = IBX_AUD_CNTL_ST(pipe);
++		aud_cntrl_st2 = IBX_AUD_CNTL_ST2;
++	} else if (IS_VALLEYVIEW(connector->dev)) {
++		hdmiw_hdmiedid = VLV_HDMIW_HDMIEDID(pipe);
++		aud_config = VLV_AUD_CFG(pipe);
++		aud_cntl_st = VLV_AUD_CNTL_ST(pipe);
++		aud_cntrl_st2 = VLV_AUD_CNTL_ST2;
++	} else {
++		hdmiw_hdmiedid = CPT_HDMIW_HDMIEDID(pipe);
++		aud_config = CPT_AUD_CFG(pipe);
++		aud_cntl_st = CPT_AUD_CNTL_ST(pipe);
++		aud_cntrl_st2 = CPT_AUD_CNTRL_ST2;
++	}
++
++	if (WARN_ON(!port)) {
++		eldv = IBX_ELD_VALID(PORT_B) | IBX_ELD_VALID(PORT_C) |
++			IBX_ELD_VALID(PORT_D);
++	} else {
++		eldv = IBX_ELD_VALID(port);
++	}
++
++	/* Invalidate ELD */
++	tmp = I915_READ(aud_cntrl_st2);
++	tmp &= ~eldv;
++	I915_WRITE(aud_cntrl_st2, tmp);
++
++	/* Reset ELD write address */
++	tmp = I915_READ(aud_cntl_st);
++	tmp &= ~IBX_ELD_ADDRESS_MASK;
++	I915_WRITE(aud_cntl_st, tmp);
++
++	/* Up to 84 bytes of hw ELD buffer */
++	len = min(drm_eld_size(eld), 84);
++	for (i = 0; i < len / 4; i++)
++		I915_WRITE(hdmiw_hdmiedid, *((uint32_t *)eld + i));
++
++	/* ELD valid */
++	tmp = I915_READ(aud_cntrl_st2);
++	tmp |= eldv;
++	I915_WRITE(aud_cntrl_st2, tmp);
++
++	/* Enable timestamps */
++	tmp = I915_READ(aud_config);
++	tmp &= ~AUD_CONFIG_N_VALUE_INDEX;
++	tmp &= ~AUD_CONFIG_N_PROG_ENABLE;
++	tmp &= ~AUD_CONFIG_PIXEL_CLOCK_HDMI_MASK;
++	if (intel_pipe_has_type(intel_crtc, INTEL_OUTPUT_DISPLAYPORT))
++		tmp |= AUD_CONFIG_N_VALUE_INDEX;
++	else
++		tmp |= audio_config_hdmi_pixel_clock(mode);
++	I915_WRITE(aud_config, tmp);
++}
++
++/**
++ * intel_audio_codec_enable - Enable the audio codec for HD audio
++ * @intel_encoder: encoder on which to enable audio
++ *
++ * The enable sequences may only be performed after enabling the transcoder and
++ * port, and after completed link training.
++ */
++void intel_audio_codec_enable(struct intel_encoder *intel_encoder)
++{
++	struct drm_encoder *encoder = &intel_encoder->base;
++	struct intel_crtc *crtc = to_intel_crtc(encoder->crtc);
++	struct drm_display_mode *mode = &crtc->config.adjusted_mode;
++	struct drm_connector *connector;
++	struct drm_device *dev = encoder->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++
++	connector = drm_select_eld(encoder, mode);
++	if (!connector)
++		return;
++
++	DRM_DEBUG_DRIVER("ELD on [CONNECTOR:%d:%s], [ENCODER:%d:%s]\n",
++			 connector->base.id,
++			 connector->name,
++			 connector->encoder->base.id,
++			 connector->encoder->name);
++
++	/* ELD Conn_Type */
++	connector->eld[5] &= ~(3 << 2);
++	if (intel_pipe_has_type(crtc, INTEL_OUTPUT_DISPLAYPORT))
++		connector->eld[5] |= (1 << 2);
++
++	connector->eld[6] = drm_av_sync_delay(connector, mode) / 2;
++
++	if (dev_priv->display.audio_codec_enable)
++		dev_priv->display.audio_codec_enable(connector, intel_encoder, mode);
++}
++
++/**
++ * intel_audio_codec_disable - Disable the audio codec for HD audio
++ * @encoder: encoder on which to disable audio
++ *
++ * The disable sequences must be performed before disabling the transcoder or
++ * port.
++ */
++void intel_audio_codec_disable(struct intel_encoder *encoder)
++{
++	struct drm_device *dev = encoder->base.dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++
++	if (dev_priv->display.audio_codec_disable)
++		dev_priv->display.audio_codec_disable(encoder);
++}
++
++/**
++ * intel_init_audio - Set up chip specific audio functions
++ * @dev: drm device
++ */
++void intel_init_audio(struct drm_device *dev)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++
++	if (IS_G4X(dev)) {
++		dev_priv->display.audio_codec_enable = g4x_audio_codec_enable;
++		dev_priv->display.audio_codec_disable = g4x_audio_codec_disable;
++	} else if (IS_VALLEYVIEW(dev)) {
++		dev_priv->display.audio_codec_enable = ilk_audio_codec_enable;
++		dev_priv->display.audio_codec_disable = ilk_audio_codec_disable;
++	} else if (IS_HASWELL(dev) || INTEL_INFO(dev)->gen >= 8) {
++		dev_priv->display.audio_codec_enable = hsw_audio_codec_enable;
++		dev_priv->display.audio_codec_disable = hsw_audio_codec_disable;
++	} else if (HAS_PCH_SPLIT(dev)) {
++		dev_priv->display.audio_codec_enable = ilk_audio_codec_enable;
++		dev_priv->display.audio_codec_disable = ilk_audio_codec_disable;
++	}
++}
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_bios.c b/drivers/gpu/drm/i915/intel_bios.c
+--- a/drivers/gpu/drm/i915/intel_bios.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_bios.c	2014-11-20 09:53:37.976762837 -0700
+@@ -287,7 +287,7 @@
+ 			downclock = dvo_timing->clock;
+ 	}
+ 
+-	if (downclock < panel_dvo_timing->clock && i915.lvds_downclock) {
++	if (downclock < panel_dvo_timing->clock && i915_module.lvds_downclock) {
+ 		dev_priv->lvds_downclock_avail = 1;
+ 		dev_priv->lvds_downclock = downclock * 10;
+ 		DRM_DEBUG_KMS("LVDS downclock is found in VBT. "
+@@ -354,7 +354,7 @@
+ 	struct drm_display_mode *panel_fixed_mode;
+ 	int index;
+ 
+-	index = i915.vbt_sdvo_panel_type;
++	index = i915_module.vbt_sdvo_panel_type;
+ 	if (index == -2) {
+ 		DRM_DEBUG_KMS("Ignore SDVO panel mode from BIOS VBT tables.\n");
+ 		return;
+@@ -627,16 +627,16 @@
+ 
+ 	switch (edp_link_params->preemphasis) {
+ 	case EDP_PREEMPHASIS_NONE:
+-		dev_priv->vbt.edp_preemphasis = DP_TRAIN_PRE_EMPHASIS_0;
++		dev_priv->vbt.edp_preemphasis = DP_TRAIN_PRE_EMPH_LEVEL_0;
+ 		break;
+ 	case EDP_PREEMPHASIS_3_5dB:
+-		dev_priv->vbt.edp_preemphasis = DP_TRAIN_PRE_EMPHASIS_3_5;
++		dev_priv->vbt.edp_preemphasis = DP_TRAIN_PRE_EMPH_LEVEL_1;
+ 		break;
+ 	case EDP_PREEMPHASIS_6dB:
+-		dev_priv->vbt.edp_preemphasis = DP_TRAIN_PRE_EMPHASIS_6;
++		dev_priv->vbt.edp_preemphasis = DP_TRAIN_PRE_EMPH_LEVEL_2;
+ 		break;
+ 	case EDP_PREEMPHASIS_9_5dB:
+-		dev_priv->vbt.edp_preemphasis = DP_TRAIN_PRE_EMPHASIS_9_5;
++		dev_priv->vbt.edp_preemphasis = DP_TRAIN_PRE_EMPH_LEVEL_3;
+ 		break;
+ 	default:
+ 		DRM_DEBUG_KMS("VBT has unknown eDP pre-emphasis value %u\n",
+@@ -646,16 +646,16 @@
+ 
+ 	switch (edp_link_params->vswing) {
+ 	case EDP_VSWING_0_4V:
+-		dev_priv->vbt.edp_vswing = DP_TRAIN_VOLTAGE_SWING_400;
++		dev_priv->vbt.edp_vswing = DP_TRAIN_VOLTAGE_SWING_LEVEL_0;
+ 		break;
+ 	case EDP_VSWING_0_6V:
+-		dev_priv->vbt.edp_vswing = DP_TRAIN_VOLTAGE_SWING_600;
++		dev_priv->vbt.edp_vswing = DP_TRAIN_VOLTAGE_SWING_LEVEL_1;
+ 		break;
+ 	case EDP_VSWING_0_8V:
+-		dev_priv->vbt.edp_vswing = DP_TRAIN_VOLTAGE_SWING_800;
++		dev_priv->vbt.edp_vswing = DP_TRAIN_VOLTAGE_SWING_LEVEL_2;
+ 		break;
+ 	case EDP_VSWING_1_2V:
+-		dev_priv->vbt.edp_vswing = DP_TRAIN_VOLTAGE_SWING_1200;
++		dev_priv->vbt.edp_vswing = DP_TRAIN_VOLTAGE_SWING_LEVEL_3;
+ 		break;
+ 	default:
+ 		DRM_DEBUG_KMS("VBT has unknown eDP voltage swing value %u\n",
+@@ -946,7 +946,7 @@
+ 		DRM_DEBUG_KMS("Analog port %c is also DP or TMDS compatible\n",
+ 			      port_name(port));
+ 	if (is_dvi && (port == PORT_A || port == PORT_E))
+-		DRM_DEBUG_KMS("Port %c is TMDS compabile\n", port_name(port));
++		DRM_DEBUG_KMS("Port %c is TMDS compatible\n", port_name(port));
+ 	if (!is_dvi && !is_dp && !is_crt)
+ 		DRM_DEBUG_KMS("Port %c is not DP/TMDS/CRT compatible\n",
+ 			      port_name(port));
+@@ -976,12 +976,10 @@
+ 	if (bdb->version >= 158) {
+ 		/* The VBT HDMI level shift values match the table we have. */
+ 		hdmi_level_shift = child->raw[7] & 0xF;
+-		if (hdmi_level_shift < 0xC) {
+-			DRM_DEBUG_KMS("VBT HDMI level shift for port %c: %d\n",
+-				      port_name(port),
+-				      hdmi_level_shift);
+-			info->hdmi_level_shift = hdmi_level_shift;
+-		}
++		DRM_DEBUG_KMS("VBT HDMI level shift for port %c: %d\n",
++			      port_name(port),
++			      hdmi_level_shift);
++		info->hdmi_level_shift = hdmi_level_shift;
+ 	}
+ }
+ 
+@@ -1114,8 +1112,7 @@
+ 		struct ddi_vbt_port_info *info =
+ 			&dev_priv->vbt.ddi_port_info[port];
+ 
+-		/* Recommended BSpec default: 800mV 0dB. */
+-		info->hdmi_level_shift = 6;
++		info->hdmi_level_shift = HDMI_LEVEL_SHIFT_UNKNOWN;
+ 
+ 		info->supports_dvi = (port != PORT_A && port != PORT_E);
+ 		info->supports_hdmi = info->supports_dvi;
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_bios.h b/drivers/gpu/drm/i915/intel_bios.h
+--- a/drivers/gpu/drm/i915/intel_bios.h	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_bios.h	2014-11-20 09:53:37.976762837 -0700
+@@ -46,7 +46,7 @@
+ 	u16 version;			/**< decimal */
+ 	u16 header_size;		/**< in bytes */
+ 	u16 bdb_size;			/**< in bytes */
+-};
++} __packed;
+ 
+ /* strictly speaking, this is a "skip" block, but it has interesting info */
+ struct vbios_data {
+@@ -252,7 +252,7 @@
+ 	/* This one should also be safe to use anywhere, even without version
+ 	 * checks. */
+ 	struct common_child_dev_config common;
+-};
++} __packed;
+ 
+ struct bdb_general_definitions {
+ 	/* DDC GPIO */
+@@ -802,7 +802,8 @@
+ 
+ 	u16 rsvd4;
+ 
+-	u8 rsvd5[5];
++	u8 rsvd5;
++	u32 target_burst_mode_freq;
+ 	u32 dsi_ddr_clk;
+ 	u32 bridge_ref_clk;
+ 
+@@ -887,12 +888,12 @@
+ 	u16 bl_disable_delay;
+ 	u16 panel_off_delay;
+ 	u16 panel_power_cycle_delay;
+-};
++} __packed;
+ 
+ struct bdb_mipi_config {
+ 	struct mipi_config config[MAX_MIPI_CONFIGURATIONS];
+ 	struct mipi_pps_data pps[MAX_MIPI_CONFIGURATIONS];
+-};
++} __packed;
+ 
+ /* Block 53 contains MIPI sequences as needed by the panel
+  * for enabling it. This block can be variable in size and
+@@ -901,7 +902,7 @@
+ struct bdb_mipi_sequence {
+ 	u8 version;
+ 	u8 data[0];
+-};
++} __packed;
+ 
+ /* MIPI Sequnece Block definitions */
+ enum mipi_seq {
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_crt.c b/drivers/gpu/drm/i915/intel_crt.c
+--- a/drivers/gpu/drm/i915/intel_crt.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_crt.c	2014-11-20 09:53:37.976762837 -0700
+@@ -72,7 +72,7 @@
+ 	u32 tmp;
+ 
+ 	power_domain = intel_display_port_power_domain(encoder);
+-	if (!intel_display_power_enabled(dev_priv, power_domain))
++	if (!intel_display_power_is_enabled(dev_priv, power_domain))
+ 		return false;
+ 
+ 	tmp = I915_READ(crt->adpa_reg);
+@@ -775,7 +775,7 @@
+ 		I915_WRITE(crt->adpa_reg, adpa);
+ 		POSTING_READ(crt->adpa_reg);
+ 
+-		DRM_DEBUG_KMS("pch crt adpa set to 0x%x\n", adpa);
++		DRM_DEBUG_KMS("crt adpa set to 0x%x\n", adpa);
+ 		crt->force_hotplug_required = 1;
+ 	}
+ 
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_ddi.c b/drivers/gpu/drm/i915/intel_ddi.c
+--- a/drivers/gpu/drm/i915/intel_ddi.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_ddi.c	2014-11-20 09:53:37.976762837 -0700
+@@ -28,87 +28,129 @@
+ #include "i915_drv.h"
+ #include "intel_drv.h"
+ 
++struct ddi_buf_trans {
++	u32 trans1;	/* balance leg enable, de-emph level */
++	u32 trans2;	/* vref sel, vswing */
++};
++
+ /* HDMI/DVI modes ignore everything but the last 2 items. So we share
+  * them for both DP and FDI transports, allowing those ports to
+  * automatically adapt to HDMI connections as well
+  */
+-static const u32 hsw_ddi_translations_dp[] = {
+-	0x00FFFFFF, 0x0006000E,		/* DP parameters */
+-	0x00D75FFF, 0x0005000A,
+-	0x00C30FFF, 0x00040006,
+-	0x80AAAFFF, 0x000B0000,
+-	0x00FFFFFF, 0x0005000A,
+-	0x00D75FFF, 0x000C0004,
+-	0x80C30FFF, 0x000B0000,
+-	0x00FFFFFF, 0x00040006,
+-	0x80D75FFF, 0x000B0000,
++static const struct ddi_buf_trans hsw_ddi_translations_dp[] = {
++	{ 0x00FFFFFF, 0x0006000E },
++	{ 0x00D75FFF, 0x0005000A },
++	{ 0x00C30FFF, 0x00040006 },
++	{ 0x80AAAFFF, 0x000B0000 },
++	{ 0x00FFFFFF, 0x0005000A },
++	{ 0x00D75FFF, 0x000C0004 },
++	{ 0x80C30FFF, 0x000B0000 },
++	{ 0x00FFFFFF, 0x00040006 },
++	{ 0x80D75FFF, 0x000B0000 },
++};
++
++static const struct ddi_buf_trans hsw_ddi_translations_fdi[] = {
++	{ 0x00FFFFFF, 0x0007000E },
++	{ 0x00D75FFF, 0x000F000A },
++	{ 0x00C30FFF, 0x00060006 },
++	{ 0x00AAAFFF, 0x001E0000 },
++	{ 0x00FFFFFF, 0x000F000A },
++	{ 0x00D75FFF, 0x00160004 },
++	{ 0x00C30FFF, 0x001E0000 },
++	{ 0x00FFFFFF, 0x00060006 },
++	{ 0x00D75FFF, 0x001E0000 },
++};
++
++static const struct ddi_buf_trans hsw_ddi_translations_hdmi[] = {
++					/* Idx	NT mV d	T mV d	db	*/
++	{ 0x00FFFFFF, 0x0006000E },	/* 0:	400	400	0	*/
++	{ 0x00E79FFF, 0x000E000C },	/* 1:	400	500	2	*/
++	{ 0x00D75FFF, 0x0005000A },	/* 2:	400	600	3.5	*/
++	{ 0x00FFFFFF, 0x0005000A },	/* 3:	600	600	0	*/
++	{ 0x00E79FFF, 0x001D0007 },	/* 4:	600	750	2	*/
++	{ 0x00D75FFF, 0x000C0004 },	/* 5:	600	900	3.5	*/
++	{ 0x00FFFFFF, 0x00040006 },	/* 6:	800	800	0	*/
++	{ 0x80E79FFF, 0x00030002 },	/* 7:	800	1000	2	*/
++	{ 0x00FFFFFF, 0x00140005 },	/* 8:	850	850	0	*/
++	{ 0x00FFFFFF, 0x000C0004 },	/* 9:	900	900	0	*/
++	{ 0x00FFFFFF, 0x001C0003 },	/* 10:	950	950	0	*/
++	{ 0x80FFFFFF, 0x00030002 },	/* 11:	1000	1000	0	*/
++};
++
++static const struct ddi_buf_trans bdw_ddi_translations_edp[] = {
++	{ 0x00FFFFFF, 0x00000012 },
++	{ 0x00EBAFFF, 0x00020011 },
++	{ 0x00C71FFF, 0x0006000F },
++	{ 0x00AAAFFF, 0x000E000A },
++	{ 0x00FFFFFF, 0x00020011 },
++	{ 0x00DB6FFF, 0x0005000F },
++	{ 0x00BEEFFF, 0x000A000C },
++	{ 0x00FFFFFF, 0x0005000F },
++	{ 0x00DB6FFF, 0x000A000C },
+ };
+ 
+-static const u32 hsw_ddi_translations_fdi[] = {
+-	0x00FFFFFF, 0x0007000E,		/* FDI parameters */
+-	0x00D75FFF, 0x000F000A,
+-	0x00C30FFF, 0x00060006,
+-	0x00AAAFFF, 0x001E0000,
+-	0x00FFFFFF, 0x000F000A,
+-	0x00D75FFF, 0x00160004,
+-	0x00C30FFF, 0x001E0000,
+-	0x00FFFFFF, 0x00060006,
+-	0x00D75FFF, 0x001E0000,
++static const struct ddi_buf_trans bdw_ddi_translations_dp[] = {
++	{ 0x00FFFFFF, 0x0007000E },
++	{ 0x00D75FFF, 0x000E000A },
++	{ 0x00BEFFFF, 0x00140006 },
++	{ 0x80B2CFFF, 0x001B0002 },
++	{ 0x00FFFFFF, 0x000E000A },
++	{ 0x00DB6FFF, 0x00160005 },
++	{ 0x80C71FFF, 0x001A0002 },
++	{ 0x00F7DFFF, 0x00180004 },
++	{ 0x80D75FFF, 0x001B0002 },
+ };
+ 
+-static const u32 hsw_ddi_translations_hdmi[] = {
+-				/* Idx	NT mV diff	T mV diff	db  */
+-	0x00FFFFFF, 0x0006000E, /* 0:	400		400		0   */
+-	0x00E79FFF, 0x000E000C, /* 1:	400		500		2   */
+-	0x00D75FFF, 0x0005000A, /* 2:	400		600		3.5 */
+-	0x00FFFFFF, 0x0005000A, /* 3:	600		600		0   */
+-	0x00E79FFF, 0x001D0007, /* 4:	600		750		2   */
+-	0x00D75FFF, 0x000C0004, /* 5:	600		900		3.5 */
+-	0x00FFFFFF, 0x00040006, /* 6:	800		800		0   */
+-	0x80E79FFF, 0x00030002, /* 7:	800		1000		2   */
+-	0x00FFFFFF, 0x00140005, /* 8:	850		850		0   */
+-	0x00FFFFFF, 0x000C0004, /* 9:	900		900		0   */
+-	0x00FFFFFF, 0x001C0003, /* 10:	950		950		0   */
+-	0x80FFFFFF, 0x00030002, /* 11:	1000		1000		0   */
++static const struct ddi_buf_trans bdw_ddi_translations_fdi[] = {
++	{ 0x00FFFFFF, 0x0001000E },
++	{ 0x00D75FFF, 0x0004000A },
++	{ 0x00C30FFF, 0x00070006 },
++	{ 0x00AAAFFF, 0x000C0000 },
++	{ 0x00FFFFFF, 0x0004000A },
++	{ 0x00D75FFF, 0x00090004 },
++	{ 0x00C30FFF, 0x000C0000 },
++	{ 0x00FFFFFF, 0x00070006 },
++	{ 0x00D75FFF, 0x000C0000 },
+ };
+ 
+-static const u32 bdw_ddi_translations_edp[] = {
+-	0x00FFFFFF, 0x00000012,		/* eDP parameters */
+-	0x00EBAFFF, 0x00020011,
+-	0x00C71FFF, 0x0006000F,
+-	0x00AAAFFF, 0x000E000A,
+-	0x00FFFFFF, 0x00020011,
+-	0x00DB6FFF, 0x0005000F,
+-	0x00BEEFFF, 0x000A000C,
+-	0x00FFFFFF, 0x0005000F,
+-	0x00DB6FFF, 0x000A000C,
+-	0x00FFFFFF, 0x00140006		/* HDMI parameters 800mV 0dB*/
++static const struct ddi_buf_trans bdw_ddi_translations_hdmi[] = {
++					/* Idx	NT mV d	T mV df	db	*/
++	{ 0x00FFFFFF, 0x0007000E },	/* 0:	400	400	0	*/
++	{ 0x00D75FFF, 0x000E000A },	/* 1:	400	600	3.5	*/
++	{ 0x00BEFFFF, 0x00140006 },	/* 2:	400	800	6	*/
++	{ 0x00FFFFFF, 0x0009000D },	/* 3:	450	450	0	*/
++	{ 0x00FFFFFF, 0x000E000A },	/* 4:	600	600	0	*/
++	{ 0x00D7FFFF, 0x00140006 },	/* 5:	600	800	2.5	*/
++	{ 0x80CB2FFF, 0x001B0002 },	/* 6:	600	1000	4.5	*/
++	{ 0x00FFFFFF, 0x00140006 },	/* 7:	800	800	0	*/
++	{ 0x80E79FFF, 0x001B0002 },	/* 8:	800	1000	2	*/
++	{ 0x80FFFFFF, 0x001B0002 },	/* 9:	1000	1000	0	*/
+ };
+ 
+-static const u32 bdw_ddi_translations_dp[] = {
+-	0x00FFFFFF, 0x0007000E,		/* DP parameters */
+-	0x00D75FFF, 0x000E000A,
+-	0x00BEFFFF, 0x00140006,
+-	0x80B2CFFF, 0x001B0002,
+-	0x00FFFFFF, 0x000E000A,
+-	0x00D75FFF, 0x00180004,
+-	0x80CB2FFF, 0x001B0002,
+-	0x00F7DFFF, 0x00180004,
+-	0x80D75FFF, 0x001B0002,
+-	0x00FFFFFF, 0x00140006		/* HDMI parameters 800mV 0dB*/
++static const struct ddi_buf_trans skl_ddi_translations_dp[] = {
++	{ 0x00000018, 0x000000a0 },
++	{ 0x00004014, 0x00000098 },
++	{ 0x00006012, 0x00000088 },
++	{ 0x00008010, 0x00000080 },
++	{ 0x00000018, 0x00000098 },
++	{ 0x00004014, 0x00000088 },
++	{ 0x00006012, 0x00000080 },
++	{ 0x00000018, 0x00000088 },
++	{ 0x00004014, 0x00000080 },
+ };
+ 
+-static const u32 bdw_ddi_translations_fdi[] = {
+-	0x00FFFFFF, 0x0001000E,		/* FDI parameters */
+-	0x00D75FFF, 0x0004000A,
+-	0x00C30FFF, 0x00070006,
+-	0x00AAAFFF, 0x000C0000,
+-	0x00FFFFFF, 0x0004000A,
+-	0x00D75FFF, 0x00090004,
+-	0x00C30FFF, 0x000C0000,
+-	0x00FFFFFF, 0x00070006,
+-	0x00D75FFF, 0x000C0000,
+-	0x00FFFFFF, 0x00140006		/* HDMI parameters 800mV 0dB*/
++static const struct ddi_buf_trans skl_ddi_translations_hdmi[] = {
++					/* Idx	NT mV   T mV    db  */
++	{ 0x00000018, 0x000000a0 },	/* 0:	400	400	0   */
++	{ 0x00004014, 0x00000098 },	/* 1:	400	600	3.5 */
++	{ 0x00006012, 0x00000088 },	/* 2:	400	800	6   */
++	{ 0x00000018, 0x0000003c },	/* 3:	450	450	0   */
++	{ 0x00000018, 0x00000098 },	/* 4:	600	600	0   */
++	{ 0x00003015, 0x00000088 },	/* 5:	600	800	2.5 */
++	{ 0x00005013, 0x00000080 },	/* 6:	600	1000	4.5 */
++	{ 0x00000018, 0x00000088 },	/* 7:	800	800	0   */
++	{ 0x00000096, 0x00000080 },	/* 8:	800	1000	2   */
++	{ 0x00000018, 0x00000080 },	/* 9:	1200	1200	0   */
+ };
+ 
+ enum port intel_ddi_get_encoder_port(struct intel_encoder *intel_encoder)
+@@ -145,26 +187,43 @@
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	u32 reg;
+-	int i;
++	int i, n_hdmi_entries, hdmi_800mV_0dB;
+ 	int hdmi_level = dev_priv->vbt.ddi_port_info[port].hdmi_level_shift;
+-	const u32 *ddi_translations_fdi;
+-	const u32 *ddi_translations_dp;
+-	const u32 *ddi_translations_edp;
+-	const u32 *ddi_translations;
+-
+-	if (IS_BROADWELL(dev)) {
++	const struct ddi_buf_trans *ddi_translations_fdi;
++	const struct ddi_buf_trans *ddi_translations_dp;
++	const struct ddi_buf_trans *ddi_translations_edp;
++	const struct ddi_buf_trans *ddi_translations_hdmi;
++	const struct ddi_buf_trans *ddi_translations;
++
++	if (IS_SKYLAKE(dev)) {
++		ddi_translations_fdi = NULL;
++		ddi_translations_dp = skl_ddi_translations_dp;
++		ddi_translations_edp = skl_ddi_translations_dp;
++		ddi_translations_hdmi = skl_ddi_translations_hdmi;
++		n_hdmi_entries = ARRAY_SIZE(skl_ddi_translations_hdmi);
++		hdmi_800mV_0dB = 7;
++	} else if (IS_BROADWELL(dev)) {
+ 		ddi_translations_fdi = bdw_ddi_translations_fdi;
+ 		ddi_translations_dp = bdw_ddi_translations_dp;
+ 		ddi_translations_edp = bdw_ddi_translations_edp;
++		ddi_translations_hdmi = bdw_ddi_translations_hdmi;
++		n_hdmi_entries = ARRAY_SIZE(bdw_ddi_translations_hdmi);
++		hdmi_800mV_0dB = 7;
+ 	} else if (IS_HASWELL(dev)) {
+ 		ddi_translations_fdi = hsw_ddi_translations_fdi;
+ 		ddi_translations_dp = hsw_ddi_translations_dp;
+ 		ddi_translations_edp = hsw_ddi_translations_dp;
++		ddi_translations_hdmi = hsw_ddi_translations_hdmi;
++		n_hdmi_entries = ARRAY_SIZE(hsw_ddi_translations_hdmi);
++		hdmi_800mV_0dB = 6;
+ 	} else {
+ 		WARN(1, "ddi translation table missing\n");
+ 		ddi_translations_edp = bdw_ddi_translations_dp;
+ 		ddi_translations_fdi = bdw_ddi_translations_fdi;
+ 		ddi_translations_dp = bdw_ddi_translations_dp;
++		ddi_translations_hdmi = bdw_ddi_translations_hdmi;
++		n_hdmi_entries = ARRAY_SIZE(bdw_ddi_translations_hdmi);
++		hdmi_800mV_0dB = 7;
+ 	}
+ 
+ 	switch (port) {
+@@ -182,7 +241,10 @@
+ 			ddi_translations = ddi_translations_dp;
+ 		break;
+ 	case PORT_E:
+-		ddi_translations = ddi_translations_fdi;
++		if (ddi_translations_fdi)
++			ddi_translations = ddi_translations_fdi;
++		else
++			ddi_translations = ddi_translations_dp;
+ 		break;
+ 	default:
+ 		BUG();
+@@ -190,14 +252,22 @@
+ 
+ 	for (i = 0, reg = DDI_BUF_TRANS(port);
+ 	     i < ARRAY_SIZE(hsw_ddi_translations_fdi); i++) {
+-		I915_WRITE(reg, ddi_translations[i]);
++		I915_WRITE(reg, ddi_translations[i].trans1);
+ 		reg += 4;
+-	}
+-	/* Entry 9 is for HDMI: */
+-	for (i = 0; i < 2; i++) {
+-		I915_WRITE(reg, hsw_ddi_translations_hdmi[hdmi_level * 2 + i]);
++		I915_WRITE(reg, ddi_translations[i].trans2);
+ 		reg += 4;
+ 	}
++
++	/* Choose a good default if VBT is badly populated */
++	if (hdmi_level == HDMI_LEVEL_SHIFT_UNKNOWN ||
++	    hdmi_level >= n_hdmi_entries)
++		hdmi_level = hdmi_800mV_0dB;
++
++	/* Entry 9 is for HDMI: */
++	I915_WRITE(reg, ddi_translations_hdmi[hdmi_level].trans1);
++	reg += 4;
++	I915_WRITE(reg, ddi_translations_hdmi[hdmi_level].trans2);
++	reg += 4;
+ }
+ 
+ /* Program DDI buffers translations for DP. By default, program ports A-D in DP
+@@ -214,18 +284,6 @@
+ 		intel_prepare_ddi_buffers(dev, port);
+ }
+ 
+-static const long hsw_ddi_buf_ctl_values[] = {
+-	DDI_BUF_EMP_400MV_0DB_HSW,
+-	DDI_BUF_EMP_400MV_3_5DB_HSW,
+-	DDI_BUF_EMP_400MV_6DB_HSW,
+-	DDI_BUF_EMP_400MV_9_5DB_HSW,
+-	DDI_BUF_EMP_600MV_0DB_HSW,
+-	DDI_BUF_EMP_600MV_3_5DB_HSW,
+-	DDI_BUF_EMP_600MV_6DB_HSW,
+-	DDI_BUF_EMP_800MV_0DB_HSW,
+-	DDI_BUF_EMP_800MV_3_5DB_HSW
+-};
+-
+ static void intel_wait_ddi_buf_idle(struct drm_i915_private *dev_priv,
+ 				    enum port port)
+ {
+@@ -285,7 +343,7 @@
+ 
+ 	/* Start the training iterating through available voltages and emphasis,
+ 	 * testing each value twice. */
+-	for (i = 0; i < ARRAY_SIZE(hsw_ddi_buf_ctl_values) * 2; i++) {
++	for (i = 0; i < ARRAY_SIZE(hsw_ddi_translations_fdi) * 2; i++) {
+ 		/* Configure DP_TP_CTL with auto-training */
+ 		I915_WRITE(DP_TP_CTL(PORT_E),
+ 					DP_TP_CTL_FDI_AUTOTRAIN |
+@@ -300,7 +358,7 @@
+ 		I915_WRITE(DDI_BUF_CTL(PORT_E),
+ 			   DDI_BUF_CTL_ENABLE |
+ 			   ((intel_crtc->config.fdi_lanes - 1) << 1) |
+-			   hsw_ddi_buf_ctl_values[i / 2]);
++			   DDI_BUF_TRANS_SELECT(i / 2));
+ 		POSTING_READ(DDI_BUF_CTL(PORT_E));
+ 
+ 		udelay(600);
+@@ -375,7 +433,7 @@
+ 		enc_to_dig_port(&encoder->base);
+ 
+ 	intel_dp->DP = intel_dig_port->saved_port_bits |
+-		DDI_BUF_CTL_ENABLE | DDI_BUF_EMP_400MV_0DB_HSW;
++		DDI_BUF_CTL_ENABLE | DDI_BUF_TRANS_SELECT(0);
+ 	intel_dp->DP |= DDI_PORT_WIDTH(intel_dp->lane_count);
+ 
+ }
+@@ -401,8 +459,29 @@
+ 	return ret;
+ }
+ 
++static struct intel_encoder *
++intel_ddi_get_crtc_new_encoder(struct intel_crtc *crtc)
++{
++	struct drm_device *dev = crtc->base.dev;
++	struct intel_encoder *intel_encoder, *ret = NULL;
++	int num_encoders = 0;
++
++	for_each_intel_encoder(dev, intel_encoder) {
++		if (intel_encoder->new_crtc == crtc) {
++			ret = intel_encoder;
++			num_encoders++;
++		}
++	}
++
++	WARN(num_encoders != 1, "%d encoders on crtc for pipe %c\n", num_encoders,
++	     pipe_name(crtc->pipe));
++
++	BUG_ON(ret == NULL);
++	return ret;
++}
++
+ #define LC_FREQ 2700
+-#define LC_FREQ_2K (LC_FREQ * 2000)
++#define LC_FREQ_2K U64_C(LC_FREQ * 2000)
+ 
+ #define P_MIN 2
+ #define P_MAX 64
+@@ -414,7 +493,11 @@
+ #define VCO_MIN 2400
+ #define VCO_MAX 4800
+ 
+-#define ABS_DIFF(a, b) ((a > b) ? (a - b) : (b - a))
++#define abs_diff(a, b) ({			\
++	typeof(a) __a = (a);			\
++	typeof(b) __b = (b);			\
++	(void) (&__a == &__b);			\
++	__a > __b ? (__a - __b) : (__b - __a); })
+ 
+ struct wrpll_rnp {
+ 	unsigned p, n2, r2;
+@@ -524,9 +607,9 @@
+ 	 */
+ 	a = freq2k * budget * p * r2;
+ 	b = freq2k * budget * best->p * best->r2;
+-	diff = ABS_DIFF((freq2k * p * r2), (LC_FREQ_2K * n2));
+-	diff_best = ABS_DIFF((freq2k * best->p * best->r2),
+-			     (LC_FREQ_2K * best->n2));
++	diff = abs_diff(freq2k * p * r2, LC_FREQ_2K * n2);
++	diff_best = abs_diff(freq2k * best->p * best->r2,
++			     LC_FREQ_2K * best->n2);
+ 	c = 1000000 * diff;
+ 	d = 1000000 * diff_best;
+ 
+@@ -587,8 +670,117 @@
+ 	return (refclk * n * 100) / (p * r);
+ }
+ 
+-void intel_ddi_clock_get(struct intel_encoder *encoder,
+-			 struct intel_crtc_config *pipe_config)
++static int skl_calc_wrpll_link(struct drm_i915_private *dev_priv,
++			       uint32_t dpll)
++{
++	uint32_t cfgcr1_reg, cfgcr2_reg;
++	uint32_t cfgcr1_val, cfgcr2_val;
++	uint32_t p0, p1, p2, dco_freq;
++
++	cfgcr1_reg = GET_CFG_CR1_REG(dpll);
++	cfgcr2_reg = GET_CFG_CR2_REG(dpll);
++
++	cfgcr1_val = I915_READ(cfgcr1_reg);
++	cfgcr2_val = I915_READ(cfgcr2_reg);
++
++	p0 = cfgcr2_val & DPLL_CFGCR2_PDIV_MASK;
++	p2 = cfgcr2_val & DPLL_CFGCR2_KDIV_MASK;
++
++	if (cfgcr2_val &  DPLL_CFGCR2_QDIV_MODE(1))
++		p1 = (cfgcr2_val & DPLL_CFGCR2_QDIV_RATIO_MASK) >> 8;
++	else
++		p1 = 1;
++
++
++	switch (p0) {
++	case DPLL_CFGCR2_PDIV_1:
++		p0 = 1;
++		break;
++	case DPLL_CFGCR2_PDIV_2:
++		p0 = 2;
++		break;
++	case DPLL_CFGCR2_PDIV_3:
++		p0 = 3;
++		break;
++	case DPLL_CFGCR2_PDIV_7:
++		p0 = 7;
++		break;
++	}
++
++	switch (p2) {
++	case DPLL_CFGCR2_KDIV_5:
++		p2 = 5;
++		break;
++	case DPLL_CFGCR2_KDIV_2:
++		p2 = 2;
++		break;
++	case DPLL_CFGCR2_KDIV_3:
++		p2 = 3;
++		break;
++	case DPLL_CFGCR2_KDIV_1:
++		p2 = 1;
++		break;
++	}
++
++	dco_freq = (cfgcr1_val & DPLL_CFGCR1_DCO_INTEGER_MASK) * 24 * 1000;
++
++	dco_freq += (((cfgcr1_val & DPLL_CFGCR1_DCO_FRACTION_MASK) >> 9) * 24 *
++		1000) / 0x8000;
++
++	return dco_freq / (p0 * p1 * p2 * 5);
++}
++
++
++static void skl_ddi_clock_get(struct intel_encoder *encoder,
++				struct intel_crtc_config *pipe_config)
++{
++	struct drm_i915_private *dev_priv = encoder->base.dev->dev_private;
++	enum port port = intel_ddi_get_encoder_port(encoder);
++	int link_clock = 0;
++	uint32_t dpll_ctl1, dpll;
++
++	/* FIXME: This should be tracked in the pipe config. */
++	dpll = I915_READ(DPLL_CTRL2);
++	dpll &= DPLL_CTRL2_DDI_CLK_SEL_MASK(port);
++	dpll >>= DPLL_CTRL2_DDI_CLK_SEL_SHIFT(port);
++
++	dpll_ctl1 = I915_READ(DPLL_CTRL1);
++
++	if (dpll_ctl1 & DPLL_CTRL1_HDMI_MODE(dpll)) {
++		link_clock = skl_calc_wrpll_link(dev_priv, dpll);
++	} else {
++		link_clock = dpll_ctl1 & DPLL_CRTL1_LINK_RATE_MASK(dpll);
++		link_clock >>= DPLL_CRTL1_LINK_RATE_SHIFT(dpll);
++
++		switch (link_clock) {
++		case DPLL_CRTL1_LINK_RATE_810:
++			link_clock = 81000;
++			break;
++		case DPLL_CRTL1_LINK_RATE_1350:
++			link_clock = 135000;
++			break;
++		case DPLL_CRTL1_LINK_RATE_2700:
++			link_clock = 270000;
++			break;
++		default:
++			WARN(1, "Unsupported link rate\n");
++			break;
++		}
++		link_clock *= 2;
++	}
++
++	pipe_config->port_clock = link_clock;
++
++	if (pipe_config->has_dp_encoder)
++		pipe_config->adjusted_mode.crtc_clock =
++			intel_dotclock_calculate(pipe_config->port_clock,
++						 &pipe_config->dp_m_n);
++	else
++		pipe_config->adjusted_mode.crtc_clock = pipe_config->port_clock;
++}
++
++static void hsw_ddi_clock_get(struct intel_encoder *encoder,
++			      struct intel_crtc_config *pipe_config)
+ {
+ 	struct drm_i915_private *dev_priv = encoder->base.dev->dev_private;
+ 	int link_clock = 0;
+@@ -643,9 +835,15 @@
+ 		pipe_config->adjusted_mode.crtc_clock = pipe_config->port_clock;
+ }
+ 
++void intel_ddi_clock_get(struct intel_encoder *encoder,
++			 struct intel_crtc_config *pipe_config)
++{
++	hsw_ddi_clock_get(encoder, pipe_config);
++}
++
+ static void
+-intel_ddi_calculate_wrpll(int clock /* in Hz */,
+-			  unsigned *r2_out, unsigned *n2_out, unsigned *p_out)
++hsw_ddi_calculate_wrpll(int clock /* in Hz */,
++			unsigned *r2_out, unsigned *n2_out, unsigned *p_out)
+ {
+ 	uint64_t freq2k;
+ 	unsigned p, n2, r2;
+@@ -708,33 +906,23 @@
+ 	*r2_out = best.r2;
+ }
+ 
+-/*
+- * Tries to find a PLL for the CRTC. If it finds, it increases the refcount and
+- * stores it in intel_crtc->ddi_pll_sel, so other mode sets won't be able to
+- * steal the selected PLL. You need to call intel_ddi_pll_enable to actually
+- * enable the PLL.
+- */
+-bool intel_ddi_pll_select(struct intel_crtc *intel_crtc)
++static bool
++hsw_ddi_pll_select(struct intel_crtc *intel_crtc,
++		   struct intel_encoder *intel_encoder,
++		   int clock)
+ {
+-	struct drm_crtc *crtc = &intel_crtc->base;
+-	struct intel_encoder *intel_encoder = intel_ddi_get_crtc_encoder(crtc);
+-	int type = intel_encoder->type;
+-	int clock = intel_crtc->config.port_clock;
+-
+-	intel_put_shared_dpll(intel_crtc);
+-
+-	if (type == INTEL_OUTPUT_HDMI) {
++	if (intel_encoder->type == INTEL_OUTPUT_HDMI) {
+ 		struct intel_shared_dpll *pll;
+ 		uint32_t val;
+ 		unsigned p, n2, r2;
+ 
+-		intel_ddi_calculate_wrpll(clock * 1000, &r2, &n2, &p);
++		hsw_ddi_calculate_wrpll(clock * 1000, &r2, &n2, &p);
+ 
+ 		val = WRPLL_PLL_ENABLE | WRPLL_PLL_LCPLL |
+ 		      WRPLL_DIVIDER_REFERENCE(r2) | WRPLL_DIVIDER_FEEDBACK(n2) |
+ 		      WRPLL_DIVIDER_POST(p);
+ 
+-		intel_crtc->config.dpll_hw_state.wrpll = val;
++		intel_crtc->new_config->dpll_hw_state.wrpll = val;
+ 
+ 		pll = intel_get_shared_dpll(intel_crtc);
+ 		if (pll == NULL) {
+@@ -743,12 +931,255 @@
+ 			return false;
+ 		}
+ 
+-		intel_crtc->config.ddi_pll_sel = PORT_CLK_SEL_WRPLL(pll->id);
++		intel_crtc->new_config->ddi_pll_sel = PORT_CLK_SEL_WRPLL(pll->id);
++	}
++
++	return true;
++}
++
++struct skl_wrpll_params {
++	uint32_t        dco_fraction;
++	uint32_t        dco_integer;
++	uint32_t        qdiv_ratio;
++	uint32_t        qdiv_mode;
++	uint32_t        kdiv;
++	uint32_t        pdiv;
++	uint32_t        central_freq;
++};
++
++static void
++skl_ddi_calculate_wrpll(int clock /* in Hz */,
++			struct skl_wrpll_params *wrpll_params)
++{
++	uint64_t afe_clock = clock * 5; /* AFE Clock is 5x Pixel clock */
++	uint64_t dco_central_freq[3] = {8400000000ULL,
++					9000000000ULL,
++					9600000000ULL};
++	uint32_t min_dco_deviation = 400;
++	uint32_t min_dco_index = 3;
++	uint32_t P0[4] = {1, 2, 3, 7};
++	uint32_t P2[4] = {1, 2, 3, 5};
++	bool found = false;
++	uint32_t candidate_p = 0;
++	uint32_t candidate_p0[3] = {0}, candidate_p1[3] = {0};
++	uint32_t candidate_p2[3] = {0};
++	uint32_t dco_central_freq_deviation[3];
++	uint32_t i, P1, k, dco_count;
++	bool retry_with_odd = false;
++	uint64_t dco_freq;
++
++	/* Determine P0, P1 or P2 */
++	for (dco_count = 0; dco_count < 3; dco_count++) {
++		found = false;
++		candidate_p =
++			div64_u64(dco_central_freq[dco_count], afe_clock);
++		if (retry_with_odd == false)
++			candidate_p = (candidate_p % 2 == 0 ?
++				candidate_p : candidate_p + 1);
++
++		for (P1 = 1; P1 < candidate_p; P1++) {
++			for (i = 0; i < 4; i++) {
++				if (!(P0[i] != 1 || P1 == 1))
++					continue;
++
++				for (k = 0; k < 4; k++) {
++					if (P1 != 1 && P2[k] != 2)
++						continue;
++
++					if (candidate_p == P0[i] * P1 * P2[k]) {
++						/* Found possible P0, P1, P2 */
++						found = true;
++						candidate_p0[dco_count] = P0[i];
++						candidate_p1[dco_count] = P1;
++						candidate_p2[dco_count] = P2[k];
++						goto found;
++					}
++
++				}
++			}
++		}
++
++found:
++		if (found) {
++			dco_central_freq_deviation[dco_count] =
++				div64_u64(10000 *
++					  abs_diff((candidate_p * afe_clock),
++						   dco_central_freq[dco_count]),
++					  dco_central_freq[dco_count]);
++
++			if (dco_central_freq_deviation[dco_count] <
++				min_dco_deviation) {
++				min_dco_deviation =
++					dco_central_freq_deviation[dco_count];
++				min_dco_index = dco_count;
++			}
++		}
++
++		if (min_dco_index > 2 && dco_count == 2) {
++			retry_with_odd = true;
++			dco_count = 0;
++		}
++	}
++
++	if (min_dco_index > 2) {
++		WARN(1, "No valid values found for the given pixel clock\n");
++	} else {
++		 wrpll_params->central_freq = dco_central_freq[min_dco_index];
++
++		 switch (dco_central_freq[min_dco_index]) {
++		 case 9600000000ULL:
++			wrpll_params->central_freq = 0;
++			break;
++		 case 9000000000ULL:
++			wrpll_params->central_freq = 1;
++			break;
++		 case 8400000000ULL:
++			wrpll_params->central_freq = 3;
++		 }
++
++		 switch (candidate_p0[min_dco_index]) {
++		 case 1:
++			wrpll_params->pdiv = 0;
++			break;
++		 case 2:
++			wrpll_params->pdiv = 1;
++			break;
++		 case 3:
++			wrpll_params->pdiv = 2;
++			break;
++		 case 7:
++			wrpll_params->pdiv = 4;
++			break;
++		 default:
++			WARN(1, "Incorrect PDiv\n");
++		 }
++
++		 switch (candidate_p2[min_dco_index]) {
++		 case 5:
++			wrpll_params->kdiv = 0;
++			break;
++		 case 2:
++			wrpll_params->kdiv = 1;
++			break;
++		 case 3:
++			wrpll_params->kdiv = 2;
++			break;
++		 case 1:
++			wrpll_params->kdiv = 3;
++			break;
++		 default:
++			WARN(1, "Incorrect KDiv\n");
++		 }
++
++		 wrpll_params->qdiv_ratio = candidate_p1[min_dco_index];
++		 wrpll_params->qdiv_mode =
++			(wrpll_params->qdiv_ratio == 1) ? 0 : 1;
++
++		 dco_freq = candidate_p0[min_dco_index] *
++			 candidate_p1[min_dco_index] *
++			 candidate_p2[min_dco_index] * afe_clock;
++
++		/*
++		* Intermediate values are in Hz.
++		* Divide by MHz to match bsepc
++		*/
++		 wrpll_params->dco_integer = div_u64(dco_freq, (24 * MHz(1)));
++		 wrpll_params->dco_fraction =
++			 div_u64(((div_u64(dco_freq, 24) -
++				   wrpll_params->dco_integer * MHz(1)) * 0x8000), MHz(1));
++
++	}
++}
++
++
++static bool
++skl_ddi_pll_select(struct intel_crtc *intel_crtc,
++		   struct intel_encoder *intel_encoder,
++		   int clock)
++{
++	struct intel_shared_dpll *pll;
++	uint32_t ctrl1, cfgcr1, cfgcr2;
++
++	/*
++	 * See comment in intel_dpll_hw_state to understand why we always use 0
++	 * as the DPLL id in this function.
++	 */
++
++	ctrl1 = DPLL_CTRL1_OVERRIDE(0);
++
++	if (intel_encoder->type == INTEL_OUTPUT_HDMI) {
++		struct skl_wrpll_params wrpll_params = { 0, };
++
++		ctrl1 |= DPLL_CTRL1_HDMI_MODE(0);
++
++		skl_ddi_calculate_wrpll(clock * 1000, &wrpll_params);
++
++		cfgcr1 = DPLL_CFGCR1_FREQ_ENABLE |
++			 DPLL_CFGCR1_DCO_FRACTION(wrpll_params.dco_fraction) |
++			 wrpll_params.dco_integer;
++
++		cfgcr2 = DPLL_CFGCR2_QDIV_RATIO(wrpll_params.qdiv_ratio) |
++			 DPLL_CFGCR2_QDIV_MODE(wrpll_params.qdiv_mode) |
++			 DPLL_CFGCR2_KDIV(wrpll_params.kdiv) |
++			 DPLL_CFGCR2_PDIV(wrpll_params.pdiv) |
++			 wrpll_params.central_freq;
++	} else if (intel_encoder->type == INTEL_OUTPUT_DISPLAYPORT) {
++		struct drm_encoder *encoder = &intel_encoder->base;
++		struct intel_dp *intel_dp = enc_to_intel_dp(encoder);
++
++		switch (intel_dp->link_bw) {
++		case DP_LINK_BW_1_62:
++			ctrl1 |= DPLL_CRTL1_LINK_RATE(DPLL_CRTL1_LINK_RATE_810, 0);
++			break;
++		case DP_LINK_BW_2_7:
++			ctrl1 |= DPLL_CRTL1_LINK_RATE(DPLL_CRTL1_LINK_RATE_1350, 0);
++			break;
++		case DP_LINK_BW_5_4:
++			ctrl1 |= DPLL_CRTL1_LINK_RATE(DPLL_CRTL1_LINK_RATE_2700, 0);
++			break;
++		}
++
++		cfgcr1 = cfgcr2 = 0;
++	} else /* eDP */
++		return true;
++
++	intel_crtc->new_config->dpll_hw_state.ctrl1 = ctrl1;
++	intel_crtc->new_config->dpll_hw_state.cfgcr1 = cfgcr1;
++	intel_crtc->new_config->dpll_hw_state.cfgcr2 = cfgcr2;
++
++	pll = intel_get_shared_dpll(intel_crtc);
++	if (pll == NULL) {
++		DRM_DEBUG_DRIVER("failed to find PLL for pipe %c\n",
++				 pipe_name(intel_crtc->pipe));
++		return false;
+ 	}
+ 
++	/* shared DPLL id 0 is DPLL 1 */
++	intel_crtc->new_config->ddi_pll_sel = pll->id + 1;
++
+ 	return true;
+ }
+ 
++/*
++ * Tries to find a *shared* PLL for the CRTC and store it in
++ * intel_crtc->ddi_pll_sel.
++ *
++ * For private DPLLs, compute_config() should do the selection for us. This
++ * function should be folded into compute_config() eventually.
++ */
++bool intel_ddi_pll_select(struct intel_crtc *intel_crtc)
++{
++	struct drm_device *dev = intel_crtc->base.dev;
++	struct intel_encoder *intel_encoder =
++		intel_ddi_get_crtc_new_encoder(intel_crtc);
++	int clock = intel_crtc->new_config->port_clock;
++
++	if (IS_SKYLAKE(dev))
++		return skl_ddi_pll_select(intel_crtc, intel_encoder, clock);
++	else
++		return hsw_ddi_pll_select(intel_crtc, intel_encoder, clock);
++}
++
+ void intel_ddi_set_pipe_settings(struct drm_crtc *crtc)
+ {
+ 	struct drm_i915_private *dev_priv = crtc->dev->dev_private;
+@@ -921,7 +1352,7 @@
+ 	uint32_t tmp;
+ 
+ 	power_domain = intel_display_port_power_domain(intel_encoder);
+-	if (!intel_display_power_enabled(dev_priv, power_domain))
++	if (!intel_display_power_is_enabled(dev_priv, power_domain))
+ 		return false;
+ 
+ 	if (!intel_encoder->get_hw_state(intel_encoder, &pipe))
+@@ -967,7 +1398,7 @@
+ 	int i;
+ 
+ 	power_domain = intel_display_port_power_domain(encoder);
+-	if (!intel_display_power_enabled(dev_priv, power_domain))
++	if (!intel_display_power_is_enabled(dev_priv, power_domain))
+ 		return false;
+ 
+ 	tmp = I915_READ(DDI_BUF_CTL(port));
+@@ -1038,27 +1469,33 @@
+ static void intel_ddi_pre_enable(struct intel_encoder *intel_encoder)
+ {
+ 	struct drm_encoder *encoder = &intel_encoder->base;
+-	struct drm_i915_private *dev_priv = encoder->dev->dev_private;
++	struct drm_device *dev = encoder->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct intel_crtc *crtc = to_intel_crtc(encoder->crtc);
+ 	enum port port = intel_ddi_get_encoder_port(intel_encoder);
+ 	int type = intel_encoder->type;
+ 
+-	if (crtc->config.has_audio) {
+-		DRM_DEBUG_DRIVER("Audio on pipe %c on DDI\n",
+-				 pipe_name(crtc->pipe));
+-
+-		/* write eld */
+-		DRM_DEBUG_DRIVER("DDI audio: write eld information\n");
+-		intel_write_eld(encoder, &crtc->config.adjusted_mode);
+-	}
+-
+ 	if (type == INTEL_OUTPUT_EDP) {
+ 		struct intel_dp *intel_dp = enc_to_intel_dp(encoder);
+ 		intel_edp_panel_on(intel_dp);
+ 	}
+ 
+-	WARN_ON(crtc->config.ddi_pll_sel == PORT_CLK_SEL_NONE);
+-	I915_WRITE(PORT_CLK_SEL(port), crtc->config.ddi_pll_sel);
++	if (IS_SKYLAKE(dev)) {
++		uint32_t dpll = crtc->config.ddi_pll_sel;
++		uint32_t val;
++
++		val = I915_READ(DPLL_CTRL2);
++
++		val &= ~(DPLL_CTRL2_DDI_CLK_OFF(port) |
++			DPLL_CTRL2_DDI_CLK_SEL_MASK(port));
++		val |= (DPLL_CTRL2_DDI_CLK_SEL(dpll, port) |
++			DPLL_CTRL2_DDI_SEL_OVERRIDE(port));
++
++		I915_WRITE(DPLL_CTRL2, val);
++	} else {
++		WARN_ON(crtc->config.ddi_pll_sel == PORT_CLK_SEL_NONE);
++		I915_WRITE(PORT_CLK_SEL(port), crtc->config.ddi_pll_sel);
++	}
+ 
+ 	if (type == INTEL_OUTPUT_DISPLAYPORT || type == INTEL_OUTPUT_EDP) {
+ 		struct intel_dp *intel_dp = enc_to_intel_dp(encoder);
+@@ -1068,7 +1505,7 @@
+ 		intel_dp_sink_dpms(intel_dp, DRM_MODE_DPMS_ON);
+ 		intel_dp_start_link_train(intel_dp);
+ 		intel_dp_complete_link_train(intel_dp);
+-		if (port != PORT_A)
++		if (port != PORT_A || INTEL_INFO(dev)->gen >= 9)
+ 			intel_dp_stop_link_train(intel_dp);
+ 	} else if (type == INTEL_OUTPUT_HDMI) {
+ 		struct intel_hdmi *intel_hdmi = enc_to_intel_hdmi(encoder);
+@@ -1082,7 +1519,8 @@
+ static void intel_ddi_post_disable(struct intel_encoder *intel_encoder)
+ {
+ 	struct drm_encoder *encoder = &intel_encoder->base;
+-	struct drm_i915_private *dev_priv = encoder->dev->dev_private;
++	struct drm_device *dev = encoder->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	enum port port = intel_ddi_get_encoder_port(intel_encoder);
+ 	int type = intel_encoder->type;
+ 	uint32_t val;
+@@ -1110,7 +1548,11 @@
+ 		intel_edp_panel_off(intel_dp);
+ 	}
+ 
+-	I915_WRITE(PORT_CLK_SEL(port), PORT_CLK_SEL_NONE);
++	if (IS_SKYLAKE(dev))
++		I915_WRITE(DPLL_CTRL2, (I915_READ(DPLL_CTRL2) |
++					DPLL_CTRL2_DDI_CLK_OFF(port)));
++	else
++		I915_WRITE(PORT_CLK_SEL(port), PORT_CLK_SEL_NONE);
+ }
+ 
+ static void intel_enable_ddi(struct intel_encoder *intel_encoder)
+@@ -1118,12 +1560,10 @@
+ 	struct drm_encoder *encoder = &intel_encoder->base;
+ 	struct drm_crtc *crtc = encoder->crtc;
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+-	int pipe = intel_crtc->pipe;
+ 	struct drm_device *dev = encoder->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	enum port port = intel_ddi_get_encoder_port(intel_encoder);
+ 	int type = intel_encoder->type;
+-	uint32_t tmp;
+ 
+ 	if (type == INTEL_OUTPUT_HDMI) {
+ 		struct intel_digital_port *intel_dig_port =
+@@ -1139,7 +1579,7 @@
+ 	} else if (type == INTEL_OUTPUT_EDP) {
+ 		struct intel_dp *intel_dp = enc_to_intel_dp(encoder);
+ 
+-		if (port == PORT_A)
++		if (port == PORT_A && INTEL_INFO(dev)->gen < 9)
+ 			intel_dp_stop_link_train(intel_dp);
+ 
+ 		intel_edp_backlight_on(intel_dp);
+@@ -1148,9 +1588,7 @@
+ 
+ 	if (intel_crtc->config.has_audio) {
+ 		intel_display_power_get(dev_priv, POWER_DOMAIN_AUDIO);
+-		tmp = I915_READ(HSW_AUD_PIN_ELD_CP_VLD);
+-		tmp |= ((AUDIO_OUTPUT_ENABLE_A | AUDIO_ELD_VALID_A) << (pipe * 4));
+-		I915_WRITE(HSW_AUD_PIN_ELD_CP_VLD, tmp);
++		intel_audio_codec_enable(intel_encoder);
+ 	}
+ }
+ 
+@@ -1159,19 +1597,12 @@
+ 	struct drm_encoder *encoder = &intel_encoder->base;
+ 	struct drm_crtc *crtc = encoder->crtc;
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+-	int pipe = intel_crtc->pipe;
+ 	int type = intel_encoder->type;
+ 	struct drm_device *dev = encoder->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	uint32_t tmp;
+ 
+-	/* We can't touch HSW_AUD_PIN_ELD_CP_VLD uncionditionally because this
+-	 * register is part of the power well on Haswell. */
+ 	if (intel_crtc->config.has_audio) {
+-		tmp = I915_READ(HSW_AUD_PIN_ELD_CP_VLD);
+-		tmp &= ~((AUDIO_OUTPUT_ENABLE_A | AUDIO_ELD_VALID_A) <<
+-			 (pipe * 4));
+-		I915_WRITE(HSW_AUD_PIN_ELD_CP_VLD, tmp);
++		intel_audio_codec_disable(intel_encoder);
+ 		intel_display_power_put(dev_priv, POWER_DOMAIN_AUDIO);
+ 	}
+ 
+@@ -1183,37 +1614,109 @@
+ 	}
+ }
+ 
+-int intel_ddi_get_cdclk_freq(struct drm_i915_private *dev_priv)
++static int skl_get_cdclk_freq(struct drm_i915_private *dev_priv)
++{
++	uint32_t lcpll1 = I915_READ(LCPLL1_CTL);
++	uint32_t cdctl = I915_READ(CDCLK_CTL);
++	uint32_t linkrate;
++
++	if (!(lcpll1 & LCPLL_PLL_ENABLE)) {
++		WARN(1, "LCPLL1 not enabled\n");
++		return 24000; /* 24MHz is the cd freq with NSSC ref */
++	}
++
++	if ((cdctl & CDCLK_FREQ_SEL_MASK) == CDCLK_FREQ_540)
++		return 540000;
++
++	linkrate = (I915_READ(DPLL_CTRL1) &
++		    DPLL_CRTL1_LINK_RATE_MASK(SKL_DPLL0)) >> 1;
++
++	if (linkrate == DPLL_CRTL1_LINK_RATE_2160 ||
++	    linkrate == DPLL_CRTL1_LINK_RATE_1080) {
++		/* vco 8640 */
++		switch (cdctl & CDCLK_FREQ_SEL_MASK) {
++		case CDCLK_FREQ_450_432:
++			return 432000;
++		case CDCLK_FREQ_337_308:
++			return 308570;
++		case CDCLK_FREQ_675_617:
++			return 617140;
++		default:
++			WARN(1, "Unknown cd freq selection\n");
++		}
++	} else {
++		/* vco 8100 */
++		switch (cdctl & CDCLK_FREQ_SEL_MASK) {
++		case CDCLK_FREQ_450_432:
++			return 450000;
++		case CDCLK_FREQ_337_308:
++			return 337500;
++		case CDCLK_FREQ_675_617:
++			return 675000;
++		default:
++			WARN(1, "Unknown cd freq selection\n");
++		}
++	}
++
++	/* error case, do as if DPLL0 isn't enabled */
++	return 24000;
++}
++
++static int bdw_get_cdclk_freq(struct drm_i915_private *dev_priv)
++{
++	uint32_t lcpll = I915_READ(LCPLL_CTL);
++	uint32_t freq = lcpll & LCPLL_CLK_FREQ_MASK;
++
++	if (lcpll & LCPLL_CD_SOURCE_FCLK)
++		return 800000;
++	else if (I915_READ(FUSE_STRAP) & HSW_CDCLK_LIMIT)
++		return 450000;
++	else if (freq == LCPLL_CLK_FREQ_450)
++		return 450000;
++	else if (freq == LCPLL_CLK_FREQ_54O_BDW)
++		return 540000;
++	else if (freq == LCPLL_CLK_FREQ_337_5_BDW)
++		return 337500;
++	else
++		return 675000;
++}
++
++static int hsw_get_cdclk_freq(struct drm_i915_private *dev_priv)
+ {
+ 	struct drm_device *dev = dev_priv->dev;
+ 	uint32_t lcpll = I915_READ(LCPLL_CTL);
+ 	uint32_t freq = lcpll & LCPLL_CLK_FREQ_MASK;
+ 
+-	if (lcpll & LCPLL_CD_SOURCE_FCLK) {
++	if (lcpll & LCPLL_CD_SOURCE_FCLK)
+ 		return 800000;
+-	} else if (I915_READ(FUSE_STRAP) & HSW_CDCLK_LIMIT) {
++	else if (I915_READ(FUSE_STRAP) & HSW_CDCLK_LIMIT)
+ 		return 450000;
+-	} else if (freq == LCPLL_CLK_FREQ_450) {
++	else if (freq == LCPLL_CLK_FREQ_450)
+ 		return 450000;
+-	} else if (IS_HASWELL(dev)) {
+-		if (IS_ULT(dev))
+-			return 337500;
+-		else
+-			return 540000;
+-	} else {
+-		if (freq == LCPLL_CLK_FREQ_54O_BDW)
+-			return 540000;
+-		else if (freq == LCPLL_CLK_FREQ_337_5_BDW)
+-			return 337500;
+-		else
+-			return 675000;
+-	}
++	else if (IS_HSW_ULT(dev))
++		return 337500;
++	else
++		return 540000;
++}
++
++int intel_ddi_get_cdclk_freq(struct drm_i915_private *dev_priv)
++{
++	struct drm_device *dev = dev_priv->dev;
++
++	if (IS_SKYLAKE(dev))
++		return skl_get_cdclk_freq(dev_priv);
++
++	if (IS_BROADWELL(dev))
++		return bdw_get_cdclk_freq(dev_priv);
++
++	/* Haswell */
++	return hsw_get_cdclk_freq(dev_priv);
+ }
+ 
+ static void hsw_ddi_pll_enable(struct drm_i915_private *dev_priv,
+ 			       struct intel_shared_dpll *pll)
+ {
+-	I915_WRITE(WRPLL_CTL(pll->id), pll->hw_state.wrpll);
++	I915_WRITE(WRPLL_CTL(pll->id), pll->config.hw_state.wrpll);
+ 	POSTING_READ(WRPLL_CTL(pll->id));
+ 	udelay(20);
+ }
+@@ -1234,7 +1737,7 @@
+ {
+ 	uint32_t val;
+ 
+-	if (!intel_display_power_enabled(dev_priv, POWER_DOMAIN_PLLS))
++	if (!intel_display_power_is_enabled(dev_priv, POWER_DOMAIN_PLLS))
+ 		return false;
+ 
+ 	val = I915_READ(WRPLL_CTL(pll->id));
+@@ -1248,10 +1751,8 @@
+ 	"WRPLL 2",
+ };
+ 
+-void intel_ddi_pll_init(struct drm_device *dev)
++static void hsw_shared_dplls_init(struct drm_i915_private *dev_priv)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	uint32_t val = I915_READ(LCPLL_CTL);
+ 	int i;
+ 
+ 	dev_priv->num_shared_dpll = 2;
+@@ -1264,20 +1765,158 @@
+ 		dev_priv->shared_dplls[i].get_hw_state =
+ 			hsw_ddi_pll_get_hw_state;
+ 	}
++}
+ 
+-	/* The LCPLL register should be turned on by the BIOS. For now let's
+-	 * just check its state and print errors in case something is wrong.
+-	 * Don't even try to turn it on.
+-	 */
++static const char * const skl_ddi_pll_names[] = {
++	"DPLL 1",
++	"DPLL 2",
++	"DPLL 3",
++};
++
++struct skl_dpll_regs {
++	u32 ctl, cfgcr1, cfgcr2;
++};
++
++/* this array is indexed by the *shared* pll id */
++static const struct skl_dpll_regs skl_dpll_regs[3] = {
++	{
++		/* DPLL 1 */
++		.ctl = LCPLL2_CTL,
++		.cfgcr1 = DPLL1_CFGCR1,
++		.cfgcr2 = DPLL1_CFGCR2,
++	},
++	{
++		/* DPLL 2 */
++		.ctl = WRPLL_CTL1,
++		.cfgcr1 = DPLL2_CFGCR1,
++		.cfgcr2 = DPLL2_CFGCR2,
++	},
++	{
++		/* DPLL 3 */
++		.ctl = WRPLL_CTL2,
++		.cfgcr1 = DPLL3_CFGCR1,
++		.cfgcr2 = DPLL3_CFGCR2,
++	},
++};
++
++static void skl_ddi_pll_enable(struct drm_i915_private *dev_priv,
++			       struct intel_shared_dpll *pll)
++{
++	uint32_t val;
++	unsigned int dpll;
++	const struct skl_dpll_regs *regs = skl_dpll_regs;
++
++	/* DPLL0 is not part of the shared DPLLs, so pll->id is 0 for DPLL1 */
++	dpll = pll->id + 1;
++
++	val = I915_READ(DPLL_CTRL1);
++
++	val &= ~(DPLL_CTRL1_HDMI_MODE(dpll) | DPLL_CTRL1_SSC(dpll) |
++		 DPLL_CRTL1_LINK_RATE_MASK(dpll));
++	val |= pll->config.hw_state.ctrl1 << (dpll * 6);
++
++	I915_WRITE(DPLL_CTRL1, val);
++	POSTING_READ(DPLL_CTRL1);
++
++	I915_WRITE(regs[pll->id].cfgcr1, pll->config.hw_state.cfgcr1);
++	I915_WRITE(regs[pll->id].cfgcr2, pll->config.hw_state.cfgcr2);
++	POSTING_READ(regs[pll->id].cfgcr1);
++	POSTING_READ(regs[pll->id].cfgcr2);
++
++	/* the enable bit is always bit 31 */
++	I915_WRITE(regs[pll->id].ctl,
++		   I915_READ(regs[pll->id].ctl) | LCPLL_PLL_ENABLE);
++
++	if (wait_for(I915_READ(DPLL_STATUS) & DPLL_LOCK(dpll), 5))
++		DRM_ERROR("DPLL %d not locked\n", dpll);
++}
++
++static void skl_ddi_pll_disable(struct drm_i915_private *dev_priv,
++				struct intel_shared_dpll *pll)
++{
++	const struct skl_dpll_regs *regs = skl_dpll_regs;
++
++	/* the enable bit is always bit 31 */
++	I915_WRITE(regs[pll->id].ctl,
++		   I915_READ(regs[pll->id].ctl) & ~LCPLL_PLL_ENABLE);
++	POSTING_READ(regs[pll->id].ctl);
++}
++
++static bool skl_ddi_pll_get_hw_state(struct drm_i915_private *dev_priv,
++				     struct intel_shared_dpll *pll,
++				     struct intel_dpll_hw_state *hw_state)
++{
++	uint32_t val;
++	unsigned int dpll;
++	const struct skl_dpll_regs *regs = skl_dpll_regs;
++
++	if (!intel_display_power_is_enabled(dev_priv, POWER_DOMAIN_PLLS))
++		return false;
++
++	/* DPLL0 is not part of the shared DPLLs, so pll->id is 0 for DPLL1 */
++	dpll = pll->id + 1;
++
++	val = I915_READ(regs[pll->id].ctl);
++	if (!(val & LCPLL_PLL_ENABLE))
++		return false;
++
++	val = I915_READ(DPLL_CTRL1);
++	hw_state->ctrl1 = (val >> (dpll * 6)) & 0x3f;
++
++	/* avoid reading back stale values if HDMI mode is not enabled */
++	if (val & DPLL_CTRL1_HDMI_MODE(dpll)) {
++		hw_state->cfgcr1 = I915_READ(regs[pll->id].cfgcr1);
++		hw_state->cfgcr2 = I915_READ(regs[pll->id].cfgcr2);
++	}
++
++	return true;
++}
++
++static void skl_shared_dplls_init(struct drm_i915_private *dev_priv)
++{
++	int i;
++
++	dev_priv->num_shared_dpll = 3;
++
++	for (i = 0; i < dev_priv->num_shared_dpll; i++) {
++		dev_priv->shared_dplls[i].id = i;
++		dev_priv->shared_dplls[i].name = skl_ddi_pll_names[i];
++		dev_priv->shared_dplls[i].disable = skl_ddi_pll_disable;
++		dev_priv->shared_dplls[i].enable = skl_ddi_pll_enable;
++		dev_priv->shared_dplls[i].get_hw_state =
++			skl_ddi_pll_get_hw_state;
++	}
++}
++
++void intel_ddi_pll_init(struct drm_device *dev)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	uint32_t val = I915_READ(LCPLL_CTL);
++
++	if (IS_SKYLAKE(dev))
++		skl_shared_dplls_init(dev_priv);
++	else
++		hsw_shared_dplls_init(dev_priv);
+ 
+ 	DRM_DEBUG_KMS("CDCLK running at %dKHz\n",
+ 		      intel_ddi_get_cdclk_freq(dev_priv));
+ 
+-	if (val & LCPLL_CD_SOURCE_FCLK)
+-		DRM_ERROR("CDCLK source is not LCPLL\n");
++	if (IS_SKYLAKE(dev)) {
++		if (!(I915_READ(LCPLL1_CTL) & LCPLL_PLL_ENABLE))
++			DRM_ERROR("LCPLL1 is disabled\n");
++	} else {
++		/*
++		 * The LCPLL register should be turned on by the BIOS. For now
++		 * let's just check its state and print errors in case
++		 * something is wrong.  Don't even try to turn it on.
++		 */
+ 
+-	if (val & LCPLL_PLL_DISABLE)
+-		DRM_ERROR("LCPLL is disabled\n");
++		if (val & LCPLL_CD_SOURCE_FCLK)
++			DRM_ERROR("CDCLK source is not LCPLL\n");
++
++		if (val & LCPLL_PLL_DISABLE)
++			DRM_ERROR("LCPLL is disabled\n");
++	}
+ }
+ 
+ void intel_ddi_prepare_link_retrain(struct drm_encoder *encoder)
+@@ -1373,6 +2012,7 @@
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(encoder->base.crtc);
+ 	enum transcoder cpu_transcoder = intel_crtc->config.cpu_transcoder;
+ 	u32 temp, flags = 0;
++	struct drm_device *dev = dev_priv->dev;
+ 
+ 	temp = I915_READ(TRANS_DDI_FUNC_CTL(cpu_transcoder));
+ 	if (temp & TRANS_DDI_PHSYNC)
+@@ -1418,9 +2058,9 @@
+ 		break;
+ 	}
+ 
+-	if (intel_display_power_enabled(dev_priv, POWER_DOMAIN_AUDIO)) {
++	if (intel_display_power_is_enabled(dev_priv, POWER_DOMAIN_AUDIO)) {
+ 		temp = I915_READ(HSW_AUD_PIN_ELD_CP_VLD);
+-		if (temp & (AUDIO_OUTPUT_ENABLE_A << (intel_crtc->pipe * 4)))
++		if (temp & AUDIO_OUTPUT_ENABLE(intel_crtc->pipe))
+ 			pipe_config->has_audio = true;
+ 	}
+ 
+@@ -1444,7 +2084,10 @@
+ 		dev_priv->vbt.edp_bpp = pipe_config->pipe_bpp;
+ 	}
+ 
+-	intel_ddi_clock_get(encoder, pipe_config);
++	if (INTEL_INFO(dev)->gen <= 8)
++		hsw_ddi_clock_get(encoder, pipe_config);
++	else
++		skl_ddi_clock_get(encoder, pipe_config);
+ }
+ 
+ static void intel_ddi_destroy(struct drm_encoder *encoder)
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_display.c b/drivers/gpu/drm/i915/intel_display.c
+--- a/drivers/gpu/drm/i915/intel_display.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_display.c	2014-11-21 21:14:47.678689075 -0700
+@@ -73,11 +73,6 @@
+ 	DRM_FORMAT_ARGB8888,
+ };
+ 
+-#define DIV_ROUND_CLOSEST_ULL(ll, d)	\
+-({ unsigned long long _tmp = (ll)+(d)/2; do_div(_tmp, d); _tmp; })
+-
+-static void intel_increase_pllclock(struct drm_device *dev,
+-				    enum pipe pipe);
+ static void intel_crtc_update_cursor(struct drm_crtc *crtc, bool on);
+ 
+ static void i9xx_crtc_clock_get(struct intel_crtc *crtc,
+@@ -91,15 +86,18 @@
+ 				  struct intel_framebuffer *ifb,
+ 				  struct drm_mode_fb_cmd2 *mode_cmd,
+ 				  struct drm_i915_gem_object *obj);
+-static void intel_dp_set_m_n(struct intel_crtc *crtc);
+ static void i9xx_set_pipeconf(struct intel_crtc *intel_crtc);
+ static void intel_set_pipe_timings(struct intel_crtc *intel_crtc);
+ static void intel_cpu_transcoder_set_m_n(struct intel_crtc *crtc,
+-					 struct intel_link_m_n *m_n);
++					 struct intel_link_m_n *m_n,
++					 struct intel_link_m_n *m2_n2);
+ static void ironlake_set_pipeconf(struct drm_crtc *crtc);
+ static void haswell_set_pipeconf(struct drm_crtc *crtc);
+ static void intel_set_pipe_csc(struct drm_crtc *crtc);
+-static void vlv_prepare_pll(struct intel_crtc *crtc);
++static void vlv_prepare_pll(struct intel_crtc *crtc,
++			    const struct intel_crtc_config *pipe_config);
++static void chv_prepare_pll(struct intel_crtc *crtc,
++			    const struct intel_crtc_config *pipe_config);
+ 
+ static struct intel_encoder *intel_find_encoder(struct intel_connector *connector, int pipe)
+ {
+@@ -410,25 +408,43 @@
+ /**
+  * Returns whether any output on the specified pipe is of the specified type
+  */
+-static bool intel_pipe_has_type(struct drm_crtc *crtc, int type)
++bool intel_pipe_has_type(struct intel_crtc *crtc, enum intel_output_type type)
+ {
+-	struct drm_device *dev = crtc->dev;
++	struct drm_device *dev = crtc->base.dev;
+ 	struct intel_encoder *encoder;
+ 
+-	for_each_encoder_on_crtc(dev, crtc, encoder)
++	for_each_encoder_on_crtc(dev, &crtc->base, encoder)
+ 		if (encoder->type == type)
+ 			return true;
+ 
+ 	return false;
+ }
+ 
+-static const intel_limit_t *intel_ironlake_limit(struct drm_crtc *crtc,
++/**
++ * Returns whether any output on the specified pipe will have the specified
++ * type after a staged modeset is complete, i.e., the same as
++ * intel_pipe_has_type() but looking at encoder->new_crtc instead of
++ * encoder->crtc.
++ */
++static bool intel_pipe_will_have_type(struct intel_crtc *crtc, int type)
++{
++	struct drm_device *dev = crtc->base.dev;
++	struct intel_encoder *encoder;
++
++	for_each_intel_encoder(dev, encoder)
++		if (encoder->new_crtc == crtc && encoder->type == type)
++			return true;
++
++	return false;
++}
++
++static const intel_limit_t *intel_ironlake_limit(struct intel_crtc *crtc,
+ 						int refclk)
+ {
+-	struct drm_device *dev = crtc->dev;
++	struct drm_device *dev = crtc->base.dev;
+ 	const intel_limit_t *limit;
+ 
+-	if (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS)) {
++	if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS)) {
+ 		if (intel_is_dual_link_lvds(dev)) {
+ 			if (refclk == 100000)
+ 				limit = &intel_limits_ironlake_dual_lvds_100m;
+@@ -446,20 +462,20 @@
+ 	return limit;
+ }
+ 
+-static const intel_limit_t *intel_g4x_limit(struct drm_crtc *crtc)
++static const intel_limit_t *intel_g4x_limit(struct intel_crtc *crtc)
+ {
+-	struct drm_device *dev = crtc->dev;
++	struct drm_device *dev = crtc->base.dev;
+ 	const intel_limit_t *limit;
+ 
+-	if (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS)) {
++	if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS)) {
+ 		if (intel_is_dual_link_lvds(dev))
+ 			limit = &intel_limits_g4x_dual_channel_lvds;
+ 		else
+ 			limit = &intel_limits_g4x_single_channel_lvds;
+-	} else if (intel_pipe_has_type(crtc, INTEL_OUTPUT_HDMI) ||
+-		   intel_pipe_has_type(crtc, INTEL_OUTPUT_ANALOG)) {
++	} else if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_HDMI) ||
++		   intel_pipe_will_have_type(crtc, INTEL_OUTPUT_ANALOG)) {
+ 		limit = &intel_limits_g4x_hdmi;
+-	} else if (intel_pipe_has_type(crtc, INTEL_OUTPUT_SDVO)) {
++	} else if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_SDVO)) {
+ 		limit = &intel_limits_g4x_sdvo;
+ 	} else /* The option is for other outputs */
+ 		limit = &intel_limits_i9xx_sdvo;
+@@ -467,9 +483,9 @@
+ 	return limit;
+ }
+ 
+-static const intel_limit_t *intel_limit(struct drm_crtc *crtc, int refclk)
++static const intel_limit_t *intel_limit(struct intel_crtc *crtc, int refclk)
+ {
+-	struct drm_device *dev = crtc->dev;
++	struct drm_device *dev = crtc->base.dev;
+ 	const intel_limit_t *limit;
+ 
+ 	if (HAS_PCH_SPLIT(dev))
+@@ -477,7 +493,7 @@
+ 	else if (IS_G4X(dev)) {
+ 		limit = intel_g4x_limit(crtc);
+ 	} else if (IS_PINEVIEW(dev)) {
+-		if (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS))
++		if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS))
+ 			limit = &intel_limits_pineview_lvds;
+ 		else
+ 			limit = &intel_limits_pineview_sdvo;
+@@ -486,14 +502,14 @@
+ 	} else if (IS_VALLEYVIEW(dev)) {
+ 		limit = &intel_limits_vlv;
+ 	} else if (!IS_GEN2(dev)) {
+-		if (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS))
++		if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS))
+ 			limit = &intel_limits_i9xx_lvds;
+ 		else
+ 			limit = &intel_limits_i9xx_sdvo;
+ 	} else {
+-		if (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS))
++		if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS))
+ 			limit = &intel_limits_i8xx_lvds;
+-		else if (intel_pipe_has_type(crtc, INTEL_OUTPUT_DVO))
++		else if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_DVO))
+ 			limit = &intel_limits_i8xx_dvo;
+ 		else
+ 			limit = &intel_limits_i8xx_dac;
+@@ -580,15 +596,15 @@
+ }
+ 
+ static bool
+-i9xx_find_best_dpll(const intel_limit_t *limit, struct drm_crtc *crtc,
++i9xx_find_best_dpll(const intel_limit_t *limit, struct intel_crtc *crtc,
+ 		    int target, int refclk, intel_clock_t *match_clock,
+ 		    intel_clock_t *best_clock)
+ {
+-	struct drm_device *dev = crtc->dev;
++	struct drm_device *dev = crtc->base.dev;
+ 	intel_clock_t clock;
+ 	int err = target;
+ 
+-	if (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS)) {
++	if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS)) {
+ 		/*
+ 		 * For LVDS just rely on its current settings for dual-channel.
+ 		 * We haven't figured out how to reliably set up different
+@@ -641,15 +657,15 @@
+ }
+ 
+ static bool
+-pnv_find_best_dpll(const intel_limit_t *limit, struct drm_crtc *crtc,
++pnv_find_best_dpll(const intel_limit_t *limit, struct intel_crtc *crtc,
+ 		   int target, int refclk, intel_clock_t *match_clock,
+ 		   intel_clock_t *best_clock)
+ {
+-	struct drm_device *dev = crtc->dev;
++	struct drm_device *dev = crtc->base.dev;
+ 	intel_clock_t clock;
+ 	int err = target;
+ 
+-	if (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS)) {
++	if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS)) {
+ 		/*
+ 		 * For LVDS just rely on its current settings for dual-channel.
+ 		 * We haven't figured out how to reliably set up different
+@@ -700,11 +716,11 @@
+ }
+ 
+ static bool
+-g4x_find_best_dpll(const intel_limit_t *limit, struct drm_crtc *crtc,
++g4x_find_best_dpll(const intel_limit_t *limit, struct intel_crtc *crtc,
+ 		   int target, int refclk, intel_clock_t *match_clock,
+ 		   intel_clock_t *best_clock)
+ {
+-	struct drm_device *dev = crtc->dev;
++	struct drm_device *dev = crtc->base.dev;
+ 	intel_clock_t clock;
+ 	int max_n;
+ 	bool found;
+@@ -712,7 +728,7 @@
+ 	int err_most = (target >> 8) + (target >> 9);
+ 	found = false;
+ 
+-	if (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS)) {
++	if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS)) {
+ 		if (intel_is_dual_link_lvds(dev))
+ 			clock.p2 = limit->p2.p2_fast;
+ 		else
+@@ -757,11 +773,11 @@
+ }
+ 
+ static bool
+-vlv_find_best_dpll(const intel_limit_t *limit, struct drm_crtc *crtc,
++vlv_find_best_dpll(const intel_limit_t *limit, struct intel_crtc *crtc,
+ 		   int target, int refclk, intel_clock_t *match_clock,
+ 		   intel_clock_t *best_clock)
+ {
+-	struct drm_device *dev = crtc->dev;
++	struct drm_device *dev = crtc->base.dev;
+ 	intel_clock_t clock;
+ 	unsigned int bestppm = 1000000;
+ 	/* min update 19.2 MHz */
+@@ -814,11 +830,11 @@
+ }
+ 
+ static bool
+-chv_find_best_dpll(const intel_limit_t *limit, struct drm_crtc *crtc,
++chv_find_best_dpll(const intel_limit_t *limit, struct intel_crtc *crtc,
+ 		   int target, int refclk, intel_clock_t *match_clock,
+ 		   intel_clock_t *best_clock)
+ {
+-	struct drm_device *dev = crtc->dev;
++	struct drm_device *dev = crtc->base.dev;
+ 	intel_clock_t clock;
+ 	uint64_t m2;
+ 	int found = false;
+@@ -891,58 +907,6 @@
+ 	return intel_crtc->config.cpu_transcoder;
+ }
+ 
+-static void g4x_wait_for_vblank(struct drm_device *dev, int pipe)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	u32 frame, frame_reg = PIPE_FRMCOUNT_GM45(pipe);
+-
+-	frame = I915_READ(frame_reg);
+-
+-	if (wait_for(I915_READ_NOTRACE(frame_reg) != frame, 50))
+-		WARN(1, "vblank wait timed out\n");
+-}
+-
+-/**
+- * intel_wait_for_vblank - wait for vblank on a given pipe
+- * @dev: drm device
+- * @pipe: pipe to wait for
+- *
+- * Wait for vblank to occur on a given pipe.  Needed for various bits of
+- * mode setting code.
+- */
+-void intel_wait_for_vblank(struct drm_device *dev, int pipe)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	int pipestat_reg = PIPESTAT(pipe);
+-
+-	if (IS_G4X(dev) || INTEL_INFO(dev)->gen >= 5) {
+-		g4x_wait_for_vblank(dev, pipe);
+-		return;
+-	}
+-
+-	/* Clear existing vblank status. Note this will clear any other
+-	 * sticky status fields as well.
+-	 *
+-	 * This races with i915_driver_irq_handler() with the result
+-	 * that either function could miss a vblank event.  Here it is not
+-	 * fatal, as we will either wait upon the next vblank interrupt or
+-	 * timeout.  Generally speaking intel_wait_for_vblank() is only
+-	 * called during modeset at which time the GPU should be idle and
+-	 * should *not* be performing page flips and thus not waiting on
+-	 * vblanks...
+-	 * Currently, the result of us stealing a vblank from the irq
+-	 * handler is that a single frame will be skipped during swapbuffers.
+-	 */
+-	I915_WRITE(pipestat_reg,
+-		   I915_READ(pipestat_reg) | PIPE_VBLANK_INTERRUPT_STATUS);
+-
+-	/* Wait for vblank interrupt bit to set */
+-	if (wait_for(I915_READ(pipestat_reg) &
+-		     PIPE_VBLANK_INTERRUPT_STATUS,
+-		     50))
+-		DRM_DEBUG_KMS("vblank wait timed out\n");
+-}
+-
+ static bool pipe_dsl_stopped(struct drm_device *dev, enum pipe pipe)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+@@ -964,8 +928,7 @@
+ 
+ /*
+  * intel_wait_for_pipe_off - wait for pipe to turn off
+- * @dev: drm device
+- * @pipe: pipe to wait for
++ * @crtc: crtc whose pipe to wait for
+  *
+  * After disabling a pipe, we can't wait for vblank in the usual way,
+  * spinning on the vblank interrupt status bit, since we won't actually
+@@ -979,11 +942,12 @@
+  *   ends up stopping at the start of the next frame).
+  *
+  */
+-void intel_wait_for_pipe_off(struct drm_device *dev, int pipe)
++static void intel_wait_for_pipe_off(struct intel_crtc *crtc)
+ {
++	struct drm_device *dev = crtc->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	enum transcoder cpu_transcoder = intel_pipe_to_cpu_transcoder(dev_priv,
+-								      pipe);
++	enum transcoder cpu_transcoder = crtc->config.cpu_transcoder;
++	enum pipe pipe = crtc->pipe;
+ 
+ 	if (INTEL_INFO(dev)->gen >= 4) {
+ 		int reg = PIPECONF(cpu_transcoder);
+@@ -1189,30 +1153,43 @@
+ 	     state_string(state), state_string(cur_state));
+ }
+ 
+-static void assert_panel_unlocked(struct drm_i915_private *dev_priv,
+-				  enum pipe pipe)
++void assert_panel_unlocked(struct drm_i915_private *dev_priv,
++			   enum pipe pipe)
+ {
+-	int pp_reg, lvds_reg;
++	struct drm_device *dev = dev_priv->dev;
++	int pp_reg;
+ 	u32 val;
+ 	enum pipe panel_pipe = PIPE_A;
+ 	bool locked = true;
+ 
+-	if (HAS_PCH_SPLIT(dev_priv->dev)) {
++	if (WARN_ON(HAS_DDI(dev)))
++		return;
++
++	if (HAS_PCH_SPLIT(dev)) {
++		u32 port_sel;
++
+ 		pp_reg = PCH_PP_CONTROL;
+-		lvds_reg = PCH_LVDS;
++		port_sel = I915_READ(PCH_PP_ON_DELAYS) & PANEL_PORT_SELECT_MASK;
++
++		if (port_sel == PANEL_PORT_SELECT_LVDS &&
++		    I915_READ(PCH_LVDS) & LVDS_PIPEB_SELECT)
++			panel_pipe = PIPE_B;
++		/* XXX: else fix for eDP */
++	} else if (IS_VALLEYVIEW(dev)) {
++		/* presumably write lock depends on pipe, not port select */
++		pp_reg = VLV_PIPE_PP_CONTROL(pipe);
++		panel_pipe = pipe;
+ 	} else {
+ 		pp_reg = PP_CONTROL;
+-		lvds_reg = LVDS;
++		if (I915_READ(LVDS) & LVDS_PIPEB_SELECT)
++			panel_pipe = PIPE_B;
+ 	}
+ 
+ 	val = I915_READ(pp_reg);
+ 	if (!(val & PANEL_POWER_ON) ||
+-	    ((val & PANEL_UNLOCK_REGS) == PANEL_UNLOCK_REGS))
++	    ((val & PANEL_UNLOCK_MASK) == PANEL_UNLOCK_REGS))
+ 		locked = false;
+ 
+-	if (I915_READ(lvds_reg) & LVDS_PIPEB_SELECT)
+-		panel_pipe = PIPE_B;
+-
+ 	WARN(panel_pipe == pipe && locked,
+ 	     "panel assertion failure, pipe %c regs locked\n",
+ 	     pipe_name(pipe));
+@@ -1236,7 +1213,7 @@
+ #define assert_cursor_enabled(d, p) assert_cursor(d, p, true)
+ #define assert_cursor_disabled(d, p) assert_cursor(d, p, false)
+ 
+-void assert_pipe(struct drm_i915_private *dev_priv,
++bool assert_pipe(struct drm_i915_private *dev_priv,
+ 		 enum pipe pipe, bool state)
+ {
+ 	int reg;
+@@ -1245,11 +1222,12 @@
+ 	enum transcoder cpu_transcoder = intel_pipe_to_cpu_transcoder(dev_priv,
+ 								      pipe);
+ 
+-	/* if we need the pipe A quirk it must be always on */
+-	if (pipe == PIPE_A && dev_priv->quirks & QUIRK_PIPEA_FORCE)
++	/* if we need the pipe quirk it must be always on */
++	if ((pipe == PIPE_A && dev_priv->quirks & QUIRK_PIPEA_FORCE) ||
++	    (pipe == PIPE_B && dev_priv->quirks & QUIRK_PIPEB_FORCE))
+ 		state = true;
+ 
+-	if (!intel_display_power_enabled(dev_priv,
++	if (!intel_display_power_is_enabled(dev_priv,
+ 				POWER_DOMAIN_TRANSCODER(cpu_transcoder))) {
+ 		cur_state = false;
+ 	} else {
+@@ -1258,12 +1236,12 @@
+ 		cur_state = !!(val & PIPECONF_ENABLE);
+ 	}
+ 
+-	WARN(cur_state != state,
+-	     "pipe %c assertion failure (expected %s, current %s)\n",
+-	     pipe_name(pipe), state_string(state), state_string(cur_state));
++	return !WARN(cur_state != state,
++		     "pipe %c assertion failure (expected %s, current %s)\n",
++		     pipe_name(pipe), state_string(state), state_string(cur_state));
+ }
+ 
+-static void assert_plane(struct drm_i915_private *dev_priv,
++static bool assert_plane(struct drm_i915_private *dev_priv,
+ 			 enum plane plane, bool state)
+ {
+ 	int reg;
+@@ -1273,9 +1251,9 @@
+ 	reg = DSPCNTR(plane);
+ 	val = I915_READ(reg);
+ 	cur_state = !!(val & DISPLAY_PLANE_ENABLE);
+-	WARN(cur_state != state,
+-	     "plane %c assertion failure (expected %s, current %s)\n",
+-	     plane_name(plane), state_string(state), state_string(cur_state));
++	return !WARN(cur_state != state,
++		     "plane %c assertion failure (expected %s, current %s)\n",
++		     plane_name(plane), state_string(state), state_string(cur_state));
+ }
+ 
+ #define assert_plane_enabled(d, p) assert_plane(d, p, true)
+@@ -1300,7 +1278,7 @@
+ 	}
+ 
+ 	/* Need to check both planes against the pipe */
+-	for_each_pipe(i) {
++	for_each_pipe(dev_priv, i) {
+ 		reg = DSPCNTR(i);
+ 		val = I915_READ(reg);
+ 		cur_pipe = (val & DISPPLANE_SEL_PIPE_MASK) >>
+@@ -1318,7 +1296,14 @@
+ 	int reg, sprite;
+ 	u32 val;
+ 
+-	if (IS_VALLEYVIEW(dev)) {
++	if (INTEL_INFO(dev)->gen >= 9) {
++		for_each_sprite(pipe, sprite) {
++			val = I915_READ(PLANE_CTL(pipe, sprite));
++			WARN(val & PLANE_CTL_ENABLE,
++			     "plane %d assertion failure, should be off on pipe %c but is still active\n",
++			     sprite, pipe_name(pipe));
++		}
++	} else if (IS_VALLEYVIEW(dev)) {
+ 		for_each_sprite(pipe, sprite) {
+ 			reg = SPCNTR(pipe, sprite);
+ 			val = I915_READ(reg);
+@@ -1341,6 +1326,12 @@
+ 	}
+ }
+ 
++static void assert_vblank_disabled(struct drm_crtc *crtc)
++{
++  	if (WARN_ON(drm_crtc_vblank_get(crtc) == 0))
++		drm_crtc_vblank_put(crtc);
++}
++
+ static void ibx_assert_pch_refclk_enabled(struct drm_i915_private *dev_priv)
+ {
+ 	u32 val;
+@@ -1513,40 +1504,13 @@
+ 	}
+ }
+ 
+-static void intel_reset_dpio(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-
+-	if (IS_CHERRYVIEW(dev)) {
+-		enum dpio_phy phy;
+-		u32 val;
+-
+-		for (phy = DPIO_PHY0; phy < I915_NUM_PHYS_VLV; phy++) {
+-			/* Poll for phypwrgood signal */
+-			if (wait_for(I915_READ(DISPLAY_PHY_STATUS) &
+-						PHY_POWERGOOD(phy), 1))
+-				DRM_ERROR("Display PHY %d is not power up\n", phy);
+-
+-			/*
+-			 * Deassert common lane reset for PHY.
+-			 *
+-			 * This should only be done on init and resume from S3
+-			 * with both PLLs disabled, or we risk losing DPIO and
+-			 * PLL synchronization.
+-			 */
+-			val = I915_READ(DISPLAY_PHY_CONTROL);
+-			I915_WRITE(DISPLAY_PHY_CONTROL,
+-				PHY_COM_LANE_RESET_DEASSERT(phy, val));
+-		}
+-	}
+-}
+-
+-static void vlv_enable_pll(struct intel_crtc *crtc)
++static void vlv_enable_pll(struct intel_crtc *crtc,
++			   const struct intel_crtc_config *pipe_config)
+ {
+ 	struct drm_device *dev = crtc->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	int reg = DPLL(crtc->pipe);
+-	u32 dpll = crtc->config.dpll_hw_state.dpll;
++	u32 dpll = pipe_config->dpll_hw_state.dpll;
+ 
+ 	assert_pipe_disabled(dev_priv, crtc->pipe);
+ 
+@@ -1554,7 +1518,7 @@
+ 	BUG_ON(!IS_VALLEYVIEW(dev_priv->dev));
+ 
+ 	/* PLL is protected by panel, make sure we can write it */
+-	if (IS_MOBILE(dev_priv->dev) && !IS_I830(dev_priv->dev))
++	if (IS_MOBILE(dev_priv->dev))
+ 		assert_panel_unlocked(dev_priv, crtc->pipe);
+ 
+ 	I915_WRITE(reg, dpll);
+@@ -1564,7 +1528,7 @@
+ 	if (wait_for(((I915_READ(reg) & DPLL_LOCK_VLV) == DPLL_LOCK_VLV), 1))
+ 		DRM_ERROR("DPLL %d failed to lock\n", crtc->pipe);
+ 
+-	I915_WRITE(DPLL_MD(crtc->pipe), crtc->config.dpll_hw_state.dpll_md);
++	I915_WRITE(DPLL_MD(crtc->pipe), pipe_config->dpll_hw_state.dpll_md);
+ 	POSTING_READ(DPLL_MD(crtc->pipe));
+ 
+ 	/* We do this three times for luck */
+@@ -1579,7 +1543,8 @@
+ 	udelay(150); /* wait for warmup */
+ }
+ 
+-static void chv_enable_pll(struct intel_crtc *crtc)
++static void chv_enable_pll(struct intel_crtc *crtc,
++			   const struct intel_crtc_config *pipe_config)
+ {
+ 	struct drm_device *dev = crtc->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+@@ -1604,19 +1569,31 @@
+ 	udelay(1);
+ 
+ 	/* Enable PLL */
+-	I915_WRITE(DPLL(pipe), crtc->config.dpll_hw_state.dpll);
++	I915_WRITE(DPLL(pipe), pipe_config->dpll_hw_state.dpll);
+ 
+ 	/* Check PLL is locked */
+ 	if (wait_for(((I915_READ(DPLL(pipe)) & DPLL_LOCK_VLV) == DPLL_LOCK_VLV), 1))
+ 		DRM_ERROR("PLL %d failed to lock\n", pipe);
+ 
+ 	/* not sure when this should be written */
+-	I915_WRITE(DPLL_MD(pipe), crtc->config.dpll_hw_state.dpll_md);
++	I915_WRITE(DPLL_MD(pipe), pipe_config->dpll_hw_state.dpll_md);
+ 	POSTING_READ(DPLL_MD(pipe));
+ 
+ 	mutex_unlock(&dev_priv->dpio_lock);
+ }
+ 
++static int intel_num_dvo_pipes(struct drm_device *dev)
++{
++	struct intel_crtc *crtc;
++	int count = 0;
++
++	for_each_intel_crtc(dev, crtc)
++		count += crtc->active &&
++			intel_pipe_has_type(crtc, INTEL_OUTPUT_DVO);
++
++	return count;
++}
++
+ static void i9xx_enable_pll(struct intel_crtc *crtc)
+ {
+ 	struct drm_device *dev = crtc->base.dev;
+@@ -1633,7 +1610,18 @@
+ 	if (IS_MOBILE(dev) && !IS_I830(dev))
+ 		assert_panel_unlocked(dev_priv, crtc->pipe);
+ 
+-	I915_WRITE(reg, dpll);
++	/* Enable DVO 2x clock on both PLLs if necessary */
++	if (IS_I830(dev) && intel_num_dvo_pipes(dev) > 0) {
++		/*
++		 * It appears to be important that we don't enable this
++		 * for the current pipe before otherwise configuring the
++		 * PLL. No idea how this should be handled if multiple
++		 * DVO outputs are enabled simultaneosly.
++		 */
++		dpll |= DPLL_DVO_2X_MODE;
++		I915_WRITE(DPLL(!crtc->pipe),
++			   I915_READ(DPLL(!crtc->pipe)) | DPLL_DVO_2X_MODE);
++	}
+ 
+ 	/* Wait for the clocks to stabilize. */
+ 	POSTING_READ(reg);
+@@ -1672,10 +1660,25 @@
+  *
+  * Note!  This is for pre-ILK only.
+  */
+-static void i9xx_disable_pll(struct drm_i915_private *dev_priv, enum pipe pipe)
++static void i9xx_disable_pll(struct intel_crtc *crtc)
+ {
+-	/* Don't disable pipe A or pipe A PLLs if needed */
+-	if (pipe == PIPE_A && (dev_priv->quirks & QUIRK_PIPEA_FORCE))
++	struct drm_device *dev = crtc->base.dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	enum pipe pipe = crtc->pipe;
++
++	/* Disable DVO 2x clock on both PLLs if necessary */
++	if (IS_I830(dev) &&
++	    intel_pipe_has_type(crtc, INTEL_OUTPUT_DVO) &&
++	    intel_num_dvo_pipes(dev) == 1) {
++		I915_WRITE(DPLL(PIPE_B),
++			   I915_READ(DPLL(PIPE_B)) & ~DPLL_DVO_2X_MODE);
++		I915_WRITE(DPLL(PIPE_A),
++			   I915_READ(DPLL(PIPE_A)) & ~DPLL_DVO_2X_MODE);
++	}
++
++	/* Don't disable pipe or pipe PLLs if needed */
++	if ((pipe == PIPE_A && dev_priv->quirks & QUIRK_PIPEA_FORCE) ||
++	    (pipe == PIPE_B && dev_priv->quirks & QUIRK_PIPEB_FORCE))
+ 		return;
+ 
+ 	/* Make sure the pipe isn't still relying on us */
+@@ -1712,7 +1715,7 @@
+ 	assert_pipe_disabled(dev_priv, pipe);
+ 
+ 	/* Set PLL en = 0 */
+-	val = DPLL_SSC_REF_CLOCK_CHV;
++	val = DPLL_SSC_REF_CLOCK_CHV | DPLL_REFA_CLK_ENABLE_VLV;
+ 	if (pipe != PIPE_A)
+ 		val |= DPLL_INTEGRATED_CRI_CLK_VLV;
+ 	I915_WRITE(DPLL(pipe), val);
+@@ -1776,7 +1779,7 @@
+ 	if (WARN_ON(pll == NULL))
+ 		return;
+ 
+-	WARN_ON(!pll->refcount);
++	WARN_ON(!pll->config.crtc_mask);
+ 	if (pll->active == 0) {
+ 		DRM_DEBUG_DRIVER("setting up %s\n", pll->name);
+ 		WARN_ON(pll->on);
+@@ -1803,10 +1806,10 @@
+ 	if (WARN_ON(pll == NULL))
+ 		return;
+ 
+-	if (WARN_ON(pll->refcount == 0))
++	if (WARN_ON(pll->config.crtc_mask == 0))
+ 		return;
+ 
+-	DRM_DEBUG_KMS("enable %s (active %d, on? %d)for crtc %d\n",
++	DRM_DEBUG_KMS("enable %s (active %d, on? %d) for crtc %d\n",
+ 		      pll->name, pll->active, pll->on,
+ 		      crtc->base.base.id);
+ 
+@@ -1824,7 +1827,7 @@
+ 	pll->on = true;
+ }
+ 
+-void intel_disable_shared_dpll(struct intel_crtc *crtc)
++static void intel_disable_shared_dpll(struct intel_crtc *crtc)
+ {
+ 	struct drm_device *dev = crtc->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+@@ -1835,7 +1838,7 @@
+ 	if (WARN_ON(pll == NULL))
+ 	       return;
+ 
+-	if (WARN_ON(pll->refcount == 0))
++	if (WARN_ON(pll->config.crtc_mask == 0))
+ 		return;
+ 
+ 	DRM_DEBUG_KMS("disable %s (active %d, on? %d) for crtc %d\n",
+@@ -1868,7 +1871,7 @@
+ 	uint32_t reg, val, pipeconf_val;
+ 
+ 	/* PCH only available on ILK+ */
+-	BUG_ON(INTEL_INFO(dev)->gen < 5);
++	BUG_ON(!HAS_PCH_SPLIT(dev));
+ 
+ 	/* Make sure PCH DPLL is enabled */
+ 	assert_shared_dpll_enabled(dev_priv,
+@@ -1903,7 +1906,7 @@
+ 	val &= ~TRANS_INTERLACE_MASK;
+ 	if ((pipeconf_val & PIPECONF_INTERLACE_MASK) == PIPECONF_INTERLACED_ILK)
+ 		if (HAS_PCH_IBX(dev_priv->dev) &&
+-		    intel_pipe_has_type(crtc, INTEL_OUTPUT_SDVO))
++		    intel_pipe_has_type(intel_crtc, INTEL_OUTPUT_SDVO))
+ 			val |= TRANS_LEGACY_INTERLACED_ILK;
+ 		else
+ 			val |= TRANS_INTERLACED;
+@@ -1921,7 +1924,7 @@
+ 	u32 val, pipeconf_val;
+ 
+ 	/* PCH only available on ILK+ */
+-	BUG_ON(INTEL_INFO(dev_priv->dev)->gen < 5);
++	BUG_ON(!HAS_PCH_SPLIT(dev_priv->dev));
+ 
+ 	/* FDI must be feeding us bits for PCH ports */
+ 	assert_fdi_tx_enabled(dev_priv, (enum pipe) cpu_transcoder);
+@@ -2026,7 +2029,7 @@
+ 	 * need the check.
+ 	 */
+ 	if (!HAS_PCH_SPLIT(dev_priv->dev))
+-		if (intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_DSI))
++		if (intel_pipe_has_type(crtc, INTEL_OUTPUT_DSI))
+ 			assert_dsi_pll_enabled(dev_priv);
+ 		else
+ 			assert_pll_enabled(dev_priv, pipe);
+@@ -2043,8 +2046,8 @@
+ 	reg = PIPECONF(cpu_transcoder);
+ 	val = I915_READ(reg);
+ 	if (val & PIPECONF_ENABLE) {
+-		WARN_ON(!(pipe == PIPE_A &&
+-			  dev_priv->quirks & QUIRK_PIPEA_FORCE));
++		WARN_ON(!((pipe == PIPE_A && dev_priv->quirks & QUIRK_PIPEA_FORCE) ||
++			  (pipe == PIPE_B && dev_priv->quirks & QUIRK_PIPEB_FORCE)));
+ 		return;
+ 	}
+ 
+@@ -2054,21 +2057,19 @@
+ 
+ /**
+  * intel_disable_pipe - disable a pipe, asserting requirements
+- * @dev_priv: i915 private structure
+- * @pipe: pipe to disable
++ * @crtc: crtc whose pipes is to be disabled
+  *
+- * Disable @pipe, making sure that various hardware specific requirements
+- * are met, if applicable, e.g. plane disabled, panel fitter off, etc.
+- *
+- * @pipe should be %PIPE_A or %PIPE_B.
++ * Disable the pipe of @crtc, making sure that various hardware
++ * specific requirements are met, if applicable, e.g. plane
++ * disabled, panel fitter off, etc.
+  *
+  * Will wait until the pipe has shut down before returning.
+  */
+-static void intel_disable_pipe(struct drm_i915_private *dev_priv,
+-			       enum pipe pipe)
++static void intel_disable_pipe(struct intel_crtc *crtc)
+ {
+-	enum transcoder cpu_transcoder = intel_pipe_to_cpu_transcoder(dev_priv,
+-								      pipe);
++	struct drm_i915_private *dev_priv = crtc->base.dev->dev_private;
++	enum transcoder cpu_transcoder = crtc->config.cpu_transcoder;
++	enum pipe pipe = crtc->pipe;
+ 	int reg;
+ 	u32 val;
+ 
+@@ -2080,17 +2081,26 @@
+ 	assert_cursor_disabled(dev_priv, pipe);
+ 	assert_sprites_disabled(dev_priv, pipe);
+ 
+-	/* Don't disable pipe A or pipe A PLLs if needed */
+-	if (pipe == PIPE_A && (dev_priv->quirks & QUIRK_PIPEA_FORCE))
+-		return;
+-
+ 	reg = PIPECONF(cpu_transcoder);
+ 	val = I915_READ(reg);
+ 	if ((val & PIPECONF_ENABLE) == 0)
+ 		return;
+ 
+-	I915_WRITE(reg, val & ~PIPECONF_ENABLE);
+-	intel_wait_for_pipe_off(dev_priv->dev, pipe);
++	/*
++	 * Double wide has implications for planes
++	 * so best keep it disabled when not needed.
++	 */
++	if (crtc->config.double_wide)
++		val &= ~PIPECONF_DOUBLE_WIDE;
++
++	/* Don't disable pipe or pipe PLLs if needed */
++	if (!(pipe == PIPE_A && dev_priv->quirks & QUIRK_PIPEA_FORCE) &&
++	    !(pipe == PIPE_B && dev_priv->quirks & QUIRK_PIPEB_FORCE))
++		val &= ~PIPECONF_ENABLE;
++
++	I915_WRITE(reg, val);
++	if ((val & PIPECONF_ENABLE) == 0)
++		intel_wait_for_pipe_off(crtc);
+ }
+ 
+ /*
+@@ -2109,35 +2119,28 @@
+ 
+ /**
+  * intel_enable_primary_hw_plane - enable the primary plane on a given pipe
+- * @dev_priv: i915 private structure
+- * @plane: plane to enable
+- * @pipe: pipe being fed
++ * @plane:  plane to be enabled
++ * @crtc: crtc for the plane
+  *
+- * Enable @plane on @pipe, making sure that @pipe is running first.
++ * Enable @plane on @crtc, making sure that the pipe is running first.
+  */
+-static void intel_enable_primary_hw_plane(struct drm_i915_private *dev_priv,
+-					  enum plane plane, enum pipe pipe)
++static void intel_enable_primary_hw_plane(struct drm_plane *plane,
++					  struct drm_crtc *crtc)
+ {
+-	struct drm_device *dev = dev_priv->dev;
+-	struct intel_crtc *intel_crtc =
+-		to_intel_crtc(dev_priv->pipe_to_crtc_mapping[pipe]);
+-	int reg;
+-	u32 val;
++	struct drm_device *dev = plane->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+ 
+ 	/* If the pipe isn't enabled, we can't pump pixels and may hang */
+-	assert_pipe_enabled(dev_priv, pipe);
++	assert_pipe_enabled(dev_priv, intel_crtc->pipe);
+ 
+ 	if (intel_crtc->primary_enabled)
+ 		return;
+ 
+ 	intel_crtc->primary_enabled = true;
+ 
+-	reg = DSPCNTR(plane);
+-	val = I915_READ(reg);
+-	WARN_ON(val & DISPLAY_PLANE_ENABLE);
+-
+-	I915_WRITE(reg, val | DISPLAY_PLANE_ENABLE);
+-	intel_flush_primary_plane(dev_priv, plane);
++	dev_priv->display.update_primary_plane(crtc, plane->fb,
++					       crtc->x, crtc->y);
+ 
+ 	/*
+ 	 * BDW signals flip done immediately if the plane
+@@ -2150,31 +2153,27 @@
+ 
+ /**
+  * intel_disable_primary_hw_plane - disable the primary hardware plane
+- * @dev_priv: i915 private structure
+- * @plane: plane to disable
+- * @pipe: pipe consuming the data
++ * @plane: plane to be disabled
++ * @crtc: crtc for the plane
+  *
+- * Disable @plane; should be an independent operation.
++ * Disable @plane on @crtc, making sure that the pipe is running first.
+  */
+-static void intel_disable_primary_hw_plane(struct drm_i915_private *dev_priv,
+-					   enum plane plane, enum pipe pipe)
++static void intel_disable_primary_hw_plane(struct drm_plane *plane,
++					   struct drm_crtc *crtc)
+ {
+-	struct intel_crtc *intel_crtc =
+-		to_intel_crtc(dev_priv->pipe_to_crtc_mapping[pipe]);
+-	int reg;
+-	u32 val;
++	struct drm_device *dev = plane->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++
++	assert_pipe_enabled(dev_priv, intel_crtc->pipe);
+ 
+ 	if (!intel_crtc->primary_enabled)
+ 		return;
+ 
+ 	intel_crtc->primary_enabled = false;
+ 
+-	reg = DSPCNTR(plane);
+-	val = I915_READ(reg);
+-	WARN_ON((val & DISPLAY_PLANE_ENABLE) == 0);
+-
+-	I915_WRITE(reg, val & ~DISPLAY_PLANE_ENABLE);
+-	intel_flush_primary_plane(dev_priv, plane);
++	dev_priv->display.update_primary_plane(crtc, plane->fb,
++					       crtc->x, crtc->y);
+ }
+ 
+ static bool need_vtd_wa(struct drm_device *dev)
+@@ -2195,11 +2194,13 @@
+ }
+ 
+ int
+-intel_pin_and_fence_fb_obj(struct drm_device *dev,
+-			   struct drm_i915_gem_object *obj,
+-			   struct intel_engine_cs *pipelined)
++intel_pin_and_fence_fb_obj(struct drm_plane *plane,
++			   struct drm_framebuffer *fb,
++			   struct i915_gem_request *pipelined)
+ {
++	struct drm_device *dev = fb->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_i915_gem_object *obj = intel_fb_obj(fb);
+ 	u32 alignment;
+ 	int ret;
+ 
+@@ -2207,7 +2208,9 @@
+ 
+ 	switch (obj->tiling_mode) {
+ 	case I915_TILING_NONE:
+-		if (IS_BROADWATER(dev) || IS_CRESTLINE(dev))
++		if (INTEL_INFO(dev)->gen >= 9)
++			alignment = 256 * 1024;
++		else if (IS_BROADWATER(dev) || IS_CRESTLINE(dev))
+ 			alignment = 128 * 1024;
+ 		else if (INTEL_INFO(dev)->gen >= 4)
+ 			alignment = 4 * 1024;
+@@ -2215,8 +2218,15 @@
+ 			alignment = 64 * 1024;
+ 		break;
+ 	case I915_TILING_X:
+-		/* pin() will align the object as required by fence */
+-		alignment = 0;
++		if (INTEL_INFO(dev)->gen >= 9) {
++			alignment = 256 * 1024;
++		} else {
++			/* Async page flipping requires X tiling and 
++			 * 32kB alignment, so just make all X tiled
++			 * frame buffers aligned for that.
++			 */
++			alignment = 32 * 1024;
++		}
+ 		break;
+ 	case I915_TILING_Y:
+ 		WARN(1, "Y tiled bo slipped through, driver bug!\n");
+@@ -2376,6 +2386,7 @@
+ 				 struct intel_plane_config *plane_config)
+ {
+ 	struct drm_device *dev = intel_crtc->base.dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct drm_crtc *c;
+ 	struct intel_crtc *i;
+ 	struct drm_i915_gem_object *obj;
+@@ -2407,6 +2418,9 @@
+ 			continue;
+ 
+ 		if (i915_gem_obj_ggtt_offset(obj) == plane_config->base) {
++			if (obj->tiling_mode != I915_TILING_NONE)
++				dev_priv->preserve_bios_swizzle = true;
++
+ 			drm_framebuffer_reference(c->primary->fb);
+ 			intel_crtc->base.primary->fb = c->primary->fb;
+ 			obj->frontbuffer_bits |= INTEL_FRONTBUFFER_PRIMARY(intel_crtc->pipe);
+@@ -2422,16 +2436,52 @@
+ 	struct drm_device *dev = crtc->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+-	struct drm_i915_gem_object *obj = intel_fb_obj(fb);
++	struct drm_i915_gem_object *obj;
+ 	int plane = intel_crtc->plane;
+ 	unsigned long linear_offset;
+ 	u32 dspcntr;
+-	u32 reg;
++	u32 reg = DSPCNTR(plane);
++	int pixel_size;
++
++	if (!intel_crtc->primary_enabled) {
++		I915_WRITE(reg, 0);
++		if (INTEL_INFO(dev)->gen >= 4)
++			I915_WRITE(DSPSURF(plane), 0);
++		else
++			I915_WRITE(DSPADDR(plane), 0);
++		POSTING_READ(reg);
++		return;
++	}
++
++	obj = intel_fb_obj(fb);
++	if (WARN_ON(obj == NULL))
++		return;
++
++	pixel_size = drm_format_plane_cpp(fb->pixel_format, 0);
++
++	dspcntr = DISPPLANE_GAMMA_ENABLE;
++
++	dspcntr |= DISPLAY_PLANE_ENABLE;
++
++	if (INTEL_INFO(dev)->gen < 4) {
++		if (intel_crtc->pipe == PIPE_B)
++			dspcntr |= DISPPLANE_SEL_PIPE_B;
++
++		/* pipesrc and dspsize control the size that is scaled from,
++		 * which should always be the user's requested size.
++		 */
++		I915_WRITE(DSPSIZE(plane),
++			   ((intel_crtc->config.pipe_src_h - 1) << 16) |
++			   (intel_crtc->config.pipe_src_w - 1));
++		I915_WRITE(DSPPOS(plane), 0);
++	} else if (IS_CHERRYVIEW(dev) && plane == PLANE_B) {
++		I915_WRITE(PRIMSIZE(plane),
++			   ((intel_crtc->config.pipe_src_h - 1) << 16) |
++			   (intel_crtc->config.pipe_src_w - 1));
++		I915_WRITE(PRIMPOS(plane), 0);
++		I915_WRITE(PRIMCNSTALPHA(plane), 0);
++	}
+ 
+-	reg = DSPCNTR(plane);
+-	dspcntr = I915_READ(reg);
+-	/* Mask out pixel format bits in case we change it */
+-	dspcntr &= ~DISPPLANE_PIXFORMAT_MASK;
+ 	switch (fb->pixel_format) {
+ 	case DRM_FORMAT_C8:
+ 		dspcntr |= DISPPLANE_8BPP;
+@@ -2463,30 +2513,40 @@
+ 		BUG();
+ 	}
+ 
+-	if (INTEL_INFO(dev)->gen >= 4) {
+-		if (obj->tiling_mode != I915_TILING_NONE)
+-			dspcntr |= DISPPLANE_TILED;
+-		else
+-			dspcntr &= ~DISPPLANE_TILED;
+-	}
++	if (INTEL_INFO(dev)->gen >= 4 &&
++	    obj->tiling_mode != I915_TILING_NONE)
++		dspcntr |= DISPPLANE_TILED;
+ 
+ 	if (IS_G4X(dev))
+ 		dspcntr |= DISPPLANE_TRICKLE_FEED_DISABLE;
+ 
+-	I915_WRITE(reg, dspcntr);
+-
+-	linear_offset = y * fb->pitches[0] + x * (fb->bits_per_pixel / 8);
++	linear_offset = y * fb->pitches[0] + x * pixel_size;
+ 
+ 	if (INTEL_INFO(dev)->gen >= 4) {
+ 		intel_crtc->dspaddr_offset =
+ 			intel_gen4_compute_page_offset(&x, &y, obj->tiling_mode,
+-						       fb->bits_per_pixel / 8,
++						       pixel_size,
+ 						       fb->pitches[0]);
+ 		linear_offset -= intel_crtc->dspaddr_offset;
+ 	} else {
+ 		intel_crtc->dspaddr_offset = linear_offset;
+ 	}
+ 
++	if (to_intel_plane(crtc->primary)->rotation == BIT(DRM_ROTATE_180)) {
++		dspcntr |= DISPPLANE_ROTATE_180;
++
++		x += (intel_crtc->config.pipe_src_w - 1);
++		y += (intel_crtc->config.pipe_src_h - 1);
++
++		/* Finding the last pixel of the last line of the display
++		data and adding to linear_offset*/
++		linear_offset +=
++			(intel_crtc->config.pipe_src_h - 1) * fb->pitches[0] +
++			(intel_crtc->config.pipe_src_w - 1) * pixel_size;
++	}
++
++	I915_WRITE(reg, dspcntr);
++
+ 	DRM_DEBUG_KMS("Writing base %08lX %08lX %d %d %d\n",
+ 		      i915_gem_obj_ggtt_offset(obj), linear_offset, x, y,
+ 		      fb->pitches[0]);
+@@ -2508,16 +2568,33 @@
+ 	struct drm_device *dev = crtc->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+-	struct drm_i915_gem_object *obj = intel_fb_obj(fb);
++	struct drm_i915_gem_object *obj;
+ 	int plane = intel_crtc->plane;
+ 	unsigned long linear_offset;
+ 	u32 dspcntr;
+-	u32 reg;
++	u32 reg = DSPCNTR(plane);
++	int pixel_size;
++
++	if (!intel_crtc->primary_enabled) {
++		I915_WRITE(reg, 0);
++		I915_WRITE(DSPSURF(plane), 0);
++		POSTING_READ(reg);
++		return;
++	}
++
++	obj = intel_fb_obj(fb);
++	if (WARN_ON(obj == NULL))
++		return;
++
++	pixel_size = drm_format_plane_cpp(fb->pixel_format, 0);
++
++	dspcntr = DISPPLANE_GAMMA_ENABLE;
++
++	dspcntr |= DISPLAY_PLANE_ENABLE;
++
++	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
++		dspcntr |= DISPPLANE_PIPE_CSC_ENABLE;
+ 
+-	reg = DSPCNTR(plane);
+-	dspcntr = I915_READ(reg);
+-	/* Mask out pixel format bits in case we change it */
+-	dspcntr &= ~DISPPLANE_PIXFORMAT_MASK;
+ 	switch (fb->pixel_format) {
+ 	case DRM_FORMAT_C8:
+ 		dspcntr |= DISPPLANE_8BPP;
+@@ -2547,22 +2624,32 @@
+ 
+ 	if (obj->tiling_mode != I915_TILING_NONE)
+ 		dspcntr |= DISPPLANE_TILED;
+-	else
+-		dspcntr &= ~DISPPLANE_TILED;
+ 
+-	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
+-		dspcntr &= ~DISPPLANE_TRICKLE_FEED_DISABLE;
+-	else
++	if (!IS_HASWELL(dev) && !IS_BROADWELL(dev))
+ 		dspcntr |= DISPPLANE_TRICKLE_FEED_DISABLE;
+ 
+-	I915_WRITE(reg, dspcntr);
+-
+-	linear_offset = y * fb->pitches[0] + x * (fb->bits_per_pixel / 8);
++	linear_offset = y * fb->pitches[0] + x * pixel_size;
+ 	intel_crtc->dspaddr_offset =
+ 		intel_gen4_compute_page_offset(&x, &y, obj->tiling_mode,
+-					       fb->bits_per_pixel / 8,
++					       pixel_size,
+ 					       fb->pitches[0]);
+ 	linear_offset -= intel_crtc->dspaddr_offset;
++	if (to_intel_plane(crtc->primary)->rotation == BIT(DRM_ROTATE_180)) {
++		dspcntr |= DISPPLANE_ROTATE_180;
++
++		if (!IS_HASWELL(dev) && !IS_BROADWELL(dev)) {
++			x += (intel_crtc->config.pipe_src_w - 1);
++			y += (intel_crtc->config.pipe_src_h - 1);
++
++			/* Finding the last pixel of the last line of the display
++			data and adding to linear_offset*/
++			linear_offset +=
++				(intel_crtc->config.pipe_src_h - 1) * fb->pitches[0] +
++				(intel_crtc->config.pipe_src_w - 1) * pixel_size;
++		}
++	}
++
++	I915_WRITE(reg, dspcntr);
+ 
+ 	DRM_DEBUG_KMS("Writing base %08lX %08lX %d %d %d\n",
+ 		      i915_gem_obj_ggtt_offset(obj), linear_offset, x, y,
+@@ -2579,34 +2666,119 @@
+ 	POSTING_READ(reg);
+ }
+ 
+-/* Assume fb object is pinned & idle & fenced and just update base pointers */
+-static int
+-intel_pipe_set_base_atomic(struct drm_crtc *crtc, struct drm_framebuffer *fb,
+-			   int x, int y, enum mode_set_atomic state)
++static void skylake_update_primary_plane(struct drm_crtc *crtc,
++					 struct drm_framebuffer *fb,
++					 int x, int y)
+ {
+ 	struct drm_device *dev = crtc->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	struct intel_framebuffer *intel_fb;
++	struct drm_i915_gem_object *obj;
++	int pipe = intel_crtc->pipe;
++	u32 plane_ctl, stride;
+ 
+-	if (dev_priv->display.disable_fbc)
+-		dev_priv->display.disable_fbc(dev);
+-	intel_increase_pllclock(dev, to_intel_crtc(crtc)->pipe);
++	if (!intel_crtc->primary_enabled) {
++		I915_WRITE(PLANE_CTL(pipe, 0), 0);
++		I915_WRITE(PLANE_SURF(pipe, 0), 0);
++		POSTING_READ(PLANE_CTL(pipe, 0));
++		return;
++	}
+ 
+-	dev_priv->display.update_primary_plane(crtc, fb, x, y);
++	plane_ctl = PLANE_CTL_ENABLE |
++		    PLANE_CTL_PIPE_GAMMA_ENABLE |
++		    PLANE_CTL_PIPE_CSC_ENABLE;
+ 
+-	return 0;
+-}
++	switch (fb->pixel_format) {
++	case DRM_FORMAT_RGB565:
++		plane_ctl |= PLANE_CTL_FORMAT_RGB_565;
++		break;
++	case DRM_FORMAT_XRGB8888:
++		plane_ctl |= PLANE_CTL_FORMAT_XRGB_8888;
++		break;
++	case DRM_FORMAT_XBGR8888:
++		plane_ctl |= PLANE_CTL_ORDER_RGBX;
++		plane_ctl |= PLANE_CTL_FORMAT_XRGB_8888;
++		break;
++	case DRM_FORMAT_XRGB2101010:
++		plane_ctl |= PLANE_CTL_FORMAT_XRGB_2101010;
++		break;
++	case DRM_FORMAT_XBGR2101010:
++		plane_ctl |= PLANE_CTL_ORDER_RGBX;
++		plane_ctl |= PLANE_CTL_FORMAT_XRGB_2101010;
++		break;
++	default:
++		BUG();
++	}
+ 
+-void intel_display_handle_reset(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct drm_crtc *crtc;
++	intel_fb = to_intel_framebuffer(fb);
++	obj = intel_fb->obj;
+ 
+ 	/*
+-	 * Flips in the rings have been nuked by the reset,
+-	 * so complete all pending flips so that user space
+-	 * will get its events and not get stuck.
+-	 *
+-	 * Also update the base address of all primary
++	 * The stride is either expressed as a multiple of 64 bytes chunks for
++	 * linear buffers or in number of tiles for tiled buffers.
++	 */
++	switch (obj->tiling_mode) {
++	case I915_TILING_NONE:
++		stride = fb->pitches[0] >> 6;
++		break;
++	case I915_TILING_X:
++		plane_ctl |= PLANE_CTL_TILED_X;
++		stride = fb->pitches[0] >> 9;
++		break;
++	default:
++		BUG();
++	}
++
++	plane_ctl |= PLANE_CTL_PLANE_GAMMA_DISABLE;
++	if (to_intel_plane(crtc->primary)->rotation == BIT(DRM_ROTATE_180))
++		plane_ctl |= PLANE_CTL_ROTATE_180;
++
++	I915_WRITE(PLANE_CTL(pipe, 0), plane_ctl);
++
++	DRM_DEBUG_KMS("Writing base %08lX %d,%d,%d,%d pitch=%d\n",
++		      i915_gem_obj_ggtt_offset(obj),
++		      x, y, fb->width, fb->height,
++		      fb->pitches[0]);
++
++	I915_WRITE(PLANE_POS(pipe, 0), 0);
++	I915_WRITE(PLANE_OFFSET(pipe, 0), (y << 16) | x);
++	I915_WRITE(PLANE_SIZE(pipe, 0),
++		   (intel_crtc->config.pipe_src_h - 1) << 16 |
++		   (intel_crtc->config.pipe_src_w - 1));
++	I915_WRITE(PLANE_STRIDE(pipe, 0), stride);
++	I915_WRITE(PLANE_SURF(pipe, 0), i915_gem_obj_ggtt_offset(obj));
++
++	POSTING_READ(PLANE_SURF(pipe, 0));
++}
++
++/* Assume fb object is pinned & idle & fenced and just update base pointers */
++static int
++intel_pipe_set_base_atomic(struct drm_crtc *crtc, struct drm_framebuffer *fb,
++			   int x, int y, enum mode_set_atomic state)
++{
++	struct drm_device *dev = crtc->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++
++	if (dev_priv->display.disable_fbc)
++		dev_priv->display.disable_fbc(dev);
++
++	dev_priv->display.update_primary_plane(crtc, fb, x, y);
++
++	return 0;
++}
++
++void intel_display_handle_reset(struct drm_device *dev)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_crtc *crtc;
++
++	/*
++	 * Flips in the rings have been nuked by the reset,
++	 * so complete all pending flips so that user space
++	 * will get its events and not get stuck.
++	 *
++	 * Also update the base address of all primary
+ 	 * planes to the the last fb to make sure we're
+ 	 * showing the correct fb after a reset.
+ 	 *
+@@ -2669,20 +2841,57 @@
+ 	struct drm_device *dev = crtc->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+-	unsigned long flags;
+ 	bool pending;
+ 
+-	if (i915_reset_in_progress(&dev_priv->gpu_error) ||
+-	    intel_crtc->reset_counter != atomic_read(&dev_priv->gpu_error.reset_counter))
++	if (intel_crtc->reset_counter != atomic_read(&dev_priv->gpu_error.reset_counter))
+ 		return false;
+ 
+-	spin_lock_irqsave(&dev->event_lock, flags);
++	spin_lock_irq(&dev->event_lock);
+ 	pending = to_intel_crtc(crtc)->unpin_work != NULL;
+-	spin_unlock_irqrestore(&dev->event_lock, flags);
++	spin_unlock_irq(&dev->event_lock);
+ 
+ 	return pending;
+ }
+ 
++static void intel_update_pipe_size(struct intel_crtc *crtc)
++{
++	struct drm_device *dev = crtc->base.dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	const struct drm_display_mode *adjusted_mode;
++
++	if (!i915_module.fastboot)
++		return;
++
++	/*
++	 * Update pipe size and adjust fitter if needed: the reason for this is
++	 * that in compute_mode_changes we check the native mode (not the pfit
++	 * mode) to see if we can flip rather than do a full mode set. In the
++	 * fastboot case, we'll flip, but if we don't update the pipesrc and
++	 * pfit state, we'll end up with a big fb scanned out into the wrong
++	 * sized surface.
++	 *
++	 * To fix this properly, we need to hoist the checks up into
++	 * compute_mode_changes (or above), check the actual pfit state and
++	 * whether the platform allows pfit disable with pipe active, and only
++	 * then update the pipesrc and pfit state, even on the flip path.
++	 */
++
++	adjusted_mode = &crtc->config.adjusted_mode;
++
++	I915_WRITE(PIPESRC(crtc->pipe),
++		   ((adjusted_mode->crtc_hdisplay - 1) << 16) |
++		   (adjusted_mode->crtc_vdisplay - 1));
++	if (!crtc->config.pch_pfit.enabled &&
++	    (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS) ||
++	     intel_pipe_has_type(crtc, INTEL_OUTPUT_EDP))) {
++		I915_WRITE(PF_CTL(crtc->pipe), 0);
++		I915_WRITE(PF_WIN_POS(crtc->pipe), 0);
++		I915_WRITE(PF_WIN_SZ(crtc->pipe), 0);
++	}
++	crtc->config.pipe_src_w = adjusted_mode->crtc_hdisplay;
++	crtc->config.pipe_src_h = adjusted_mode->crtc_vdisplay;
++}
++
+ static int
+ intel_pipe_set_base(struct drm_crtc *crtc, int x, int y,
+ 		    struct drm_framebuffer *fb)
+@@ -2692,7 +2901,6 @@
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+ 	enum pipe pipe = intel_crtc->pipe;
+ 	struct drm_framebuffer *old_fb = crtc->primary->fb;
+-	struct drm_i915_gem_object *obj = intel_fb_obj(fb);
+ 	struct drm_i915_gem_object *old_obj = intel_fb_obj(old_fb);
+ 	int ret;
+ 
+@@ -2715,9 +2923,9 @@
+ 	}
+ 
+ 	mutex_lock(&dev->struct_mutex);
+-	ret = intel_pin_and_fence_fb_obj(dev, obj, NULL);
++	ret = intel_pin_and_fence_fb_obj(crtc->primary, fb, NULL);
+ 	if (ret == 0)
+-		i915_gem_track_fb(old_obj, obj,
++		i915_gem_track_fb(old_obj, intel_fb_obj(fb),
+ 				  INTEL_FRONTBUFFER_PRIMARY(pipe));
+ 	mutex_unlock(&dev->struct_mutex);
+ 	if (ret != 0) {
+@@ -2725,37 +2933,6 @@
+ 		return ret;
+ 	}
+ 
+-	/*
+-	 * Update pipe size and adjust fitter if needed: the reason for this is
+-	 * that in compute_mode_changes we check the native mode (not the pfit
+-	 * mode) to see if we can flip rather than do a full mode set. In the
+-	 * fastboot case, we'll flip, but if we don't update the pipesrc and
+-	 * pfit state, we'll end up with a big fb scanned out into the wrong
+-	 * sized surface.
+-	 *
+-	 * To fix this properly, we need to hoist the checks up into
+-	 * compute_mode_changes (or above), check the actual pfit state and
+-	 * whether the platform allows pfit disable with pipe active, and only
+-	 * then update the pipesrc and pfit state, even on the flip path.
+-	 */
+-	if (i915.fastboot) {
+-		const struct drm_display_mode *adjusted_mode =
+-			&intel_crtc->config.adjusted_mode;
+-
+-		I915_WRITE(PIPESRC(intel_crtc->pipe),
+-			   ((adjusted_mode->crtc_hdisplay - 1) << 16) |
+-			   (adjusted_mode->crtc_vdisplay - 1));
+-		if (!intel_crtc->config.pch_pfit.enabled &&
+-		    (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS) ||
+-		     intel_pipe_has_type(crtc, INTEL_OUTPUT_EDP))) {
+-			I915_WRITE(PF_CTL(intel_crtc->pipe), 0);
+-			I915_WRITE(PF_WIN_POS(intel_crtc->pipe), 0);
+-			I915_WRITE(PF_WIN_SZ(intel_crtc->pipe), 0);
+-		}
+-		intel_crtc->config.pipe_src_w = adjusted_mode->crtc_hdisplay;
+-		intel_crtc->config.pipe_src_h = adjusted_mode->crtc_vdisplay;
+-	}
+-
+ 	dev_priv->display.update_primary_plane(crtc, fb, x, y);
+ 
+ 	if (intel_crtc->active)
+@@ -3346,23 +3523,56 @@
+ 	return false;
+ }
+ 
++static void page_flip_completed(struct intel_crtc *intel_crtc)
++{
++	struct drm_i915_private *dev_priv = to_i915(intel_crtc->base.dev);
++	struct intel_unpin_work *work = intel_crtc->unpin_work;
++
++	/* ensure that the unpin work is consistent wrt ->pending. */
++	smp_rmb();
++	intel_crtc->unpin_work = NULL;
++
++	if (work->event)
++		drm_send_vblank_event(intel_crtc->base.dev,
++				      intel_crtc->pipe,
++				      work->event);
++
++	drm_crtc_vblank_put(&intel_crtc->base);
++
++	wake_up_all(&dev_priv->pending_flip_queue);
++	queue_work(dev_priv->wq, &work->work);
++
++	trace_i915_flip_complete(intel_crtc->plane,
++				 work->pending_flip_obj);
++}
++
+ void intel_crtc_wait_for_pending_flips(struct drm_crtc *crtc)
+ {
+ 	struct drm_device *dev = crtc->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	if (crtc->primary->fb == NULL)
+-		return;
++	/* Kick hangcheck before waiting upon GPU results */
++	i915_queue_hangcheck(dev);
+ 
+ 	WARN_ON(waitqueue_active(&dev_priv->pending_flip_queue));
++	if (WARN_ON(wait_event_timeout(dev_priv->pending_flip_queue,
++				       !intel_crtc_has_pending_flip(crtc),
++				       60*HZ) == 0)) {
++		struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+ 
+-	WARN_ON(wait_event_timeout(dev_priv->pending_flip_queue,
+-				   !intel_crtc_has_pending_flip(crtc),
+-				   60*HZ) == 0);
++		spin_lock_irq(&dev->event_lock);
++		if (intel_crtc->unpin_work) {
++			WARN_ONCE(1, "Removing stuck page flip\n");
++			page_flip_completed(intel_crtc);
++		}
++		spin_unlock_irq(&dev->event_lock);
++	}
+ 
+-	mutex_lock(&dev->struct_mutex);
+-	intel_finish_fb(crtc->primary->fb);
+-	mutex_unlock(&dev->struct_mutex);
++	if (crtc->primary->fb) {
++		mutex_lock(&dev->struct_mutex);
++		intel_finish_fb(crtc->primary->fb);
++		mutex_unlock(&dev->struct_mutex);
++	}
+ }
+ 
+ /* Program iCLKIP clock to the desired frequency */
+@@ -3580,9 +3790,7 @@
+ 	intel_fdi_normal_train(crtc);
+ 
+ 	/* For PCH DP, enable TRANS_DP_CTL */
+-	if (HAS_PCH_CPT(dev) &&
+-	    (intel_pipe_has_type(crtc, INTEL_OUTPUT_DISPLAYPORT) ||
+-	     intel_pipe_has_type(crtc, INTEL_OUTPUT_EDP))) {
++	if (HAS_PCH_CPT(dev) && intel_crtc->config.has_dp_encoder) {
+ 		u32 bpc = (I915_READ(PIPECONF(pipe)) & PIPECONF_BPC_MASK) >> 5;
+ 		reg = TRANS_DP_CTL(pipe);
+ 		temp = I915_READ(reg);
+@@ -3642,12 +3850,13 @@
+ 	if (pll == NULL)
+ 		return;
+ 
+-	if (pll->refcount == 0) {
+-		WARN(1, "bad %s refcount\n", pll->name);
++	if (!(pll->config.crtc_mask & (1 << crtc->pipe))) {
++		WARN(1, "bad %s crtc mask\n", pll->name);
+ 		return;
+ 	}
+ 
+-	if (--pll->refcount == 0) {
++	pll->config.crtc_mask &= ~(1 << crtc->pipe);
++	if (pll->config.crtc_mask == 0) {
+ 		WARN_ON(pll->on);
+ 		WARN_ON(pll->active);
+ 	}
+@@ -3658,15 +3867,9 @@
+ struct intel_shared_dpll *intel_get_shared_dpll(struct intel_crtc *crtc)
+ {
+ 	struct drm_i915_private *dev_priv = crtc->base.dev->dev_private;
+-	struct intel_shared_dpll *pll = intel_crtc_to_shared_dpll(crtc);
++	struct intel_shared_dpll *pll;
+ 	enum intel_dpll_id i;
+ 
+-	if (pll) {
+-		DRM_DEBUG_KMS("CRTC:%d dropping existing %s\n",
+-			      crtc->base.base.id, pll->name);
+-		intel_put_shared_dpll(crtc);
+-	}
+-
+ 	if (HAS_PCH_IBX(dev_priv->dev)) {
+ 		/* Ironlake PCH has a fixed PLL->PCH pipe mapping. */
+ 		i = (enum intel_dpll_id) crtc->pipe;
+@@ -3675,7 +3878,7 @@
+ 		DRM_DEBUG_KMS("CRTC:%d using pre-allocated %s\n",
+ 			      crtc->base.base.id, pll->name);
+ 
+-		WARN_ON(pll->refcount);
++		WARN_ON(pll->new_config->crtc_mask);
+ 
+ 		goto found;
+ 	}
+@@ -3684,15 +3887,16 @@
+ 		pll = &dev_priv->shared_dplls[i];
+ 
+ 		/* Only want to check enabled timings first */
+-		if (pll->refcount == 0)
++		if (pll->new_config->crtc_mask == 0)
+ 			continue;
+ 
+-		if (memcmp(&crtc->config.dpll_hw_state, &pll->hw_state,
+-			   sizeof(pll->hw_state)) == 0) {
+-			DRM_DEBUG_KMS("CRTC:%d sharing existing %s (refcount %d, ative %d)\n",
+-				      crtc->base.base.id,
+-				      pll->name, pll->refcount, pll->active);
+-
++		if (memcmp(&crtc->new_config->dpll_hw_state,
++			   &pll->new_config->hw_state,
++			   sizeof(pll->new_config->hw_state)) == 0) {
++			DRM_DEBUG_KMS("CRTC:%d sharing existing %s (crtc mask 0x%08x, ative %d)\n",
++				      crtc->base.base.id, pll->name,
++				      pll->new_config->crtc_mask,
++				      pll->active);
+ 			goto found;
+ 		}
+ 	}
+@@ -3700,7 +3904,7 @@
+ 	/* Ok no matching timings, maybe there's a free one? */
+ 	for (i = 0; i < dev_priv->num_shared_dpll; i++) {
+ 		pll = &dev_priv->shared_dplls[i];
+-		if (pll->refcount == 0) {
++		if (pll->new_config->crtc_mask == 0) {
+ 			DRM_DEBUG_KMS("CRTC:%d allocated %s\n",
+ 				      crtc->base.base.id, pll->name);
+ 			goto found;
+@@ -3710,18 +3914,86 @@
+ 	return NULL;
+ 
+ found:
+-	if (pll->refcount == 0)
+-		pll->hw_state = crtc->config.dpll_hw_state;
++	if (pll->new_config->crtc_mask == 0)
++		pll->new_config->hw_state = crtc->new_config->dpll_hw_state;
+ 
+-	crtc->config.shared_dpll = i;
++	crtc->new_config->shared_dpll = i;
+ 	DRM_DEBUG_DRIVER("using %s for pipe %c\n", pll->name,
+ 			 pipe_name(crtc->pipe));
+ 
+-	pll->refcount++;
++	pll->new_config->crtc_mask |= 1 << crtc->pipe;
+ 
+ 	return pll;
+ }
+ 
++/**
++ * intel_shared_dpll_start_config - start a new PLL staged config
++ * @dev_priv: DRM device
++ * @clear_pipes: mask of pipes that will have their PLLs freed
++ *
++ * Starts a new PLL staged config, copying the current config but
++ * releasing the references of pipes specified in clear_pipes.
++ */
++static int intel_shared_dpll_start_config(struct drm_i915_private *dev_priv,
++					  unsigned clear_pipes)
++{
++	struct intel_shared_dpll *pll;
++	enum intel_dpll_id i;
++
++	for (i = 0; i < dev_priv->num_shared_dpll; i++) {
++		pll = &dev_priv->shared_dplls[i];
++
++		pll->new_config = kmemdup(&pll->config, sizeof pll->config,
++					  GFP_KERNEL);
++		if (!pll->new_config)
++			goto cleanup;
++
++		pll->new_config->crtc_mask &= ~clear_pipes;
++	}
++
++	return 0;
++
++cleanup:
++	while (--i >= 0) {
++		pll = &dev_priv->shared_dplls[i];
++		kfree(pll->new_config);
++		pll->new_config = NULL;
++	}
++
++	return -ENOMEM;
++}
++
++static void intel_shared_dpll_commit(struct drm_i915_private *dev_priv)
++{
++	struct intel_shared_dpll *pll;
++	enum intel_dpll_id i;
++
++	for (i = 0; i < dev_priv->num_shared_dpll; i++) {
++		pll = &dev_priv->shared_dplls[i];
++
++		WARN_ON(pll->new_config == &pll->config);
++
++		pll->config = *pll->new_config;
++		kfree(pll->new_config);
++		pll->new_config = NULL;
++	}
++}
++
++static void intel_shared_dpll_abort_config(struct drm_i915_private *dev_priv)
++{
++	struct intel_shared_dpll *pll;
++	enum intel_dpll_id i;
++
++	for (i = 0; i < dev_priv->num_shared_dpll; i++) {
++		pll = &dev_priv->shared_dplls[i];
++
++		WARN_ON(pll->new_config == &pll->config);
++
++		kfree(pll->new_config);
++		pll->new_config = NULL;
++	}
++}
++
+ static void cpt_verify_modeset(struct drm_device *dev, int pipe)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+@@ -3736,6 +4008,19 @@
+ 	}
+ }
+ 
++static void skylake_pfit_enable(struct intel_crtc *crtc)
++{
++	struct drm_device *dev = crtc->base.dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	int pipe = crtc->pipe;
++
++	if (crtc->config.pch_pfit.enabled) {
++		I915_WRITE(PS_CTL(pipe), PS_ENABLE);
++		I915_WRITE(PS_WIN_POS(pipe), crtc->config.pch_pfit.pos);
++		I915_WRITE(PS_WIN_SZ(pipe), crtc->config.pch_pfit.size);
++	}
++}
++
+ static void ironlake_pfit_enable(struct intel_crtc *crtc)
+ {
+ 	struct drm_device *dev = crtc->base.dev;
+@@ -3859,7 +4144,7 @@
+ 		return;
+ 
+ 	if (!HAS_PCH_SPLIT(dev_priv->dev)) {
+-		if (intel_pipe_has_type(crtc, INTEL_OUTPUT_DSI))
++		if (intel_pipe_has_type(intel_crtc, INTEL_OUTPUT_DSI))
+ 			assert_dsi_pll_enabled(dev_priv);
+ 		else
+ 			assert_pll_enabled(dev_priv, pipe);
+@@ -3911,14 +4196,10 @@
+ static void intel_crtc_enable_planes(struct drm_crtc *crtc)
+ {
+ 	struct drm_device *dev = crtc->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+ 	int pipe = intel_crtc->pipe;
+-	int plane = intel_crtc->plane;
+ 
+-	drm_vblank_on(dev, pipe);
+-
+-	intel_enable_primary_hw_plane(dev_priv, plane, pipe);
++	intel_enable_primary_hw_plane(crtc->primary, crtc);
+ 	intel_enable_planes(crtc);
+ 	intel_crtc_update_cursor(crtc, true);
+ 	intel_crtc_dpms_overlay(intel_crtc, true);
+@@ -3955,7 +4236,7 @@
+ 	intel_crtc_dpms_overlay(intel_crtc, false);
+ 	intel_crtc_update_cursor(crtc, false);
+ 	intel_disable_planes(crtc);
+-	intel_disable_primary_hw_plane(dev_priv, plane, pipe);
++	intel_disable_primary_hw_plane(crtc->primary, crtc);
+ 
+ 	/*
+ 	 * FIXME: Once we grow proper nuclear flip support out of this we need
+@@ -3963,8 +4244,6 @@
+ 	 * consider this a flip to a NULL plane.
+ 	 */
+ 	intel_frontbuffer_flip(dev, INTEL_FRONTBUFFER_ALL_MASK(pipe));
+-
+-	drm_vblank_off(dev, pipe);
+ }
+ 
+ static void ironlake_crtc_enable(struct drm_crtc *crtc)
+@@ -3974,7 +4253,6 @@
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+ 	struct intel_encoder *encoder;
+ 	int pipe = intel_crtc->pipe;
+-	enum plane plane = intel_crtc->plane;
+ 
+ 	WARN_ON(!crtc->enabled);
+ 
+@@ -3991,22 +4269,15 @@
+ 
+ 	if (intel_crtc->config.has_pch_encoder) {
+ 		intel_cpu_transcoder_set_m_n(intel_crtc,
+-					     &intel_crtc->config.fdi_m_n);
++				     &intel_crtc->config.fdi_m_n, NULL);
+ 	}
+ 
+ 	ironlake_set_pipeconf(crtc);
+ 
+-	/* Set up the display plane register */
+-	I915_WRITE(DSPCNTR(plane), DISPPLANE_GAMMA_ENABLE);
+-	POSTING_READ(DSPCNTR(plane));
+-
+-	dev_priv->display.update_primary_plane(crtc, crtc->primary->fb,
+-					       crtc->x, crtc->y);
+-
+ 	intel_crtc->active = true;
+ 
+-	intel_set_cpu_fifo_underrun_reporting(dev, pipe, true);
+-	intel_set_pch_fifo_underrun_reporting(dev, pipe, true);
++	intel_set_cpu_fifo_underrun_reporting(dev_priv, pipe, true);
++	intel_set_pch_fifo_underrun_reporting(dev_priv, pipe, true);
+ 
+ 	for_each_encoder_on_crtc(dev, crtc, encoder)
+ 		if (encoder->pre_enable)
+@@ -4042,6 +4313,9 @@
+ 	if (HAS_PCH_CPT(dev))
+ 		cpt_verify_modeset(dev, intel_crtc->pipe);
+ 
++	assert_vblank_disabled(crtc);
++	drm_crtc_vblank_on(crtc);
++
+ 	intel_crtc_enable_planes(crtc);
+ }
+ 
+@@ -4087,7 +4361,6 @@
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+ 	struct intel_encoder *encoder;
+ 	int pipe = intel_crtc->pipe;
+-	enum plane plane = intel_crtc->plane;
+ 
+ 	WARN_ON(!crtc->enabled);
+ 
+@@ -4102,37 +4375,39 @@
+ 
+ 	intel_set_pipe_timings(intel_crtc);
+ 
++	if (intel_crtc->config.cpu_transcoder != TRANSCODER_EDP) {
++		I915_WRITE(PIPE_MULT(intel_crtc->config.cpu_transcoder),
++			   intel_crtc->config.pixel_multiplier - 1);
++	}
++
+ 	if (intel_crtc->config.has_pch_encoder) {
+ 		intel_cpu_transcoder_set_m_n(intel_crtc,
+-					     &intel_crtc->config.fdi_m_n);
++				     &intel_crtc->config.fdi_m_n, NULL);
+ 	}
+ 
+ 	haswell_set_pipeconf(crtc);
+ 
+ 	intel_set_pipe_csc(crtc);
+ 
+-	/* Set up the display plane register */
+-	I915_WRITE(DSPCNTR(plane), DISPPLANE_GAMMA_ENABLE | DISPPLANE_PIPE_CSC_ENABLE);
+-	POSTING_READ(DSPCNTR(plane));
+-
+-	dev_priv->display.update_primary_plane(crtc, crtc->primary->fb,
+-					       crtc->x, crtc->y);
+-
+ 	intel_crtc->active = true;
+ 
+-	intel_set_cpu_fifo_underrun_reporting(dev, pipe, true);
++	intel_set_cpu_fifo_underrun_reporting(dev_priv, pipe, true);
+ 	for_each_encoder_on_crtc(dev, crtc, encoder)
+ 		if (encoder->pre_enable)
+ 			encoder->pre_enable(encoder);
+ 
+ 	if (intel_crtc->config.has_pch_encoder) {
+-		intel_set_pch_fifo_underrun_reporting(dev, TRANSCODER_A, true);
++		intel_set_pch_fifo_underrun_reporting(dev_priv, TRANSCODER_A,
++						      true);
+ 		dev_priv->display.fdi_link_train(crtc);
+ 	}
+ 
+ 	intel_ddi_enable_pipe_clock(intel_crtc);
+ 
+-	ironlake_pfit_enable(intel_crtc);
++	if (IS_SKYLAKE(dev))
++		skylake_pfit_enable(intel_crtc);
++	else
++		ironlake_pfit_enable(intel_crtc);
+ 
+ 	/*
+ 	 * On ILK+ LUT must be loaded before the pipe is running but with
+@@ -4157,12 +4432,30 @@
+ 		intel_opregion_notify_encoder(encoder, true);
+ 	}
+ 
++	assert_vblank_disabled(crtc);
++	drm_crtc_vblank_on(crtc);
++
+ 	/* If we change the relative order between pipe/planes enabling, we need
+ 	 * to change the workaround. */
+ 	haswell_mode_set_planes_workaround(intel_crtc);
+ 	intel_crtc_enable_planes(crtc);
+ }
+ 
++static void skylake_pfit_disable(struct intel_crtc *crtc)
++{
++	struct drm_device *dev = crtc->base.dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	int pipe = crtc->pipe;
++
++	/* To avoid upsetting the power well on haswell only disable the pfit if
++	 * it's in use. The hw state code will make sure we get this right. */
++	if (crtc->config.pch_pfit.enabled) {
++		I915_WRITE(PS_CTL(pipe), 0);
++		I915_WRITE(PS_WIN_POS(pipe), 0);
++		I915_WRITE(PS_WIN_SZ(pipe), 0);
++	}
++}
++
+ static void ironlake_pfit_disable(struct intel_crtc *crtc)
+ {
+ 	struct drm_device *dev = crtc->base.dev;
+@@ -4192,13 +4485,17 @@
+ 
+ 	intel_crtc_disable_planes(crtc);
+ 
++	drm_crtc_vblank_off(crtc);
++	assert_vblank_disabled(crtc);
++
+ 	for_each_encoder_on_crtc(dev, crtc, encoder)
+ 		encoder->disable(encoder);
+ 
+ 	if (intel_crtc->config.has_pch_encoder)
+-		intel_set_pch_fifo_underrun_reporting(dev, pipe, false);
++		intel_set_pch_fifo_underrun_reporting(dev_priv, pipe, false);
++
++	intel_disable_pipe(intel_crtc);
+ 
+-	intel_disable_pipe(dev_priv, pipe);
+ 	ironlake_pfit_disable(intel_crtc);
+ 
+ 	for_each_encoder_on_crtc(dev, crtc, encoder)
+@@ -4209,7 +4506,7 @@
+ 		ironlake_fdi_disable(crtc);
+ 
+ 		ironlake_disable_pch_transcoder(dev_priv, pipe);
+-		intel_set_pch_fifo_underrun_reporting(dev, pipe, true);
++		intel_set_pch_fifo_underrun_reporting(dev_priv, pipe, true);
+ 
+ 		if (HAS_PCH_CPT(dev)) {
+ 			/* disable TRANS_DP_CTL */
+@@ -4246,7 +4543,6 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+ 	struct intel_encoder *encoder;
+-	int pipe = intel_crtc->pipe;
+ 	enum transcoder cpu_transcoder = intel_crtc->config.cpu_transcoder;
+ 
+ 	if (!intel_crtc->active)
+@@ -4254,27 +4550,35 @@
+ 
+ 	intel_crtc_disable_planes(crtc);
+ 
++	drm_crtc_vblank_off(crtc);
++	assert_vblank_disabled(crtc);
++
+ 	for_each_encoder_on_crtc(dev, crtc, encoder) {
+ 		intel_opregion_notify_encoder(encoder, false);
+ 		encoder->disable(encoder);
+ 	}
+ 
+ 	if (intel_crtc->config.has_pch_encoder)
+-		intel_set_pch_fifo_underrun_reporting(dev, TRANSCODER_A, false);
+-	intel_disable_pipe(dev_priv, pipe);
++		intel_set_pch_fifo_underrun_reporting(dev_priv, TRANSCODER_A,
++						      false);
++	intel_disable_pipe(intel_crtc);
+ 
+ 	if (intel_crtc->config.dp_encoder_is_mst)
+ 		intel_ddi_set_vc_payload_alloc(crtc, false);
+ 
+ 	intel_ddi_disable_transcoder_func(dev_priv, cpu_transcoder);
+ 
+-	ironlake_pfit_disable(intel_crtc);
++	if (IS_SKYLAKE(dev))
++		skylake_pfit_disable(intel_crtc);
++	else
++		ironlake_pfit_disable(intel_crtc);
+ 
+ 	intel_ddi_disable_pipe_clock(intel_crtc);
+ 
+ 	if (intel_crtc->config.has_pch_encoder) {
+ 		lpt_disable_pch_transcoder(dev_priv);
+-		intel_set_pch_fifo_underrun_reporting(dev, TRANSCODER_A, true);
++		intel_set_pch_fifo_underrun_reporting(dev_priv, TRANSCODER_A,
++						      true);
+ 		intel_ddi_fdi_disable(crtc);
+ 	}
+ 
+@@ -4395,20 +4699,6 @@
+ 	return mask;
+ }
+ 
+-void intel_display_set_init_power(struct drm_i915_private *dev_priv,
+-				  bool enable)
+-{
+-	if (dev_priv->power_domains.init_power_on == enable)
+-		return;
+-
+-	if (enable)
+-		intel_display_power_get(dev_priv, POWER_DOMAIN_INIT);
+-	else
+-		intel_display_power_put(dev_priv, POWER_DOMAIN_INIT);
+-
+-	dev_priv->power_domains.init_power_on = enable;
+-}
+-
+ static void modeset_update_crtc_power_domains(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+@@ -4431,6 +4721,9 @@
+ 			intel_display_power_get(dev_priv, domain);
+ 	}
+ 
++	if (dev_priv->display.modeset_global_resources)
++		dev_priv->display.modeset_global_resources(dev);
++
+ 	for_each_intel_crtc(dev, crtc) {
+ 		enum intel_display_power_domain domain;
+ 
+@@ -4462,7 +4755,7 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+ 	dev_priv->vlv_cdclk_freq = dev_priv->display.get_display_clock_speed(dev);
+-	DRM_DEBUG_DRIVER("Current CD clock rate: %d kHz",
++	DRM_DEBUG_DRIVER("Current CD clock rate: %d kHz\n",
+ 			 dev_priv->vlv_cdclk_freq);
+ 
+ 	/*
+@@ -4470,7 +4763,7 @@
+ 	 * BSpec erroneously claims we should aim for 4MHz, but
+ 	 * in fact 1MHz is the correct frequency.
+ 	 */
+-	I915_WRITE(GMBUSFREQ_VLV, dev_priv->vlv_cdclk_freq);
++	I915_WRITE(GMBUSFREQ_VLV, DIV_ROUND_UP(dev_priv->vlv_cdclk_freq, 1000));
+ }
+ 
+ /* Adjust CDclk dividers to allow high res or save power if possible */
+@@ -4501,10 +4794,9 @@
+ 	mutex_unlock(&dev_priv->rps.hw_lock);
+ 
+ 	if (cdclk == 400000) {
+-		u32 divider, vco;
++		u32 divider;
+ 
+-		vco = valleyview_get_vco(dev_priv);
+-		divider = DIV_ROUND_CLOSEST(vco << 1, cdclk) - 1;
++		divider = DIV_ROUND_CLOSEST(dev_priv->hpll_freq << 1, cdclk) - 1;
+ 
+ 		mutex_lock(&dev_priv->dpio_lock);
+ 		/* adjust cdclk divider */
+@@ -4539,11 +4831,55 @@
+ 	vlv_update_cdclk(dev);
+ }
+ 
++static void cherryview_set_cdclk(struct drm_device *dev, int cdclk)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	u32 val, cmd;
++
++	WARN_ON(dev_priv->display.get_display_clock_speed(dev) != dev_priv->vlv_cdclk_freq);
++
++	switch (cdclk) {
++	case 400000:
++		cmd = 3;
++		break;
++	case 333333:
++	case 320000:
++		cmd = 2;
++		break;
++	case 266667:
++		cmd = 1;
++		break;
++	case 200000:
++		cmd = 0;
++		break;
++	default:
++		WARN_ON(1);
++		return;
++	}
++
++	mutex_lock(&dev_priv->rps.hw_lock);
++	val = vlv_punit_read(dev_priv, PUNIT_REG_DSPFREQ);
++	val &= ~DSPFREQGUAR_MASK_CHV;
++	val |= (cmd << DSPFREQGUAR_SHIFT_CHV);
++	vlv_punit_write(dev_priv, PUNIT_REG_DSPFREQ, val);
++	if (wait_for((vlv_punit_read(dev_priv, PUNIT_REG_DSPFREQ) &
++		      DSPFREQSTAT_MASK_CHV) == (cmd << DSPFREQSTAT_SHIFT_CHV),
++		     50)) {
++		DRM_ERROR("timed out waiting for CDclk change\n");
++	}
++	mutex_unlock(&dev_priv->rps.hw_lock);
++
++	vlv_update_cdclk(dev);
++}
++
+ static int valleyview_calc_cdclk(struct drm_i915_private *dev_priv,
+ 				 int max_pixclk)
+ {
+-	int vco = valleyview_get_vco(dev_priv);
+-	int freq_320 = (vco <<  1) % 320000 != 0 ? 333333 : 320000;
++	int freq_320 = (dev_priv->hpll_freq <<  1) % 320000 != 0 ? 333333 : 320000;
++
++	/* FIXME: Punit isn't quite ready yet */
++	if (IS_CHERRYVIEW(dev_priv->dev))
++		return 400000;
+ 
+ 	/*
+ 	 * Really only a few cases to deal with, as only 4 CDclks are supported:
+@@ -4607,59 +4943,54 @@
+ 	int max_pixclk = intel_mode_max_pixclk(dev_priv);
+ 	int req_cdclk = valleyview_calc_cdclk(dev_priv, max_pixclk);
+ 
+-	if (req_cdclk != dev_priv->vlv_cdclk_freq)
+-		valleyview_set_cdclk(dev, req_cdclk);
+-	modeset_update_crtc_power_domains(dev);
++	if (req_cdclk != dev_priv->vlv_cdclk_freq) {
++		if (IS_CHERRYVIEW(dev))
++			cherryview_set_cdclk(dev, req_cdclk);
++		else
++			valleyview_set_cdclk(dev, req_cdclk);
++	}
+ }
+ 
+ static void valleyview_crtc_enable(struct drm_crtc *crtc)
+ {
+ 	struct drm_device *dev = crtc->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_i915_private *dev_priv = to_i915(dev);
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+ 	struct intel_encoder *encoder;
+ 	int pipe = intel_crtc->pipe;
+-	int plane = intel_crtc->plane;
+ 	bool is_dsi;
+-	u32 dspcntr;
+ 
+ 	WARN_ON(!crtc->enabled);
+ 
+ 	if (intel_crtc->active)
+ 		return;
+ 
+-	is_dsi = intel_pipe_has_type(crtc, INTEL_OUTPUT_DSI);
+-
+-	if (!is_dsi && !IS_CHERRYVIEW(dev))
+-		vlv_prepare_pll(intel_crtc);
+-
+-	/* Set up the display plane register */
+-	dspcntr = DISPPLANE_GAMMA_ENABLE;
++	is_dsi = intel_pipe_has_type(intel_crtc, INTEL_OUTPUT_DSI);
++
++	if (!is_dsi) {
++		if (IS_CHERRYVIEW(dev))
++			chv_prepare_pll(intel_crtc, &intel_crtc->config);
++		else
++			vlv_prepare_pll(intel_crtc, &intel_crtc->config);
++	}
+ 
+ 	if (intel_crtc->config.has_dp_encoder)
+ 		intel_dp_set_m_n(intel_crtc);
+ 
+ 	intel_set_pipe_timings(intel_crtc);
+ 
+-	/* pipesrc and dspsize control the size that is scaled from,
+-	 * which should always be the user's requested size.
+-	 */
+-	I915_WRITE(DSPSIZE(plane),
+-		   ((intel_crtc->config.pipe_src_h - 1) << 16) |
+-		   (intel_crtc->config.pipe_src_w - 1));
+-	I915_WRITE(DSPPOS(plane), 0);
+-
+-	i9xx_set_pipeconf(intel_crtc);
++	if (IS_CHERRYVIEW(dev) && pipe == PIPE_B) {
++		struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	I915_WRITE(DSPCNTR(plane), dspcntr);
+-	POSTING_READ(DSPCNTR(plane));
++		I915_WRITE(CHV_BLEND(pipe), CHV_BLEND_LEGACY);
++		I915_WRITE(CHV_CANVAS(pipe), 0);
++	}
+ 
+-	dev_priv->display.update_primary_plane(crtc, crtc->primary->fb,
+-					       crtc->x, crtc->y);
++	i9xx_set_pipeconf(intel_crtc);
+ 
+ 	intel_crtc->active = true;
+ 
+-	intel_set_cpu_fifo_underrun_reporting(dev, pipe, true);
++	intel_set_cpu_fifo_underrun_reporting(dev_priv, pipe, true);
+ 
+ 	for_each_encoder_on_crtc(dev, crtc, encoder)
+ 		if (encoder->pre_pll_enable)
+@@ -4667,9 +4998,9 @@
+ 
+ 	if (!is_dsi) {
+ 		if (IS_CHERRYVIEW(dev))
+-			chv_enable_pll(intel_crtc);
++			chv_enable_pll(intel_crtc, &intel_crtc->config);
+ 		else
+-			vlv_enable_pll(intel_crtc);
++			vlv_enable_pll(intel_crtc, &intel_crtc->config);
+ 	}
+ 
+ 	for_each_encoder_on_crtc(dev, crtc, encoder)
+@@ -4686,10 +5017,13 @@
+ 	for_each_encoder_on_crtc(dev, crtc, encoder)
+ 		encoder->enable(encoder);
+ 
++	assert_vblank_disabled(crtc);
++	drm_crtc_vblank_on(crtc);
++
+ 	intel_crtc_enable_planes(crtc);
+ 
+ 	/* Underruns don't raise interrupts, so check manually. */
+-	i9xx_check_fifo_underruns(dev);
++	i9xx_check_fifo_underruns(dev_priv);
+ }
+ 
+ static void i9xx_set_pll_dividers(struct intel_crtc *crtc)
+@@ -4704,12 +5038,10 @@
+ static void i9xx_crtc_enable(struct drm_crtc *crtc)
+ {
+ 	struct drm_device *dev = crtc->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_i915_private *dev_priv = to_i915(dev);
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+ 	struct intel_encoder *encoder;
+ 	int pipe = intel_crtc->pipe;
+-	int plane = intel_crtc->plane;
+-	u32 dspcntr;
+ 
+ 	WARN_ON(!crtc->enabled);
+ 
+@@ -4718,39 +5050,17 @@
+ 
+ 	i9xx_set_pll_dividers(intel_crtc);
+ 
+-	/* Set up the display plane register */
+-	dspcntr = DISPPLANE_GAMMA_ENABLE;
+-
+-	if (pipe == 0)
+-		dspcntr &= ~DISPPLANE_SEL_PIPE_MASK;
+-	else
+-		dspcntr |= DISPPLANE_SEL_PIPE_B;
+-
+ 	if (intel_crtc->config.has_dp_encoder)
+ 		intel_dp_set_m_n(intel_crtc);
+ 
+ 	intel_set_pipe_timings(intel_crtc);
+ 
+-	/* pipesrc and dspsize control the size that is scaled from,
+-	 * which should always be the user's requested size.
+-	 */
+-	I915_WRITE(DSPSIZE(plane),
+-		   ((intel_crtc->config.pipe_src_h - 1) << 16) |
+-		   (intel_crtc->config.pipe_src_w - 1));
+-	I915_WRITE(DSPPOS(plane), 0);
+-
+ 	i9xx_set_pipeconf(intel_crtc);
+ 
+-	I915_WRITE(DSPCNTR(plane), dspcntr);
+-	POSTING_READ(DSPCNTR(plane));
+-
+-	dev_priv->display.update_primary_plane(crtc, crtc->primary->fb,
+-					       crtc->x, crtc->y);
+-
+ 	intel_crtc->active = true;
+ 
+ 	if (!IS_GEN2(dev))
+-		intel_set_cpu_fifo_underrun_reporting(dev, pipe, true);
++		intel_set_cpu_fifo_underrun_reporting(dev_priv, pipe, true);
+ 
+ 	for_each_encoder_on_crtc(dev, crtc, encoder)
+ 		if (encoder->pre_enable)
+@@ -4768,6 +5078,9 @@
+ 	for_each_encoder_on_crtc(dev, crtc, encoder)
+ 		encoder->enable(encoder);
+ 
++	assert_vblank_disabled(crtc);
++	drm_crtc_vblank_on(crtc);
++
+ 	intel_crtc_enable_planes(crtc);
+ 
+ 	/*
+@@ -4778,10 +5091,10 @@
+ 	 * but leave the pipe running.
+ 	 */
+ 	if (IS_GEN2(dev))
+-		intel_set_cpu_fifo_underrun_reporting(dev, pipe, true);
++		intel_set_cpu_fifo_underrun_reporting(dev_priv, pipe, true);
+ 
+ 	/* Underruns don't raise interrupts, so check manually. */
+-	i9xx_check_fifo_underruns(dev);
++	i9xx_check_fifo_underruns(dev_priv);
+ }
+ 
+ static void i9xx_pfit_disable(struct intel_crtc *crtc)
+@@ -4817,7 +5130,7 @@
+ 	 * but leave the pipe running.
+ 	 */
+ 	if (IS_GEN2(dev))
+-		intel_set_cpu_fifo_underrun_reporting(dev, pipe, false);
++		intel_set_cpu_fifo_underrun_reporting(dev_priv, pipe, false);
+ 
+ 	/*
+ 	 * Vblank time updates from the shadow to live plane control register
+@@ -4831,9 +5144,6 @@
+ 	intel_set_memory_cxsr(dev_priv, false);
+ 	intel_crtc_disable_planes(crtc);
+ 
+-	for_each_encoder_on_crtc(dev, crtc, encoder)
+-		encoder->disable(encoder);
+-
+ 	/*
+ 	 * On gen2 planes are double buffered but the pipe isn't, so we must
+ 	 * wait for planes to fully turn off before disabling the pipe.
+@@ -4842,7 +5152,13 @@
+ 	 */
+ 	intel_wait_for_vblank(dev, pipe);
+ 
+-	intel_disable_pipe(dev_priv, pipe);
++	drm_crtc_vblank_off(crtc);
++	assert_vblank_disabled(crtc);
++
++	for_each_encoder_on_crtc(dev, crtc, encoder)
++		encoder->disable(encoder);
++
++	intel_disable_pipe(intel_crtc);
+ 
+ 	i9xx_pfit_disable(intel_crtc);
+ 
+@@ -4850,17 +5166,17 @@
+ 		if (encoder->post_disable)
+ 			encoder->post_disable(encoder);
+ 
+-	if (!intel_pipe_has_type(crtc, INTEL_OUTPUT_DSI)) {
++	if (!intel_pipe_has_type(intel_crtc, INTEL_OUTPUT_DSI)) {
+ 		if (IS_CHERRYVIEW(dev))
+ 			chv_disable_pll(dev_priv, pipe);
+ 		else if (IS_VALLEYVIEW(dev))
+ 			vlv_disable_pll(dev_priv, pipe);
+ 		else
+-			i9xx_disable_pll(dev_priv, pipe);
++			i9xx_disable_pll(intel_crtc);
+ 	}
+ 
+ 	if (!IS_GEN2(dev))
+-		intel_set_cpu_fifo_underrun_reporting(dev, pipe, false);
++		intel_set_cpu_fifo_underrun_reporting(dev_priv, pipe, false);
+ 
+ 	intel_crtc->active = false;
+ 	intel_update_watermarks(crtc);
+@@ -4874,36 +5190,6 @@
+ {
+ }
+ 
+-static void intel_crtc_update_sarea(struct drm_crtc *crtc,
+-				    bool enabled)
+-{
+-	struct drm_device *dev = crtc->dev;
+-	struct drm_i915_master_private *master_priv;
+-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+-	int pipe = intel_crtc->pipe;
+-
+-	if (!dev->primary->master)
+-		return;
+-
+-	master_priv = dev->primary->master->driver_priv;
+-	if (!master_priv->sarea_priv)
+-		return;
+-
+-	switch (pipe) {
+-	case 0:
+-		master_priv->sarea_priv->pipeA_w = enabled ? crtc->mode.hdisplay : 0;
+-		master_priv->sarea_priv->pipeA_h = enabled ? crtc->mode.vdisplay : 0;
+-		break;
+-	case 1:
+-		master_priv->sarea_priv->pipeB_w = enabled ? crtc->mode.hdisplay : 0;
+-		master_priv->sarea_priv->pipeB_h = enabled ? crtc->mode.vdisplay : 0;
+-		break;
+-	default:
+-		DRM_ERROR("Can't update pipe %c in SAREA\n", pipe_name(pipe));
+-		break;
+-	}
+-}
+-
+ /* Master function to enable/disable CRTC and corresponding power wells */
+ void intel_crtc_control(struct drm_crtc *crtc, bool enable)
+ {
+@@ -4947,8 +5233,6 @@
+ 		enable |= intel_encoder->connectors_active;
+ 
+ 	intel_crtc_control(crtc, enable);
+-
+-	intel_crtc_update_sarea(crtc, enable);
+ }
+ 
+ static void intel_crtc_disable(struct drm_crtc *crtc)
+@@ -4963,7 +5247,6 @@
+ 	WARN_ON(!crtc->enabled);
+ 
+ 	dev_priv->display.crtc_disable(crtc);
+-	intel_crtc_update_sarea(crtc, false);
+ 	dev_priv->display.off(crtc);
+ 
+ 	if (crtc->primary->fb) {
+@@ -5001,15 +5284,8 @@
+  * state of the entire output pipe. */
+ static void intel_encoder_dpms(struct intel_encoder *encoder, int mode)
+ {
+-	if (mode == DRM_MODE_DPMS_ON) {
+-		encoder->connectors_active = true;
+-
+-		intel_crtc_update_dpms(encoder->base.crtc);
+-	} else {
+-		encoder->connectors_active = false;
+-
+-		intel_crtc_update_dpms(encoder->base.crtc);
+-	}
++	encoder->connectors_active = mode == DRM_MODE_DPMS_ON;
++	intel_crtc_update_dpms(encoder->base.crtc);
+ }
+ 
+ /* Cross check the actual hw state with our own modeset state tracking (and it's
+@@ -5193,7 +5469,7 @@
+ static void hsw_compute_ips_config(struct intel_crtc *crtc,
+ 				   struct intel_crtc_config *pipe_config)
+ {
+-	pipe_config->ips_enabled = i915.enable_ips &&
++	pipe_config->ips_enabled = i915_module.enable_ips &&
+ 				   hsw_crtc_supports_ips(crtc) &&
+ 				   pipe_config->pipe_bpp <= 24;
+ }
+@@ -5202,11 +5478,11 @@
+ 				     struct intel_crtc_config *pipe_config)
+ {
+ 	struct drm_device *dev = crtc->base.dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct drm_display_mode *adjusted_mode = &pipe_config->adjusted_mode;
+ 
+ 	/* FIXME should check pixel clock limits on all platforms */
+ 	if (INTEL_INFO(dev)->gen < 4) {
+-		struct drm_i915_private *dev_priv = dev->dev_private;
+ 		int clock_limit =
+ 			dev_priv->display.get_display_clock_speed(dev);
+ 
+@@ -5233,7 +5509,7 @@
+ 	 * - LVDS dual channel mode
+ 	 * - Double wide pipe
+ 	 */
+-	if ((intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_LVDS) &&
++	if ((intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS) &&
+ 	     intel_is_dual_link_lvds(dev)) || pipe_config->double_wide)
+ 		pipe_config->pipe_src_w &= ~1;
+ 
+@@ -5255,13 +5531,6 @@
+ 	if (HAS_IPS(dev))
+ 		hsw_compute_ips_config(crtc, pipe_config);
+ 
+-	/*
+-	 * XXX: PCH/WRPLL clock sharing is done in ->mode_set, so make sure the
+-	 * old clock survives for now.
+-	 */
+-	if (HAS_PCH_IBX(dev) || HAS_PCH_CPT(dev) || HAS_DDI(dev))
+-		pipe_config->shared_dpll = crtc->config.shared_dpll;
+-
+ 	if (pipe_config->has_pch_encoder)
+ 		return ironlake_fdi_compute_config(crtc, pipe_config);
+ 
+@@ -5271,10 +5540,16 @@
+ static int valleyview_get_display_clock_speed(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	int vco = valleyview_get_vco(dev_priv);
+ 	u32 val;
+ 	int divider;
+ 
++	/* FIXME: Punit isn't quite ready yet */
++	if (IS_CHERRYVIEW(dev))
++		return 400000;
++
++	if (dev_priv->hpll_freq == 0)
++		dev_priv->hpll_freq = valleyview_get_vco(dev_priv);
++
+ 	mutex_lock(&dev_priv->dpio_lock);
+ 	val = vlv_cck_read(dev_priv, CCK_DISPLAY_CLOCK_CONTROL);
+ 	mutex_unlock(&dev_priv->dpio_lock);
+@@ -5285,7 +5560,7 @@
+ 	     (divider << DISPLAY_FREQUENCY_STATUS_SHIFT),
+ 	     "cdclk change in progress\n");
+ 
+-	return DIV_ROUND_CLOSEST(vco << 1, divider + 1);
++	return DIV_ROUND_CLOSEST(dev_priv->hpll_freq << 1, divider + 1);
+ }
+ 
+ static int i945_get_display_clock_speed(struct drm_device *dev)
+@@ -5411,21 +5686,21 @@
+ 
+ static inline bool intel_panel_use_ssc(struct drm_i915_private *dev_priv)
+ {
+-	if (i915.panel_use_ssc >= 0)
+-		return i915.panel_use_ssc != 0;
++	if (i915_module.panel_use_ssc >= 0)
++		return i915_module.panel_use_ssc != 0;
+ 	return dev_priv->vbt.lvds_use_ssc
+ 		&& !(dev_priv->quirks & QUIRK_LVDS_SSC_DISABLE);
+ }
+ 
+-static int i9xx_get_refclk(struct drm_crtc *crtc, int num_connectors)
++static int i9xx_get_refclk(struct intel_crtc *crtc, int num_connectors)
+ {
+-	struct drm_device *dev = crtc->dev;
++	struct drm_device *dev = crtc->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	int refclk;
+ 
+ 	if (IS_VALLEYVIEW(dev)) {
+ 		refclk = 100000;
+-	} else if (intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS) &&
++	} else if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS) &&
+ 	    intel_panel_use_ssc(dev_priv) && num_connectors < 2) {
+ 		refclk = dev_priv->vbt.lvds_ssc_freq;
+ 		DRM_DEBUG_KMS("using SSC reference clock of %d kHz\n", refclk);
+@@ -5455,24 +5730,24 @@
+ 	u32 fp, fp2 = 0;
+ 
+ 	if (IS_PINEVIEW(dev)) {
+-		fp = pnv_dpll_compute_fp(&crtc->config.dpll);
++		fp = pnv_dpll_compute_fp(&crtc->new_config->dpll);
+ 		if (reduced_clock)
+ 			fp2 = pnv_dpll_compute_fp(reduced_clock);
+ 	} else {
+-		fp = i9xx_dpll_compute_fp(&crtc->config.dpll);
++		fp = i9xx_dpll_compute_fp(&crtc->new_config->dpll);
+ 		if (reduced_clock)
+ 			fp2 = i9xx_dpll_compute_fp(reduced_clock);
+ 	}
+ 
+-	crtc->config.dpll_hw_state.fp0 = fp;
++	crtc->new_config->dpll_hw_state.fp0 = fp;
+ 
+ 	crtc->lowfreq_avail = false;
+-	if (intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_LVDS) &&
+-	    reduced_clock && i915.powersave) {
+-		crtc->config.dpll_hw_state.fp1 = fp2;
++	if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS) &&
++	    reduced_clock && i915_module.powersave) {
++		crtc->new_config->dpll_hw_state.fp1 = fp2;
+ 		crtc->lowfreq_avail = true;
+ 	} else {
+-		crtc->config.dpll_hw_state.fp1 = fp;
++		crtc->new_config->dpll_hw_state.fp1 = fp;
+ 	}
+ }
+ 
+@@ -5519,7 +5794,8 @@
+ }
+ 
+ static void intel_cpu_transcoder_set_m_n(struct intel_crtc *crtc,
+-					 struct intel_link_m_n *m_n)
++					 struct intel_link_m_n *m_n,
++					 struct intel_link_m_n *m2_n2)
+ {
+ 	struct drm_device *dev = crtc->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+@@ -5531,6 +5807,18 @@
+ 		I915_WRITE(PIPE_DATA_N1(transcoder), m_n->gmch_n);
+ 		I915_WRITE(PIPE_LINK_M1(transcoder), m_n->link_m);
+ 		I915_WRITE(PIPE_LINK_N1(transcoder), m_n->link_n);
++		/* M2_N2 registers to be set only for gen < 8 (M2_N2 available
++		 * for gen < 8) and if DRRS is supported (to make sure the
++		 * registers are not unnecessarily accessed).
++		 */
++		if (m2_n2 && INTEL_INFO(dev)->gen < 8 &&
++			crtc->config.has_drrs) {
++			I915_WRITE(PIPE_DATA_M2(transcoder),
++					TU_SIZE(m2_n2->tu) | m2_n2->gmch_m);
++			I915_WRITE(PIPE_DATA_N2(transcoder), m2_n2->gmch_n);
++			I915_WRITE(PIPE_LINK_M2(transcoder), m2_n2->link_m);
++			I915_WRITE(PIPE_LINK_N2(transcoder), m2_n2->link_n);
++		}
+ 	} else {
+ 		I915_WRITE(PIPE_DATA_M_G4X(pipe), TU_SIZE(m_n->tu) | m_n->gmch_m);
+ 		I915_WRITE(PIPE_DATA_N_G4X(pipe), m_n->gmch_n);
+@@ -5539,15 +5827,17 @@
+ 	}
+ }
+ 
+-static void intel_dp_set_m_n(struct intel_crtc *crtc)
++void intel_dp_set_m_n(struct intel_crtc *crtc)
+ {
+ 	if (crtc->config.has_pch_encoder)
+ 		intel_pch_transcoder_set_m_n(crtc, &crtc->config.dp_m_n);
+ 	else
+-		intel_cpu_transcoder_set_m_n(crtc, &crtc->config.dp_m_n);
++		intel_cpu_transcoder_set_m_n(crtc, &crtc->config.dp_m_n,
++						   &crtc->config.dp_m2_n2);
+ }
+ 
+-static void vlv_update_pll(struct intel_crtc *crtc)
++static void vlv_update_pll(struct intel_crtc *crtc,
++			   struct intel_crtc_config *pipe_config)
+ {
+ 	u32 dpll, dpll_md;
+ 
+@@ -5562,14 +5852,15 @@
+ 	if (crtc->pipe == PIPE_B)
+ 		dpll |= DPLL_INTEGRATED_CRI_CLK_VLV;
+ 	dpll |= DPLL_VCO_ENABLE;
+-	crtc->config.dpll_hw_state.dpll = dpll;
++	pipe_config->dpll_hw_state.dpll = dpll;
+ 
+-	dpll_md = (crtc->config.pixel_multiplier - 1)
++	dpll_md = (pipe_config->pixel_multiplier - 1)
+ 		<< DPLL_MD_UDI_MULTIPLIER_SHIFT;
+-	crtc->config.dpll_hw_state.dpll_md = dpll_md;
++	pipe_config->dpll_hw_state.dpll_md = dpll_md;
+ }
+ 
+-static void vlv_prepare_pll(struct intel_crtc *crtc)
++static void vlv_prepare_pll(struct intel_crtc *crtc,
++			    const struct intel_crtc_config *pipe_config)
+ {
+ 	struct drm_device *dev = crtc->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+@@ -5580,11 +5871,11 @@
+ 
+ 	mutex_lock(&dev_priv->dpio_lock);
+ 
+-	bestn = crtc->config.dpll.n;
+-	bestm1 = crtc->config.dpll.m1;
+-	bestm2 = crtc->config.dpll.m2;
+-	bestp1 = crtc->config.dpll.p1;
+-	bestp2 = crtc->config.dpll.p2;
++	bestn = pipe_config->dpll.n;
++	bestm1 = pipe_config->dpll.m1;
++	bestm2 = pipe_config->dpll.m2;
++	bestp1 = pipe_config->dpll.p1;
++	bestp2 = pipe_config->dpll.p2;
+ 
+ 	/* See eDP HDMI DPIO driver vbios notes doc */
+ 
+@@ -5621,17 +5912,16 @@
+ 	vlv_dpio_write(dev_priv, pipe, VLV_PLL_DW3(pipe), mdiv);
+ 
+ 	/* Set HBR and RBR LPF coefficients */
+-	if (crtc->config.port_clock == 162000 ||
+-	    intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_ANALOG) ||
+-	    intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_HDMI))
++	if (pipe_config->port_clock == 162000 ||
++	    intel_pipe_has_type(crtc, INTEL_OUTPUT_ANALOG) ||
++	    intel_pipe_has_type(crtc, INTEL_OUTPUT_HDMI))
+ 		vlv_dpio_write(dev_priv, pipe, VLV_PLL_DW10(pipe),
+ 				 0x009f0003);
+ 	else
+ 		vlv_dpio_write(dev_priv, pipe, VLV_PLL_DW10(pipe),
+ 				 0x00d0000f);
+ 
+-	if (intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_EDP) ||
+-	    intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_DISPLAYPORT)) {
++	if (crtc->config.has_dp_encoder) {
+ 		/* Use SSC source */
+ 		if (pipe == PIPE_A)
+ 			vlv_dpio_write(dev_priv, pipe, VLV_PLL_DW5(pipe),
+@@ -5651,8 +5941,8 @@
+ 
+ 	coreclk = vlv_dpio_read(dev_priv, pipe, VLV_PLL_DW7(pipe));
+ 	coreclk = (coreclk & 0x0000ff00) | 0x01c00000;
+-	if (intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_DISPLAYPORT) ||
+-	    intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_EDP))
++	if (intel_pipe_has_type(crtc, INTEL_OUTPUT_DISPLAYPORT) ||
++	    intel_pipe_has_type(crtc, INTEL_OUTPUT_EDP))
+ 		coreclk |= 0x01000000;
+ 	vlv_dpio_write(dev_priv, pipe, VLV_PLL_DW7(pipe), coreclk);
+ 
+@@ -5660,7 +5950,21 @@
+ 	mutex_unlock(&dev_priv->dpio_lock);
+ }
+ 
+-static void chv_update_pll(struct intel_crtc *crtc)
++static void chv_update_pll(struct intel_crtc *crtc,
++			   struct intel_crtc_config *pipe_config)
++{
++	pipe_config->dpll_hw_state.dpll = DPLL_SSC_REF_CLOCK_CHV |
++		DPLL_REFA_CLK_ENABLE_VLV | DPLL_VGA_MODE_DIS |
++		DPLL_VCO_ENABLE;
++	if (crtc->pipe != PIPE_A)
++		pipe_config->dpll_hw_state.dpll |= DPLL_INTEGRATED_CRI_CLK_VLV;
++
++	pipe_config->dpll_hw_state.dpll_md =
++		(pipe_config->pixel_multiplier - 1) << DPLL_MD_UDI_MULTIPLIER_SHIFT;
++}
++
++static void chv_prepare_pll(struct intel_crtc *crtc,
++			    const struct intel_crtc_config *pipe_config)
+ {
+ 	struct drm_device *dev = crtc->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+@@ -5671,27 +5975,18 @@
+ 	u32 bestn, bestm1, bestm2, bestp1, bestp2, bestm2_frac;
+ 	int refclk;
+ 
+-	crtc->config.dpll_hw_state.dpll = DPLL_SSC_REF_CLOCK_CHV |
+-		DPLL_REFA_CLK_ENABLE_VLV | DPLL_VGA_MODE_DIS |
+-		DPLL_VCO_ENABLE;
+-	if (pipe != PIPE_A)
+-		crtc->config.dpll_hw_state.dpll |= DPLL_INTEGRATED_CRI_CLK_VLV;
+-
+-	crtc->config.dpll_hw_state.dpll_md =
+-		(crtc->config.pixel_multiplier - 1) << DPLL_MD_UDI_MULTIPLIER_SHIFT;
+-
+-	bestn = crtc->config.dpll.n;
+-	bestm2_frac = crtc->config.dpll.m2 & 0x3fffff;
+-	bestm1 = crtc->config.dpll.m1;
+-	bestm2 = crtc->config.dpll.m2 >> 22;
+-	bestp1 = crtc->config.dpll.p1;
+-	bestp2 = crtc->config.dpll.p2;
++	bestn = pipe_config->dpll.n;
++	bestm2_frac = pipe_config->dpll.m2 & 0x3fffff;
++	bestm1 = pipe_config->dpll.m1;
++	bestm2 = pipe_config->dpll.m2 >> 22;
++	bestp1 = pipe_config->dpll.p1;
++	bestp2 = pipe_config->dpll.p2;
+ 
+ 	/*
+ 	 * Enable Refclk and SSC
+ 	 */
+ 	I915_WRITE(dpll_reg,
+-		   crtc->config.dpll_hw_state.dpll & ~DPLL_VCO_ENABLE);
++		   pipe_config->dpll_hw_state.dpll & ~DPLL_VCO_ENABLE);
+ 
+ 	mutex_lock(&dev_priv->dpio_lock);
+ 
+@@ -5719,7 +6014,7 @@
+ 		       (2 << DPIO_CHV_FEEDFWD_GAIN_SHIFT));
+ 
+ 	/* Loop filter */
+-	refclk = i9xx_get_refclk(&crtc->base, 0);
++	refclk = i9xx_get_refclk(crtc, 0);
+ 	loopfilter = 5 << DPIO_CHV_PROP_COEFF_SHIFT |
+ 		2 << DPIO_CHV_GAIN_CTRL_SHIFT;
+ 	if (refclk == 100000)
+@@ -5739,6 +6034,53 @@
+ 	mutex_unlock(&dev_priv->dpio_lock);
+ }
+ 
++/**
++ * vlv_force_pll_on - forcibly enable just the PLL
++ * @dev_priv: i915 private structure
++ * @pipe: pipe PLL to enable
++ * @dpll: PLL configuration
++ *
++ * Enable the PLL for @pipe using the supplied @dpll config. To be used
++ * in cases where we need the PLL enabled even when @pipe is not going to
++ * be enabled.
++ */
++void vlv_force_pll_on(struct drm_device *dev, enum pipe pipe,
++		      const struct dpll *dpll)
++{
++	struct intel_crtc *crtc =
++		to_intel_crtc(intel_get_crtc_for_pipe(dev, pipe));
++	struct intel_crtc_config pipe_config = {
++		.pixel_multiplier = 1,
++		.dpll = *dpll,
++	};
++
++	if (IS_CHERRYVIEW(dev)) {
++		chv_update_pll(crtc, &pipe_config);
++		chv_prepare_pll(crtc, &pipe_config);
++		chv_enable_pll(crtc, &pipe_config);
++	} else {
++		vlv_update_pll(crtc, &pipe_config);
++		vlv_prepare_pll(crtc, &pipe_config);
++		vlv_enable_pll(crtc, &pipe_config);
++	}
++}
++
++/**
++ * vlv_force_pll_off - forcibly disable just the PLL
++ * @dev_priv: i915 private structure
++ * @pipe: pipe PLL to disable
++ *
++ * Disable the PLL for @pipe. To be used in cases where we need
++ * the PLL enabled even when @pipe is not going to be enabled.
++ */
++void vlv_force_pll_off(struct drm_device *dev, enum pipe pipe)
++{
++	if (IS_CHERRYVIEW(dev))
++		chv_disable_pll(to_i915(dev), pipe);
++	else
++		vlv_disable_pll(to_i915(dev), pipe);
++}
++
+ static void i9xx_update_pll(struct intel_crtc *crtc,
+ 			    intel_clock_t *reduced_clock,
+ 			    int num_connectors)
+@@ -5747,29 +6089,29 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	u32 dpll;
+ 	bool is_sdvo;
+-	struct dpll *clock = &crtc->config.dpll;
++	struct dpll *clock = &crtc->new_config->dpll;
+ 
+ 	i9xx_update_pll_dividers(crtc, reduced_clock);
+ 
+-	is_sdvo = intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_SDVO) ||
+-		intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_HDMI);
++	is_sdvo = intel_pipe_will_have_type(crtc, INTEL_OUTPUT_SDVO) ||
++		intel_pipe_will_have_type(crtc, INTEL_OUTPUT_HDMI);
+ 
+ 	dpll = DPLL_VGA_MODE_DIS;
+ 
+-	if (intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_LVDS))
++	if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS))
+ 		dpll |= DPLLB_MODE_LVDS;
+ 	else
+ 		dpll |= DPLLB_MODE_DAC_SERIAL;
+ 
+ 	if (IS_I945G(dev) || IS_I945GM(dev) || IS_G33(dev)) {
+-		dpll |= (crtc->config.pixel_multiplier - 1)
++		dpll |= (crtc->new_config->pixel_multiplier - 1)
+ 			<< SDVO_MULTIPLIER_SHIFT_HIRES;
+ 	}
+ 
+ 	if (is_sdvo)
+ 		dpll |= DPLL_SDVO_HIGH_SPEED;
+ 
+-	if (intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_DISPLAYPORT))
++	if (crtc->new_config->has_dp_encoder)
+ 		dpll |= DPLL_SDVO_HIGH_SPEED;
+ 
+ 	/* compute bitmask from p1 value */
+@@ -5797,21 +6139,21 @@
+ 	if (INTEL_INFO(dev)->gen >= 4)
+ 		dpll |= (6 << PLL_LOAD_PULSE_PHASE_SHIFT);
+ 
+-	if (crtc->config.sdvo_tv_clock)
++	if (crtc->new_config->sdvo_tv_clock)
+ 		dpll |= PLL_REF_INPUT_TVCLKINBC;
+-	else if (intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_LVDS) &&
++	else if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS) &&
+ 		 intel_panel_use_ssc(dev_priv) && num_connectors < 2)
+ 		dpll |= PLLB_REF_INPUT_SPREADSPECTRUMIN;
+ 	else
+ 		dpll |= PLL_REF_INPUT_DREFCLK;
+ 
+ 	dpll |= DPLL_VCO_ENABLE;
+-	crtc->config.dpll_hw_state.dpll = dpll;
++	crtc->new_config->dpll_hw_state.dpll = dpll;
+ 
+ 	if (INTEL_INFO(dev)->gen >= 4) {
+-		u32 dpll_md = (crtc->config.pixel_multiplier - 1)
++		u32 dpll_md = (crtc->new_config->pixel_multiplier - 1)
+ 			<< DPLL_MD_UDI_MULTIPLIER_SHIFT;
+-		crtc->config.dpll_hw_state.dpll_md = dpll_md;
++		crtc->new_config->dpll_hw_state.dpll_md = dpll_md;
+ 	}
+ }
+ 
+@@ -5822,13 +6164,13 @@
+ 	struct drm_device *dev = crtc->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	u32 dpll;
+-	struct dpll *clock = &crtc->config.dpll;
++	struct dpll *clock = &crtc->new_config->dpll;
+ 
+ 	i9xx_update_pll_dividers(crtc, reduced_clock);
+ 
+ 	dpll = DPLL_VGA_MODE_DIS;
+ 
+-	if (intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_LVDS)) {
++	if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS)) {
+ 		dpll |= (1 << (clock->p1 - 1)) << DPLL_FPA01_P1_POST_DIV_SHIFT;
+ 	} else {
+ 		if (clock->p1 == 2)
+@@ -5839,17 +6181,17 @@
+ 			dpll |= PLL_P2_DIVIDE_BY_4;
+ 	}
+ 
+-	if (intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_DVO))
++	if (!IS_I830(dev) && intel_pipe_will_have_type(crtc, INTEL_OUTPUT_DVO))
+ 		dpll |= DPLL_DVO_2X_MODE;
+ 
+-	if (intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_LVDS) &&
++	if (intel_pipe_will_have_type(crtc, INTEL_OUTPUT_LVDS) &&
+ 		 intel_panel_use_ssc(dev_priv) && num_connectors < 2)
+ 		dpll |= PLLB_REF_INPUT_SPREADSPECTRUMIN;
+ 	else
+ 		dpll |= PLL_REF_INPUT_DREFCLK;
+ 
+ 	dpll |= DPLL_VCO_ENABLE;
+-	crtc->config.dpll_hw_state.dpll = dpll;
++	crtc->new_config->dpll_hw_state.dpll = dpll;
+ }
+ 
+ static void intel_set_pipe_timings(struct intel_crtc *intel_crtc)
+@@ -5873,7 +6215,7 @@
+ 		crtc_vtotal -= 1;
+ 		crtc_vblank_end -= 1;
+ 
+-		if (intel_pipe_has_type(&intel_crtc->base, INTEL_OUTPUT_SDVO))
++		if (intel_pipe_has_type(intel_crtc, INTEL_OUTPUT_SDVO))
+ 			vsyncshift = (adjusted_mode->crtc_htotal - 1) / 2;
+ 		else
+ 			vsyncshift = adjusted_mode->crtc_hsync_start -
+@@ -5990,9 +6332,9 @@
+ 
+ 	pipeconf = 0;
+ 
+-	if (dev_priv->quirks & QUIRK_PIPEA_FORCE &&
+-	    I915_READ(PIPECONF(intel_crtc->pipe)) & PIPECONF_ENABLE)
+-		pipeconf |= PIPECONF_ENABLE;
++	if ((intel_crtc->pipe == PIPE_A && dev_priv->quirks & QUIRK_PIPEA_FORCE) ||
++	    (intel_crtc->pipe == PIPE_B && dev_priv->quirks & QUIRK_PIPEB_FORCE))
++		pipeconf |= I915_READ(PIPECONF(intel_crtc->pipe)) & PIPECONF_ENABLE;
+ 
+ 	if (intel_crtc->config.double_wide)
+ 		pipeconf |= PIPECONF_DOUBLE_WIDE;
+@@ -6031,7 +6373,7 @@
+ 
+ 	if (intel_crtc->config.adjusted_mode.flags & DRM_MODE_FLAG_INTERLACE) {
+ 		if (INTEL_INFO(dev)->gen < 4 ||
+-		    intel_pipe_has_type(&intel_crtc->base, INTEL_OUTPUT_SDVO))
++		    intel_pipe_has_type(intel_crtc, INTEL_OUTPUT_SDVO))
+ 			pipeconf |= PIPECONF_INTERLACE_W_FIELD_INDICATION;
+ 		else
+ 			pipeconf |= PIPECONF_INTERLACE_W_SYNC_SHIFT;
+@@ -6045,13 +6387,10 @@
+ 	POSTING_READ(PIPECONF(intel_crtc->pipe));
+ }
+ 
+-static int i9xx_crtc_mode_set(struct drm_crtc *crtc,
+-			      int x, int y,
+-			      struct drm_framebuffer *fb)
++static int i9xx_crtc_compute_clock(struct intel_crtc *crtc)
+ {
+-	struct drm_device *dev = crtc->dev;
++	struct drm_device *dev = crtc->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+ 	int refclk, num_connectors = 0;
+ 	intel_clock_t clock, reduced_clock;
+ 	bool ok, has_reduced_clock = false;
+@@ -6059,7 +6398,10 @@
+ 	struct intel_encoder *encoder;
+ 	const intel_limit_t *limit;
+ 
+-	for_each_encoder_on_crtc(dev, crtc, encoder) {
++	for_each_intel_encoder(dev, encoder) {
++		if (encoder->new_crtc != crtc)
++			continue;
++
+ 		switch (encoder->type) {
+ 		case INTEL_OUTPUT_LVDS:
+ 			is_lvds = true;
+@@ -6067,6 +6409,8 @@
+ 		case INTEL_OUTPUT_DSI:
+ 			is_dsi = true;
+ 			break;
++		default:
++			break;
+ 		}
+ 
+ 		num_connectors++;
+@@ -6075,7 +6419,7 @@
+ 	if (is_dsi)
+ 		return 0;
+ 
+-	if (!intel_crtc->config.clock_set) {
++	if (!crtc->new_config->clock_set) {
+ 		refclk = i9xx_get_refclk(crtc, num_connectors);
+ 
+ 		/*
+@@ -6086,7 +6430,7 @@
+ 		 */
+ 		limit = intel_limit(crtc, refclk);
+ 		ok = dev_priv->display.find_dpll(limit, crtc,
+-						 intel_crtc->config.port_clock,
++						 crtc->new_config->port_clock,
+ 						 refclk, NULL, &clock);
+ 		if (!ok) {
+ 			DRM_ERROR("Couldn't find PLL settings for mode!\n");
+@@ -6107,23 +6451,23 @@
+ 							    &reduced_clock);
+ 		}
+ 		/* Compat-code for transition, will disappear. */
+-		intel_crtc->config.dpll.n = clock.n;
+-		intel_crtc->config.dpll.m1 = clock.m1;
+-		intel_crtc->config.dpll.m2 = clock.m2;
+-		intel_crtc->config.dpll.p1 = clock.p1;
+-		intel_crtc->config.dpll.p2 = clock.p2;
++		crtc->new_config->dpll.n = clock.n;
++		crtc->new_config->dpll.m1 = clock.m1;
++		crtc->new_config->dpll.m2 = clock.m2;
++		crtc->new_config->dpll.p1 = clock.p1;
++		crtc->new_config->dpll.p2 = clock.p2;
+ 	}
+ 
+ 	if (IS_GEN2(dev)) {
+-		i8xx_update_pll(intel_crtc,
++		i8xx_update_pll(crtc,
+ 				has_reduced_clock ? &reduced_clock : NULL,
+ 				num_connectors);
+ 	} else if (IS_CHERRYVIEW(dev)) {
+-		chv_update_pll(intel_crtc);
++		chv_update_pll(crtc, crtc->new_config);
+ 	} else if (IS_VALLEYVIEW(dev)) {
+-		vlv_update_pll(intel_crtc);
++		vlv_update_pll(crtc, crtc->new_config);
+ 	} else {
+-		i9xx_update_pll(intel_crtc,
++		i9xx_update_pll(crtc,
+ 				has_reduced_clock ? &reduced_clock : NULL,
+ 				num_connectors);
+ 	}
+@@ -6235,7 +6579,7 @@
+ 	crtc->base.primary->fb->height = ((val >> 0) & 0xfff) + 1;
+ 
+ 	val = I915_READ(DSPSTRIDE(pipe));
+-	crtc->base.primary->fb->pitches[0] = val & 0xffffff80;
++	crtc->base.primary->fb->pitches[0] = val & 0xffffffc0;
+ 
+ 	aligned_height = intel_align_height(dev, crtc->base.primary->fb->height,
+ 					    plane_config->tiled);
+@@ -6289,8 +6633,8 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	uint32_t tmp;
+ 
+-	if (!intel_display_power_enabled(dev_priv,
+-					 POWER_DOMAIN_PIPE(crtc->pipe)))
++	if (!intel_display_power_is_enabled(dev_priv,
++					    POWER_DOMAIN_PIPE(crtc->pipe)))
+ 		return false;
+ 
+ 	pipe_config->cpu_transcoder = (enum transcoder) crtc->pipe;
+@@ -6345,6 +6689,14 @@
+ 	}
+ 	pipe_config->dpll_hw_state.dpll = I915_READ(DPLL(crtc->pipe));
+ 	if (!IS_VALLEYVIEW(dev)) {
++		/*
++		 * DPLL_DVO_2X_MODE must be enabled for both DPLLs
++		 * on 830. Filter it out here so that we don't
++		 * report errors due to that.
++		 */
++		if (IS_I830(dev))
++			pipe_config->dpll_hw_state.dpll &= ~DPLL_DVO_2X_MODE;
++
+ 		pipe_config->dpll_hw_state.fp0 = I915_READ(FP0(crtc->pipe));
+ 		pipe_config->dpll_hw_state.fp1 = I915_READ(FP1(crtc->pipe));
+ 	} else {
+@@ -6367,7 +6719,6 @@
+ static void ironlake_init_pch_refclk(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct drm_mode_config *mode_config = &dev->mode_config;
+ 	struct intel_encoder *encoder;
+ 	u32 val, final;
+ 	bool has_lvds = false;
+@@ -6377,8 +6728,7 @@
+ 	bool can_ssc = false;
+ 
+ 	/* We need to take the global config into account */
+-	list_for_each_entry(encoder, &mode_config->encoder_list,
+-			    base.head) {
++	for_each_intel_encoder(dev, encoder) {
+ 		switch (encoder->type) {
+ 		case INTEL_OUTPUT_LVDS:
+ 			has_panel = true;
+@@ -6389,6 +6739,8 @@
+ 			if (enc_to_dig_port(&encoder->base)->port == PORT_A)
+ 				has_cpu_edp = true;
+ 			break;
++		default:
++			break;
+ 		}
+ 	}
+ 
+@@ -6685,15 +7037,16 @@
+ 
+ static void lpt_init_pch_refclk(struct drm_device *dev)
+ {
+-	struct drm_mode_config *mode_config = &dev->mode_config;
+ 	struct intel_encoder *encoder;
+ 	bool has_vga = false;
+ 
+-	list_for_each_entry(encoder, &mode_config->encoder_list, base.head) {
++	for_each_intel_encoder(dev, encoder) {
+ 		switch (encoder->type) {
+ 		case INTEL_OUTPUT_ANALOG:
+ 			has_vga = true;
+ 			break;
++		default:
++			break;
+ 		}
+ 	}
+ 
+@@ -6722,11 +7075,16 @@
+ 	int num_connectors = 0;
+ 	bool is_lvds = false;
+ 
+-	for_each_encoder_on_crtc(dev, crtc, encoder) {
++	for_each_intel_encoder(dev, encoder) {
++		if (encoder->new_crtc != to_intel_crtc(crtc))
++			continue;
++
+ 		switch (encoder->type) {
+ 		case INTEL_OUTPUT_LVDS:
+ 			is_lvds = true;
+ 			break;
++		default:
++			break;
+ 		}
+ 		num_connectors++;
+ 	}
+@@ -6871,7 +7229,7 @@
+ 	I915_WRITE(GAMMA_MODE(intel_crtc->pipe), GAMMA_MODE_MODE_8BIT);
+ 	POSTING_READ(GAMMA_MODE(intel_crtc->pipe));
+ 
+-	if (IS_BROADWELL(dev)) {
++	if (IS_BROADWELL(dev) || INTEL_INFO(dev)->gen >= 9) {
+ 		val = 0;
+ 
+ 		switch (intel_crtc->config.pipe_bpp) {
+@@ -6906,18 +7264,12 @@
+ {
+ 	struct drm_device *dev = crtc->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_encoder *intel_encoder;
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+ 	int refclk;
+ 	const intel_limit_t *limit;
+ 	bool ret, is_lvds = false;
+ 
+-	for_each_encoder_on_crtc(dev, crtc, intel_encoder) {
+-		switch (intel_encoder->type) {
+-		case INTEL_OUTPUT_LVDS:
+-			is_lvds = true;
+-			break;
+-		}
+-	}
++	is_lvds = intel_pipe_will_have_type(intel_crtc, INTEL_OUTPUT_LVDS);
+ 
+ 	refclk = ironlake_get_refclk(crtc);
+ 
+@@ -6926,9 +7278,9 @@
+ 	 * refclk, or FALSE.  The returned values represent the clock equation:
+ 	 * reflck * (5 * (m1 + 2) + (m2 + 2)) / (n + 2) / p1 / p2.
+ 	 */
+-	limit = intel_limit(crtc, refclk);
+-	ret = dev_priv->display.find_dpll(limit, crtc,
+-					  to_intel_crtc(crtc)->config.port_clock,
++	limit = intel_limit(intel_crtc, refclk);
++	ret = dev_priv->display.find_dpll(limit, intel_crtc,
++					  intel_crtc->new_config->port_clock,
+ 					  refclk, NULL, clock);
+ 	if (!ret)
+ 		return false;
+@@ -6941,7 +7293,7 @@
+ 		 * downclock feature.
+ 		*/
+ 		*has_reduced_clock =
+-			dev_priv->display.find_dpll(limit, crtc,
++			dev_priv->display.find_dpll(limit, intel_crtc,
+ 						    dev_priv->lvds_downclock,
+ 						    refclk, clock,
+ 						    reduced_clock);
+@@ -6978,7 +7330,10 @@
+ 	int factor, num_connectors = 0;
+ 	bool is_lvds = false, is_sdvo = false;
+ 
+-	for_each_encoder_on_crtc(dev, crtc, intel_encoder) {
++	for_each_intel_encoder(dev, intel_encoder) {
++		if (intel_encoder->new_crtc != to_intel_crtc(crtc))
++			continue;
++
+ 		switch (intel_encoder->type) {
+ 		case INTEL_OUTPUT_LVDS:
+ 			is_lvds = true;
+@@ -6987,6 +7342,8 @@
+ 		case INTEL_OUTPUT_HDMI:
+ 			is_sdvo = true;
+ 			break;
++		default:
++			break;
+ 		}
+ 
+ 		num_connectors++;
+@@ -6999,10 +7356,10 @@
+ 		     dev_priv->vbt.lvds_ssc_freq == 100000) ||
+ 		    (HAS_PCH_IBX(dev) && intel_is_dual_link_lvds(dev)))
+ 			factor = 25;
+-	} else if (intel_crtc->config.sdvo_tv_clock)
++	} else if (intel_crtc->new_config->sdvo_tv_clock)
+ 		factor = 20;
+ 
+-	if (ironlake_needs_fb_cb_tune(&intel_crtc->config.dpll, factor))
++	if (ironlake_needs_fb_cb_tune(&intel_crtc->new_config->dpll, factor))
+ 		*fp |= FP_CB_TUNE;
+ 
+ 	if (fp2 && (reduced_clock->m < factor * reduced_clock->n))
+@@ -7015,20 +7372,20 @@
+ 	else
+ 		dpll |= DPLLB_MODE_DAC_SERIAL;
+ 
+-	dpll |= (intel_crtc->config.pixel_multiplier - 1)
++	dpll |= (intel_crtc->new_config->pixel_multiplier - 1)
+ 		<< PLL_REF_SDVO_HDMI_MULTIPLIER_SHIFT;
+ 
+ 	if (is_sdvo)
+ 		dpll |= DPLL_SDVO_HIGH_SPEED;
+-	if (intel_crtc->config.has_dp_encoder)
++	if (intel_crtc->new_config->has_dp_encoder)
+ 		dpll |= DPLL_SDVO_HIGH_SPEED;
+ 
+ 	/* compute bitmask from p1 value */
+-	dpll |= (1 << (intel_crtc->config.dpll.p1 - 1)) << DPLL_FPA01_P1_POST_DIV_SHIFT;
++	dpll |= (1 << (intel_crtc->new_config->dpll.p1 - 1)) << DPLL_FPA01_P1_POST_DIV_SHIFT;
+ 	/* also FPA1 */
+-	dpll |= (1 << (intel_crtc->config.dpll.p1 - 1)) << DPLL_FPA1_P1_POST_DIV_SHIFT;
++	dpll |= (1 << (intel_crtc->new_config->dpll.p1 - 1)) << DPLL_FPA1_P1_POST_DIV_SHIFT;
+ 
+-	switch (intel_crtc->config.dpll.p2) {
++	switch (intel_crtc->new_config->dpll.p2) {
+ 	case 5:
+ 		dpll |= DPLL_DAC_SERIAL_P2_CLOCK_DIV_5;
+ 		break;
+@@ -7051,78 +7408,64 @@
+ 	return dpll | DPLL_VCO_ENABLE;
+ }
+ 
+-static int ironlake_crtc_mode_set(struct drm_crtc *crtc,
+-				  int x, int y,
+-				  struct drm_framebuffer *fb)
++static int ironlake_crtc_compute_clock(struct intel_crtc *crtc)
+ {
+-	struct drm_device *dev = crtc->dev;
+-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+-	int num_connectors = 0;
++	struct drm_device *dev = crtc->base.dev;
+ 	intel_clock_t clock, reduced_clock;
+ 	u32 dpll = 0, fp = 0, fp2 = 0;
+ 	bool ok, has_reduced_clock = false;
+ 	bool is_lvds = false;
+-	struct intel_encoder *encoder;
+ 	struct intel_shared_dpll *pll;
+ 
+-	for_each_encoder_on_crtc(dev, crtc, encoder) {
+-		switch (encoder->type) {
+-		case INTEL_OUTPUT_LVDS:
+-			is_lvds = true;
+-			break;
+-		}
+-
+-		num_connectors++;
+-	}
++	is_lvds = intel_pipe_has_type(crtc, INTEL_OUTPUT_LVDS);
+ 
+ 	WARN(!(HAS_PCH_IBX(dev) || HAS_PCH_CPT(dev)),
+ 	     "Unexpected PCH type %d\n", INTEL_PCH_TYPE(dev));
+ 
+-	ok = ironlake_compute_clocks(crtc, &clock,
++	ok = ironlake_compute_clocks(&crtc->base, &clock,
+ 				     &has_reduced_clock, &reduced_clock);
+-	if (!ok && !intel_crtc->config.clock_set) {
++	if (!ok && !crtc->new_config->clock_set) {
+ 		DRM_ERROR("Couldn't find PLL settings for mode!\n");
+ 		return -EINVAL;
+ 	}
+ 	/* Compat-code for transition, will disappear. */
+-	if (!intel_crtc->config.clock_set) {
+-		intel_crtc->config.dpll.n = clock.n;
+-		intel_crtc->config.dpll.m1 = clock.m1;
+-		intel_crtc->config.dpll.m2 = clock.m2;
+-		intel_crtc->config.dpll.p1 = clock.p1;
+-		intel_crtc->config.dpll.p2 = clock.p2;
++	if (!crtc->new_config->clock_set) {
++		crtc->new_config->dpll.n = clock.n;
++		crtc->new_config->dpll.m1 = clock.m1;
++		crtc->new_config->dpll.m2 = clock.m2;
++		crtc->new_config->dpll.p1 = clock.p1;
++		crtc->new_config->dpll.p2 = clock.p2;
+ 	}
+ 
+ 	/* CPU eDP is the only output that doesn't need a PCH PLL of its own. */
+-	if (intel_crtc->config.has_pch_encoder) {
+-		fp = i9xx_dpll_compute_fp(&intel_crtc->config.dpll);
++	if (crtc->new_config->has_pch_encoder) {
++		fp = i9xx_dpll_compute_fp(&crtc->new_config->dpll);
+ 		if (has_reduced_clock)
+ 			fp2 = i9xx_dpll_compute_fp(&reduced_clock);
+ 
+-		dpll = ironlake_compute_dpll(intel_crtc,
++		dpll = ironlake_compute_dpll(crtc,
+ 					     &fp, &reduced_clock,
+ 					     has_reduced_clock ? &fp2 : NULL);
+ 
+-		intel_crtc->config.dpll_hw_state.dpll = dpll;
+-		intel_crtc->config.dpll_hw_state.fp0 = fp;
++		crtc->new_config->dpll_hw_state.dpll = dpll;
++		crtc->new_config->dpll_hw_state.fp0 = fp;
+ 		if (has_reduced_clock)
+-			intel_crtc->config.dpll_hw_state.fp1 = fp2;
++			crtc->new_config->dpll_hw_state.fp1 = fp2;
+ 		else
+-			intel_crtc->config.dpll_hw_state.fp1 = fp;
++			crtc->new_config->dpll_hw_state.fp1 = fp;
+ 
+-		pll = intel_get_shared_dpll(intel_crtc);
++		pll = intel_get_shared_dpll(crtc);
+ 		if (pll == NULL) {
+ 			DRM_DEBUG_DRIVER("failed to find PLL for pipe %c\n",
+-					 pipe_name(intel_crtc->pipe));
++					 pipe_name(crtc->pipe));
+ 			return -EINVAL;
+ 		}
+-	} else
+-		intel_put_shared_dpll(intel_crtc);
++	}
+ 
+-	if (is_lvds && has_reduced_clock && i915.powersave)
+-		intel_crtc->lowfreq_avail = true;
++	if (is_lvds && has_reduced_clock && i915_module.powersave)
++		crtc->lowfreq_avail = true;
+ 	else
+-		intel_crtc->lowfreq_avail = false;
++		crtc->lowfreq_avail = false;
+ 
+ 	return 0;
+ }
+@@ -7145,7 +7488,8 @@
+ 
+ static void intel_cpu_transcoder_get_m_n(struct intel_crtc *crtc,
+ 					 enum transcoder transcoder,
+-					 struct intel_link_m_n *m_n)
++					 struct intel_link_m_n *m_n,
++					 struct intel_link_m_n *m2_n2)
+ {
+ 	struct drm_device *dev = crtc->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+@@ -7159,6 +7503,20 @@
+ 		m_n->gmch_n = I915_READ(PIPE_DATA_N1(transcoder));
+ 		m_n->tu = ((I915_READ(PIPE_DATA_M1(transcoder))
+ 			    & TU_SIZE_MASK) >> TU_SIZE_SHIFT) + 1;
++		/* Read M2_N2 registers only for gen < 8 (M2_N2 available for
++		 * gen < 8) and if DRRS is supported (to make sure the
++		 * registers are not unnecessarily read).
++		 */
++		if (m2_n2 && INTEL_INFO(dev)->gen < 8 &&
++			crtc->config.has_drrs) {
++			m2_n2->link_m = I915_READ(PIPE_LINK_M2(transcoder));
++			m2_n2->link_n =	I915_READ(PIPE_LINK_N2(transcoder));
++			m2_n2->gmch_m =	I915_READ(PIPE_DATA_M2(transcoder))
++					& ~TU_SIZE_MASK;
++			m2_n2->gmch_n =	I915_READ(PIPE_DATA_N2(transcoder));
++			m2_n2->tu = ((I915_READ(PIPE_DATA_M2(transcoder))
++					& TU_SIZE_MASK) >> TU_SIZE_SHIFT) + 1;
++		}
+ 	} else {
+ 		m_n->link_m = I915_READ(PIPE_LINK_M_G4X(pipe));
+ 		m_n->link_n = I915_READ(PIPE_LINK_N_G4X(pipe));
+@@ -7177,14 +7535,31 @@
+ 		intel_pch_transcoder_get_m_n(crtc, &pipe_config->dp_m_n);
+ 	else
+ 		intel_cpu_transcoder_get_m_n(crtc, pipe_config->cpu_transcoder,
+-					     &pipe_config->dp_m_n);
++					     &pipe_config->dp_m_n,
++					     &pipe_config->dp_m2_n2);
+ }
+ 
+ static void ironlake_get_fdi_m_n_config(struct intel_crtc *crtc,
+ 					struct intel_crtc_config *pipe_config)
+ {
+ 	intel_cpu_transcoder_get_m_n(crtc, pipe_config->cpu_transcoder,
+-				     &pipe_config->fdi_m_n);
++				     &pipe_config->fdi_m_n, NULL);
++}
++
++static void skylake_get_pfit_config(struct intel_crtc *crtc,
++				    struct intel_crtc_config *pipe_config)
++{
++	struct drm_device *dev = crtc->base.dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	uint32_t tmp;
++
++	tmp = I915_READ(PS_CTL(crtc->pipe));
++
++	if (tmp & PS_ENABLE) {
++		pipe_config->pch_pfit.enabled = true;
++		pipe_config->pch_pfit.pos = I915_READ(PS_WIN_POS(crtc->pipe));
++		pipe_config->pch_pfit.size = I915_READ(PS_WIN_SZ(crtc->pipe));
++	}
+ }
+ 
+ static void ironlake_get_pfit_config(struct intel_crtc *crtc,
+@@ -7255,7 +7630,7 @@
+ 	crtc->base.primary->fb->height = ((val >> 0) & 0xfff) + 1;
+ 
+ 	val = I915_READ(DSPSTRIDE(pipe));
+-	crtc->base.primary->fb->pitches[0] = val & 0xffffff80;
++	crtc->base.primary->fb->pitches[0] = val & 0xffffffc0;
+ 
+ 	aligned_height = intel_align_height(dev, crtc->base.primary->fb->height,
+ 					    plane_config->tiled);
+@@ -7278,8 +7653,8 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	uint32_t tmp;
+ 
+-	if (!intel_display_power_enabled(dev_priv,
+-					 POWER_DOMAIN_PIPE(crtc->pipe)))
++	if (!intel_display_power_is_enabled(dev_priv,
++					    POWER_DOMAIN_PIPE(crtc->pipe)))
+ 		return false;
+ 
+ 	pipe_config->cpu_transcoder = (enum transcoder) crtc->pipe;
+@@ -7472,7 +7847,6 @@
+ static void hsw_restore_lcpll(struct drm_i915_private *dev_priv)
+ {
+ 	uint32_t val;
+-	unsigned long irqflags;
+ 
+ 	val = I915_READ(LCPLL_CTL);
+ 
+@@ -7483,19 +7857,8 @@
+ 	/*
+ 	 * Make sure we're not on PC8 state before disabling PC8, otherwise
+ 	 * we'll hang the machine. To prevent PC8 state, just enable force_wake.
+-	 *
+-	 * The other problem is that hsw_restore_lcpll() is called as part of
+-	 * the runtime PM resume sequence, so we can't just call
+-	 * gen6_gt_force_wake_get() because that function calls
+-	 * intel_runtime_pm_get(), and we can't change the runtime PM refcount
+-	 * while we are on the resume sequence. So to solve this problem we have
+-	 * to call special forcewake code that doesn't touch runtime PM and
+-	 * doesn't enable the forcewake delayed work.
+-	 */
+-	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
+-	if (dev_priv->uncore.forcewake_count++ == 0)
+-		dev_priv->uncore.funcs.force_wake_get(dev_priv, FORCEWAKE_ALL);
+-	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
++	 */
++	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
+ 
+ 	if (val & LCPLL_POWER_DOWN_ALLOW) {
+ 		val &= ~LCPLL_POWER_DOWN_ALLOW;
+@@ -7525,11 +7888,7 @@
+ 			DRM_ERROR("Switching back to LCPLL failed\n");
+ 	}
+ 
+-	/* See the big comment above. */
+-	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
+-	if (--dev_priv->uncore.forcewake_count == 0)
+-		dev_priv->uncore.funcs.force_wake_put(dev_priv, FORCEWAKE_ALL);
+-	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
++	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
+ }
+ 
+ /*
+@@ -7591,28 +7950,54 @@
+ 	intel_prepare_ddi(dev);
+ }
+ 
+-static void snb_modeset_global_resources(struct drm_device *dev)
++static int haswell_crtc_compute_clock(struct intel_crtc *crtc)
+ {
+-	modeset_update_crtc_power_domains(dev);
+-}
++	if (!intel_ddi_pll_select(crtc))
++		return -EINVAL;
+ 
+-static void haswell_modeset_global_resources(struct drm_device *dev)
+-{
+-	modeset_update_crtc_power_domains(dev);
++	crtc->lowfreq_avail = false;
++
++	return 0;
+ }
+ 
+-static int haswell_crtc_mode_set(struct drm_crtc *crtc,
+-				 int x, int y,
+-				 struct drm_framebuffer *fb)
++static void skylake_get_ddi_pll(struct drm_i915_private *dev_priv,
++				enum port port,
++				struct intel_crtc_config *pipe_config)
+ {
+-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	u32 temp;
+ 
+-	if (!intel_ddi_pll_select(intel_crtc))
+-		return -EINVAL;
++	temp = I915_READ(DPLL_CTRL2) & DPLL_CTRL2_DDI_CLK_SEL_MASK(port);
++	pipe_config->ddi_pll_sel = temp >> (port * 3 + 1);
++
++	switch (pipe_config->ddi_pll_sel) {
++	case SKL_DPLL1:
++		pipe_config->shared_dpll = DPLL_ID_SKL_DPLL1;
++		break;
++	case SKL_DPLL2:
++		pipe_config->shared_dpll = DPLL_ID_SKL_DPLL2;
++		break;
++	case SKL_DPLL3:
++		pipe_config->shared_dpll = DPLL_ID_SKL_DPLL3;
++		break;
++	default:
++		WARN(1, "Unknown DPLL programmed\n");
++	}
++}
+ 
+-	intel_crtc->lowfreq_avail = false;
++static void haswell_get_ddi_pll(struct drm_i915_private *dev_priv,
++				enum port port,
++				struct intel_crtc_config *pipe_config)
++{
++	pipe_config->ddi_pll_sel = I915_READ(PORT_CLK_SEL(port));
+ 
+-	return 0;
++	switch (pipe_config->ddi_pll_sel) {
++	case PORT_CLK_SEL_WRPLL1:
++		pipe_config->shared_dpll = DPLL_ID_WRPLL1;
++		break;
++	case PORT_CLK_SEL_WRPLL2:
++		pipe_config->shared_dpll = DPLL_ID_WRPLL2;
++		break;
++	}
+ }
+ 
+ static void haswell_get_ddi_port_state(struct intel_crtc *crtc,
+@@ -7628,16 +8013,10 @@
+ 
+ 	port = (tmp & TRANS_DDI_PORT_MASK) >> TRANS_DDI_PORT_SHIFT;
+ 
+-	pipe_config->ddi_pll_sel = I915_READ(PORT_CLK_SEL(port));
+-
+-	switch (pipe_config->ddi_pll_sel) {
+-	case PORT_CLK_SEL_WRPLL1:
+-		pipe_config->shared_dpll = DPLL_ID_WRPLL1;
+-		break;
+-	case PORT_CLK_SEL_WRPLL2:
+-		pipe_config->shared_dpll = DPLL_ID_WRPLL2;
+-		break;
+-	}
++	if (IS_SKYLAKE(dev))
++		skylake_get_ddi_pll(dev_priv, port, pipe_config);
++	else
++		haswell_get_ddi_pll(dev_priv, port, pipe_config);
+ 
+ 	if (pipe_config->shared_dpll >= 0) {
+ 		pll = &dev_priv->shared_dplls[pipe_config->shared_dpll];
+@@ -7651,7 +8030,8 @@
+ 	 * DDI E. So just check whether this pipe is wired to DDI E and whether
+ 	 * the PCH transcoder is on.
+ 	 */
+-	if ((port == PORT_E) && I915_READ(LPT_TRANSCONF) & TRANS_ENABLE) {
++	if (INTEL_INFO(dev)->gen < 9 &&
++	    (port == PORT_E) && I915_READ(LPT_TRANSCONF) & TRANS_ENABLE) {
+ 		pipe_config->has_pch_encoder = true;
+ 
+ 		tmp = I915_READ(FDI_RX_CTL(PIPE_A));
+@@ -7670,7 +8050,7 @@
+ 	enum intel_display_power_domain pfit_domain;
+ 	uint32_t tmp;
+ 
+-	if (!intel_display_power_enabled(dev_priv,
++	if (!intel_display_power_is_enabled(dev_priv,
+ 					 POWER_DOMAIN_PIPE(crtc->pipe)))
+ 		return false;
+ 
+@@ -7699,7 +8079,7 @@
+ 			pipe_config->cpu_transcoder = TRANSCODER_EDP;
+ 	}
+ 
+-	if (!intel_display_power_enabled(dev_priv,
++	if (!intel_display_power_is_enabled(dev_priv,
+ 			POWER_DOMAIN_TRANSCODER(pipe_config->cpu_transcoder)))
+ 		return false;
+ 
+@@ -7712,361 +8092,90 @@
+ 	intel_get_pipe_timings(crtc, pipe_config);
+ 
+ 	pfit_domain = POWER_DOMAIN_PIPE_PANEL_FITTER(crtc->pipe);
+-	if (intel_display_power_enabled(dev_priv, pfit_domain))
+-		ironlake_get_pfit_config(crtc, pipe_config);
++	if (intel_display_power_is_enabled(dev_priv, pfit_domain)) {
++		if (IS_SKYLAKE(dev))
++			skylake_get_pfit_config(crtc, pipe_config);
++		else
++			ironlake_get_pfit_config(crtc, pipe_config);
++	}
+ 
+ 	if (IS_HASWELL(dev))
+ 		pipe_config->ips_enabled = hsw_crtc_supports_ips(crtc) &&
+ 			(I915_READ(IPS_CTL) & IPS_ENABLE);
+ 
+-	pipe_config->pixel_multiplier = 1;
++	if (pipe_config->cpu_transcoder != TRANSCODER_EDP) {
++		pipe_config->pixel_multiplier =
++			I915_READ(PIPE_MULT(pipe_config->cpu_transcoder)) + 1;
++	} else {
++		pipe_config->pixel_multiplier = 1;
++	}
+ 
+ 	return true;
+ }
+ 
+-static struct {
+-	int clock;
+-	u32 config;
+-} hdmi_audio_clock[] = {
+-	{ DIV_ROUND_UP(25200 * 1000, 1001), AUD_CONFIG_PIXEL_CLOCK_HDMI_25175 },
+-	{ 25200, AUD_CONFIG_PIXEL_CLOCK_HDMI_25200 }, /* default per bspec */
+-	{ 27000, AUD_CONFIG_PIXEL_CLOCK_HDMI_27000 },
+-	{ 27000 * 1001 / 1000, AUD_CONFIG_PIXEL_CLOCK_HDMI_27027 },
+-	{ 54000, AUD_CONFIG_PIXEL_CLOCK_HDMI_54000 },
+-	{ 54000 * 1001 / 1000, AUD_CONFIG_PIXEL_CLOCK_HDMI_54054 },
+-	{ DIV_ROUND_UP(74250 * 1000, 1001), AUD_CONFIG_PIXEL_CLOCK_HDMI_74176 },
+-	{ 74250, AUD_CONFIG_PIXEL_CLOCK_HDMI_74250 },
+-	{ DIV_ROUND_UP(148500 * 1000, 1001), AUD_CONFIG_PIXEL_CLOCK_HDMI_148352 },
+-	{ 148500, AUD_CONFIG_PIXEL_CLOCK_HDMI_148500 },
+-};
+-
+-/* get AUD_CONFIG_PIXEL_CLOCK_HDMI_* value for mode */
+-static u32 audio_config_hdmi_pixel_clock(struct drm_display_mode *mode)
++static void i845_update_cursor(struct drm_crtc *crtc, u32 base)
+ {
+-	int i;
++	struct drm_device *dev = crtc->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	uint32_t cntl = 0, size = 0;
+ 
+-	for (i = 0; i < ARRAY_SIZE(hdmi_audio_clock); i++) {
+-		if (mode->clock == hdmi_audio_clock[i].clock)
++	if (base) {
++		unsigned int width = intel_crtc->cursor_width;
++		unsigned int height = intel_crtc->cursor_height;
++		unsigned int stride = roundup_pow_of_two(width) * 4;
++
++		switch (stride) {
++		default:
++			WARN_ONCE(1, "Invalid cursor width/stride, width=%u, stride=%u\n",
++				  width, stride);
++			stride = 256;
++			/* fallthrough */
++		case 256:
++		case 512:
++		case 1024:
++		case 2048:
+ 			break;
++		}
++
++		cntl |= CURSOR_ENABLE |
++			CURSOR_GAMMA_ENABLE |
++			CURSOR_FORMAT_ARGB |
++			CURSOR_STRIDE(stride);
++
++		size = (height << 12) | width;
++	}
++
++	if (intel_crtc->cursor_cntl != 0 &&
++	    (intel_crtc->cursor_base != base ||
++	     intel_crtc->cursor_size != size ||
++	     intel_crtc->cursor_cntl != cntl)) {
++		/* On these chipsets we can only modify the base/size/stride
++		 * whilst the cursor is disabled.
++		 */
++		I915_WRITE(_CURACNTR, 0);
++		POSTING_READ(_CURACNTR);
++		intel_crtc->cursor_cntl = 0;
+ 	}
+ 
+-	if (i == ARRAY_SIZE(hdmi_audio_clock)) {
+-		DRM_DEBUG_KMS("HDMI audio pixel clock setting for %d not found, falling back to defaults\n", mode->clock);
+-		i = 1;
++	if (intel_crtc->cursor_base != base) {
++		I915_WRITE(_CURABASE, base);
++		intel_crtc->cursor_base = base;
+ 	}
+ 
+-	DRM_DEBUG_KMS("Configuring HDMI audio for pixel clock %d (0x%08x)\n",
+-		      hdmi_audio_clock[i].clock,
+-		      hdmi_audio_clock[i].config);
++	if (intel_crtc->cursor_size != size) {
++		I915_WRITE(CURSIZE, size);
++		intel_crtc->cursor_size = size;
++	}
+ 
+-	return hdmi_audio_clock[i].config;
++	if (intel_crtc->cursor_cntl != cntl) {
++		I915_WRITE(_CURACNTR, cntl);
++		POSTING_READ(_CURACNTR);
++		intel_crtc->cursor_cntl = cntl;
++	}
+ }
+ 
+-static bool intel_eld_uptodate(struct drm_connector *connector,
+-			       int reg_eldv, uint32_t bits_eldv,
+-			       int reg_elda, uint32_t bits_elda,
+-			       int reg_edid)
+-{
+-	struct drm_i915_private *dev_priv = connector->dev->dev_private;
+-	uint8_t *eld = connector->eld;
+-	uint32_t i;
+-
+-	i = I915_READ(reg_eldv);
+-	i &= bits_eldv;
+-
+-	if (!eld[0])
+-		return !i;
+-
+-	if (!i)
+-		return false;
+-
+-	i = I915_READ(reg_elda);
+-	i &= ~bits_elda;
+-	I915_WRITE(reg_elda, i);
+-
+-	for (i = 0; i < eld[2]; i++)
+-		if (I915_READ(reg_edid) != *((uint32_t *)eld + i))
+-			return false;
+-
+-	return true;
+-}
+-
+-static void g4x_write_eld(struct drm_connector *connector,
+-			  struct drm_crtc *crtc,
+-			  struct drm_display_mode *mode)
+-{
+-	struct drm_i915_private *dev_priv = connector->dev->dev_private;
+-	uint8_t *eld = connector->eld;
+-	uint32_t eldv;
+-	uint32_t len;
+-	uint32_t i;
+-
+-	i = I915_READ(G4X_AUD_VID_DID);
+-
+-	if (i == INTEL_AUDIO_DEVBLC || i == INTEL_AUDIO_DEVCL)
+-		eldv = G4X_ELDV_DEVCL_DEVBLC;
+-	else
+-		eldv = G4X_ELDV_DEVCTG;
+-
+-	if (intel_eld_uptodate(connector,
+-			       G4X_AUD_CNTL_ST, eldv,
+-			       G4X_AUD_CNTL_ST, G4X_ELD_ADDR,
+-			       G4X_HDMIW_HDMIEDID))
+-		return;
+-
+-	i = I915_READ(G4X_AUD_CNTL_ST);
+-	i &= ~(eldv | G4X_ELD_ADDR);
+-	len = (i >> 9) & 0x1f;		/* ELD buffer size */
+-	I915_WRITE(G4X_AUD_CNTL_ST, i);
+-
+-	if (!eld[0])
+-		return;
+-
+-	len = min_t(uint8_t, eld[2], len);
+-	DRM_DEBUG_DRIVER("ELD size %d\n", len);
+-	for (i = 0; i < len; i++)
+-		I915_WRITE(G4X_HDMIW_HDMIEDID, *((uint32_t *)eld + i));
+-
+-	i = I915_READ(G4X_AUD_CNTL_ST);
+-	i |= eldv;
+-	I915_WRITE(G4X_AUD_CNTL_ST, i);
+-}
+-
+-static void haswell_write_eld(struct drm_connector *connector,
+-			      struct drm_crtc *crtc,
+-			      struct drm_display_mode *mode)
+-{
+-	struct drm_i915_private *dev_priv = connector->dev->dev_private;
+-	uint8_t *eld = connector->eld;
+-	uint32_t eldv;
+-	uint32_t i;
+-	int len;
+-	int pipe = to_intel_crtc(crtc)->pipe;
+-	int tmp;
+-
+-	int hdmiw_hdmiedid = HSW_AUD_EDID_DATA(pipe);
+-	int aud_cntl_st = HSW_AUD_DIP_ELD_CTRL(pipe);
+-	int aud_config = HSW_AUD_CFG(pipe);
+-	int aud_cntrl_st2 = HSW_AUD_PIN_ELD_CP_VLD;
+-
+-	/* Audio output enable */
+-	DRM_DEBUG_DRIVER("HDMI audio: enable codec\n");
+-	tmp = I915_READ(aud_cntrl_st2);
+-	tmp |= (AUDIO_OUTPUT_ENABLE_A << (pipe * 4));
+-	I915_WRITE(aud_cntrl_st2, tmp);
+-	POSTING_READ(aud_cntrl_st2);
+-
+-	assert_pipe_disabled(dev_priv, to_intel_crtc(crtc)->pipe);
+-
+-	/* Set ELD valid state */
+-	tmp = I915_READ(aud_cntrl_st2);
+-	DRM_DEBUG_DRIVER("HDMI audio: pin eld vld status=0x%08x\n", tmp);
+-	tmp |= (AUDIO_ELD_VALID_A << (pipe * 4));
+-	I915_WRITE(aud_cntrl_st2, tmp);
+-	tmp = I915_READ(aud_cntrl_st2);
+-	DRM_DEBUG_DRIVER("HDMI audio: eld vld status=0x%08x\n", tmp);
+-
+-	/* Enable HDMI mode */
+-	tmp = I915_READ(aud_config);
+-	DRM_DEBUG_DRIVER("HDMI audio: audio conf: 0x%08x\n", tmp);
+-	/* clear N_programing_enable and N_value_index */
+-	tmp &= ~(AUD_CONFIG_N_VALUE_INDEX | AUD_CONFIG_N_PROG_ENABLE);
+-	I915_WRITE(aud_config, tmp);
+-
+-	DRM_DEBUG_DRIVER("ELD on pipe %c\n", pipe_name(pipe));
+-
+-	eldv = AUDIO_ELD_VALID_A << (pipe * 4);
+-
+-	if (intel_pipe_has_type(crtc, INTEL_OUTPUT_DISPLAYPORT)) {
+-		DRM_DEBUG_DRIVER("ELD: DisplayPort detected\n");
+-		eld[5] |= (1 << 2);	/* Conn_Type, 0x1 = DisplayPort */
+-		I915_WRITE(aud_config, AUD_CONFIG_N_VALUE_INDEX); /* 0x1 = DP */
+-	} else {
+-		I915_WRITE(aud_config, audio_config_hdmi_pixel_clock(mode));
+-	}
+-
+-	if (intel_eld_uptodate(connector,
+-			       aud_cntrl_st2, eldv,
+-			       aud_cntl_st, IBX_ELD_ADDRESS,
+-			       hdmiw_hdmiedid))
+-		return;
+-
+-	i = I915_READ(aud_cntrl_st2);
+-	i &= ~eldv;
+-	I915_WRITE(aud_cntrl_st2, i);
+-
+-	if (!eld[0])
+-		return;
+-
+-	i = I915_READ(aud_cntl_st);
+-	i &= ~IBX_ELD_ADDRESS;
+-	I915_WRITE(aud_cntl_st, i);
+-	i = (i >> 29) & DIP_PORT_SEL_MASK;		/* DIP_Port_Select, 0x1 = PortB */
+-	DRM_DEBUG_DRIVER("port num:%d\n", i);
+-
+-	len = min_t(uint8_t, eld[2], 21);	/* 84 bytes of hw ELD buffer */
+-	DRM_DEBUG_DRIVER("ELD size %d\n", len);
+-	for (i = 0; i < len; i++)
+-		I915_WRITE(hdmiw_hdmiedid, *((uint32_t *)eld + i));
+-
+-	i = I915_READ(aud_cntrl_st2);
+-	i |= eldv;
+-	I915_WRITE(aud_cntrl_st2, i);
+-
+-}
+-
+-static void ironlake_write_eld(struct drm_connector *connector,
+-			       struct drm_crtc *crtc,
+-			       struct drm_display_mode *mode)
+-{
+-	struct drm_i915_private *dev_priv = connector->dev->dev_private;
+-	uint8_t *eld = connector->eld;
+-	uint32_t eldv;
+-	uint32_t i;
+-	int len;
+-	int hdmiw_hdmiedid;
+-	int aud_config;
+-	int aud_cntl_st;
+-	int aud_cntrl_st2;
+-	int pipe = to_intel_crtc(crtc)->pipe;
+-
+-	if (HAS_PCH_IBX(connector->dev)) {
+-		hdmiw_hdmiedid = IBX_HDMIW_HDMIEDID(pipe);
+-		aud_config = IBX_AUD_CFG(pipe);
+-		aud_cntl_st = IBX_AUD_CNTL_ST(pipe);
+-		aud_cntrl_st2 = IBX_AUD_CNTL_ST2;
+-	} else if (IS_VALLEYVIEW(connector->dev)) {
+-		hdmiw_hdmiedid = VLV_HDMIW_HDMIEDID(pipe);
+-		aud_config = VLV_AUD_CFG(pipe);
+-		aud_cntl_st = VLV_AUD_CNTL_ST(pipe);
+-		aud_cntrl_st2 = VLV_AUD_CNTL_ST2;
+-	} else {
+-		hdmiw_hdmiedid = CPT_HDMIW_HDMIEDID(pipe);
+-		aud_config = CPT_AUD_CFG(pipe);
+-		aud_cntl_st = CPT_AUD_CNTL_ST(pipe);
+-		aud_cntrl_st2 = CPT_AUD_CNTRL_ST2;
+-	}
+-
+-	DRM_DEBUG_DRIVER("ELD on pipe %c\n", pipe_name(pipe));
+-
+-	if (IS_VALLEYVIEW(connector->dev))  {
+-		struct intel_encoder *intel_encoder;
+-		struct intel_digital_port *intel_dig_port;
+-
+-		intel_encoder = intel_attached_encoder(connector);
+-		intel_dig_port = enc_to_dig_port(&intel_encoder->base);
+-		i = intel_dig_port->port;
+-	} else {
+-		i = I915_READ(aud_cntl_st);
+-		i = (i >> 29) & DIP_PORT_SEL_MASK;
+-		/* DIP_Port_Select, 0x1 = PortB */
+-	}
+-
+-	if (!i) {
+-		DRM_DEBUG_DRIVER("Audio directed to unknown port\n");
+-		/* operate blindly on all ports */
+-		eldv = IBX_ELD_VALIDB;
+-		eldv |= IBX_ELD_VALIDB << 4;
+-		eldv |= IBX_ELD_VALIDB << 8;
+-	} else {
+-		DRM_DEBUG_DRIVER("ELD on port %c\n", port_name(i));
+-		eldv = IBX_ELD_VALIDB << ((i - 1) * 4);
+-	}
+-
+-	if (intel_pipe_has_type(crtc, INTEL_OUTPUT_DISPLAYPORT)) {
+-		DRM_DEBUG_DRIVER("ELD: DisplayPort detected\n");
+-		eld[5] |= (1 << 2);	/* Conn_Type, 0x1 = DisplayPort */
+-		I915_WRITE(aud_config, AUD_CONFIG_N_VALUE_INDEX); /* 0x1 = DP */
+-	} else {
+-		I915_WRITE(aud_config, audio_config_hdmi_pixel_clock(mode));
+-	}
+-
+-	if (intel_eld_uptodate(connector,
+-			       aud_cntrl_st2, eldv,
+-			       aud_cntl_st, IBX_ELD_ADDRESS,
+-			       hdmiw_hdmiedid))
+-		return;
+-
+-	i = I915_READ(aud_cntrl_st2);
+-	i &= ~eldv;
+-	I915_WRITE(aud_cntrl_st2, i);
+-
+-	if (!eld[0])
+-		return;
+-
+-	i = I915_READ(aud_cntl_st);
+-	i &= ~IBX_ELD_ADDRESS;
+-	I915_WRITE(aud_cntl_st, i);
+-
+-	len = min_t(uint8_t, eld[2], 21);	/* 84 bytes of hw ELD buffer */
+-	DRM_DEBUG_DRIVER("ELD size %d\n", len);
+-	for (i = 0; i < len; i++)
+-		I915_WRITE(hdmiw_hdmiedid, *((uint32_t *)eld + i));
+-
+-	i = I915_READ(aud_cntrl_st2);
+-	i |= eldv;
+-	I915_WRITE(aud_cntrl_st2, i);
+-}
+-
+-void intel_write_eld(struct drm_encoder *encoder,
+-		     struct drm_display_mode *mode)
+-{
+-	struct drm_crtc *crtc = encoder->crtc;
+-	struct drm_connector *connector;
+-	struct drm_device *dev = encoder->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-
+-	connector = drm_select_eld(encoder, mode);
+-	if (!connector)
+-		return;
+-
+-	DRM_DEBUG_DRIVER("ELD on [CONNECTOR:%d:%s], [ENCODER:%d:%s]\n",
+-			 connector->base.id,
+-			 connector->name,
+-			 connector->encoder->base.id,
+-			 connector->encoder->name);
+-
+-	connector->eld[6] = drm_av_sync_delay(connector, mode) / 2;
+-
+-	if (dev_priv->display.write_eld)
+-		dev_priv->display.write_eld(connector, crtc, mode);
+-}
+-
+-static void i845_update_cursor(struct drm_crtc *crtc, u32 base)
+-{
+-	struct drm_device *dev = crtc->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+-	uint32_t cntl;
+-
+-	if (base != intel_crtc->cursor_base) {
+-		/* On these chipsets we can only modify the base whilst
+-		 * the cursor is disabled.
+-		 */
+-		if (intel_crtc->cursor_cntl) {
+-			I915_WRITE(_CURACNTR, 0);
+-			POSTING_READ(_CURACNTR);
+-			intel_crtc->cursor_cntl = 0;
+-		}
+-
+-		I915_WRITE(_CURABASE, base);
+-		POSTING_READ(_CURABASE);
+-	}
+-
+-	/* XXX width must be 64, stride 256 => 0x00 << 28 */
+-	cntl = 0;
+-	if (base)
+-		cntl = (CURSOR_ENABLE |
+-			CURSOR_GAMMA_ENABLE |
+-			CURSOR_FORMAT_ARGB);
+-	if (intel_crtc->cursor_cntl != cntl) {
+-		I915_WRITE(_CURACNTR, cntl);
+-		POSTING_READ(_CURACNTR);
+-		intel_crtc->cursor_cntl = cntl;
+-	}
+-}
+-
+-static void i9xx_update_cursor(struct drm_crtc *crtc, u32 base)
++static void i9xx_update_cursor(struct drm_crtc *crtc, u32 base)
+ {
+ 	struct drm_device *dev = crtc->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+@@ -8092,46 +8201,13 @@
+ 				return;
+ 		}
+ 		cntl |= pipe << 28; /* Connect to correct pipe */
+-	}
+-	if (intel_crtc->cursor_cntl != cntl) {
+-		I915_WRITE(CURCNTR(pipe), cntl);
+-		POSTING_READ(CURCNTR(pipe));
+-		intel_crtc->cursor_cntl = cntl;
+-	}
+ 
+-	/* and commit changes on next vblank */
+-	I915_WRITE(CURBASE(pipe), base);
+-	POSTING_READ(CURBASE(pipe));
+-}
+-
+-static void ivb_update_cursor(struct drm_crtc *crtc, u32 base)
+-{
+-	struct drm_device *dev = crtc->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+-	int pipe = intel_crtc->pipe;
+-	uint32_t cntl;
+-
+-	cntl = 0;
+-	if (base) {
+-		cntl = MCURSOR_GAMMA_ENABLE;
+-		switch (intel_crtc->cursor_width) {
+-			case 64:
+-				cntl |= CURSOR_MODE_64_ARGB_AX;
+-				break;
+-			case 128:
+-				cntl |= CURSOR_MODE_128_ARGB_AX;
+-				break;
+-			case 256:
+-				cntl |= CURSOR_MODE_256_ARGB_AX;
+-				break;
+-			default:
+-				WARN_ON(1);
+-				return;
+-		}
++		if (IS_HASWELL(dev) || IS_BROADWELL(dev))
++			cntl |= CURSOR_PIPE_CSC_ENABLE;
+ 	}
+-	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
+-		cntl |= CURSOR_PIPE_CSC_ENABLE;
++
++	if (to_intel_plane(crtc->cursor)->rotation == BIT(DRM_ROTATE_180))
++		cntl |= CURSOR_ROTATE_180;
+ 
+ 	if (intel_crtc->cursor_cntl != cntl) {
+ 		I915_WRITE(CURCNTR(pipe), cntl);
+@@ -8142,6 +8218,8 @@
+ 	/* and commit changes on next vblank */
+ 	I915_WRITE(CURBASE(pipe), base);
+ 	POSTING_READ(CURBASE(pipe));
++
++	intel_crtc->cursor_base = base;
+ }
+ 
+ /* If no-part of the cursor is visible on the framebuffer, then the GPU may hang... */
+@@ -8188,28 +8266,62 @@
+ 
+ 	I915_WRITE(CURPOS(pipe), pos);
+ 
+-	if (IS_IVYBRIDGE(dev) || IS_HASWELL(dev) || IS_BROADWELL(dev))
+-		ivb_update_cursor(crtc, base);
+-	else if (IS_845G(dev) || IS_I865G(dev))
++	/* ILK+ do this automagically */
++	if (HAS_GMCH_DISPLAY(dev) &&
++		to_intel_plane(crtc->cursor)->rotation == BIT(DRM_ROTATE_180)) {
++		base += (intel_crtc->cursor_height *
++			intel_crtc->cursor_width - 1) * 4;
++	}
++
++	if (IS_845G(dev) || IS_I865G(dev))
+ 		i845_update_cursor(crtc, base);
+ 	else
+ 		i9xx_update_cursor(crtc, base);
+-	intel_crtc->cursor_base = base;
+ }
+ 
+-/*
+- * intel_crtc_cursor_set_obj - Set cursor to specified GEM object
+- *
+- * Note that the object's reference will be consumed if the update fails.  If
+- * the update succeeds, the reference of the old object (if any) will be
+- * consumed.
+- */
++static bool cursor_size_ok(struct drm_device *dev,
++			   uint32_t width, uint32_t height)
++{
++	if (width == 0 || height == 0)
++		return false;
++
++	/*
++	 * 845g/865g are special in that they are only limited by
++	 * the width of their cursors, the height is arbitrary up to
++	 * the precision of the register. Everything else requires
++	 * square cursors, limited to a few power-of-two sizes.
++	 */
++	if (IS_845G(dev) || IS_I865G(dev)) {
++		if ((width & 63) != 0)
++			return false;
++
++		if (width > (IS_845G(dev) ? 64 : 512))
++			return false;
++
++		if (height > 1023)
++			return false;
++	} else {
++		switch (width | height) {
++		case 256:
++		case 128:
++			if (IS_GEN2(dev))
++				return false;
++		case 64:
++			break;
++		default:
++			return false;
++		}
++	}
++
++	return true;
++}
++
+ static int intel_crtc_cursor_set_obj(struct drm_crtc *crtc,
+ 				     struct drm_i915_gem_object *obj,
+ 				     uint32_t width, uint32_t height)
+ {
+ 	struct drm_device *dev = crtc->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_i915_private *dev_priv = to_i915(dev);
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+ 	enum pipe pipe = intel_crtc->pipe;
+ 	unsigned old_width;
+@@ -8220,36 +8332,15 @@
+ 	if (!obj) {
+ 		DRM_DEBUG_KMS("cursor off\n");
+ 		addr = 0;
+-		obj = NULL;
+ 		mutex_lock(&dev->struct_mutex);
+ 		goto finish;
+ 	}
+ 
+-	/* Check for which cursor types we support */
+-	if (!((width == 64 && height == 64) ||
+-			(width == 128 && height == 128 && !IS_GEN2(dev)) ||
+-			(width == 256 && height == 256 && !IS_GEN2(dev)))) {
+-		DRM_DEBUG("Cursor dimension not supported\n");
+-		return -EINVAL;
+-	}
+-
+-	if (obj->base.size < width * height * 4) {
+-		DRM_DEBUG_KMS("buffer is too small\n");
+-		ret = -ENOMEM;
+-		goto fail;
+-	}
+-
+ 	/* we only need to pin inside GTT if cursor is non-phy */
+ 	mutex_lock(&dev->struct_mutex);
+ 	if (!INTEL_INFO(dev)->cursor_needs_physical) {
+ 		unsigned alignment;
+ 
+-		if (obj->tiling_mode) {
+-			DRM_DEBUG_KMS("cursor cannot be tiled\n");
+-			ret = -EINVAL;
+-			goto fail_locked;
+-		}
+-
+ 		/*
+ 		 * Global gtt pte registers are special registers which actually
+ 		 * forward writes to a chunk of system memory. Which means that
+@@ -8295,9 +8386,6 @@
+ 		addr = obj->phys_handle->busaddr;
+ 	}
+ 
+-	if (IS_GEN2(dev))
+-		I915_WRITE(CURSIZE, (height << 12) | width);
+-
+  finish:
+ 	if (intel_crtc->cursor_bo) {
+ 		if (!INTEL_INFO(dev)->cursor_needs_physical)
+@@ -8319,17 +8407,15 @@
+ 		if (old_width != width)
+ 			intel_update_watermarks(crtc);
+ 		intel_crtc_update_cursor(crtc, intel_crtc->cursor_bo != NULL);
+-	}
+ 
+-	intel_frontbuffer_flip(dev, INTEL_FRONTBUFFER_CURSOR(pipe));
++		intel_frontbuffer_flip(dev, INTEL_FRONTBUFFER_CURSOR(pipe));
++	}
+ 
+ 	return 0;
+ fail_unpin:
+ 	i915_gem_object_unpin_from_display_plane(obj);
+ fail_locked:
+ 	mutex_unlock(&dev->struct_mutex);
+-fail:
+-	drm_gem_object_unreference_unlocked(&obj->base);
+ 	return ret;
+ }
+ 
+@@ -8364,7 +8450,7 @@
+ 
+ 	intel_fb = kzalloc(sizeof(*intel_fb), GFP_KERNEL);
+ 	if (!intel_fb) {
+-		drm_gem_object_unreference_unlocked(&obj->base);
++		drm_gem_object_unreference(&obj->base);
+ 		return ERR_PTR(-ENOMEM);
+ 	}
+ 
+@@ -8374,7 +8460,7 @@
+ 
+ 	return &intel_fb->base;
+ err:
+-	drm_gem_object_unreference_unlocked(&obj->base);
++	drm_gem_object_unreference(&obj->base);
+ 	kfree(intel_fb);
+ 
+ 	return ERR_PTR(ret);
+@@ -8507,6 +8593,9 @@
+ 		ret = drm_modeset_lock(&crtc->mutex, ctx);
+ 		if (ret)
+ 			goto fail_unlock;
++		ret = drm_modeset_lock(&crtc->primary->mutex, ctx);
++		if (ret)
++			goto fail_unlock;
+ 
+ 		old->dpms_mode = connector->dpms;
+ 		old->load_detect_temp = false;
+@@ -8544,6 +8633,9 @@
+ 	ret = drm_modeset_lock(&crtc->mutex, ctx);
+ 	if (ret)
+ 		goto fail_unlock;
++	ret = drm_modeset_lock(&crtc->primary->mutex, ctx);
++	if (ret)
++		goto fail_unlock;
+ 	intel_encoder->new_crtc = to_intel_crtc(crtc);
+ 	to_intel_connector(connector)->new_encoder = intel_encoder;
+ 
+@@ -8826,35 +8918,6 @@
+ 	return mode;
+ }
+ 
+-static void intel_increase_pllclock(struct drm_device *dev,
+-				    enum pipe pipe)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	int dpll_reg = DPLL(pipe);
+-	int dpll;
+-
+-	if (!HAS_GMCH_DISPLAY(dev))
+-		return;
+-
+-	if (!dev_priv->lvds_downclock_avail)
+-		return;
+-
+-	dpll = I915_READ(dpll_reg);
+-	if (!HAS_PIPE_CXSR(dev) && (dpll & DISPLAY_RATE_SELECT_FPA1)) {
+-		DRM_DEBUG_DRIVER("upclocking LVDS\n");
+-
+-		assert_panel_unlocked(dev_priv, pipe);
+-
+-		dpll &= ~DISPLAY_RATE_SELECT_FPA1;
+-		I915_WRITE(dpll_reg, dpll);
+-		intel_wait_for_vblank(dev, pipe);
+-
+-		dpll = I915_READ(dpll_reg);
+-		if (dpll & DISPLAY_RATE_SELECT_FPA1)
+-			DRM_DEBUG_DRIVER("failed to upclock LVDS!\n");
+-	}
+-}
+-
+ static void intel_decrease_pllclock(struct drm_crtc *crtc)
+ {
+ 	struct drm_device *dev = crtc->dev;
+@@ -8900,6 +8963,8 @@
+ 
+ 	intel_runtime_pm_get(dev_priv);
+ 	i915_update_gfx_val(dev_priv);
++	if (INTEL_INFO(dev)->gen >= 6)
++		gen6_rps_busy(dev_priv);
+ 	dev_priv->mm.busy = true;
+ }
+ 
+@@ -8913,7 +8978,7 @@
+ 
+ 	dev_priv->mm.busy = false;
+ 
+-	if (!i915.powersave)
++	if (!i915_module.powersave)
+ 		goto out;
+ 
+ 	for_each_crtc(dev, crtc) {
+@@ -8930,190 +8995,16 @@
+ 	intel_runtime_pm_put(dev_priv);
+ }
+ 
+-
+-/**
+- * intel_mark_fb_busy - mark given planes as busy
+- * @dev: DRM device
+- * @frontbuffer_bits: bits for the affected planes
+- * @ring: optional ring for asynchronous commands
+- *
+- * This function gets called every time the screen contents change. It can be
+- * used to keep e.g. the update rate at the nominal refresh rate with DRRS.
+- */
+-static void intel_mark_fb_busy(struct drm_device *dev,
+-			       unsigned frontbuffer_bits,
+-			       struct intel_engine_cs *ring)
+-{
+-	enum pipe pipe;
+-
+-	if (!i915.powersave)
+-		return;
+-
+-	for_each_pipe(pipe) {
+-		if (!(frontbuffer_bits & INTEL_FRONTBUFFER_ALL_MASK(pipe)))
+-			continue;
+-
+-		intel_increase_pllclock(dev, pipe);
+-		if (ring && intel_fbc_enabled(dev))
+-			ring->fbc_dirty = true;
+-	}
+-}
+-
+-/**
+- * intel_fb_obj_invalidate - invalidate frontbuffer object
+- * @obj: GEM object to invalidate
+- * @ring: set for asynchronous rendering
+- *
+- * This function gets called every time rendering on the given object starts and
+- * frontbuffer caching (fbc, low refresh rate for DRRS, panel self refresh) must
+- * be invalidated. If @ring is non-NULL any subsequent invalidation will be delayed
+- * until the rendering completes or a flip on this frontbuffer plane is
+- * scheduled.
+- */
+-void intel_fb_obj_invalidate(struct drm_i915_gem_object *obj,
+-			     struct intel_engine_cs *ring)
+-{
+-	struct drm_device *dev = obj->base.dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-
+-	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
+-
+-	if (!obj->frontbuffer_bits)
+-		return;
+-
+-	if (ring) {
+-		mutex_lock(&dev_priv->fb_tracking.lock);
+-		dev_priv->fb_tracking.busy_bits
+-			|= obj->frontbuffer_bits;
+-		dev_priv->fb_tracking.flip_bits
+-			&= ~obj->frontbuffer_bits;
+-		mutex_unlock(&dev_priv->fb_tracking.lock);
+-	}
+-
+-	intel_mark_fb_busy(dev, obj->frontbuffer_bits, ring);
+-
+-	intel_edp_psr_invalidate(dev, obj->frontbuffer_bits);
+-}
+-
+-/**
+- * intel_frontbuffer_flush - flush frontbuffer
+- * @dev: DRM device
+- * @frontbuffer_bits: frontbuffer plane tracking bits
+- *
+- * This function gets called every time rendering on the given planes has
+- * completed and frontbuffer caching can be started again. Flushes will get
+- * delayed if they're blocked by some oustanding asynchronous rendering.
+- *
+- * Can be called without any locks held.
+- */
+-void intel_frontbuffer_flush(struct drm_device *dev,
+-			     unsigned frontbuffer_bits)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-
+-	/* Delay flushing when rings are still busy.*/
+-	mutex_lock(&dev_priv->fb_tracking.lock);
+-	frontbuffer_bits &= ~dev_priv->fb_tracking.busy_bits;
+-	mutex_unlock(&dev_priv->fb_tracking.lock);
+-
+-	intel_mark_fb_busy(dev, frontbuffer_bits, NULL);
+-
+-	intel_edp_psr_flush(dev, frontbuffer_bits);
+-}
+-
+-/**
+- * intel_fb_obj_flush - flush frontbuffer object
+- * @obj: GEM object to flush
+- * @retire: set when retiring asynchronous rendering
+- *
+- * This function gets called every time rendering on the given object has
+- * completed and frontbuffer caching can be started again. If @retire is true
+- * then any delayed flushes will be unblocked.
+- */
+-void intel_fb_obj_flush(struct drm_i915_gem_object *obj,
+-			bool retire)
+-{
+-	struct drm_device *dev = obj->base.dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	unsigned frontbuffer_bits;
+-
+-	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
+-
+-	if (!obj->frontbuffer_bits)
+-		return;
+-
+-	frontbuffer_bits = obj->frontbuffer_bits;
+-
+-	if (retire) {
+-		mutex_lock(&dev_priv->fb_tracking.lock);
+-		/* Filter out new bits since rendering started. */
+-		frontbuffer_bits &= dev_priv->fb_tracking.busy_bits;
+-
+-		dev_priv->fb_tracking.busy_bits &= ~frontbuffer_bits;
+-		mutex_unlock(&dev_priv->fb_tracking.lock);
+-	}
+-
+-	intel_frontbuffer_flush(dev, frontbuffer_bits);
+-}
+-
+-/**
+- * intel_frontbuffer_flip_prepare - prepare asnychronous frontbuffer flip
+- * @dev: DRM device
+- * @frontbuffer_bits: frontbuffer plane tracking bits
+- *
+- * This function gets called after scheduling a flip on @obj. The actual
+- * frontbuffer flushing will be delayed until completion is signalled with
+- * intel_frontbuffer_flip_complete. If an invalidate happens in between this
+- * flush will be cancelled.
+- *
+- * Can be called without any locks held.
+- */
+-void intel_frontbuffer_flip_prepare(struct drm_device *dev,
+-				    unsigned frontbuffer_bits)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-
+-	mutex_lock(&dev_priv->fb_tracking.lock);
+-	dev_priv->fb_tracking.flip_bits
+-		|= frontbuffer_bits;
+-	mutex_unlock(&dev_priv->fb_tracking.lock);
+-}
+-
+-/**
+- * intel_frontbuffer_flip_complete - complete asynchronous frontbuffer flush
+- * @dev: DRM device
+- * @frontbuffer_bits: frontbuffer plane tracking bits
+- *
+- * This function gets called after the flip has been latched and will complete
+- * on the next vblank. It will execute the fush if it hasn't been cancalled yet.
+- *
+- * Can be called without any locks held.
+- */
+-void intel_frontbuffer_flip_complete(struct drm_device *dev,
+-				     unsigned frontbuffer_bits)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-
+-	mutex_lock(&dev_priv->fb_tracking.lock);
+-	/* Mask any cancelled flips. */
+-	frontbuffer_bits &= dev_priv->fb_tracking.flip_bits;
+-	dev_priv->fb_tracking.flip_bits &= ~frontbuffer_bits;
+-	mutex_unlock(&dev_priv->fb_tracking.lock);
+-
+-	intel_frontbuffer_flush(dev, frontbuffer_bits);
+-}
+-
+ static void intel_crtc_destroy(struct drm_crtc *crtc)
+ {
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+ 	struct drm_device *dev = crtc->dev;
+ 	struct intel_unpin_work *work;
+-	unsigned long flags;
+ 
+-	spin_lock_irqsave(&dev->event_lock, flags);
++	spin_lock_irq(&dev->event_lock);
+ 	work = intel_crtc->unpin_work;
+ 	intel_crtc->unpin_work = NULL;
+-	spin_unlock_irqrestore(&dev->event_lock, flags);
++	spin_unlock_irq(&dev->event_lock);
+ 
+ 	if (work) {
+ 		cancel_work_sync(&work->work);
+@@ -9136,6 +9027,7 @@
+ 	intel_unpin_fb_obj(work->old_fb_obj);
+ 	drm_gem_object_unreference(&work->pending_flip_obj->base);
+ 	drm_gem_object_unreference(&work->old_fb_obj->base);
++	i915_request_put(work->flip_queued_request);
+ 
+ 	intel_update_fbc(dev);
+ 	mutex_unlock(&dev->struct_mutex);
+@@ -9151,7 +9043,6 @@
+ static void do_intel_finish_page_flip(struct drm_device *dev,
+ 				      struct drm_crtc *crtc)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+ 	struct intel_unpin_work *work;
+ 	unsigned long flags;
+@@ -9160,6 +9051,10 @@
+ 	if (intel_crtc == NULL)
+ 		return;
+ 
++	/*
++	 * This is called both by irq handlers and the reset code (to complete
++	 * lost pageflips) so needs the full irqsave spinlocks.
++	 */
+ 	spin_lock_irqsave(&dev->event_lock, flags);
+ 	work = intel_crtc->unpin_work;
+ 
+@@ -9171,23 +9066,9 @@
+ 		return;
+ 	}
+ 
+-	/* and that the unpin work is consistent wrt ->pending. */
+-	smp_rmb();
+-
+-	intel_crtc->unpin_work = NULL;
+-
+-	if (work->event)
+-		drm_send_vblank_event(dev, intel_crtc->pipe, work->event);
+-
+-	drm_crtc_vblank_put(crtc);
++	page_flip_completed(intel_crtc);
+ 
+ 	spin_unlock_irqrestore(&dev->event_lock, flags);
+-
+-	wake_up_all(&dev_priv->pending_flip_queue);
+-
+-	queue_work(dev_priv->wq, &work->work);
+-
+-	trace_i915_flip_complete(intel_crtc->plane, work->pending_flip_obj);
+ }
+ 
+ void intel_finish_page_flip(struct drm_device *dev, int pipe)
+@@ -9255,7 +9136,12 @@
+ 		to_intel_crtc(dev_priv->plane_to_crtc_mapping[plane]);
+ 	unsigned long flags;
+ 
+-	/* NB: An MMIO update of the plane base pointer will also
++
++	/*
++	 * This is called both by irq handlers and the reset code (to complete
++	 * lost pageflips) so needs the full irqsave spinlocks.
++	 *
++	 * NB: An MMIO update of the plane base pointer will also
+ 	 * generate a page-flip completion irq, i.e. every modeset
+ 	 * is also accompanied by a spurious intel_prepare_page_flip().
+ 	 */
+@@ -9274,97 +9160,86 @@
+ 	smp_wmb();
+ }
+ 
+-static int intel_gen2_queue_flip(struct drm_device *dev,
+-				 struct drm_crtc *crtc,
++static int intel_gen2_queue_flip(struct i915_gem_request *rq,
++				 struct intel_crtc *crtc,
+ 				 struct drm_framebuffer *fb,
+ 				 struct drm_i915_gem_object *obj,
+-				 struct intel_engine_cs *ring,
+ 				 uint32_t flags)
+ {
+-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	struct intel_ringbuffer *ring;
+ 	u32 flip_mask;
+-	int ret;
+ 
+-	ret = intel_ring_begin(ring, 6);
+-	if (ret)
+-		return ret;
++	ring = intel_ring_begin(rq, 5);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+ 	/* Can't queue multiple flips, so wait for the previous
+ 	 * one to finish before executing the next.
+ 	 */
+-	if (intel_crtc->plane)
++	if (crtc->plane)
+ 		flip_mask = MI_WAIT_FOR_PLANE_B_FLIP;
+ 	else
+ 		flip_mask = MI_WAIT_FOR_PLANE_A_FLIP;
+ 	intel_ring_emit(ring, MI_WAIT_FOR_EVENT | flip_mask);
+-	intel_ring_emit(ring, MI_NOOP);
+ 	intel_ring_emit(ring, MI_DISPLAY_FLIP |
+-			MI_DISPLAY_FLIP_PLANE(intel_crtc->plane));
++			MI_DISPLAY_FLIP_PLANE(crtc->plane));
+ 	intel_ring_emit(ring, fb->pitches[0]);
+-	intel_ring_emit(ring, intel_crtc->unpin_work->gtt_offset);
++	intel_ring_emit(ring, crtc->unpin_work->gtt_offset);
+ 	intel_ring_emit(ring, 0); /* aux display base address, unused */
++	intel_ring_advance(ring);
+ 
+-	intel_mark_page_flip_active(intel_crtc);
+-	__intel_ring_advance(ring);
+ 	return 0;
+ }
+ 
+-static int intel_gen3_queue_flip(struct drm_device *dev,
+-				 struct drm_crtc *crtc,
++static int intel_gen3_queue_flip(struct i915_gem_request *rq,
++				 struct intel_crtc *crtc,
+ 				 struct drm_framebuffer *fb,
+ 				 struct drm_i915_gem_object *obj,
+-				 struct intel_engine_cs *ring,
+ 				 uint32_t flags)
+ {
+-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	struct intel_ringbuffer *ring;
+ 	u32 flip_mask;
+-	int ret;
+ 
+-	ret = intel_ring_begin(ring, 6);
+-	if (ret)
+-		return ret;
++	ring = intel_ring_begin(rq, 4);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+-	if (intel_crtc->plane)
++	if (crtc->plane)
+ 		flip_mask = MI_WAIT_FOR_PLANE_B_FLIP;
+ 	else
+ 		flip_mask = MI_WAIT_FOR_PLANE_A_FLIP;
+ 	intel_ring_emit(ring, MI_WAIT_FOR_EVENT | flip_mask);
+-	intel_ring_emit(ring, MI_NOOP);
+ 	intel_ring_emit(ring, MI_DISPLAY_FLIP_I915 |
+-			MI_DISPLAY_FLIP_PLANE(intel_crtc->plane));
++			MI_DISPLAY_FLIP_PLANE(crtc->plane));
+ 	intel_ring_emit(ring, fb->pitches[0]);
+-	intel_ring_emit(ring, intel_crtc->unpin_work->gtt_offset);
+-	intel_ring_emit(ring, MI_NOOP);
++	intel_ring_emit(ring, crtc->unpin_work->gtt_offset);
++	intel_ring_advance(ring);
+ 
+-	intel_mark_page_flip_active(intel_crtc);
+-	__intel_ring_advance(ring);
+ 	return 0;
+ }
+ 
+-static int intel_gen4_queue_flip(struct drm_device *dev,
+-				 struct drm_crtc *crtc,
++static int intel_gen4_queue_flip(struct i915_gem_request *rq,
++				 struct intel_crtc *crtc,
+ 				 struct drm_framebuffer *fb,
+ 				 struct drm_i915_gem_object *obj,
+-				 struct intel_engine_cs *ring,
+ 				 uint32_t flags)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	struct drm_i915_private *dev_priv = rq->i915;
++	struct intel_ringbuffer *ring;
+ 	uint32_t pf, pipesrc;
+-	int ret;
+ 
+-	ret = intel_ring_begin(ring, 4);
+-	if (ret)
+-		return ret;
++	ring = intel_ring_begin(rq, 4);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+ 	/* i965+ uses the linear or tiled offsets from the
+ 	 * Display Registers (which do not change across a page-flip)
+ 	 * so we need only reprogram the base address.
+ 	 */
+ 	intel_ring_emit(ring, MI_DISPLAY_FLIP |
+-			MI_DISPLAY_FLIP_PLANE(intel_crtc->plane));
++			MI_DISPLAY_FLIP_PLANE(crtc->plane));
+ 	intel_ring_emit(ring, fb->pitches[0]);
+-	intel_ring_emit(ring, intel_crtc->unpin_work->gtt_offset |
++	intel_ring_emit(ring, crtc->unpin_work->gtt_offset |
+ 			obj->tiling_mode);
+ 
+ 	/* XXX Enabling the panel-fitter across page-flip is so far
+@@ -9372,62 +9247,53 @@
+ 	 * pf = I915_READ(pipe == 0 ? PFA_CTL_1 : PFB_CTL_1) & PF_ENABLE;
+ 	 */
+ 	pf = 0;
+-	pipesrc = I915_READ(PIPESRC(intel_crtc->pipe)) & 0x0fff0fff;
++	pipesrc = I915_READ(PIPESRC(crtc->pipe)) & 0x0fff0fff;
+ 	intel_ring_emit(ring, pf | pipesrc);
++	intel_ring_advance(ring);
+ 
+-	intel_mark_page_flip_active(intel_crtc);
+-	__intel_ring_advance(ring);
+ 	return 0;
+ }
+ 
+-static int intel_gen6_queue_flip(struct drm_device *dev,
+-				 struct drm_crtc *crtc,
++static int intel_gen6_queue_flip(struct i915_gem_request *rq,
++				 struct intel_crtc *crtc,
+ 				 struct drm_framebuffer *fb,
+ 				 struct drm_i915_gem_object *obj,
+-				 struct intel_engine_cs *ring,
+ 				 uint32_t flags)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+-	uint32_t pf, pipesrc;
+-	int ret;
++	struct intel_ringbuffer *ring;
++	u32 cmd, base;
+ 
+-	ret = intel_ring_begin(ring, 4);
+-	if (ret)
+-		return ret;
++	ring = intel_ring_begin(rq, 3);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+-	intel_ring_emit(ring, MI_DISPLAY_FLIP |
+-			MI_DISPLAY_FLIP_PLANE(intel_crtc->plane));
+-	intel_ring_emit(ring, fb->pitches[0] | obj->tiling_mode);
+-	intel_ring_emit(ring, intel_crtc->unpin_work->gtt_offset);
++	cmd = MI_DISPLAY_FLIP_I915 | MI_DISPLAY_FLIP_PLANE(crtc->plane);
++	base = crtc->unpin_work->gtt_offset;
++	if (flags & DRM_MODE_PAGE_FLIP_ASYNC) {
++		cmd |= MI_DISPLAY_FLIP_ASYNC;
++		base |= MI_DISPLAY_FLIP_TYPE_ASYNC;
++	}
+ 
+-	/* Contrary to the suggestions in the documentation,
+-	 * "Enable Panel Fitter" does not seem to be required when page
+-	 * flipping with a non-native mode, and worse causes a normal
+-	 * modeset to fail.
+-	 * pf = I915_READ(PF_CTL(intel_crtc->pipe)) & PF_ENABLE;
+-	 */
+-	pf = 0;
+-	pipesrc = I915_READ(PIPESRC(intel_crtc->pipe)) & 0x0fff0fff;
+-	intel_ring_emit(ring, pf | pipesrc);
++	intel_ring_emit(ring, cmd);
++	intel_ring_emit(ring, fb->pitches[0] | obj->tiling_mode);
++	intel_ring_emit(ring, base);
++	intel_ring_advance(ring);
+ 
+-	intel_mark_page_flip_active(intel_crtc);
+-	__intel_ring_advance(ring);
+ 	return 0;
+ }
+ 
+-static int intel_gen7_queue_flip(struct drm_device *dev,
+-				 struct drm_crtc *crtc,
++static int intel_gen7_queue_flip(struct i915_gem_request *rq,
++				 struct intel_crtc *crtc,
+ 				 struct drm_framebuffer *fb,
+ 				 struct drm_i915_gem_object *obj,
+-				 struct intel_engine_cs *ring,
+ 				 uint32_t flags)
+ {
+-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	struct intel_ringbuffer *ring;
+ 	uint32_t plane_bit = 0;
++	u32 cmd, base;
+ 	int len, ret;
+ 
+-	switch (intel_crtc->plane) {
++	switch (crtc->plane) {
+ 	case PLANE_A:
+ 		plane_bit = MI_DISPLAY_FLIP_IVB_PLANE_A;
+ 		break;
+@@ -9442,16 +9308,16 @@
+ 		return -ENODEV;
+ 	}
+ 
+-	len = 4;
+-	if (ring->id == RCS) {
++	len = 3;
++	if (rq->engine->id == RCS) {
+ 		len += 6;
+ 		/*
+ 		 * On Gen 8, SRM is now taking an extra dword to accommodate
+ 		 * 48bits addresses, and we need a NOOP for the batch size to
+ 		 * stay even.
+ 		 */
+-		if (IS_GEN8(dev))
+-			len += 2;
++		if (IS_GEN8(rq->i915))
++			len += 1;
+ 	}
+ 
+ 	/*
+@@ -9464,13 +9330,13 @@
+ 	 * then do the cacheline alignment, and finally emit the
+ 	 * MI_DISPLAY_FLIP.
+ 	 */
+-	ret = intel_ring_cacheline_align(ring);
++	ret = intel_ring_cacheline_align(rq);
+ 	if (ret)
+ 		return ret;
+ 
+-	ret = intel_ring_begin(ring, len);
+-	if (ret)
+-		return ret;
++	ring = intel_ring_begin(rq, len);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+ 	/* Unmask the flip-done completion message. Note that the bspec says that
+ 	 * we should do this for both the BCS and RCS, and that we must not unmask
+@@ -9481,38 +9347,102 @@
+ 	 * for the RCS also doesn't appear to drop events. Setting the DERRMR
+ 	 * to zero does lead to lockups within MI_DISPLAY_FLIP.
+ 	 */
+-	if (ring->id == RCS) {
++	if (rq->engine->id == RCS) {
+ 		intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
+ 		intel_ring_emit(ring, DERRMR);
+ 		intel_ring_emit(ring, ~(DERRMR_PIPEA_PRI_FLIP_DONE |
+ 					DERRMR_PIPEB_PRI_FLIP_DONE |
+ 					DERRMR_PIPEC_PRI_FLIP_DONE));
+-		if (IS_GEN8(dev))
++		if (IS_GEN8(rq->i915))
+ 			intel_ring_emit(ring, MI_STORE_REGISTER_MEM_GEN8(1) |
+ 					      MI_SRM_LRM_GLOBAL_GTT);
+ 		else
+ 			intel_ring_emit(ring, MI_STORE_REGISTER_MEM(1) |
+ 					      MI_SRM_LRM_GLOBAL_GTT);
+ 		intel_ring_emit(ring, DERRMR);
+-		intel_ring_emit(ring, ring->scratch.gtt_offset + 256);
+-		if (IS_GEN8(dev)) {
++		intel_ring_emit(ring, rq->engine->scratch.gtt_offset + 256);
++		if (IS_GEN8(rq->i915))
+ 			intel_ring_emit(ring, 0);
+-			intel_ring_emit(ring, MI_NOOP);
+-		}
+ 	}
+ 
+-	intel_ring_emit(ring, MI_DISPLAY_FLIP_I915 | plane_bit);
+-	intel_ring_emit(ring, (fb->pitches[0] | obj->tiling_mode));
+-	intel_ring_emit(ring, intel_crtc->unpin_work->gtt_offset);
+-	intel_ring_emit(ring, (MI_NOOP));
++	cmd = MI_DISPLAY_FLIP_I915 | plane_bit;
++	base = crtc->unpin_work->gtt_offset;
++	if (flags & DRM_MODE_PAGE_FLIP_ASYNC) {
++		cmd |= MI_DISPLAY_FLIP_ASYNC;
++		base |= MI_DISPLAY_FLIP_TYPE_ASYNC;
++	}
++
++	intel_ring_emit(ring, cmd);
++	intel_ring_emit(ring, fb->pitches[0] | obj->tiling_mode);
++	intel_ring_emit(ring, base);
++	intel_ring_advance(ring);
++
++	return 0;
++}
++
++static int intel_gen9_queue_flip(struct i915_gem_request *rq,
++				 struct intel_crtc *crtc,
++				 struct drm_framebuffer *fb,
++				 struct drm_i915_gem_object *obj,
++				 uint32_t flags)
++{
++	struct intel_ringbuffer *ring;
++	uint32_t plane = 0, stride;
++
++	switch (crtc->pipe) {
++	case PIPE_A:
++		plane = MI_DISPLAY_FLIP_SKL_PLANE_1_A;
++		break;
++	case PIPE_B:
++		plane = MI_DISPLAY_FLIP_SKL_PLANE_1_B;
++		break;
++	case PIPE_C:
++		plane = MI_DISPLAY_FLIP_SKL_PLANE_1_C;
++		break;
++	default:
++		WARN_ONCE(1, "unknown plane in flip command\n");
++		return -ENODEV;
++	}
++
++	switch (obj->tiling_mode) {
++	case I915_TILING_NONE:
++		stride = fb->pitches[0] >> 6;
++		break;
++	case I915_TILING_X:
++		stride = fb->pitches[0] >> 9;
++		break;
++	default:
++		WARN_ONCE(1, "unknown tiling in flip command\n");
++		return -ENODEV;
++	}
++
++	ring = intel_ring_begin(rq, 10);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
++
++	intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
++	intel_ring_emit(ring, DERRMR);
++	intel_ring_emit(ring, ~(DERRMR_PIPEA_PRI_FLIP_DONE |
++				DERRMR_PIPEB_PRI_FLIP_DONE |
++				DERRMR_PIPEC_PRI_FLIP_DONE));
++	intel_ring_emit(ring, MI_STORE_REGISTER_MEM_GEN8(1) |
++			      MI_SRM_LRM_GLOBAL_GTT);
++	intel_ring_emit(ring, DERRMR);
++	intel_ring_emit(ring, rq->engine->scratch.gtt_offset + 256);
++	intel_ring_emit(ring, 0);
++
++	intel_ring_emit(ring, MI_DISPLAY_FLIP_I915 | plane);
++	intel_ring_emit(ring, stride << 6 | obj->tiling_mode);
++	intel_ring_emit(ring, crtc->unpin_work->gtt_offset);
++	intel_ring_advance(ring);
+ 
+-	intel_mark_page_flip_active(intel_crtc);
+-	__intel_ring_advance(ring);
+ 	return 0;
+ }
+ 
+-static bool use_mmio_flip(struct intel_engine_cs *ring,
+-			  struct drm_i915_gem_object *obj)
++static bool use_mmio_flip(struct intel_crtc *intel_crtc,
++			  struct intel_engine_cs *engine,
++			  struct drm_i915_gem_object *obj,
++			  unsigned flags)
+ {
+ 	/*
+ 	 * This is not being used for older platforms, because
+@@ -9522,18 +9452,25 @@
+ 	 * So using MMIO flips there would disrupt this mechanism.
+ 	 */
+ 
+-	if (ring == NULL)
++	if (engine == NULL)
+ 		return true;
+ 
+-	if (INTEL_INFO(ring->dev)->gen < 5)
++	if (INTEL_INFO(engine->i915)->gen < 5)
+ 		return false;
+ 
+-	if (i915.use_mmio_flip < 0)
++	if (__i915_reset_in_progress(intel_crtc->reset_counter) |
++	    __i915_terminally_wedged(intel_crtc->reset_counter))
++		return true;
++
++	if (flags & DRM_MODE_PAGE_FLIP_ASYNC)
++		return false; /* XXX undecided */
++
++	if (i915_module.use_mmio_flip < 0)
+ 		return false;
+-	else if (i915.use_mmio_flip > 0)
++	else if (i915_module.use_mmio_flip > 0)
+ 		return true;
+ 	else
+-		return ring != obj->ring;
++		return engine != i915_request_engine(obj->last_write.request);
+ }
+ 
+ static void intel_do_mmio_flip(struct intel_crtc *intel_crtc)
+@@ -9543,126 +9480,161 @@
+ 	struct intel_framebuffer *intel_fb =
+ 		to_intel_framebuffer(intel_crtc->base.primary->fb);
+ 	struct drm_i915_gem_object *obj = intel_fb->obj;
++	bool atomic_update;
++	u32 start_vbl_count;
+ 	u32 dspcntr;
+ 	u32 reg;
+ 
+ 	intel_mark_page_flip_active(intel_crtc);
+ 
++	atomic_update = intel_pipe_update_start(intel_crtc, &start_vbl_count);
++
+ 	reg = DSPCNTR(intel_crtc->plane);
+ 	dspcntr = I915_READ(reg);
+ 
+-	if (INTEL_INFO(dev)->gen >= 4) {
+-		if (obj->tiling_mode != I915_TILING_NONE)
+-			dspcntr |= DISPPLANE_TILED;
+-		else
+-			dspcntr &= ~DISPPLANE_TILED;
+-	}
++	if (obj->tiling_mode != I915_TILING_NONE)
++		dspcntr |= DISPPLANE_TILED;
++	else
++		dspcntr &= ~DISPPLANE_TILED;
++
+ 	I915_WRITE(reg, dspcntr);
+ 
+ 	I915_WRITE(DSPSURF(intel_crtc->plane),
+ 		   intel_crtc->unpin_work->gtt_offset);
+ 	POSTING_READ(DSPSURF(intel_crtc->plane));
++
++	if (atomic_update)
++		intel_pipe_update_end(intel_crtc, start_vbl_count);
+ }
+ 
+-static int intel_postpone_flip(struct drm_i915_gem_object *obj)
++struct flip_work {
++	struct work_struct work;
++	struct i915_gem_request *rq;
++	struct intel_crtc *crtc;
++};
++
++static void intel_mmio_flip_work(struct work_struct *work)
+ {
+-	struct intel_engine_cs *ring;
+-	int ret;
++	struct flip_work *flip = container_of(work, struct flip_work, work);
+ 
+-	lockdep_assert_held(&obj->base.dev->struct_mutex);
++	if (WARN_ON(__i915_request_wait(flip->rq, false, NULL, NULL)))
++		/* should never happen, but still prevent a lockup */
++		page_flip_completed(flip->crtc);
++	else
++		intel_do_mmio_flip(flip->crtc);
+ 
+-	if (!obj->last_write_seqno)
+-		return 0;
++	i915_request_put__unlocked(flip->rq);
++	kfree(flip);
++}
++
++static int intel_queue_mmio_flip(struct intel_crtc *crtc,
++				 struct i915_gem_request *rq)
++{
++	struct flip_work *flip;
+ 
+-	ring = obj->ring;
++	if (WARN_ON(crtc->mmio_flip))
++		return -EBUSY;
+ 
+-	if (i915_seqno_passed(ring->get_seqno(ring, true),
+-			      obj->last_write_seqno))
++	if (rq == NULL) {
++		intel_do_mmio_flip(crtc);
+ 		return 0;
++	}
+ 
+-	ret = i915_gem_check_olr(ring, obj->last_write_seqno);
+-	if (ret)
++	if (i915_request_complete(rq)) {
++		intel_do_mmio_flip(crtc);
++		return 0;
++	}
++
++	flip = kmalloc(sizeof(*flip), GFP_KERNEL);
++	if (flip == NULL)
++		return -ENOMEM;
++
++	INIT_WORK(&flip->work, intel_mmio_flip_work);
++	flip->crtc = crtc;
++	flip->rq = i915_request_get_breadcrumb(rq);
++	if (IS_ERR(flip->rq)) {
++		int ret = PTR_ERR(flip->rq);
++		kfree(flip);
+ 		return ret;
++	}
+ 
+-	if (WARN_ON(!ring->irq_get(ring)))
+-		return 0;
++	schedule_work(&flip->work);
++	return 0;
++}
+ 
+-	return 1;
++static int intel_default_queue_flip(struct i915_gem_request *rq,
++				    struct intel_crtc *crtc,
++				    struct drm_framebuffer *fb,
++				    struct drm_i915_gem_object *obj,
++				    uint32_t flags)
++{
++	return -ENODEV;
+ }
+ 
+-void intel_notify_mmio_flip(struct intel_engine_cs *ring)
++static bool __intel_pageflip_stall_check(struct drm_device *dev,
++					 struct drm_crtc *crtc)
+ {
+-	struct drm_i915_private *dev_priv = to_i915(ring->dev);
+-	struct intel_crtc *intel_crtc;
+-	unsigned long irq_flags;
+-	u32 seqno;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	struct intel_unpin_work *work = intel_crtc->unpin_work;
++	u32 addr;
+ 
+-	seqno = ring->get_seqno(ring, false);
++	if (atomic_read(&work->pending) >= INTEL_FLIP_COMPLETE)
++		return true;
+ 
+-	spin_lock_irqsave(&dev_priv->mmio_flip_lock, irq_flags);
+-	for_each_intel_crtc(ring->dev, intel_crtc) {
+-		struct intel_mmio_flip *mmio_flip;
++	if (!work->enable_stall_check)
++		return false;
+ 
+-		mmio_flip = &intel_crtc->mmio_flip;
+-		if (mmio_flip->seqno == 0)
+-			continue;
++	if (intel_crtc->reset_counter != atomic_read(&dev_priv->gpu_error.reset_counter))
++		return true;
+ 
+-		if (ring->id != mmio_flip->ring_id)
+-			continue;
++	if (work->flip_ready_vblank == 0) {
++		if (work->flip_queued_request &&
++		    !i915_request_complete(work->flip_queued_request))
++			return false;
+ 
+-		if (i915_seqno_passed(seqno, mmio_flip->seqno)) {
+-			intel_do_mmio_flip(intel_crtc);
+-			mmio_flip->seqno = 0;
+-			ring->irq_put(ring);
+-		}
++		work->flip_ready_vblank = drm_vblank_count(dev, intel_crtc->pipe);
+ 	}
+-	spin_unlock_irqrestore(&dev_priv->mmio_flip_lock, irq_flags);
++
++	if (drm_vblank_count(dev, intel_crtc->pipe) - work->flip_ready_vblank < 3)
++		return false;
++
++	/* Potential stall - if we see that the flip has happened,
++	 * assume a missed interrupt. */
++	if (INTEL_INFO(dev)->gen >= 4)
++		addr = I915_HI_DISPBASE(I915_READ(DSPSURF(intel_crtc->plane)));
++	else
++		addr = I915_READ(DSPADDR(intel_crtc->plane));
++
++	/* There is a potential issue here with a false positive after a flip
++	 * to the same address. We could address this by checking for a
++	 * non-incrementing frame counter.
++	 */
++	return addr == work->gtt_offset;
+ }
+ 
+-static int intel_queue_mmio_flip(struct drm_device *dev,
+-				 struct drm_crtc *crtc,
+-				 struct drm_framebuffer *fb,
+-				 struct drm_i915_gem_object *obj,
+-				 struct intel_engine_cs *ring,
+-				 uint32_t flags)
++void intel_check_page_flip(struct drm_device *dev, int pipe)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_crtc *crtc = dev_priv->pipe_to_crtc_mapping[pipe];
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+-	unsigned long irq_flags;
+-	int ret;
+-
+-	if (WARN_ON(intel_crtc->mmio_flip.seqno))
+-		return -EBUSY;
+-
+-	ret = intel_postpone_flip(obj);
+-	if (ret < 0)
+-		return ret;
+-	if (ret == 0) {
+-		intel_do_mmio_flip(intel_crtc);
+-		return 0;
+-	}
+ 
+-	spin_lock_irqsave(&dev_priv->mmio_flip_lock, irq_flags);
+-	intel_crtc->mmio_flip.seqno = obj->last_write_seqno;
+-	intel_crtc->mmio_flip.ring_id = obj->ring->id;
+-	spin_unlock_irqrestore(&dev_priv->mmio_flip_lock, irq_flags);
++	WARN_ON(!in_irq());
+ 
+-	/*
+-	 * Double check to catch cases where irq fired before
+-	 * mmio flip data was ready
+-	 */
+-	intel_notify_mmio_flip(obj->ring);
+-	return 0;
+-}
++	if (crtc == NULL)
++		return;
+ 
+-static int intel_default_queue_flip(struct drm_device *dev,
+-				    struct drm_crtc *crtc,
+-				    struct drm_framebuffer *fb,
+-				    struct drm_i915_gem_object *obj,
+-				    struct intel_engine_cs *ring,
+-				    uint32_t flags)
+-{
+-	return -ENODEV;
++	spin_lock(&dev->event_lock);
++	if (intel_crtc->unpin_work && __intel_pageflip_stall_check(dev, crtc)) {
++		WARN_ONCE(1, "Kicking stuck page flip: queued at %d, now %d\n",
++			 intel_crtc->unpin_work->flip_queued_vblank, drm_vblank_count(dev, pipe));
++		page_flip_completed(intel_crtc);
++	}
++	if (intel_crtc->unpin_work != NULL &&
++	    intel_crtc->unpin_work->rcs_active &&
++	    drm_vblank_count(dev, pipe) - intel_crtc->unpin_work->flip_queued_vblank > 1)
++		intel_queue_rps_boost_for_request(dev, intel_crtc->unpin_work->flip_queued_request);
++	spin_unlock(&dev->event_lock);
+ }
+ 
+ static int intel_crtc_page_flip(struct drm_crtc *crtc,
+@@ -9677,8 +9649,8 @@
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+ 	enum pipe pipe = intel_crtc->pipe;
+ 	struct intel_unpin_work *work;
+-	struct intel_engine_cs *ring;
+-	unsigned long flags;
++	struct intel_engine_cs *engine;
++	struct i915_gem_request *rq;
+ 	int ret;
+ 
+ 	/*
+@@ -9702,8 +9674,13 @@
+ 	     fb->pitches[0] != crtc->primary->fb->pitches[0]))
+ 		return -EINVAL;
+ 
+-	if (i915_terminally_wedged(&dev_priv->gpu_error))
+-		goto out_hang;
++	if (page_flip_flags & DRM_MODE_PAGE_FLIP_ASYNC) {
++		if (obj->tiling_mode != I915_TILING_X)
++			return -EINVAL;
++
++		if (to_intel_framebuffer(old_fb)->obj->tiling_mode != I915_TILING_X)
++			return -EINVAL;
++	}
+ 
+ 	work = kzalloc(sizeof(*work), GFP_KERNEL);
+ 	if (work == NULL)
+@@ -9719,17 +9696,29 @@
+ 		goto free_work;
+ 
+ 	/* We borrow the event spin lock for protecting unpin_work */
+-	spin_lock_irqsave(&dev->event_lock, flags);
++	spin_lock_irq(&dev->event_lock);
+ 	if (intel_crtc->unpin_work) {
+-		spin_unlock_irqrestore(&dev->event_lock, flags);
+-		kfree(work);
+-		drm_crtc_vblank_put(crtc);
+-
+-		DRM_DEBUG_DRIVER("flip queue: crtc already busy\n");
+-		return -EBUSY;
++		/* Before declaring the flip queue wedged, check if
++		 * the hardware completed the operation behind our backs.
++		 */
++		if (intel_crtc->unpin_work->async ||
++		    __intel_pageflip_stall_check(dev, crtc)) {
++			DRM_DEBUG_DRIVER("flip queue: previous flip completed, continuing\n");
++			page_flip_completed(intel_crtc);
++		} else {
++			DRM_DEBUG_DRIVER("flip queue: crtc already busy: flip queud at %d, ready at %d, now %d\n",
++					 intel_crtc->unpin_work->flip_queued_vblank,
++					 intel_crtc->unpin_work->flip_ready_vblank,
++					 drm_vblank_count(dev, intel_crtc->pipe));
++			spin_unlock_irq(&dev->event_lock);
++
++			drm_crtc_vblank_put(crtc);
++			kfree(work);
++			return -EBUSY;
++		}
+ 	}
+ 	intel_crtc->unpin_work = work;
+-	spin_unlock_irqrestore(&dev->event_lock, flags);
++	spin_unlock_irq(&dev->event_lock);
+ 
+ 	if (atomic_read(&intel_crtc->unpin_work_count) >= 2)
+ 		flush_workqueue(dev_priv->wq);
+@@ -9745,45 +9734,84 @@
+ 	crtc->primary->fb = fb;
+ 
+ 	work->pending_flip_obj = obj;
+-
+-	work->enable_stall_check = true;
++	work->rcs_active = RCS_ENGINE(dev_priv)->last_request != NULL;
++	work->async = page_flip_flags & DRM_MODE_PAGE_FLIP_ASYNC;
+ 
+ 	atomic_inc(&intel_crtc->unpin_work_count);
+ 	intel_crtc->reset_counter = atomic_read(&dev_priv->gpu_error.reset_counter);
++	if (__i915_reset_in_progress(intel_crtc->reset_counter) |
++	    __i915_terminally_wedged(intel_crtc->reset_counter)) {
++		ret = -EIO;
++		goto cleanup_pending;
++	}
+ 
+ 	if (INTEL_INFO(dev)->gen >= 5 || IS_G4X(dev))
+ 		work->flip_count = I915_READ(PIPE_FLIPCOUNT_GM45(pipe)) + 1;
+ 
+ 	if (IS_VALLEYVIEW(dev)) {
+-		ring = &dev_priv->ring[BCS];
++		engine = &dev_priv->engine[BCS];
+ 		if (obj->tiling_mode != work->old_fb_obj->tiling_mode)
+ 			/* vlv: DISPLAY_FLIP fails to change tiling */
+-			ring = NULL;
++			engine = NULL;
+ 	} else if (IS_IVYBRIDGE(dev)) {
+-		ring = &dev_priv->ring[BCS];
++		engine = &dev_priv->engine[BCS];
+ 	} else if (INTEL_INFO(dev)->gen >= 7) {
+-		ring = obj->ring;
+-		if (ring == NULL || ring->id != RCS)
+-			ring = &dev_priv->ring[BCS];
++		engine = i915_request_engine(obj->last_write.request);
++		if (engine == NULL || engine->id != RCS)
++			engine = &dev_priv->engine[BCS];
+ 	} else {
+-		ring = &dev_priv->ring[RCS];
++		engine = &dev_priv->engine[RCS];
+ 	}
+ 
+-	ret = intel_pin_and_fence_fb_obj(dev, obj, ring);
+-	if (ret)
+-		goto cleanup_pending;
++	if (use_mmio_flip(intel_crtc, engine, obj, page_flip_flags)) {
++		rq = i915_request_get(obj->last_write.request);
+ 
+-	work->gtt_offset =
+-		i915_gem_obj_ggtt_offset(obj) + intel_crtc->dspaddr_offset;
++		ret = intel_pin_and_fence_fb_obj(crtc->primary, fb, rq);
++		if (ret)
++			goto cleanup_rq;
+ 
+-	if (use_mmio_flip(ring, obj))
+-		ret = intel_queue_mmio_flip(dev, crtc, fb, obj, ring,
+-					    page_flip_flags);
+-	else
+-		ret = dev_priv->display.queue_flip(dev, crtc, fb, obj, ring,
+-				page_flip_flags);
+-	if (ret)
+-		goto cleanup_unpin;
++		work->gtt_offset =
++			i915_gem_obj_ggtt_offset(obj) + intel_crtc->dspaddr_offset;
++
++		ret = intel_queue_mmio_flip(intel_crtc, rq);
++		if (ret)
++			goto cleanup_unpin;
++	} else {
++		struct intel_context *ctx = engine->default_context;
++		if (obj->last_write.request)
++			ctx = obj->last_write.request->ctx;
++		rq = i915_request_create(ctx, engine);
++		if (IS_ERR(rq)) {
++			ret = PTR_ERR(rq);
++			goto cleanup_pending;
++		}
++
++		ret = intel_pin_and_fence_fb_obj(crtc->primary, fb, rq);
++		if (ret)
++			goto cleanup_rq;
++
++		work->gtt_offset =
++			i915_gem_obj_ggtt_offset(obj) + intel_crtc->dspaddr_offset;
++
++		ret = i915_request_emit_flush(rq, I915_FLUSH_CACHES);
++		if (ret)
++			goto cleanup_unpin;
++
++		ret = dev_priv->display.queue_flip(rq, intel_crtc, fb, obj,
++						   page_flip_flags);
++		if (ret)
++			goto cleanup_unpin;
++
++		intel_mark_page_flip_active(intel_crtc);
++
++		ret = i915_request_commit(rq);
++		if (ret)
++			goto cleanup_unpin;
++	}
++
++	work->flip_queued_request = rq;
++	work->flip_queued_vblank = drm_vblank_count(dev, intel_crtc->pipe);
++	work->enable_stall_check = true;
+ 
+ 	i915_gem_track_fb(work->old_fb_obj, obj,
+ 			  INTEL_FRONTBUFFER_PRIMARY(pipe));
+@@ -9798,6 +9826,8 @@
+ 
+ cleanup_unpin:
+ 	intel_unpin_fb_obj(obj);
++cleanup_rq:
++	i915_request_put(rq);
+ cleanup_pending:
+ 	atomic_dec(&intel_crtc->unpin_work_count);
+ 	crtc->primary->fb = old_fb;
+@@ -9806,20 +9836,22 @@
+ 	mutex_unlock(&dev->struct_mutex);
+ 
+ cleanup:
+-	spin_lock_irqsave(&dev->event_lock, flags);
++	spin_lock_irq(&dev->event_lock);
+ 	intel_crtc->unpin_work = NULL;
+-	spin_unlock_irqrestore(&dev->event_lock, flags);
++	spin_unlock_irq(&dev->event_lock);
+ 
+ 	drm_crtc_vblank_put(crtc);
+ free_work:
+ 	kfree(work);
+ 
+ 	if (ret == -EIO) {
+-out_hang:
+ 		intel_crtc_wait_for_pending_flips(crtc);
+ 		ret = intel_pipe_set_base(crtc, crtc->x, crtc->y, fb);
+-		if (ret == 0 && event)
++		if (ret == 0 && event) {
++			spin_lock_irq(&dev->event_lock);
+ 			drm_send_vblank_event(dev, pipe, event);
++			spin_unlock_irq(&dev->event_lock);
++		}
+ 	}
+ 	return ret;
+ }
+@@ -9847,8 +9879,7 @@
+ 			to_intel_encoder(connector->base.encoder);
+ 	}
+ 
+-	list_for_each_entry(encoder, &dev->mode_config.encoder_list,
+-			    base.head) {
++	for_each_intel_encoder(dev, encoder) {
+ 		encoder->new_crtc =
+ 			to_intel_crtc(encoder->base.crtc);
+ 	}
+@@ -9879,8 +9910,7 @@
+ 		connector->base.encoder = &connector->new_encoder->base;
+ 	}
+ 
+-	list_for_each_entry(encoder, &dev->mode_config.encoder_list,
+-			    base.head) {
++	for_each_intel_encoder(dev, encoder) {
+ 		encoder->base.crtc = &encoder->new_crtc->base;
+ 	}
+ 
+@@ -10007,6 +10037,15 @@
+ 		      pipe_config->dp_m_n.gmch_m, pipe_config->dp_m_n.gmch_n,
+ 		      pipe_config->dp_m_n.link_m, pipe_config->dp_m_n.link_n,
+ 		      pipe_config->dp_m_n.tu);
++
++	DRM_DEBUG_KMS("dp: %i, gmch_m2: %u, gmch_n2: %u, link_m2: %u, link_n2: %u, tu2: %u\n",
++		      pipe_config->has_dp_encoder,
++		      pipe_config->dp_m2_n2.gmch_m,
++		      pipe_config->dp_m2_n2.gmch_n,
++		      pipe_config->dp_m2_n2.link_m,
++		      pipe_config->dp_m2_n2.link_n,
++		      pipe_config->dp_m2_n2.tu);
++
+ 	DRM_DEBUG_KMS("requested mode:\n");
+ 	drm_mode_debug_printmodeline(&pipe_config->requested_mode);
+ 	DRM_DEBUG_KMS("adjusted mode:\n");
+@@ -10041,8 +10080,7 @@
+ 	struct drm_device *dev = crtc->base.dev;
+ 	struct intel_encoder *source_encoder;
+ 
+-	list_for_each_entry(source_encoder,
+-			    &dev->mode_config.encoder_list, base.head) {
++	for_each_intel_encoder(dev, source_encoder) {
+ 		if (source_encoder->new_crtc != crtc)
+ 			continue;
+ 
+@@ -10058,8 +10096,7 @@
+ 	struct drm_device *dev = crtc->base.dev;
+ 	struct intel_encoder *encoder;
+ 
+-	list_for_each_entry(encoder,
+-			    &dev->mode_config.encoder_list, base.head) {
++	for_each_intel_encoder(dev, encoder) {
+ 		if (encoder->new_crtc != crtc)
+ 			continue;
+ 
+@@ -10143,8 +10180,7 @@
+ 	 * adjust it according to limitations or connector properties, and also
+ 	 * a chance to reject the mode entirely.
+ 	 */
+-	list_for_each_entry(encoder, &dev->mode_config.encoder_list,
+-			    base.head) {
++	for_each_intel_encoder(dev, encoder) {
+ 
+ 		if (&encoder->new_crtc->base != crtc)
+ 			continue;
+@@ -10222,8 +10258,7 @@
+ 				1 << connector->new_encoder->new_crtc->pipe;
+ 	}
+ 
+-	list_for_each_entry(encoder, &dev->mode_config.encoder_list,
+-			    base.head) {
++	for_each_intel_encoder(dev, encoder) {
+ 		if (encoder->base.crtc == &encoder->new_crtc->base)
+ 			continue;
+ 
+@@ -10293,12 +10328,14 @@
+ static void
+ intel_modeset_update_state(struct drm_device *dev, unsigned prepare_pipes)
+ {
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct intel_encoder *intel_encoder;
+ 	struct intel_crtc *intel_crtc;
+ 	struct drm_connector *connector;
+ 
+-	list_for_each_entry(intel_encoder, &dev->mode_config.encoder_list,
+-			    base.head) {
++	intel_shared_dpll_commit(dev_priv);
++
++	for_each_intel_encoder(dev, intel_encoder) {
+ 		if (!intel_encoder->base.crtc)
+ 			continue;
+ 
+@@ -10387,6 +10424,22 @@
+ 		return false; \
+ 	}
+ 
++/* This is required for BDW+ where there is only one set of registers for
++ * switching between high and low RR.
++ * This macro can be used whenever a comparison has to be made between one
++ * hw state and multiple sw state variables.
++ */
++#define PIPE_CONF_CHECK_I_ALT(name, alt_name) \
++	if ((current_config->name != pipe_config->name) && \
++		(current_config->alt_name != pipe_config->name)) { \
++			DRM_ERROR("mismatch in " #name " " \
++				  "(expected %i or %i, found %i)\n", \
++				  current_config->name, \
++				  current_config->alt_name, \
++				  pipe_config->name); \
++			return false; \
++	}
++
+ #define PIPE_CONF_CHECK_FLAGS(name, mask)	\
+ 	if ((current_config->name ^ pipe_config->name) & (mask)) { \
+ 		DRM_ERROR("mismatch in " #name "(" #mask ") "	   \
+@@ -10419,11 +10472,28 @@
+ 	PIPE_CONF_CHECK_I(fdi_m_n.tu);
+ 
+ 	PIPE_CONF_CHECK_I(has_dp_encoder);
+-	PIPE_CONF_CHECK_I(dp_m_n.gmch_m);
+-	PIPE_CONF_CHECK_I(dp_m_n.gmch_n);
+-	PIPE_CONF_CHECK_I(dp_m_n.link_m);
+-	PIPE_CONF_CHECK_I(dp_m_n.link_n);
+-	PIPE_CONF_CHECK_I(dp_m_n.tu);
++
++	if (INTEL_INFO(dev)->gen < 8) {
++		PIPE_CONF_CHECK_I(dp_m_n.gmch_m);
++		PIPE_CONF_CHECK_I(dp_m_n.gmch_n);
++		PIPE_CONF_CHECK_I(dp_m_n.link_m);
++		PIPE_CONF_CHECK_I(dp_m_n.link_n);
++		PIPE_CONF_CHECK_I(dp_m_n.tu);
++
++		if (current_config->has_drrs) {
++			PIPE_CONF_CHECK_I(dp_m2_n2.gmch_m);
++			PIPE_CONF_CHECK_I(dp_m2_n2.gmch_n);
++			PIPE_CONF_CHECK_I(dp_m2_n2.link_m);
++			PIPE_CONF_CHECK_I(dp_m2_n2.link_n);
++			PIPE_CONF_CHECK_I(dp_m2_n2.tu);
++		}
++	} else {
++		PIPE_CONF_CHECK_I_ALT(dp_m_n.gmch_m, dp_m2_n2.gmch_m);
++		PIPE_CONF_CHECK_I_ALT(dp_m_n.gmch_n, dp_m2_n2.gmch_n);
++		PIPE_CONF_CHECK_I_ALT(dp_m_n.link_m, dp_m2_n2.link_m);
++		PIPE_CONF_CHECK_I_ALT(dp_m_n.link_n, dp_m2_n2.link_n);
++		PIPE_CONF_CHECK_I_ALT(dp_m_n.tu, dp_m2_n2.tu);
++	}
+ 
+ 	PIPE_CONF_CHECK_I(adjusted_mode.crtc_hdisplay);
+ 	PIPE_CONF_CHECK_I(adjusted_mode.crtc_htotal);
+@@ -10444,6 +10514,7 @@
+ 	if ((INTEL_INFO(dev)->gen < 8 && !IS_HASWELL(dev)) ||
+ 	    IS_VALLEYVIEW(dev))
+ 		PIPE_CONF_CHECK_I(limited_color_range);
++	PIPE_CONF_CHECK_I(has_infoframe);
+ 
+ 	PIPE_CONF_CHECK_I(has_audio);
+ 
+@@ -10500,6 +10571,9 @@
+ 	PIPE_CONF_CHECK_X(dpll_hw_state.fp0);
+ 	PIPE_CONF_CHECK_X(dpll_hw_state.fp1);
+ 	PIPE_CONF_CHECK_X(dpll_hw_state.wrpll);
++	PIPE_CONF_CHECK_X(dpll_hw_state.ctrl1);
++	PIPE_CONF_CHECK_X(dpll_hw_state.cfgcr1);
++	PIPE_CONF_CHECK_X(dpll_hw_state.cfgcr2);
+ 
+ 	if (IS_G4X(dev) || INTEL_INFO(dev)->gen >= 5)
+ 		PIPE_CONF_CHECK_I(pipe_bpp);
+@@ -10509,6 +10583,7 @@
+ 
+ #undef PIPE_CONF_CHECK_X
+ #undef PIPE_CONF_CHECK_I
++#undef PIPE_CONF_CHECK_I_ALT
+ #undef PIPE_CONF_CHECK_FLAGS
+ #undef PIPE_CONF_CHECK_CLOCK_FUZZY
+ #undef PIPE_CONF_QUIRK
+@@ -10516,6 +10591,56 @@
+ 	return true;
+ }
+ 
++static void check_wm_state(struct drm_device *dev)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct skl_ddb_allocation hw_ddb, *sw_ddb;
++	struct intel_crtc *intel_crtc;
++	int plane;
++
++	if (INTEL_INFO(dev)->gen < 9)
++		return;
++
++	skl_ddb_get_hw_state(dev_priv, &hw_ddb);
++	sw_ddb = &dev_priv->wm.skl_hw.ddb;
++
++	for_each_intel_crtc(dev, intel_crtc) {
++		struct skl_ddb_entry *hw_entry, *sw_entry;
++		const enum pipe pipe = intel_crtc->pipe;
++
++		if (!intel_crtc->active)
++			continue;
++
++		/* planes */
++		for_each_plane(pipe, plane) {
++			hw_entry = &hw_ddb.plane[pipe][plane];
++			sw_entry = &sw_ddb->plane[pipe][plane];
++
++			if (skl_ddb_entry_equal(hw_entry, sw_entry))
++				continue;
++
++			DRM_ERROR("mismatch in DDB state pipe %c plane %d "
++				  "(expected (%u,%u), found (%u,%u))\n",
++				  pipe_name(pipe), plane + 1,
++				  sw_entry->start, sw_entry->end,
++				  hw_entry->start, hw_entry->end);
++		}
++
++		/* cursor */
++		hw_entry = &hw_ddb.cursor[pipe];
++		sw_entry = &sw_ddb->cursor[pipe];
++
++		if (skl_ddb_entry_equal(hw_entry, sw_entry))
++			continue;
++
++		DRM_ERROR("mismatch in DDB state pipe %c cursor "
++			  "(expected (%u,%u), found (%u,%u))\n",
++			  pipe_name(pipe),
++			  sw_entry->start, sw_entry->end,
++			  hw_entry->start, hw_entry->end);
++	}
++}
++
+ static void
+ check_connector_state(struct drm_device *dev)
+ {
+@@ -10538,8 +10663,7 @@
+ 	struct intel_encoder *encoder;
+ 	struct intel_connector *connector;
+ 
+-	list_for_each_entry(encoder, &dev->mode_config.encoder_list,
+-			    base.head) {
++	for_each_intel_encoder(dev, encoder) {
+ 		bool enabled = false;
+ 		bool active = false;
+ 		enum pipe pipe, tracked_pipe;
+@@ -10618,8 +10742,7 @@
+ 		WARN(crtc->active && !crtc->base.enabled,
+ 		     "active crtc, but not enabled in sw tracking\n");
+ 
+-		list_for_each_entry(encoder, &dev->mode_config.encoder_list,
+-				    base.head) {
++		for_each_intel_encoder(dev, encoder) {
+ 			if (encoder->base.crtc != &crtc->base)
+ 				continue;
+ 			enabled = true;
+@@ -10637,12 +10760,12 @@
+ 		active = dev_priv->display.get_pipe_config(crtc,
+ 							   &pipe_config);
+ 
+-		/* hw state is inconsistent with the pipe A quirk */
+-		if (crtc->pipe == PIPE_A && dev_priv->quirks & QUIRK_PIPEA_FORCE)
++		/* hw state is inconsistent with the pipe quirk */
++		if ((crtc->pipe == PIPE_A && dev_priv->quirks & QUIRK_PIPEA_FORCE) ||
++		    (crtc->pipe == PIPE_B && dev_priv->quirks & QUIRK_PIPEB_FORCE))
+ 			active = crtc->active;
+ 
+-		list_for_each_entry(encoder, &dev->mode_config.encoder_list,
+-				    base.head) {
++		for_each_intel_encoder(dev, encoder) {
+ 			enum pipe pipe;
+ 			if (encoder->base.crtc != &crtc->base)
+ 				continue;
+@@ -10684,9 +10807,9 @@
+ 
+ 		active = pll->get_hw_state(dev_priv, pll, &dpll_hw_state);
+ 
+-		WARN(pll->active > pll->refcount,
++		WARN(pll->active > hweight32(pll->config.crtc_mask),
+ 		     "more active pll users than references: %i vs %i\n",
+-		     pll->active, pll->refcount);
++		     pll->active, hweight32(pll->config.crtc_mask));
+ 		WARN(pll->active && !pll->on,
+ 		     "pll in active use but not on in sw tracking\n");
+ 		WARN(pll->on && !pll->active,
+@@ -10704,11 +10827,11 @@
+ 		WARN(pll->active != active_crtcs,
+ 		     "pll active crtcs mismatch (expected %i, found %i)\n",
+ 		     pll->active, active_crtcs);
+-		WARN(pll->refcount != enabled_crtcs,
++		WARN(hweight32(pll->config.crtc_mask) != enabled_crtcs,
+ 		     "pll enabled crtcs mismatch (expected %i, found %i)\n",
+-		     pll->refcount, enabled_crtcs);
++		     hweight32(pll->config.crtc_mask), enabled_crtcs);
+ 
+-		WARN(pll->on && memcmp(&pll->hw_state, &dpll_hw_state,
++		WARN(pll->on && memcmp(&pll->config.hw_state, &dpll_hw_state,
+ 				       sizeof(dpll_hw_state)),
+ 		     "pll hw state mismatch\n");
+ 	}
+@@ -10717,6 +10840,7 @@
+ void
+ intel_modeset_check_state(struct drm_device *dev)
+ {
++	check_wm_state(dev);
+ 	check_connector_state(dev);
+ 	check_encoder_state(dev);
+ 	check_crtc_state(dev);
+@@ -10767,51 +10891,66 @@
+ 
+ 		crtc->scanline_offset = vtotal - 1;
+ 	} else if (HAS_DDI(dev) &&
+-		   intel_pipe_has_type(&crtc->base, INTEL_OUTPUT_HDMI)) {
++		   intel_pipe_has_type(crtc, INTEL_OUTPUT_HDMI)) {
+ 		crtc->scanline_offset = 2;
+ 	} else
+ 		crtc->scanline_offset = 1;
+ }
+ 
++static struct intel_crtc_config *
++intel_modeset_compute_config(struct drm_crtc *crtc,
++			     struct drm_display_mode *mode,
++			     struct drm_framebuffer *fb,
++			     unsigned *modeset_pipes,
++			     unsigned *prepare_pipes,
++			     unsigned *disable_pipes)
++{
++	struct intel_crtc_config *pipe_config = NULL;
++
++	intel_modeset_affected_pipes(crtc, modeset_pipes,
++				     prepare_pipes, disable_pipes);
++
++	if ((*modeset_pipes) == 0)
++		goto out;
++
++	/*
++	 * Note this needs changes when we start tracking multiple modes
++	 * and crtcs.  At that point we'll need to compute the whole config
++	 * (i.e. one pipe_config for each crtc) rather than just the one
++	 * for this crtc.
++	 */
++	pipe_config = intel_modeset_pipe_config(crtc, fb, mode);
++	if (IS_ERR(pipe_config)) {
++		goto out;
++	}
++	intel_dump_pipe_config(to_intel_crtc(crtc), pipe_config,
++			       "[modeset]");
++	to_intel_crtc(crtc)->new_config = pipe_config;
++
++out:
++	return pipe_config;
++}
++
+ static int __intel_set_mode(struct drm_crtc *crtc,
+ 			    struct drm_display_mode *mode,
+-			    int x, int y, struct drm_framebuffer *fb)
++			    int x, int y, struct drm_framebuffer *fb,
++			    struct intel_crtc_config *pipe_config,
++			    unsigned modeset_pipes,
++			    unsigned prepare_pipes,
++			    unsigned disable_pipes)
+ {
+ 	struct drm_device *dev = crtc->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct drm_display_mode *saved_mode;
+-	struct intel_crtc_config *pipe_config = NULL;
+ 	struct intel_crtc *intel_crtc;
+-	unsigned disable_pipes, prepare_pipes, modeset_pipes;
+ 	int ret = 0;
+ 
+ 	saved_mode = kmalloc(sizeof(*saved_mode), GFP_KERNEL);
+ 	if (!saved_mode)
+ 		return -ENOMEM;
+ 
+-	intel_modeset_affected_pipes(crtc, &modeset_pipes,
+-				     &prepare_pipes, &disable_pipes);
+-
+ 	*saved_mode = crtc->mode;
+ 
+-	/* Hack: Because we don't (yet) support global modeset on multiple
+-	 * crtcs, we don't keep track of the new mode for more than one crtc.
+-	 * Hence simply check whether any bit is set in modeset_pipes in all the
+-	 * pieces of code that are not yet converted to deal with mutliple crtcs
+-	 * changing their mode at the same time. */
+-	if (modeset_pipes) {
+-		pipe_config = intel_modeset_pipe_config(crtc, fb, mode);
+-		if (IS_ERR(pipe_config)) {
+-			ret = PTR_ERR(pipe_config);
+-			pipe_config = NULL;
+-
+-			goto out;
+-		}
+-		intel_dump_pipe_config(to_intel_crtc(crtc), pipe_config,
+-				       "[modeset]");
+-		to_intel_crtc(crtc)->new_config = pipe_config;
+-	}
+-
+ 	/*
+ 	 * See if the config requires any additional preparation, e.g.
+ 	 * to adjust global state with pipes off.  We need to do this
+@@ -10826,6 +10965,22 @@
+ 		prepare_pipes &= ~disable_pipes;
+ 	}
+ 
++	if (dev_priv->display.crtc_compute_clock) {
++		unsigned clear_pipes = modeset_pipes | disable_pipes;
++
++		ret = intel_shared_dpll_start_config(dev_priv, clear_pipes);
++		if (ret)
++			goto done;
++
++		for_each_intel_crtc_masked(dev, modeset_pipes, intel_crtc) {
++			ret = dev_priv->display.crtc_compute_clock(intel_crtc);
++			if (ret) {
++				intel_shared_dpll_abort_config(dev_priv);
++				goto done;
++			}
++		}
++	}
++
+ 	for_each_intel_crtc_masked(dev, disable_pipes, intel_crtc)
+ 		intel_crtc_disable(&intel_crtc->base);
+ 
+@@ -10836,6 +10991,10 @@
+ 
+ 	/* crtc->mode is already used by the ->mode_set callbacks, hence we need
+ 	 * to set it here already despite that we pass it down the callchain.
++	 *
++	 * Note we'll need to fix this up when we start tracking multiple
++	 * pipes; here we assume a single modeset_pipe and only track the
++	 * single crtc and mode.
+ 	 */
+ 	if (modeset_pipes) {
+ 		crtc->mode = *mode;
+@@ -10857,27 +11016,37 @@
+ 	 * update the the output configuration. */
+ 	intel_modeset_update_state(dev, prepare_pipes);
+ 
+-	if (dev_priv->display.modeset_global_resources)
+-		dev_priv->display.modeset_global_resources(dev);
++	modeset_update_crtc_power_domains(dev);
+ 
+ 	/* Set up the DPLL and any encoders state that needs to adjust or depend
+ 	 * on the DPLL.
+ 	 */
+ 	for_each_intel_crtc_masked(dev, modeset_pipes, intel_crtc) {
+-		struct drm_framebuffer *old_fb = crtc->primary->fb;
+-		struct drm_i915_gem_object *old_obj = intel_fb_obj(old_fb);
++		struct drm_i915_gem_object *old_obj = intel_fb_obj(crtc->primary->fb);
+ 		struct drm_i915_gem_object *obj = intel_fb_obj(fb);
+ 
++		if (!assert_pipe_disabled(dev_priv, intel_crtc->pipe) ||
++		    !assert_plane_disabled(dev_priv, intel_crtc->plane)) {
++			ret = -EIO;
++			goto done;
++		}
++
++		/* The display engine is disabled. We can safely remove the
++		 * current object pointed to by hardware registers as before
++		 * we enable the pipe again, we will always update those
++		 * registers to point to the currently pinned object. Even
++		 * if we fail, though the hardware points to a stale address,
++		 * that address is never read.
++		 */
++
+ 		mutex_lock(&dev->struct_mutex);
+-		ret = intel_pin_and_fence_fb_obj(dev,
+-						 obj,
+-						 NULL);
++		ret = intel_pin_and_fence_fb_obj(crtc->primary, fb, NULL);
+ 		if (ret != 0) {
+ 			DRM_ERROR("pin & fence failed\n");
+ 			mutex_unlock(&dev->struct_mutex);
+ 			goto done;
+ 		}
+-		if (old_fb)
++		if (old_obj)
+ 			intel_unpin_fb_obj(old_obj);
+ 		i915_gem_track_fb(old_obj, obj,
+ 				  INTEL_FRONTBUFFER_PRIMARY(intel_crtc->pipe));
+@@ -10886,11 +11055,6 @@
+ 		crtc->primary->fb = fb;
+ 		crtc->x = x;
+ 		crtc->y = y;
+-
+-		ret = dev_priv->display.crtc_mode_set(&intel_crtc->base,
+-						      x, y, fb);
+-		if (ret)
+-			goto done;
+ 	}
+ 
+ 	/* Now enable the clocks, plane, pipe, and connectors that we set up. */
+@@ -10905,19 +11069,23 @@
+ 	if (ret && crtc->enabled)
+ 		crtc->mode = *saved_mode;
+ 
+-out:
+ 	kfree(pipe_config);
+ 	kfree(saved_mode);
+ 	return ret;
+ }
+ 
+-static int intel_set_mode(struct drm_crtc *crtc,
+-			  struct drm_display_mode *mode,
+-			  int x, int y, struct drm_framebuffer *fb)
++static int intel_set_mode_pipes(struct drm_crtc *crtc,
++				struct drm_display_mode *mode,
++				int x, int y, struct drm_framebuffer *fb,
++				struct intel_crtc_config *pipe_config,
++				unsigned modeset_pipes,
++				unsigned prepare_pipes,
++				unsigned disable_pipes)
+ {
+ 	int ret;
+ 
+-	ret = __intel_set_mode(crtc, mode, x, y, fb);
++	ret = __intel_set_mode(crtc, mode, x, y, fb, pipe_config, modeset_pipes,
++			       prepare_pipes, disable_pipes);
+ 
+ 	if (ret == 0)
+ 		intel_modeset_check_state(crtc->dev);
+@@ -10925,6 +11093,26 @@
+ 	return ret;
+ }
+ 
++static int intel_set_mode(struct drm_crtc *crtc,
++			  struct drm_display_mode *mode,
++			  int x, int y, struct drm_framebuffer *fb)
++{
++	struct intel_crtc_config *pipe_config;
++	unsigned modeset_pipes, prepare_pipes, disable_pipes;
++
++	pipe_config = intel_modeset_compute_config(crtc, mode, fb,
++						   &modeset_pipes,
++						   &prepare_pipes,
++						   &disable_pipes);
++
++	if (IS_ERR(pipe_config))
++		return PTR_ERR(pipe_config);
++
++	return intel_set_mode_pipes(crtc, mode, x, y, fb, pipe_config,
++				    modeset_pipes, prepare_pipes,
++				    disable_pipes);
++}
++
+ void intel_crtc_restore_mode(struct drm_crtc *crtc)
+ {
+ 	intel_set_mode(crtc, &crtc->mode, crtc->x, crtc->y, crtc->primary->fb);
+@@ -11010,7 +11198,7 @@
+ 	}
+ 
+ 	count = 0;
+-	list_for_each_entry(encoder, &dev->mode_config.encoder_list, base.head) {
++	for_each_intel_encoder(dev, encoder) {
+ 		encoder->new_crtc =
+ 			to_intel_crtc(config->save_encoder_crtcs[count++]);
+ 	}
+@@ -11169,8 +11357,7 @@
+ 	}
+ 
+ 	/* Check for any encoders that needs to be disabled. */
+-	list_for_each_entry(encoder, &dev->mode_config.encoder_list,
+-			    base.head) {
++	for_each_intel_encoder(dev, encoder) {
+ 		int num_connectors = 0;
+ 		list_for_each_entry(connector,
+ 				    &dev->mode_config.connector_list,
+@@ -11203,9 +11390,7 @@
+ 	for_each_intel_crtc(dev, crtc) {
+ 		crtc->new_enabled = false;
+ 
+-		list_for_each_entry(encoder,
+-				    &dev->mode_config.encoder_list,
+-				    base.head) {
++		for_each_intel_encoder(dev, encoder) {
+ 			if (encoder->new_crtc == crtc) {
+ 				crtc->new_enabled = true;
+ 				break;
+@@ -11242,7 +11427,7 @@
+ 			connector->new_encoder = NULL;
+ 	}
+ 
+-	list_for_each_entry(encoder, &dev->mode_config.encoder_list, base.head) {
++	for_each_intel_encoder(dev, encoder) {
+ 		if (encoder->new_crtc == crtc)
+ 			encoder->new_crtc = NULL;
+ 	}
+@@ -11256,6 +11441,8 @@
+ 	struct drm_device *dev;
+ 	struct drm_mode_set save_set;
+ 	struct intel_set_config *config;
++	struct intel_crtc_config *pipe_config;
++	unsigned modeset_pipes, prepare_pipes, disable_pipes;
+ 	int ret;
+ 
+ 	BUG_ON(!set);
+@@ -11301,11 +11488,36 @@
+ 	if (ret)
+ 		goto fail;
+ 
++	pipe_config = intel_modeset_compute_config(set->crtc, set->mode,
++						   set->fb,
++						   &modeset_pipes,
++						   &prepare_pipes,
++						   &disable_pipes);
++	if (IS_ERR(pipe_config)) {
++		goto fail;
++	} else if (pipe_config) {
++		if (to_intel_crtc(set->crtc)->new_config->has_audio !=
++		    to_intel_crtc(set->crtc)->config.has_audio)
++			config->mode_changed = true;
++
++		/* Force mode sets for any infoframe stuff */
++		if (to_intel_crtc(set->crtc)->new_config->has_infoframe ||
++		    to_intel_crtc(set->crtc)->config.has_infoframe)
++			config->mode_changed = true;
++	}
++
++	/* set_mode will free it in the mode_changed case */
++	if (!config->mode_changed)
++		kfree(pipe_config);
++
++	intel_update_pipe_size(to_intel_crtc(set->crtc));
++
+ 	if (config->mode_changed) {
+-		ret = intel_set_mode(set->crtc, set->mode,
+-				     set->x, set->y, set->fb);
++		ret = intel_set_mode_pipes(set->crtc, set->mode,
++					   set->x, set->y, set->fb, pipe_config,
++					   modeset_pipes, prepare_pipes,
++					   disable_pipes);
+ 	} else if (config->fb_changed) {
+-		struct drm_i915_private *dev_priv = dev->dev_private;
+ 		struct intel_crtc *intel_crtc = to_intel_crtc(set->crtc);
+ 
+ 		intel_crtc_wait_for_pending_flips(set->crtc);
+@@ -11319,8 +11531,7 @@
+ 		 */
+ 		if (!intel_crtc->primary_enabled && ret == 0) {
+ 			WARN_ON(!intel_crtc->active);
+-			intel_enable_primary_hw_plane(dev_priv, intel_crtc->plane,
+-						      intel_crtc->pipe);
++			intel_enable_primary_hw_plane(set->crtc->primary, set->crtc);
+ 		}
+ 
+ 		/*
+@@ -11331,7 +11542,7 @@
+ 		 * flipping, so increasing its cost here shouldn't be a big
+ 		 * deal).
+ 		 */
+-		if (i915.fastboot && ret == 0)
++		if (i915_module.fastboot && ret == 0)
+ 			intel_modeset_check_state(set->crtc->dev);
+ 	}
+ 
+@@ -11375,7 +11586,7 @@
+ {
+ 	uint32_t val;
+ 
+-	if (!intel_display_power_enabled(dev_priv, POWER_DOMAIN_PLLS))
++	if (!intel_display_power_is_enabled(dev_priv, POWER_DOMAIN_PLLS))
+ 		return false;
+ 
+ 	val = I915_READ(PCH_DPLL(pll->id));
+@@ -11389,8 +11600,8 @@
+ static void ibx_pch_dpll_mode_set(struct drm_i915_private *dev_priv,
+ 				  struct intel_shared_dpll *pll)
+ {
+-	I915_WRITE(PCH_FP0(pll->id), pll->hw_state.fp0);
+-	I915_WRITE(PCH_FP1(pll->id), pll->hw_state.fp1);
++	I915_WRITE(PCH_FP0(pll->id), pll->config.hw_state.fp0);
++	I915_WRITE(PCH_FP1(pll->id), pll->config.hw_state.fp1);
+ }
+ 
+ static void ibx_pch_dpll_enable(struct drm_i915_private *dev_priv,
+@@ -11399,7 +11610,7 @@
+ 	/* PCH refclock must be enabled first */
+ 	ibx_assert_pch_refclk_enabled(dev_priv);
+ 
+-	I915_WRITE(PCH_DPLL(pll->id), pll->hw_state.dpll);
++	I915_WRITE(PCH_DPLL(pll->id), pll->config.hw_state.dpll);
+ 
+ 	/* Wait for the clocks to stabilize. */
+ 	POSTING_READ(PCH_DPLL(pll->id));
+@@ -11410,7 +11621,7 @@
+ 	 *
+ 	 * So write it again.
+ 	 */
+-	I915_WRITE(PCH_DPLL(pll->id), pll->hw_state.dpll);
++	I915_WRITE(PCH_DPLL(pll->id), pll->config.hw_state.dpll);
+ 	POSTING_READ(PCH_DPLL(pll->id));
+ 	udelay(200);
+ }
+@@ -11473,8 +11684,6 @@
+ intel_primary_plane_disable(struct drm_plane *plane)
+ {
+ 	struct drm_device *dev = plane->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_plane *intel_plane = to_intel_plane(plane);
+ 	struct intel_crtc *intel_crtc;
+ 
+ 	if (!plane->fb)
+@@ -11497,8 +11706,8 @@
+ 		goto disable_unpin;
+ 
+ 	intel_crtc_wait_for_pending_flips(plane->crtc);
+-	intel_disable_primary_hw_plane(dev_priv, intel_plane->plane,
+-				       intel_plane->pipe);
++	intel_disable_primary_hw_plane(plane, plane->crtc);
++
+ disable_unpin:
+ 	mutex_lock(&dev->struct_mutex);
+ 	i915_gem_track_fb(intel_fb_obj(plane->fb), NULL,
+@@ -11511,123 +11720,195 @@
+ }
+ 
+ static int
+-intel_primary_plane_setplane(struct drm_plane *plane, struct drm_crtc *crtc,
+-			     struct drm_framebuffer *fb, int crtc_x, int crtc_y,
+-			     unsigned int crtc_w, unsigned int crtc_h,
+-			     uint32_t src_x, uint32_t src_y,
+-			     uint32_t src_w, uint32_t src_h)
++intel_check_primary_plane(struct drm_plane *plane,
++			  struct intel_plane_state *state)
++{
++	struct drm_crtc *crtc = state->crtc;
++	struct drm_framebuffer *fb = state->fb;
++	struct drm_rect *dest = &state->dst;
++	struct drm_rect *src = &state->src;
++	const struct drm_rect *clip = &state->clip;
++
++	return drm_plane_helper_check_update(plane, crtc, fb,
++					     src, dest, clip,
++					     DRM_PLANE_HELPER_NO_SCALING,
++					     DRM_PLANE_HELPER_NO_SCALING,
++					     false, true, &state->visible);
++}
++
++static int
++intel_prepare_primary_plane(struct drm_plane *plane,
++			    struct intel_plane_state *state)
+ {
++	struct drm_crtc *crtc = state->crtc;
++	struct drm_framebuffer *fb = state->fb;
+ 	struct drm_device *dev = crtc->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+-	struct intel_plane *intel_plane = to_intel_plane(plane);
++	enum pipe pipe = intel_crtc->pipe;
+ 	struct drm_i915_gem_object *obj = intel_fb_obj(fb);
+ 	struct drm_i915_gem_object *old_obj = intel_fb_obj(plane->fb);
+-	struct drm_rect dest = {
+-		/* integer pixels */
+-		.x1 = crtc_x,
+-		.y1 = crtc_y,
+-		.x2 = crtc_x + crtc_w,
+-		.y2 = crtc_y + crtc_h,
+-	};
+-	struct drm_rect src = {
+-		/* 16.16 fixed point */
+-		.x1 = src_x,
+-		.y1 = src_y,
+-		.x2 = src_x + src_w,
+-		.y2 = src_y + src_h,
+-	};
+-	const struct drm_rect clip = {
+-		/* integer pixels */
+-		.x2 = intel_crtc->active ? intel_crtc->config.pipe_src_w : 0,
+-		.y2 = intel_crtc->active ? intel_crtc->config.pipe_src_h : 0,
+-	};
+-	bool visible;
+ 	int ret;
+ 
+-	ret = drm_plane_helper_check_update(plane, crtc, fb,
+-					    &src, &dest, &clip,
+-					    DRM_PLANE_HELPER_NO_SCALING,
+-					    DRM_PLANE_HELPER_NO_SCALING,
+-					    false, true, &visible);
++	intel_crtc_wait_for_pending_flips(crtc);
+ 
+-	if (ret)
+-		return ret;
++	if (intel_crtc_has_pending_flip(crtc)) {
++		DRM_ERROR("pipe is still busy with an old pageflip\n");
++		return -EBUSY;
++	}
+ 
+-	/*
+-	 * If the CRTC isn't enabled, we're just pinning the framebuffer,
+-	 * updating the fb pointer, and returning without touching the
+-	 * hardware.  This allows us to later do a drmModeSetCrtc with fb=-1 to
+-	 * turn on the display with all planes setup as desired.
+-	 */
+-	if (!crtc->enabled) {
++	if (old_obj != obj) {
+ 		mutex_lock(&dev->struct_mutex);
+-
+-		/*
+-		 * If we already called setplane while the crtc was disabled,
+-		 * we may have an fb pinned; unpin it.
+-		 */
+-		if (plane->fb)
+-			intel_unpin_fb_obj(old_obj);
+-
+-		i915_gem_track_fb(old_obj, obj,
+-				  INTEL_FRONTBUFFER_PRIMARY(intel_crtc->pipe));
+-
+-		/* Pin and return without programming hardware */
+-		ret = intel_pin_and_fence_fb_obj(dev, obj, NULL);
++		ret = intel_pin_and_fence_fb_obj(plane, fb, NULL);
++		if (ret == 0)
++			i915_gem_track_fb(old_obj, obj,
++					  INTEL_FRONTBUFFER_PRIMARY(pipe));
+ 		mutex_unlock(&dev->struct_mutex);
+-
+-		return ret;
++		if (ret != 0) {
++			DRM_DEBUG_KMS("pin & fence failed\n");
++			return ret;
++		}
+ 	}
+ 
+-	intel_crtc_wait_for_pending_flips(crtc);
++	return 0;
++}
+ 
+-	/*
+-	 * If clipping results in a non-visible primary plane, we'll disable
+-	 * the primary plane.  Note that this is a bit different than what
+-	 * happens if userspace explicitly disables the plane by passing fb=0
+-	 * because plane->fb still gets set and pinned.
+-	 */
+-	if (!visible) {
+-		mutex_lock(&dev->struct_mutex);
++static void
++intel_commit_primary_plane(struct drm_plane *plane,
++			   struct intel_plane_state *state)
++{
++	struct drm_crtc *crtc = state->crtc;
++	struct drm_framebuffer *fb = state->fb;
++	struct drm_device *dev = crtc->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	enum pipe pipe = intel_crtc->pipe;
++	struct drm_framebuffer *old_fb = plane->fb;
++	struct drm_i915_gem_object *obj = intel_fb_obj(fb);
++	struct drm_i915_gem_object *old_obj = intel_fb_obj(plane->fb);
++	struct intel_plane *intel_plane = to_intel_plane(plane);
++	struct drm_rect *src = &state->src;
++
++	crtc->primary->fb = fb;
++	crtc->x = src->x1;
++	crtc->y = src->y1;
++
++	intel_plane->crtc_x = state->orig_dst.x1;
++	intel_plane->crtc_y = state->orig_dst.y1;
++	intel_plane->crtc_w = drm_rect_width(&state->orig_dst);
++	intel_plane->crtc_h = drm_rect_height(&state->orig_dst);
++	intel_plane->src_x = state->orig_src.x1;
++	intel_plane->src_y = state->orig_src.y1;
++	intel_plane->src_w = drm_rect_width(&state->orig_src);
++	intel_plane->src_h = drm_rect_height(&state->orig_src);
++	intel_plane->obj = obj;
+ 
++	if (intel_crtc->active) {
+ 		/*
+-		 * Try to pin the new fb first so that we can bail out if we
+-		 * fail.
++		 * FBC does not work on some platforms for rotated
++		 * planes, so disable it when rotation is not 0 and
++		 * update it when rotation is set back to 0.
++		 *
++		 * FIXME: This is redundant with the fbc update done in
++		 * the primary plane enable function except that that
++		 * one is done too late. We eventually need to unify
++		 * this.
+ 		 */
+-		if (plane->fb != fb) {
+-			ret = intel_pin_and_fence_fb_obj(dev, obj, NULL);
+-			if (ret) {
+-				mutex_unlock(&dev->struct_mutex);
+-				return ret;
+-			}
++		if (intel_crtc->primary_enabled &&
++		    INTEL_INFO(dev)->gen <= 4 && !IS_G4X(dev) &&
++		    dev_priv->fbc.plane == intel_crtc->plane &&
++		    intel_plane->rotation != BIT(DRM_ROTATE_0)) {
++			intel_disable_fbc(dev);
+ 		}
+ 
+-		i915_gem_track_fb(old_obj, obj,
+-				  INTEL_FRONTBUFFER_PRIMARY(intel_crtc->pipe));
++		if (state->visible) {
++			bool was_enabled = intel_crtc->primary_enabled;
+ 
+-		if (intel_crtc->primary_enabled)
+-			intel_disable_primary_hw_plane(dev_priv,
+-						       intel_plane->plane,
+-						       intel_plane->pipe);
++			/* FIXME: kill this fastboot hack */
++			intel_update_pipe_size(intel_crtc);
+ 
++			intel_crtc->primary_enabled = true;
++
++			dev_priv->display.update_primary_plane(crtc, plane->fb,
++					crtc->x, crtc->y);
++
++			/*
++			 * BDW signals flip done immediately if the plane
++			 * is disabled, even if the plane enable is already
++			 * armed to occur at the next vblank :(
++			 */
++			if (IS_BROADWELL(dev) && !was_enabled)
++				intel_wait_for_vblank(dev, intel_crtc->pipe);
++		} else {
++			/*
++			 * If clipping results in a non-visible primary plane,
++			 * we'll disable the primary plane.  Note that this is
++			 * a bit different than what happens if userspace
++			 * explicitly disables the plane by passing fb=0
++			 * because plane->fb still gets set and pinned.
++			 */
++			intel_disable_primary_hw_plane(plane, crtc);
++		}
+ 
+-		if (plane->fb != fb)
+-			if (plane->fb)
+-				intel_unpin_fb_obj(old_obj);
++		intel_frontbuffer_flip(dev, INTEL_FRONTBUFFER_PRIMARY(pipe));
+ 
++		mutex_lock(&dev->struct_mutex);
++		intel_update_fbc(dev);
+ 		mutex_unlock(&dev->struct_mutex);
++	}
+ 
+-		return 0;
++	if (old_fb && old_fb != fb) {
++		if (intel_crtc->active)
++			intel_wait_for_vblank(dev, intel_crtc->pipe);
++
++		mutex_lock(&dev->struct_mutex);
++		intel_unpin_fb_obj(old_obj);
++		mutex_unlock(&dev->struct_mutex);
+ 	}
++}
++
++static int
++intel_primary_plane_setplane(struct drm_plane *plane, struct drm_crtc *crtc,
++			     struct drm_framebuffer *fb, int crtc_x, int crtc_y,
++			     unsigned int crtc_w, unsigned int crtc_h,
++			     uint32_t src_x, uint32_t src_y,
++			     uint32_t src_w, uint32_t src_h)
++{
++	struct intel_plane_state state;
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	int ret;
+ 
+-	ret = intel_pipe_set_base(crtc, src.x1, src.y1, fb);
++	state.crtc = crtc;
++	state.fb = fb;
++
++	/* sample coordinates in 16.16 fixed point */
++	state.src.x1 = src_x;
++	state.src.x2 = src_x + src_w;
++	state.src.y1 = src_y;
++	state.src.y2 = src_y + src_h;
++
++	/* integer pixels */
++	state.dst.x1 = crtc_x;
++	state.dst.x2 = crtc_x + crtc_w;
++	state.dst.y1 = crtc_y;
++	state.dst.y2 = crtc_y + crtc_h;
++
++	state.clip.x1 = 0;
++	state.clip.y1 = 0;
++	state.clip.x2 = intel_crtc->active ? intel_crtc->config.pipe_src_w : 0;
++	state.clip.y2 = intel_crtc->active ? intel_crtc->config.pipe_src_h : 0;
++
++	state.orig_src = state.src;
++	state.orig_dst = state.dst;
++
++	ret = intel_check_primary_plane(plane, &state);
+ 	if (ret)
+ 		return ret;
+ 
+-	if (!intel_crtc->primary_enabled)
+-		intel_enable_primary_hw_plane(dev_priv, intel_crtc->plane,
+-					      intel_crtc->pipe);
++	ret = intel_prepare_primary_plane(plane, &state);
++	if (ret)
++		return ret;
++
++	intel_commit_primary_plane(plane, &state);
+ 
+ 	return 0;
+ }
+@@ -11644,6 +11925,7 @@
+ 	.update_plane = intel_primary_plane_setplane,
+ 	.disable_plane = intel_primary_plane_disable,
+ 	.destroy = intel_plane_destroy,
++	.set_property = intel_plane_set_property
+ };
+ 
+ static struct drm_plane *intel_primary_plane_create(struct drm_device *dev,
+@@ -11661,6 +11943,7 @@
+ 	primary->max_downscale = 1;
+ 	primary->pipe = pipe;
+ 	primary->plane = pipe;
++	primary->rotation = BIT(DRM_ROTATE_0);
+ 	if (HAS_FBC(dev) && INTEL_INFO(dev)->gen < 4)
+ 		primary->plane = !pipe;
+ 
+@@ -11676,6 +11959,19 @@
+ 				 &intel_primary_plane_funcs,
+ 				 intel_primary_formats, num_formats,
+ 				 DRM_PLANE_TYPE_PRIMARY);
++
++	if (INTEL_INFO(dev)->gen >= 4) {
++		if (!dev->mode_config.rotation_property)
++			dev->mode_config.rotation_property =
++				drm_mode_create_rotation_property(dev,
++							BIT(DRM_ROTATE_0) |
++							BIT(DRM_ROTATE_180));
++		if (dev->mode_config.rotation_property)
++			drm_object_attach_property(&primary->base.base,
++				dev->mode_config.rotation_property,
++				primary->rotation);
++	}
++
+ 	return &primary->base;
+ }
+ 
+@@ -11691,58 +11987,146 @@
+ }
+ 
+ static int
+-intel_cursor_plane_update(struct drm_plane *plane, struct drm_crtc *crtc,
+-			  struct drm_framebuffer *fb, int crtc_x, int crtc_y,
+-			  unsigned int crtc_w, unsigned int crtc_h,
+-			  uint32_t src_x, uint32_t src_y,
+-			  uint32_t src_w, uint32_t src_h)
++intel_check_cursor_plane(struct drm_plane *plane,
++			 struct intel_plane_state *state)
+ {
+-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+-	struct intel_framebuffer *intel_fb = to_intel_framebuffer(fb);
+-	struct drm_i915_gem_object *obj = intel_fb->obj;
+-	struct drm_rect dest = {
+-		/* integer pixels */
+-		.x1 = crtc_x,
+-		.y1 = crtc_y,
+-		.x2 = crtc_x + crtc_w,
+-		.y2 = crtc_y + crtc_h,
+-	};
+-	struct drm_rect src = {
+-		/* 16.16 fixed point */
+-		.x1 = src_x,
+-		.y1 = src_y,
+-		.x2 = src_x + src_w,
+-		.y2 = src_y + src_h,
+-	};
+-	const struct drm_rect clip = {
+-		/* integer pixels */
+-		.x2 = intel_crtc->active ? intel_crtc->config.pipe_src_w : 0,
+-		.y2 = intel_crtc->active ? intel_crtc->config.pipe_src_h : 0,
+-	};
+-	bool visible;
++	struct drm_crtc *crtc = state->crtc;
++	struct drm_device *dev = crtc->dev;
++	struct drm_framebuffer *fb = state->fb;
++	struct drm_rect *dest = &state->dst;
++	struct drm_rect *src = &state->src;
++	const struct drm_rect *clip = &state->clip;
++	struct drm_i915_gem_object *obj = intel_fb_obj(fb);
++	int crtc_w, crtc_h;
++	unsigned stride;
+ 	int ret;
+ 
+ 	ret = drm_plane_helper_check_update(plane, crtc, fb,
+-					    &src, &dest, &clip,
++					    src, dest, clip,
+ 					    DRM_PLANE_HELPER_NO_SCALING,
+ 					    DRM_PLANE_HELPER_NO_SCALING,
+-					    true, true, &visible);
++					    true, true, &state->visible);
+ 	if (ret)
+ 		return ret;
+ 
+-	crtc->cursor_x = crtc_x;
+-	crtc->cursor_y = crtc_y;
++
++	/* if we want to turn off the cursor ignore width and height */
++	if (!obj)
++		return 0;
++
++	/* Check for which cursor types we support */
++	crtc_w = drm_rect_width(&state->orig_dst);
++	crtc_h = drm_rect_height(&state->orig_dst);
++	if (!cursor_size_ok(dev, crtc_w, crtc_h)) {
++		DRM_DEBUG("Cursor dimension not supported\n");
++		return -EINVAL;
++	}
++
++	stride = roundup_pow_of_two(crtc_w) * 4;
++	if (obj->base.size < stride * crtc_h) {
++		DRM_DEBUG_KMS("buffer is too small\n");
++		return -ENOMEM;
++	}
++
++	if (fb == crtc->cursor->fb)
++		return 0;
++
++	/* we only need to pin inside GTT if cursor is non-phy */
++	mutex_lock(&dev->struct_mutex);
++	if (!INTEL_INFO(dev)->cursor_needs_physical && obj->tiling_mode) {
++		DRM_DEBUG_KMS("cursor cannot be tiled\n");
++		ret = -EINVAL;
++	}
++	mutex_unlock(&dev->struct_mutex);
++
++	return ret;
++}
++
++static int
++intel_commit_cursor_plane(struct drm_plane *plane,
++			  struct intel_plane_state *state)
++{
++	struct drm_crtc *crtc = state->crtc;
++	struct drm_framebuffer *fb = state->fb;
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	struct intel_plane *intel_plane = to_intel_plane(plane);
++	struct intel_framebuffer *intel_fb = to_intel_framebuffer(fb);
++	struct drm_i915_gem_object *obj = intel_fb->obj;
++	int crtc_w, crtc_h;
++
++	crtc->cursor_x = state->orig_dst.x1;
++	crtc->cursor_y = state->orig_dst.y1;
++
++	intel_plane->crtc_x = state->orig_dst.x1;
++	intel_plane->crtc_y = state->orig_dst.y1;
++	intel_plane->crtc_w = drm_rect_width(&state->orig_dst);
++	intel_plane->crtc_h = drm_rect_height(&state->orig_dst);
++	intel_plane->src_x = state->orig_src.x1;
++	intel_plane->src_y = state->orig_src.y1;
++	intel_plane->src_w = drm_rect_width(&state->orig_src);
++	intel_plane->src_h = drm_rect_height(&state->orig_src);
++	intel_plane->obj = obj;
++
+ 	if (fb != crtc->cursor->fb) {
++		crtc_w = drm_rect_width(&state->orig_dst);
++		crtc_h = drm_rect_height(&state->orig_dst);
+ 		return intel_crtc_cursor_set_obj(crtc, obj, crtc_w, crtc_h);
+ 	} else {
+-		intel_crtc_update_cursor(crtc, visible);
++		intel_crtc_update_cursor(crtc, state->visible);
++
++		intel_frontbuffer_flip(crtc->dev,
++				       INTEL_FRONTBUFFER_CURSOR(intel_crtc->pipe));
++
+ 		return 0;
+ 	}
+ }
++
++static int
++intel_cursor_plane_update(struct drm_plane *plane, struct drm_crtc *crtc,
++			  struct drm_framebuffer *fb, int crtc_x, int crtc_y,
++			  unsigned int crtc_w, unsigned int crtc_h,
++			  uint32_t src_x, uint32_t src_y,
++			  uint32_t src_w, uint32_t src_h)
++{
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	struct intel_plane_state state;
++	int ret;
++
++	state.crtc = crtc;
++	state.fb = fb;
++
++	/* sample coordinates in 16.16 fixed point */
++	state.src.x1 = src_x;
++	state.src.x2 = src_x + src_w;
++	state.src.y1 = src_y;
++	state.src.y2 = src_y + src_h;
++
++	/* integer pixels */
++	state.dst.x1 = crtc_x;
++	state.dst.x2 = crtc_x + crtc_w;
++	state.dst.y1 = crtc_y;
++	state.dst.y2 = crtc_y + crtc_h;
++
++	state.clip.x1 = 0;
++	state.clip.y1 = 0;
++	state.clip.x2 = intel_crtc->active ? intel_crtc->config.pipe_src_w : 0;
++	state.clip.y2 = intel_crtc->active ? intel_crtc->config.pipe_src_h : 0;
++
++	state.orig_src = state.src;
++	state.orig_dst = state.dst;
++
++	ret = intel_check_cursor_plane(plane, &state);
++	if (ret)
++		return ret;
++
++	return intel_commit_cursor_plane(plane, &state);
++}
++
+ static const struct drm_plane_funcs intel_cursor_plane_funcs = {
+ 	.update_plane = intel_cursor_plane_update,
+ 	.disable_plane = intel_cursor_plane_disable,
+ 	.destroy = intel_plane_destroy,
++	.set_property = intel_plane_set_property,
+ };
+ 
+ static struct drm_plane *intel_cursor_plane_create(struct drm_device *dev,
+@@ -11758,12 +12142,26 @@
+ 	cursor->max_downscale = 1;
+ 	cursor->pipe = pipe;
+ 	cursor->plane = pipe;
++	cursor->rotation = BIT(DRM_ROTATE_0);
+ 
+ 	drm_universal_plane_init(dev, &cursor->base, 0,
+ 				 &intel_cursor_plane_funcs,
+ 				 intel_cursor_formats,
+ 				 ARRAY_SIZE(intel_cursor_formats),
+ 				 DRM_PLANE_TYPE_CURSOR);
++
++	if (INTEL_INFO(dev)->gen >= 4) {
++		if (!dev->mode_config.rotation_property)
++			dev->mode_config.rotation_property =
++				drm_mode_create_rotation_property(dev,
++							BIT(DRM_ROTATE_0) |
++							BIT(DRM_ROTATE_180));
++		if (dev->mode_config.rotation_property)
++			drm_object_attach_property(&cursor->base.base,
++				dev->mode_config.rotation_property,
++				cursor->rotation);
++	}
++
+ 	return &cursor->base;
+ }
+ 
+@@ -11812,8 +12210,7 @@
+ 
+ 	intel_crtc->cursor_base = ~0;
+ 	intel_crtc->cursor_cntl = ~0;
+-
+-	init_waitqueue_head(&intel_crtc->vbl_wait);
++	intel_crtc->cursor_size = ~0;
+ 
+ 	BUG_ON(pipe >= ARRAY_SIZE(dev_priv->plane_to_crtc_mapping) ||
+ 	       dev_priv->plane_to_crtc_mapping[intel_crtc->plane] != NULL);
+@@ -11840,7 +12237,7 @@
+ 
+ 	WARN_ON(!drm_modeset_is_locked(&dev->mode_config.connection_mutex));
+ 
+-	if (!encoder)
++	if (!encoder || WARN_ON(!encoder->crtc))
+ 		return INVALID_PIPE;
+ 
+ 	return to_intel_crtc(encoder->crtc)->pipe;
+@@ -11876,8 +12273,7 @@
+ 	int index_mask = 0;
+ 	int entry = 0;
+ 
+-	list_for_each_entry(source_encoder,
+-			    &dev->mode_config.encoder_list, base.head) {
++	for_each_intel_encoder(dev, source_encoder) {
+ 		if (encoders_cloneable(encoder, source_encoder))
+ 			index_mask |= (1 << entry);
+ 
+@@ -11929,7 +12325,10 @@
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	if (IS_ULT(dev))
++	if (INTEL_INFO(dev)->gen >= 9)
++		return false;
++
++	if (IS_HSW_ULT(dev) || IS_BDW_ULT(dev))
+ 		return false;
+ 
+ 	if (IS_CHERRYVIEW(dev))
+@@ -11999,27 +12398,36 @@
+ 		if (I915_READ(PCH_DP_D) & DP_DETECTED)
+ 			intel_dp_init(dev, PCH_DP_D, PORT_D);
+ 	} else if (IS_VALLEYVIEW(dev)) {
+-		if (I915_READ(VLV_DISPLAY_BASE + GEN4_HDMIB) & SDVO_DETECTED) {
++		/*
++		 * The DP_DETECTED bit is the latched state of the DDC
++		 * SDA pin at boot. However since eDP doesn't require DDC
++		 * (no way to plug in a DP->HDMI dongle) the DDC pins for
++		 * eDP ports may have been muxed to an alternate function.
++		 * Thus we can't rely on the DP_DETECTED bit alone to detect
++		 * eDP ports. Consult the VBT as well as DP_DETECTED to
++		 * detect eDP ports.
++		 */
++		if (I915_READ(VLV_DISPLAY_BASE + GEN4_HDMIB) & SDVO_DETECTED)
+ 			intel_hdmi_init(dev, VLV_DISPLAY_BASE + GEN4_HDMIB,
+ 					PORT_B);
+-			if (I915_READ(VLV_DISPLAY_BASE + DP_B) & DP_DETECTED)
+-				intel_dp_init(dev, VLV_DISPLAY_BASE + DP_B, PORT_B);
+-		}
++		if (I915_READ(VLV_DISPLAY_BASE + DP_B) & DP_DETECTED ||
++		    intel_dp_is_edp(dev, PORT_B))
++			intel_dp_init(dev, VLV_DISPLAY_BASE + DP_B, PORT_B);
+ 
+-		if (I915_READ(VLV_DISPLAY_BASE + GEN4_HDMIC) & SDVO_DETECTED) {
++		if (I915_READ(VLV_DISPLAY_BASE + GEN4_HDMIC) & SDVO_DETECTED)
+ 			intel_hdmi_init(dev, VLV_DISPLAY_BASE + GEN4_HDMIC,
+ 					PORT_C);
+-			if (I915_READ(VLV_DISPLAY_BASE + DP_C) & DP_DETECTED)
+-				intel_dp_init(dev, VLV_DISPLAY_BASE + DP_C, PORT_C);
+-		}
++		if (I915_READ(VLV_DISPLAY_BASE + DP_C) & DP_DETECTED ||
++		    intel_dp_is_edp(dev, PORT_C))
++			intel_dp_init(dev, VLV_DISPLAY_BASE + DP_C, PORT_C);
+ 
+ 		if (IS_CHERRYVIEW(dev)) {
+-			if (I915_READ(VLV_DISPLAY_BASE + CHV_HDMID) & SDVO_DETECTED) {
++			if (I915_READ(VLV_DISPLAY_BASE + CHV_HDMID) & SDVO_DETECTED)
+ 				intel_hdmi_init(dev, VLV_DISPLAY_BASE + CHV_HDMID,
+ 						PORT_D);
+-				if (I915_READ(VLV_DISPLAY_BASE + DP_D) & DP_DETECTED)
+-					intel_dp_init(dev, VLV_DISPLAY_BASE + DP_D, PORT_D);
+-			}
++			/* eDP not supported on port D, so don't check VBT */
++			if (I915_READ(VLV_DISPLAY_BASE + DP_D) & DP_DETECTED)
++				intel_dp_init(dev, VLV_DISPLAY_BASE + DP_D, PORT_D);
+ 		}
+ 
+ 		intel_dsi_init(dev);
+@@ -12066,7 +12474,7 @@
+ 
+ 	intel_edp_psr_init(dev);
+ 
+-	list_for_each_entry(encoder, &dev->mode_config.encoder_list, base.head) {
++	for_each_intel_encoder(dev, encoder) {
+ 		encoder->base.possible_crtcs = encoder->crtc_mask;
+ 		encoder->base.possible_clones =
+ 			intel_encoder_clones(encoder);
+@@ -12112,6 +12520,8 @@
+ {
+ 	int aligned_height;
+ 	int pitch_limit;
++	int depth;
++	int bpp;
+ 	int ret;
+ 
+ 	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
+@@ -12127,6 +12537,15 @@
+ 		return -EINVAL;
+ 	}
+ 
++	drm_fb_get_bpp_depth(mode_cmd->pixel_format, &bpp, &depth);
++	if (mode_cmd->pitches[0] < intel_framebuffer_pitch_for_width(mode_cmd->width,
++								     bpp)) {
++		DRM_DEBUG("pitch (%d) must be at least the linear stride (%d)\n",
++			  mode_cmd->pitches[0],
++			  intel_framebuffer_pitch_for_width(mode_cmd->width, bpp));
++		return -EINVAL;
++	}
++
+ 	if (INTEL_INFO(dev)->gen >= 5 && !IS_VALLEYVIEW(dev)) {
+ 		pitch_limit = 32*1024;
+ 	} else if (INTEL_INFO(dev)->gen >= 4) {
+@@ -12268,16 +12687,22 @@
+ 	if (HAS_DDI(dev)) {
+ 		dev_priv->display.get_pipe_config = haswell_get_pipe_config;
+ 		dev_priv->display.get_plane_config = ironlake_get_plane_config;
+-		dev_priv->display.crtc_mode_set = haswell_crtc_mode_set;
++		dev_priv->display.crtc_compute_clock =
++			haswell_crtc_compute_clock;
+ 		dev_priv->display.crtc_enable = haswell_crtc_enable;
+ 		dev_priv->display.crtc_disable = haswell_crtc_disable;
+ 		dev_priv->display.off = ironlake_crtc_off;
+-		dev_priv->display.update_primary_plane =
+-			ironlake_update_primary_plane;
++		if (INTEL_INFO(dev)->gen >= 9)
++			dev_priv->display.update_primary_plane =
++				skylake_update_primary_plane;
++		else
++			dev_priv->display.update_primary_plane =
++				ironlake_update_primary_plane;
+ 	} else if (HAS_PCH_SPLIT(dev)) {
+ 		dev_priv->display.get_pipe_config = ironlake_get_pipe_config;
+ 		dev_priv->display.get_plane_config = ironlake_get_plane_config;
+-		dev_priv->display.crtc_mode_set = ironlake_crtc_mode_set;
++		dev_priv->display.crtc_compute_clock =
++			ironlake_crtc_compute_clock;
+ 		dev_priv->display.crtc_enable = ironlake_crtc_enable;
+ 		dev_priv->display.crtc_disable = ironlake_crtc_disable;
+ 		dev_priv->display.off = ironlake_crtc_off;
+@@ -12286,7 +12711,7 @@
+ 	} else if (IS_VALLEYVIEW(dev)) {
+ 		dev_priv->display.get_pipe_config = i9xx_get_pipe_config;
+ 		dev_priv->display.get_plane_config = i9xx_get_plane_config;
+-		dev_priv->display.crtc_mode_set = i9xx_crtc_mode_set;
++		dev_priv->display.crtc_compute_clock = i9xx_crtc_compute_clock;
+ 		dev_priv->display.crtc_enable = valleyview_crtc_enable;
+ 		dev_priv->display.crtc_disable = i9xx_crtc_disable;
+ 		dev_priv->display.off = i9xx_crtc_off;
+@@ -12295,7 +12720,7 @@
+ 	} else {
+ 		dev_priv->display.get_pipe_config = i9xx_get_pipe_config;
+ 		dev_priv->display.get_plane_config = i9xx_get_plane_config;
+-		dev_priv->display.crtc_mode_set = i9xx_crtc_mode_set;
++		dev_priv->display.crtc_compute_clock = i9xx_crtc_compute_clock;
+ 		dev_priv->display.crtc_enable = i9xx_crtc_enable;
+ 		dev_priv->display.crtc_disable = i9xx_crtc_disable;
+ 		dev_priv->display.off = i9xx_crtc_off;
+@@ -12332,33 +12757,20 @@
+ 		dev_priv->display.get_display_clock_speed =
+ 			i830_get_display_clock_speed;
+ 
+-	if (HAS_PCH_SPLIT(dev)) {
+-		if (IS_GEN5(dev)) {
+-			dev_priv->display.fdi_link_train = ironlake_fdi_link_train;
+-			dev_priv->display.write_eld = ironlake_write_eld;
+-		} else if (IS_GEN6(dev)) {
+-			dev_priv->display.fdi_link_train = gen6_fdi_link_train;
+-			dev_priv->display.write_eld = ironlake_write_eld;
+-			dev_priv->display.modeset_global_resources =
+-				snb_modeset_global_resources;
+-		} else if (IS_IVYBRIDGE(dev)) {
+-			/* FIXME: detect B0+ stepping and use auto training */
+-			dev_priv->display.fdi_link_train = ivb_manual_fdi_link_train;
+-			dev_priv->display.write_eld = ironlake_write_eld;
+-			dev_priv->display.modeset_global_resources =
+-				ivb_modeset_global_resources;
+-		} else if (IS_HASWELL(dev) || IS_GEN8(dev)) {
+-			dev_priv->display.fdi_link_train = hsw_fdi_link_train;
+-			dev_priv->display.write_eld = haswell_write_eld;
+-			dev_priv->display.modeset_global_resources =
+-				haswell_modeset_global_resources;
+-		}
+-	} else if (IS_G4X(dev)) {
+-		dev_priv->display.write_eld = g4x_write_eld;
++	if (IS_GEN5(dev)) {
++		dev_priv->display.fdi_link_train = ironlake_fdi_link_train;
++	} else if (IS_GEN6(dev)) {
++		dev_priv->display.fdi_link_train = gen6_fdi_link_train;
++	} else if (IS_IVYBRIDGE(dev)) {
++		/* FIXME: detect B0+ stepping and use auto training */
++		dev_priv->display.fdi_link_train = ivb_manual_fdi_link_train;
++		dev_priv->display.modeset_global_resources =
++			ivb_modeset_global_resources;
++	} else if (IS_HASWELL(dev) || IS_BROADWELL(dev)) {
++		dev_priv->display.fdi_link_train = hsw_fdi_link_train;
+ 	} else if (IS_VALLEYVIEW(dev)) {
+ 		dev_priv->display.modeset_global_resources =
+ 			valleyview_modeset_global_resources;
+-		dev_priv->display.write_eld = ironlake_write_eld;
+ 	}
+ 
+ 	/* Default just returns -ENODEV to indicate unsupported */
+@@ -12385,9 +12797,14 @@
+ 	case 8: /* FIXME(BDW): Check that the gen8 RCS flip works. */
+ 		dev_priv->display.queue_flip = intel_gen7_queue_flip;
+ 		break;
++	case 9:
++		dev_priv->display.queue_flip = intel_gen9_queue_flip;
++		break;
+ 	}
+ 
+ 	intel_panel_init_backlight_funcs(dev);
++
++	mutex_init(&dev_priv->pps_mutex);
+ }
+ 
+ /*
+@@ -12403,6 +12820,14 @@
+ 	DRM_INFO("applying pipe a force quirk\n");
+ }
+ 
++static void quirk_pipeb_force(struct drm_device *dev)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++
++	dev_priv->quirks |= QUIRK_PIPEB_FORCE;
++	DRM_INFO("applying pipe b force quirk\n");
++}
++
+ /*
+  * Some machines (Lenovo U160) do not work with SSC on LVDS for some reason
+  */
+@@ -12477,6 +12902,12 @@
+ 	/* ThinkPad T60 needs pipe A force quirk (bug #16494) */
+ 	{ 0x2782, 0x17aa, 0x201a, quirk_pipea_force },
+ 
++	/* 830 needs to leave pipe A & dpll A up */
++	{ 0x3577, PCI_ANY_ID, PCI_ANY_ID, quirk_pipea_force },
++
++	/* 830 needs to leave pipe B & dpll B up */
++	{ 0x3577, PCI_ANY_ID, PCI_ANY_ID, quirk_pipeb_force },
++
+ 	/* Lenovo U160 cannot use SSC on LVDS */
+ 	{ 0x0046, 0x17aa, 0x3920, quirk_ssc_force_disable },
+ 
+@@ -12507,6 +12938,9 @@
+ 	/* Acer C720 Chromebook (Core i3 4005U) */
+ 	{ 0x0a16, 0x1025, 0x0a11, quirk_backlight_present },
+ 
++	/* Apple Macbook 2,1 (Core 2 T7400) */
++	{ 0x27a2, 0x8086, 0x7270, quirk_backlight_present },
++
+ 	/* Toshiba CB35 Chromebook (Celeron 2955U) */
+ 	{ 0x0a06, 0x1179, 0x0a88, quirk_backlight_present },
+ 
+@@ -12550,7 +12984,11 @@
+ 	vga_put(dev->pdev, VGA_RSRC_LEGACY_IO);
+ 	udelay(300);
+ 
+-	I915_WRITE(vga_reg, VGA_DISP_DISABLE);
++	/*
++	 * Fujitsu-Siemens Lifebook S6010 (830) has problems resuming
++	 * from S3 without preserving (some of?) the other bits.
++	 */
++	I915_WRITE(vga_reg, dev_priv->bios_vgacntr | VGA_DISP_DISABLE);
+ 	POSTING_READ(vga_reg);
+ }
+ 
+@@ -12563,16 +13001,9 @@
+ 
+ 	intel_init_clock_gating(dev);
+ 
+-	intel_reset_dpio(dev);
+-
+ 	intel_enable_gt_powersave(dev);
+ }
+ 
+-void intel_modeset_suspend_hw(struct drm_device *dev)
+-{
+-	intel_suspend_hw(dev);
+-}
+-
+ void intel_modeset_init(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+@@ -12598,6 +13029,7 @@
+ 		return;
+ 
+ 	intel_init_display(dev);
++	intel_init_audio(dev);
+ 
+ 	if (IS_GEN2(dev)) {
+ 		dev->mode_config.max_width = 2048;
+@@ -12610,7 +13042,10 @@
+ 		dev->mode_config.max_height = 8192;
+ 	}
+ 
+-	if (IS_GEN2(dev)) {
++	if (IS_845G(dev) || IS_I865G(dev)) {
++		dev->mode_config.cursor_width = IS_845G(dev) ? 64 : 512;
++		dev->mode_config.cursor_height = 1023;
++	} else if (IS_GEN2(dev)) {
+ 		dev->mode_config.cursor_width = GEN2_CURSOR_WIDTH;
+ 		dev->mode_config.cursor_height = GEN2_CURSOR_HEIGHT;
+ 	} else {
+@@ -12618,13 +13053,16 @@
+ 		dev->mode_config.cursor_height = MAX_CURSOR_HEIGHT;
+ 	}
+ 
++	if (INTEL_INFO(dev)->gen >= 6)
++		dev->mode_config.async_page_flip = true;
++
+ 	dev->mode_config.fb_base = dev_priv->gtt.mappable_base;
+ 
+ 	DRM_DEBUG_KMS("%d display pipe%s available.\n",
+ 		      INTEL_INFO(dev)->num_pipes,
+ 		      INTEL_INFO(dev)->num_pipes > 1 ? "s" : "");
+ 
+-	for_each_pipe(pipe) {
++	for_each_pipe(dev_priv, pipe) {
+ 		intel_crtc_init(dev, pipe);
+ 		for_each_sprite(pipe, sprite) {
+ 			ret = intel_plane_init(dev, pipe, sprite);
+@@ -12635,10 +13073,11 @@
+ 	}
+ 
+ 	intel_init_dpio(dev);
+-	intel_reset_dpio(dev);
+ 
+ 	intel_shared_dpll_init(dev);
+ 
++	/* save the BIOS value before clobbering it */
++	dev_priv->bios_vgacntr = I915_READ(i915_vgacntrl_reg(dev));
+ 	/* Just disable it once at startup */
+ 	i915_disable_vga(dev);
+ 	intel_setup_outputs(dev);
+@@ -12730,9 +13169,10 @@
+ 	I915_WRITE(reg, I915_READ(reg) & ~PIPECONF_FRAME_START_DELAY_MASK);
+ 
+ 	/* restore vblank interrupts to correct state */
+-	if (crtc->active)
++	if (crtc->active) {
++		update_scanline_offset(crtc);
+ 		drm_vblank_on(dev, crtc->pipe);
+-	else
++	} else
+ 		drm_vblank_off(dev, crtc->pipe);
+ 
+ 	/* We need to sanitize the plane -> pipe mapping first because this will
+@@ -12788,6 +13228,7 @@
+ 	/* Adjust the state of the output pipe according to whether we
+ 	 * have active connectors/encoders. */
+ 	intel_crtc_update_dpms(&crtc->base);
++	intel_crtc_update_cursor(&crtc->base, crtc->active && crtc->cursor_bo);
+ 
+ 	if (crtc->active != crtc->base.enabled) {
+ 		struct intel_encoder *encoder;
+@@ -12815,7 +13256,7 @@
+ 		}
+ 	}
+ 
+-	if (crtc->active || IS_VALLEYVIEW(dev) || INTEL_INFO(dev)->gen < 5) {
++	if (crtc->active || HAS_GMCH_DISPLAY(dev)) {
+ 		/*
+ 		 * We start out with underrun reporting disabled to avoid races.
+ 		 * For correct bookkeeping mark this on active crtcs.
+@@ -12831,8 +13272,6 @@
+ 		 */
+ 		crtc->cpu_fifo_underrun_disabled = true;
+ 		crtc->pch_fifo_underrun_disabled = true;
+-
+-		update_scanline_offset(crtc);
+ 	}
+ }
+ 
+@@ -12905,7 +13344,7 @@
+ 	 * level, just check if the power well is enabled instead of trying to
+ 	 * follow the "don't touch the power well if we don't need it" policy
+ 	 * the rest of the driver uses. */
+-	if (!intel_display_power_enabled(dev_priv, POWER_DOMAIN_VGA))
++	if (!intel_display_power_is_enabled(dev_priv, POWER_DOMAIN_VGA))
+ 		return;
+ 
+ 	i915_redisable_vga_power_on(dev);
+@@ -12949,23 +13388,25 @@
+ 	for (i = 0; i < dev_priv->num_shared_dpll; i++) {
+ 		struct intel_shared_dpll *pll = &dev_priv->shared_dplls[i];
+ 
+-		pll->on = pll->get_hw_state(dev_priv, pll, &pll->hw_state);
++		pll->on = pll->get_hw_state(dev_priv, pll,
++					    &pll->config.hw_state);
+ 		pll->active = 0;
++		pll->config.crtc_mask = 0;
+ 		for_each_intel_crtc(dev, crtc) {
+-			if (crtc->active && intel_crtc_to_shared_dpll(crtc) == pll)
++			if (crtc->active && intel_crtc_to_shared_dpll(crtc) == pll) {
+ 				pll->active++;
++				pll->config.crtc_mask |= 1 << crtc->pipe;
++			}
+ 		}
+-		pll->refcount = pll->active;
+ 
+-		DRM_DEBUG_KMS("%s hw state readout: refcount %i, on %i\n",
+-			      pll->name, pll->refcount, pll->on);
++		DRM_DEBUG_KMS("%s hw state readout: crtc_mask 0x%08x, on %i\n",
++			      pll->name, pll->config.crtc_mask, pll->on);
+ 
+-		if (pll->refcount)
++		if (pll->config.crtc_mask)
+ 			intel_display_power_get(dev_priv, POWER_DOMAIN_PLLS);
+ 	}
+ 
+-	list_for_each_entry(encoder, &dev->mode_config.encoder_list,
+-			    base.head) {
++	for_each_intel_encoder(dev, encoder) {
+ 		pipe = 0;
+ 
+ 		if (encoder->get_hw_state(encoder, &pipe)) {
+@@ -13020,7 +13461,7 @@
+ 	 * checking everywhere.
+ 	 */
+ 	for_each_intel_crtc(dev, crtc) {
+-		if (crtc->active && i915.fastboot) {
++		if (crtc->active && i915_module.fastboot) {
+ 			intel_mode_from_pipe_config(&crtc->base.mode, &crtc->config);
+ 			DRM_DEBUG_KMS("[CRTC:%d] found active mode: ",
+ 				      crtc->base.base.id);
+@@ -13029,12 +13470,11 @@
+ 	}
+ 
+ 	/* HW state is read out, now we need to sanitize this mess. */
+-	list_for_each_entry(encoder, &dev->mode_config.encoder_list,
+-			    base.head) {
++	for_each_intel_encoder(dev, encoder) {
+ 		intel_sanitize_encoder(encoder);
+ 	}
+ 
+-	for_each_pipe(pipe) {
++	for_each_pipe(dev_priv, pipe) {
+ 		crtc = to_intel_crtc(dev_priv->pipe_to_crtc_mapping[pipe]);
+ 		intel_sanitize_crtc(crtc);
+ 		intel_dump_pipe_config(crtc, &crtc->config, "[setup_hw_state]");
+@@ -13052,7 +13492,9 @@
+ 		pll->on = false;
+ 	}
+ 
+-	if (HAS_PCH_SPLIT(dev))
++	if (IS_GEN9(dev))
++		skl_wm_get_hw_state(dev);
++	else if (HAS_PCH_SPLIT(dev))
+ 		ilk_wm_get_hw_state(dev);
+ 
+ 	if (force_restore) {
+@@ -13062,12 +13504,12 @@
+ 		 * We need to use raw interfaces for restoring state to avoid
+ 		 * checking (bogus) intermediate states.
+ 		 */
+-		for_each_pipe(pipe) {
++		for_each_pipe(dev_priv, pipe) {
+ 			struct drm_crtc *crtc =
+ 				dev_priv->pipe_to_crtc_mapping[pipe];
+ 
+-			__intel_set_mode(crtc, &crtc->mode, crtc->x, crtc->y,
+-					 crtc->primary->fb);
++			intel_set_mode(crtc, &crtc->mode, crtc->x, crtc->y,
++				       crtc->primary->fb);
+ 		}
+ 	} else {
+ 		intel_modeset_update_staged_output_state(dev);
+@@ -13078,6 +13520,7 @@
+ 
+ void intel_modeset_gem_init(struct drm_device *dev)
+ {
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct drm_crtc *c;
+ 	struct drm_i915_gem_object *obj;
+ 
+@@ -13085,6 +13528,16 @@
+ 	intel_init_gt_powersave(dev);
+ 	mutex_unlock(&dev->struct_mutex);
+ 
++	/*
++	 * There may be no VBT; and if the BIOS enabled SSC we can
++	 * just keep using it to avoid unnecessary flicker.  Whereas if the
++	 * BIOS isn't using it, don't assume it will work even if the VBT
++	 * indicates as much.
++	 */
++	if (HAS_PCH_IBX(dev) || HAS_PCH_CPT(dev))
++		dev_priv->vbt.lvds_use_ssc = !!(I915_READ(PCH_DREF_CONTROL) &
++						DREF_SSC1_ENABLE);
++
+ 	intel_modeset_init_hw(dev);
+ 
+ 	intel_setup_overlay(dev);
+@@ -13100,7 +13553,9 @@
+ 		if (obj == NULL)
+ 			continue;
+ 
+-		if (intel_pin_and_fence_fb_obj(dev, obj, NULL)) {
++		if (intel_pin_and_fence_fb_obj(c->primary,
++					       c->primary->fb,
++					       NULL)) {
+ 			DRM_ERROR("failed to pin boot fb on pipe %d\n",
+ 				  to_intel_crtc(c)->pipe);
+ 			drm_framebuffer_unreference(c->primary->fb);
+@@ -13108,6 +13563,8 @@
+ 		}
+ 	}
+ 	mutex_unlock(&dev->struct_mutex);
++
++	intel_backlight_register(dev);
+ }
+ 
+ void intel_connector_unregister(struct intel_connector *intel_connector)
+@@ -13123,14 +13580,14 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct drm_connector *connector;
+ 
++	intel_backlight_unregister(dev);
++
+ 	/*
+ 	 * Interrupts and polling as the first thing to avoid creating havoc.
+ 	 * Too much stuff here (turning of rps, connectors, ...) would
+ 	 * experience fancy races otherwise.
+ 	 */
+-	drm_irq_uninstall(dev);
+-	intel_hpd_cancel_work(dev_priv);
+-	dev_priv->pm._irqs_disabled = true;
++	intel_irq_uninstall(dev_priv);
+ 
+ 	/*
+ 	 * Due to the hpd irq storm handling the hotplug work can re-arm the
+@@ -13283,10 +13740,10 @@
+ 	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
+ 		error->power_well_driver = I915_READ(HSW_PWR_WELL_DRIVER);
+ 
+-	for_each_pipe(i) {
++	for_each_pipe(dev_priv, i) {
+ 		error->pipe[i].power_domain_on =
+-			intel_display_power_enabled_unlocked(dev_priv,
+-							   POWER_DOMAIN_PIPE(i));
++			__intel_display_power_is_enabled(dev_priv,
++							 POWER_DOMAIN_PIPE(i));
+ 		if (!error->pipe[i].power_domain_on)
+ 			continue;
+ 
+@@ -13321,7 +13778,7 @@
+ 		enum transcoder cpu_transcoder = transcoders[i];
+ 
+ 		error->transcoder[i].power_domain_on =
+-			intel_display_power_enabled_unlocked(dev_priv,
++			__intel_display_power_is_enabled(dev_priv,
+ 				POWER_DOMAIN_TRANSCODER(cpu_transcoder));
+ 		if (!error->transcoder[i].power_domain_on)
+ 			continue;
+@@ -13347,6 +13804,7 @@
+ 				struct drm_device *dev,
+ 				struct intel_display_error_state *error)
+ {
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	int i;
+ 
+ 	if (!error)
+@@ -13356,7 +13814,7 @@
+ 	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
+ 		err_printf(m, "PWR_WELL_CTL2: %08x\n",
+ 			   error->power_well_driver);
+-	for_each_pipe(i) {
++	for_each_pipe(dev_priv, i) {
+ 		err_printf(m, "Pipe [%d]:\n", i);
+ 		err_printf(m, "  Power: %s\n",
+ 			   error->pipe[i].power_domain_on ? "on" : "off");
+@@ -13397,3 +13855,24 @@
+ 		err_printf(m, "  VSYNC: %08x\n", error->transcoder[i].vsync);
+ 	}
+ }
++
++void intel_modeset_preclose(struct drm_device *dev, struct drm_file *file)
++{
++	struct intel_crtc *crtc;
++
++	for_each_intel_crtc(dev, crtc) {
++		struct intel_unpin_work *work;
++
++		spin_lock_irq(&dev->event_lock);
++
++		work = crtc->unpin_work;
++
++		if (work && work->event &&
++		    work->event->base.file_priv == file) {
++			kfree(work->event);
++			work->event = NULL;
++		}
++
++		spin_unlock_irq(&dev->event_lock);
++	}
++}
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_dp.c b/drivers/gpu/drm/i915/intel_dp.c
+--- a/drivers/gpu/drm/i915/intel_dp.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_dp.c	2014-11-20 09:53:37.984762837 -0700
+@@ -111,8 +111,11 @@
+ }
+ 
+ static void intel_dp_link_down(struct intel_dp *intel_dp);
+-static bool _edp_panel_vdd_on(struct intel_dp *intel_dp);
++static bool edp_panel_vdd_on(struct intel_dp *intel_dp);
+ static void edp_panel_vdd_off(struct intel_dp *intel_dp, bool sync);
++static void vlv_init_panel_power_sequencer(struct intel_dp *intel_dp);
++static void vlv_steal_power_sequencer(struct drm_device *dev,
++				      enum pipe pipe);
+ 
+ int
+ intel_dp_max_link_bw(struct intel_dp *intel_dp)
+@@ -212,6 +215,10 @@
+ 	max_rate = intel_dp_max_data_rate(max_link_clock, max_lanes);
+ 	mode_rate = intel_dp_link_required(target_clock, 18);
+ 
++	DRM_DEBUG_KMS("mode %s: maximum link clock %d with %d lanes,"
++		      " giving a maximum dot clock of %d but mode requires %d\n",
++		     mode->name,  max_link_clock, max_lanes, max_rate, mode_rate);
++
+ 	if (mode_rate > max_rate)
+ 		return MODE_CLOCK_HIGH;
+ 
+@@ -225,7 +232,7 @@
+ }
+ 
+ static uint32_t
+-pack_aux(uint8_t *src, int src_bytes)
++pack_aux(const uint8_t *src, int src_bytes)
+ {
+ 	int	i;
+ 	uint32_t v = 0;
+@@ -283,39 +290,275 @@
+ 
+ static void
+ intel_dp_init_panel_power_sequencer(struct drm_device *dev,
+-				    struct intel_dp *intel_dp,
+-				    struct edp_power_seq *out);
++				    struct intel_dp *intel_dp);
+ static void
+ intel_dp_init_panel_power_sequencer_registers(struct drm_device *dev,
+-					      struct intel_dp *intel_dp,
+-					      struct edp_power_seq *out);
++					      struct intel_dp *intel_dp);
++
++static void pps_lock(struct intel_dp *intel_dp)
++{
++	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
++	struct intel_encoder *encoder = &intel_dig_port->base;
++	struct drm_device *dev = encoder->base.dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	enum intel_display_power_domain power_domain;
++
++	/*
++	 * See vlv_power_sequencer_reset() why we need
++	 * a power domain reference here.
++	 */
++	power_domain = intel_display_port_power_domain(encoder);
++	intel_display_power_get(dev_priv, power_domain);
++
++	mutex_lock(&dev_priv->pps_mutex);
++}
++
++static void pps_unlock(struct intel_dp *intel_dp)
++{
++	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
++	struct intel_encoder *encoder = &intel_dig_port->base;
++	struct drm_device *dev = encoder->base.dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	enum intel_display_power_domain power_domain;
++
++	mutex_unlock(&dev_priv->pps_mutex);
++
++	power_domain = intel_display_port_power_domain(encoder);
++	intel_display_power_put(dev_priv, power_domain);
++}
++
++static void
++vlv_power_sequencer_kick(struct intel_dp *intel_dp)
++{
++	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
++	struct drm_device *dev = intel_dig_port->base.base.dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	enum pipe pipe = intel_dp->pps_pipe;
++	bool pll_enabled;
++	uint32_t DP;
++
++	if (WARN(I915_READ(intel_dp->output_reg) & DP_PORT_EN,
++		 "skipping pipe %c power seqeuncer kick due to port %c being active\n",
++		 pipe_name(pipe), port_name(intel_dig_port->port)))
++		return;
++
++	DRM_DEBUG_KMS("kicking pipe %c power sequencer for port %c\n",
++		      pipe_name(pipe), port_name(intel_dig_port->port));
++
++	/* Preserve the BIOS-computed detected bit. This is
++	 * supposed to be read-only.
++	 */
++	DP = I915_READ(intel_dp->output_reg) & DP_DETECTED;
++	DP |= DP_VOLTAGE_0_4 | DP_PRE_EMPHASIS_0;
++	DP |= DP_PORT_WIDTH(1);
++	DP |= DP_LINK_TRAIN_PAT_1;
++
++	if (IS_CHERRYVIEW(dev))
++		DP |= DP_PIPE_SELECT_CHV(pipe);
++	else if (pipe == PIPE_B)
++		DP |= DP_PIPEB_SELECT;
++
++	pll_enabled = I915_READ(DPLL(pipe)) & DPLL_VCO_ENABLE;
++
++	/*
++	 * The DPLL for the pipe must be enabled for this to work.
++	 * So enable temporarily it if it's not already enabled.
++	 */
++	if (!pll_enabled)
++		vlv_force_pll_on(dev, pipe, IS_CHERRYVIEW(dev) ?
++				 &chv_dpll[0].dpll : &vlv_dpll[0].dpll);
++
++	/*
++	 * Similar magic as in intel_dp_enable_port().
++	 * We _must_ do this port enable + disable trick
++	 * to make this power seqeuencer lock onto the port.
++	 * Otherwise even VDD force bit won't work.
++	 */
++	I915_WRITE(intel_dp->output_reg, DP);
++	POSTING_READ(intel_dp->output_reg);
++
++	I915_WRITE(intel_dp->output_reg, DP | DP_PORT_EN);
++	POSTING_READ(intel_dp->output_reg);
++
++	I915_WRITE(intel_dp->output_reg, DP & ~DP_PORT_EN);
++	POSTING_READ(intel_dp->output_reg);
++
++	if (!pll_enabled)
++		vlv_force_pll_off(dev, pipe);
++}
+ 
+ static enum pipe
+ vlv_power_sequencer_pipe(struct intel_dp *intel_dp)
+ {
+ 	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+-	struct drm_crtc *crtc = intel_dig_port->base.base.crtc;
+ 	struct drm_device *dev = intel_dig_port->base.base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	enum port port = intel_dig_port->port;
++	struct intel_encoder *encoder;
++	unsigned int pipes = (1 << PIPE_A) | (1 << PIPE_B);
+ 	enum pipe pipe;
+ 
+-	/* modeset should have pipe */
+-	if (crtc)
+-		return to_intel_crtc(crtc)->pipe;
++	lockdep_assert_held(&dev_priv->pps_mutex);
++
++	/* We should never land here with regular DP ports */
++	WARN_ON(!is_edp(intel_dp));
++
++	if (intel_dp->pps_pipe != INVALID_PIPE)
++		return intel_dp->pps_pipe;
++
++	/*
++	 * We don't have power sequencer currently.
++	 * Pick one that's not used by other ports.
++	 */
++	list_for_each_entry(encoder, &dev->mode_config.encoder_list,
++			    base.head) {
++		struct intel_dp *tmp;
++
++		if (encoder->type != INTEL_OUTPUT_EDP)
++			continue;
++
++		tmp = enc_to_intel_dp(&encoder->base);
++
++		if (tmp->pps_pipe != INVALID_PIPE)
++			pipes &= ~(1 << tmp->pps_pipe);
++	}
++
++	/*
++	 * Didn't find one. This should not happen since there
++	 * are two power sequencers and up to two eDP ports.
++	 */
++	if (WARN_ON(pipes == 0))
++		pipe = PIPE_A;
++	else
++		pipe = ffs(pipes) - 1;
++
++	vlv_steal_power_sequencer(dev, pipe);
++	intel_dp->pps_pipe = pipe;
++
++	DRM_DEBUG_KMS("picked pipe %c power sequencer for port %c\n",
++		      pipe_name(intel_dp->pps_pipe),
++		      port_name(intel_dig_port->port));
++
++	/* init power sequencer on this pipe and port */
++	intel_dp_init_panel_power_sequencer(dev, intel_dp);
++	intel_dp_init_panel_power_sequencer_registers(dev, intel_dp);
++
++	/*
++	 * Even vdd force doesn't work until we've made
++	 * the power sequencer lock in on the port.
++	 */
++	vlv_power_sequencer_kick(intel_dp);
++
++	return intel_dp->pps_pipe;
++}
++
++typedef bool (*vlv_pipe_check)(struct drm_i915_private *dev_priv,
++			       enum pipe pipe);
++
++static bool vlv_pipe_has_pp_on(struct drm_i915_private *dev_priv,
++			       enum pipe pipe)
++{
++	return I915_READ(VLV_PIPE_PP_STATUS(pipe)) & PP_ON;
++}
++
++static bool vlv_pipe_has_vdd_on(struct drm_i915_private *dev_priv,
++				enum pipe pipe)
++{
++	return I915_READ(VLV_PIPE_PP_CONTROL(pipe)) & EDP_FORCE_VDD;
++}
++
++static bool vlv_pipe_any(struct drm_i915_private *dev_priv,
++			 enum pipe pipe)
++{
++	return true;
++}
++
++static enum pipe
++vlv_initial_pps_pipe(struct drm_i915_private *dev_priv,
++		     enum port port,
++		     vlv_pipe_check pipe_check)
++{
++	enum pipe pipe;
+ 
+-	/* init time, try to find a pipe with this port selected */
+ 	for (pipe = PIPE_A; pipe <= PIPE_B; pipe++) {
+ 		u32 port_sel = I915_READ(VLV_PIPE_PP_ON_DELAYS(pipe)) &
+ 			PANEL_PORT_SELECT_MASK;
+-		if (port_sel == PANEL_PORT_SELECT_DPB_VLV && port == PORT_B)
+-			return pipe;
+-		if (port_sel == PANEL_PORT_SELECT_DPC_VLV && port == PORT_C)
+-			return pipe;
++
++		if (port_sel != PANEL_PORT_SELECT_VLV(port))
++			continue;
++
++		if (!pipe_check(dev_priv, pipe))
++			continue;
++
++		return pipe;
++	}
++
++	return INVALID_PIPE;
++}
++
++static void
++vlv_initial_power_sequencer_setup(struct intel_dp *intel_dp)
++{
++	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
++	struct drm_device *dev = intel_dig_port->base.base.dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	enum port port = intel_dig_port->port;
++
++	lockdep_assert_held(&dev_priv->pps_mutex);
++
++	/* try to find a pipe with this port selected */
++	/* first pick one where the panel is on */
++	intel_dp->pps_pipe = vlv_initial_pps_pipe(dev_priv, port,
++						  vlv_pipe_has_pp_on);
++	/* didn't find one? pick one where vdd is on */
++	if (intel_dp->pps_pipe == INVALID_PIPE)
++		intel_dp->pps_pipe = vlv_initial_pps_pipe(dev_priv, port,
++							  vlv_pipe_has_vdd_on);
++	/* didn't find one? pick one with just the correct port */
++	if (intel_dp->pps_pipe == INVALID_PIPE)
++		intel_dp->pps_pipe = vlv_initial_pps_pipe(dev_priv, port,
++							  vlv_pipe_any);
++
++	/* didn't find one? just let vlv_power_sequencer_pipe() pick one when needed */
++	if (intel_dp->pps_pipe == INVALID_PIPE) {
++		DRM_DEBUG_KMS("no initial power sequencer for port %c\n",
++			      port_name(port));
++		return;
+ 	}
+ 
+-	/* shrug */
+-	return PIPE_A;
++	DRM_DEBUG_KMS("initial power sequencer for port %c: pipe %c\n",
++		      port_name(port), pipe_name(intel_dp->pps_pipe));
++
++	intel_dp_init_panel_power_sequencer(dev, intel_dp);
++	intel_dp_init_panel_power_sequencer_registers(dev, intel_dp);
++}
++
++void vlv_power_sequencer_reset(struct drm_i915_private *dev_priv)
++{
++	struct drm_device *dev = dev_priv->dev;
++	struct intel_encoder *encoder;
++
++	if (WARN_ON(!IS_VALLEYVIEW(dev)))
++		return;
++
++	/*
++	 * We can't grab pps_mutex here due to deadlock with power_domain
++	 * mutex when power_domain functions are called while holding pps_mutex.
++	 * That also means that in order to use pps_pipe the code needs to
++	 * hold both a power domain reference and pps_mutex, and the power domain
++	 * reference get/put must be done while _not_ holding pps_mutex.
++	 * pps_{lock,unlock}() do these steps in the correct order, so one
++	 * should use them always.
++	 */
++
++	list_for_each_entry(encoder, &dev->mode_config.encoder_list, base.head) {
++		struct intel_dp *intel_dp;
++
++		if (encoder->type != INTEL_OUTPUT_EDP)
++			continue;
++
++		intel_dp = enc_to_intel_dp(&encoder->base);
++		intel_dp->pps_pipe = INVALID_PIPE;
++	}
+ }
+ 
+ static u32 _pp_ctrl_reg(struct intel_dp *intel_dp)
+@@ -349,12 +592,15 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	u32 pp_div;
+ 	u32 pp_ctrl_reg, pp_div_reg;
+-	enum pipe pipe = vlv_power_sequencer_pipe(intel_dp);
+ 
+ 	if (!is_edp(intel_dp) || code != SYS_RESTART)
+ 		return 0;
+ 
++	pps_lock(intel_dp);
++
+ 	if (IS_VALLEYVIEW(dev)) {
++		enum pipe pipe = vlv_power_sequencer_pipe(intel_dp);
++
+ 		pp_ctrl_reg = VLV_PIPE_PP_CONTROL(pipe);
+ 		pp_div_reg  = VLV_PIPE_PP_DIVISOR(pipe);
+ 		pp_div = I915_READ(pp_div_reg);
+@@ -366,6 +612,8 @@
+ 		msleep(intel_dp->panel_power_cycle_delay);
+ 	}
+ 
++	pps_unlock(intel_dp);
++
+ 	return 0;
+ }
+ 
+@@ -374,6 +622,12 @@
+ 	struct drm_device *dev = intel_dp_to_dev(intel_dp);
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
++	lockdep_assert_held(&dev_priv->pps_mutex);
++
++	if (IS_VALLEYVIEW(dev) &&
++	    intel_dp->pps_pipe == INVALID_PIPE)
++		return false;
++
+ 	return (I915_READ(_pp_stat_reg(intel_dp)) & PP_ON) != 0;
+ }
+ 
+@@ -381,13 +635,14 @@
+ {
+ 	struct drm_device *dev = intel_dp_to_dev(intel_dp);
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+-	struct intel_encoder *intel_encoder = &intel_dig_port->base;
+-	enum intel_display_power_domain power_domain;
+ 
+-	power_domain = intel_display_port_power_domain(intel_encoder);
+-	return intel_display_power_enabled(dev_priv, power_domain) &&
+-	       (I915_READ(_pp_ctrl_reg(intel_dp)) & EDP_FORCE_VDD) != 0;
++	lockdep_assert_held(&dev_priv->pps_mutex);
++
++	if (IS_VALLEYVIEW(dev) &&
++	    intel_dp->pps_pipe == INVALID_PIPE)
++		return false;
++
++	return I915_READ(_pp_ctrl_reg(intel_dp)) & EDP_FORCE_VDD;
+ }
+ 
+ static void
+@@ -488,6 +743,16 @@
+ 	return index ? 0 : 100;
+ }
+ 
++static uint32_t skl_get_aux_clock_divider(struct intel_dp *intel_dp, int index)
++{
++	/*
++	 * SKL doesn't need us to program the AUX clock divider (Hardware will
++	 * derive the clock from CDCLK automatically). We still implement the
++	 * get_aux_clock_divider vfunc to plug-in into the existing code.
++	 */
++	return index ? 0 : 1;
++}
++
+ static uint32_t i9xx_get_aux_send_ctl(struct intel_dp *intel_dp,
+ 				      bool has_aux_irq,
+ 				      int send_bytes,
+@@ -518,9 +783,24 @@
+ 	       (aux_clock_divider << DP_AUX_CH_CTL_BIT_CLOCK_2X_SHIFT);
+ }
+ 
++static uint32_t skl_get_aux_send_ctl(struct intel_dp *intel_dp,
++				      bool has_aux_irq,
++				      int send_bytes,
++				      uint32_t unused)
++{
++	return DP_AUX_CH_CTL_SEND_BUSY |
++	       DP_AUX_CH_CTL_DONE |
++	       (has_aux_irq ? DP_AUX_CH_CTL_INTERRUPT : 0) |
++	       DP_AUX_CH_CTL_TIME_OUT_ERROR |
++	       DP_AUX_CH_CTL_TIME_OUT_1600us |
++	       DP_AUX_CH_CTL_RECEIVE_ERROR |
++	       (send_bytes << DP_AUX_CH_CTL_MESSAGE_SIZE_SHIFT) |
++	       DP_AUX_CH_CTL_SYNC_PULSE_SKL(32);
++}
++
+ static int
+ intel_dp_aux_ch(struct intel_dp *intel_dp,
+-		uint8_t *send, int send_bytes,
++		const uint8_t *send, int send_bytes,
+ 		uint8_t *recv, int recv_size)
+ {
+ 	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+@@ -535,7 +815,15 @@
+ 	bool has_aux_irq = HAS_AUX_IRQ(dev);
+ 	bool vdd;
+ 
+-	vdd = _edp_panel_vdd_on(intel_dp);
++	pps_lock(intel_dp);
++
++	/*
++	 * We will be called with VDD already enabled for dpcd/edid/oui reads.
++	 * In such cases we want to leave VDD enabled and it's up to upper layers
++	 * to turn it off. But for eg. i2c-dev access we need to turn it on/off
++	 * ourselves.
++	 */
++	vdd = edp_panel_vdd_on(intel_dp);
+ 
+ 	/* dp aux is extremely sensitive to irq latency, hence request the
+ 	 * lowest possible wakeup latency and so prevent the cpu from going into
+@@ -644,6 +932,8 @@
+ 	if (vdd)
+ 		edp_panel_vdd_off(intel_dp, false);
+ 
++	pps_unlock(intel_dp);
++
+ 	return ret;
+ }
+ 
+@@ -742,7 +1032,16 @@
+ 		BUG();
+ 	}
+ 
+-	if (!HAS_DDI(dev))
++	/*
++	 * The AUX_CTL register is usually DP_CTL + 0x10.
++	 *
++	 * On Haswell and Broadwell though:
++	 *   - Both port A DDI_BUF_CTL and DDI_AUX_CTL are on the CPU
++	 *   - Port B/C/D AUX channels are on the PCH, DDI_BUF_CTL on the CPU
++	 *
++	 * Skylake moves AUX_CTL back next to DDI_BUF_CTL, on the CPU.
++	 */
++	if (!IS_HASWELL(dev) && !IS_BROADWELL(dev))
+ 		intel_dp->aux_ch_ctl_reg = intel_dp->output_reg + 0x10;
+ 
+ 	intel_dp->aux.name = name;
+@@ -828,20 +1127,6 @@
+ 	}
+ }
+ 
+-static void
+-intel_dp_set_m2_n2(struct intel_crtc *crtc, struct intel_link_m_n *m_n)
+-{
+-	struct drm_device *dev = crtc->base.dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	enum transcoder transcoder = crtc->config.cpu_transcoder;
+-
+-	I915_WRITE(PIPE_DATA_M2(transcoder),
+-		TU_SIZE(m_n->tu) | m_n->gmch_m);
+-	I915_WRITE(PIPE_DATA_N2(transcoder), m_n->gmch_n);
+-	I915_WRITE(PIPE_LINK_M2(transcoder), m_n->link_m);
+-	I915_WRITE(PIPE_LINK_N2(transcoder), m_n->link_n);
+-}
+-
+ bool
+ intel_dp_compute_config(struct intel_encoder *encoder,
+ 			struct intel_crtc_config *pipe_config)
+@@ -867,6 +1152,7 @@
+ 		pipe_config->has_pch_encoder = true;
+ 
+ 	pipe_config->has_dp_encoder = true;
++	pipe_config->has_drrs = false;
+ 	pipe_config->has_audio = intel_dp->has_audio;
+ 
+ 	if (is_edp(intel_dp) && intel_connector->panel.fixed_mode) {
+@@ -898,23 +1184,15 @@
+ 			bpp = dev_priv->vbt.edp_bpp;
+ 		}
+ 
+-		if (IS_BROADWELL(dev)) {
+-			/* Yes, it's an ugly hack. */
+-			min_lane_count = max_lane_count;
+-			DRM_DEBUG_KMS("forcing lane count to max (%u) on BDW\n",
+-				      min_lane_count);
+-		} else if (dev_priv->vbt.edp_lanes) {
+-			min_lane_count = min(dev_priv->vbt.edp_lanes,
+-					     max_lane_count);
+-			DRM_DEBUG_KMS("using min %u lanes per VBT\n",
+-				      min_lane_count);
+-		}
+-
+-		if (dev_priv->vbt.edp_rate) {
+-			min_clock = min(dev_priv->vbt.edp_rate >> 3, max_clock);
+-			DRM_DEBUG_KMS("using min %02x link bw per VBT\n",
+-				      bws[min_clock]);
+-		}
++		/*
++		 * Use the maximum clock and number of lanes the eDP panel
++		 * advertizes being capable of. The panels are generally
++		 * designed to support only a single clock and lane
++		 * configuration, and typically these values correspond to the
++		 * native resolution of the panel.
++		 */
++		min_lane_count = max_lane_count;
++		min_clock = max_clock;
+ 	}
+ 
+ 	for (; bpp >= 6*3; bpp -= 2*3) {
+@@ -970,13 +1248,14 @@
+ 
+ 	if (intel_connector->panel.downclock_mode != NULL &&
+ 		intel_dp->drrs_state.type == SEAMLESS_DRRS_SUPPORT) {
++			pipe_config->has_drrs = true;
+ 			intel_link_compute_m_n(bpp, lane_count,
+ 				intel_connector->panel.downclock_mode->clock,
+ 				pipe_config->port_clock,
+ 				&pipe_config->dp_m2_n2);
+ 	}
+ 
+-	if (HAS_DDI(dev))
++	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
+ 		hsw_dp_set_ddi_pll_sel(pipe_config, intel_dp->link_bw);
+ 	else
+ 		intel_dp_set_clock(encoder, pipe_config, intel_dp->link_bw);
+@@ -1049,12 +1328,8 @@
+ 	intel_dp->DP |= DP_VOLTAGE_0_4 | DP_PRE_EMPHASIS_0;
+ 	intel_dp->DP |= DP_PORT_WIDTH(intel_dp->lane_count);
+ 
+-	if (crtc->config.has_audio) {
+-		DRM_DEBUG_DRIVER("Enabling DP audio on pipe %c\n",
+-				 pipe_name(crtc->pipe));
++	if (crtc->config.has_audio)
+ 		intel_dp->DP |= DP_AUDIO_OUTPUT_ENABLE;
+-		intel_write_eld(&encoder->base, adjusted_mode);
+-	}
+ 
+ 	/* Split out the IBX/CPU vs CPT settings */
+ 
+@@ -1110,6 +1385,8 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	u32 pp_stat_reg, pp_ctrl_reg;
+ 
++	lockdep_assert_held(&dev_priv->pps_mutex);
++
+ 	pp_stat_reg = _pp_stat_reg(intel_dp);
+ 	pp_ctrl_reg = _pp_ctrl_reg(intel_dp);
+ 
+@@ -1173,13 +1450,20 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	u32 control;
+ 
++	lockdep_assert_held(&dev_priv->pps_mutex);
++
+ 	control = I915_READ(_pp_ctrl_reg(intel_dp));
+ 	control &= ~PANEL_UNLOCK_MASK;
+ 	control |= PANEL_UNLOCK_REGS;
+ 	return control;
+ }
+ 
+-static bool _edp_panel_vdd_on(struct intel_dp *intel_dp)
++/*
++ * Must be paired with edp_panel_vdd_off().
++ * Must hold pps_mutex around the whole on/off sequence.
++ * Can be nested with intel_edp_panel_vdd_{on,off}() calls.
++ */
++static bool edp_panel_vdd_on(struct intel_dp *intel_dp)
+ {
+ 	struct drm_device *dev = intel_dp_to_dev(intel_dp);
+ 	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+@@ -1190,6 +1474,8 @@
+ 	u32 pp_stat_reg, pp_ctrl_reg;
+ 	bool need_to_disable = !intel_dp->want_panel_vdd;
+ 
++	lockdep_assert_held(&dev_priv->pps_mutex);
++
+ 	if (!is_edp(intel_dp))
+ 		return false;
+ 
+@@ -1201,7 +1487,8 @@
+ 	power_domain = intel_display_port_power_domain(intel_encoder);
+ 	intel_display_power_get(dev_priv, power_domain);
+ 
+-	DRM_DEBUG_KMS("Turning eDP VDD on\n");
++	DRM_DEBUG_KMS("Turning eDP port %c VDD on\n",
++		      port_name(intel_dig_port->port));
+ 
+ 	if (!edp_have_panel_power(intel_dp))
+ 		wait_panel_power_cycle(intel_dp);
+@@ -1220,69 +1507,86 @@
+ 	 * If the panel wasn't on, delay before accessing aux channel
+ 	 */
+ 	if (!edp_have_panel_power(intel_dp)) {
+-		DRM_DEBUG_KMS("eDP was not running\n");
++		DRM_DEBUG_KMS("eDP port %c panel power wasn't enabled\n",
++			      port_name(intel_dig_port->port));
+ 		msleep(intel_dp->panel_power_up_delay);
+ 	}
+ 
+ 	return need_to_disable;
+ }
+ 
++/*
++ * Must be paired with intel_edp_panel_vdd_off() or
++ * intel_edp_panel_off().
++ * Nested calls to these functions are not allowed since
++ * we drop the lock. Caller must use some higher level
++ * locking to prevent nested calls from other threads.
++ */
+ void intel_edp_panel_vdd_on(struct intel_dp *intel_dp)
+ {
+-	if (is_edp(intel_dp)) {
+-		bool vdd = _edp_panel_vdd_on(intel_dp);
++	bool vdd;
+ 
+-		WARN(!vdd, "eDP VDD already requested on\n");
+-	}
++	if (!is_edp(intel_dp))
++		return;
++
++	pps_lock(intel_dp);
++	vdd = edp_panel_vdd_on(intel_dp);
++	pps_unlock(intel_dp);
++
++	WARN(!vdd, "eDP port %c VDD already requested on\n",
++	     port_name(dp_to_dig_port(intel_dp)->port));
+ }
+ 
+ static void edp_panel_vdd_off_sync(struct intel_dp *intel_dp)
+ {
+ 	struct drm_device *dev = intel_dp_to_dev(intel_dp);
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_digital_port *intel_dig_port =
++		dp_to_dig_port(intel_dp);
++	struct intel_encoder *intel_encoder = &intel_dig_port->base;
++	enum intel_display_power_domain power_domain;
+ 	u32 pp;
+ 	u32 pp_stat_reg, pp_ctrl_reg;
+ 
+-	WARN_ON(!drm_modeset_is_locked(&dev->mode_config.connection_mutex));
++	lockdep_assert_held(&dev_priv->pps_mutex);
+ 
+-	if (!intel_dp->want_panel_vdd && edp_have_panel_vdd(intel_dp)) {
+-		struct intel_digital_port *intel_dig_port =
+-						dp_to_dig_port(intel_dp);
+-		struct intel_encoder *intel_encoder = &intel_dig_port->base;
+-		enum intel_display_power_domain power_domain;
++	WARN_ON(intel_dp->want_panel_vdd);
+ 
+-		DRM_DEBUG_KMS("Turning eDP VDD off\n");
++	if (!edp_have_panel_vdd(intel_dp))
++		return;
+ 
+-		pp = ironlake_get_pp_control(intel_dp);
+-		pp &= ~EDP_FORCE_VDD;
++	DRM_DEBUG_KMS("Turning eDP port %c VDD off\n",
++		      port_name(intel_dig_port->port));
+ 
+-		pp_ctrl_reg = _pp_ctrl_reg(intel_dp);
+-		pp_stat_reg = _pp_stat_reg(intel_dp);
++	pp = ironlake_get_pp_control(intel_dp);
++	pp &= ~EDP_FORCE_VDD;
+ 
+-		I915_WRITE(pp_ctrl_reg, pp);
+-		POSTING_READ(pp_ctrl_reg);
++	pp_ctrl_reg = _pp_ctrl_reg(intel_dp);
++	pp_stat_reg = _pp_stat_reg(intel_dp);
+ 
+-		/* Make sure sequencer is idle before allowing subsequent activity */
+-		DRM_DEBUG_KMS("PP_STATUS: 0x%08x PP_CONTROL: 0x%08x\n",
+-		I915_READ(pp_stat_reg), I915_READ(pp_ctrl_reg));
++	I915_WRITE(pp_ctrl_reg, pp);
++	POSTING_READ(pp_ctrl_reg);
+ 
+-		if ((pp & POWER_TARGET_ON) == 0)
+-			intel_dp->last_power_cycle = jiffies;
++	/* Make sure sequencer is idle before allowing subsequent activity */
++	DRM_DEBUG_KMS("PP_STATUS: 0x%08x PP_CONTROL: 0x%08x\n",
++	I915_READ(pp_stat_reg), I915_READ(pp_ctrl_reg));
+ 
+-		power_domain = intel_display_port_power_domain(intel_encoder);
+-		intel_display_power_put(dev_priv, power_domain);
+-	}
++	if ((pp & POWER_TARGET_ON) == 0)
++		intel_dp->last_power_cycle = jiffies;
++
++	power_domain = intel_display_port_power_domain(intel_encoder);
++	intel_display_power_put(dev_priv, power_domain);
+ }
+ 
+ static void edp_panel_vdd_work(struct work_struct *__work)
+ {
+ 	struct intel_dp *intel_dp = container_of(to_delayed_work(__work),
+ 						 struct intel_dp, panel_vdd_work);
+-	struct drm_device *dev = intel_dp_to_dev(intel_dp);
+ 
+-	drm_modeset_lock(&dev->mode_config.connection_mutex, NULL);
+-	edp_panel_vdd_off_sync(intel_dp);
+-	drm_modeset_unlock(&dev->mode_config.connection_mutex);
++	pps_lock(intel_dp);
++	if (!intel_dp->want_panel_vdd)
++		edp_panel_vdd_off_sync(intel_dp);
++	pps_unlock(intel_dp);
+ }
+ 
+ static void edp_panel_vdd_schedule_off(struct intel_dp *intel_dp)
+@@ -1298,12 +1602,23 @@
+ 	schedule_delayed_work(&intel_dp->panel_vdd_work, delay);
+ }
+ 
++/*
++ * Must be paired with edp_panel_vdd_on().
++ * Must hold pps_mutex around the whole on/off sequence.
++ * Can be nested with intel_edp_panel_vdd_{on,off}() calls.
++ */
+ static void edp_panel_vdd_off(struct intel_dp *intel_dp, bool sync)
+ {
++	struct drm_i915_private *dev_priv =
++		intel_dp_to_dev(intel_dp)->dev_private;
++
++	lockdep_assert_held(&dev_priv->pps_mutex);
++
+ 	if (!is_edp(intel_dp))
+ 		return;
+ 
+-	WARN(!intel_dp->want_panel_vdd, "eDP VDD not forced on");
++	WARN(!intel_dp->want_panel_vdd, "eDP port %c VDD not forced on",
++	     port_name(dp_to_dig_port(intel_dp)->port));
+ 
+ 	intel_dp->want_panel_vdd = false;
+ 
+@@ -1313,22 +1628,25 @@
+ 		edp_panel_vdd_schedule_off(intel_dp);
+ }
+ 
+-void intel_edp_panel_on(struct intel_dp *intel_dp)
++static void edp_panel_on(struct intel_dp *intel_dp)
+ {
+ 	struct drm_device *dev = intel_dp_to_dev(intel_dp);
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	u32 pp;
+ 	u32 pp_ctrl_reg;
+ 
++	lockdep_assert_held(&dev_priv->pps_mutex);
++
+ 	if (!is_edp(intel_dp))
+ 		return;
+ 
+-	DRM_DEBUG_KMS("Turn eDP power on\n");
++	DRM_DEBUG_KMS("Turn eDP port %c panel power on\n",
++		      port_name(dp_to_dig_port(intel_dp)->port));
+ 
+-	if (edp_have_panel_power(intel_dp)) {
+-		DRM_DEBUG_KMS("eDP power already on\n");
++	if (WARN(edp_have_panel_power(intel_dp),
++		 "eDP port %c panel power already on\n",
++		 port_name(dp_to_dig_port(intel_dp)->port)))
+ 		return;
+-	}
+ 
+ 	wait_panel_power_cycle(intel_dp);
+ 
+@@ -1358,7 +1676,18 @@
+ 	}
+ }
+ 
+-void intel_edp_panel_off(struct intel_dp *intel_dp)
++void intel_edp_panel_on(struct intel_dp *intel_dp)
++{
++	if (!is_edp(intel_dp))
++		return;
++
++	pps_lock(intel_dp);
++	edp_panel_on(intel_dp);
++	pps_unlock(intel_dp);
++}
++
++
++static void edp_panel_off(struct intel_dp *intel_dp)
+ {
+ 	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+ 	struct intel_encoder *intel_encoder = &intel_dig_port->base;
+@@ -1368,12 +1697,16 @@
+ 	u32 pp;
+ 	u32 pp_ctrl_reg;
+ 
++	lockdep_assert_held(&dev_priv->pps_mutex);
++
+ 	if (!is_edp(intel_dp))
+ 		return;
+ 
+-	DRM_DEBUG_KMS("Turn eDP power off\n");
++	DRM_DEBUG_KMS("Turn eDP port %c panel power off\n",
++		      port_name(dp_to_dig_port(intel_dp)->port));
+ 
+-	WARN(!intel_dp->want_panel_vdd, "Need VDD to turn off panel\n");
++	WARN(!intel_dp->want_panel_vdd, "Need eDP port %c VDD to turn off panel\n",
++	     port_name(dp_to_dig_port(intel_dp)->port));
+ 
+ 	pp = ironlake_get_pp_control(intel_dp);
+ 	/* We need to switch off panel power _and_ force vdd, for otherwise some
+@@ -1396,7 +1729,18 @@
+ 	intel_display_power_put(dev_priv, power_domain);
+ }
+ 
+-void intel_edp_backlight_on(struct intel_dp *intel_dp)
++void intel_edp_panel_off(struct intel_dp *intel_dp)
++{
++	if (!is_edp(intel_dp))
++		return;
++
++	pps_lock(intel_dp);
++	edp_panel_off(intel_dp);
++	pps_unlock(intel_dp);
++}
++
++/* Enable backlight in the panel power control. */
++static void _intel_edp_backlight_on(struct intel_dp *intel_dp)
+ {
+ 	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+ 	struct drm_device *dev = intel_dig_port->base.base.dev;
+@@ -1404,13 +1748,6 @@
+ 	u32 pp;
+ 	u32 pp_ctrl_reg;
+ 
+-	if (!is_edp(intel_dp))
+-		return;
+-
+-	DRM_DEBUG_KMS("\n");
+-
+-	intel_panel_enable_backlight(intel_dp->attached_connector);
+-
+ 	/*
+ 	 * If we enable the backlight right away following a panel power
+ 	 * on, we may see slight flicker as the panel syncs with the eDP
+@@ -1418,6 +1755,9 @@
+ 	 * allowing it to appear.
+ 	 */
+ 	wait_backlight_on(intel_dp);
++
++	pps_lock(intel_dp);
++
+ 	pp = ironlake_get_pp_control(intel_dp);
+ 	pp |= EDP_BLC_ENABLE;
+ 
+@@ -1425,19 +1765,35 @@
+ 
+ 	I915_WRITE(pp_ctrl_reg, pp);
+ 	POSTING_READ(pp_ctrl_reg);
++
++	pps_unlock(intel_dp);
+ }
+ 
+-void intel_edp_backlight_off(struct intel_dp *intel_dp)
++/* Enable backlight PWM and backlight PP control. */
++void intel_edp_backlight_on(struct intel_dp *intel_dp)
+ {
+-	struct drm_device *dev = intel_dp_to_dev(intel_dp);
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	u32 pp;
+-	u32 pp_ctrl_reg;
+-
+ 	if (!is_edp(intel_dp))
+ 		return;
+ 
+ 	DRM_DEBUG_KMS("\n");
++
++	intel_panel_enable_backlight(intel_dp->attached_connector);
++	_intel_edp_backlight_on(intel_dp);
++}
++
++/* Disable backlight in the panel power control. */
++static void _intel_edp_backlight_off(struct intel_dp *intel_dp)
++{
++	struct drm_device *dev = intel_dp_to_dev(intel_dp);
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	u32 pp;
++	u32 pp_ctrl_reg;
++
++	if (!is_edp(intel_dp))
++		return;
++
++	pps_lock(intel_dp);
++
+ 	pp = ironlake_get_pp_control(intel_dp);
+ 	pp &= ~EDP_BLC_ENABLE;
+ 
+@@ -1445,13 +1801,51 @@
+ 
+ 	I915_WRITE(pp_ctrl_reg, pp);
+ 	POSTING_READ(pp_ctrl_reg);
+-	intel_dp->last_backlight_off = jiffies;
+ 
++	pps_unlock(intel_dp);
++
++	intel_dp->last_backlight_off = jiffies;
+ 	edp_wait_backlight_off(intel_dp);
++}
+ 
++/* Disable backlight PP control and backlight PWM. */
++void intel_edp_backlight_off(struct intel_dp *intel_dp)
++{
++	if (!is_edp(intel_dp))
++		return;
++
++	DRM_DEBUG_KMS("\n");
++
++	_intel_edp_backlight_off(intel_dp);
+ 	intel_panel_disable_backlight(intel_dp->attached_connector);
+ }
+ 
++/*
++ * Hook for controlling the panel power control backlight through the bl_power
++ * sysfs attribute. Take care to handle multiple calls.
++ */
++static void intel_edp_backlight_power(struct intel_connector *connector,
++				      bool enable)
++{
++	struct intel_dp *intel_dp = intel_attached_dp(&connector->base);
++	bool is_enabled;
++
++	pps_lock(intel_dp);
++	is_enabled = ironlake_get_pp_control(intel_dp) & EDP_BLC_ENABLE;
++	pps_unlock(intel_dp);
++
++	if (is_enabled == enable)
++		return;
++
++	DRM_DEBUG_KMS("panel power control backlight %s\n",
++		      enable ? "enable" : "disable");
++
++	if (enable)
++		_intel_edp_backlight_on(intel_dp);
++	else
++		_intel_edp_backlight_off(intel_dp);
++}
++
+ static void ironlake_edp_pll_on(struct intel_dp *intel_dp)
+ {
+ 	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+@@ -1515,8 +1909,6 @@
+ 	if (mode != DRM_MODE_DPMS_ON) {
+ 		ret = drm_dp_dpcd_writeb(&intel_dp->aux, DP_SET_POWER,
+ 					 DP_SET_POWER_D3);
+-		if (ret != 1)
+-			DRM_DEBUG_DRIVER("failed to write sink power state\n");
+ 	} else {
+ 		/*
+ 		 * When turning on, we need to retry for 1ms to give the sink
+@@ -1530,6 +1922,10 @@
+ 			msleep(1);
+ 		}
+ 	}
++
++	if (ret != 1)
++		DRM_DEBUG_KMS("failed to %s sink power state\n",
++			      mode == DRM_MODE_DPMS_ON ? "enable" : "disable");
+ }
+ 
+ static bool intel_dp_get_hw_state(struct intel_encoder *encoder,
+@@ -1543,7 +1939,7 @@
+ 	u32 tmp;
+ 
+ 	power_domain = intel_display_port_power_domain(encoder);
+-	if (!intel_display_power_enabled(dev_priv, power_domain))
++	if (!intel_display_power_is_enabled(dev_priv, power_domain))
+ 		return false;
+ 
+ 	tmp = I915_READ(intel_dp->output_reg);
+@@ -1576,7 +1972,7 @@
+ 			return true;
+ 		}
+ 
+-		for_each_pipe(i) {
++		for_each_pipe(dev_priv, i) {
+ 			trans_dp = I915_READ(TRANS_DP_CTL(i));
+ 			if ((trans_dp & TRANS_DP_PORT_SEL_MASK) == trans_sel) {
+ 				*pipe = i;
+@@ -1719,10 +2115,8 @@
+ 	POSTING_READ(ctl_reg);
+ }
+ 
+-static void intel_edp_psr_setup(struct intel_dp *intel_dp)
++static void intel_edp_psr_setup_vsc(struct intel_dp *intel_dp)
+ {
+-	struct drm_device *dev = intel_dp_to_dev(intel_dp);
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct edp_vsc_psr psr_vsc;
+ 
+ 	/* Prepare VSC packet as per EDP 1.3 spec, Table 3.10 */
+@@ -1732,10 +2126,6 @@
+ 	psr_vsc.sdp_header.HB2 = 0x2;
+ 	psr_vsc.sdp_header.HB3 = 0x8;
+ 	intel_edp_psr_write_vsc(intel_dp, &psr_vsc);
+-
+-	/* Avoid continuous PSR exit by masking memup and hpd */
+-	I915_WRITE(EDP_PSR_DEBUG_CTL(dev), EDP_PSR_DEBUG_MASK_MEMUP |
+-		   EDP_PSR_DEBUG_MASK_HPD | EDP_PSR_DEBUG_MASK_LPSP);
+ }
+ 
+ static void intel_edp_psr_enable_sink(struct intel_dp *intel_dp)
+@@ -1745,8 +2135,17 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	uint32_t aux_clock_divider;
+ 	int precharge = 0x3;
+-	int msg_size = 5;       /* Header(4) + Message(1) */
+ 	bool only_standby = false;
++	static const uint8_t aux_msg[] = {
++		[0] = DP_AUX_NATIVE_WRITE << 4,
++		[1] = DP_SET_POWER >> 8,
++		[2] = DP_SET_POWER & 0xff,
++		[3] = 1 - 1,
++		[4] = DP_SET_POWER_D0,
++	};
++	int i;
++
++	BUILD_BUG_ON(sizeof(aux_msg) > 20);
+ 
+ 	aux_clock_divider = intel_dp->get_aux_clock_divider(intel_dp, 0);
+ 
+@@ -1762,11 +2161,13 @@
+ 				   DP_PSR_ENABLE | DP_PSR_MAIN_LINK_ACTIVE);
+ 
+ 	/* Setup AUX registers */
+-	I915_WRITE(EDP_PSR_AUX_DATA1(dev), EDP_PSR_DPCD_COMMAND);
+-	I915_WRITE(EDP_PSR_AUX_DATA2(dev), EDP_PSR_DPCD_NORMAL_OPERATION);
++	for (i = 0; i < sizeof(aux_msg); i += 4)
++		I915_WRITE(EDP_PSR_AUX_DATA1(dev) + i,
++			   pack_aux(&aux_msg[i], sizeof(aux_msg) - i));
++
+ 	I915_WRITE(EDP_PSR_AUX_CTL(dev),
+ 		   DP_AUX_CH_CTL_TIME_OUT_400us |
+-		   (msg_size << DP_AUX_CH_CTL_MESSAGE_SIZE_SHIFT) |
++		   (sizeof(aux_msg) << DP_AUX_CH_CTL_MESSAGE_SIZE_SHIFT) |
+ 		   (precharge << DP_AUX_CH_CTL_PRECHARGE_2US_SHIFT) |
+ 		   (aux_clock_divider << DP_AUX_CH_CTL_BIT_CLOCK_2X_SHIFT));
+ }
+@@ -1820,7 +2221,7 @@
+ 		return false;
+ 	}
+ 
+-	if (!i915.enable_psr) {
++	if (!i915_module.enable_psr) {
+ 		DRM_DEBUG_KMS("PSR disable by flag\n");
+ 		return false;
+ 	}
+@@ -1845,6 +2246,12 @@
+ 	return true;
+ }
+ 
++static void intel_edp_set_psr_property(struct intel_connector *connector, uint64_t val)
++{
++	struct drm_property *p = to_i915(connector->base.dev)->psr.property;
++	drm_object_property_set_value(&connector->base.base,p, val);
++}
++
+ static void intel_edp_psr_do_enable(struct intel_dp *intel_dp)
+ {
+ 	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+@@ -1855,12 +2262,11 @@
+ 	WARN_ON(dev_priv->psr.active);
+ 	lockdep_assert_held(&dev_priv->psr.lock);
+ 
+-	/* Enable PSR on the panel */
+-	intel_edp_psr_enable_sink(intel_dp);
+-
+-	/* Enable PSR on the host */
++	/* Enable/Re-enable PSR on the host */
+ 	intel_edp_psr_enable_source(intel_dp);
+ 
++	intel_edp_set_psr_property(intel_dp->attached_connector, 1);
++
+ 	dev_priv->psr.active = true;
+ }
+ 
+@@ -1882,17 +2288,25 @@
+ 	mutex_lock(&dev_priv->psr.lock);
+ 	if (dev_priv->psr.enabled) {
+ 		DRM_DEBUG_KMS("PSR already in use\n");
+-		mutex_unlock(&dev_priv->psr.lock);
+-		return;
++		goto unlock;
+ 	}
+ 
++	if (!intel_edp_psr_match_conditions(intel_dp))
++		goto unlock;
++
+ 	dev_priv->psr.busy_frontbuffer_bits = 0;
+ 
+-	/* Setup PSR once */
+-	intel_edp_psr_setup(intel_dp);
++	intel_edp_psr_setup_vsc(intel_dp);
+ 
+-	if (intel_edp_psr_match_conditions(intel_dp))
+-		dev_priv->psr.enabled = intel_dp;
++	/* Avoid continuous PSR exit by masking memup and hpd */
++	I915_WRITE(EDP_PSR_DEBUG_CTL(dev), EDP_PSR_DEBUG_MASK_MEMUP |
++		   EDP_PSR_DEBUG_MASK_HPD | EDP_PSR_DEBUG_MASK_LPSP);
++
++	/* Enable PSR on the panel */
++	intel_edp_psr_enable_sink(intel_dp);
++
++	dev_priv->psr.enabled = intel_dp;
++unlock:
+ 	mutex_unlock(&dev_priv->psr.lock);
+ }
+ 
+@@ -1933,6 +2347,17 @@
+ 		container_of(work, typeof(*dev_priv), psr.work.work);
+ 	struct intel_dp *intel_dp = dev_priv->psr.enabled;
+ 
++	/* We have to make sure PSR is ready for re-enable
++	 * otherwise it keeps disabled until next full enable/disable cycle.
++	 * PSR might take some time to get fully disabled
++	 * and be ready for re-enable.
++	 */
++	if (wait_for((I915_READ(EDP_PSR_STATUS_CTL(dev_priv->dev)) &
++		      EDP_PSR_STATUS_STATE_MASK) == 0, 50)) {
++		DRM_ERROR("Timed out waiting for PSR Idle for re-enable\n");
++		return;
++	}
++
+ 	mutex_lock(&dev_priv->psr.lock);
+ 	intel_dp = dev_priv->psr.enabled;
+ 
+@@ -1955,6 +2380,7 @@
+ static void intel_edp_psr_do_exit(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_encoder *encoder;
+ 
+ 	if (dev_priv->psr.active) {
+ 		u32 val = I915_READ(EDP_PSR_CTL(dev));
+@@ -1966,6 +2392,11 @@
+ 		dev_priv->psr.active = false;
+ 	}
+ 
++	list_for_each_entry(encoder, &dev->mode_config.encoder_list, base.head)
++		if (encoder->type == INTEL_OUTPUT_EDP) {
++			struct intel_dp *intel_dp = enc_to_intel_dp(&encoder->base);
++			intel_edp_set_psr_property(intel_dp->attached_connector, 0);
++		}
+ }
+ 
+ void intel_edp_psr_invalidate(struct drm_device *dev,
+@@ -2036,8 +2467,11 @@
+ static void intel_disable_dp(struct intel_encoder *encoder)
+ {
+ 	struct intel_dp *intel_dp = enc_to_intel_dp(&encoder->base);
+-	enum port port = dp_to_dig_port(intel_dp)->port;
+ 	struct drm_device *dev = encoder->base.dev;
++	struct intel_crtc *crtc = to_intel_crtc(encoder->base.crtc);
++
++	if (crtc->config.has_audio)
++		intel_audio_codec_disable(encoder);
+ 
+ 	/* Make sure the panel is off before trying to change the mode. But also
+ 	 * ensure that we have vdd while we switch off the panel. */
+@@ -2046,21 +2480,19 @@
+ 	intel_dp_sink_dpms(intel_dp, DRM_MODE_DPMS_OFF);
+ 	intel_edp_panel_off(intel_dp);
+ 
+-	/* cpu edp my only be disable _after_ the cpu pipe/plane is disabled. */
+-	if (!(port == PORT_A || IS_VALLEYVIEW(dev)))
++	/* disable the port before the pipe on g4x */
++	if (INTEL_INFO(dev)->gen < 5)
+ 		intel_dp_link_down(intel_dp);
+ }
+ 
+-static void g4x_post_disable_dp(struct intel_encoder *encoder)
++static void ilk_post_disable_dp(struct intel_encoder *encoder)
+ {
+ 	struct intel_dp *intel_dp = enc_to_intel_dp(&encoder->base);
+ 	enum port port = dp_to_dig_port(intel_dp)->port;
+ 
+-	if (port != PORT_A)
+-		return;
+-
+ 	intel_dp_link_down(intel_dp);
+-	ironlake_edp_pll_off(intel_dp);
++	if (port == PORT_A)
++		ironlake_edp_pll_off(intel_dp);
+ }
+ 
+ static void vlv_post_disable_dp(struct intel_encoder *encoder)
+@@ -2106,23 +2538,150 @@
+ 	mutex_unlock(&dev_priv->dpio_lock);
+ }
+ 
++static void
++_intel_dp_set_link_train(struct intel_dp *intel_dp,
++			 uint32_t *DP,
++			 uint8_t dp_train_pat)
++{
++	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
++	struct drm_device *dev = intel_dig_port->base.base.dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	enum port port = intel_dig_port->port;
++
++	if (HAS_DDI(dev)) {
++		uint32_t temp = I915_READ(DP_TP_CTL(port));
++
++		if (dp_train_pat & DP_LINK_SCRAMBLING_DISABLE)
++			temp |= DP_TP_CTL_SCRAMBLE_DISABLE;
++		else
++			temp &= ~DP_TP_CTL_SCRAMBLE_DISABLE;
++
++		temp &= ~DP_TP_CTL_LINK_TRAIN_MASK;
++		switch (dp_train_pat & DP_TRAINING_PATTERN_MASK) {
++		case DP_TRAINING_PATTERN_DISABLE:
++			temp |= DP_TP_CTL_LINK_TRAIN_NORMAL;
++
++			break;
++		case DP_TRAINING_PATTERN_1:
++			temp |= DP_TP_CTL_LINK_TRAIN_PAT1;
++			break;
++		case DP_TRAINING_PATTERN_2:
++			temp |= DP_TP_CTL_LINK_TRAIN_PAT2;
++			break;
++		case DP_TRAINING_PATTERN_3:
++			temp |= DP_TP_CTL_LINK_TRAIN_PAT3;
++			break;
++		}
++		I915_WRITE(DP_TP_CTL(port), temp);
++
++	} else if (HAS_PCH_CPT(dev) && (IS_GEN7(dev) || port != PORT_A)) {
++		*DP &= ~DP_LINK_TRAIN_MASK_CPT;
++
++		switch (dp_train_pat & DP_TRAINING_PATTERN_MASK) {
++		case DP_TRAINING_PATTERN_DISABLE:
++			*DP |= DP_LINK_TRAIN_OFF_CPT;
++			break;
++		case DP_TRAINING_PATTERN_1:
++			*DP |= DP_LINK_TRAIN_PAT_1_CPT;
++			break;
++		case DP_TRAINING_PATTERN_2:
++			*DP |= DP_LINK_TRAIN_PAT_2_CPT;
++			break;
++		case DP_TRAINING_PATTERN_3:
++			DRM_ERROR("DP training pattern 3 not supported\n");
++			*DP |= DP_LINK_TRAIN_PAT_2_CPT;
++			break;
++		}
++
++	} else {
++		if (IS_CHERRYVIEW(dev))
++			*DP &= ~DP_LINK_TRAIN_MASK_CHV;
++		else
++			*DP &= ~DP_LINK_TRAIN_MASK;
++
++		switch (dp_train_pat & DP_TRAINING_PATTERN_MASK) {
++		case DP_TRAINING_PATTERN_DISABLE:
++			*DP |= DP_LINK_TRAIN_OFF;
++			break;
++		case DP_TRAINING_PATTERN_1:
++			*DP |= DP_LINK_TRAIN_PAT_1;
++			break;
++		case DP_TRAINING_PATTERN_2:
++			*DP |= DP_LINK_TRAIN_PAT_2;
++			break;
++		case DP_TRAINING_PATTERN_3:
++			if (IS_CHERRYVIEW(dev)) {
++				*DP |= DP_LINK_TRAIN_PAT_3_CHV;
++			} else {
++				DRM_ERROR("DP training pattern 3 not supported\n");
++				*DP |= DP_LINK_TRAIN_PAT_2;
++			}
++			break;
++		}
++	}
++}
++
++static void intel_dp_enable_port(struct intel_dp *intel_dp)
++{
++	struct drm_device *dev = intel_dp_to_dev(intel_dp);
++	struct drm_i915_private *dev_priv = dev->dev_private;
++
++	/* enable with pattern 1 (as per spec) */
++	_intel_dp_set_link_train(intel_dp, &intel_dp->DP,
++				 DP_TRAINING_PATTERN_1);
++
++	I915_WRITE(intel_dp->output_reg, intel_dp->DP);
++	POSTING_READ(intel_dp->output_reg);
++
++	/*
++	 * Magic for VLV/CHV. We _must_ first set up the register
++	 * without actually enabling the port, and then do another
++	 * write to enable the port. Otherwise link training will
++	 * fail when the power sequencer is freshly used for this port.
++	 */
++	intel_dp->DP |= DP_PORT_EN;
++
++	I915_WRITE(intel_dp->output_reg, intel_dp->DP);
++	POSTING_READ(intel_dp->output_reg);
++}
++
+ static void intel_enable_dp(struct intel_encoder *encoder)
+ {
+ 	struct intel_dp *intel_dp = enc_to_intel_dp(&encoder->base);
+ 	struct drm_device *dev = encoder->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_crtc *crtc = to_intel_crtc(encoder->base.crtc);
+ 	uint32_t dp_reg = I915_READ(intel_dp->output_reg);
+ 
+ 	if (WARN_ON(dp_reg & DP_PORT_EN))
+ 		return;
+ 
+-	intel_edp_panel_vdd_on(intel_dp);
++	pps_lock(intel_dp);
++
++	if (IS_VALLEYVIEW(dev))
++		vlv_init_panel_power_sequencer(intel_dp);
++
++	intel_dp_enable_port(intel_dp);
++
++	edp_panel_vdd_on(intel_dp);
++	edp_panel_on(intel_dp);
++	edp_panel_vdd_off(intel_dp, true);
++
++	pps_unlock(intel_dp);
++
++	if (IS_VALLEYVIEW(dev))
++		vlv_wait_port_ready(dev_priv, dp_to_dig_port(intel_dp));
++
+ 	intel_dp_sink_dpms(intel_dp, DRM_MODE_DPMS_ON);
+ 	intel_dp_start_link_train(intel_dp);
+-	intel_edp_panel_on(intel_dp);
+-	edp_panel_vdd_off(intel_dp, true);
+ 	intel_dp_complete_link_train(intel_dp);
+ 	intel_dp_stop_link_train(intel_dp);
++
++	if (crtc->config.has_audio) {
++		DRM_DEBUG_DRIVER("Enabling DP audio on pipe %c\n",
++				 pipe_name(crtc->pipe));
++		intel_audio_codec_enable(encoder);
++	}
+ }
+ 
+ static void g4x_enable_dp(struct intel_encoder *encoder)
+@@ -2154,6 +2713,110 @@
+ 	}
+ }
+ 
++static void vlv_detach_power_sequencer(struct intel_dp *intel_dp)
++{
++	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
++	struct drm_i915_private *dev_priv = intel_dig_port->base.base.dev->dev_private;
++	enum pipe pipe = intel_dp->pps_pipe;
++	int pp_on_reg = VLV_PIPE_PP_ON_DELAYS(pipe);
++
++	edp_panel_vdd_off_sync(intel_dp);
++
++	/*
++	 * VLV seems to get confused when multiple power seqeuencers
++	 * have the same port selected (even if only one has power/vdd
++	 * enabled). The failure manifests as vlv_wait_port_ready() failing
++	 * CHV on the other hand doesn't seem to mind having the same port
++	 * selected in multiple power seqeuencers, but let's clear the
++	 * port select always when logically disconnecting a power sequencer
++	 * from a port.
++	 */
++	DRM_DEBUG_KMS("detaching pipe %c power sequencer from port %c\n",
++		      pipe_name(pipe), port_name(intel_dig_port->port));
++	I915_WRITE(pp_on_reg, 0);
++	POSTING_READ(pp_on_reg);
++
++	intel_dp->pps_pipe = INVALID_PIPE;
++}
++
++static void vlv_steal_power_sequencer(struct drm_device *dev,
++				      enum pipe pipe)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_encoder *encoder;
++
++	lockdep_assert_held(&dev_priv->pps_mutex);
++
++	if (WARN_ON(pipe != PIPE_A && pipe != PIPE_B))
++		return;
++
++	list_for_each_entry(encoder, &dev->mode_config.encoder_list,
++			    base.head) {
++		struct intel_dp *intel_dp;
++		enum port port;
++
++		if (encoder->type != INTEL_OUTPUT_EDP)
++			continue;
++
++		intel_dp = enc_to_intel_dp(&encoder->base);
++		port = dp_to_dig_port(intel_dp)->port;
++
++		if (intel_dp->pps_pipe != pipe)
++			continue;
++
++		DRM_DEBUG_KMS("stealing pipe %c power sequencer from port %c\n",
++			      pipe_name(pipe), port_name(port));
++
++		WARN(encoder->connectors_active,
++		     "stealing pipe %c power sequencer from active eDP port %c\n",
++		     pipe_name(pipe), port_name(port));
++
++		/* make sure vdd is off before we steal it */
++		vlv_detach_power_sequencer(intel_dp);
++	}
++}
++
++static void vlv_init_panel_power_sequencer(struct intel_dp *intel_dp)
++{
++	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
++	struct intel_encoder *encoder = &intel_dig_port->base;
++	struct drm_device *dev = encoder->base.dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_crtc *crtc = to_intel_crtc(encoder->base.crtc);
++
++	lockdep_assert_held(&dev_priv->pps_mutex);
++
++	if (!is_edp(intel_dp))
++		return;
++
++	if (intel_dp->pps_pipe == crtc->pipe)
++		return;
++
++	/*
++	 * If another power sequencer was being used on this
++	 * port previously make sure to turn off vdd there while
++	 * we still have control of it.
++	 */
++	if (intel_dp->pps_pipe != INVALID_PIPE)
++		vlv_detach_power_sequencer(intel_dp);
++
++	/*
++	 * We may be stealing the power
++	 * sequencer from another port.
++	 */
++	vlv_steal_power_sequencer(dev, crtc->pipe);
++
++	/* now it's all ours */
++	intel_dp->pps_pipe = crtc->pipe;
++
++	DRM_DEBUG_KMS("initializing pipe %c power sequencer for port %c\n",
++		      pipe_name(intel_dp->pps_pipe), port_name(intel_dig_port->port));
++
++	/* init power sequencer on this pipe and port */
++	intel_dp_init_panel_power_sequencer(dev, intel_dp);
++	intel_dp_init_panel_power_sequencer_registers(dev, intel_dp);
++}
++
+ static void vlv_pre_enable_dp(struct intel_encoder *encoder)
+ {
+ 	struct intel_dp *intel_dp = enc_to_intel_dp(&encoder->base);
+@@ -2163,7 +2826,6 @@
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(encoder->base.crtc);
+ 	enum dpio_channel port = vlv_dport_to_channel(dport);
+ 	int pipe = intel_crtc->pipe;
+-	struct edp_power_seq power_seq;
+ 	u32 val;
+ 
+ 	mutex_lock(&dev_priv->dpio_lock);
+@@ -2181,16 +2843,7 @@
+ 
+ 	mutex_unlock(&dev_priv->dpio_lock);
+ 
+-	if (is_edp(intel_dp)) {
+-		/* init power sequencer on this pipe and port */
+-		intel_dp_init_panel_power_sequencer(dev, intel_dp, &power_seq);
+-		intel_dp_init_panel_power_sequencer_registers(dev, intel_dp,
+-							      &power_seq);
+-	}
+-
+ 	intel_enable_dp(encoder);
+-
+-	vlv_wait_port_ready(dev_priv, dport);
+ }
+ 
+ static void vlv_dp_pre_pll_enable(struct intel_encoder *encoder)
+@@ -2229,7 +2882,6 @@
+ 	struct intel_digital_port *dport = dp_to_dig_port(intel_dp);
+ 	struct drm_device *dev = encoder->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct edp_power_seq power_seq;
+ 	struct intel_crtc *intel_crtc =
+ 		to_intel_crtc(encoder->base.crtc);
+ 	enum dpio_channel ch = vlv_dport_to_channel(dport);
+@@ -2239,6 +2891,15 @@
+ 
+ 	mutex_lock(&dev_priv->dpio_lock);
+ 
++	/* allow hardware to manage TX FIFO reset source */
++	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS01_DW11(ch));
++	val &= ~DPIO_LANEDESKEW_STRAP_OVRD;
++	vlv_dpio_write(dev_priv, pipe, VLV_PCS01_DW11(ch), val);
++
++	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS23_DW11(ch));
++	val &= ~DPIO_LANEDESKEW_STRAP_OVRD;
++	vlv_dpio_write(dev_priv, pipe, VLV_PCS23_DW11(ch), val);
++
+ 	/* Deassert soft data lane reset*/
+ 	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS01_DW1(ch));
+ 	val |= CHV_PCS_REQ_SOFTRESET_EN;
+@@ -2274,16 +2935,7 @@
+ 
+ 	mutex_unlock(&dev_priv->dpio_lock);
+ 
+-	if (is_edp(intel_dp)) {
+-		/* init power sequencer on this pipe and port */
+-		intel_dp_init_panel_power_sequencer(dev, intel_dp, &power_seq);
+-		intel_dp_init_panel_power_sequencer_registers(dev, intel_dp,
+-							      &power_seq);
+-	}
+-
+ 	intel_enable_dp(encoder);
+-
+-	vlv_wait_port_ready(dev_priv, dport);
+ }
+ 
+ static void chv_dp_pre_pll_enable(struct intel_encoder *encoder)
+@@ -2297,6 +2949,8 @@
+ 	enum pipe pipe = intel_crtc->pipe;
+ 	u32 val;
+ 
++	intel_dp_prepare(encoder);
++
+ 	mutex_lock(&dev_priv->dpio_lock);
+ 
+ 	/* program left/right clock distribution */
+@@ -2364,6 +3018,13 @@
+ 	ssize_t ret;
+ 	int i;
+ 
++	/*
++	 * Sometime we just get the same incorrect byte repeated
++	 * over the entire buffer. Doing just one throw away read
++	 * initially seems to "solve" it.
++	 */
++	drm_dp_dpcd_read(aux, DP_DPCD_REV, buffer, 1);
++
+ 	for (i = 0; i < 3; i++) {
+ 		ret = drm_dp_dpcd_read(aux, offset, buffer, size);
+ 		if (ret == size)
+@@ -2394,14 +3055,16 @@
+ 	struct drm_device *dev = intel_dp_to_dev(intel_dp);
+ 	enum port port = dp_to_dig_port(intel_dp)->port;
+ 
+-	if (IS_VALLEYVIEW(dev))
+-		return DP_TRAIN_VOLTAGE_SWING_1200;
++	if (INTEL_INFO(dev)->gen >= 9)
++		return DP_TRAIN_VOLTAGE_SWING_LEVEL_2;
++	else if (IS_VALLEYVIEW(dev))
++		return DP_TRAIN_VOLTAGE_SWING_LEVEL_3;
+ 	else if (IS_GEN7(dev) && port == PORT_A)
+-		return DP_TRAIN_VOLTAGE_SWING_800;
++		return DP_TRAIN_VOLTAGE_SWING_LEVEL_2;
+ 	else if (HAS_PCH_CPT(dev) && port != PORT_A)
+-		return DP_TRAIN_VOLTAGE_SWING_1200;
++		return DP_TRAIN_VOLTAGE_SWING_LEVEL_3;
+ 	else
+-		return DP_TRAIN_VOLTAGE_SWING_800;
++		return DP_TRAIN_VOLTAGE_SWING_LEVEL_2;
+ }
+ 
+ static uint8_t
+@@ -2410,51 +3073,62 @@
+ 	struct drm_device *dev = intel_dp_to_dev(intel_dp);
+ 	enum port port = dp_to_dig_port(intel_dp)->port;
+ 
+-	if (IS_HASWELL(dev) || IS_BROADWELL(dev)) {
++	if (INTEL_INFO(dev)->gen >= 9) {
++		switch (voltage_swing & DP_TRAIN_VOLTAGE_SWING_MASK) {
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
++			return DP_TRAIN_PRE_EMPH_LEVEL_3;
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_1:
++			return DP_TRAIN_PRE_EMPH_LEVEL_2;
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_2:
++			return DP_TRAIN_PRE_EMPH_LEVEL_1;
++		default:
++			return DP_TRAIN_PRE_EMPH_LEVEL_0;
++		}
++	} else if (IS_HASWELL(dev) || IS_BROADWELL(dev)) {
+ 		switch (voltage_swing & DP_TRAIN_VOLTAGE_SWING_MASK) {
+-		case DP_TRAIN_VOLTAGE_SWING_400:
+-			return DP_TRAIN_PRE_EMPHASIS_9_5;
+-		case DP_TRAIN_VOLTAGE_SWING_600:
+-			return DP_TRAIN_PRE_EMPHASIS_6;
+-		case DP_TRAIN_VOLTAGE_SWING_800:
+-			return DP_TRAIN_PRE_EMPHASIS_3_5;
+-		case DP_TRAIN_VOLTAGE_SWING_1200:
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
++			return DP_TRAIN_PRE_EMPH_LEVEL_3;
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_1:
++			return DP_TRAIN_PRE_EMPH_LEVEL_2;
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_2:
++			return DP_TRAIN_PRE_EMPH_LEVEL_1;
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_3:
+ 		default:
+-			return DP_TRAIN_PRE_EMPHASIS_0;
++			return DP_TRAIN_PRE_EMPH_LEVEL_0;
+ 		}
+ 	} else if (IS_VALLEYVIEW(dev)) {
+ 		switch (voltage_swing & DP_TRAIN_VOLTAGE_SWING_MASK) {
+-		case DP_TRAIN_VOLTAGE_SWING_400:
+-			return DP_TRAIN_PRE_EMPHASIS_9_5;
+-		case DP_TRAIN_VOLTAGE_SWING_600:
+-			return DP_TRAIN_PRE_EMPHASIS_6;
+-		case DP_TRAIN_VOLTAGE_SWING_800:
+-			return DP_TRAIN_PRE_EMPHASIS_3_5;
+-		case DP_TRAIN_VOLTAGE_SWING_1200:
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
++			return DP_TRAIN_PRE_EMPH_LEVEL_3;
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_1:
++			return DP_TRAIN_PRE_EMPH_LEVEL_2;
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_2:
++			return DP_TRAIN_PRE_EMPH_LEVEL_1;
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_3:
+ 		default:
+-			return DP_TRAIN_PRE_EMPHASIS_0;
++			return DP_TRAIN_PRE_EMPH_LEVEL_0;
+ 		}
+ 	} else if (IS_GEN7(dev) && port == PORT_A) {
+ 		switch (voltage_swing & DP_TRAIN_VOLTAGE_SWING_MASK) {
+-		case DP_TRAIN_VOLTAGE_SWING_400:
+-			return DP_TRAIN_PRE_EMPHASIS_6;
+-		case DP_TRAIN_VOLTAGE_SWING_600:
+-		case DP_TRAIN_VOLTAGE_SWING_800:
+-			return DP_TRAIN_PRE_EMPHASIS_3_5;
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
++			return DP_TRAIN_PRE_EMPH_LEVEL_2;
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_1:
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_2:
++			return DP_TRAIN_PRE_EMPH_LEVEL_1;
+ 		default:
+-			return DP_TRAIN_PRE_EMPHASIS_0;
++			return DP_TRAIN_PRE_EMPH_LEVEL_0;
+ 		}
+ 	} else {
+ 		switch (voltage_swing & DP_TRAIN_VOLTAGE_SWING_MASK) {
+-		case DP_TRAIN_VOLTAGE_SWING_400:
+-			return DP_TRAIN_PRE_EMPHASIS_6;
+-		case DP_TRAIN_VOLTAGE_SWING_600:
+-			return DP_TRAIN_PRE_EMPHASIS_6;
+-		case DP_TRAIN_VOLTAGE_SWING_800:
+-			return DP_TRAIN_PRE_EMPHASIS_3_5;
+-		case DP_TRAIN_VOLTAGE_SWING_1200:
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
++			return DP_TRAIN_PRE_EMPH_LEVEL_2;
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_1:
++			return DP_TRAIN_PRE_EMPH_LEVEL_2;
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_2:
++			return DP_TRAIN_PRE_EMPH_LEVEL_1;
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_3:
+ 		default:
+-			return DP_TRAIN_PRE_EMPHASIS_0;
++			return DP_TRAIN_PRE_EMPH_LEVEL_0;
+ 		}
+ 	}
+ }
+@@ -2473,22 +3147,22 @@
+ 	int pipe = intel_crtc->pipe;
+ 
+ 	switch (train_set & DP_TRAIN_PRE_EMPHASIS_MASK) {
+-	case DP_TRAIN_PRE_EMPHASIS_0:
++	case DP_TRAIN_PRE_EMPH_LEVEL_0:
+ 		preemph_reg_value = 0x0004000;
+ 		switch (train_set & DP_TRAIN_VOLTAGE_SWING_MASK) {
+-		case DP_TRAIN_VOLTAGE_SWING_400:
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
+ 			demph_reg_value = 0x2B405555;
+ 			uniqtranscale_reg_value = 0x552AB83A;
+ 			break;
+-		case DP_TRAIN_VOLTAGE_SWING_600:
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_1:
+ 			demph_reg_value = 0x2B404040;
+ 			uniqtranscale_reg_value = 0x5548B83A;
+ 			break;
+-		case DP_TRAIN_VOLTAGE_SWING_800:
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_2:
+ 			demph_reg_value = 0x2B245555;
+ 			uniqtranscale_reg_value = 0x5560B83A;
+ 			break;
+-		case DP_TRAIN_VOLTAGE_SWING_1200:
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_3:
+ 			demph_reg_value = 0x2B405555;
+ 			uniqtranscale_reg_value = 0x5598DA3A;
+ 			break;
+@@ -2496,18 +3170,18 @@
+ 			return 0;
+ 		}
+ 		break;
+-	case DP_TRAIN_PRE_EMPHASIS_3_5:
++	case DP_TRAIN_PRE_EMPH_LEVEL_1:
+ 		preemph_reg_value = 0x0002000;
+ 		switch (train_set & DP_TRAIN_VOLTAGE_SWING_MASK) {
+-		case DP_TRAIN_VOLTAGE_SWING_400:
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
+ 			demph_reg_value = 0x2B404040;
+ 			uniqtranscale_reg_value = 0x5552B83A;
+ 			break;
+-		case DP_TRAIN_VOLTAGE_SWING_600:
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_1:
+ 			demph_reg_value = 0x2B404848;
+ 			uniqtranscale_reg_value = 0x5580B83A;
+ 			break;
+-		case DP_TRAIN_VOLTAGE_SWING_800:
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_2:
+ 			demph_reg_value = 0x2B404040;
+ 			uniqtranscale_reg_value = 0x55ADDA3A;
+ 			break;
+@@ -2515,14 +3189,14 @@
+ 			return 0;
+ 		}
+ 		break;
+-	case DP_TRAIN_PRE_EMPHASIS_6:
++	case DP_TRAIN_PRE_EMPH_LEVEL_2:
+ 		preemph_reg_value = 0x0000000;
+ 		switch (train_set & DP_TRAIN_VOLTAGE_SWING_MASK) {
+-		case DP_TRAIN_VOLTAGE_SWING_400:
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
+ 			demph_reg_value = 0x2B305555;
+ 			uniqtranscale_reg_value = 0x5570B83A;
+ 			break;
+-		case DP_TRAIN_VOLTAGE_SWING_600:
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_1:
+ 			demph_reg_value = 0x2B2B4040;
+ 			uniqtranscale_reg_value = 0x55ADDA3A;
+ 			break;
+@@ -2530,10 +3204,10 @@
+ 			return 0;
+ 		}
+ 		break;
+-	case DP_TRAIN_PRE_EMPHASIS_9_5:
++	case DP_TRAIN_PRE_EMPH_LEVEL_3:
+ 		preemph_reg_value = 0x0006000;
+ 		switch (train_set & DP_TRAIN_VOLTAGE_SWING_MASK) {
+-		case DP_TRAIN_VOLTAGE_SWING_400:
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
+ 			demph_reg_value = 0x1B405555;
+ 			uniqtranscale_reg_value = 0x55ADDA3A;
+ 			break;
+@@ -2572,21 +3246,21 @@
+ 	int i;
+ 
+ 	switch (train_set & DP_TRAIN_PRE_EMPHASIS_MASK) {
+-	case DP_TRAIN_PRE_EMPHASIS_0:
++	case DP_TRAIN_PRE_EMPH_LEVEL_0:
+ 		switch (train_set & DP_TRAIN_VOLTAGE_SWING_MASK) {
+-		case DP_TRAIN_VOLTAGE_SWING_400:
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
+ 			deemph_reg_value = 128;
+ 			margin_reg_value = 52;
+ 			break;
+-		case DP_TRAIN_VOLTAGE_SWING_600:
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_1:
+ 			deemph_reg_value = 128;
+ 			margin_reg_value = 77;
+ 			break;
+-		case DP_TRAIN_VOLTAGE_SWING_800:
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_2:
+ 			deemph_reg_value = 128;
+ 			margin_reg_value = 102;
+ 			break;
+-		case DP_TRAIN_VOLTAGE_SWING_1200:
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_3:
+ 			deemph_reg_value = 128;
+ 			margin_reg_value = 154;
+ 			/* FIXME extra to set for 1200 */
+@@ -2595,17 +3269,17 @@
+ 			return 0;
+ 		}
+ 		break;
+-	case DP_TRAIN_PRE_EMPHASIS_3_5:
++	case DP_TRAIN_PRE_EMPH_LEVEL_1:
+ 		switch (train_set & DP_TRAIN_VOLTAGE_SWING_MASK) {
+-		case DP_TRAIN_VOLTAGE_SWING_400:
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
+ 			deemph_reg_value = 85;
+ 			margin_reg_value = 78;
+ 			break;
+-		case DP_TRAIN_VOLTAGE_SWING_600:
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_1:
+ 			deemph_reg_value = 85;
+ 			margin_reg_value = 116;
+ 			break;
+-		case DP_TRAIN_VOLTAGE_SWING_800:
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_2:
+ 			deemph_reg_value = 85;
+ 			margin_reg_value = 154;
+ 			break;
+@@ -2613,13 +3287,13 @@
+ 			return 0;
+ 		}
+ 		break;
+-	case DP_TRAIN_PRE_EMPHASIS_6:
++	case DP_TRAIN_PRE_EMPH_LEVEL_2:
+ 		switch (train_set & DP_TRAIN_VOLTAGE_SWING_MASK) {
+-		case DP_TRAIN_VOLTAGE_SWING_400:
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
+ 			deemph_reg_value = 64;
+ 			margin_reg_value = 104;
+ 			break;
+-		case DP_TRAIN_VOLTAGE_SWING_600:
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_1:
+ 			deemph_reg_value = 64;
+ 			margin_reg_value = 154;
+ 			break;
+@@ -2627,9 +3301,9 @@
+ 			return 0;
+ 		}
+ 		break;
+-	case DP_TRAIN_PRE_EMPHASIS_9_5:
++	case DP_TRAIN_PRE_EMPH_LEVEL_3:
+ 		switch (train_set & DP_TRAIN_VOLTAGE_SWING_MASK) {
+-		case DP_TRAIN_VOLTAGE_SWING_400:
++		case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
+ 			deemph_reg_value = 43;
+ 			margin_reg_value = 154;
+ 			break;
+@@ -2646,12 +3320,26 @@
+ 	/* Clear calc init */
+ 	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS01_DW10(ch));
+ 	val &= ~(DPIO_PCS_SWING_CALC_TX0_TX2 | DPIO_PCS_SWING_CALC_TX1_TX3);
++	val &= ~(DPIO_PCS_TX1DEEMP_MASK | DPIO_PCS_TX2DEEMP_MASK);
++	val |= DPIO_PCS_TX1DEEMP_9P5 | DPIO_PCS_TX2DEEMP_9P5;
+ 	vlv_dpio_write(dev_priv, pipe, VLV_PCS01_DW10(ch), val);
+ 
+ 	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS23_DW10(ch));
+ 	val &= ~(DPIO_PCS_SWING_CALC_TX0_TX2 | DPIO_PCS_SWING_CALC_TX1_TX3);
++	val &= ~(DPIO_PCS_TX1DEEMP_MASK | DPIO_PCS_TX2DEEMP_MASK);
++	val |= DPIO_PCS_TX1DEEMP_9P5 | DPIO_PCS_TX2DEEMP_9P5;
+ 	vlv_dpio_write(dev_priv, pipe, VLV_PCS23_DW10(ch), val);
+ 
++	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS01_DW9(ch));
++	val &= ~(DPIO_PCS_TX1MARGIN_MASK | DPIO_PCS_TX2MARGIN_MASK);
++	val |= DPIO_PCS_TX1MARGIN_000 | DPIO_PCS_TX2MARGIN_000;
++	vlv_dpio_write(dev_priv, pipe, VLV_PCS01_DW9(ch), val);
++
++	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS23_DW9(ch));
++	val &= ~(DPIO_PCS_TX1MARGIN_MASK | DPIO_PCS_TX2MARGIN_MASK);
++	val |= DPIO_PCS_TX1MARGIN_000 | DPIO_PCS_TX2MARGIN_000;
++	vlv_dpio_write(dev_priv, pipe, VLV_PCS23_DW9(ch), val);
++
+ 	/* Program swing deemph */
+ 	for (i = 0; i < 4; i++) {
+ 		val = vlv_dpio_read(dev_priv, pipe, CHV_TX_DW4(ch, i));
+@@ -2663,8 +3351,8 @@
+ 	/* Program swing margin */
+ 	for (i = 0; i < 4; i++) {
+ 		val = vlv_dpio_read(dev_priv, pipe, CHV_TX_DW2(ch, i));
+-		val &= ~DPIO_SWING_MARGIN_MASK;
+-		val |= margin_reg_value << DPIO_SWING_MARGIN_SHIFT;
++		val &= ~DPIO_SWING_MARGIN000_MASK;
++		val |= margin_reg_value << DPIO_SWING_MARGIN000_SHIFT;
+ 		vlv_dpio_write(dev_priv, pipe, CHV_TX_DW2(ch, i), val);
+ 	}
+ 
+@@ -2676,9 +3364,9 @@
+ 	}
+ 
+ 	if (((train_set & DP_TRAIN_PRE_EMPHASIS_MASK)
+-			== DP_TRAIN_PRE_EMPHASIS_0) &&
++			== DP_TRAIN_PRE_EMPH_LEVEL_0) &&
+ 		((train_set & DP_TRAIN_VOLTAGE_SWING_MASK)
+-			== DP_TRAIN_VOLTAGE_SWING_1200)) {
++			== DP_TRAIN_VOLTAGE_SWING_LEVEL_3)) {
+ 
+ 		/*
+ 		 * The document said it needs to set bit 27 for ch0 and bit 26
+@@ -2757,32 +3445,32 @@
+ 	uint32_t	signal_levels = 0;
+ 
+ 	switch (train_set & DP_TRAIN_VOLTAGE_SWING_MASK) {
+-	case DP_TRAIN_VOLTAGE_SWING_400:
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_0:
+ 	default:
+ 		signal_levels |= DP_VOLTAGE_0_4;
+ 		break;
+-	case DP_TRAIN_VOLTAGE_SWING_600:
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_1:
+ 		signal_levels |= DP_VOLTAGE_0_6;
+ 		break;
+-	case DP_TRAIN_VOLTAGE_SWING_800:
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_2:
+ 		signal_levels |= DP_VOLTAGE_0_8;
+ 		break;
+-	case DP_TRAIN_VOLTAGE_SWING_1200:
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_3:
+ 		signal_levels |= DP_VOLTAGE_1_2;
+ 		break;
+ 	}
+ 	switch (train_set & DP_TRAIN_PRE_EMPHASIS_MASK) {
+-	case DP_TRAIN_PRE_EMPHASIS_0:
++	case DP_TRAIN_PRE_EMPH_LEVEL_0:
+ 	default:
+ 		signal_levels |= DP_PRE_EMPHASIS_0;
+ 		break;
+-	case DP_TRAIN_PRE_EMPHASIS_3_5:
++	case DP_TRAIN_PRE_EMPH_LEVEL_1:
+ 		signal_levels |= DP_PRE_EMPHASIS_3_5;
+ 		break;
+-	case DP_TRAIN_PRE_EMPHASIS_6:
++	case DP_TRAIN_PRE_EMPH_LEVEL_2:
+ 		signal_levels |= DP_PRE_EMPHASIS_6;
+ 		break;
+-	case DP_TRAIN_PRE_EMPHASIS_9_5:
++	case DP_TRAIN_PRE_EMPH_LEVEL_3:
+ 		signal_levels |= DP_PRE_EMPHASIS_9_5;
+ 		break;
+ 	}
+@@ -2796,19 +3484,19 @@
+ 	int signal_levels = train_set & (DP_TRAIN_VOLTAGE_SWING_MASK |
+ 					 DP_TRAIN_PRE_EMPHASIS_MASK);
+ 	switch (signal_levels) {
+-	case DP_TRAIN_VOLTAGE_SWING_400 | DP_TRAIN_PRE_EMPHASIS_0:
+-	case DP_TRAIN_VOLTAGE_SWING_600 | DP_TRAIN_PRE_EMPHASIS_0:
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_0 | DP_TRAIN_PRE_EMPH_LEVEL_0:
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_1 | DP_TRAIN_PRE_EMPH_LEVEL_0:
+ 		return EDP_LINK_TRAIN_400_600MV_0DB_SNB_B;
+-	case DP_TRAIN_VOLTAGE_SWING_400 | DP_TRAIN_PRE_EMPHASIS_3_5:
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_0 | DP_TRAIN_PRE_EMPH_LEVEL_1:
+ 		return EDP_LINK_TRAIN_400MV_3_5DB_SNB_B;
+-	case DP_TRAIN_VOLTAGE_SWING_400 | DP_TRAIN_PRE_EMPHASIS_6:
+-	case DP_TRAIN_VOLTAGE_SWING_600 | DP_TRAIN_PRE_EMPHASIS_6:
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_0 | DP_TRAIN_PRE_EMPH_LEVEL_2:
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_1 | DP_TRAIN_PRE_EMPH_LEVEL_2:
+ 		return EDP_LINK_TRAIN_400_600MV_6DB_SNB_B;
+-	case DP_TRAIN_VOLTAGE_SWING_600 | DP_TRAIN_PRE_EMPHASIS_3_5:
+-	case DP_TRAIN_VOLTAGE_SWING_800 | DP_TRAIN_PRE_EMPHASIS_3_5:
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_1 | DP_TRAIN_PRE_EMPH_LEVEL_1:
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_2 | DP_TRAIN_PRE_EMPH_LEVEL_1:
+ 		return EDP_LINK_TRAIN_600_800MV_3_5DB_SNB_B;
+-	case DP_TRAIN_VOLTAGE_SWING_800 | DP_TRAIN_PRE_EMPHASIS_0:
+-	case DP_TRAIN_VOLTAGE_SWING_1200 | DP_TRAIN_PRE_EMPHASIS_0:
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_2 | DP_TRAIN_PRE_EMPH_LEVEL_0:
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_3 | DP_TRAIN_PRE_EMPH_LEVEL_0:
+ 		return EDP_LINK_TRAIN_800_1200MV_0DB_SNB_B;
+ 	default:
+ 		DRM_DEBUG_KMS("Unsupported voltage swing/pre-emphasis level:"
+@@ -2824,21 +3512,21 @@
+ 	int signal_levels = train_set & (DP_TRAIN_VOLTAGE_SWING_MASK |
+ 					 DP_TRAIN_PRE_EMPHASIS_MASK);
+ 	switch (signal_levels) {
+-	case DP_TRAIN_VOLTAGE_SWING_400 | DP_TRAIN_PRE_EMPHASIS_0:
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_0 | DP_TRAIN_PRE_EMPH_LEVEL_0:
+ 		return EDP_LINK_TRAIN_400MV_0DB_IVB;
+-	case DP_TRAIN_VOLTAGE_SWING_400 | DP_TRAIN_PRE_EMPHASIS_3_5:
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_0 | DP_TRAIN_PRE_EMPH_LEVEL_1:
+ 		return EDP_LINK_TRAIN_400MV_3_5DB_IVB;
+-	case DP_TRAIN_VOLTAGE_SWING_400 | DP_TRAIN_PRE_EMPHASIS_6:
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_0 | DP_TRAIN_PRE_EMPH_LEVEL_2:
+ 		return EDP_LINK_TRAIN_400MV_6DB_IVB;
+ 
+-	case DP_TRAIN_VOLTAGE_SWING_600 | DP_TRAIN_PRE_EMPHASIS_0:
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_1 | DP_TRAIN_PRE_EMPH_LEVEL_0:
+ 		return EDP_LINK_TRAIN_600MV_0DB_IVB;
+-	case DP_TRAIN_VOLTAGE_SWING_600 | DP_TRAIN_PRE_EMPHASIS_3_5:
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_1 | DP_TRAIN_PRE_EMPH_LEVEL_1:
+ 		return EDP_LINK_TRAIN_600MV_3_5DB_IVB;
+ 
+-	case DP_TRAIN_VOLTAGE_SWING_800 | DP_TRAIN_PRE_EMPHASIS_0:
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_2 | DP_TRAIN_PRE_EMPH_LEVEL_0:
+ 		return EDP_LINK_TRAIN_800MV_0DB_IVB;
+-	case DP_TRAIN_VOLTAGE_SWING_800 | DP_TRAIN_PRE_EMPHASIS_3_5:
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_2 | DP_TRAIN_PRE_EMPH_LEVEL_1:
+ 		return EDP_LINK_TRAIN_800MV_3_5DB_IVB;
+ 
+ 	default:
+@@ -2855,30 +3543,30 @@
+ 	int signal_levels = train_set & (DP_TRAIN_VOLTAGE_SWING_MASK |
+ 					 DP_TRAIN_PRE_EMPHASIS_MASK);
+ 	switch (signal_levels) {
+-	case DP_TRAIN_VOLTAGE_SWING_400 | DP_TRAIN_PRE_EMPHASIS_0:
+-		return DDI_BUF_EMP_400MV_0DB_HSW;
+-	case DP_TRAIN_VOLTAGE_SWING_400 | DP_TRAIN_PRE_EMPHASIS_3_5:
+-		return DDI_BUF_EMP_400MV_3_5DB_HSW;
+-	case DP_TRAIN_VOLTAGE_SWING_400 | DP_TRAIN_PRE_EMPHASIS_6:
+-		return DDI_BUF_EMP_400MV_6DB_HSW;
+-	case DP_TRAIN_VOLTAGE_SWING_400 | DP_TRAIN_PRE_EMPHASIS_9_5:
+-		return DDI_BUF_EMP_400MV_9_5DB_HSW;
+-
+-	case DP_TRAIN_VOLTAGE_SWING_600 | DP_TRAIN_PRE_EMPHASIS_0:
+-		return DDI_BUF_EMP_600MV_0DB_HSW;
+-	case DP_TRAIN_VOLTAGE_SWING_600 | DP_TRAIN_PRE_EMPHASIS_3_5:
+-		return DDI_BUF_EMP_600MV_3_5DB_HSW;
+-	case DP_TRAIN_VOLTAGE_SWING_600 | DP_TRAIN_PRE_EMPHASIS_6:
+-		return DDI_BUF_EMP_600MV_6DB_HSW;
+-
+-	case DP_TRAIN_VOLTAGE_SWING_800 | DP_TRAIN_PRE_EMPHASIS_0:
+-		return DDI_BUF_EMP_800MV_0DB_HSW;
+-	case DP_TRAIN_VOLTAGE_SWING_800 | DP_TRAIN_PRE_EMPHASIS_3_5:
+-		return DDI_BUF_EMP_800MV_3_5DB_HSW;
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_0 | DP_TRAIN_PRE_EMPH_LEVEL_0:
++		return DDI_BUF_TRANS_SELECT(0);
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_0 | DP_TRAIN_PRE_EMPH_LEVEL_1:
++		return DDI_BUF_TRANS_SELECT(1);
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_0 | DP_TRAIN_PRE_EMPH_LEVEL_2:
++		return DDI_BUF_TRANS_SELECT(2);
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_0 | DP_TRAIN_PRE_EMPH_LEVEL_3:
++		return DDI_BUF_TRANS_SELECT(3);
++
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_1 | DP_TRAIN_PRE_EMPH_LEVEL_0:
++		return DDI_BUF_TRANS_SELECT(4);
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_1 | DP_TRAIN_PRE_EMPH_LEVEL_1:
++		return DDI_BUF_TRANS_SELECT(5);
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_1 | DP_TRAIN_PRE_EMPH_LEVEL_2:
++		return DDI_BUF_TRANS_SELECT(6);
++
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_2 | DP_TRAIN_PRE_EMPH_LEVEL_0:
++		return DDI_BUF_TRANS_SELECT(7);
++	case DP_TRAIN_VOLTAGE_SWING_LEVEL_2 | DP_TRAIN_PRE_EMPH_LEVEL_1:
++		return DDI_BUF_TRANS_SELECT(8);
+ 	default:
+ 		DRM_DEBUG_KMS("Unsupported voltage swing/pre-emphasis level:"
+ 			      "0x%x\n", signal_levels);
+-		return DDI_BUF_EMP_400MV_0DB_HSW;
++		return DDI_BUF_TRANS_SELECT(0);
+ 	}
+ }
+ 
+@@ -2892,7 +3580,7 @@
+ 	uint32_t signal_levels, mask;
+ 	uint8_t train_set = intel_dp->train_set[0];
+ 
+-	if (IS_HASWELL(dev) || IS_BROADWELL(dev)) {
++	if (IS_HASWELL(dev) || IS_BROADWELL(dev) || INTEL_INFO(dev)->gen >= 9) {
+ 		signal_levels = intel_hsw_signal_levels(train_set);
+ 		mask = DDI_BUF_EMP_MASK;
+ 	} else if (IS_CHERRYVIEW(dev)) {
+@@ -2919,80 +3607,16 @@
+ 
+ static bool
+ intel_dp_set_link_train(struct intel_dp *intel_dp,
+-			uint32_t *DP,
+-			uint8_t dp_train_pat)
+-{
+-	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+-	struct drm_device *dev = intel_dig_port->base.base.dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	enum port port = intel_dig_port->port;
+-	uint8_t buf[sizeof(intel_dp->train_set) + 1];
+-	int ret, len;
+-
+-	if (HAS_DDI(dev)) {
+-		uint32_t temp = I915_READ(DP_TP_CTL(port));
+-
+-		if (dp_train_pat & DP_LINK_SCRAMBLING_DISABLE)
+-			temp |= DP_TP_CTL_SCRAMBLE_DISABLE;
+-		else
+-			temp &= ~DP_TP_CTL_SCRAMBLE_DISABLE;
+-
+-		temp &= ~DP_TP_CTL_LINK_TRAIN_MASK;
+-		switch (dp_train_pat & DP_TRAINING_PATTERN_MASK) {
+-		case DP_TRAINING_PATTERN_DISABLE:
+-			temp |= DP_TP_CTL_LINK_TRAIN_NORMAL;
+-
+-			break;
+-		case DP_TRAINING_PATTERN_1:
+-			temp |= DP_TP_CTL_LINK_TRAIN_PAT1;
+-			break;
+-		case DP_TRAINING_PATTERN_2:
+-			temp |= DP_TP_CTL_LINK_TRAIN_PAT2;
+-			break;
+-		case DP_TRAINING_PATTERN_3:
+-			temp |= DP_TP_CTL_LINK_TRAIN_PAT3;
+-			break;
+-		}
+-		I915_WRITE(DP_TP_CTL(port), temp);
+-
+-	} else if (HAS_PCH_CPT(dev) && (IS_GEN7(dev) || port != PORT_A)) {
+-		*DP &= ~DP_LINK_TRAIN_MASK_CPT;
+-
+-		switch (dp_train_pat & DP_TRAINING_PATTERN_MASK) {
+-		case DP_TRAINING_PATTERN_DISABLE:
+-			*DP |= DP_LINK_TRAIN_OFF_CPT;
+-			break;
+-		case DP_TRAINING_PATTERN_1:
+-			*DP |= DP_LINK_TRAIN_PAT_1_CPT;
+-			break;
+-		case DP_TRAINING_PATTERN_2:
+-			*DP |= DP_LINK_TRAIN_PAT_2_CPT;
+-			break;
+-		case DP_TRAINING_PATTERN_3:
+-			DRM_ERROR("DP training pattern 3 not supported\n");
+-			*DP |= DP_LINK_TRAIN_PAT_2_CPT;
+-			break;
+-		}
+-
+-	} else {
+-		*DP &= ~DP_LINK_TRAIN_MASK;
+-
+-		switch (dp_train_pat & DP_TRAINING_PATTERN_MASK) {
+-		case DP_TRAINING_PATTERN_DISABLE:
+-			*DP |= DP_LINK_TRAIN_OFF;
+-			break;
+-		case DP_TRAINING_PATTERN_1:
+-			*DP |= DP_LINK_TRAIN_PAT_1;
+-			break;
+-		case DP_TRAINING_PATTERN_2:
+-			*DP |= DP_LINK_TRAIN_PAT_2;
+-			break;
+-		case DP_TRAINING_PATTERN_3:
+-			DRM_ERROR("DP training pattern 3 not supported\n");
+-			*DP |= DP_LINK_TRAIN_PAT_2;
+-			break;
+-		}
+-	}
++			uint32_t *DP,
++			uint8_t dp_train_pat)
++{
++	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
++	struct drm_device *dev = intel_dig_port->base.base.dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	uint8_t buf[sizeof(intel_dp->train_set) + 1];
++	int ret, len;
++
++	_intel_dp_set_link_train(intel_dp, DP, dp_train_pat);
+ 
+ 	I915_WRITE(intel_dp->output_reg, *DP);
+ 	POSTING_READ(intel_dp->output_reg);
+@@ -3220,7 +3844,6 @@
+ 
+ 		/* Try 5 times, then try clock recovery if that fails */
+ 		if (tries > 5) {
+-			intel_dp_link_down(intel_dp);
+ 			intel_dp_start_link_train(intel_dp);
+ 			intel_dp_set_link_train(intel_dp, &DP,
+ 						training_pattern |
+@@ -3276,7 +3899,10 @@
+ 		DP &= ~DP_LINK_TRAIN_MASK_CPT;
+ 		I915_WRITE(intel_dp->output_reg, DP | DP_LINK_TRAIN_PAT_IDLE_CPT);
+ 	} else {
+-		DP &= ~DP_LINK_TRAIN_MASK;
++		if (IS_CHERRYVIEW(dev))
++			DP &= ~DP_LINK_TRAIN_MASK_CHV;
++		else
++			DP &= ~DP_LINK_TRAIN_MASK;
+ 		I915_WRITE(intel_dp->output_reg, DP | DP_LINK_TRAIN_PAT_IDLE);
+ 	}
+ 	POSTING_READ(intel_dp->output_reg);
+@@ -3322,15 +3948,11 @@
+ 	struct drm_device *dev = dig_port->base.base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	char dpcd_hex_dump[sizeof(intel_dp->dpcd) * 3];
+-
+ 	if (intel_dp_dpcd_read_wake(&intel_dp->aux, 0x000, intel_dp->dpcd,
+ 				    sizeof(intel_dp->dpcd)) < 0)
+ 		return false; /* aux transfer failed */
+ 
+-	hex_dump_to_buffer(intel_dp->dpcd, sizeof(intel_dp->dpcd),
+-			   32, 1, dpcd_hex_dump, sizeof(dpcd_hex_dump), false);
+-	DRM_DEBUG_KMS("DPCD: %s\n", dpcd_hex_dump);
++	DRM_DEBUG_KMS("DPCD: %*ph\n", (int) sizeof(intel_dp->dpcd), intel_dp->dpcd);
+ 
+ 	if (intel_dp->dpcd[DP_DPCD_REV] == 0)
+ 		return false; /* DPCD not present */
+@@ -3345,13 +3967,16 @@
+ 			dev_priv->psr.sink_support = true;
+ 			DRM_DEBUG_KMS("Detected EDP PSR Panel.\n");
+ 		}
++		intel_edp_set_psr_property(intel_dp->attached_connector,
++					   dev_priv->psr.sink_support ? dev_priv->psr.active : -1);
+ 	}
+ 
+-	/* Training Pattern 3 support */
++	/* Training Pattern 3 support, both source and sink */
+ 	if (intel_dp->dpcd[DP_DPCD_REV] >= 0x12 &&
+-	    intel_dp->dpcd[DP_MAX_LANE_COUNT] & DP_TPS3_SUPPORTED) {
++	    intel_dp->dpcd[DP_MAX_LANE_COUNT] & DP_TPS3_SUPPORTED &&
++	    (IS_HASWELL(dev_priv) || INTEL_INFO(dev_priv)->gen >= 8)) {
+ 		intel_dp->use_tps3 = true;
+-		DRM_DEBUG_KMS("Displayport TPS3 supported");
++		DRM_DEBUG_KMS("Displayport TPS3 supported\n");
+ 	} else
+ 		intel_dp->use_tps3 = false;
+ 
+@@ -3378,8 +4003,6 @@
+ 	if (!(intel_dp->dpcd[DP_DOWN_STREAM_PORT_COUNT] & DP_OUI_SUPPORT))
+ 		return;
+ 
+-	intel_edp_panel_vdd_on(intel_dp);
+-
+ 	if (intel_dp_dpcd_read_wake(&intel_dp->aux, DP_SINK_OUI, buf, 3) == 3)
+ 		DRM_DEBUG_KMS("Sink OUI: %02hx%02hx%02hx\n",
+ 			      buf[0], buf[1], buf[2]);
+@@ -3387,8 +4010,6 @@
+ 	if (intel_dp_dpcd_read_wake(&intel_dp->aux, DP_BRANCH_OUI, buf, 3) == 3)
+ 		DRM_DEBUG_KMS("Branch OUI: %02hx%02hx%02hx\n",
+ 			      buf[0], buf[1], buf[2]);
+-
+-	edp_panel_vdd_off(intel_dp, false);
+ }
+ 
+ static bool
+@@ -3402,7 +4023,6 @@
+ 	if (intel_dp->dpcd[DP_DPCD_REV] < 0x12)
+ 		return false;
+ 
+-	_edp_panel_vdd_on(intel_dp);
+ 	if (intel_dp_dpcd_read_wake(&intel_dp->aux, DP_MSTM_CAP, buf, 1)) {
+ 		if (buf[0] & DP_MST_CAP) {
+ 			DRM_DEBUG_KMS("Sink is MST capable\n");
+@@ -3412,7 +4032,6 @@
+ 			intel_dp->is_mst = false;
+ 		}
+ 	}
+-	edp_panel_vdd_off(intel_dp, false);
+ 
+ 	drm_dp_mst_topology_mgr_set_mst(&intel_dp->mst_mgr, intel_dp->is_mst);
+ 	return intel_dp->is_mst;
+@@ -3424,26 +4043,48 @@
+ 	struct drm_device *dev = intel_dig_port->base.base.dev;
+ 	struct intel_crtc *intel_crtc =
+ 		to_intel_crtc(intel_dig_port->base.base.crtc);
+-	u8 buf[1];
++	u8 buf;
++	int test_crc_count;
++	int attempts = 6;
+ 
+-	if (drm_dp_dpcd_readb(&intel_dp->aux, DP_TEST_SINK_MISC, buf) < 0)
+-		return -EAGAIN;
++	if (drm_dp_dpcd_readb(&intel_dp->aux, DP_TEST_SINK_MISC, &buf) < 0)
++		return -EIO;
+ 
+-	if (!(buf[0] & DP_TEST_CRC_SUPPORTED))
++	if (!(buf & DP_TEST_CRC_SUPPORTED))
+ 		return -ENOTTY;
+ 
++	if (drm_dp_dpcd_readb(&intel_dp->aux, DP_TEST_SINK, &buf) < 0)
++		return -EIO;
++
+ 	if (drm_dp_dpcd_writeb(&intel_dp->aux, DP_TEST_SINK,
+-			       DP_TEST_SINK_START) < 0)
+-		return -EAGAIN;
++				buf | DP_TEST_SINK_START) < 0)
++		return -EIO;
+ 
+-	/* Wait 2 vblanks to be sure we will have the correct CRC value */
+-	intel_wait_for_vblank(dev, intel_crtc->pipe);
+-	intel_wait_for_vblank(dev, intel_crtc->pipe);
++	if (drm_dp_dpcd_readb(&intel_dp->aux, DP_TEST_SINK_MISC, &buf) < 0)
++		return -EIO;
++	test_crc_count = buf & DP_TEST_COUNT_MASK;
++
++	do {
++		if (drm_dp_dpcd_readb(&intel_dp->aux,
++				      DP_TEST_SINK_MISC, &buf) < 0)
++			return -EIO;
++		intel_wait_for_vblank(dev, intel_crtc->pipe);
++	} while (--attempts && (buf & DP_TEST_COUNT_MASK) == test_crc_count);
++
++	if (attempts == 0) {
++		DRM_ERROR("Panel is unable to calculate CRC after 6 vblanks\n");
++		return -EIO;
++	}
+ 
+ 	if (drm_dp_dpcd_read(&intel_dp->aux, DP_TEST_CRC_R_CR, crc, 6) < 0)
+-		return -EAGAIN;
++		return -EIO;
++
++	if (drm_dp_dpcd_readb(&intel_dp->aux, DP_TEST_SINK, &buf) < 0)
++		return -EIO;
++	if (drm_dp_dpcd_writeb(&intel_dp->aux, DP_TEST_SINK,
++			       buf & ~DP_TEST_SINK_START) < 0)
++		return -EIO;
+ 
+-	drm_dp_dpcd_writeb(&intel_dp->aux, DP_TEST_SINK, 0);
+ 	return 0;
+ }
+ 
+@@ -3644,20 +4285,24 @@
+ }
+ 
+ static enum drm_connector_status
++edp_detect(struct intel_dp *intel_dp)
++{
++	struct drm_device *dev = intel_dp_to_dev(intel_dp);
++	enum drm_connector_status status;
++
++	status = intel_panel_detect(dev);
++	if (status == connector_status_unknown)
++		status = connector_status_connected;
++
++	return status;
++}
++
++static enum drm_connector_status
+ ironlake_dp_detect(struct intel_dp *intel_dp)
+ {
+ 	struct drm_device *dev = intel_dp_to_dev(intel_dp);
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+-	enum drm_connector_status status;
+-
+-	/* Can't disconnect eDP, but you can close the lid... */
+-	if (is_edp(intel_dp)) {
+-		status = intel_panel_detect(dev);
+-		if (status == connector_status_unknown)
+-			status = connector_status_connected;
+-		return status;
+-	}
+ 
+ 	if (!ibx_digital_port_connected(dev_priv, intel_dig_port))
+ 		return connector_status_disconnected;
+@@ -3713,16 +4358,6 @@
+ 	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+ 	int ret;
+ 
+-	/* Can't disconnect eDP, but you can close the lid... */
+-	if (is_edp(intel_dp)) {
+-		enum drm_connector_status status;
+-
+-		status = intel_panel_detect(dev);
+-		if (status == connector_status_unknown)
+-			status = connector_status_connected;
+-		return status;
+-	}
+-
+ 	ret = g4x_digital_port_connected(dev, intel_dig_port);
+ 	if (ret == -EINVAL)
+ 		return connector_status_unknown;
+@@ -3733,9 +4368,9 @@
+ }
+ 
+ static struct edid *
+-intel_dp_get_edid(struct drm_connector *connector, struct i2c_adapter *adapter)
++intel_dp_get_edid(struct intel_dp *intel_dp)
+ {
+-	struct intel_connector *intel_connector = to_intel_connector(connector);
++	struct intel_connector *intel_connector = intel_dp->attached_connector;
+ 
+ 	/* use cached edid if we have one */
+ 	if (intel_connector->edid) {
+@@ -3744,27 +4379,71 @@
+ 			return NULL;
+ 
+ 		return drm_edid_duplicate(intel_connector->edid);
+-	}
++	} else
++		return drm_get_edid(&intel_connector->base,
++				    &intel_dp->aux.ddc);
++}
++
++static bool
++intel_dp_update_audio(struct intel_dp *intel_dp)
++{
++	struct intel_connector *intel_connector = intel_dp->attached_connector;
++	struct edid *edid = intel_connector->detect_edid ;
++	bool has_audio;
++
++	if (intel_dp->force_audio != HDMI_AUDIO_AUTO)
++		has_audio = intel_dp->force_audio == HDMI_AUDIO_ON;
++	else
++		has_audio = drm_detect_monitor_audio(edid);
++
++	if (has_audio == intel_dp->has_audio)
++		return false;
+ 
+-	return drm_get_edid(connector, adapter);
++	intel_dp->has_audio = has_audio;
++	return true;
+ }
+ 
+-static int
+-intel_dp_get_edid_modes(struct drm_connector *connector, struct i2c_adapter *adapter)
++static void
++intel_dp_set_edid(struct intel_dp *intel_dp)
+ {
+-	struct intel_connector *intel_connector = to_intel_connector(connector);
++	struct intel_connector *intel_connector = intel_dp->attached_connector;
++	struct edid *edid;
+ 
+-	/* use cached edid if we have one */
+-	if (intel_connector->edid) {
+-		/* invalid edid */
+-		if (IS_ERR(intel_connector->edid))
+-			return 0;
++	edid = intel_dp_get_edid(intel_dp);
++	intel_connector->detect_edid = edid;
+ 
+-		return intel_connector_update_modes(connector,
+-						    intel_connector->edid);
+-	}
++	intel_dp_update_audio(intel_dp);
++}
++
++static void
++intel_dp_unset_edid(struct intel_dp *intel_dp)
++{
++	struct intel_connector *intel_connector = intel_dp->attached_connector;
++
++	kfree(intel_connector->detect_edid);
++	intel_connector->detect_edid = NULL;
++
++	intel_dp->has_audio = false;
++}
++
++static enum intel_display_power_domain
++intel_dp_power_get(struct intel_dp *dp)
++{
++	struct intel_encoder *encoder = &dp_to_dig_port(dp)->base;
++	enum intel_display_power_domain power_domain;
++
++	power_domain = intel_display_port_power_domain(encoder);
++	intel_display_power_get(to_i915(encoder->base.dev), power_domain);
++
++	return power_domain;
++}
+ 
+-	return intel_ddc_get_modes(connector, adapter);
++static void
++intel_dp_power_put(struct intel_dp *dp,
++		   enum intel_display_power_domain power_domain)
++{
++	struct intel_encoder *encoder = &dp_to_dig_port(dp)->base;
++	intel_display_power_put(to_i915(encoder->base.dev), power_domain);
+ }
+ 
+ static enum drm_connector_status
+@@ -3774,33 +4453,30 @@
+ 	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+ 	struct intel_encoder *intel_encoder = &intel_dig_port->base;
+ 	struct drm_device *dev = connector->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	enum drm_connector_status status;
+ 	enum intel_display_power_domain power_domain;
+-	struct edid *edid = NULL;
+ 	bool ret;
+ 
+-	power_domain = intel_display_port_power_domain(intel_encoder);
+-	intel_display_power_get(dev_priv, power_domain);
+-
+ 	DRM_DEBUG_KMS("[CONNECTOR:%d:%s]\n",
+ 		      connector->base.id, connector->name);
++	intel_dp_unset_edid(intel_dp);
+ 
+ 	if (intel_dp->is_mst) {
+ 		/* MST devices are disconnected from a monitor POV */
+ 		if (intel_encoder->type != INTEL_OUTPUT_EDP)
+ 			intel_encoder->type = INTEL_OUTPUT_DISPLAYPORT;
+-		status = connector_status_disconnected;
+-		goto out;
++		return connector_status_disconnected;
+ 	}
+ 
+-	intel_dp->has_audio = false;
++	power_domain = intel_dp_power_get(intel_dp);
+ 
+-	if (HAS_PCH_SPLIT(dev))
++	/* Can't disconnect eDP, but you can close the lid... */
++	if (is_edp(intel_dp))
++		status = edp_detect(intel_dp);
++	else if (HAS_PCH_SPLIT(dev))
+ 		status = ironlake_dp_detect(intel_dp);
+ 	else
+ 		status = g4x_dp_detect(intel_dp);
+-
+ 	if (status != connector_status_connected)
+ 		goto out;
+ 
+@@ -3816,84 +4492,67 @@
+ 		goto out;
+ 	}
+ 
+-	if (intel_dp->force_audio != HDMI_AUDIO_AUTO) {
+-		intel_dp->has_audio = (intel_dp->force_audio == HDMI_AUDIO_ON);
+-	} else {
+-		edid = intel_dp_get_edid(connector, &intel_dp->aux.ddc);
+-		if (edid) {
+-			intel_dp->has_audio = drm_detect_monitor_audio(edid);
+-			kfree(edid);
+-		}
+-	}
++	intel_dp_set_edid(intel_dp);
+ 
+ 	if (intel_encoder->type != INTEL_OUTPUT_EDP)
+ 		intel_encoder->type = INTEL_OUTPUT_DISPLAYPORT;
+ 	status = connector_status_connected;
+ 
+ out:
+-	intel_display_power_put(dev_priv, power_domain);
++	intel_dp_power_put(intel_dp, power_domain);
+ 	return status;
+ }
+ 
+-static int intel_dp_get_modes(struct drm_connector *connector)
++static void
++intel_dp_force(struct drm_connector *connector)
+ {
+ 	struct intel_dp *intel_dp = intel_attached_dp(connector);
+-	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+-	struct intel_encoder *intel_encoder = &intel_dig_port->base;
+-	struct intel_connector *intel_connector = to_intel_connector(connector);
+-	struct drm_device *dev = connector->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_encoder *intel_encoder = &dp_to_dig_port(intel_dp)->base;
+ 	enum intel_display_power_domain power_domain;
+-	int ret;
+ 
+-	/* We should parse the EDID data and find out if it has an audio sink
+-	 */
++	DRM_DEBUG_KMS("[CONNECTOR:%d:%s]\n",
++		      connector->base.id, connector->name);
++	intel_dp_unset_edid(intel_dp);
+ 
+-	power_domain = intel_display_port_power_domain(intel_encoder);
+-	intel_display_power_get(dev_priv, power_domain);
++	if (connector->status != connector_status_connected)
++		return;
+ 
+-	ret = intel_dp_get_edid_modes(connector, &intel_dp->aux.ddc);
+-	intel_display_power_put(dev_priv, power_domain);
+-	if (ret)
+-		return ret;
++	power_domain = intel_dp_power_get(intel_dp);
+ 
+-	/* if eDP has no EDID, fall back to fixed mode */
+-	if (is_edp(intel_dp) && intel_connector->panel.fixed_mode) {
+-		struct drm_display_mode *mode;
+-		mode = drm_mode_duplicate(dev,
+-					  intel_connector->panel.fixed_mode);
+-		if (mode) {
+-			drm_mode_probed_add(connector, mode);
+-			return 1;
+-		}
+-	}
+-	return 0;
++	intel_dp_set_edid(intel_dp);
++
++	intel_dp_power_put(intel_dp, power_domain);
++
++	if (intel_encoder->type != INTEL_OUTPUT_EDP)
++		intel_encoder->type = INTEL_OUTPUT_DISPLAYPORT;
+ }
+ 
+-static bool
+-intel_dp_detect_audio(struct drm_connector *connector)
++static int intel_dp_get_modes(struct drm_connector *connector)
+ {
+-	struct intel_dp *intel_dp = intel_attached_dp(connector);
+-	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+-	struct intel_encoder *intel_encoder = &intel_dig_port->base;
+-	struct drm_device *dev = connector->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	enum intel_display_power_domain power_domain;
++	struct intel_connector *intel_connector = to_intel_connector(connector);
+ 	struct edid *edid;
+-	bool has_audio = false;
+ 
+-	power_domain = intel_display_port_power_domain(intel_encoder);
+-	intel_display_power_get(dev_priv, power_domain);
+-
+-	edid = intel_dp_get_edid(connector, &intel_dp->aux.ddc);
++	edid = intel_connector->detect_edid;
+ 	if (edid) {
+-		has_audio = drm_detect_monitor_audio(edid);
+-		kfree(edid);
++		int ret = intel_connector_update_modes(connector, edid);
++		if (ret)
++			return ret;
+ 	}
+ 
+-	intel_display_power_put(dev_priv, power_domain);
++	/* if eDP has no EDID, fall back to fixed mode */
++	if (is_edp(intel_attached_dp(connector)) &&
++	    intel_connector->panel.fixed_mode) {
++		struct drm_display_mode *mode;
++
++		mode = drm_mode_duplicate(connector->dev,
++					  intel_connector->panel.fixed_mode);
++		if (mode) {
++			drm_mode_probed_add(connector, mode);
++			return 1;
++		}
++	}
+ 
+-	return has_audio;
++	return 0;
+ }
+ 
+ static int
+@@ -3913,22 +4572,14 @@
+ 
+ 	if (property == dev_priv->force_audio_property) {
+ 		int i = val;
+-		bool has_audio;
+ 
+ 		if (i == intel_dp->force_audio)
+ 			return 0;
+ 
+ 		intel_dp->force_audio = i;
+-
+-		if (i == HDMI_AUDIO_AUTO)
+-			has_audio = intel_dp_detect_audio(connector);
+-		else
+-			has_audio = (i == HDMI_AUDIO_ON);
+-
+-		if (has_audio == intel_dp->has_audio)
++		if (!intel_dp_update_audio(intel_dp))
+ 			return 0;
+ 
+-		intel_dp->has_audio = has_audio;
+ 		goto done;
+ 	}
+ 
+@@ -3989,6 +4640,8 @@
+ {
+ 	struct intel_connector *intel_connector = to_intel_connector(connector);
+ 
++	kfree(intel_connector->detect_edid);
++
+ 	if (!IS_ERR_OR_NULL(intel_connector->edid))
+ 		kfree(intel_connector->edid);
+ 
+@@ -4005,16 +4658,20 @@
+ {
+ 	struct intel_digital_port *intel_dig_port = enc_to_dig_port(encoder);
+ 	struct intel_dp *intel_dp = &intel_dig_port->dp;
+-	struct drm_device *dev = intel_dp_to_dev(intel_dp);
+ 
+ 	drm_dp_aux_unregister(&intel_dp->aux);
+ 	intel_dp_mst_encoder_cleanup(intel_dig_port);
+ 	drm_encoder_cleanup(encoder);
+ 	if (is_edp(intel_dp)) {
+ 		cancel_delayed_work_sync(&intel_dp->panel_vdd_work);
+-		drm_modeset_lock(&dev->mode_config.connection_mutex, NULL);
++		/*
++		 * vdd might still be enabled do to the delayed vdd off.
++		 * Make sure vdd is actually turned off here.
++		 */
++		pps_lock(intel_dp);
+ 		edp_panel_vdd_off_sync(intel_dp);
+-		drm_modeset_unlock(&dev->mode_config.connection_mutex);
++		pps_unlock(intel_dp);
++
+ 		if (intel_dp->edp_notifier.notifier_call) {
+ 			unregister_reboot_notifier(&intel_dp->edp_notifier);
+ 			intel_dp->edp_notifier.notifier_call = NULL;
+@@ -4030,17 +4687,67 @@
+ 	if (!is_edp(intel_dp))
+ 		return;
+ 
++	/*
++	 * vdd might still be enabled do to the delayed vdd off.
++	 * Make sure vdd is actually turned off here.
++	 */
++	pps_lock(intel_dp);
+ 	edp_panel_vdd_off_sync(intel_dp);
++	pps_unlock(intel_dp);
++}
++
++static void intel_edp_panel_vdd_sanitize(struct intel_dp *intel_dp)
++{
++	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
++	struct drm_device *dev = intel_dig_port->base.base.dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	enum intel_display_power_domain power_domain;
++
++	lockdep_assert_held(&dev_priv->pps_mutex);
++
++	if (!edp_have_panel_vdd(intel_dp))
++		return;
++
++	/*
++	 * The VDD bit needs a power domain reference, so if the bit is
++	 * already enabled when we boot or resume, grab this reference and
++	 * schedule a vdd off, so we don't hold on to the reference
++	 * indefinitely.
++	 */
++	DRM_DEBUG_KMS("VDD left on by BIOS, adjusting state tracking\n");
++	power_domain = intel_display_port_power_domain(&intel_dig_port->base);
++	intel_display_power_get(dev_priv, power_domain);
++
++	edp_panel_vdd_schedule_off(intel_dp);
+ }
+ 
+ static void intel_dp_encoder_reset(struct drm_encoder *encoder)
+ {
+-	intel_edp_panel_vdd_sanitize(to_intel_encoder(encoder));
++	struct intel_dp *intel_dp;
++
++	if (to_intel_encoder(encoder)->type != INTEL_OUTPUT_EDP)
++		return;
++
++	intel_dp = enc_to_intel_dp(encoder);
++
++	pps_lock(intel_dp);
++
++	/*
++	 * Read out the current power sequencer assignment,
++	 * in case the BIOS did something with it.
++	 */
++	if (IS_VALLEYVIEW(encoder->dev))
++		vlv_initial_power_sequencer_setup(intel_dp);
++
++	intel_edp_panel_vdd_sanitize(intel_dp);
++
++	pps_unlock(intel_dp);
+ }
+ 
+ static const struct drm_connector_funcs intel_dp_connector_funcs = {
+ 	.dpms = intel_connector_dpms,
+ 	.detect = intel_dp_detect,
++	.force = intel_dp_force,
+ 	.fill_modes = drm_helper_probe_single_connector_modes,
+ 	.set_property = intel_dp_set_property,
+ 	.destroy = intel_dp_connector_destroy,
+@@ -4076,7 +4783,20 @@
+ 	if (intel_dig_port->base.type != INTEL_OUTPUT_EDP)
+ 		intel_dig_port->base.type = INTEL_OUTPUT_DISPLAYPORT;
+ 
+-	DRM_DEBUG_KMS("got hpd irq on port %d - %s\n", intel_dig_port->port,
++	if (long_hpd && intel_dig_port->base.type == INTEL_OUTPUT_EDP) {
++		/*
++		 * vdd off can generate a long pulse on eDP which
++		 * would require vdd on to handle it, and thus we
++		 * would end up in an endless cycle of
++		 * "vdd off -> long hpd -> vdd on -> detect -> vdd off -> ..."
++		 */
++		DRM_DEBUG_KMS("ignoring long hpd on eDP port %c\n",
++			      port_name(intel_dig_port->port));
++		return false;
++	}
++
++	DRM_DEBUG_KMS("got hpd irq on port %c - %s\n",
++		      port_name(intel_dig_port->port),
+ 		      long_hpd ? "long" : "short");
+ 
+ 	power_domain = intel_display_port_power_domain(intel_encoder);
+@@ -4190,6 +4910,8 @@
+ 	intel_dp->color_range_auto = true;
+ 
+ 	if (is_edp(intel_dp)) {
++		if (HAS_PSR(connector->dev))
++			intel_attach_psr_property(connector);
+ 		drm_mode_create_scaling_mode_property(connector->dev);
+ 		drm_object_attach_property(
+ 			&connector->base,
+@@ -4208,14 +4930,20 @@
+ 
+ static void
+ intel_dp_init_panel_power_sequencer(struct drm_device *dev,
+-				    struct intel_dp *intel_dp,
+-				    struct edp_power_seq *out)
++				    struct intel_dp *intel_dp)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct edp_power_seq cur, vbt, spec, final;
++	struct edp_power_seq cur, vbt, spec,
++		*final = &intel_dp->pps_delays;
+ 	u32 pp_on, pp_off, pp_div, pp;
+ 	int pp_ctrl_reg, pp_on_reg, pp_off_reg, pp_div_reg;
+ 
++	lockdep_assert_held(&dev_priv->pps_mutex);
++
++	/* already initialized? */
++	if (final->t11_t12 != 0)
++		return;
++
+ 	if (HAS_PCH_SPLIT(dev)) {
+ 		pp_ctrl_reg = PCH_PP_CONTROL;
+ 		pp_on_reg = PCH_PP_ON_DELAYS;
+@@ -4277,7 +5005,7 @@
+ 
+ 	/* Use the max of the register settings and vbt. If both are
+ 	 * unset, fall back to the spec limits. */
+-#define assign_final(field)	final.field = (max(cur.field, vbt.field) == 0 ? \
++#define assign_final(field)	final->field = (max(cur.field, vbt.field) == 0 ? \
+ 				       spec.field : \
+ 				       max(cur.field, vbt.field))
+ 	assign_final(t1_t3);
+@@ -4287,7 +5015,7 @@
+ 	assign_final(t11_t12);
+ #undef assign_final
+ 
+-#define get_delay(field)	(DIV_ROUND_UP(final.field, 10))
++#define get_delay(field)	(DIV_ROUND_UP(final->field, 10))
+ 	intel_dp->panel_power_up_delay = get_delay(t1_t3);
+ 	intel_dp->backlight_on_delay = get_delay(t8);
+ 	intel_dp->backlight_off_delay = get_delay(t9);
+@@ -4301,20 +5029,20 @@
+ 
+ 	DRM_DEBUG_KMS("backlight on delay %d, off delay %d\n",
+ 		      intel_dp->backlight_on_delay, intel_dp->backlight_off_delay);
+-
+-	if (out)
+-		*out = final;
+ }
+ 
+ static void
+ intel_dp_init_panel_power_sequencer_registers(struct drm_device *dev,
+-					      struct intel_dp *intel_dp,
+-					      struct edp_power_seq *seq)
++					      struct intel_dp *intel_dp)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	u32 pp_on, pp_off, pp_div, port_sel = 0;
+ 	int div = HAS_PCH_SPLIT(dev) ? intel_pch_rawclk(dev) : intel_hrawclk(dev);
+ 	int pp_on_reg, pp_off_reg, pp_div_reg;
++	enum port port = dp_to_dig_port(intel_dp)->port;
++	const struct edp_power_seq *seq = &intel_dp->pps_delays;
++
++	lockdep_assert_held(&dev_priv->pps_mutex);
+ 
+ 	if (HAS_PCH_SPLIT(dev)) {
+ 		pp_on_reg = PCH_PP_ON_DELAYS;
+@@ -4349,12 +5077,9 @@
+ 	/* Haswell doesn't have any port selection bits for the panel
+ 	 * power sequencer any more. */
+ 	if (IS_VALLEYVIEW(dev)) {
+-		if (dp_to_dig_port(intel_dp)->port == PORT_B)
+-			port_sel = PANEL_PORT_SELECT_DPB_VLV;
+-		else
+-			port_sel = PANEL_PORT_SELECT_DPC_VLV;
++		port_sel = PANEL_PORT_SELECT_VLV(port);
+ 	} else if (HAS_PCH_IBX(dev) || HAS_PCH_CPT(dev)) {
+-		if (dp_to_dig_port(intel_dp)->port == PORT_A)
++		if (port == PORT_A)
+ 			port_sel = PANEL_PORT_SELECT_DPA;
+ 		else
+ 			port_sel = PANEL_PORT_SELECT_DPD;
+@@ -4438,7 +5163,7 @@
+ 		val = I915_READ(reg);
+ 		if (index > DRRS_HIGH_RR) {
+ 			val |= PIPECONF_EDP_RR_MODE_SWITCH;
+-			intel_dp_set_m2_n2(intel_crtc, &config->dp_m2_n2);
++			intel_dp_set_m_n(intel_crtc);
+ 		} else {
+ 			val &= ~PIPECONF_EDP_RR_MODE_SWITCH;
+ 		}
+@@ -4478,7 +5203,7 @@
+ 	}
+ 
+ 	if (dev_priv->vbt.drrs_type != SEAMLESS_DRRS_SUPPORT) {
+-		DRM_INFO("VBT doesn't support DRRS\n");
++		DRM_DEBUG_KMS("VBT doesn't support DRRS\n");
+ 		return NULL;
+ 	}
+ 
+@@ -4486,7 +5211,7 @@
+ 					(dev, fixed_mode, connector);
+ 
+ 	if (!downclock_mode) {
+-		DRM_INFO("DRRS not supported\n");
++		DRM_DEBUG_KMS("DRRS not supported\n");
+ 		return NULL;
+ 	}
+ 
+@@ -4497,39 +5222,12 @@
+ 	intel_dp->drrs_state.type = dev_priv->vbt.drrs_type;
+ 
+ 	intel_dp->drrs_state.refresh_rate_type = DRRS_HIGH_RR;
+-	DRM_INFO("seamless DRRS supported for eDP panel.\n");
++	DRM_DEBUG_KMS("seamless DRRS supported for eDP panel.\n");
+ 	return downclock_mode;
+ }
+ 
+-void intel_edp_panel_vdd_sanitize(struct intel_encoder *intel_encoder)
+-{
+-	struct drm_device *dev = intel_encoder->base.dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_dp *intel_dp;
+-	enum intel_display_power_domain power_domain;
+-
+-	if (intel_encoder->type != INTEL_OUTPUT_EDP)
+-		return;
+-
+-	intel_dp = enc_to_intel_dp(&intel_encoder->base);
+-	if (!edp_have_panel_vdd(intel_dp))
+-		return;
+-	/*
+-	 * The VDD bit needs a power domain reference, so if the bit is
+-	 * already enabled when we boot or resume, grab this reference and
+-	 * schedule a vdd off, so we don't hold on to the reference
+-	 * indefinitely.
+-	 */
+-	DRM_DEBUG_KMS("VDD left on by BIOS, adjusting state tracking\n");
+-	power_domain = intel_display_port_power_domain(intel_encoder);
+-	intel_display_power_get(dev_priv, power_domain);
+-
+-	edp_panel_vdd_schedule_off(intel_dp);
+-}
+-
+ static bool intel_edp_init_connector(struct intel_dp *intel_dp,
+-				     struct intel_connector *intel_connector,
+-				     struct edp_power_seq *power_seq)
++				     struct intel_connector *intel_connector)
+ {
+ 	struct drm_connector *connector = &intel_connector->base;
+ 	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+@@ -4541,18 +5239,19 @@
+ 	bool has_dpcd;
+ 	struct drm_display_mode *scan;
+ 	struct edid *edid;
++	enum pipe pipe = INVALID_PIPE;
+ 
+ 	intel_dp->drrs_state.type = DRRS_NOT_SUPPORTED;
+ 
+ 	if (!is_edp(intel_dp))
+ 		return true;
+ 
+-	intel_edp_panel_vdd_sanitize(intel_encoder);
++	pps_lock(intel_dp);
++	intel_edp_panel_vdd_sanitize(intel_dp);
++	pps_unlock(intel_dp);
+ 
+ 	/* Cache DPCD and EDID for edp. */
+-	intel_edp_panel_vdd_on(intel_dp);
+ 	has_dpcd = intel_dp_get_dpcd(intel_dp);
+-	edp_panel_vdd_off(intel_dp, false);
+ 
+ 	if (has_dpcd) {
+ 		if (intel_dp->dpcd[DP_DPCD_REV] >= 0x11)
+@@ -4566,7 +5265,9 @@
+ 	}
+ 
+ 	/* We now know it's not a ghost, init power sequence regs. */
+-	intel_dp_init_panel_power_sequencer_registers(dev, intel_dp, power_seq);
++	pps_lock(intel_dp);
++	intel_dp_init_panel_power_sequencer_registers(dev, intel_dp);
++	pps_unlock(intel_dp);
+ 
+ 	mutex_lock(&dev->mode_config.mutex);
+ 	edid = drm_get_edid(connector, &intel_dp->aux.ddc);
+@@ -4607,10 +5308,30 @@
+ 	if (IS_VALLEYVIEW(dev)) {
+ 		intel_dp->edp_notifier.notifier_call = edp_notify_handler;
+ 		register_reboot_notifier(&intel_dp->edp_notifier);
++
++		/*
++		 * Figure out the current pipe for the initial backlight setup.
++		 * If the current pipe isn't valid, try the PPS pipe, and if that
++		 * fails just assume pipe A.
++		 */
++		if (IS_CHERRYVIEW(dev))
++			pipe = DP_PORT_TO_PIPE_CHV(intel_dp->DP);
++		else
++			pipe = PORT_TO_PIPE(intel_dp->DP);
++
++		if (pipe != PIPE_A && pipe != PIPE_B)
++			pipe = intel_dp->pps_pipe;
++
++		if (pipe != PIPE_A && pipe != PIPE_B)
++			pipe = PIPE_A;
++
++		DRM_DEBUG_KMS("using pipe %c for initial backlight setup\n",
++			      pipe_name(pipe));
+ 	}
+ 
+ 	intel_panel_init(&intel_connector->panel, fixed_mode, downclock_mode);
+-	intel_panel_setup_backlight(connector);
++	intel_connector->panel.backlight_power = intel_edp_backlight_power;
++	intel_panel_setup_backlight(connector, pipe);
+ 
+ 	return true;
+ }
+@@ -4625,11 +5346,14 @@
+ 	struct drm_device *dev = intel_encoder->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	enum port port = intel_dig_port->port;
+-	struct edp_power_seq power_seq = { 0 };
+ 	int type;
+ 
++	intel_dp->pps_pipe = INVALID_PIPE;
++
+ 	/* intel_dp vfuncs */
+-	if (IS_VALLEYVIEW(dev))
++	if (INTEL_INFO(dev)->gen >= 9)
++		intel_dp->get_aux_clock_divider = skl_get_aux_clock_divider;
++	else if (IS_VALLEYVIEW(dev))
+ 		intel_dp->get_aux_clock_divider = vlv_get_aux_clock_divider;
+ 	else if (IS_HASWELL(dev) || IS_BROADWELL(dev))
+ 		intel_dp->get_aux_clock_divider = hsw_get_aux_clock_divider;
+@@ -4638,7 +5362,10 @@
+ 	else
+ 		intel_dp->get_aux_clock_divider = i9xx_get_aux_clock_divider;
+ 
+-	intel_dp->get_aux_send_ctl = i9xx_get_aux_send_ctl;
++	if (INTEL_INFO(dev)->gen >= 9)
++		intel_dp->get_aux_send_ctl = skl_get_aux_send_ctl;
++	else
++		intel_dp->get_aux_send_ctl = i9xx_get_aux_send_ctl;
+ 
+ 	/* Preserve the current hw state. */
+ 	intel_dp->DP = I915_READ(intel_dp->output_reg);
+@@ -4657,6 +5384,11 @@
+ 	if (type == DRM_MODE_CONNECTOR_eDP)
+ 		intel_encoder->type = INTEL_OUTPUT_EDP;
+ 
++	/* eDP only on port B and/or C on vlv/chv */
++	if (WARN_ON(IS_VALLEYVIEW(dev) && is_edp(intel_dp) &&
++		    port != PORT_B && port != PORT_C))
++		return false;
++
+ 	DRM_DEBUG_KMS("Adding %s connector on port %c\n",
+ 			type == DRM_MODE_CONNECTOR_eDP ? "eDP" : "DP",
+ 			port_name(port));
+@@ -4673,6 +5405,8 @@
+ 	intel_connector_attach_encoder(intel_connector, intel_encoder);
+ 	drm_connector_register(connector);
+ 
++	intel_dp_add_properties(intel_dp, connector);
++
+ 	if (HAS_DDI(dev))
+ 		intel_connector->get_hw_state = intel_ddi_connector_get_hw_state;
+ 	else
+@@ -4698,8 +5432,13 @@
+ 	}
+ 
+ 	if (is_edp(intel_dp)) {
++		pps_lock(intel_dp);
+ 		intel_dp_init_panel_power_timestamps(intel_dp);
+-		intel_dp_init_panel_power_sequencer(dev, intel_dp, &power_seq);
++		if (IS_VALLEYVIEW(dev))
++			vlv_initial_power_sequencer_setup(intel_dp);
++		else
++			intel_dp_init_panel_power_sequencer(dev, intel_dp);
++		pps_unlock(intel_dp);
+ 	}
+ 
+ 	intel_dp_aux_init(intel_dp, intel_connector);
+@@ -4707,25 +5446,28 @@
+ 	/* init MST on ports that can support it */
+ 	if (IS_HASWELL(dev) || IS_BROADWELL(dev)) {
+ 		if (port == PORT_B || port == PORT_C || port == PORT_D) {
+-			intel_dp_mst_encoder_init(intel_dig_port, intel_connector->base.base.id);
++			intel_dp_mst_encoder_init(intel_dig_port,
++						  intel_connector->base.base.id);
+ 		}
+ 	}
+ 
+-	if (!intel_edp_init_connector(intel_dp, intel_connector, &power_seq)) {
++	if (!intel_edp_init_connector(intel_dp, intel_connector)) {
+ 		drm_dp_aux_unregister(&intel_dp->aux);
+ 		if (is_edp(intel_dp)) {
+ 			cancel_delayed_work_sync(&intel_dp->panel_vdd_work);
+-			drm_modeset_lock(&dev->mode_config.connection_mutex, NULL);
++			/*
++			 * vdd might still be enabled do to the delayed vdd off.
++			 * Make sure vdd is actually turned off here.
++			 */
++			pps_lock(intel_dp);
+ 			edp_panel_vdd_off_sync(intel_dp);
+-			drm_modeset_unlock(&dev->mode_config.connection_mutex);
++			pps_unlock(intel_dp);
+ 		}
+ 		drm_connector_unregister(connector);
+ 		drm_connector_cleanup(connector);
+ 		return false;
+ 	}
+ 
+-	intel_dp_add_properties(intel_dp, connector);
+-
+ 	/* For G4X desktop chip, PEG_BAND_GAP_DATA 3:0 must first be written
+ 	 * 0xd.  Failure to do so will result in spurious interrupts being
+ 	 * generated on the port when a cable is not attached.
+@@ -4781,7 +5523,8 @@
+ 	} else {
+ 		intel_encoder->pre_enable = g4x_pre_enable_dp;
+ 		intel_encoder->enable = g4x_enable_dp;
+-		intel_encoder->post_disable = g4x_post_disable_dp;
++		if (INTEL_INFO(dev)->gen >= 5)
++			intel_encoder->post_disable = ilk_post_disable_dp;
+ 	}
+ 
+ 	intel_dig_port->port = port;
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_dp_mst.c b/drivers/gpu/drm/i915/intel_dp_mst.c
+--- a/drivers/gpu/drm/i915/intel_dp_mst.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_dp_mst.c	2014-11-20 09:53:37.984762837 -0700
+@@ -278,7 +278,7 @@
+ }
+ 
+ static enum drm_connector_status
+-intel_mst_port_dp_detect(struct drm_connector *connector)
++intel_dp_mst_detect(struct drm_connector *connector, bool force)
+ {
+ 	struct intel_connector *intel_connector = to_intel_connector(connector);
+ 	struct intel_dp *intel_dp = intel_connector->mst_port;
+@@ -286,14 +286,6 @@
+ 	return drm_dp_mst_detect_port(&intel_dp->mst_mgr, intel_connector->port);
+ }
+ 
+-static enum drm_connector_status
+-intel_dp_mst_detect(struct drm_connector *connector, bool force)
+-{
+-	enum drm_connector_status status;
+-	status = intel_mst_port_dp_detect(connector);
+-	return status;
+-}
+-
+ static int
+ intel_dp_mst_set_property(struct drm_connector *connector,
+ 			  struct drm_property *property,
+@@ -393,7 +385,7 @@
+ #endif
+ }
+ 
+-static struct drm_connector *intel_dp_add_mst_connector(struct drm_dp_mst_topology_mgr *mgr, struct drm_dp_mst_port *port, char *pathprop)
++static struct drm_connector *intel_dp_add_mst_connector(struct drm_dp_mst_topology_mgr *mgr, struct drm_dp_mst_port *port, const char *pathprop)
+ {
+ 	struct intel_dp *intel_dp = container_of(mgr, struct intel_dp, mst_mgr);
+ 	struct intel_digital_port *intel_dig_port = dp_to_dig_port(intel_dp);
+@@ -436,6 +428,7 @@
+ {
+ 	struct intel_connector *intel_connector = to_intel_connector(connector);
+ 	struct drm_device *dev = connector->dev;
++
+ 	/* need to nuke the connector */
+ 	mutex_lock(&dev->mode_config.mutex);
+ 	intel_connector_dpms(connector, DRM_MODE_DPMS_OFF);
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_drv.h b/drivers/gpu/drm/i915/intel_drv.h
+--- a/drivers/gpu/drm/i915/intel_drv.h	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_drv.h	2014-11-20 09:53:37.984762837 -0700
+@@ -25,6 +25,7 @@
+ #ifndef __INTEL_DRV_H__
+ #define __INTEL_DRV_H__
+ 
++#include <linux/async.h>
+ #include <linux/i2c.h>
+ #include <linux/hdmi.h>
+ #include <drm/i915_drm.h>
+@@ -33,6 +34,10 @@
+ #include <drm/drm_crtc_helper.h>
+ #include <drm/drm_fb_helper.h>
+ #include <drm/drm_dp_mst_helper.h>
++#include <drm/drm_rect.h>
++
++#define DIV_ROUND_CLOSEST_ULL(ll, d)	\
++({ unsigned long long _tmp = (ll)+(d)/2; do_div(_tmp, d); _tmp; })
+ 
+ /**
+  * _wait_for - magic (register) wait macro
+@@ -89,18 +94,20 @@
+ 
+ /* these are outputs from the chip - integrated only
+    external chips are via DVO or SDVO output */
+-#define INTEL_OUTPUT_UNUSED 0
+-#define INTEL_OUTPUT_ANALOG 1
+-#define INTEL_OUTPUT_DVO 2
+-#define INTEL_OUTPUT_SDVO 3
+-#define INTEL_OUTPUT_LVDS 4
+-#define INTEL_OUTPUT_TVOUT 5
+-#define INTEL_OUTPUT_HDMI 6
+-#define INTEL_OUTPUT_DISPLAYPORT 7
+-#define INTEL_OUTPUT_EDP 8
+-#define INTEL_OUTPUT_DSI 9
+-#define INTEL_OUTPUT_UNKNOWN 10
+-#define INTEL_OUTPUT_DP_MST 11
++enum intel_output_type {
++	INTEL_OUTPUT_UNUSED = 0,
++	INTEL_OUTPUT_ANALOG = 1,
++	INTEL_OUTPUT_DVO = 2,
++	INTEL_OUTPUT_SDVO = 3,
++	INTEL_OUTPUT_LVDS = 4,
++	INTEL_OUTPUT_TVOUT = 5,
++	INTEL_OUTPUT_HDMI = 6,
++	INTEL_OUTPUT_DISPLAYPORT = 7,
++	INTEL_OUTPUT_EDP = 8,
++	INTEL_OUTPUT_DSI = 9,
++	INTEL_OUTPUT_UNKNOWN = 10,
++	INTEL_OUTPUT_DP_MST = 11,
++};
+ 
+ #define INTEL_DVO_CHIP_NONE 0
+ #define INTEL_DVO_CHIP_LVDS 1
+@@ -131,7 +138,7 @@
+ 	 */
+ 	struct intel_crtc *new_crtc;
+ 
+-	int type;
++	enum intel_output_type type;
+ 	unsigned int cloneable;
+ 	bool connectors_active;
+ 	void (*hot_plug)(struct intel_encoder *);
+@@ -179,6 +186,8 @@
+ 		bool active_low_pwm;
+ 		struct backlight_device *device;
+ 	} backlight;
++
++	void (*backlight_power)(struct intel_connector *, bool enable);
+ };
+ 
+ struct intel_connector {
+@@ -211,6 +220,7 @@
+ 
+ 	/* Cached EDID for eDP and LVDS. May hold ERR_PTR for invalid EDID. */
+ 	struct edid *edid;
++	struct edid *detect_edid;
+ 
+ 	/* since POLL and HPD connectors may use the same HPD line keep the native
+ 	   state of connector->polled in case hotplug storm detection changes it */
+@@ -233,6 +243,17 @@
+ 	int	p;
+ } intel_clock_t;
+ 
++struct intel_plane_state {
++	struct drm_crtc *crtc;
++	struct drm_framebuffer *fb;
++	struct drm_rect src;
++	struct drm_rect dst;
++	struct drm_rect clip;
++	struct drm_rect orig_src;
++	struct drm_rect orig_dst;
++	bool visible;
++};
++
+ struct intel_plane_config {
+ 	bool tiled;
+ 	int size;
+@@ -271,6 +292,9 @@
+ 	 * between pch encoders and cpu encoders. */
+ 	bool has_pch_encoder;
+ 
++	/* Are we sending infoframes on the attached port */
++	bool has_infoframe;
++
+ 	/* CPU Transcoder for the pipe. Currently this can only differ from the
+ 	 * pipe on Haswell (where we have a special eDP transcoder). */
+ 	enum transcoder cpu_transcoder;
+@@ -319,7 +343,10 @@
+ 	/* Selected dpll when shared or DPLL_ID_PRIVATE. */
+ 	enum intel_dpll_id shared_dpll;
+ 
+-	/* PORT_CLK_SEL for DDI ports. */
++	/*
++	 * - PORT_CLK_SEL for DDI ports on HSW/BDW.
++	 * - enum skl_dpll on SKL
++	 */
+ 	uint32_t ddi_pll_sel;
+ 
+ 	/* Actual register state of the dpll, for shared dpll cross-checking. */
+@@ -330,6 +357,7 @@
+ 
+ 	/* m2_n2 for eDP downclock */
+ 	struct intel_link_m_n dp_m2_n2;
++	bool has_drrs;
+ 
+ 	/*
+ 	 * Frequence the dpll for the port should run at. Differs from the
+@@ -377,9 +405,10 @@
+ 	bool sprites_scaled;
+ };
+ 
+-struct intel_mmio_flip {
+-	u32 seqno;
+-	u32 ring_id;
++struct skl_pipe_wm {
++	struct skl_wm_level wm[8];
++	struct skl_wm_level trans_wm;
++	uint32_t linetime;
+ };
+ 
+ struct intel_crtc {
+@@ -410,6 +439,7 @@
+ 	uint32_t cursor_addr;
+ 	int16_t cursor_width, cursor_height;
+ 	uint32_t cursor_cntl;
++	uint32_t cursor_size;
+ 	uint32_t cursor_base;
+ 
+ 	struct intel_plane_config plane_config;
+@@ -428,12 +458,12 @@
+ 	struct {
+ 		/* watermarks currently being used  */
+ 		struct intel_pipe_wm active;
++		/* SKL wm values currently in use */
++		struct skl_pipe_wm skl_active;
+ 	} wm;
+ 
+-	wait_queue_head_t vbl_wait;
+-
+ 	int scanline_offset;
+-	struct intel_mmio_flip mmio_flip;
++	struct i915_gem_request *mmio_flip;
+ };
+ 
+ struct intel_plane_wm_parameters {
+@@ -455,6 +485,7 @@
+ 	unsigned int crtc_w, crtc_h;
+ 	uint32_t src_x, src_y;
+ 	uint32_t src_w, src_h;
++	unsigned int rotation;
+ 
+ 	/* Since we need to change the watermarks before/after
+ 	 * enabling/disabling the planes, we need to store the parameters here
+@@ -521,6 +552,7 @@
+ 	void (*set_infoframes)(struct drm_encoder *encoder,
+ 			       bool enable,
+ 			       struct drm_display_mode *adjusted_mode);
++	bool (*infoframe_enabled)(struct drm_encoder *encoder);
+ };
+ 
+ struct intel_dp_mst_encoder;
+@@ -565,6 +597,13 @@
+ 
+ 	struct notifier_block edp_notifier;
+ 
++	/*
++	 * Pipe whose power sequencer is currently locked into
++	 * this port. Only relevant on VLV/CHV.
++	 */
++	enum pipe pps_pipe;
++	struct edp_power_seq pps_delays;
++
+ 	bool use_tps3;
+ 	bool can_mst; /* this port supports mst */
+ 	bool is_mst;
+@@ -663,7 +702,12 @@
+ #define INTEL_FLIP_COMPLETE	2
+ 	u32 flip_count;
+ 	u32 gtt_offset;
+-	bool enable_stall_check;
++	struct i915_gem_request *flip_queued_request;
++	int flip_queued_vblank;
++	int flip_ready_vblank;
++	bool enable_stall_check:1;
++	bool rcs_active:1;
++	bool async:1;
+ };
+ 
+ struct intel_set_config {
+@@ -716,32 +760,46 @@
+ 	return container_of(intel_hdmi, struct intel_digital_port, hdmi);
+ }
+ 
++/*
++ * Returns the number of planes for this pipe, ie the number of sprites + 1
++ * (primary plane). This doesn't count the cursor plane then.
++ */
++static inline unsigned int intel_num_planes(struct intel_crtc *crtc)
++{
++	return INTEL_INFO(crtc->base.dev)->num_sprites[crtc->pipe] + 1;
++}
+ 
+-/* i915_irq.c */
+-bool intel_set_cpu_fifo_underrun_reporting(struct drm_device *dev,
++/* intel_fifo_underrun.c */
++bool intel_set_cpu_fifo_underrun_reporting(struct drm_i915_private *dev_priv,
+ 					   enum pipe pipe, bool enable);
+-bool intel_set_pch_fifo_underrun_reporting(struct drm_device *dev,
++bool intel_set_pch_fifo_underrun_reporting(struct drm_i915_private *dev_priv,
+ 					   enum transcoder pch_transcoder,
+ 					   bool enable);
++void intel_cpu_fifo_underrun_irq_handler(struct drm_i915_private *dev_priv,
++					 enum pipe pipe);
++void intel_pch_fifo_underrun_irq_handler(struct drm_i915_private *dev_priv,
++					 enum transcoder pch_transcoder);
++void i9xx_check_fifo_underruns(struct drm_i915_private *dev_priv);
++
++/* i915_irq.c */
+ void gen5_enable_gt_irq(struct drm_i915_private *dev_priv, uint32_t mask);
+ void gen5_disable_gt_irq(struct drm_i915_private *dev_priv, uint32_t mask);
+ void gen6_enable_pm_irq(struct drm_i915_private *dev_priv, uint32_t mask);
+ void gen6_disable_pm_irq(struct drm_i915_private *dev_priv, uint32_t mask);
+-void gen8_enable_pm_irq(struct drm_i915_private *dev_priv, uint32_t mask);
+-void gen8_disable_pm_irq(struct drm_i915_private *dev_priv, uint32_t mask);
+-void intel_runtime_pm_disable_interrupts(struct drm_device *dev);
+-void intel_runtime_pm_restore_interrupts(struct drm_device *dev);
++void gen6_enable_rps_interrupts(struct drm_device *dev);
++void gen6_disable_rps_interrupts(struct drm_device *dev);
++void intel_runtime_pm_disable_interrupts(struct drm_i915_private *dev_priv);
++void intel_runtime_pm_enable_interrupts(struct drm_i915_private *dev_priv);
+ static inline bool intel_irqs_enabled(struct drm_i915_private *dev_priv)
+ {
+ 	/*
+ 	 * We only use drm_irq_uninstall() at unload and VT switch, so
+ 	 * this is the only thing we need to check.
+ 	 */
+-	return !dev_priv->pm._irqs_disabled;
++	return dev_priv->pm.irqs_enabled;
+ }
+ 
+ int intel_get_crtc_scanline(struct intel_crtc *crtc);
+-void i9xx_check_fifo_underruns(struct drm_device *dev);
+ void gen8_irq_power_well_post_enable(struct drm_i915_private *dev_priv);
+ 
+ /* intel_crt.c */
+@@ -774,13 +832,9 @@
+ 			 struct intel_crtc_config *pipe_config);
+ void intel_ddi_set_vc_payload_alloc(struct drm_crtc *crtc, bool state);
+ 
+-/* intel_display.c */
+-const char *intel_output_name(int output);
+-bool intel_has_pending_fb_unpin(struct drm_device *dev);
+-int intel_pch_rawclk(struct drm_device *dev);
+-void intel_mark_busy(struct drm_device *dev);
++/* intel_frontbuffer.c */
+ void intel_fb_obj_invalidate(struct drm_i915_gem_object *obj,
+-			     struct intel_engine_cs *ring);
++			     struct i915_gem_request *rq);
+ void intel_frontbuffer_flip_prepare(struct drm_device *dev,
+ 				    unsigned frontbuffer_bits);
+ void intel_frontbuffer_flip_complete(struct drm_device *dev,
+@@ -788,7 +842,7 @@
+ void intel_frontbuffer_flush(struct drm_device *dev,
+ 			     unsigned frontbuffer_bits);
+ /**
+- * intel_frontbuffer_flip - prepare frontbuffer flip
++ * intel_frontbuffer_flip - synchronous frontbuffer flip
+  * @dev: DRM device
+  * @frontbuffer_bits: frontbuffer plane tracking bits
+  *
+@@ -806,6 +860,18 @@
+ }
+ 
+ void intel_fb_obj_flush(struct drm_i915_gem_object *obj, bool retire);
++
++
++/* intel_audio.c */
++void intel_init_audio(struct drm_device *dev);
++void intel_audio_codec_enable(struct intel_encoder *encoder);
++void intel_audio_codec_disable(struct intel_encoder *encoder);
++
++/* intel_display.c */
++const char *intel_output_name(int output);
++bool intel_has_pending_fb_unpin(struct drm_device *dev);
++int intel_pch_rawclk(struct drm_device *dev);
++void intel_mark_busy(struct drm_device *dev);
+ void intel_mark_idle(struct drm_device *dev);
+ void intel_crtc_restore_mode(struct drm_crtc *crtc);
+ void intel_crtc_control(struct drm_crtc *crtc, bool enable);
+@@ -826,8 +892,12 @@
+ 				struct drm_file *file_priv);
+ enum transcoder intel_pipe_to_cpu_transcoder(struct drm_i915_private *dev_priv,
+ 					     enum pipe pipe);
+-void intel_wait_for_vblank(struct drm_device *dev, int pipe);
+-void intel_wait_for_pipe_off(struct drm_device *dev, int pipe);
++bool intel_pipe_has_type(struct intel_crtc *crtc, enum intel_output_type type);
++static inline void
++intel_wait_for_vblank(struct drm_device *dev, int pipe)
++{
++	drm_wait_one_vblank(dev, pipe);
++}
+ int ironlake_get_lanes_required(int target_clock, int link_bw, int bpp);
+ void vlv_wait_port_ready(struct drm_i915_private *dev_priv,
+ 			 struct intel_digital_port *dport);
+@@ -837,9 +907,9 @@
+ 				struct drm_modeset_acquire_ctx *ctx);
+ void intel_release_load_detect_pipe(struct drm_connector *connector,
+ 				    struct intel_load_detect_pipe *old);
+-int intel_pin_and_fence_fb_obj(struct drm_device *dev,
+-			       struct drm_i915_gem_object *obj,
+-			       struct intel_engine_cs *pipelined);
++int intel_pin_and_fence_fb_obj(struct drm_plane *plane,
++			       struct drm_framebuffer *fb,
++			       struct i915_gem_request *pipelined);
+ void intel_unpin_fb_obj(struct drm_i915_gem_object *obj);
+ struct drm_framebuffer *
+ __intel_framebuffer_create(struct drm_device *dev,
+@@ -848,6 +918,7 @@
+ void intel_prepare_page_flip(struct drm_device *dev, int plane);
+ void intel_finish_page_flip(struct drm_device *dev, int pipe);
+ void intel_finish_page_flip_plane(struct drm_device *dev, int plane);
++void intel_check_page_flip(struct drm_device *dev, int pipe);
+ 
+ /* shared dpll functions */
+ struct intel_shared_dpll *intel_crtc_to_shared_dpll(struct intel_crtc *crtc);
+@@ -859,7 +930,13 @@
+ struct intel_shared_dpll *intel_get_shared_dpll(struct intel_crtc *crtc);
+ void intel_put_shared_dpll(struct intel_crtc *crtc);
+ 
++void vlv_force_pll_on(struct drm_device *dev, enum pipe pipe,
++		      const struct dpll *dpll);
++void vlv_force_pll_off(struct drm_device *dev, enum pipe pipe);
++
+ /* modesetting asserts */
++void assert_panel_unlocked(struct drm_i915_private *dev_priv,
++			   enum pipe pipe);
+ void assert_pll(struct drm_i915_private *dev_priv,
+ 		enum pipe pipe, bool state);
+ #define assert_pll_enabled(d, p) assert_pll(d, p, true)
+@@ -868,11 +945,9 @@
+ 		       enum pipe pipe, bool state);
+ #define assert_fdi_rx_pll_enabled(d, p) assert_fdi_rx_pll(d, p, true)
+ #define assert_fdi_rx_pll_disabled(d, p) assert_fdi_rx_pll(d, p, false)
+-void assert_pipe(struct drm_i915_private *dev_priv, enum pipe pipe, bool state);
++bool assert_pipe(struct drm_i915_private *dev_priv, enum pipe pipe, bool state);
+ #define assert_pipe_enabled(d, p) assert_pipe(d, p, true)
+ #define assert_pipe_disabled(d, p) assert_pipe(d, p, false)
+-void intel_write_eld(struct drm_encoder *encoder,
+-		     struct drm_display_mode *mode);
+ unsigned long intel_gen4_compute_page_offset(int *x, int *y,
+ 					     unsigned int tiling_mode,
+ 					     unsigned int bpp,
+@@ -882,6 +957,7 @@
+ void hsw_disable_pc8(struct drm_i915_private *dev_priv);
+ void intel_dp_get_m_n(struct intel_crtc *crtc,
+ 		      struct intel_crtc_config *pipe_config);
++void intel_dp_set_m_n(struct intel_crtc *crtc);
+ int intel_dotclock_calculate(int link_freq, const struct intel_link_m_n *m_n);
+ void
+ ironlake_check_encoder_dotclock(const struct intel_crtc_config *pipe_config,
+@@ -889,14 +965,13 @@
+ bool intel_crtc_active(struct drm_crtc *crtc);
+ void hsw_enable_ips(struct intel_crtc *crtc);
+ void hsw_disable_ips(struct intel_crtc *crtc);
+-void intel_display_set_init_power(struct drm_i915_private *dev, bool enable);
+ enum intel_display_power_domain
+ intel_display_port_power_domain(struct intel_encoder *intel_encoder);
+ void intel_mode_from_pipe_config(struct drm_display_mode *mode,
+ 				 struct intel_crtc_config *pipe_config);
+ int intel_format_to_fourcc(int format);
+ void intel_crtc_wait_for_pending_flips(struct drm_crtc *crtc);
+-
++void intel_modeset_preclose(struct drm_device *dev, struct drm_file *file);
+ 
+ /* intel_dp.c */
+ void intel_dp_init(struct drm_device *dev, int output_reg, enum port port);
+@@ -917,7 +992,6 @@
+ void intel_edp_backlight_on(struct intel_dp *intel_dp);
+ void intel_edp_backlight_off(struct intel_dp *intel_dp);
+ void intel_edp_panel_vdd_on(struct intel_dp *intel_dp);
+-void intel_edp_panel_vdd_sanitize(struct intel_encoder *intel_encoder);
+ void intel_edp_panel_on(struct intel_dp *intel_dp);
+ void intel_edp_panel_off(struct intel_dp *intel_dp);
+ void intel_edp_psr_enable(struct intel_dp *intel_dp);
+@@ -929,12 +1003,12 @@
+ 			 unsigned frontbuffer_bits);
+ void intel_edp_psr_init(struct drm_device *dev);
+ 
+-int intel_dp_handle_hpd_irq(struct intel_digital_port *digport, bool long_hpd);
+ void intel_dp_add_properties(struct intel_dp *intel_dp, struct drm_connector *connector);
+ void intel_dp_mst_suspend(struct drm_device *dev);
+ void intel_dp_mst_resume(struct drm_device *dev);
+ int intel_dp_max_link_bw(struct intel_dp *intel_dp);
+ void intel_dp_hot_plug(struct intel_encoder *intel_encoder);
++void vlv_power_sequencer_reset(struct drm_i915_private *dev_priv);
+ /* intel_dp_mst.c */
+ int intel_dp_mst_encoder_init(struct intel_digital_port *intel_dig_port, int conn_id);
+ void intel_dp_mst_encoder_cleanup(struct intel_digital_port *intel_dig_port);
+@@ -949,9 +1023,9 @@
+ /* legacy fbdev emulation in intel_fbdev.c */
+ #ifdef CONFIG_DRM_I915_FBDEV
+ extern int intel_fbdev_init(struct drm_device *dev);
+-extern void intel_fbdev_initial_config(struct drm_device *dev);
++extern void intel_fbdev_initial_config(void *data, async_cookie_t cookie);
+ extern void intel_fbdev_fini(struct drm_device *dev);
+-extern void intel_fbdev_set_suspend(struct drm_device *dev, int state);
++extern void intel_fbdev_set_suspend(struct drm_device *dev, int state, bool synchronous);
+ extern void intel_fbdev_output_poll_changed(struct drm_device *dev);
+ extern void intel_fbdev_restore_mode(struct drm_device *dev);
+ #else
+@@ -960,7 +1034,7 @@
+ 	return 0;
+ }
+ 
+-static inline void intel_fbdev_initial_config(struct drm_device *dev)
++static inline void intel_fbdev_initial_config(void *data, async_cookie_t cookie)
+ {
+ }
+ 
+@@ -968,7 +1042,7 @@
+ {
+ }
+ 
+-static inline void intel_fbdev_set_suspend(struct drm_device *dev, int state)
++static inline void intel_fbdev_set_suspend(struct drm_device *dev, int state, bool synchronous)
+ {
+ }
+ 
+@@ -997,6 +1071,7 @@
+ int intel_ddc_get_modes(struct drm_connector *c, struct i2c_adapter *adapter);
+ void intel_attach_force_audio_property(struct drm_connector *connector);
+ void intel_attach_broadcast_rgb_property(struct drm_connector *connector);
++void intel_attach_psr_property(struct drm_connector *connector);
+ 
+ 
+ /* intel_overlay.c */
+@@ -1024,7 +1099,7 @@
+ 			      int fitting_mode);
+ void intel_panel_set_backlight_acpi(struct intel_connector *connector,
+ 				    u32 level, u32 max);
+-int intel_panel_setup_backlight(struct drm_connector *connector);
++int intel_panel_setup_backlight(struct drm_connector *connector, enum pipe pipe);
+ void intel_panel_enable_backlight(struct intel_connector *connector);
+ void intel_panel_disable_backlight(struct intel_connector *connector);
+ void intel_panel_destroy_backlight(struct drm_connector *connector);
+@@ -1034,6 +1109,31 @@
+ 				struct drm_device *dev,
+ 				struct drm_display_mode *fixed_mode,
+ 				struct drm_connector *connector);
++void intel_backlight_register(struct drm_device *dev);
++void intel_backlight_unregister(struct drm_device *dev);
++
++
++/* intel_runtime_pm.c */
++int intel_power_domains_init(struct drm_i915_private *);
++void intel_power_domains_fini(struct drm_i915_private *);
++void intel_power_domains_init_hw(struct drm_i915_private *dev_priv);
++void intel_runtime_pm_enable(struct drm_i915_private *dev_priv);
++
++bool intel_display_power_is_enabled(struct drm_i915_private *dev_priv,
++				    enum intel_display_power_domain domain);
++bool __intel_display_power_is_enabled(struct drm_i915_private *dev_priv,
++				      enum intel_display_power_domain domain);
++void intel_display_power_get(struct drm_i915_private *dev_priv,
++			     enum intel_display_power_domain domain);
++void intel_display_power_put(struct drm_i915_private *dev_priv,
++			     enum intel_display_power_domain domain);
++void intel_aux_display_runtime_get(struct drm_i915_private *dev_priv);
++void intel_aux_display_runtime_put(struct drm_i915_private *dev_priv);
++void intel_runtime_pm_get(struct drm_i915_private *dev_priv);
++void intel_runtime_pm_get_noresume(struct drm_i915_private *dev_priv);
++void intel_runtime_pm_put(struct drm_i915_private *dev_priv);
++
++void intel_display_set_init_power(struct drm_i915_private *dev, bool enable);
+ 
+ /* intel_pm.c */
+ void intel_init_clock_gating(struct drm_device *dev);
+@@ -1052,17 +1152,6 @@
+ void intel_update_fbc(struct drm_device *dev);
+ void intel_gpu_ips_init(struct drm_i915_private *dev_priv);
+ void intel_gpu_ips_teardown(void);
+-int intel_power_domains_init(struct drm_i915_private *);
+-void intel_power_domains_remove(struct drm_i915_private *);
+-bool intel_display_power_enabled(struct drm_i915_private *dev_priv,
+-				 enum intel_display_power_domain domain);
+-bool intel_display_power_enabled_unlocked(struct drm_i915_private *dev_priv,
+-					  enum intel_display_power_domain domain);
+-void intel_display_power_get(struct drm_i915_private *dev_priv,
+-			     enum intel_display_power_domain domain);
+-void intel_display_power_put(struct drm_i915_private *dev_priv,
+-			     enum intel_display_power_domain domain);
+-void intel_power_domains_init_hw(struct drm_i915_private *dev_priv);
+ void intel_init_gt_powersave(struct drm_device *dev);
+ void intel_cleanup_gt_powersave(struct drm_device *dev);
+ void intel_enable_gt_powersave(struct drm_device *dev);
+@@ -1071,16 +1160,17 @@
+ void intel_reset_gt_powersave(struct drm_device *dev);
+ void ironlake_teardown_rc6(struct drm_device *dev);
+ void gen6_update_ring_freq(struct drm_device *dev);
++void gen6_rps_busy(struct drm_i915_private *dev_priv);
++void gen6_rps_reset_ei(struct drm_i915_private *dev_priv);
+ void gen6_rps_idle(struct drm_i915_private *dev_priv);
+-void gen6_rps_boost(struct drm_i915_private *dev_priv);
+-void intel_aux_display_runtime_get(struct drm_i915_private *dev_priv);
+-void intel_aux_display_runtime_put(struct drm_i915_private *dev_priv);
+-void intel_runtime_pm_get(struct drm_i915_private *dev_priv);
+-void intel_runtime_pm_get_noresume(struct drm_i915_private *dev_priv);
+-void intel_runtime_pm_put(struct drm_i915_private *dev_priv);
+-void intel_init_runtime_pm(struct drm_i915_private *dev_priv);
+-void intel_fini_runtime_pm(struct drm_i915_private *dev_priv);
++void gen6_rps_boost(struct drm_i915_private *dev_priv,
++		    struct drm_i915_file_private *file_priv);
++void intel_queue_rps_boost_for_request(struct drm_device *dev,
++				       struct i915_gem_request *rq);
+ void ilk_wm_get_hw_state(struct drm_device *dev);
++void skl_wm_get_hw_state(struct drm_device *dev);
++void skl_ddb_get_hw_state(struct drm_i915_private *dev_priv,
++			  struct skl_ddb_allocation *ddb /* out */);
+ 
+ 
+ /* intel_sdvo.c */
+@@ -1091,13 +1181,18 @@
+ int intel_plane_init(struct drm_device *dev, enum pipe pipe, int plane);
+ void intel_flush_primary_plane(struct drm_i915_private *dev_priv,
+ 			       enum plane plane);
+-void intel_plane_restore(struct drm_plane *plane);
++int intel_plane_set_property(struct drm_plane *plane,
++			     struct drm_property *prop,
++			     uint64_t val);
++int intel_plane_restore(struct drm_plane *plane);
+ void intel_plane_disable(struct drm_plane *plane);
+ int intel_sprite_set_colorkey(struct drm_device *dev, void *data,
+ 			      struct drm_file *file_priv);
+ int intel_sprite_get_colorkey(struct drm_device *dev, void *data,
+ 			      struct drm_file *file_priv);
+-
++bool intel_pipe_update_start(struct intel_crtc *crtc,
++			     uint32_t *start_vbl_count);
++void intel_pipe_update_end(struct intel_crtc *crtc, u32 start_vbl_count);
+ 
+ /* intel_tv.c */
+ void intel_tv_init(struct drm_device *dev);
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_dsi.c b/drivers/gpu/drm/i915/intel_dsi.c
+--- a/drivers/gpu/drm/i915/intel_dsi.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_dsi.c	2014-11-20 09:53:37.984762837 -0700
+@@ -184,7 +184,7 @@
+ 
+ 	/* update the hw state for DPLL */
+ 	intel_crtc->config.dpll_hw_state.dpll = DPLL_INTEGRATED_CLOCK_VLV |
+-						DPLL_REFA_CLK_ENABLE_VLV;
++		DPLL_REFA_CLK_ENABLE_VLV;
+ 
+ 	tmp = I915_READ(DSPCLK_GATE_D);
+ 	tmp |= DPOUNIT_CLOCK_GATE_DISABLE;
+@@ -259,8 +259,8 @@
+ 	temp = I915_READ(MIPI_CTRL(pipe));
+ 	temp &= ~ESCAPE_CLOCK_DIVIDER_MASK;
+ 	I915_WRITE(MIPI_CTRL(pipe), temp |
+-			intel_dsi->escape_clk_div <<
+-			ESCAPE_CLOCK_DIVIDER_SHIFT);
++		   intel_dsi->escape_clk_div <<
++		   ESCAPE_CLOCK_DIVIDER_SHIFT);
+ 
+ 	I915_WRITE(MIPI_EOT_DISABLE(pipe), CLOCKSTOP);
+ 
+@@ -297,7 +297,7 @@
+ 	usleep_range(2000, 2500);
+ 
+ 	if (wait_for(((I915_READ(MIPI_PORT_CTRL(pipe)) & AFE_LATCHOUT)
+-					== 0x00000), 30))
++		      == 0x00000), 30))
+ 		DRM_ERROR("DSI LP not going Low\n");
+ 
+ 	val = I915_READ(MIPI_PORT_CTRL(pipe));
+@@ -344,7 +344,7 @@
+ 	DRM_DEBUG_KMS("\n");
+ 
+ 	power_domain = intel_display_port_power_domain(encoder);
+-	if (!intel_display_power_enabled(dev_priv, power_domain))
++	if (!intel_display_power_is_enabled(dev_priv, power_domain))
+ 		return false;
+ 
+ 	/* XXX: this only works for one DSI output */
+@@ -423,9 +423,11 @@
+ }
+ 
+ /* return pixels in terms of txbyteclkhs */
+-static u16 txbyteclkhs(u16 pixels, int bpp, int lane_count)
++static u16 txbyteclkhs(u16 pixels, int bpp, int lane_count,
++		       u16 burst_mode_ratio)
+ {
+-	return DIV_ROUND_UP(DIV_ROUND_UP(pixels * bpp, 8), lane_count);
++	return DIV_ROUND_UP(DIV_ROUND_UP(pixels * bpp * burst_mode_ratio,
++					 8 * 100), lane_count);
+ }
+ 
+ static void set_dsi_timings(struct drm_encoder *encoder,
+@@ -451,10 +453,12 @@
+ 	vbp = mode->vtotal - mode->vsync_end;
+ 
+ 	/* horizontal values are in terms of high speed byte clock */
+-	hactive = txbyteclkhs(hactive, bpp, lane_count);
+-	hfp = txbyteclkhs(hfp, bpp, lane_count);
+-	hsync = txbyteclkhs(hsync, bpp, lane_count);
+-	hbp = txbyteclkhs(hbp, bpp, lane_count);
++	hactive = txbyteclkhs(hactive, bpp, lane_count,
++			      intel_dsi->burst_mode_ratio);
++	hfp = txbyteclkhs(hfp, bpp, lane_count, intel_dsi->burst_mode_ratio);
++	hsync = txbyteclkhs(hsync, bpp, lane_count,
++			    intel_dsi->burst_mode_ratio);
++	hbp = txbyteclkhs(hbp, bpp, lane_count, intel_dsi->burst_mode_ratio);
+ 
+ 	I915_WRITE(MIPI_HACTIVE_AREA_COUNT(pipe), hactive);
+ 	I915_WRITE(MIPI_HFP_COUNT(pipe), hfp);
+@@ -541,12 +545,14 @@
+ 	    intel_dsi->video_mode_format == VIDEO_MODE_BURST) {
+ 		I915_WRITE(MIPI_HS_TX_TIMEOUT(pipe),
+ 			   txbyteclkhs(adjusted_mode->htotal, bpp,
+-				       intel_dsi->lane_count) + 1);
++				       intel_dsi->lane_count,
++				       intel_dsi->burst_mode_ratio) + 1);
+ 	} else {
+ 		I915_WRITE(MIPI_HS_TX_TIMEOUT(pipe),
+ 			   txbyteclkhs(adjusted_mode->vtotal *
+ 				       adjusted_mode->htotal,
+-				       bpp, intel_dsi->lane_count) + 1);
++				       bpp, intel_dsi->lane_count,
++				       intel_dsi->burst_mode_ratio) + 1);
+ 	}
+ 	I915_WRITE(MIPI_LP_RX_TIMEOUT(pipe), intel_dsi->lp_rx_timeout);
+ 	I915_WRITE(MIPI_TURN_AROUND_TIMEOUT(pipe), intel_dsi->turn_arnd_val);
+@@ -576,7 +582,7 @@
+ 	 * XXX: write MIPI_STOP_STATE_STALL?
+ 	 */
+ 	I915_WRITE(MIPI_HIGH_LOW_SWITCH_COUNT(pipe),
+-						intel_dsi->hs_to_lp_count);
++		   intel_dsi->hs_to_lp_count);
+ 
+ 	/* XXX: low power clock equivalence in terms of byte clock. the number
+ 	 * of byte clocks occupied in one low power clock. based on txbyteclkhs
+@@ -601,10 +607,10 @@
+ 		 * 64 like 1366 x 768. Enable RANDOM resolution support for such
+ 		 * panels by default */
+ 		I915_WRITE(MIPI_VIDEO_MODE_FORMAT(pipe),
+-				intel_dsi->video_frmt_cfg_bits |
+-				intel_dsi->video_mode_format |
+-				IP_TG_CONFIG |
+-				RANDOM_DPI_DISPLAY_RESOLUTION);
++			   intel_dsi->video_frmt_cfg_bits |
++			   intel_dsi->video_mode_format |
++			   IP_TG_CONFIG |
++			   RANDOM_DPI_DISPLAY_RESOLUTION);
+ }
+ 
+ static void intel_dsi_pre_pll_enable(struct intel_encoder *encoder)
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_dsi_cmd.c b/drivers/gpu/drm/i915/intel_dsi_cmd.c
+--- a/drivers/gpu/drm/i915/intel_dsi_cmd.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_dsi_cmd.c	2014-11-20 09:53:37.984762837 -0700
+@@ -430,7 +430,7 @@
+ 	u32 mask;
+ 
+ 	mask = LP_CTRL_FIFO_EMPTY | HS_CTRL_FIFO_EMPTY |
+-					LP_DATA_FIFO_EMPTY | HS_DATA_FIFO_EMPTY;
++		LP_DATA_FIFO_EMPTY | HS_DATA_FIFO_EMPTY;
+ 
+ 	if (wait_for((I915_READ(MIPI_GEN_FIFO_STAT(pipe)) & mask) == mask, 100))
+ 		DRM_ERROR("DPI FIFOs are not empty\n");
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_dsi.h b/drivers/gpu/drm/i915/intel_dsi.h
+--- a/drivers/gpu/drm/i915/intel_dsi.h	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_dsi.h	2014-11-20 09:53:37.984762837 -0700
+@@ -116,6 +116,8 @@
+ 	u16 clk_hs_to_lp_count;
+ 
+ 	u16 init_count;
++	u32 pclk;
++	u16 burst_mode_ratio;
+ 
+ 	/* all delays in ms */
+ 	u16 backlight_off_delay;
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_dsi_panel_vbt.c b/drivers/gpu/drm/i915/intel_dsi_panel_vbt.c
+--- a/drivers/gpu/drm/i915/intel_dsi_panel_vbt.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_dsi_panel_vbt.c	2014-11-20 09:53:37.988762837 -0700
+@@ -271,6 +271,8 @@
+ 	u32 ths_prepare_ns, tclk_trail_ns;
+ 	u32 tclk_prepare_clkzero, ths_prepare_hszero;
+ 	u32 lp_to_hs_switch, hs_to_lp_switch;
++	u32 pclk, computed_ddr;
++	u16 burst_mode_ratio;
+ 
+ 	DRM_DEBUG_KMS("\n");
+ 
+@@ -284,8 +286,6 @@
+ 	else if (intel_dsi->pixel_format == VID_MODE_FORMAT_RGB565)
+ 		bits_per_pixel = 16;
+ 
+-	bitrate = (mode->clock * bits_per_pixel) / intel_dsi->lane_count;
+-
+ 	intel_dsi->operation_mode = mipi_config->is_cmd_mode;
+ 	intel_dsi->video_mode_format = mipi_config->video_transfer_mode;
+ 	intel_dsi->escape_clk_div = mipi_config->byte_clk_sel;
+@@ -297,6 +297,40 @@
+ 	intel_dsi->video_frmt_cfg_bits =
+ 		mipi_config->bta_enabled ? DISABLE_VIDEO_BTA : 0;
+ 
++	pclk = mode->clock;
++
++	/* Burst Mode Ratio
++	 * Target ddr frequency from VBT / non burst ddr freq
++	 * multiply by 100 to preserve remainder
++	 */
++	if (intel_dsi->video_mode_format == VIDEO_MODE_BURST) {
++		if (mipi_config->target_burst_mode_freq) {
++			computed_ddr =
++				(pclk * bits_per_pixel) / intel_dsi->lane_count;
++
++			if (mipi_config->target_burst_mode_freq <
++								computed_ddr) {
++				DRM_ERROR("Burst mode freq is less than computed\n");
++				return false;
++			}
++
++			burst_mode_ratio = DIV_ROUND_UP(
++				mipi_config->target_burst_mode_freq * 100,
++				computed_ddr);
++
++			pclk = DIV_ROUND_UP(pclk * burst_mode_ratio, 100);
++		} else {
++			DRM_ERROR("Burst mode target is not set\n");
++			return false;
++		}
++	} else
++		burst_mode_ratio = 100;
++
++	intel_dsi->burst_mode_ratio = burst_mode_ratio;
++	intel_dsi->pclk = pclk;
++
++	bitrate = (pclk * bits_per_pixel) / intel_dsi->lane_count;
++
+ 	switch (intel_dsi->escape_clk_div) {
+ 	case 0:
+ 		tlpx_ns = 50;
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_dsi_pll.c b/drivers/gpu/drm/i915/intel_dsi_pll.c
+--- a/drivers/gpu/drm/i915/intel_dsi_pll.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_dsi_pll.c	2014-11-20 09:53:37.988762837 -0700
+@@ -134,8 +134,7 @@
+ #else
+ 
+ /* Get DSI clock from pixel clock */
+-static u32 dsi_clk_from_pclk(const struct drm_display_mode *mode,
+-			  int pixel_format, int lane_count)
++static u32 dsi_clk_from_pclk(u32 pclk, int pixel_format, int lane_count)
+ {
+ 	u32 dsi_clk_khz;
+ 	u32 bpp;
+@@ -156,7 +155,7 @@
+ 
+ 	/* DSI data rate = pixel clock * bits per pixel / lane count
+ 	   pixel clock is converted from KHz to Hz */
+-	dsi_clk_khz = DIV_ROUND_CLOSEST(mode->clock * bpp, lane_count);
++	dsi_clk_khz = DIV_ROUND_CLOSEST(pclk * bpp, lane_count);
+ 
+ 	return dsi_clk_khz;
+ }
+@@ -191,7 +190,7 @@
+ 	for (m = 62; m <= 92; m++) {
+ 		for (p = 2; p <= 6; p++) {
+ 			/* Find the optimal m and p divisors
+-			with minimal error +/- the required clock */
++			   with minimal error +/- the required clock */
+ 			calc_dsi_clk = (m * ref_clk) / p;
+ 			if (calc_dsi_clk == target_dsi_clk) {
+ 				calc_m = m;
+@@ -228,15 +227,13 @@
+ static void vlv_configure_dsi_pll(struct intel_encoder *encoder)
+ {
+ 	struct drm_i915_private *dev_priv = encoder->base.dev->dev_private;
+-	struct intel_crtc *intel_crtc = to_intel_crtc(encoder->base.crtc);
+-	const struct drm_display_mode *mode = &intel_crtc->config.adjusted_mode;
+ 	struct intel_dsi *intel_dsi = enc_to_intel_dsi(&encoder->base);
+ 	int ret;
+ 	struct dsi_mnp dsi_mnp;
+ 	u32 dsi_clk;
+ 
+-	dsi_clk = dsi_clk_from_pclk(mode, intel_dsi->pixel_format,
+-						intel_dsi->lane_count);
++	dsi_clk = dsi_clk_from_pclk(intel_dsi->pclk, intel_dsi->pixel_format,
++				    intel_dsi->lane_count);
+ 
+ 	ret = dsi_calc_mnp(dsi_clk, &dsi_mnp);
+ 	if (ret) {
+@@ -318,8 +315,8 @@
+ 	}
+ 
+ 	WARN(bpp != pipe_bpp,
+-		"bpp match assertion failure (expected %d, current %d)\n",
+-		bpp, pipe_bpp);
++	     "bpp match assertion failure (expected %d, current %d)\n",
++	     bpp, pipe_bpp);
+ }
+ 
+ u32 vlv_get_dsi_pclk(struct intel_encoder *encoder, int pipe_bpp)
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_dvo.c b/drivers/gpu/drm/i915/intel_dvo.c
+--- a/drivers/gpu/drm/i915/intel_dvo.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_dvo.c	2014-11-20 09:53:37.988762837 -0700
+@@ -85,7 +85,7 @@
+ 	{
+ 	        .type = INTEL_DVO_CHIP_TMDS,
+ 		.name = "ns2501",
+-		.dvo_reg = DVOC,
++		.dvo_reg = DVOB,
+ 		.slave_addr = NS2501_ADDR,
+ 		.dev_ops = &ns2501_ops,
+        }
+@@ -185,12 +185,13 @@
+ 	u32 dvo_reg = intel_dvo->dev.dvo_reg;
+ 	u32 temp = I915_READ(dvo_reg);
+ 
+-	I915_WRITE(dvo_reg, temp | DVO_ENABLE);
+-	I915_READ(dvo_reg);
+ 	intel_dvo->dev.dev_ops->mode_set(&intel_dvo->dev,
+ 					 &crtc->config.requested_mode,
+ 					 &crtc->config.adjusted_mode);
+ 
++	I915_WRITE(dvo_reg, temp | DVO_ENABLE);
++	I915_READ(dvo_reg);
++
+ 	intel_dvo->dev.dev_ops->dpms(&intel_dvo->dev, true);
+ }
+ 
+@@ -226,10 +227,6 @@
+ 
+ 		intel_crtc_update_dpms(crtc);
+ 
+-		intel_dvo->dev.dev_ops->mode_set(&intel_dvo->dev,
+-						 &config->requested_mode,
+-						 &config->adjusted_mode);
+-
+ 		intel_dvo->dev.dev_ops->dpms(&intel_dvo->dev, true);
+ 	} else {
+ 		intel_dvo->dev.dev_ops->dpms(&intel_dvo->dev, false);
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_fbdev.c b/drivers/gpu/drm/i915/intel_fbdev.c
+--- a/drivers/gpu/drm/i915/intel_fbdev.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_fbdev.c	2014-11-20 09:53:37.988762837 -0700
+@@ -24,8 +24,10 @@
+  *     David Airlie
+  */
+ 
++#include <linux/async.h>
+ #include <linux/module.h>
+ #include <linux/kernel.h>
++#include <linux/console.h>
+ #include <linux/errno.h>
+ #include <linux/string.h>
+ #include <linux/mm.h>
+@@ -117,25 +119,25 @@
+ 		goto out;
+ 	}
+ 
+-	/* Flush everything out, we'll be doing GTT only from now on */
+-	ret = intel_pin_and_fence_fb_obj(dev, obj, NULL);
+-	if (ret) {
+-		DRM_ERROR("failed to pin obj: %d\n", ret);
+-		goto out_unref;
+-	}
+-
+ 	fb = __intel_framebuffer_create(dev, &mode_cmd, obj);
+ 	if (IS_ERR(fb)) {
+ 		ret = PTR_ERR(fb);
+-		goto out_unpin;
++		goto out_unref;
++	}
++
++	/* Flush everything out, we'll be doing GTT only from now on */
++	ret = intel_pin_and_fence_fb_obj(NULL, fb, NULL);
++	if (ret) {
++		DRM_ERROR("failed to pin obj: %d\n", ret);
++		goto out_fb;
+ 	}
+ 
+ 	ifbdev->fb = to_intel_framebuffer(fb);
+ 
+ 	return 0;
+ 
+-out_unpin:
+-	i915_gem_object_ggtt_unpin(obj);
++out_fb:
++	drm_framebuffer_remove(fb);
+ out_unref:
+ 	drm_gem_object_unreference(&obj->base);
+ out:
+@@ -331,24 +333,6 @@
+ 	int num_connectors_enabled = 0;
+ 	int num_connectors_detected = 0;
+ 
+-	/*
+-	 * If the user specified any force options, just bail here
+-	 * and use that config.
+-	 */
+-	for (i = 0; i < fb_helper->connector_count; i++) {
+-		struct drm_fb_helper_connector *fb_conn;
+-		struct drm_connector *connector;
+-
+-		fb_conn = fb_helper->connector_info[i];
+-		connector = fb_conn->connector;
+-
+-		if (!enabled[i])
+-			continue;
+-
+-		if (connector->force != DRM_FORCE_UNSPECIFIED)
+-			return false;
+-	}
+-
+ 	save_enabled = kcalloc(dev->mode_config.num_connector, sizeof(bool),
+ 			       GFP_KERNEL);
+ 	if (!save_enabled)
+@@ -374,8 +358,18 @@
+ 			continue;
+ 		}
+ 
++		if (connector->force == DRM_FORCE_OFF) {
++			DRM_DEBUG_KMS("connector %s is disabled by user, skipping\n",
++				      connector->name);
++			enabled[i] = false;
++			continue;
++		}
++
+ 		encoder = connector->encoder;
+ 		if (!encoder || WARN_ON(!encoder->crtc)) {
++			if (connector->force > DRM_FORCE_OFF)
++				goto bail;
++
+ 			DRM_DEBUG_KMS("connector %s has no encoder or crtc, skipping\n",
+ 				      connector->name);
+ 			enabled[i] = false;
+@@ -394,8 +388,7 @@
+ 		for (j = 0; j < fb_helper->connector_count; j++) {
+ 			if (crtcs[j] == new_crtc) {
+ 				DRM_DEBUG_KMS("fallback: cloned configuration\n");
+-				fallback = true;
+-				goto out;
++				goto bail;
+ 			}
+ 		}
+ 
+@@ -466,8 +459,8 @@
+ 		fallback = true;
+ 	}
+ 
+-out:
+ 	if (fallback) {
++bail:
+ 		DRM_DEBUG_KMS("Not using firmware configuration\n");
+ 		memcpy(enabled, save_enabled, dev->mode_config.num_connector);
+ 		kfree(save_enabled);
+@@ -523,7 +516,7 @@
+ 	struct intel_plane_config *plane_config = NULL;
+ 	unsigned int max_size = 0;
+ 
+-	if (!i915.fastboot)
++	if (!i915_module.fastboot)
+ 		return false;
+ 
+ 	/* Find the largest fb */
+@@ -636,6 +629,15 @@
+ 	return false;
+ }
+ 
++static void intel_fbdev_suspend_worker(struct work_struct *work)
++{
++	intel_fbdev_set_suspend(container_of(work,
++					     struct drm_i915_private,
++					     fbdev_suspend_work)->dev,
++				FBINFO_STATE_RUNNING,
++				true);
++}
++
+ int intel_fbdev_init(struct drm_device *dev)
+ {
+ 	struct intel_fbdev *ifbdev;
+@@ -662,14 +664,16 @@
+ 	}
+ 
+ 	dev_priv->fbdev = ifbdev;
++	INIT_WORK(&dev_priv->fbdev_suspend_work, intel_fbdev_suspend_worker);
++
+ 	drm_fb_helper_single_add_all_connectors(&ifbdev->helper);
+ 
+ 	return 0;
+ }
+ 
+-void intel_fbdev_initial_config(struct drm_device *dev)
++void intel_fbdev_initial_config(void *data, async_cookie_t cookie)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_i915_private *dev_priv = data;
+ 	struct intel_fbdev *ifbdev = dev_priv->fbdev;
+ 
+ 	/* Due to peculiar init order wrt to hpd handling this is separate. */
+@@ -682,12 +686,15 @@
+ 	if (!dev_priv->fbdev)
+ 		return;
+ 
++	flush_work(&dev_priv->fbdev_suspend_work);
++
++	async_synchronize_full();
+ 	intel_fbdev_destroy(dev, dev_priv->fbdev);
+ 	kfree(dev_priv->fbdev);
+ 	dev_priv->fbdev = NULL;
+ }
+ 
+-void intel_fbdev_set_suspend(struct drm_device *dev, int state)
++void intel_fbdev_set_suspend(struct drm_device *dev, int state, bool synchronous)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct intel_fbdev *ifbdev = dev_priv->fbdev;
+@@ -698,6 +705,33 @@
+ 
+ 	info = ifbdev->helper.fbdev;
+ 
++	if (synchronous) {
++		/* Flush any pending work to turn the console on, and then
++		 * wait to turn it off. It must be synchronous as we are
++		 * about to suspend or unload the driver.
++		 *
++		 * Note that from within the work-handler, we cannot flush
++		 * ourselves, so only flush outstanding work upon suspend!
++		 */
++		if (state != FBINFO_STATE_RUNNING)
++			flush_work(&dev_priv->fbdev_suspend_work);
++		console_lock();
++	} else {
++		/*
++		 * The console lock can be pretty contented on resume due
++		 * to all the printk activity.  Try to keep it out of the hot
++		 * path of resume if possible.
++		 */
++		WARN_ON(state != FBINFO_STATE_RUNNING);
++		if (!console_trylock()) {
++			/* Don't block our own workqueue as this can
++			 * be run in parallel with other i915.ko tasks.
++			 */
++			schedule_work(&dev_priv->fbdev_suspend_work);
++			return;
++		}
++	}
++
+ 	/* On resume from hibernation: If the object is shmemfs backed, it has
+ 	 * been restored from swap. If the object is stolen however, it will be
+ 	 * full of whatever garbage was left in there.
+@@ -706,6 +740,7 @@
+ 		memset_io(info->screen_base, 0, info->screen_size);
+ 
+ 	fb_set_suspend(info, state);
++	console_unlock();
+ }
+ 
+ void intel_fbdev_output_poll_changed(struct drm_device *dev)
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_fifo_underrun.c b/drivers/gpu/drm/i915/intel_fifo_underrun.c
+--- a/drivers/gpu/drm/i915/intel_fifo_underrun.c	1969-12-31 17:00:00.000000000 -0700
++++ b/drivers/gpu/drm/i915/intel_fifo_underrun.c	2014-11-20 09:53:37.988762837 -0700
+@@ -0,0 +1,381 @@
++/*
++ * Copyright  2014 Intel Corporation
++ *
++ * Permission is hereby granted, free of charge, to any person obtaining a
++ * copy of this software and associated documentation files (the "Software"),
++ * to deal in the Software without restriction, including without limitation
++ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
++ * and/or sell copies of the Software, and to permit persons to whom the
++ * Software is furnished to do so, subject to the following conditions:
++ *
++ * The above copyright notice and this permission notice (including the next
++ * paragraph) shall be included in all copies or substantial portions of the
++ * Software.
++ *
++ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
++ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
++ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
++ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
++ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
++ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
++ * IN THE SOFTWARE.
++ *
++ * Authors:
++ *    Daniel Vetter <daniel.vetter@ffwll.ch>
++ *
++ */
++
++#include "i915_drv.h"
++#include "intel_drv.h"
++
++/**
++ * DOC: fifo underrun handling
++ *
++ * The i915 driver checks for display fifo underruns using the interrupt signals
++ * provided by the hardware. This is enabled by default and fairly useful to
++ * debug display issues, especially watermark settings.
++ *
++ * If an underrun is detected this is logged into dmesg. To avoid flooding logs
++ * and occupying the cpu underrun interrupts are disabled after the first
++ * occurrence until the next modeset on a given pipe.
++ *
++ * Note that underrun detection on gmch platforms is a bit more ugly since there
++ * is no interrupt (despite that the signalling bit is in the PIPESTAT pipe
++ * interrupt register). Also on some other platforms underrun interrupts are
++ * shared, which means that if we detect an underrun we need to disable underrun
++ * reporting on all pipes.
++ *
++ * The code also supports underrun detection on the PCH transcoder.
++ */
++
++static bool ivb_can_enable_err_int(struct drm_device *dev)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_crtc *crtc;
++	enum pipe pipe;
++
++	assert_spin_locked(&dev_priv->irq_lock);
++
++	for_each_pipe(dev_priv, pipe) {
++		crtc = to_intel_crtc(dev_priv->pipe_to_crtc_mapping[pipe]);
++
++		if (crtc->cpu_fifo_underrun_disabled)
++			return false;
++	}
++
++	return true;
++}
++
++static bool cpt_can_enable_serr_int(struct drm_device *dev)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	enum pipe pipe;
++	struct intel_crtc *crtc;
++
++	assert_spin_locked(&dev_priv->irq_lock);
++
++	for_each_pipe(dev_priv, pipe) {
++		crtc = to_intel_crtc(dev_priv->pipe_to_crtc_mapping[pipe]);
++
++		if (crtc->pch_fifo_underrun_disabled)
++			return false;
++	}
++
++	return true;
++}
++
++/**
++ * i9xx_check_fifo_underruns - check for fifo underruns
++ * @dev_priv: i915 device instance
++ *
++ * This function checks for fifo underruns on GMCH platforms. This needs to be
++ * done manually on modeset to make sure that we catch all underruns since they
++ * do not generate an interrupt by themselves on these platforms.
++ */
++void i9xx_check_fifo_underruns(struct drm_i915_private *dev_priv)
++{
++	struct intel_crtc *crtc;
++
++	spin_lock_irq(&dev_priv->irq_lock);
++
++	for_each_intel_crtc(dev_priv->dev, crtc) {
++		u32 reg = PIPESTAT(crtc->pipe);
++		u32 pipestat;
++
++		if (crtc->cpu_fifo_underrun_disabled)
++			continue;
++
++		pipestat = I915_READ(reg) & 0xffff0000;
++		if ((pipestat & PIPE_FIFO_UNDERRUN_STATUS) == 0)
++			continue;
++
++		I915_WRITE(reg, pipestat | PIPE_FIFO_UNDERRUN_STATUS);
++		POSTING_READ(reg);
++
++		DRM_ERROR("pipe %c underrun\n", pipe_name(crtc->pipe));
++	}
++
++	spin_unlock_irq(&dev_priv->irq_lock);
++}
++
++static void i9xx_set_fifo_underrun_reporting(struct drm_device *dev,
++					     enum pipe pipe,
++					     bool enable, bool old)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	u32 reg = PIPESTAT(pipe);
++	u32 pipestat = I915_READ(reg) & 0xffff0000;
++
++	assert_spin_locked(&dev_priv->irq_lock);
++
++	if (enable) {
++		I915_WRITE(reg, pipestat | PIPE_FIFO_UNDERRUN_STATUS);
++		POSTING_READ(reg);
++	} else {
++		if (old && pipestat & PIPE_FIFO_UNDERRUN_STATUS)
++			DRM_ERROR("pipe %c underrun\n", pipe_name(pipe));
++	}
++}
++
++static void ironlake_set_fifo_underrun_reporting(struct drm_device *dev,
++						 enum pipe pipe, bool enable)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	uint32_t bit = (pipe == PIPE_A) ? DE_PIPEA_FIFO_UNDERRUN :
++					  DE_PIPEB_FIFO_UNDERRUN;
++
++	if (enable)
++		ironlake_enable_display_irq(dev_priv, bit);
++	else
++		ironlake_disable_display_irq(dev_priv, bit);
++}
++
++static void ivybridge_set_fifo_underrun_reporting(struct drm_device *dev,
++						  enum pipe pipe,
++						  bool enable, bool old)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	if (enable) {
++		I915_WRITE(GEN7_ERR_INT, ERR_INT_FIFO_UNDERRUN(pipe));
++
++		if (!ivb_can_enable_err_int(dev))
++			return;
++
++		ironlake_enable_display_irq(dev_priv, DE_ERR_INT_IVB);
++	} else {
++		ironlake_disable_display_irq(dev_priv, DE_ERR_INT_IVB);
++
++		if (old &&
++		    I915_READ(GEN7_ERR_INT) & ERR_INT_FIFO_UNDERRUN(pipe)) {
++			DRM_ERROR("uncleared fifo underrun on pipe %c\n",
++				  pipe_name(pipe));
++		}
++	}
++}
++
++static void broadwell_set_fifo_underrun_reporting(struct drm_device *dev,
++						  enum pipe pipe, bool enable)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++
++	assert_spin_locked(&dev_priv->irq_lock);
++
++	if (enable)
++		dev_priv->de_irq_mask[pipe] &= ~GEN8_PIPE_FIFO_UNDERRUN;
++	else
++		dev_priv->de_irq_mask[pipe] |= GEN8_PIPE_FIFO_UNDERRUN;
++	I915_WRITE(GEN8_DE_PIPE_IMR(pipe), dev_priv->de_irq_mask[pipe]);
++	POSTING_READ(GEN8_DE_PIPE_IMR(pipe));
++}
++
++static void ibx_set_fifo_underrun_reporting(struct drm_device *dev,
++					    enum transcoder pch_transcoder,
++					    bool enable)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	uint32_t bit = (pch_transcoder == TRANSCODER_A) ?
++		       SDE_TRANSA_FIFO_UNDER : SDE_TRANSB_FIFO_UNDER;
++
++	if (enable)
++		ibx_enable_display_interrupt(dev_priv, bit);
++	else
++		ibx_disable_display_interrupt(dev_priv, bit);
++}
++
++static void cpt_set_fifo_underrun_reporting(struct drm_device *dev,
++					    enum transcoder pch_transcoder,
++					    bool enable, bool old)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++
++	if (enable) {
++		I915_WRITE(SERR_INT,
++			   SERR_INT_TRANS_FIFO_UNDERRUN(pch_transcoder));
++
++		if (!cpt_can_enable_serr_int(dev))
++			return;
++
++		ibx_enable_display_interrupt(dev_priv, SDE_ERROR_CPT);
++	} else {
++		ibx_disable_display_interrupt(dev_priv, SDE_ERROR_CPT);
++
++		if (old && I915_READ(SERR_INT) &
++		    SERR_INT_TRANS_FIFO_UNDERRUN(pch_transcoder)) {
++			DRM_ERROR("uncleared pch fifo underrun on pch transcoder %c\n",
++				  transcoder_name(pch_transcoder));
++		}
++	}
++}
++
++static bool __intel_set_cpu_fifo_underrun_reporting(struct drm_device *dev,
++						    enum pipe pipe, bool enable)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_crtc *crtc = dev_priv->pipe_to_crtc_mapping[pipe];
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	bool old;
++
++	assert_spin_locked(&dev_priv->irq_lock);
++
++	old = !intel_crtc->cpu_fifo_underrun_disabled;
++	intel_crtc->cpu_fifo_underrun_disabled = !enable;
++
++	if (HAS_GMCH_DISPLAY(dev))
++		i9xx_set_fifo_underrun_reporting(dev, pipe, enable, old);
++	else if (IS_GEN5(dev) || IS_GEN6(dev))
++		ironlake_set_fifo_underrun_reporting(dev, pipe, enable);
++	else if (IS_GEN7(dev))
++		ivybridge_set_fifo_underrun_reporting(dev, pipe, enable, old);
++	else if (IS_GEN8(dev) || IS_GEN9(dev))
++		broadwell_set_fifo_underrun_reporting(dev, pipe, enable);
++
++	return old;
++}
++
++/**
++ * intel_set_cpu_fifo_underrun_reporting - set cpu fifo underrrun reporting state
++ * @dev_priv: i915 device instance
++ * @pipe: (CPU) pipe to set state for
++ * @enable: whether underruns should be reported or not
++ *
++ * This function sets the fifo underrun state for @pipe. It is used in the
++ * modeset code to avoid false positives since on many platforms underruns are
++ * expected when disabling or enabling the pipe.
++ *
++ * Notice that on some platforms disabling underrun reports for one pipe
++ * disables for all due to shared interrupts. Actual reporting is still per-pipe
++ * though.
++ *
++ * Returns the previous state of underrun reporting.
++ */
++bool intel_set_cpu_fifo_underrun_reporting(struct drm_i915_private *dev_priv,
++					   enum pipe pipe, bool enable)
++{
++	unsigned long flags;
++	bool ret;
++
++	spin_lock_irqsave(&dev_priv->irq_lock, flags);
++	ret = __intel_set_cpu_fifo_underrun_reporting(dev_priv->dev, pipe,
++						      enable);
++	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
++
++	return ret;
++}
++
++static bool
++__cpu_fifo_underrun_reporting_enabled(struct drm_i915_private *dev_priv,
++				      enum pipe pipe)
++{
++	struct drm_crtc *crtc = dev_priv->pipe_to_crtc_mapping[pipe];
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++
++	return !intel_crtc->cpu_fifo_underrun_disabled;
++}
++
++/**
++ * intel_set_pch_fifo_underrun_reporting - set PCH fifo underrun reporting state
++ * @dev_priv: i915 device instance
++ * @pch_transcoder: the PCH transcoder (same as pipe on IVB and older)
++ * @enable: whether underruns should be reported or not
++ *
++ * This function makes us disable or enable PCH fifo underruns for a specific
++ * PCH transcoder. Notice that on some PCHs (e.g. CPT/PPT), disabling FIFO
++ * underrun reporting for one transcoder may also disable all the other PCH
++ * error interruts for the other transcoders, due to the fact that there's just
++ * one interrupt mask/enable bit for all the transcoders.
++ *
++ * Returns the previous state of underrun reporting.
++ */
++bool intel_set_pch_fifo_underrun_reporting(struct drm_i915_private *dev_priv,
++					   enum transcoder pch_transcoder,
++					   bool enable)
++{
++	struct drm_crtc *crtc = dev_priv->pipe_to_crtc_mapping[pch_transcoder];
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	unsigned long flags;
++	bool old;
++
++	/*
++	 * NOTE: Pre-LPT has a fixed cpu pipe -> pch transcoder mapping, but LPT
++	 * has only one pch transcoder A that all pipes can use. To avoid racy
++	 * pch transcoder -> pipe lookups from interrupt code simply store the
++	 * underrun statistics in crtc A. Since we never expose this anywhere
++	 * nor use it outside of the fifo underrun code here using the "wrong"
++	 * crtc on LPT won't cause issues.
++	 */
++
++	spin_lock_irqsave(&dev_priv->irq_lock, flags);
++
++	old = !intel_crtc->pch_fifo_underrun_disabled;
++	intel_crtc->pch_fifo_underrun_disabled = !enable;
++
++	if (HAS_PCH_IBX(dev_priv->dev))
++		ibx_set_fifo_underrun_reporting(dev_priv->dev, pch_transcoder,
++						enable);
++	else
++		cpt_set_fifo_underrun_reporting(dev_priv->dev, pch_transcoder,
++						enable, old);
++
++	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
++	return old;
++}
++
++/**
++ * intel_pch_fifo_underrun_irq_handler - handle PCH fifo underrun interrupt
++ * @dev_priv: i915 device instance
++ * @pipe: (CPU) pipe to set state for
++ *
++ * This handles a CPU fifo underrun interrupt, generating an underrun warning
++ * into dmesg if underrun reporting is enabled and then disables the underrun
++ * interrupt to avoid an irq storm.
++ */
++void intel_cpu_fifo_underrun_irq_handler(struct drm_i915_private *dev_priv,
++					 enum pipe pipe)
++{
++	/* GMCH can't disable fifo underruns, filter them. */
++	if (HAS_GMCH_DISPLAY(dev_priv->dev) &&
++	    !__cpu_fifo_underrun_reporting_enabled(dev_priv, pipe))
++		return;
++
++	if (intel_set_cpu_fifo_underrun_reporting(dev_priv, pipe, false))
++		DRM_ERROR("CPU pipe %c FIFO underrun\n",
++			  pipe_name(pipe));
++}
++
++/**
++ * intel_pch_fifo_underrun_irq_handler - handle PCH fifo underrun interrupt
++ * @dev_priv: i915 device instance
++ * @pch_transcoder: the PCH transcoder (same as pipe on IVB and older)
++ *
++ * This handles a PCH fifo underrun interrupt, generating an underrun warning
++ * into dmesg if underrun reporting is enabled and then disables the underrun
++ * interrupt to avoid an irq storm.
++ */
++void intel_pch_fifo_underrun_irq_handler(struct drm_i915_private *dev_priv,
++					 enum transcoder pch_transcoder)
++{
++	if (intel_set_pch_fifo_underrun_reporting(dev_priv, pch_transcoder,
++						  false))
++		DRM_ERROR("PCH transcoder %c FIFO underrun\n",
++			  transcoder_name(pch_transcoder));
++}
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_frontbuffer.c b/drivers/gpu/drm/i915/intel_frontbuffer.c
+--- a/drivers/gpu/drm/i915/intel_frontbuffer.c	1969-12-31 17:00:00.000000000 -0700
++++ b/drivers/gpu/drm/i915/intel_frontbuffer.c	2014-11-20 09:53:37.988762837 -0700
+@@ -0,0 +1,279 @@
++/*
++ * Copyright  2014 Intel Corporation
++ *
++ * Permission is hereby granted, free of charge, to any person obtaining a
++ * copy of this software and associated documentation files (the "Software"),
++ * to deal in the Software without restriction, including without limitation
++ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
++ * and/or sell copies of the Software, and to permit persons to whom the
++ * Software is furnished to do so, subject to the following conditions:
++ *
++ * The above copyright notice and this permission notice (including the next
++ * paragraph) shall be included in all copies or substantial portions of the
++ * Software.
++ *
++ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
++ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
++ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
++ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
++ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
++ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
++ * DEALINGS IN THE SOFTWARE.
++ *
++ * Authors:
++ *	Daniel Vetter <daniel.vetter@ffwll.ch>
++ */
++
++/**
++ * DOC: frontbuffer tracking
++ *
++ * Many features require us to track changes to the currently active
++ * frontbuffer, especially rendering targeted at the frontbuffer.
++ *
++ * To be able to do so GEM tracks frontbuffers using a bitmask for all possible
++ * frontbuffer slots through i915_gem_track_fb(). The function in this file are
++ * then called when the contents of the frontbuffer are invalidated, when
++ * frontbuffer rendering has stopped again to flush out all the changes and when
++ * the frontbuffer is exchanged with a flip. Subsystems interested in
++ * frontbuffer changes (e.g. PSR, FBC, DRRS) should directly put their callbacks
++ * into the relevant places and filter for the frontbuffer slots that they are
++ * interested int.
++ *
++ * On a high level there are two types of powersaving features. The first one
++ * work like a special cache (FBC and PSR) and are interested when they should
++ * stop caching and when to restart caching. This is done by placing callbacks
++ * into the invalidate and the flush functions: At invalidate the caching must
++ * be stopped and at flush time it can be restarted. And maybe they need to know
++ * when the frontbuffer changes (e.g. when the hw doesn't initiate an invalidate
++ * and flush on its own) which can be achieved with placing callbacks into the
++ * flip functions.
++ *
++ * The other type of display power saving feature only cares about busyness
++ * (e.g. DRRS). In that case all three (invalidate, flush and flip) indicate
++ * busyness. There is no direct way to detect idleness. Instead an idle timer
++ * work delayed work should be started from the flush and flip functions and
++ * cancelled as soon as busyness is detected.
++ *
++ * Note that there's also an older frontbuffer activity tracking scheme which
++ * just tracks general activity. This is done by the various mark_busy and
++ * mark_idle functions. For display power management features using these
++ * functions is deprecated and should be avoided.
++ */
++
++#include <drm/drmP.h>
++
++#include "intel_drv.h"
++#include "i915_drv.h"
++
++static void intel_increase_pllclock(struct drm_device *dev,
++				    enum pipe pipe)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	int dpll_reg = DPLL(pipe);
++	int dpll;
++
++	if (!HAS_GMCH_DISPLAY(dev))
++		return;
++
++	if (!dev_priv->lvds_downclock_avail)
++		return;
++
++	dpll = I915_READ(dpll_reg);
++	if (!HAS_PIPE_CXSR(dev) && (dpll & DISPLAY_RATE_SELECT_FPA1)) {
++		DRM_DEBUG_DRIVER("upclocking LVDS\n");
++
++		assert_panel_unlocked(dev_priv, pipe);
++
++		dpll &= ~DISPLAY_RATE_SELECT_FPA1;
++		I915_WRITE(dpll_reg, dpll);
++		intel_wait_for_vblank(dev, pipe);
++
++		dpll = I915_READ(dpll_reg);
++		if (dpll & DISPLAY_RATE_SELECT_FPA1)
++			DRM_DEBUG_DRIVER("failed to upclock LVDS!\n");
++	}
++}
++
++/**
++ * intel_mark_fb_busy - mark given planes as busy
++ * @dev: DRM device
++ * @frontbuffer_bits: bits for the affected planes
++ * @rq: optional request for asynchronous commands
++ *
++ * This function gets called every time the screen contents change. It can be
++ * used to keep e.g. the update rate at the nominal refresh rate with DRRS.
++ */
++static void intel_mark_fb_busy(struct drm_device *dev,
++			       unsigned frontbuffer_bits,
++			       struct i915_gem_request *rq)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	enum pipe pipe;
++
++	if (!i915_module.powersave)
++		return;
++
++	for_each_pipe(dev_priv, pipe) {
++		if (!(frontbuffer_bits & INTEL_FRONTBUFFER_ALL_MASK(pipe)))
++			continue;
++
++		intel_increase_pllclock(dev, pipe);
++		if (rq && intel_fbc_enabled(dev))
++			rq->pending_flush |= I915_KICK_FBC;
++	}
++}
++
++/**
++ * intel_fb_obj_invalidate - invalidate frontbuffer object
++ * @obj: GEM object to invalidate
++ * @rq: set for asynchronous rendering
++ *
++ * This function gets called every time rendering on the given object starts and
++ * frontbuffer caching (fbc, low refresh rate for DRRS, panel self refresh) must
++ * be invalidated. If @ring is non-NULL any subsequent invalidation will be delayed
++ * until the rendering completes or a flip on this frontbuffer plane is
++ * scheduled.
++ */
++void intel_fb_obj_invalidate(struct drm_i915_gem_object *obj,
++			     struct i915_gem_request *rq)
++{
++	struct drm_device *dev = obj->base.dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++
++	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
++
++	if (!obj->frontbuffer_bits)
++		return;
++
++	if (rq) {
++		mutex_lock(&dev_priv->fb_tracking.lock);
++		dev_priv->fb_tracking.busy_bits
++			|= obj->frontbuffer_bits;
++		dev_priv->fb_tracking.flip_bits
++			&= ~obj->frontbuffer_bits;
++		mutex_unlock(&dev_priv->fb_tracking.lock);
++	}
++
++	intel_mark_fb_busy(dev, obj->frontbuffer_bits, rq);
++
++	intel_edp_psr_invalidate(dev, obj->frontbuffer_bits);
++}
++
++/**
++ * intel_frontbuffer_flush - flush frontbuffer
++ * @dev: DRM device
++ * @frontbuffer_bits: frontbuffer plane tracking bits
++ *
++ * This function gets called every time rendering on the given planes has
++ * completed and frontbuffer caching can be started again. Flushes will get
++ * delayed if they're blocked by some outstanding asynchronous rendering.
++ *
++ * Can be called without any locks held.
++ */
++void intel_frontbuffer_flush(struct drm_device *dev,
++			     unsigned frontbuffer_bits)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++
++	/* Delay flushing when rings are still busy.*/
++	mutex_lock(&dev_priv->fb_tracking.lock);
++	frontbuffer_bits &= ~dev_priv->fb_tracking.busy_bits;
++	mutex_unlock(&dev_priv->fb_tracking.lock);
++
++	intel_mark_fb_busy(dev, frontbuffer_bits, NULL);
++
++	intel_edp_psr_flush(dev, frontbuffer_bits);
++
++	/*
++	 * FIXME: Unconditional fbc flushing here is a rather gross hack and
++	 * needs to be reworked into a proper frontbuffer tracking scheme like
++	 * psr employs.
++	 */
++	if (dev_priv->fbc.need_sw_cache_clean) {
++		dev_priv->fbc.need_sw_cache_clean = false;
++		bdw_fbc_sw_flush(dev, FBC_REND_CACHE_CLEAN);
++	}
++}
++
++/**
++ * intel_fb_obj_flush - flush frontbuffer object
++ * @obj: GEM object to flush
++ * @retire: set when retiring asynchronous rendering
++ *
++ * This function gets called every time rendering on the given object has
++ * completed and frontbuffer caching can be started again. If @retire is true
++ * then any delayed flushes will be unblocked.
++ */
++void intel_fb_obj_flush(struct drm_i915_gem_object *obj,
++			bool retire)
++{
++	struct drm_device *dev = obj->base.dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	unsigned frontbuffer_bits;
++
++	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
++
++	if (!obj->frontbuffer_bits)
++		return;
++
++	frontbuffer_bits = obj->frontbuffer_bits;
++
++	if (retire) {
++		mutex_lock(&dev_priv->fb_tracking.lock);
++		/* Filter out new bits since rendering started. */
++		frontbuffer_bits &= dev_priv->fb_tracking.busy_bits;
++
++		dev_priv->fb_tracking.busy_bits &= ~frontbuffer_bits;
++		mutex_unlock(&dev_priv->fb_tracking.lock);
++	}
++
++	intel_frontbuffer_flush(dev, frontbuffer_bits);
++}
++
++/**
++ * intel_frontbuffer_flip_prepare - prepare asynchronous frontbuffer flip
++ * @dev: DRM device
++ * @frontbuffer_bits: frontbuffer plane tracking bits
++ *
++ * This function gets called after scheduling a flip on @obj. The actual
++ * frontbuffer flushing will be delayed until completion is signalled with
++ * intel_frontbuffer_flip_complete. If an invalidate happens in between this
++ * flush will be cancelled.
++ *
++ * Can be called without any locks held.
++ */
++void intel_frontbuffer_flip_prepare(struct drm_device *dev,
++				    unsigned frontbuffer_bits)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++
++	mutex_lock(&dev_priv->fb_tracking.lock);
++	dev_priv->fb_tracking.flip_bits |= frontbuffer_bits;
++	/* Remove stale busy bits due to the old buffer. */
++	dev_priv->fb_tracking.busy_bits &= ~frontbuffer_bits;
++	mutex_unlock(&dev_priv->fb_tracking.lock);
++}
++
++/**
++ * intel_frontbuffer_flip_complete - complete asynchronous frontbuffer flip
++ * @dev: DRM device
++ * @frontbuffer_bits: frontbuffer plane tracking bits
++ *
++ * This function gets called after the flip has been latched and will complete
++ * on the next vblank. It will execute the flush if it hasn't been cancelled yet.
++ *
++ * Can be called without any locks held.
++ */
++void intel_frontbuffer_flip_complete(struct drm_device *dev,
++				     unsigned frontbuffer_bits)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++
++	mutex_lock(&dev_priv->fb_tracking.lock);
++	/* Mask any cancelled flips. */
++	frontbuffer_bits &= dev_priv->fb_tracking.flip_bits;
++	dev_priv->fb_tracking.flip_bits &= ~frontbuffer_bits;
++	mutex_unlock(&dev_priv->fb_tracking.lock);
++
++	intel_frontbuffer_flush(dev, frontbuffer_bits);
++}
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_hdmi.c b/drivers/gpu/drm/i915/intel_hdmi.c
+--- a/drivers/gpu/drm/i915/intel_hdmi.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_hdmi.c	2014-11-20 09:53:37.988762837 -0700
+@@ -166,6 +166,15 @@
+ 	POSTING_READ(VIDEO_DIP_CTL);
+ }
+ 
++static bool g4x_infoframe_enabled(struct drm_encoder *encoder)
++{
++	struct drm_device *dev = encoder->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	u32 val = I915_READ(VIDEO_DIP_CTL);
++
++	return val & VIDEO_DIP_ENABLE;
++}
++
+ static void ibx_write_infoframe(struct drm_encoder *encoder,
+ 				enum hdmi_infoframe_type type,
+ 				const void *frame, ssize_t len)
+@@ -204,6 +213,17 @@
+ 	POSTING_READ(reg);
+ }
+ 
++static bool ibx_infoframe_enabled(struct drm_encoder *encoder)
++{
++	struct drm_device *dev = encoder->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_crtc *intel_crtc = to_intel_crtc(encoder->crtc);
++	int reg = TVIDEO_DIP_CTL(intel_crtc->pipe);
++	u32 val = I915_READ(reg);
++
++	return val & VIDEO_DIP_ENABLE;
++}
++
+ static void cpt_write_infoframe(struct drm_encoder *encoder,
+ 				enum hdmi_infoframe_type type,
+ 				const void *frame, ssize_t len)
+@@ -245,6 +265,17 @@
+ 	POSTING_READ(reg);
+ }
+ 
++static bool cpt_infoframe_enabled(struct drm_encoder *encoder)
++{
++	struct drm_device *dev = encoder->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_crtc *intel_crtc = to_intel_crtc(encoder->crtc);
++	int reg = TVIDEO_DIP_CTL(intel_crtc->pipe);
++	u32 val = I915_READ(reg);
++
++	return val & VIDEO_DIP_ENABLE;
++}
++
+ static void vlv_write_infoframe(struct drm_encoder *encoder,
+ 				enum hdmi_infoframe_type type,
+ 				const void *frame, ssize_t len)
+@@ -283,6 +314,17 @@
+ 	POSTING_READ(reg);
+ }
+ 
++static bool vlv_infoframe_enabled(struct drm_encoder *encoder)
++{
++	struct drm_device *dev = encoder->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_crtc *intel_crtc = to_intel_crtc(encoder->crtc);
++	int reg = VLV_TVIDEO_DIP_CTL(intel_crtc->pipe);
++	u32 val = I915_READ(reg);
++
++	return val & VIDEO_DIP_ENABLE;
++}
++
+ static void hsw_write_infoframe(struct drm_encoder *encoder,
+ 				enum hdmi_infoframe_type type,
+ 				const void *frame, ssize_t len)
+@@ -320,6 +362,18 @@
+ 	POSTING_READ(ctl_reg);
+ }
+ 
++static bool hsw_infoframe_enabled(struct drm_encoder *encoder)
++{
++	struct drm_device *dev = encoder->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_crtc *intel_crtc = to_intel_crtc(encoder->crtc);
++	u32 ctl_reg = HSW_TVIDEO_DIP_CTL(intel_crtc->config.cpu_transcoder);
++	u32 val = I915_READ(ctl_reg);
++
++	return val & (VIDEO_DIP_ENABLE_AVI_HSW | VIDEO_DIP_ENABLE_SPD_HSW |
++		      VIDEO_DIP_ENABLE_VS_HSW);
++}
++
+ /*
+  * The data we write to the DIP data buffer registers is 1 byte bigger than the
+  * HDMI infoframe size because of an ECC/reserved byte at position 3 (starting
+@@ -661,14 +715,6 @@
+ 	if (crtc->config.has_hdmi_sink)
+ 		hdmi_val |= HDMI_MODE_SELECT_HDMI;
+ 
+-	if (crtc->config.has_audio) {
+-		WARN_ON(!crtc->config.has_hdmi_sink);
+-		DRM_DEBUG_DRIVER("Enabling HDMI audio on pipe %c\n",
+-				 pipe_name(crtc->pipe));
+-		hdmi_val |= SDVO_AUDIO_ENABLE;
+-		intel_write_eld(&encoder->base, adjusted_mode);
+-	}
+-
+ 	if (HAS_PCH_CPT(dev))
+ 		hdmi_val |= SDVO_PIPE_SEL_CPT(crtc->pipe);
+ 	else if (IS_CHERRYVIEW(dev))
+@@ -690,7 +736,7 @@
+ 	u32 tmp;
+ 
+ 	power_domain = intel_display_port_power_domain(encoder);
+-	if (!intel_display_power_enabled(dev_priv, power_domain))
++	if (!intel_display_power_is_enabled(dev_priv, power_domain))
+ 		return false;
+ 
+ 	tmp = I915_READ(intel_hdmi->hdmi_reg);
+@@ -732,6 +778,9 @@
+ 	if (tmp & HDMI_MODE_SELECT_HDMI)
+ 		pipe_config->has_hdmi_sink = true;
+ 
++	if (intel_hdmi->infoframe_enabled(&encoder->base))
++		pipe_config->has_infoframe = true;
++
+ 	if (tmp & SDVO_AUDIO_ENABLE)
+ 		pipe_config->has_audio = true;
+ 
+@@ -791,6 +840,13 @@
+ 		I915_WRITE(intel_hdmi->hdmi_reg, temp);
+ 		POSTING_READ(intel_hdmi->hdmi_reg);
+ 	}
++
++	if (intel_crtc->config.has_audio) {
++		WARN_ON(!intel_crtc->config.has_hdmi_sink);
++		DRM_DEBUG_DRIVER("Enabling HDMI audio on pipe %c\n",
++				 pipe_name(intel_crtc->pipe));
++		intel_audio_codec_enable(encoder);
++	}
+ }
+ 
+ static void vlv_enable_hdmi(struct intel_encoder *encoder)
+@@ -802,9 +858,13 @@
+ 	struct drm_device *dev = encoder->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct intel_hdmi *intel_hdmi = enc_to_intel_hdmi(&encoder->base);
++	struct intel_crtc *crtc = to_intel_crtc(encoder->base.crtc);
+ 	u32 temp;
+ 	u32 enable_bits = SDVO_ENABLE | SDVO_AUDIO_ENABLE;
+ 
++	if (crtc->config.has_audio)
++		intel_audio_codec_disable(encoder);
++
+ 	temp = I915_READ(intel_hdmi->hdmi_reg);
+ 
+ 	/* HW workaround for IBX, we need to move the port to transcoder A
+@@ -869,10 +929,15 @@
+ intel_hdmi_mode_valid(struct drm_connector *connector,
+ 		      struct drm_display_mode *mode)
+ {
+-	if (mode->clock > hdmi_portclock_limit(intel_attached_hdmi(connector),
+-					       true))
++	int clock = mode->clock;
++
++	if (mode->flags & DRM_MODE_FLAG_DBLCLK)
++		clock *= 2;
++
++	if (clock > hdmi_portclock_limit(intel_attached_hdmi(connector),
++					 true))
+ 		return MODE_CLOCK_HIGH;
+-	if (mode->clock < 20000)
++	if (clock < 20000)
+ 		return MODE_CLOCK_LOW;
+ 
+ 	if (mode->flags & DRM_MODE_FLAG_DBLSCAN)
+@@ -890,7 +955,7 @@
+ 	if (HAS_GMCH_DISPLAY(dev))
+ 		return false;
+ 
+-	list_for_each_entry(encoder, &dev->mode_config.encoder_list, base.head) {
++	for_each_intel_encoder(dev, encoder) {
+ 		if (encoder->new_crtc != crtc)
+ 			continue;
+ 
+@@ -917,6 +982,9 @@
+ 
+ 	pipe_config->has_hdmi_sink = intel_hdmi->has_hdmi_sink;
+ 
++	if (pipe_config->has_hdmi_sink)
++		pipe_config->has_infoframe = true;
++
+ 	if (intel_hdmi->color_range_auto) {
+ 		/* See CEA-861-E - 5.1 Default Encoding Parameters */
+ 		if (pipe_config->has_hdmi_sink &&
+@@ -926,6 +994,10 @@
+ 			intel_hdmi->color_range = 0;
+ 	}
+ 
++	if (adjusted_mode->flags & DRM_MODE_FLAG_DBLCLK) {
++		pipe_config->pixel_multiplier = 2;
++	}
++
+ 	if (intel_hdmi->color_range)
+ 		pipe_config->limited_color_range = true;
+ 
+@@ -967,106 +1039,121 @@
+ 	return true;
+ }
+ 
+-static enum drm_connector_status
+-intel_hdmi_detect(struct drm_connector *connector, bool force)
++static void
++intel_hdmi_unset_edid(struct drm_connector *connector)
+ {
+-	struct drm_device *dev = connector->dev;
+ 	struct intel_hdmi *intel_hdmi = intel_attached_hdmi(connector);
+-	struct intel_digital_port *intel_dig_port =
+-		hdmi_to_dig_port(intel_hdmi);
+-	struct intel_encoder *intel_encoder = &intel_dig_port->base;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct edid *edid;
+-	enum intel_display_power_domain power_domain;
+-	enum drm_connector_status status = connector_status_disconnected;
+ 
+-	DRM_DEBUG_KMS("[CONNECTOR:%d:%s]\n",
+-		      connector->base.id, connector->name);
++	intel_hdmi->has_hdmi_sink = false;
++	intel_hdmi->has_audio = false;
++	intel_hdmi->rgb_quant_range_selectable = false;
++
++	kfree(to_intel_connector(connector)->detect_edid);
++	to_intel_connector(connector)->detect_edid = NULL;
++}
++
++static bool
++intel_hdmi_update_audio(struct drm_connector *connector)
++{
++	struct intel_hdmi *intel_hdmi = intel_attached_hdmi(connector);
++	struct edid *edid = to_intel_connector(connector)->detect_edid;
++	bool has_audio, has_sink;
++	bool changed = false;
++
++	if (intel_hdmi->force_audio == HDMI_AUDIO_AUTO)
++		has_audio = drm_detect_monitor_audio(edid);
++	else
++		has_audio = intel_hdmi->force_audio == HDMI_AUDIO_ON;
++	changed |= intel_hdmi->has_audio != has_audio;
++	intel_hdmi->has_audio = has_audio;
++
++	has_sink = false;
++	if (intel_hdmi->force_audio != HDMI_AUDIO_OFF_DVI)
++		has_sink = drm_detect_hdmi_monitor(edid);
++	changed |= intel_hdmi->has_hdmi_sink != has_sink;
++	intel_hdmi->has_hdmi_sink = has_sink;
++
++	return changed;
++}
++
++static bool
++intel_hdmi_set_edid(struct drm_connector *connector)
++{
++	struct drm_i915_private *dev_priv = to_i915(connector->dev);
++	struct intel_hdmi *intel_hdmi = intel_attached_hdmi(connector);
++	struct intel_encoder *intel_encoder =
++		&hdmi_to_dig_port(intel_hdmi)->base;
++	enum intel_display_power_domain power_domain;
++	struct edid *edid;
++	bool connected = false;
+ 
+ 	power_domain = intel_display_port_power_domain(intel_encoder);
+ 	intel_display_power_get(dev_priv, power_domain);
+ 
+-	intel_hdmi->has_hdmi_sink = false;
+-	intel_hdmi->has_audio = false;
+-	intel_hdmi->rgb_quant_range_selectable = false;
+ 	edid = drm_get_edid(connector,
+ 			    intel_gmbus_get_adapter(dev_priv,
+ 						    intel_hdmi->ddc_bus));
+ 
+-	if (edid) {
+-		if (edid->input & DRM_EDID_INPUT_DIGITAL) {
+-			status = connector_status_connected;
+-			if (intel_hdmi->force_audio != HDMI_AUDIO_OFF_DVI)
+-				intel_hdmi->has_hdmi_sink =
+-						drm_detect_hdmi_monitor(edid);
+-			intel_hdmi->has_audio = drm_detect_monitor_audio(edid);
+-			intel_hdmi->rgb_quant_range_selectable =
+-				drm_rgb_quant_range_selectable(edid);
+-		}
+-		kfree(edid);
+-	}
++	intel_display_power_put(dev_priv, power_domain);
+ 
+-	if (status == connector_status_connected) {
+-		if (intel_hdmi->force_audio != HDMI_AUDIO_AUTO)
+-			intel_hdmi->has_audio =
+-				(intel_hdmi->force_audio == HDMI_AUDIO_ON);
+-		intel_encoder->type = INTEL_OUTPUT_HDMI;
++	to_intel_connector(connector)->detect_edid = edid;
++	if (edid && edid->input & DRM_EDID_INPUT_DIGITAL) {
++		intel_hdmi->rgb_quant_range_selectable =
++			drm_rgb_quant_range_selectable(edid);
++		intel_hdmi_update_audio(connector);
++		connected = true;
+ 	}
+ 
+-	intel_display_power_put(dev_priv, power_domain);
+-
+-	return status;
++	return connected;
+ }
+ 
+-static int intel_hdmi_get_modes(struct drm_connector *connector)
++static enum drm_connector_status
++intel_hdmi_detect(struct drm_connector *connector, bool force)
+ {
+-	struct intel_encoder *intel_encoder = intel_attached_encoder(connector);
+-	struct intel_hdmi *intel_hdmi = enc_to_intel_hdmi(&intel_encoder->base);
+-	struct drm_i915_private *dev_priv = connector->dev->dev_private;
+-	enum intel_display_power_domain power_domain;
+-	int ret;
++	enum drm_connector_status status;
+ 
+-	/* We should parse the EDID data and find out if it's an HDMI sink so
+-	 * we can send audio to it.
+-	 */
++	DRM_DEBUG_KMS("[CONNECTOR:%d:%s]\n",
++		      connector->base.id, connector->name);
+ 
+-	power_domain = intel_display_port_power_domain(intel_encoder);
+-	intel_display_power_get(dev_priv, power_domain);
++	intel_hdmi_unset_edid(connector);
+ 
+-	ret = intel_ddc_get_modes(connector,
+-				   intel_gmbus_get_adapter(dev_priv,
+-							   intel_hdmi->ddc_bus));
++	if (intel_hdmi_set_edid(connector)) {
++		struct intel_hdmi *intel_hdmi = intel_attached_hdmi(connector);
+ 
+-	intel_display_power_put(dev_priv, power_domain);
++		hdmi_to_dig_port(intel_hdmi)->base.type = INTEL_OUTPUT_HDMI;
++		status = connector_status_connected;
++	} else
++		status = connector_status_disconnected;
+ 
+-	return ret;
++	return status;
+ }
+ 
+-static bool
+-intel_hdmi_detect_audio(struct drm_connector *connector)
++static void
++intel_hdmi_force(struct drm_connector *connector)
+ {
+-	struct intel_encoder *intel_encoder = intel_attached_encoder(connector);
+-	struct intel_hdmi *intel_hdmi = enc_to_intel_hdmi(&intel_encoder->base);
+-	struct drm_i915_private *dev_priv = connector->dev->dev_private;
+-	enum intel_display_power_domain power_domain;
+-	struct edid *edid;
+-	bool has_audio = false;
++	struct intel_hdmi *intel_hdmi = intel_attached_hdmi(connector);
+ 
+-	power_domain = intel_display_port_power_domain(intel_encoder);
+-	intel_display_power_get(dev_priv, power_domain);
++	DRM_DEBUG_KMS("[CONNECTOR:%d:%s]\n",
++		      connector->base.id, connector->name);
+ 
+-	edid = drm_get_edid(connector,
+-			    intel_gmbus_get_adapter(dev_priv,
+-						    intel_hdmi->ddc_bus));
+-	if (edid) {
+-		if (edid->input & DRM_EDID_INPUT_DIGITAL)
+-			has_audio = drm_detect_monitor_audio(edid);
+-		kfree(edid);
+-	}
++	intel_hdmi_unset_edid(connector);
+ 
+-	intel_display_power_put(dev_priv, power_domain);
++	if (connector->status != connector_status_connected)
++		return;
++
++	intel_hdmi_set_edid(connector);
++	hdmi_to_dig_port(intel_hdmi)->base.type = INTEL_OUTPUT_HDMI;
++}
++
++static int intel_hdmi_get_modes(struct drm_connector *connector)
++{
++	struct edid *edid;
++
++	edid = to_intel_connector(connector)->detect_edid;
++	if (edid == NULL)
++		return 0;
+ 
+-	return has_audio;
++	return intel_connector_update_modes(connector, edid);
+ }
+ 
+ static int
+@@ -1086,22 +1173,14 @@
+ 
+ 	if (property == dev_priv->force_audio_property) {
+ 		enum hdmi_force_audio i = val;
+-		bool has_audio;
+ 
+ 		if (i == intel_hdmi->force_audio)
+ 			return 0;
+ 
+ 		intel_hdmi->force_audio = i;
++		if (!intel_hdmi_update_audio(connector))
++			return 0;
+ 
+-		if (i == HDMI_AUDIO_AUTO)
+-			has_audio = intel_hdmi_detect_audio(connector);
+-		else
+-			has_audio = (i == HDMI_AUDIO_ON);
+-
+-		if (i == HDMI_AUDIO_OFF_DVI)
+-			intel_hdmi->has_hdmi_sink = 0;
+-
+-		intel_hdmi->has_audio = has_audio;
+ 		goto done;
+ 	}
+ 
+@@ -1265,6 +1344,8 @@
+ 	enum pipe pipe = intel_crtc->pipe;
+ 	u32 val;
+ 
++	intel_hdmi_prepare(encoder);
++
+ 	mutex_lock(&dev_priv->dpio_lock);
+ 
+ 	/* program left/right clock distribution */
+@@ -1381,6 +1462,15 @@
+ 
+ 	mutex_lock(&dev_priv->dpio_lock);
+ 
++	/* allow hardware to manage TX FIFO reset source */
++	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS01_DW11(ch));
++	val &= ~DPIO_LANEDESKEW_STRAP_OVRD;
++	vlv_dpio_write(dev_priv, pipe, VLV_PCS01_DW11(ch), val);
++
++	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS23_DW11(ch));
++	val &= ~DPIO_LANEDESKEW_STRAP_OVRD;
++	vlv_dpio_write(dev_priv, pipe, VLV_PCS23_DW11(ch), val);
++
+ 	/* Deassert soft data lane reset*/
+ 	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS01_DW1(ch));
+ 	val |= CHV_PCS_REQ_SOFTRESET_EN;
+@@ -1417,12 +1507,26 @@
+ 	/* Clear calc init */
+ 	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS01_DW10(ch));
+ 	val &= ~(DPIO_PCS_SWING_CALC_TX0_TX2 | DPIO_PCS_SWING_CALC_TX1_TX3);
++	val &= ~(DPIO_PCS_TX1DEEMP_MASK | DPIO_PCS_TX2DEEMP_MASK);
++	val |= DPIO_PCS_TX1DEEMP_9P5 | DPIO_PCS_TX2DEEMP_9P5;
+ 	vlv_dpio_write(dev_priv, pipe, VLV_PCS01_DW10(ch), val);
+ 
+ 	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS23_DW10(ch));
+ 	val &= ~(DPIO_PCS_SWING_CALC_TX0_TX2 | DPIO_PCS_SWING_CALC_TX1_TX3);
++	val &= ~(DPIO_PCS_TX1DEEMP_MASK | DPIO_PCS_TX2DEEMP_MASK);
++	val |= DPIO_PCS_TX1DEEMP_9P5 | DPIO_PCS_TX2DEEMP_9P5;
+ 	vlv_dpio_write(dev_priv, pipe, VLV_PCS23_DW10(ch), val);
+ 
++	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS01_DW9(ch));
++	val &= ~(DPIO_PCS_TX1MARGIN_MASK | DPIO_PCS_TX2MARGIN_MASK);
++	val |= DPIO_PCS_TX1MARGIN_000 | DPIO_PCS_TX2MARGIN_000;
++	vlv_dpio_write(dev_priv, pipe, VLV_PCS01_DW9(ch), val);
++
++	val = vlv_dpio_read(dev_priv, pipe, VLV_PCS23_DW9(ch));
++	val &= ~(DPIO_PCS_TX1MARGIN_MASK | DPIO_PCS_TX2MARGIN_MASK);
++	val |= DPIO_PCS_TX1MARGIN_000 | DPIO_PCS_TX2MARGIN_000;
++	vlv_dpio_write(dev_priv, pipe, VLV_PCS23_DW9(ch), val);
++
+ 	/* FIXME: Program the support xxx V-dB */
+ 	/* Use 800mV-0dB */
+ 	for (i = 0; i < 4; i++) {
+@@ -1434,8 +1538,8 @@
+ 
+ 	for (i = 0; i < 4; i++) {
+ 		val = vlv_dpio_read(dev_priv, pipe, CHV_TX_DW2(ch, i));
+-		val &= ~DPIO_SWING_MARGIN_MASK;
+-		val |= 102 << DPIO_SWING_MARGIN_SHIFT;
++		val &= ~DPIO_SWING_MARGIN000_MASK;
++		val |= 102 << DPIO_SWING_MARGIN000_SHIFT;
+ 		vlv_dpio_write(dev_priv, pipe, CHV_TX_DW2(ch, i), val);
+ 	}
+ 
+@@ -1482,6 +1586,7 @@
+ 
+ static void intel_hdmi_destroy(struct drm_connector *connector)
+ {
++	kfree(to_intel_connector(connector)->detect_edid);
+ 	drm_connector_cleanup(connector);
+ 	kfree(connector);
+ }
+@@ -1489,6 +1594,7 @@
+ static const struct drm_connector_funcs intel_hdmi_connector_funcs = {
+ 	.dpms = intel_connector_dpms,
+ 	.detect = intel_hdmi_detect,
++	.force = intel_hdmi_force,
+ 	.fill_modes = drm_helper_probe_single_connector_modes,
+ 	.set_property = intel_hdmi_set_property,
+ 	.destroy = intel_hdmi_destroy,
+@@ -1567,18 +1673,23 @@
+ 	if (IS_VALLEYVIEW(dev)) {
+ 		intel_hdmi->write_infoframe = vlv_write_infoframe;
+ 		intel_hdmi->set_infoframes = vlv_set_infoframes;
++		intel_hdmi->infoframe_enabled = vlv_infoframe_enabled;
+ 	} else if (IS_G4X(dev)) {
+ 		intel_hdmi->write_infoframe = g4x_write_infoframe;
+ 		intel_hdmi->set_infoframes = g4x_set_infoframes;
++		intel_hdmi->infoframe_enabled = g4x_infoframe_enabled;
+ 	} else if (HAS_DDI(dev)) {
+ 		intel_hdmi->write_infoframe = hsw_write_infoframe;
+ 		intel_hdmi->set_infoframes = hsw_set_infoframes;
++		intel_hdmi->infoframe_enabled = hsw_infoframe_enabled;
+ 	} else if (HAS_PCH_IBX(dev)) {
+ 		intel_hdmi->write_infoframe = ibx_write_infoframe;
+ 		intel_hdmi->set_infoframes = ibx_set_infoframes;
++		intel_hdmi->infoframe_enabled = ibx_infoframe_enabled;
+ 	} else {
+ 		intel_hdmi->write_infoframe = cpt_write_infoframe;
+ 		intel_hdmi->set_infoframes = cpt_set_infoframes;
++		intel_hdmi->infoframe_enabled = cpt_infoframe_enabled;
+ 	}
+ 
+ 	if (HAS_DDI(dev))
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_lrc.c b/drivers/gpu/drm/i915/intel_lrc.c
+--- a/drivers/gpu/drm/i915/intel_lrc.c	1969-12-31 17:00:00.000000000 -0700
++++ b/drivers/gpu/drm/i915/intel_lrc.c	2014-11-20 09:53:37.988762837 -0700
+@@ -0,0 +1,788 @@
++/*
++ * Copyright  2014 Intel Corporation
++ *
++ * Permission is hereby granted, free of charge, to any person obtaining a
++ * copy of this software and associated documentation files (the "Software"),
++ * to deal in the Software without restriction, including without limitation
++ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
++ * and/or sell copies of the Software, and to permit persons to whom the
++ * Software is furnished to do so, subject to the following conditions:
++ *
++ * The above copyright notice and this permission notice (including the next
++ * paragraph) shall be included in all copies or substantial portions of the
++ * Software.
++ *
++ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
++ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
++ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
++ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
++ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
++ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
++ * IN THE SOFTWARE.
++ *
++ * Authors:
++ *    Ben Widawsky <ben@bwidawsk.net>
++ *    Michel Thierry <michel.thierry@intel.com>
++ *    Thomas Daniel <thomas.daniel@intel.com>
++ *    Oscar Mateo <oscar.mateo@intel.com>
++ *
++ */
++
++/**
++ * DOC: Logical Rings, Logical Ring Contexts and Execlists
++ *
++ * Motivation:
++ * GEN8 brings an expansion of the HW contexts: "Logical Ring Contexts".
++ * These expanded contexts enable a number of new abilities, especially
++ * "Execlists" (also implemented in this file).
++ *
++ * One of the main differences with the legacy HW contexts is that logical
++ * ring contexts incorporate many more things to the context's state, like
++ * PDPs or ringbuffer control registers:
++ *
++ * The reason why PDPs are included in the context is straightforward: as
++ * PPGTTs (per-process GTTs) are actually per-context, having the PDPs
++ * contained there mean you don't need to do a ppgtt->switch_mm yourself,
++ * instead, the GPU will do it for you on the context switch.
++ *
++ * But, what about the ringbuffer control registers (head, tail, etc..)?
++ * shouldn't we just need a set of those per engine command streamer? This is
++ * where the name "Logical Rings" starts to make sense: by virtualizing the
++ * rings, the engine cs shifts to a new "ring buffer" with every context
++ * switch. When you want to submit a workload to the GPU you: A) choose your
++ * context, B) find its appropriate virtualized ring, C) write commands to it
++ * and then, finally, D) tell the GPU to switch to that context.
++ *
++ * Instead of the legacy MI_SET_CONTEXT, the way you tell the GPU to switch
++ * to a contexts is via a context execution list, ergo "Execlists".
++ *
++ * LRC implementation:
++ * Regarding the creation of contexts, we have:
++ *
++ * - One global default context.
++ * - One local default context for each opened fd.
++ * - One local extra context for each context create ioctl call.
++ *
++ * Now that ringbuffers belong per-context (and not per-engine, like before)
++ * and that contexts are uniquely tied to a given engine (and not reusable,
++ * like before) we need:
++ *
++ * - One ringbuffer per-engine inside each context.
++ * - One backing object per-engine inside each context.
++ *
++ * The global default context starts its life with these new objects fully
++ * allocated and populated. The local default context for each opened fd is
++ * more complex, because we don't know at creation time which engine is going
++ * to use them. To handle this, we have implemented a deferred creation of LR
++ * contexts:
++ *
++ * The local context starts its life as a hollow or blank holder, that only
++ * gets populated for a given engine once we receive an execbuffer. If later
++ * on we receive another execbuffer ioctl for the same context but a different
++ * engine, we allocate/populate a new ringbuffer and context backing object and
++ * so on.
++ *
++ * Finally, regarding local contexts created using the ioctl call: as they are
++ * only allowed with the render ring, we can allocate & populate them right
++ * away (no need to defer anything, at least for now).
++ *
++ * Execlists implementation:
++ * Execlists are the new method by which, on gen8+ hardware, workloads are
++ * submitted for execution (as opposed to the legacy, ringbuffer-based, method).
++ * This method works as follows:
++ *
++ * When a request is committed, its commands (the BB start and any leading or
++ * trailing commands, like the seqno breadcrumbs) are placed in the ringbuffer
++ * for the appropriate context. The tail pointer in the hardware context is not
++ * updated at this time, but instead, kept by the driver in the ringbuffer
++ * structure. A structure representing this request is added to a request queue
++ * for the appropriate engine: this structure contains a copy of the context's
++ * tail after the request was written to the ring buffer and a pointer to the
++ * context itself.
++ *
++ * If the engine's request queue was empty before the request was added, the
++ * queue is processed immediately. Otherwise the queue will be processed during
++ * a context switch interrupt. In any case, elements on the queue will get sent
++ * (in pairs) to the GPU's ExecLists Submit Port (ELSP, for short) with a
++ * globally unique 20-bits submission ID.
++ *
++ * When execution of a request completes, the GPU updates the context status
++ * buffer with a context complete event and generates a context switch interrupt.
++ * During the interrupt handling, the driver examines the events in the buffer:
++ * for each context complete event, if the announced ID matches that on the head
++ * of the request queue, then that request is retired and removed from the queue.
++ *
++ * After processing, if any requests were retired and the queue is not empty
++ * then a new execution list can be submitted. The two requests at the front of
++ * the queue are next to be submitted but since a context may not occur twice in
++ * an execution list, if subsequent requests have the same ID as the first then
++ * the two requests must be combined. This is done simply by discarding requests
++ * at the head of the queue until either only one requests is left (in which case
++ * we use a NULL second context) or the first two requests have unique IDs.
++ *
++ * By always executing the first two requests in the queue the driver ensures
++ * that the GPU is kept as busy as possible. In the case where a single context
++ * completes but a second context is still executing, the request for this second
++ * context will be at the head of the queue when we remove the first one. This
++ * request will then be resubmitted along with a new request for a different context,
++ * which will cause the hardware to continue executing the second request and queue
++ * the new request (the GPU detects the condition of a context getting preempted
++ * with the same context and optimizes the context switch flow by not doing
++ * preemption, but just sampling the new tail pointer).
++ *
++ */
++
++#include <drm/drmP.h>
++#include <drm/i915_drm.h>
++#include "i915_drv.h"
++
++#define GEN9_LR_CONTEXT_RENDER_SIZE (22 * PAGE_SIZE)
++#define GEN8_LR_CONTEXT_RENDER_SIZE (20 * PAGE_SIZE)
++#define GEN8_LR_CONTEXT_OTHER_SIZE (2 * PAGE_SIZE)
++
++#define RING_EXECLIST_QFULL		(1 << 0x2)
++#define RING_EXECLIST1_VALID		(1 << 0x3)
++#define RING_EXECLIST0_VALID		(1 << 0x4)
++#define RING_EXECLIST_ACTIVE_STATUS	(3 << 0xE)
++#define RING_EXECLIST1_ACTIVE		(1 << 0x11)
++#define RING_EXECLIST0_ACTIVE		(1 << 0x12)
++
++#define GEN8_CTX_STATUS_IDLE_ACTIVE	(1 << 0)
++#define GEN8_CTX_STATUS_PREEMPTED	(1 << 1)
++#define GEN8_CTX_STATUS_ELEMENT_SWITCH	(1 << 2)
++#define GEN8_CTX_STATUS_ACTIVE_IDLE	(1 << 3)
++#define GEN8_CTX_STATUS_COMPLETE	(1 << 4)
++#define GEN8_CTX_STATUS_LITE_RESTORE	(1 << 15)
++
++#define CTX_LRI_HEADER_0		0x01
++#define CTX_CONTEXT_CONTROL		0x02
++#define CTX_RING_HEAD			0x04
++#define CTX_RING_TAIL			0x06
++#define CTX_RING_BUFFER_START		0x08
++#define CTX_RING_BUFFER_CONTROL		0x0a
++#define CTX_BB_HEAD_U			0x0c
++#define CTX_BB_HEAD_L			0x0e
++#define CTX_BB_STATE			0x10
++#define CTX_SECOND_BB_HEAD_U		0x12
++#define CTX_SECOND_BB_HEAD_L		0x14
++#define CTX_SECOND_BB_STATE		0x16
++#define CTX_BB_PER_CTX_PTR		0x18
++#define CTX_RCS_INDIRECT_CTX		0x1a
++#define CTX_RCS_INDIRECT_CTX_OFFSET	0x1c
++#define CTX_LRI_HEADER_1		0x21
++#define CTX_CTX_TIMESTAMP		0x22
++#define CTX_PDP3_UDW			0x24
++#define CTX_PDP3_LDW			0x26
++#define CTX_PDP2_UDW			0x28
++#define CTX_PDP2_LDW			0x2a
++#define CTX_PDP1_UDW			0x2c
++#define CTX_PDP1_LDW			0x2e
++#define CTX_PDP0_UDW			0x30
++#define CTX_PDP0_LDW			0x32
++#define CTX_LRI_HEADER_2		0x41
++#define CTX_R_PWR_CLK_STATE		0x42
++#define CTX_GPGPU_CSR_BASE_ADDRESS	0x44
++
++#define GEN8_CTX_VALID (1<<0)
++#define GEN8_CTX_FORCE_PD_RESTORE (1<<1)
++#define GEN8_CTX_FORCE_RESTORE (1<<2)
++#define GEN8_CTX_L3LLC_COHERENT (1<<5)
++#define GEN8_CTX_PRIVILEGE (1<<8)
++enum {
++	ADVANCED_CONTEXT = 0,
++	LEGACY_CONTEXT,
++	ADVANCED_AD_CONTEXT,
++	LEGACY_64B_CONTEXT
++};
++#define GEN8_CTX_MODE_SHIFT 3
++enum {
++	FAULT_AND_HANG = 0,
++	FAULT_AND_HALT, /* Debug only */
++	FAULT_AND_STREAM,
++	FAULT_AND_CONTINUE /* Unsupported */
++};
++#define GEN8_CTX_ID_SHIFT 32
++
++static u32 execlists_ctx_descriptor(struct drm_i915_gem_object *ctx)
++{
++	u32 desc;
++
++	desc = GEN8_CTX_VALID;
++	desc |= LEGACY_CONTEXT << GEN8_CTX_MODE_SHIFT;
++	desc |= GEN8_CTX_L3LLC_COHERENT;
++	desc |= GEN8_CTX_PRIVILEGE;
++	desc |= i915_gem_obj_ggtt_offset(ctx);
++
++	/* TODO: WaDisableLiteRestore when we start using semaphore
++	 * signalling between Command Streamers */
++	/* desc |= GEN8_CTX_FORCE_RESTORE; */
++
++	return desc;
++}
++
++static u32 *ctx_get_regs(struct drm_i915_gem_object *obj)
++{
++	int ret;
++
++	/* The second page of the context object contains some fields which
++	 * must be set up prior to the first execution.
++	 */
++
++	ret = i915_gem_object_get_pages(obj);
++	if (ret)
++		return ERR_PTR(ret);
++
++	ret = i915_gem_object_set_to_cpu_domain(obj, true);
++	if (ret)
++		return ERR_PTR(ret);
++
++	return kmap_atomic(i915_gem_object_get_page(obj, 1));
++}
++
++static u32 execlists_ctx_write_tail(const struct i915_gem_request *rq)
++{
++	struct drm_i915_gem_object *obj = rq->ctx->ring[rq->engine->id].state;
++	u32 *regs;
++
++	regs = kmap_atomic(i915_gem_object_get_page(obj, 1));
++	regs[CTX_RING_TAIL+1] = rq->tail;
++	kunmap_atomic(regs);
++
++	return execlists_ctx_descriptor(obj);
++}
++
++static void execlists_submit_pair(struct intel_engine_cs *engine,
++				  const struct i915_gem_request *rq[2])
++{
++	struct drm_i915_private *dev_priv = engine->i915;
++	u32 desc[4];
++
++	desc[2] = execlists_ctx_write_tail(rq[0]);
++	desc[3] = rq[0]->tag;
++
++	if (rq[1]) {
++		desc[0] = execlists_ctx_write_tail(rq[1]);
++		desc[1] = rq[1]->tag;
++	} else
++		desc[1] = desc[0] = 0;
++
++	gen6_gt_force_wake_get(dev_priv, engine->power_domains);
++	/* XXX: You must always write both descriptors in the order below. */
++	I915_WRITE(RING_ELSP(engine), desc[1]);
++	I915_WRITE(RING_ELSP(engine), desc[0]);
++	I915_WRITE(RING_ELSP(engine), desc[3]);
++	/* The context is automatically loaded after the following */
++	I915_WRITE(RING_ELSP(engine), desc[2]);
++
++	/* ELSP is a wo register, so use another nearby reg for posting instead */
++	POSTING_READ(RING_EXECLIST_STATUS(engine));
++	gen6_gt_force_wake_put(dev_priv, engine->power_domains);
++}
++
++static u16 next_tag(struct intel_engine_cs *engine)
++{
++	/* status tags are limited to 20b, so we use a u16 for convenience */
++	if (++engine->next_tag == 0)
++		++engine->next_tag;
++	WARN_ON((s16)(engine->next_tag - engine->tag) < 0);
++	return engine->next_tag;
++}
++
++static void execlists_submit(struct intel_engine_cs *engine)
++{
++	const struct i915_gem_request *rq[2] = {};
++	int i = 0;
++
++	assert_spin_locked(&engine->irqlock);
++
++	/* Try to submit requests in pairs */
++	while (!list_empty(&engine->pending)) {
++		struct i915_gem_request *next;
++
++		next = list_first_entry(&engine->pending,
++					typeof(*next),
++					engine_link);
++
++		if (rq[i] == NULL) {
++new_slot:
++			next->tag = next_tag(engine);
++			rq[i] = next;
++		} else if (rq[i]->ctx == next->ctx) {
++			/* Same ctx: ignore first request, as second request
++			 * will update tail past first request's workload */
++			next->tag = rq[i]->tag;
++			rq[i] = next;
++		} else {
++			if (++i == ARRAY_SIZE(rq))
++				break;
++
++			goto new_slot;
++		}
++
++		/* Move to requests is staged via the submitted list
++		 * so that we can keep the main request list out of
++		 * the spinlock coverage.
++		 */
++		list_move_tail(&next->engine_link, &engine->submitted);
++	}
++
++	if (rq[0] == NULL)
++		return;
++
++	execlists_submit_pair(engine, rq);
++
++	engine->execlists_submitted++;
++	if (rq[1])
++		engine->execlists_submitted++;
++}
++
++/**
++ * intel_execlists_handle_ctx_events() - handle Context Switch interrupts
++ * @ring: Engine Command Streamer to handle.
++ *
++ * Check the unread Context Status Buffers and manage the submission of new
++ * contexts to the ELSP accordingly.
++ */
++void intel_execlists_irq_handler(struct intel_engine_cs *engine)
++{
++	struct drm_i915_private *dev_priv = engine->i915;
++	unsigned long flags;
++	u8 read_pointer;
++	u8 write_pointer;
++
++	read_pointer = engine->next_context_status_buffer;
++	write_pointer = I915_READ(RING_CONTEXT_STATUS_PTR(engine)) & 0x07;
++	if (read_pointer > write_pointer)
++		write_pointer += 6;
++
++	spin_lock_irqsave(&engine->irqlock, flags);
++
++	while (read_pointer++ < write_pointer) {
++		u32 reg = (RING_CONTEXT_STATUS_BUF(engine) +
++			   (read_pointer % 6) * 8);
++		u32 status = I915_READ(reg);
++
++		if (status & GEN8_CTX_STATUS_PREEMPTED) {
++			if (status & GEN8_CTX_STATUS_LITE_RESTORE)
++				WARN_ONCE(1, "Lite Restored request removed from queue\n");
++			else
++				WARN_ONCE(1, "Preemption without Lite Restore\n");
++		}
++
++		if (status & (GEN8_CTX_STATUS_ACTIVE_IDLE | GEN8_CTX_STATUS_ELEMENT_SWITCH)) {
++			engine->tag = I915_READ(reg + 4);
++			engine->execlists_submitted--;
++		}
++	}
++
++	if (engine->execlists_submitted < 2)
++		execlists_submit(engine);
++
++	spin_unlock_irqrestore(&engine->irqlock, flags);
++
++	engine->next_context_status_buffer = write_pointer % 6;
++	I915_WRITE(RING_CONTEXT_STATUS_PTR(engine),
++		   ((u32)engine->next_context_status_buffer & 0x07) << 8);
++}
++
++static u32 get_lr_context_size(const struct intel_engine_cs *engine)
++{
++	if (engine->id == RCS) {
++		if (INTEL_INFO(engine->i915)->gen >= 9)
++			return ALIGN(GEN9_LR_CONTEXT_RENDER_SIZE,4096);
++		else
++			return ALIGN(GEN8_LR_CONTEXT_RENDER_SIZE, 4096);
++	} else
++		return ALIGN(GEN8_LR_CONTEXT_OTHER_SIZE, 4096);
++}
++
++static int
++populate_lr_context(struct intel_context *ctx,
++		    struct intel_engine_cs *engine,
++		    struct drm_i915_gem_object *state,
++		    struct intel_ringbuffer *ring)
++{
++	struct i915_hw_ppgtt *ppgtt;
++	u32 *regs;
++
++	/* The second page of the context object contains some fields which must
++	 * be set up prior to the first execution. */
++	regs = ctx_get_regs(state);
++	if (IS_ERR(regs))
++		return PTR_ERR(regs);
++
++	/* A context is actually a big batch buffer with several MI_LOAD_REGISTER_IMM
++	 * commands followed by (reg, value) pairs. The values we are setting here are
++	 * only for the first context restore: on a subsequent save, the GPU will
++	 * recreate this batchbuffer with new values (including all the missing
++	 * MI_LOAD_REGISTER_IMM commands that we are not initializing here). */
++	if (engine->id == RCS)
++		regs[CTX_LRI_HEADER_0] = MI_LOAD_REGISTER_IMM(14);
++	else
++		regs[CTX_LRI_HEADER_0] = MI_LOAD_REGISTER_IMM(11);
++	regs[CTX_LRI_HEADER_0] |= MI_LRI_FORCE_POSTED;
++
++	regs[CTX_CONTEXT_CONTROL] = RING_CONTEXT_CONTROL(engine);
++	regs[CTX_CONTEXT_CONTROL+1] =
++			_MASKED_BIT_ENABLE((1<<3) | MI_RESTORE_INHIBIT);
++
++	regs[CTX_RING_HEAD] = RING_HEAD(engine->mmio_base);
++	regs[CTX_RING_HEAD+1] = 0;
++	regs[CTX_RING_TAIL] = RING_TAIL(engine->mmio_base);
++	regs[CTX_RING_TAIL+1] = 0;
++	regs[CTX_RING_BUFFER_START] = RING_START(engine->mmio_base);
++	regs[CTX_RING_BUFFER_CONTROL] = RING_CTL(engine->mmio_base);
++	regs[CTX_RING_BUFFER_CONTROL+1] =
++			((ring->size - PAGE_SIZE) & RING_NR_PAGES) | RING_VALID;
++
++	regs[CTX_BB_HEAD_U] = engine->mmio_base + 0x168;
++	regs[CTX_BB_HEAD_U+1] = 0;
++	regs[CTX_BB_HEAD_L] = engine->mmio_base + 0x140;
++	regs[CTX_BB_HEAD_L+1] = 0;
++	regs[CTX_BB_STATE] = engine->mmio_base + 0x110;
++	regs[CTX_BB_STATE+1] = (1<<5);
++
++	regs[CTX_SECOND_BB_HEAD_U] = engine->mmio_base + 0x11c;
++	regs[CTX_SECOND_BB_HEAD_U+1] = 0;
++	regs[CTX_SECOND_BB_HEAD_L] = engine->mmio_base + 0x114;
++	regs[CTX_SECOND_BB_HEAD_L+1] = 0;
++	regs[CTX_SECOND_BB_STATE] = engine->mmio_base + 0x118;
++	regs[CTX_SECOND_BB_STATE+1] = 0;
++
++	if (engine->id == RCS) {
++		/* TODO: according to BSpec, the register state context
++		 * for CHV does not have these. OTOH, these registers do
++		 * exist in CHV. I'm waiting for a clarification */
++		regs[CTX_BB_PER_CTX_PTR] = engine->mmio_base + 0x1c0;
++		regs[CTX_BB_PER_CTX_PTR+1] = 0;
++		regs[CTX_RCS_INDIRECT_CTX] = engine->mmio_base + 0x1c4;
++		regs[CTX_RCS_INDIRECT_CTX+1] = 0;
++		regs[CTX_RCS_INDIRECT_CTX_OFFSET] = engine->mmio_base + 0x1c8;
++		regs[CTX_RCS_INDIRECT_CTX_OFFSET+1] = 0;
++	}
++
++	regs[CTX_LRI_HEADER_1] = MI_LOAD_REGISTER_IMM(9);
++	regs[CTX_LRI_HEADER_1] |= MI_LRI_FORCE_POSTED;
++	regs[CTX_CTX_TIMESTAMP] = engine->mmio_base + 0x3a8;
++	regs[CTX_CTX_TIMESTAMP+1] = 0;
++
++	regs[CTX_PDP3_UDW] = GEN8_RING_PDP_UDW(engine, 3);
++	regs[CTX_PDP3_LDW] = GEN8_RING_PDP_LDW(engine, 3);
++	regs[CTX_PDP2_UDW] = GEN8_RING_PDP_UDW(engine, 2);
++	regs[CTX_PDP2_LDW] = GEN8_RING_PDP_LDW(engine, 2);
++	regs[CTX_PDP1_UDW] = GEN8_RING_PDP_UDW(engine, 1);
++	regs[CTX_PDP1_LDW] = GEN8_RING_PDP_LDW(engine, 1);
++	regs[CTX_PDP0_UDW] = GEN8_RING_PDP_UDW(engine, 0);
++	regs[CTX_PDP0_LDW] = GEN8_RING_PDP_LDW(engine, 0);
++
++	ppgtt = ctx->ppgtt ?: engine->i915->mm.aliasing_ppgtt;
++	regs[CTX_PDP3_UDW+1] = upper_32_bits(ppgtt->pd_dma_addr[3]);
++	regs[CTX_PDP3_LDW+1] = lower_32_bits(ppgtt->pd_dma_addr[3]);
++	regs[CTX_PDP2_UDW+1] = upper_32_bits(ppgtt->pd_dma_addr[2]);
++	regs[CTX_PDP2_LDW+1] = lower_32_bits(ppgtt->pd_dma_addr[2]);
++	regs[CTX_PDP1_UDW+1] = upper_32_bits(ppgtt->pd_dma_addr[1]);
++	regs[CTX_PDP1_LDW+1] = lower_32_bits(ppgtt->pd_dma_addr[1]);
++	regs[CTX_PDP0_UDW+1] = upper_32_bits(ppgtt->pd_dma_addr[0]);
++	regs[CTX_PDP0_LDW+1] = lower_32_bits(ppgtt->pd_dma_addr[0]);
++
++	if (engine->id == RCS) {
++		regs[CTX_LRI_HEADER_2] = MI_LOAD_REGISTER_IMM(1);
++		regs[CTX_R_PWR_CLK_STATE] = 0x20c8;
++		regs[CTX_R_PWR_CLK_STATE+1] = 0;
++	}
++
++	kunmap_atomic(regs);
++
++	return 0;
++}
++
++static struct intel_ringbuffer *
++execlists_get_ring(struct intel_engine_cs *engine,
++		   struct intel_context *ctx)
++{
++	struct drm_i915_gem_object *state;
++	struct intel_ringbuffer *ring;
++	int ret;
++
++	if (ctx->ring[engine->id].ring)
++		return ctx->ring[engine->id].ring;
++
++	ring = intel_engine_alloc_ring(engine, ctx, 32 * PAGE_SIZE);
++	if (IS_ERR(ring)) {
++		ret = PTR_ERR(ring);
++		goto err;
++	}
++
++	state = i915_gem_alloc_object(engine->i915->dev,
++				      get_lr_context_size(engine));
++	if (IS_ERR(state)) {
++		ret = PTR_ERR(state);
++		goto err_ring;
++	}
++
++	ret = populate_lr_context(ctx, engine, state, ring);
++	if (ret)
++		goto err_ctx;
++
++	/* The status page is offset 0 from the context object in LRCs. */
++	if (ctx == engine->default_context) {
++		ret = i915_gem_object_ggtt_pin(state, 0, 0);
++		if (ret)
++			goto err_ctx;
++
++		engine->status_page.obj = state;
++		drm_gem_object_reference(&state->base);
++
++		engine->status_page.gfx_addr = i915_gem_obj_ggtt_offset(state);
++		engine->status_page.page_addr = kmap(i915_gem_object_get_page(state, 0));
++	}
++
++	ctx->ring[engine->id].state = state;
++	ctx->ring[engine->id].ring = ring;
++	return ring;
++
++err_ctx:
++	drm_gem_object_unreference(&state->base);
++err_ring:
++	intel_ring_free(ring);
++err:
++	DRM_DEBUG_DRIVER("Failed to allocate ring %s %s: %d\n",
++			 engine->name,
++			 ctx == engine->default_context ? "(default)" : "",
++			 ret);
++	return ERR_PTR(ret);
++}
++
++static struct intel_ringbuffer *
++execlists_pin_context(struct intel_engine_cs *engine,
++		      struct intel_context *ctx)
++{
++	struct intel_engine_context *hw = &ctx->ring[engine->id];
++	struct intel_ringbuffer *ring;
++	u32 ggtt_offset;
++	int ret;
++
++	ring = execlists_get_ring(engine, ctx);
++	if (IS_ERR(ring))
++		return ring;
++
++	ret = i915_gem_object_ggtt_pin(hw->state, 0, 0);
++	if (ret)
++		goto err;
++
++	if (ctx->ppgtt && ctx->ppgtt->state) {
++		ret = i915_gem_object_ggtt_pin(ctx->ppgtt->state, 0, 0);
++		if (ret)
++			goto err_unpin_ctx;
++	}
++
++	ret = i915_gem_object_ggtt_pin(ring->obj, 0, 0);
++	if (ret)
++		goto err_unpin_mm;
++
++	ggtt_offset = i915_gem_obj_ggtt_offset(ring->obj);
++	if (ring->ggtt_offset != ggtt_offset) {
++		u32 *regs = ctx_get_regs(hw->state);
++		if (IS_ERR(regs)) {
++			ret = PTR_ERR(regs);
++			goto err_unpin_ring;
++		}
++
++		regs[CTX_RING_BUFFER_START+1] = ggtt_offset;
++		kunmap_atomic(regs);
++
++		ring->ggtt_offset = ggtt_offset;
++	}
++	return ring;
++
++err_unpin_ring:
++	i915_gem_object_ggtt_unpin(ring->obj);
++err_unpin_mm:
++	if (ctx->ppgtt && ctx->ppgtt->state)
++		i915_gem_object_ggtt_unpin(ctx->ppgtt->state);
++err_unpin_ctx:
++	i915_gem_object_ggtt_unpin(hw->state);
++err:
++	return ERR_PTR(ret);
++}
++
++static void
++rq_add_ggtt(struct i915_gem_request *rq, struct drm_i915_gem_object *obj)
++{
++	obj->base.pending_read_domains = I915_GEM_DOMAIN_INSTRUCTION;
++	/* obj is kept alive until the next request by its active ref */
++	drm_gem_object_reference(&obj->base);
++	i915_request_add_vma(rq, i915_gem_obj_get_ggtt(obj), 0);
++}
++
++static void execlists_add_context(struct i915_gem_request *rq,
++				  struct intel_context *ctx)
++{
++	rq_add_ggtt(rq, ctx->ring[rq->engine->id].ring->obj);
++	if (ctx->ppgtt && ctx->ppgtt->state)
++		rq_add_ggtt(rq, ctx->ppgtt->state);
++	rq_add_ggtt(rq, ctx->ring[rq->engine->id].state);
++}
++
++static void
++execlists_unpin_context(struct intel_engine_cs *engine,
++			struct intel_context *ctx)
++{
++	i915_gem_object_ggtt_unpin(ctx->ring[engine->id].ring->obj);
++	if (ctx->ppgtt && ctx->ppgtt->state)
++		i915_gem_object_ggtt_unpin(ctx->ppgtt->state);
++	i915_gem_object_ggtt_unpin(ctx->ring[engine->id].state);
++}
++
++static void execlists_free_context(struct intel_engine_cs *engine,
++				   struct intel_context *ctx)
++{
++	if (ctx->ring[engine->id].ring)
++		intel_ring_free(ctx->ring[engine->id].ring);
++	if (ctx->ring[engine->id].state)
++		drm_gem_object_unreference(&ctx->ring[engine->id].state->base);
++}
++
++static int execlists_add_request(struct i915_gem_request *rq)
++{
++	unsigned long flags;
++
++	spin_lock_irqsave(&rq->engine->irqlock, flags);
++
++	list_add_tail(&rq->engine_link, &rq->engine->pending);
++	if (rq->engine->execlists_submitted < 2)
++		execlists_submit(rq->engine);
++
++	spin_unlock_irqrestore(&rq->engine->irqlock, flags);
++
++	return 0;
++}
++
++static bool execlists_rq_is_complete(struct i915_gem_request *rq)
++{
++	return (s16)(rq->engine->tag - rq->tag) >= 0;
++}
++
++static int execlists_suspend(struct intel_engine_cs *engine)
++{
++	struct drm_i915_private *dev_priv = engine->i915;
++	unsigned long flags;
++
++	/* disable submitting more requests until resume */
++	spin_lock_irqsave(&engine->irqlock, flags);
++	engine->execlists_submitted = ~0;
++	spin_unlock_irqrestore(&engine->irqlock, flags);
++
++	I915_WRITE(RING_MODE_GEN7(engine),
++		   _MASKED_BIT_ENABLE(GFX_REPLAY_MODE) |
++		   _MASKED_BIT_DISABLE(GFX_RUN_LIST_ENABLE));
++	POSTING_READ(RING_MODE_GEN7(engine));
++	DRM_DEBUG_DRIVER("Execlists disabled for %s\n", engine->name);
++
++	return 0;
++}
++
++static int execlists_resume(struct intel_engine_cs *engine)
++{
++	struct drm_i915_private *dev_priv = engine->i915;
++	unsigned long flags;
++
++	I915_WRITE(RING_HWS_PGA(engine->mmio_base),
++		   engine->status_page.gfx_addr);
++
++	/* XXX */
++	I915_WRITE(RING_HWSTAM(engine->mmio_base), 0xffffffff);
++	I915_WRITE(INSTPM, _MASKED_BIT_ENABLE(INSTPM_FORCE_ORDERING));
++
++	/* We need to disable the AsyncFlip performance optimisations in order
++	 * to use MI_WAIT_FOR_EVENT within the CS. It should already be
++	 * programmed to '1' on all products.
++	 *
++	 * WaDisableAsyncFlipPerfMode:bdw
++	 */
++	I915_WRITE(MI_MODE, _MASKED_BIT_ENABLE(ASYNC_FLIP_PERF_DISABLE));
++
++	I915_WRITE(RING_MODE_GEN7(engine),
++		   _MASKED_BIT_DISABLE(GFX_REPLAY_MODE) |
++		   _MASKED_BIT_ENABLE(GFX_RUN_LIST_ENABLE));
++	POSTING_READ(RING_MODE_GEN7(engine));
++	DRM_DEBUG_DRIVER("Execlists enabled for %s\n", engine->name);
++
++	spin_lock_irqsave(&engine->irqlock, flags);
++	engine->execlists_submitted = 0;
++	execlists_submit(engine);
++	spin_unlock_irqrestore(&engine->irqlock, flags);
++
++	return 0;
++}
++
++static void execlists_retire(struct intel_engine_cs *engine,
++			     u32 seqno)
++{
++	unsigned long flags;
++
++	spin_lock_irqsave(&engine->irqlock, flags);
++	list_splice_tail_init(&engine->submitted, &engine->requests);
++	spin_unlock_irqrestore(&engine->irqlock, flags);
++}
++
++static void execlists_reset(struct intel_engine_cs *engine)
++{
++	unsigned long flags;
++
++	spin_lock_irqsave(&engine->irqlock, flags);
++	list_splice_tail_init(&engine->pending, &engine->submitted);
++	list_splice_tail_init(&engine->submitted, &engine->requests);
++	spin_unlock_irqrestore(&engine->irqlock, flags);
++}
++
++static bool enable_execlists(struct drm_i915_private *dev_priv)
++{
++	if (!HAS_LOGICAL_RING_CONTEXTS(dev_priv) ||
++	    !USES_PPGTT(dev_priv))
++		return false;
++
++	return i915_module.enable_execlists;
++}
++
++static const int gen8_irq_shift[] = {
++	[RCS] = GEN8_RCS_IRQ_SHIFT,
++	[VCS] = GEN8_VCS1_IRQ_SHIFT,
++	[BCS] = GEN8_BCS_IRQ_SHIFT,
++	[VECS] = GEN8_VECS_IRQ_SHIFT,
++	[VCS2] = GEN8_VCS2_IRQ_SHIFT,
++};
++
++int intel_engine_enable_execlists(struct intel_engine_cs *engine)
++{
++	if (!enable_execlists(engine->i915))
++		return 0;
++
++	if (WARN_ON(!IS_GEN8(engine->i915)))
++		return 0;
++
++	engine->irq_keep_mask |=
++		GT_CONTEXT_SWITCH_INTERRUPT << gen8_irq_shift[engine->id];
++
++	engine->pin_context = execlists_pin_context;
++	engine->add_context = execlists_add_context;
++	engine->unpin_context = execlists_unpin_context;
++	engine->free_context = execlists_free_context;
++
++	engine->add_request = execlists_add_request;
++	engine->is_complete = execlists_rq_is_complete;
++
++	/* Disable semaphores until further notice */
++	engine->semaphore.wait = NULL;
++
++	engine->suspend = execlists_suspend;
++	engine->resume = execlists_resume;
++	engine->reset = execlists_reset;
++	engine->retire = execlists_retire;
++
++	/* start suspended */
++	engine->execlists_enabled = true;
++	engine->execlists_submitted = ~0;
++
++	return 0;
++}
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_lrc.h b/drivers/gpu/drm/i915/intel_lrc.h
+--- a/drivers/gpu/drm/i915/intel_lrc.h	1969-12-31 17:00:00.000000000 -0700
++++ b/drivers/gpu/drm/i915/intel_lrc.h	2014-11-20 09:53:37.988762837 -0700
+@@ -0,0 +1,38 @@
++/*
++ * Copyright  2014 Intel Corporation
++ *
++ * Permission is hereby granted, free of charge, to any person obtaining a
++ * copy of this software and associated documentation files (the "Software"),
++ * to deal in the Software without restriction, including without limitation
++ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
++ * and/or sell copies of the Software, and to permit persons to whom the
++ * Software is furnished to do so, subject to the following conditions:
++ *
++ * The above copyright notice and this permission notice (including the next
++ * paragraph) shall be included in all copies or substantial portions of the
++ * Software.
++ *
++ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
++ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
++ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
++ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
++ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
++ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
++ * DEALINGS IN THE SOFTWARE.
++ */
++
++#ifndef _INTEL_LRC_H_
++#define _INTEL_LRC_H_
++
++/* Execlists regs */
++#define RING_ELSP(ring)			((ring)->mmio_base+0x230)
++#define RING_EXECLIST_STATUS(ring)	((ring)->mmio_base+0x234)
++#define RING_CONTEXT_CONTROL(ring)	((ring)->mmio_base+0x244)
++#define RING_CONTEXT_STATUS_BUF(ring)	((ring)->mmio_base+0x370)
++#define RING_CONTEXT_STATUS_PTR(ring)	((ring)->mmio_base+0x3a0)
++
++/* Execlists */
++int intel_engine_enable_execlists(struct intel_engine_cs *engine);
++void intel_execlists_irq_handler(struct intel_engine_cs *engine);
++
++#endif /* _INTEL_LRC_H_ */
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_lvds.c b/drivers/gpu/drm/i915/intel_lvds.c
+--- a/drivers/gpu/drm/i915/intel_lvds.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_lvds.c	2014-11-20 09:53:37.988762837 -0700
+@@ -76,7 +76,7 @@
+ 	u32 tmp;
+ 
+ 	power_domain = intel_display_port_power_domain(encoder);
+-	if (!intel_display_power_enabled(dev_priv, power_domain))
++	if (!intel_display_power_is_enabled(dev_priv, power_domain))
+ 		return false;
+ 
+ 	tmp = I915_READ(lvds_encoder->reg);
+@@ -823,8 +823,7 @@
+ 	struct intel_encoder *encoder;
+ 	struct intel_lvds_encoder *lvds_encoder;
+ 
+-	list_for_each_entry(encoder, &dev->mode_config.encoder_list,
+-			    base.head) {
++	for_each_intel_encoder(dev, encoder) {
+ 		if (encoder->type == INTEL_OUTPUT_LVDS) {
+ 			lvds_encoder = to_lvds_encoder(&encoder->base);
+ 
+@@ -842,8 +841,8 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+ 	/* use the module option value if specified */
+-	if (i915.lvds_channel_mode > 0)
+-		return i915.lvds_channel_mode == 2;
++	if (i915_module.lvds_channel_mode > 0)
++		return i915_module.lvds_channel_mode == 2;
+ 
+ 	if (dmi_check_system(intel_dual_link_lvds))
+ 		return true;
+@@ -976,10 +975,10 @@
+ 	}
+ 
+ 	/* create the scaling mode property */
+-	drm_mode_create_scaling_mode_property(dev);
+-	drm_object_attach_property(&connector->base,
+-				      dev->mode_config.scaling_mode_property,
+-				      DRM_MODE_SCALE_ASPECT);
++	if (drm_mode_create_scaling_mode_property(dev) == 0)
++		drm_object_attach_property(&connector->base,
++					   dev->mode_config.scaling_mode_property,
++					   DRM_MODE_SCALE_ASPECT);
+ 	intel_connector->panel.fitting_mode = DRM_MODE_SCALE_ASPECT;
+ 	/*
+ 	 * LVDS discovery:
+@@ -1032,7 +1031,7 @@
+ 					intel_find_panel_downclock(dev,
+ 					fixed_mode, connector);
+ 				if (downclock_mode != NULL &&
+-					i915.lvds_downclock) {
++					i915_module.lvds_downclock) {
+ 					/* We found the downclock for LVDS. */
+ 					dev_priv->lvds_downclock_avail = true;
+ 					dev_priv->lvds_downclock =
+@@ -1117,7 +1116,7 @@
+ 	drm_connector_register(connector);
+ 
+ 	intel_panel_init(&intel_connector->panel, fixed_mode, downclock_mode);
+-	intel_panel_setup_backlight(connector);
++	intel_panel_setup_backlight(connector, INVALID_PIPE);
+ 
+ 	return;
+ 
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_modes.c b/drivers/gpu/drm/i915/intel_modes.c
+--- a/drivers/gpu/drm/i915/intel_modes.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_modes.c	2014-11-20 09:53:37.992762837 -0700
+@@ -126,3 +126,32 @@
+ 
+ 	drm_object_attach_property(&connector->base, prop, 0);
+ }
++
++static const struct drm_prop_enum_list psr_names[] = {
++	{ -1, "Unsupported" },
++	{  0, "Idle" },
++	{  1, "Active" },
++};
++
++void
++intel_attach_psr_property(struct drm_connector *connector)
++{
++	struct drm_device *dev = connector->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_property *prop;
++
++	prop = dev_priv->psr.property;
++	if (prop == NULL) {
++		prop = drm_property_create_enum(dev,
++						DRM_MODE_PROP_ENUM | DRM_MODE_PROP_IMMUTABLE,
++						"Panel Self-Refresh",
++						psr_names,
++						ARRAY_SIZE(psr_names));
++		if (prop == NULL)
++			return;
++
++		dev_priv->psr.property = prop;
++	}
++
++	drm_object_attach_property(&connector->base, prop, -1);
++}
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_overlay.c b/drivers/gpu/drm/i915/intel_overlay.c
+--- a/drivers/gpu/drm/i915/intel_overlay.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_overlay.c	2014-11-20 09:53:37.992762837 -0700
+@@ -175,14 +175,15 @@
+ 	int active;
+ 	int pfit_active;
+ 	u32 pfit_vscale_ratio; /* shifted-point number, (1<<12) == 1.0 */
+-	u32 color_key;
++	u32 color_key:24;
++	u32 color_key_enabled:1;
+ 	u32 brightness, contrast, saturation;
+ 	u32 old_xscale, old_yscale;
+ 	/* register access */
+ 	u32 flip_addr;
+ 	struct drm_i915_gem_object *reg_bo;
+ 	/* flip handling */
+-	uint32_t last_flip_req;
++	struct i915_gem_request *flip_request;
+ 	void (*flip_tail)(struct intel_overlay *);
+ };
+ 
+@@ -208,53 +209,85 @@
+ 		io_mapping_unmap(regs);
+ }
+ 
+-static int intel_overlay_do_wait_request(struct intel_overlay *overlay,
+-					 void (*tail)(struct intel_overlay *))
++/* recover from an interruption due to a signal
++ * We have to be careful not to repeat work forever an make forward progess. */
++static int intel_overlay_recover_from_interrupt(struct intel_overlay *overlay)
+ {
+-	struct drm_device *dev = overlay->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring = &dev_priv->ring[RCS];
+ 	int ret;
+ 
+-	BUG_ON(overlay->last_flip_req);
+-	ret = i915_add_request(ring, &overlay->last_flip_req);
+-	if (ret)
+-		return ret;
++	if (overlay->flip_request == NULL)
++		return 0;
+ 
+-	overlay->flip_tail = tail;
+-	ret = i915_wait_seqno(ring, overlay->last_flip_req);
++	ret = i915_request_wait(overlay->flip_request);
+ 	if (ret)
+ 		return ret;
+-	i915_gem_retire_requests(dev);
+ 
+-	overlay->last_flip_req = 0;
++	i915_request_put(overlay->flip_request);
++	overlay->flip_request = NULL;
++
++	i915_gem_retire_requests(overlay->dev);
++
++	if (overlay->flip_tail)
++		overlay->flip_tail(overlay);
++
+ 	return 0;
+ }
+ 
++static int intel_overlay_add_request(struct intel_overlay *overlay,
++				     struct i915_gem_request *rq,
++				     void (*tail)(struct intel_overlay *))
++{
++	BUG_ON(overlay->flip_request);
++	overlay->flip_request = rq;
++	overlay->flip_tail = tail;
++
++	return i915_request_commit(rq);
++}
++
++static int intel_overlay_do_wait_request(struct intel_overlay *overlay,
++					 struct i915_gem_request *rq,
++					 void (*tail)(struct intel_overlay *))
++{
++	intel_overlay_add_request(overlay, rq, tail);
++	return intel_overlay_recover_from_interrupt(overlay);
++}
++
++static struct i915_gem_request *
++intel_overlay_alloc_request(struct intel_overlay *overlay)
++{
++	struct intel_engine_cs *rcs = RCS_ENGINE(overlay->dev);
++	return i915_request_create(rcs->default_context, rcs);
++}
++
+ /* overlay needs to be disable in OCMD reg */
+ static int intel_overlay_on(struct intel_overlay *overlay)
+ {
+ 	struct drm_device *dev = overlay->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring = &dev_priv->ring[RCS];
+-	int ret;
++	struct i915_gem_request *rq;
++	struct intel_ringbuffer *ring;
+ 
+ 	BUG_ON(overlay->active);
+ 	overlay->active = 1;
+ 
+ 	WARN_ON(IS_I830(dev) && !(dev_priv->quirks & QUIRK_PIPEA_FORCE));
+ 
+-	ret = intel_ring_begin(ring, 4);
+-	if (ret)
+-		return ret;
++	rq = intel_overlay_alloc_request(overlay);
++	if (IS_ERR(rq))
++		return PTR_ERR(rq);
++
++	ring = intel_ring_begin(rq, 3);
++	if (IS_ERR(ring)) {
++		i915_request_put(rq);
++		return PTR_ERR(ring);
++	}
+ 
+ 	intel_ring_emit(ring, MI_OVERLAY_FLIP | MI_OVERLAY_ON);
+ 	intel_ring_emit(ring, overlay->flip_addr | OFC_UPDATE);
+ 	intel_ring_emit(ring, MI_WAIT_FOR_EVENT | MI_WAIT_FOR_OVERLAY_FLIP);
+-	intel_ring_emit(ring, MI_NOOP);
+ 	intel_ring_advance(ring);
+ 
+-	return intel_overlay_do_wait_request(overlay, NULL);
++	return intel_overlay_do_wait_request(overlay, rq, NULL);
+ }
+ 
+ /* overlay needs to be enabled in OCMD reg */
+@@ -263,10 +296,10 @@
+ {
+ 	struct drm_device *dev = overlay->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring = &dev_priv->ring[RCS];
+ 	u32 flip_addr = overlay->flip_addr;
++	struct i915_gem_request *rq;
++	struct intel_ringbuffer *ring;
+ 	u32 tmp;
+-	int ret;
+ 
+ 	BUG_ON(!overlay->active);
+ 
+@@ -278,21 +311,30 @@
+ 	if (tmp & (1 << 17))
+ 		DRM_DEBUG("overlay underrun, DOVSTA: %x\n", tmp);
+ 
+-	ret = intel_ring_begin(ring, 2);
+-	if (ret)
+-		return ret;
++	rq = intel_overlay_alloc_request(overlay);
++	if (IS_ERR(rq))
++		return PTR_ERR(rq);
++
++	ring = intel_ring_begin(rq, 2);
++	if (IS_ERR(ring)) {
++		i915_request_put(rq);
++		return PTR_ERR(ring);
++	}
+ 
+ 	intel_ring_emit(ring, MI_OVERLAY_FLIP | MI_OVERLAY_CONTINUE);
+ 	intel_ring_emit(ring, flip_addr);
+ 	intel_ring_advance(ring);
+ 
+-	return i915_add_request(ring, &overlay->last_flip_req);
++	return intel_overlay_add_request(overlay, rq, NULL);
+ }
+ 
+ static void intel_overlay_release_old_vid_tail(struct intel_overlay *overlay)
+ {
+ 	struct drm_i915_gem_object *obj = overlay->old_vid_bo;
+ 
++	i915_gem_track_fb(obj, NULL,
++			  INTEL_FRONTBUFFER_OVERLAY(overlay->crtc->pipe));
++
+ 	i915_gem_object_ggtt_unpin(obj);
+ 	drm_gem_object_unreference(&obj->base);
+ 
+@@ -319,10 +361,10 @@
+ static int intel_overlay_off(struct intel_overlay *overlay)
+ {
+ 	struct drm_device *dev = overlay->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring = &dev_priv->ring[RCS];
+ 	u32 flip_addr = overlay->flip_addr;
+-	int ret;
++	struct i915_gem_request *rq;
++	struct intel_ringbuffer *ring;
++	int len;
+ 
+ 	BUG_ON(!overlay->active);
+ 
+@@ -332,53 +374,36 @@
+ 	 * of the hw. Do it in both cases */
+ 	flip_addr |= OFC_UPDATE;
+ 
+-	ret = intel_ring_begin(ring, 6);
+-	if (ret)
+-		return ret;
++	rq = intel_overlay_alloc_request(overlay);
++	if (IS_ERR(rq))
++		return PTR_ERR(rq);
++
++	len = 3;
++	if (!IS_I830(dev))
++		len += 3;
++
++	ring = intel_ring_begin(rq, len);
++	if (IS_ERR(ring)) {
++		i915_request_put(rq);
++		return PTR_ERR(ring);
++	}
+ 
+ 	/* wait for overlay to go idle */
+ 	intel_ring_emit(ring, MI_OVERLAY_FLIP | MI_OVERLAY_CONTINUE);
+ 	intel_ring_emit(ring, flip_addr);
+ 	intel_ring_emit(ring, MI_WAIT_FOR_EVENT | MI_WAIT_FOR_OVERLAY_FLIP);
+-	/* turn overlay off */
+-	if (IS_I830(dev)) {
+-		/* Workaround: Don't disable the overlay fully, since otherwise
+-		 * it dies on the next OVERLAY_ON cmd. */
+-		intel_ring_emit(ring, MI_NOOP);
+-		intel_ring_emit(ring, MI_NOOP);
+-		intel_ring_emit(ring, MI_NOOP);
+-	} else {
++	/* turn overlay off
++	 * Workaround for i830: Don't disable the overlay fully, since
++	 * otherwise it dies on the next OVERLAY_ON cmd.
++	 */
++	if (!IS_I830(dev)) {
+ 		intel_ring_emit(ring, MI_OVERLAY_FLIP | MI_OVERLAY_OFF);
+ 		intel_ring_emit(ring, flip_addr);
+ 		intel_ring_emit(ring, MI_WAIT_FOR_EVENT | MI_WAIT_FOR_OVERLAY_FLIP);
+ 	}
+ 	intel_ring_advance(ring);
+ 
+-	return intel_overlay_do_wait_request(overlay, intel_overlay_off_tail);
+-}
+-
+-/* recover from an interruption due to a signal
+- * We have to be careful not to repeat work forever an make forward progess. */
+-static int intel_overlay_recover_from_interrupt(struct intel_overlay *overlay)
+-{
+-	struct drm_device *dev = overlay->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring = &dev_priv->ring[RCS];
+-	int ret;
+-
+-	if (overlay->last_flip_req == 0)
+-		return 0;
+-
+-	ret = i915_wait_seqno(ring, overlay->last_flip_req);
+-	if (ret)
+-		return ret;
+-	i915_gem_retire_requests(dev);
+-
+-	if (overlay->flip_tail)
+-		overlay->flip_tail(overlay);
+-
+-	overlay->last_flip_req = 0;
+-	return 0;
++	return intel_overlay_do_wait_request(overlay, rq, intel_overlay_off_tail);
+ }
+ 
+ /* Wait for pending overlay flip and release old frame.
+@@ -387,10 +412,8 @@
+  */
+ static int intel_overlay_release_old_vid(struct intel_overlay *overlay)
+ {
+-	struct drm_device *dev = overlay->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring = &dev_priv->ring[RCS];
+-	int ret;
++	struct drm_i915_private *dev_priv = to_i915(overlay->dev);
++	int ret = 0;
+ 
+ 	/* Only wait if there is actually an old frame to release to
+ 	 * guarantee forward progress.
+@@ -399,27 +422,29 @@
+ 		return 0;
+ 
+ 	if (I915_READ(ISR) & I915_OVERLAY_PLANE_FLIP_PENDING_INTERRUPT) {
++		struct i915_gem_request *rq;
++		struct intel_ringbuffer *ring;
++
++		rq = intel_overlay_alloc_request(overlay);
++		if (IS_ERR(rq))
++			return PTR_ERR(rq);
++
+ 		/* synchronous slowpath */
+-		ret = intel_ring_begin(ring, 2);
+-		if (ret)
+-			return ret;
++		ring = intel_ring_begin(rq, 1);
++		if (IS_ERR(ring)) {
++			i915_request_put(rq);
++			return PTR_ERR(ring);
++		}
+ 
+ 		intel_ring_emit(ring, MI_WAIT_FOR_EVENT | MI_WAIT_FOR_OVERLAY_FLIP);
+-		intel_ring_emit(ring, MI_NOOP);
+ 		intel_ring_advance(ring);
+ 
+-		ret = intel_overlay_do_wait_request(overlay,
++		ret = intel_overlay_do_wait_request(overlay, rq,
+ 						    intel_overlay_release_old_vid_tail);
+-		if (ret)
+-			return ret;
+-	}
+-
+-	intel_overlay_release_old_vid_tail(overlay);
++	} else
++		intel_overlay_release_old_vid_tail(overlay);
+ 
+-
+-	i915_gem_track_fb(overlay->old_vid_bo, NULL,
+-			  INTEL_FRONTBUFFER_OVERLAY(overlay->crtc->pipe));
+-	return 0;
++	return ret;
+ }
+ 
+ struct put_image_params {
+@@ -609,31 +634,36 @@
+ 			    struct overlay_registers __iomem *regs)
+ {
+ 	u32 key = overlay->color_key;
++	u32 flags;
++
++	flags = 0;
++	if (overlay->color_key_enabled)
++		flags |= DST_KEY_ENABLE;
+ 
+ 	switch (overlay->crtc->base.primary->fb->bits_per_pixel) {
+ 	case 8:
+-		iowrite32(0, &regs->DCLRKV);
+-		iowrite32(CLK_RGB8I_MASK | DST_KEY_ENABLE, &regs->DCLRKM);
++		key = 0;
++		flags |= CLK_RGB8I_MASK;
+ 		break;
+ 
+ 	case 16:
+ 		if (overlay->crtc->base.primary->fb->depth == 15) {
+-			iowrite32(RGB15_TO_COLORKEY(key), &regs->DCLRKV);
+-			iowrite32(CLK_RGB15_MASK | DST_KEY_ENABLE,
+-				  &regs->DCLRKM);
++			key = RGB15_TO_COLORKEY(key);
++			flags |= CLK_RGB15_MASK;
+ 		} else {
+-			iowrite32(RGB16_TO_COLORKEY(key), &regs->DCLRKV);
+-			iowrite32(CLK_RGB16_MASK | DST_KEY_ENABLE,
+-				  &regs->DCLRKM);
++			key = RGB16_TO_COLORKEY(key);
++			flags |= CLK_RGB16_MASK;
+ 		}
+ 		break;
+ 
+ 	case 24:
+ 	case 32:
+-		iowrite32(key, &regs->DCLRKV);
+-		iowrite32(CLK_RGB24_MASK | DST_KEY_ENABLE, &regs->DCLRKM);
++		flags |= CLK_RGB24_MASK;
+ 		break;
+ 	}
++
++	iowrite32(key, &regs->DCLRKV);
++	iowrite32(flags, &regs->DCLRKM);
+ }
+ 
+ static u32 overlay_cmd_reg(struct put_image_params *params)
+@@ -821,12 +851,7 @@
+ 	iowrite32(0, &regs->OCMD);
+ 	intel_overlay_unmap_regs(overlay, regs);
+ 
+-	ret = intel_overlay_off(overlay);
+-	if (ret != 0)
+-		return ret;
+-
+-	intel_overlay_off_tail(overlay);
+-	return 0;
++	return intel_overlay_off(overlay);
+ }
+ 
+ static int check_overlay_possible_on_crtc(struct intel_overlay *overlay,
+@@ -1310,6 +1335,7 @@
+ 			I915_WRITE(OGAMC5, attrs->gamma5);
+ 		}
+ 	}
++	overlay->color_key_enabled = (attrs->flags & I915_OVERLAY_DISABLE_DEST_COLORKEY) == 0;
+ 
+ 	ret = 0;
+ out_unlock:
+@@ -1357,7 +1383,7 @@
+ 		}
+ 		overlay->flip_addr = reg_bo->phys_handle->busaddr;
+ 	} else {
+-		ret = i915_gem_obj_ggtt_pin(reg_bo, PAGE_SIZE, PIN_MAPPABLE);
++		ret = i915_gem_object_ggtt_pin(reg_bo, PAGE_SIZE, PIN_MAPPABLE);
+ 		if (ret) {
+ 			DRM_ERROR("failed to pin overlay register bo\n");
+ 			goto out_free_bo;
+@@ -1373,6 +1399,7 @@
+ 
+ 	/* init all values */
+ 	overlay->color_key = 0x0101fe;
++	overlay->color_key_enabled = true;
+ 	overlay->brightness = -19;
+ 	overlay->contrast = 75;
+ 	overlay->saturation = 146;
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_panel.c b/drivers/gpu/drm/i915/intel_panel.c
+--- a/drivers/gpu/drm/i915/intel_panel.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_panel.c	2014-11-20 09:53:37.992762837 -0700
+@@ -382,13 +382,13 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+ 	/* Assume that the BIOS does not lie through the OpRegion... */
+-	if (!i915.panel_ignore_lid && dev_priv->opregion.lid_state) {
++	if (!i915_module.panel_ignore_lid && dev_priv->opregion.lid_state) {
+ 		return ioread32(dev_priv->opregion.lid_state) & 0x1 ?
+ 			connector_status_connected :
+ 			connector_status_disconnected;
+ 	}
+ 
+-	switch (i915.panel_ignore_lid) {
++	switch (i915_module.panel_ignore_lid) {
+ 	case -2:
+ 		return connector_status_connected;
+ 	case -1:
+@@ -419,9 +419,8 @@
+ 	source_val = clamp(source_val, source_min, source_max);
+ 
+ 	/* avoid overflows */
+-	target_val = (uint64_t)(source_val - source_min) *
+-		(target_max - target_min);
+-	do_div(target_val, source_max - source_min);
++	target_val = DIV_ROUND_CLOSEST_ULL((uint64_t)(source_val - source_min) *
++			(target_max - target_min), source_max - source_min);
+ 	target_val += target_min;
+ 
+ 	return target_val;
+@@ -470,10 +469,10 @@
+ 
+ 	WARN_ON(panel->backlight.max == 0);
+ 
+-	if (i915.invert_brightness < 0)
++	if (i915_module.invert_brightness < 0)
+ 		return val;
+ 
+-	if (i915.invert_brightness > 0 ||
++	if (i915_module.invert_brightness > 0 ||
+ 	    dev_priv->quirks & QUIRK_INVERT_BRIGHTNESS) {
+ 		return panel->backlight.max - val;
+ 	}
+@@ -522,6 +521,9 @@
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
++	if (WARN_ON(pipe != PIPE_A && pipe != PIPE_B))
++		return 0;
++
+ 	return I915_READ(VLV_BLC_PWM_CTL(pipe)) & BACKLIGHT_DUTY_CYCLE_MASK;
+ }
+ 
+@@ -537,15 +539,17 @@
+ {
+ 	struct drm_device *dev = connector->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	u32 val;
+-	unsigned long flags;
++	struct intel_panel *panel = &connector->panel;
++	u32 val = 0;
+ 
+-	spin_lock_irqsave(&dev_priv->backlight_lock, flags);
++	mutex_lock(&dev_priv->backlight_lock);
+ 
+-	val = dev_priv->display.get_backlight(connector);
+-	val = intel_panel_compute_brightness(connector, val);
++	if (panel->backlight.enabled) {
++		val = dev_priv->display.get_backlight(connector);
++		val = intel_panel_compute_brightness(connector, val);
++	}
+ 
+-	spin_unlock_irqrestore(&dev_priv->backlight_lock, flags);
++	mutex_unlock(&dev_priv->backlight_lock);
+ 
+ 	DRM_DEBUG_DRIVER("get backlight PWM = %d\n", val);
+ 	return val;
+@@ -604,6 +608,9 @@
+ 	enum pipe pipe = intel_get_pipe_from_connector(connector);
+ 	u32 tmp;
+ 
++	if (WARN_ON(pipe != PIPE_A && pipe != PIPE_B))
++		return;
++
+ 	tmp = I915_READ(VLV_BLC_PWM_CTL(pipe)) & ~BACKLIGHT_DUTY_CYCLE_MASK;
+ 	I915_WRITE(VLV_BLC_PWM_CTL(pipe), tmp | level);
+ }
+@@ -627,24 +634,25 @@
+ 	struct drm_device *dev = connector->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct intel_panel *panel = &connector->panel;
+-	enum pipe pipe = intel_get_pipe_from_connector(connector);
+ 	u32 hw_level;
+-	unsigned long flags;
+ 
+-	if (!panel->backlight.present || pipe == INVALID_PIPE)
++	if (!panel->backlight.present)
+ 		return;
+ 
+-	spin_lock_irqsave(&dev_priv->backlight_lock, flags);
++	mutex_lock(&dev_priv->backlight_lock);
+ 
+ 	WARN_ON(panel->backlight.max == 0);
+ 
+ 	hw_level = scale_user_to_hw(connector, user_level, user_max);
+ 	panel->backlight.level = hw_level;
+ 
++	DRM_DEBUG_KMS("user level = %d/%d, backlight = %d\n",
++		      user_level, user_max, hw_level);
++
+ 	if (panel->backlight.enabled)
+ 		intel_panel_actually_set_backlight(connector, hw_level);
+ 
+-	spin_unlock_irqrestore(&dev_priv->backlight_lock, flags);
++	mutex_unlock(&dev_priv->backlight_lock);
+ }
+ 
+ /* set backlight brightness to level in range [0..max], assuming hw min is
+@@ -658,28 +666,40 @@
+ 	struct intel_panel *panel = &connector->panel;
+ 	enum pipe pipe = intel_get_pipe_from_connector(connector);
+ 	u32 hw_level;
+-	unsigned long flags;
+ 
++	/*
++	 * INVALID_PIPE may occur during driver init because
++	 * connection_mutex isn't held across the entire backlight
++	 * setup + modeset readout, and the BIOS can issue the
++	 * requests at any time.
++	 */
+ 	if (!panel->backlight.present || pipe == INVALID_PIPE)
+ 		return;
+ 
+-	spin_lock_irqsave(&dev_priv->backlight_lock, flags);
++	mutex_lock(&dev_priv->backlight_lock);
+ 
+ 	WARN_ON(panel->backlight.max == 0);
+ 
+ 	hw_level = clamp_user_to_hw(connector, user_level, user_max);
+ 	panel->backlight.level = hw_level;
+ 
+-	if (panel->backlight.device)
++	DRM_DEBUG_KMS("user level = %d/%d, backlight = %d\n",
++		      user_level, user_max, hw_level);
++
++	if (panel->backlight.device) {
+ 		panel->backlight.device->props.brightness =
+ 			scale_hw_to_user(connector,
+ 					 panel->backlight.level,
+ 					 panel->backlight.device->props.max_brightness);
++		DRM_DEBUG_KMS("brightness = %d/%d\n",
++			      panel->backlight.device->props.brightness,
++			      panel->backlight.device->props.max_brightness);
++	}
+ 
+ 	if (panel->backlight.enabled)
+ 		intel_panel_actually_set_backlight(connector, hw_level);
+ 
+-	spin_unlock_irqrestore(&dev_priv->backlight_lock, flags);
++	mutex_unlock(&dev_priv->backlight_lock);
+ }
+ 
+ static void pch_disable_backlight(struct intel_connector *connector)
+@@ -721,6 +741,9 @@
+ 	enum pipe pipe = intel_get_pipe_from_connector(connector);
+ 	u32 tmp;
+ 
++	if (WARN_ON(pipe != PIPE_A && pipe != PIPE_B))
++		return;
++
+ 	intel_panel_actually_set_backlight(connector, 0);
+ 
+ 	tmp = I915_READ(VLV_BLC_PWM_CTL2(pipe));
+@@ -732,10 +755,8 @@
+ 	struct drm_device *dev = connector->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct intel_panel *panel = &connector->panel;
+-	enum pipe pipe = intel_get_pipe_from_connector(connector);
+-	unsigned long flags;
+ 
+-	if (!panel->backlight.present || pipe == INVALID_PIPE)
++	if (!panel->backlight.present)
+ 		return;
+ 
+ 	/*
+@@ -749,12 +770,14 @@
+ 		return;
+ 	}
+ 
+-	spin_lock_irqsave(&dev_priv->backlight_lock, flags);
++	mutex_lock(&dev_priv->backlight_lock);
+ 
++	if (panel->backlight.device)
++		panel->backlight.device->props.power = FB_BLANK_POWERDOWN;
+ 	panel->backlight.enabled = false;
+ 	dev_priv->display.disable_backlight(connector);
+ 
+-	spin_unlock_irqrestore(&dev_priv->backlight_lock, flags);
++	mutex_unlock(&dev_priv->backlight_lock);
+ }
+ 
+ static void bdw_enable_backlight(struct intel_connector *connector)
+@@ -778,8 +801,9 @@
+ 	if (panel->backlight.active_low_pwm)
+ 		pch_ctl1 |= BLM_PCH_POLARITY;
+ 
+-	/* BDW always uses the pch pwm controls. */
+-	pch_ctl1 |= BLM_PCH_OVERRIDE_ENABLE;
++	/* After LPT, override is the default. */
++	if (HAS_PCH_LPT(dev_priv))
++		pch_ctl1 |= BLM_PCH_OVERRIDE_ENABLE;
+ 
+ 	I915_WRITE(BLC_PWM_PCH_CTL1, pch_ctl1);
+ 	POSTING_READ(BLC_PWM_PCH_CTL1);
+@@ -908,6 +932,9 @@
+ 	enum pipe pipe = intel_get_pipe_from_connector(connector);
+ 	u32 ctl, ctl2;
+ 
++	if (WARN_ON(pipe != PIPE_A && pipe != PIPE_B))
++		return;
++
+ 	ctl2 = I915_READ(VLV_BLC_PWM_CTL2(pipe));
+ 	if (ctl2 & BLM_PWM_ENABLE) {
+ 		DRM_DEBUG_KMS("backlight already enabled\n");
+@@ -935,36 +962,44 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct intel_panel *panel = &connector->panel;
+ 	enum pipe pipe = intel_get_pipe_from_connector(connector);
+-	unsigned long flags;
+ 
+-	if (!panel->backlight.present || pipe == INVALID_PIPE)
++	if (!panel->backlight.present)
+ 		return;
+ 
+ 	DRM_DEBUG_KMS("pipe %c\n", pipe_name(pipe));
+ 
+-	spin_lock_irqsave(&dev_priv->backlight_lock, flags);
++	mutex_lock(&dev_priv->backlight_lock);
+ 
+ 	WARN_ON(panel->backlight.max == 0);
+ 
+ 	if (panel->backlight.level == 0) {
++		DRM_DEBUG_KMS("overriding backlight level of 0, setting max %d\n",
++			      panel->backlight.max);
+ 		panel->backlight.level = panel->backlight.max;
+-		if (panel->backlight.device)
++		if (panel->backlight.device) {
+ 			panel->backlight.device->props.brightness =
+ 				scale_hw_to_user(connector,
+ 						 panel->backlight.level,
+ 						 panel->backlight.device->props.max_brightness);
++			DRM_DEBUG_KMS("brightness = %d/%d\n",
++				      panel->backlight.device->props.brightness,
++				      panel->backlight.device->props.max_brightness);
++		}
+ 	}
+ 
+ 	dev_priv->display.enable_backlight(connector);
+ 	panel->backlight.enabled = true;
++	if (panel->backlight.device)
++		panel->backlight.device->props.power = FB_BLANK_UNBLANK;
+ 
+-	spin_unlock_irqrestore(&dev_priv->backlight_lock, flags);
++	mutex_unlock(&dev_priv->backlight_lock);
+ }
+ 
+ #if IS_ENABLED(CONFIG_BACKLIGHT_CLASS_DEVICE)
+ static int intel_backlight_device_update_status(struct backlight_device *bd)
+ {
+ 	struct intel_connector *connector = bl_get_data(bd);
++	struct intel_panel *panel = &connector->panel;
+ 	struct drm_device *dev = connector->base.dev;
+ 
+ 	drm_modeset_lock(&dev->mode_config.connection_mutex, NULL);
+@@ -972,6 +1007,23 @@
+ 		      bd->props.brightness, bd->props.max_brightness);
+ 	intel_panel_set_backlight(connector, bd->props.brightness,
+ 				  bd->props.max_brightness);
++
++	/*
++	 * Allow flipping bl_power as a sub-state of enabled. Sadly the
++	 * backlight class device does not make it easy to to differentiate
++	 * between callbacks for brightness and bl_power, so our backlight_power
++	 * callback needs to take this into account.
++	 */
++	if (panel->backlight.enabled) {
++		if (panel->backlight_power) {
++			bool enable = bd->props.power == FB_BLANK_UNBLANK &&
++				bd->props.brightness != 0;
++			panel->backlight_power(connector, enable);
++		}
++	} else {
++		bd->props.power = FB_BLANK_POWERDOWN;
++	}
++
+ 	drm_modeset_unlock(&dev->mode_config.connection_mutex);
+ 	return 0;
+ }
+@@ -1009,6 +1061,9 @@
+ 	if (WARN_ON(panel->backlight.device))
+ 		return -ENODEV;
+ 
++	if (!panel->backlight.present)
++		return 0;
++
+ 	WARN_ON(panel->backlight.max == 0);
+ 
+ 	memset(&props, 0, sizeof(props));
+@@ -1023,6 +1078,11 @@
+ 					    panel->backlight.level,
+ 					    props.max_brightness);
+ 
++	if (panel->backlight.enabled)
++		props.power = FB_BLANK_UNBLANK;
++	else
++		props.power = FB_BLANK_POWERDOWN;
++
+ 	/*
+ 	 * Note: using the same name independent of the connector prevents
+ 	 * registration of multiple backlight devices in the driver.
+@@ -1039,6 +1099,10 @@
+ 		panel->backlight.device = NULL;
+ 		return -ENODEV;
+ 	}
++
++	DRM_DEBUG_KMS("Connector %s backlight sysfs interface registered\n",
++		      connector->base.name);
++
+ 	return 0;
+ }
+ 
+@@ -1072,15 +1136,28 @@
+ 	struct drm_device *dev = connector->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct intel_panel *panel = &connector->panel;
++	int min;
+ 
+ 	WARN_ON(panel->backlight.max == 0);
+ 
++	/*
++	 * XXX: If the vbt value is 255, it makes min equal to max, which leads
++	 * to problems. There are such machines out there. Either our
++	 * interpretation is wrong or the vbt has bogus data. Or both. Safeguard
++	 * against this by letting the minimum be at most (arbitrarily chosen)
++	 * 25% of the max.
++	 */
++	min = clamp_t(int, dev_priv->vbt.backlight.min_brightness, 0, 64);
++	if (min != dev_priv->vbt.backlight.min_brightness) {
++		DRM_DEBUG_KMS("clamping VBT min backlight %d/255 to %d/255\n",
++			      dev_priv->vbt.backlight.min_brightness, min);
++	}
++
+ 	/* vbt value is a coefficient in range [0..255] */
+-	return scale(dev_priv->vbt.backlight.min_brightness, 0, 255,
+-		     0, panel->backlight.max);
++	return scale(min, 0, 255, 0, panel->backlight.max);
+ }
+ 
+-static int bdw_setup_backlight(struct intel_connector *connector)
++static int bdw_setup_backlight(struct intel_connector *connector, enum pipe unused)
+ {
+ 	struct drm_device *dev = connector->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+@@ -1106,7 +1183,7 @@
+ 	return 0;
+ }
+ 
+-static int pch_setup_backlight(struct intel_connector *connector)
++static int pch_setup_backlight(struct intel_connector *connector, enum pipe unused)
+ {
+ 	struct drm_device *dev = connector->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+@@ -1133,7 +1210,7 @@
+ 	return 0;
+ }
+ 
+-static int i9xx_setup_backlight(struct intel_connector *connector)
++static int i9xx_setup_backlight(struct intel_connector *connector, enum pipe unused)
+ {
+ 	struct drm_device *dev = connector->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+@@ -1165,7 +1242,7 @@
+ 	return 0;
+ }
+ 
+-static int i965_setup_backlight(struct intel_connector *connector)
++static int i965_setup_backlight(struct intel_connector *connector, enum pipe unused)
+ {
+ 	struct drm_device *dev = connector->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+@@ -1195,37 +1272,40 @@
+ 	return 0;
+ }
+ 
+-static int vlv_setup_backlight(struct intel_connector *connector)
++static int vlv_setup_backlight(struct intel_connector *connector, enum pipe pipe)
+ {
+ 	struct drm_device *dev = connector->base.dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct intel_panel *panel = &connector->panel;
+-	enum pipe pipe;
++	enum pipe p;
+ 	u32 ctl, ctl2, val;
+ 
+-	for_each_pipe(pipe) {
+-		u32 cur_val = I915_READ(VLV_BLC_PWM_CTL(pipe));
++	for_each_pipe(dev_priv, p) {
++		u32 cur_val = I915_READ(VLV_BLC_PWM_CTL(p));
+ 
+ 		/* Skip if the modulation freq is already set */
+ 		if (cur_val & ~BACKLIGHT_DUTY_CYCLE_MASK)
+ 			continue;
+ 
+ 		cur_val &= BACKLIGHT_DUTY_CYCLE_MASK;
+-		I915_WRITE(VLV_BLC_PWM_CTL(pipe), (0xf42 << 16) |
++		I915_WRITE(VLV_BLC_PWM_CTL(p), (0xf42 << 16) |
+ 			   cur_val);
+ 	}
+ 
+-	ctl2 = I915_READ(VLV_BLC_PWM_CTL2(PIPE_A));
++	if (WARN_ON(pipe != PIPE_A && pipe != PIPE_B))
++		return -ENODEV;
++
++	ctl2 = I915_READ(VLV_BLC_PWM_CTL2(pipe));
+ 	panel->backlight.active_low_pwm = ctl2 & BLM_POLARITY_I965;
+ 
+-	ctl = I915_READ(VLV_BLC_PWM_CTL(PIPE_A));
++	ctl = I915_READ(VLV_BLC_PWM_CTL(pipe));
+ 	panel->backlight.max = ctl >> 16;
+ 	if (!panel->backlight.max)
+ 		return -ENODEV;
+ 
+ 	panel->backlight.min = get_backlight_min_vbt(connector);
+ 
+-	val = _vlv_get_backlight(dev, PIPE_A);
++	val = _vlv_get_backlight(dev, pipe);
+ 	panel->backlight.level = intel_panel_compute_brightness(connector, val);
+ 
+ 	panel->backlight.enabled = (ctl2 & BLM_PWM_ENABLE) &&
+@@ -1234,13 +1314,12 @@
+ 	return 0;
+ }
+ 
+-int intel_panel_setup_backlight(struct drm_connector *connector)
++int intel_panel_setup_backlight(struct drm_connector *connector, enum pipe pipe)
+ {
+ 	struct drm_device *dev = connector->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	struct intel_connector *intel_connector = to_intel_connector(connector);
+ 	struct intel_panel *panel = &intel_connector->panel;
+-	unsigned long flags;
+ 	int ret;
+ 
+ 	if (!dev_priv->vbt.backlight.present) {
+@@ -1253,9 +1332,9 @@
+ 	}
+ 
+ 	/* set level and max in panel struct */
+-	spin_lock_irqsave(&dev_priv->backlight_lock, flags);
+-	ret = dev_priv->display.setup_backlight(intel_connector);
+-	spin_unlock_irqrestore(&dev_priv->backlight_lock, flags);
++	mutex_lock(&dev_priv->backlight_lock);
++	ret = dev_priv->display.setup_backlight(intel_connector, pipe);
++	mutex_unlock(&dev_priv->backlight_lock);
+ 
+ 	if (ret) {
+ 		DRM_DEBUG_KMS("failed to setup backlight for connector %s\n",
+@@ -1263,15 +1342,12 @@
+ 		return ret;
+ 	}
+ 
+-	intel_backlight_device_register(intel_connector);
+-
+ 	panel->backlight.present = true;
+ 
+-	DRM_DEBUG_KMS("backlight initialized, %s, brightness %u/%u, "
+-		      "sysfs interface %sregistered\n",
++	DRM_DEBUG_KMS("Connector %s backlight initialized, %s, brightness %u/%u\n",
++		      connector->name,
+ 		      panel->backlight.enabled ? "enabled" : "disabled",
+-		      panel->backlight.level, panel->backlight.max,
+-		      panel->backlight.device ? "" : "not ");
++		      panel->backlight.level, panel->backlight.max);
+ 
+ 	return 0;
+ }
+@@ -1282,7 +1358,6 @@
+ 	struct intel_panel *panel = &intel_connector->panel;
+ 
+ 	panel->backlight.present = false;
+-	intel_backlight_device_unregister(intel_connector);
+ }
+ 
+ /* Set up chip specific backlight functions */
+@@ -1290,7 +1365,7 @@
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	if (IS_BROADWELL(dev)) {
++	if (IS_BROADWELL(dev) || (INTEL_INFO(dev)->gen >= 9)) {
+ 		dev_priv->display.setup_backlight = bdw_setup_backlight;
+ 		dev_priv->display.enable_backlight = bdw_enable_backlight;
+ 		dev_priv->display.disable_backlight = pch_disable_backlight;
+@@ -1345,3 +1420,19 @@
+ 		drm_mode_destroy(intel_connector->base.dev,
+ 				panel->downclock_mode);
+ }
++
++void intel_backlight_register(struct drm_device *dev)
++{
++	struct intel_connector *connector;
++
++	list_for_each_entry(connector, &dev->mode_config.connector_list, base.head)
++		intel_backlight_device_register(connector);
++}
++
++void intel_backlight_unregister(struct drm_device *dev)
++{
++	struct intel_connector *connector;
++
++	list_for_each_entry(connector, &dev->mode_config.connector_list, base.head)
++		intel_backlight_device_unregister(connector);
++}
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_pm.c b/drivers/gpu/drm/i915/intel_pm.c
+--- a/drivers/gpu/drm/i915/intel_pm.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_pm.c	2014-11-20 09:53:37.996762837 -0700
+@@ -30,9 +30,6 @@
+ #include "intel_drv.h"
+ #include "../../../platform/x86/intel_ips.h"
+ #include <linux/module.h>
+-#include <linux/vgaarb.h>
+-#include <drm/i915_powerwell.h>
+-#include <linux/pm_runtime.h>
+ 
+ /**
+  * RC6 is a special power stage which allows the GPU to enter an very
+@@ -66,11 +63,37 @@
+  * i915.i915_enable_fbc parameter
+  */
+ 
++static void gen9_init_clock_gating(struct drm_device *dev)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++
++	/*
++	 * WaDisableSDEUnitClockGating:skl
++	 * This seems to be a pre-production w/a.
++	 */
++	I915_WRITE(GEN8_UCGCTL6, I915_READ(GEN8_UCGCTL6) |
++		   GEN8_SDEUNIT_CLOCK_GATE_DISABLE);
++
++	/*
++	 * WaDisableDgMirrorFixInHalfSliceChicken5:skl
++	 * This is a pre-production w/a.
++	 */
++	I915_WRITE(GEN9_HALF_SLICE_CHICKEN5,
++		   I915_READ(GEN9_HALF_SLICE_CHICKEN5) &
++		   ~GEN9_DG_MIRROR_FIX_ENABLE);
++
++	/* Wa4x4STCOptimizationDisable:skl */
++	I915_WRITE(CACHE_MODE_1,
++		   _MASKED_BIT_ENABLE(GEN8_4x4_STC_OPTIMIZATION_DISABLE));
++}
++
+ static void i8xx_disable_fbc(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	u32 fbc_ctl;
+ 
++	dev_priv->fbc.enabled = false;
++
+ 	/* Disable compression */
+ 	fbc_ctl = I915_READ(FBC_CONTROL);
+ 	if ((fbc_ctl & FBC_CTL_EN) == 0)
+@@ -99,6 +122,8 @@
+ 	int i;
+ 	u32 fbc_ctl;
+ 
++	dev_priv->fbc.enabled = true;
++
+ 	cfb_pitch = dev_priv->fbc.size / FBC_LL_SIZE;
+ 	if (fb->pitches[0] < cfb_pitch)
+ 		cfb_pitch = fb->pitches[0];
+@@ -153,6 +178,8 @@
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+ 	u32 dpfc_ctl;
+ 
++	dev_priv->fbc.enabled = true;
++
+ 	dpfc_ctl = DPFC_CTL_PLANE(intel_crtc->plane) | DPFC_SR_EN;
+ 	if (drm_format_plane_cpp(fb->pixel_format, 0) == 2)
+ 		dpfc_ctl |= DPFC_CTL_LIMIT_2X;
+@@ -173,6 +200,8 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	u32 dpfc_ctl;
+ 
++	dev_priv->fbc.enabled = false;
++
+ 	/* Disable compression */
+ 	dpfc_ctl = I915_READ(DPFC_CONTROL);
+ 	if (dpfc_ctl & DPFC_CTL_EN) {
+@@ -224,6 +253,8 @@
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+ 	u32 dpfc_ctl;
+ 
++	dev_priv->fbc.enabled = true;
++
+ 	dpfc_ctl = DPFC_CTL_PLANE(intel_crtc->plane);
+ 	if (drm_format_plane_cpp(fb->pixel_format, 0) == 2)
+ 		dev_priv->fbc.threshold++;
+@@ -264,6 +295,8 @@
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	u32 dpfc_ctl;
+ 
++	dev_priv->fbc.enabled = false;
++
+ 	/* Disable compression */
+ 	dpfc_ctl = I915_READ(ILK_DPFC_CONTROL);
+ 	if (dpfc_ctl & DPFC_CTL_EN) {
+@@ -290,6 +323,8 @@
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+ 	u32 dpfc_ctl;
+ 
++	dev_priv->fbc.enabled = true;
++
+ 	dpfc_ctl = IVB_DPFC_CTL_PLANE(intel_crtc->plane);
+ 	if (drm_format_plane_cpp(fb->pixel_format, 0) == 2)
+ 		dev_priv->fbc.threshold++;
+@@ -309,6 +344,9 @@
+ 
+ 	dpfc_ctl |= IVB_DPFC_CTL_FENCE_EN;
+ 
++	if (dev_priv->fbc.false_color)
++		dpfc_ctl |= FBC_CTL_FALSE_COLOR;
++
+ 	I915_WRITE(ILK_DPFC_CONTROL, dpfc_ctl | DPFC_CTL_EN);
+ 
+ 	if (IS_IVYBRIDGE(dev)) {
+@@ -336,10 +374,20 @@
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	if (!dev_priv->display.fbc_enabled)
+-		return false;
++	return dev_priv->fbc.enabled;
++}
++
++void bdw_fbc_sw_flush(struct drm_device *dev, u32 value)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++
++	if (!IS_GEN8(dev))
++		return;
++
++	if (!intel_fbc_enabled(dev))
++		return;
+ 
+-	return dev_priv->display.fbc_enabled(dev);
++	I915_WRITE(MSG_FBC_REND_STATE, value);
+ }
+ 
+ static void intel_fbc_work_fn(struct work_struct *__work)
+@@ -490,7 +538,7 @@
+ 		return;
+ 	}
+ 
+-	if (!i915.powersave) {
++	if (!i915_module.powersave) {
+ 		if (set_no_fbc_reason(dev_priv, FBC_MODULE_PARAM))
+ 			DRM_DEBUG_KMS("fbc disabled per module param\n");
+ 		return;
+@@ -528,12 +576,12 @@
+ 	obj = intel_fb_obj(fb);
+ 	adjusted_mode = &intel_crtc->config.adjusted_mode;
+ 
+-	if (i915.enable_fbc < 0) {
++	if (i915_module.enable_fbc < 0) {
+ 		if (set_no_fbc_reason(dev_priv, FBC_CHIP_DEFAULT))
+ 			DRM_DEBUG_KMS("disabled per chip default\n");
+ 		goto out_disable;
+ 	}
+-	if (!i915.enable_fbc) {
++	if (!i915_module.enable_fbc) {
+ 		if (set_no_fbc_reason(dev_priv, FBC_MODULE_PARAM))
+ 			DRM_DEBUG_KMS("fbc disabled per module param\n");
+ 		goto out_disable;
+@@ -578,6 +626,12 @@
+ 			DRM_DEBUG_KMS("framebuffer not tiled or fenced, disabling compression\n");
+ 		goto out_disable;
+ 	}
++	if (INTEL_INFO(dev)->gen <= 4 && !IS_G4X(dev) &&
++	    to_intel_plane(crtc->primary)->rotation != BIT(DRM_ROTATE_0)) {
++		if (set_no_fbc_reason(dev_priv, FBC_UNSUPPORTED_MODE))
++			DRM_DEBUG_KMS("Rotation unsupported, disabling\n");
++		goto out_disable;
++	}
+ 
+ 	/* If the kernel debugger is active, always disable compression */
+ 	if (in_dbg_master())
+@@ -853,7 +907,7 @@
+  * A value of 5us seems to be a good balance; safe for very low end
+  * platforms but not overly aggressive on lower latency configs.
+  */
+-static const int latency_ns = 5000;
++static const int pessimal_latency_ns = 5000;
+ 
+ static int i9xx_get_fifo_size(struct drm_device *dev, int plane)
+ {
+@@ -982,13 +1036,20 @@
+ 	.guard_size = 2,
+ 	.cacheline_size = I915_FIFO_LINE_SIZE,
+ };
+-static const struct intel_watermark_params i830_wm_info = {
++static const struct intel_watermark_params i830_a_wm_info = {
+ 	.fifo_size = I855GM_FIFO_SIZE,
+ 	.max_wm = I915_MAX_WM,
+ 	.default_wm = 1,
+ 	.guard_size = 2,
+ 	.cacheline_size = I830_FIFO_LINE_SIZE,
+ };
++static const struct intel_watermark_params i830_bc_wm_info = {
++	.fifo_size = I855GM_FIFO_SIZE,
++	.max_wm = I915_MAX_WM/2,
++	.default_wm = 1,
++	.guard_size = 2,
++	.cacheline_size = I830_FIFO_LINE_SIZE,
++};
+ static const struct intel_watermark_params i845_wm_info = {
+ 	.fifo_size = I830_FIFO_SIZE,
+ 	.max_wm = I915_MAX_WM,
+@@ -1044,6 +1105,17 @@
+ 		wm_size = wm->max_wm;
+ 	if (wm_size <= 0)
+ 		wm_size = wm->default_wm;
++
++	/*
++	 * Bspec seems to indicate that the value shouldn't be lower than
++	 * 'burst size + 1'. Certainly 830 is quite unhappy with low values.
++	 * Lets go for 8 which is the burst size since certain platforms
++	 * already use a hardcoded 8 (which is what the spec says should be
++	 * done).
++	 */
++	if (wm_size <= 8)
++		wm_size = 8;
++
+ 	return wm_size;
+ }
+ 
+@@ -1268,33 +1340,32 @@
+ 			      display, cursor);
+ }
+ 
+-static bool vlv_compute_drain_latency(struct drm_device *dev,
+-				     int plane,
+-				     int *plane_prec_mult,
+-				     int *plane_dl,
+-				     int *cursor_prec_mult,
+-				     int *cursor_dl)
++static bool vlv_compute_drain_latency(struct drm_crtc *crtc,
++				      int pixel_size,
++				      int *prec_mult,
++				      int *drain_latency)
+ {
+-	struct drm_crtc *crtc;
+-	int clock, pixel_size;
++	struct drm_device *dev = crtc->dev;
+ 	int entries;
++	int clock = to_intel_crtc(crtc)->config.adjusted_mode.crtc_clock;
+ 
+-	crtc = intel_get_crtc_for_plane(dev, plane);
+-	if (!intel_crtc_active(crtc))
++	if (WARN(clock == 0, "Pixel clock is zero!\n"))
+ 		return false;
+ 
+-	clock = to_intel_crtc(crtc)->config.adjusted_mode.crtc_clock;
+-	pixel_size = crtc->primary->fb->bits_per_pixel / 8;	/* BPP */
++	if (WARN(pixel_size == 0, "Pixel size is zero!\n"))
++		return false;
++
++	entries = DIV_ROUND_UP(clock, 1000) * pixel_size;
++	if (IS_CHERRYVIEW(dev))
++		*prec_mult = (entries > 128) ? DRAIN_LATENCY_PRECISION_32 :
++					       DRAIN_LATENCY_PRECISION_16;
++	else
++		*prec_mult = (entries > 128) ? DRAIN_LATENCY_PRECISION_64 :
++					       DRAIN_LATENCY_PRECISION_32;
++	*drain_latency = (64 * (*prec_mult) * 4) / entries;
+ 
+-	entries = (clock / 1000) * pixel_size;
+-	*plane_prec_mult = (entries > 128) ?
+-		DRAIN_LATENCY_PRECISION_64 : DRAIN_LATENCY_PRECISION_32;
+-	*plane_dl = (64 * (*plane_prec_mult) * 4) / entries;
+-
+-	entries = (clock / 1000) * 4;	/* BPP is always 4 for cursor */
+-	*cursor_prec_mult = (entries > 128) ?
+-		DRAIN_LATENCY_PRECISION_64 : DRAIN_LATENCY_PRECISION_32;
+-	*cursor_dl = (64 * (*cursor_prec_mult) * 4) / entries;
++	if (*drain_latency > DRAIN_LATENCY_MASK)
++		*drain_latency = DRAIN_LATENCY_MASK;
+ 
+ 	return true;
+ }
+@@ -1307,39 +1378,51 @@
+  * latency value.
+  */
+ 
+-static void vlv_update_drain_latency(struct drm_device *dev)
++static void vlv_update_drain_latency(struct drm_crtc *crtc)
+ {
++	struct drm_device *dev = crtc->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	int planea_prec, planea_dl, planeb_prec, planeb_dl;
+-	int cursora_prec, cursora_dl, cursorb_prec, cursorb_dl;
+-	int plane_prec_mult, cursor_prec_mult; /* Precision multiplier is
+-							either 16 or 32 */
+-
+-	/* For plane A, Cursor A */
+-	if (vlv_compute_drain_latency(dev, 0, &plane_prec_mult, &planea_dl,
+-				      &cursor_prec_mult, &cursora_dl)) {
+-		cursora_prec = (cursor_prec_mult == DRAIN_LATENCY_PRECISION_32) ?
+-			DDL_CURSORA_PRECISION_32 : DDL_CURSORA_PRECISION_64;
+-		planea_prec = (plane_prec_mult == DRAIN_LATENCY_PRECISION_32) ?
+-			DDL_PLANEA_PRECISION_32 : DDL_PLANEA_PRECISION_64;
+-
+-		I915_WRITE(VLV_DDL1, cursora_prec |
+-				(cursora_dl << DDL_CURSORA_SHIFT) |
+-				planea_prec | planea_dl);
+-	}
+-
+-	/* For plane B, Cursor B */
+-	if (vlv_compute_drain_latency(dev, 1, &plane_prec_mult, &planeb_dl,
+-				      &cursor_prec_mult, &cursorb_dl)) {
+-		cursorb_prec = (cursor_prec_mult == DRAIN_LATENCY_PRECISION_32) ?
+-			DDL_CURSORB_PRECISION_32 : DDL_CURSORB_PRECISION_64;
+-		planeb_prec = (plane_prec_mult == DRAIN_LATENCY_PRECISION_32) ?
+-			DDL_PLANEB_PRECISION_32 : DDL_PLANEB_PRECISION_64;
+-
+-		I915_WRITE(VLV_DDL2, cursorb_prec |
+-				(cursorb_dl << DDL_CURSORB_SHIFT) |
+-				planeb_prec | planeb_dl);
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	int pixel_size;
++	int drain_latency;
++	enum pipe pipe = intel_crtc->pipe;
++	int plane_prec, prec_mult, plane_dl;
++	const int high_precision = IS_CHERRYVIEW(dev) ?
++		DRAIN_LATENCY_PRECISION_32 : DRAIN_LATENCY_PRECISION_64;
++
++	plane_dl = I915_READ(VLV_DDL(pipe)) & ~(DDL_PLANE_PRECISION_HIGH |
++		   DRAIN_LATENCY_MASK | DDL_CURSOR_PRECISION_HIGH |
++		   (DRAIN_LATENCY_MASK << DDL_CURSOR_SHIFT));
++
++	if (!intel_crtc_active(crtc)) {
++		I915_WRITE(VLV_DDL(pipe), plane_dl);
++		return;
++	}
++
++	/* Primary plane Drain Latency */
++	pixel_size = crtc->primary->fb->bits_per_pixel / 8;	/* BPP */
++	if (vlv_compute_drain_latency(crtc, pixel_size, &prec_mult, &drain_latency)) {
++		plane_prec = (prec_mult == high_precision) ?
++					   DDL_PLANE_PRECISION_HIGH :
++					   DDL_PLANE_PRECISION_LOW;
++		plane_dl |= plane_prec | drain_latency;
++	}
++
++	/* Cursor Drain Latency
++	 * BPP is always 4 for cursor
++	 */
++	pixel_size = 4;
++
++	/* Program cursor DL only if it is enabled */
++	if (intel_crtc->cursor_base &&
++	    vlv_compute_drain_latency(crtc, pixel_size, &prec_mult, &drain_latency)) {
++		plane_prec = (prec_mult == high_precision) ?
++					   DDL_CURSOR_PRECISION_HIGH :
++					   DDL_CURSOR_PRECISION_LOW;
++		plane_dl |= plane_prec | (drain_latency << DDL_CURSOR_SHIFT);
+ 	}
++
++	I915_WRITE(VLV_DDL(pipe), plane_dl);
+ }
+ 
+ #define single_plane_enabled(mask) is_power_of_2(mask)
+@@ -1355,20 +1438,92 @@
+ 	unsigned int enabled = 0;
+ 	bool cxsr_enabled;
+ 
+-	vlv_update_drain_latency(dev);
++	vlv_update_drain_latency(crtc);
++
++	if (g4x_compute_wm0(dev, PIPE_A,
++			    &valleyview_wm_info, pessimal_latency_ns,
++			    &valleyview_cursor_wm_info, pessimal_latency_ns,
++			    &planea_wm, &cursora_wm))
++		enabled |= 1 << PIPE_A;
++
++	if (g4x_compute_wm0(dev, PIPE_B,
++			    &valleyview_wm_info, pessimal_latency_ns,
++			    &valleyview_cursor_wm_info, pessimal_latency_ns,
++			    &planeb_wm, &cursorb_wm))
++		enabled |= 1 << PIPE_B;
++
++	if (single_plane_enabled(enabled) &&
++	    g4x_compute_srwm(dev, ffs(enabled) - 1,
++			     sr_latency_ns,
++			     &valleyview_wm_info,
++			     &valleyview_cursor_wm_info,
++			     &plane_sr, &ignore_cursor_sr) &&
++	    g4x_compute_srwm(dev, ffs(enabled) - 1,
++			     2*sr_latency_ns,
++			     &valleyview_wm_info,
++			     &valleyview_cursor_wm_info,
++			     &ignore_plane_sr, &cursor_sr)) {
++		cxsr_enabled = true;
++	} else {
++		cxsr_enabled = false;
++		intel_set_memory_cxsr(dev_priv, false);
++		plane_sr = cursor_sr = 0;
++	}
++
++	DRM_DEBUG_KMS("Setting FIFO watermarks - A: plane=%d, cursor=%d, "
++		      "B: plane=%d, cursor=%d, SR: plane=%d, cursor=%d\n",
++		      planea_wm, cursora_wm,
++		      planeb_wm, cursorb_wm,
++		      plane_sr, cursor_sr);
++
++	I915_WRITE(DSPFW1,
++		   (plane_sr << DSPFW_SR_SHIFT) |
++		   (cursorb_wm << DSPFW_CURSORB_SHIFT) |
++		   (planeb_wm << DSPFW_PLANEB_SHIFT) |
++		   (planea_wm << DSPFW_PLANEA_SHIFT));
++	I915_WRITE(DSPFW2,
++		   (I915_READ(DSPFW2) & ~DSPFW_CURSORA_MASK) |
++		   (cursora_wm << DSPFW_CURSORA_SHIFT));
++	I915_WRITE(DSPFW3,
++		   (I915_READ(DSPFW3) & ~DSPFW_CURSOR_SR_MASK) |
++		   (cursor_sr << DSPFW_CURSOR_SR_SHIFT));
++
++	if (cxsr_enabled)
++		intel_set_memory_cxsr(dev_priv, true);
++}
++
++static void cherryview_update_wm(struct drm_crtc *crtc)
++{
++	struct drm_device *dev = crtc->dev;
++	static const int sr_latency_ns = 12000;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	int planea_wm, planeb_wm, planec_wm;
++	int cursora_wm, cursorb_wm, cursorc_wm;
++	int plane_sr, cursor_sr;
++	int ignore_plane_sr, ignore_cursor_sr;
++	unsigned int enabled = 0;
++	bool cxsr_enabled;
++
++	vlv_update_drain_latency(crtc);
+ 
+ 	if (g4x_compute_wm0(dev, PIPE_A,
+-			    &valleyview_wm_info, latency_ns,
+-			    &valleyview_cursor_wm_info, latency_ns,
++			    &valleyview_wm_info, pessimal_latency_ns,
++			    &valleyview_cursor_wm_info, pessimal_latency_ns,
+ 			    &planea_wm, &cursora_wm))
+ 		enabled |= 1 << PIPE_A;
+ 
+ 	if (g4x_compute_wm0(dev, PIPE_B,
+-			    &valleyview_wm_info, latency_ns,
+-			    &valleyview_cursor_wm_info, latency_ns,
++			    &valleyview_wm_info, pessimal_latency_ns,
++			    &valleyview_cursor_wm_info, pessimal_latency_ns,
+ 			    &planeb_wm, &cursorb_wm))
+ 		enabled |= 1 << PIPE_B;
+ 
++	if (g4x_compute_wm0(dev, PIPE_C,
++			    &valleyview_wm_info, pessimal_latency_ns,
++			    &valleyview_cursor_wm_info, pessimal_latency_ns,
++			    &planec_wm, &cursorc_wm))
++		enabled |= 1 << PIPE_C;
++
+ 	if (single_plane_enabled(enabled) &&
+ 	    g4x_compute_srwm(dev, ffs(enabled) - 1,
+ 			     sr_latency_ns,
+@@ -1387,27 +1542,68 @@
+ 		plane_sr = cursor_sr = 0;
+ 	}
+ 
+-	DRM_DEBUG_KMS("Setting FIFO watermarks - A: plane=%d, cursor=%d, B: plane=%d, cursor=%d, SR: plane=%d, cursor=%d\n",
++	DRM_DEBUG_KMS("Setting FIFO watermarks - A: plane=%d, cursor=%d, "
++		      "B: plane=%d, cursor=%d, C: plane=%d, cursor=%d, "
++		      "SR: plane=%d, cursor=%d\n",
+ 		      planea_wm, cursora_wm,
+ 		      planeb_wm, cursorb_wm,
++		      planec_wm, cursorc_wm,
+ 		      plane_sr, cursor_sr);
+ 
+ 	I915_WRITE(DSPFW1,
+ 		   (plane_sr << DSPFW_SR_SHIFT) |
+ 		   (cursorb_wm << DSPFW_CURSORB_SHIFT) |
+ 		   (planeb_wm << DSPFW_PLANEB_SHIFT) |
+-		   planea_wm);
++		   (planea_wm << DSPFW_PLANEA_SHIFT));
+ 	I915_WRITE(DSPFW2,
+ 		   (I915_READ(DSPFW2) & ~DSPFW_CURSORA_MASK) |
+ 		   (cursora_wm << DSPFW_CURSORA_SHIFT));
+ 	I915_WRITE(DSPFW3,
+ 		   (I915_READ(DSPFW3) & ~DSPFW_CURSOR_SR_MASK) |
+ 		   (cursor_sr << DSPFW_CURSOR_SR_SHIFT));
++	I915_WRITE(DSPFW9_CHV,
++		   (I915_READ(DSPFW9_CHV) & ~(DSPFW_PLANEC_MASK |
++					      DSPFW_CURSORC_MASK)) |
++		   (planec_wm << DSPFW_PLANEC_SHIFT) |
++		   (cursorc_wm << DSPFW_CURSORC_SHIFT));
+ 
+ 	if (cxsr_enabled)
+ 		intel_set_memory_cxsr(dev_priv, true);
+ }
+ 
++static void valleyview_update_sprite_wm(struct drm_plane *plane,
++					struct drm_crtc *crtc,
++					uint32_t sprite_width,
++					uint32_t sprite_height,
++					int pixel_size,
++					bool enabled, bool scaled)
++{
++	struct drm_device *dev = crtc->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	int pipe = to_intel_plane(plane)->pipe;
++	int sprite = to_intel_plane(plane)->plane;
++	int drain_latency;
++	int plane_prec;
++	int sprite_dl;
++	int prec_mult;
++	const int high_precision = IS_CHERRYVIEW(dev) ?
++		DRAIN_LATENCY_PRECISION_32 : DRAIN_LATENCY_PRECISION_64;
++
++	sprite_dl = I915_READ(VLV_DDL(pipe)) & ~(DDL_SPRITE_PRECISION_HIGH(sprite) |
++		    (DRAIN_LATENCY_MASK << DDL_SPRITE_SHIFT(sprite)));
++
++	if (enabled && vlv_compute_drain_latency(crtc, pixel_size, &prec_mult,
++						 &drain_latency)) {
++		plane_prec = (prec_mult == high_precision) ?
++					   DDL_SPRITE_PRECISION_HIGH(sprite) :
++					   DDL_SPRITE_PRECISION_LOW(sprite);
++		sprite_dl |= plane_prec |
++			     (drain_latency << DDL_SPRITE_SHIFT(sprite));
++	}
++
++	I915_WRITE(VLV_DDL(pipe), sprite_dl);
++}
++
+ static void g4x_update_wm(struct drm_crtc *crtc)
+ {
+ 	struct drm_device *dev = crtc->dev;
+@@ -1419,14 +1615,14 @@
+ 	bool cxsr_enabled;
+ 
+ 	if (g4x_compute_wm0(dev, PIPE_A,
+-			    &g4x_wm_info, latency_ns,
+-			    &g4x_cursor_wm_info, latency_ns,
++			    &g4x_wm_info, pessimal_latency_ns,
++			    &g4x_cursor_wm_info, pessimal_latency_ns,
+ 			    &planea_wm, &cursora_wm))
+ 		enabled |= 1 << PIPE_A;
+ 
+ 	if (g4x_compute_wm0(dev, PIPE_B,
+-			    &g4x_wm_info, latency_ns,
+-			    &g4x_cursor_wm_info, latency_ns,
++			    &g4x_wm_info, pessimal_latency_ns,
++			    &g4x_cursor_wm_info, pessimal_latency_ns,
+ 			    &planeb_wm, &cursorb_wm))
+ 		enabled |= 1 << PIPE_B;
+ 
+@@ -1443,7 +1639,8 @@
+ 		plane_sr = cursor_sr = 0;
+ 	}
+ 
+-	DRM_DEBUG_KMS("Setting FIFO watermarks - A: plane=%d, cursor=%d, B: plane=%d, cursor=%d, SR: plane=%d, cursor=%d\n",
++	DRM_DEBUG_KMS("Setting FIFO watermarks - A: plane=%d, cursor=%d, "
++		      "B: plane=%d, cursor=%d, SR: plane=%d, cursor=%d\n",
+ 		      planea_wm, cursora_wm,
+ 		      planeb_wm, cursorb_wm,
+ 		      plane_sr, cursor_sr);
+@@ -1452,7 +1649,7 @@
+ 		   (plane_sr << DSPFW_SR_SHIFT) |
+ 		   (cursorb_wm << DSPFW_CURSORB_SHIFT) |
+ 		   (planeb_wm << DSPFW_PLANEB_SHIFT) |
+-		   planea_wm);
++		   (planea_wm << DSPFW_PLANEA_SHIFT));
+ 	I915_WRITE(DSPFW2,
+ 		   (I915_READ(DSPFW2) & ~DSPFW_CURSORA_MASK) |
+ 		   (cursora_wm << DSPFW_CURSORA_SHIFT));
+@@ -1526,8 +1723,11 @@
+ 
+ 	/* 965 has limitations... */
+ 	I915_WRITE(DSPFW1, (srwm << DSPFW_SR_SHIFT) |
+-		   (8 << 16) | (8 << 8) | (8 << 0));
+-	I915_WRITE(DSPFW2, (8 << 8) | (8 << 0));
++		   (8 << DSPFW_CURSORB_SHIFT) |
++		   (8 << DSPFW_PLANEB_SHIFT) |
++		   (8 << DSPFW_PLANEA_SHIFT));
++	I915_WRITE(DSPFW2, (8 << DSPFW_CURSORA_SHIFT) |
++		   (8 << DSPFW_PLANEC_SHIFT_OLD));
+ 	/* update cursor SR watermark */
+ 	I915_WRITE(DSPFW3, (cursor_sr << DSPFW_CURSOR_SR_SHIFT));
+ 
+@@ -1552,7 +1752,7 @@
+ 	else if (!IS_GEN2(dev))
+ 		wm_info = &i915_wm_info;
+ 	else
+-		wm_info = &i830_wm_info;
++		wm_info = &i830_a_wm_info;
+ 
+ 	fifo_size = dev_priv->display.get_fifo_size(dev, 0);
+ 	crtc = intel_get_crtc_for_plane(dev, 0);
+@@ -1565,10 +1765,16 @@
+ 		adjusted_mode = &to_intel_crtc(crtc)->config.adjusted_mode;
+ 		planea_wm = intel_calculate_wm(adjusted_mode->crtc_clock,
+ 					       wm_info, fifo_size, cpp,
+-					       latency_ns);
++					       pessimal_latency_ns);
+ 		enabled = crtc;
+-	} else
++	} else {
+ 		planea_wm = fifo_size - wm_info->guard_size;
++		if (planea_wm > (long)wm_info->max_wm)
++			planea_wm = wm_info->max_wm;
++	}
++
++	if (IS_GEN2(dev))
++		wm_info = &i830_bc_wm_info;
+ 
+ 	fifo_size = dev_priv->display.get_fifo_size(dev, 1);
+ 	crtc = intel_get_crtc_for_plane(dev, 1);
+@@ -1581,13 +1787,16 @@
+ 		adjusted_mode = &to_intel_crtc(crtc)->config.adjusted_mode;
+ 		planeb_wm = intel_calculate_wm(adjusted_mode->crtc_clock,
+ 					       wm_info, fifo_size, cpp,
+-					       latency_ns);
++					       pessimal_latency_ns);
+ 		if (enabled == NULL)
+ 			enabled = crtc;
+ 		else
+ 			enabled = NULL;
+-	} else
++	} else {
+ 		planeb_wm = fifo_size - wm_info->guard_size;
++		if (planeb_wm > (long)wm_info->max_wm)
++			planeb_wm = wm_info->max_wm;
++	}
+ 
+ 	DRM_DEBUG_KMS("FIFO watermarks - A: %d, B: %d\n", planea_wm, planeb_wm);
+ 
+@@ -1674,7 +1883,7 @@
+ 	planea_wm = intel_calculate_wm(adjusted_mode->crtc_clock,
+ 				       &i845_wm_info,
+ 				       dev_priv->display.get_fifo_size(dev, 0),
+-				       4, latency_ns);
++				       4, pessimal_latency_ns);
+ 	fwater_lo = I915_READ(FW_BLC) & ~0xfff;
+ 	fwater_lo |= (3<<8) | planea_wm;
+ 
+@@ -1751,6 +1960,14 @@
+ 	return DIV_ROUND_UP(pri_val * 64, horiz_pixels * bytes_per_pixel) + 2;
+ }
+ 
++struct skl_pipe_wm_parameters {
++	bool active;
++	uint32_t pipe_htotal;
++	uint32_t pixel_rate; /* in KHz */
++	struct intel_plane_wm_parameters plane[I915_MAX_PLANES];
++	struct intel_plane_wm_parameters cursor;
++};
++
+ struct ilk_pipe_wm_parameters {
+ 	bool active;
+ 	uint32_t pipe_htotal;
+@@ -2062,11 +2279,82 @@
+ 	       PIPE_WM_LINETIME_TIME(linetime);
+ }
+ 
+-static void intel_read_wm_latency(struct drm_device *dev, uint16_t wm[5])
++static void intel_read_wm_latency(struct drm_device *dev, uint16_t wm[8])
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	if (IS_HASWELL(dev) || IS_BROADWELL(dev)) {
++	if (IS_GEN9(dev)) {
++		uint32_t val;
++		int ret, i;
++		int level, max_level = ilk_wm_max_level(dev);
++
++		/* read the first set of memory latencies[0:3] */
++		val = 0; /* data0 to be programmed to 0 for first set */
++		mutex_lock(&dev_priv->rps.hw_lock);
++		ret = sandybridge_pcode_read(dev_priv,
++					     GEN9_PCODE_READ_MEM_LATENCY,
++					     &val);
++		mutex_unlock(&dev_priv->rps.hw_lock);
++
++		if (ret) {
++			DRM_ERROR("SKL Mailbox read error = %d\n", ret);
++			return;
++		}
++
++		wm[0] = val & GEN9_MEM_LATENCY_LEVEL_MASK;
++		wm[1] = (val >> GEN9_MEM_LATENCY_LEVEL_1_5_SHIFT) &
++				GEN9_MEM_LATENCY_LEVEL_MASK;
++		wm[2] = (val >> GEN9_MEM_LATENCY_LEVEL_2_6_SHIFT) &
++				GEN9_MEM_LATENCY_LEVEL_MASK;
++		wm[3] = (val >> GEN9_MEM_LATENCY_LEVEL_3_7_SHIFT) &
++				GEN9_MEM_LATENCY_LEVEL_MASK;
++
++		/* read the second set of memory latencies[4:7] */
++		val = 1; /* data0 to be programmed to 1 for second set */
++		mutex_lock(&dev_priv->rps.hw_lock);
++		ret = sandybridge_pcode_read(dev_priv,
++					     GEN9_PCODE_READ_MEM_LATENCY,
++					     &val);
++		mutex_unlock(&dev_priv->rps.hw_lock);
++		if (ret) {
++			DRM_ERROR("SKL Mailbox read error = %d\n", ret);
++			return;
++		}
++
++		wm[4] = val & GEN9_MEM_LATENCY_LEVEL_MASK;
++		wm[5] = (val >> GEN9_MEM_LATENCY_LEVEL_1_5_SHIFT) &
++				GEN9_MEM_LATENCY_LEVEL_MASK;
++		wm[6] = (val >> GEN9_MEM_LATENCY_LEVEL_2_6_SHIFT) &
++				GEN9_MEM_LATENCY_LEVEL_MASK;
++		wm[7] = (val >> GEN9_MEM_LATENCY_LEVEL_3_7_SHIFT) &
++				GEN9_MEM_LATENCY_LEVEL_MASK;
++
++		/*
++		 * punit doesn't take into account the read latency so we need
++		 * to add 2us to the various latency levels we retrieve from
++		 * the punit.
++		 *   - W0 is a bit special in that it's the only level that
++		 *   can't be disabled if we want to have display working, so
++		 *   we always add 2us there.
++		 *   - For levels >=1, punit returns 0us latency when they are
++		 *   disabled, so we respect that and don't add 2us then
++		 *
++		 * Additionally, if a level n (n > 1) has a 0us latency, all
++		 * levels m (m >= n) need to be disabled. We make sure to
++		 * sanitize the values out of the punit to satisfy this
++		 * requirement.
++		 */
++		wm[0] += 2;
++		for (level = 1; level <= max_level; level++)
++			if (wm[level] != 0)
++				wm[level] += 2;
++			else {
++				for (i = level + 1; i <= max_level; i++)
++					wm[i] = 0;
++
++				break;
++			}
++	} else if (IS_HASWELL(dev) || IS_BROADWELL(dev)) {
+ 		uint64_t sskpd = I915_READ64(MCH_SSKPD);
+ 
+ 		wm[0] = (sskpd >> 56) & 0xFF;
+@@ -2114,7 +2402,9 @@
+ int ilk_wm_max_level(const struct drm_device *dev)
+ {
+ 	/* how many WM levels are we expecting */
+-	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
++	if (IS_GEN9(dev))
++		return 7;
++	else if (IS_HASWELL(dev) || IS_BROADWELL(dev))
+ 		return 4;
+ 	else if (INTEL_INFO(dev)->gen >= 6)
+ 		return 3;
+@@ -2124,7 +2414,7 @@
+ 
+ static void intel_print_wm_latency(struct drm_device *dev,
+ 				   const char *name,
+-				   const uint16_t wm[5])
++				   const uint16_t wm[8])
+ {
+ 	int level, max_level = ilk_wm_max_level(dev);
+ 
+@@ -2137,8 +2427,13 @@
+ 			continue;
+ 		}
+ 
+-		/* WM1+ latency values in 0.5us units */
+-		if (level > 0)
++		/*
++		 * - latencies are in us on gen9.
++		 * - before then, WM1+ latency values are in 0.5us units
++		 */
++		if (IS_GEN9(dev))
++			latency *= 10;
++		else if (level > 0)
+ 			latency *= 5;
+ 
+ 		DRM_DEBUG_KMS("%s WM%d latency %u (%u.%u usec)\n",
+@@ -2206,6 +2501,14 @@
+ 		snb_wm_latency_quirk(dev);
+ }
+ 
++static void skl_setup_wm_latency(struct drm_device *dev)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++
++	intel_read_wm_latency(dev, dev_priv->wm.skl_latency);
++	intel_print_wm_latency(dev, "Gen9 Plane", dev_priv->wm.skl_latency);
++}
++
+ static void ilk_compute_wm_parameters(struct drm_crtc *crtc,
+ 				      struct ilk_pipe_wm_parameters *p)
+ {
+@@ -2527,7 +2830,7 @@
+ #define WM_DIRTY_FBC (1 << 24)
+ #define WM_DIRTY_DDB (1 << 25)
+ 
+-static unsigned int ilk_compute_wm_dirty(struct drm_device *dev,
++static unsigned int ilk_compute_wm_dirty(struct drm_i915_private *dev_priv,
+ 					 const struct ilk_wm_values *old,
+ 					 const struct ilk_wm_values *new)
+ {
+@@ -2535,7 +2838,7 @@
+ 	enum pipe pipe;
+ 	int wm_lp;
+ 
+-	for_each_pipe(pipe) {
++	for_each_pipe(dev_priv, pipe) {
+ 		if (old->wm_linetime[pipe] != new->wm_linetime[pipe]) {
+ 			dirty |= WM_DIRTY_LINETIME(pipe);
+ 			/* Must disable LP1+ watermarks too */
+@@ -2621,7 +2924,7 @@
+ 	unsigned int dirty;
+ 	uint32_t val;
+ 
+-	dirty = ilk_compute_wm_dirty(dev, previous, results);
++	dirty = ilk_compute_wm_dirty(dev_priv, previous, results);
+ 	if (!dirty)
+ 		return;
+ 
+@@ -2696,4083 +2999,4041 @@
+ 	return _ilk_disable_lp_wm(dev_priv, WM_DIRTY_LP_ALL);
+ }
+ 
+-static void ilk_update_wm(struct drm_crtc *crtc)
+-{
+-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+-	struct drm_device *dev = crtc->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct ilk_wm_maximums max;
+-	struct ilk_pipe_wm_parameters params = {};
+-	struct ilk_wm_values results = {};
+-	enum intel_ddb_partitioning partitioning;
+-	struct intel_pipe_wm pipe_wm = {};
+-	struct intel_pipe_wm lp_wm_1_2 = {}, lp_wm_5_6 = {}, *best_lp_wm;
+-	struct intel_wm_config config = {};
++/*
++ * On gen9, we need to allocate Display Data Buffer (DDB) portions to the
++ * different active planes.
++ */
+ 
+-	ilk_compute_wm_parameters(crtc, &params);
++#define SKL_DDB_SIZE		896	/* in blocks */
+ 
+-	intel_compute_pipe_wm(crtc, &params, &pipe_wm);
++static void
++skl_ddb_get_pipe_allocation_limits(struct drm_device *dev,
++				   struct drm_crtc *for_crtc,
++				   const struct intel_wm_config *config,
++				   const struct skl_pipe_wm_parameters *params,
++				   struct skl_ddb_entry *alloc /* out */)
++{
++	struct drm_crtc *crtc;
++	unsigned int pipe_size, ddb_size;
++	int nth_active_pipe;
+ 
+-	if (!memcmp(&intel_crtc->wm.active, &pipe_wm, sizeof(pipe_wm)))
++	if (!params->active) {
++		alloc->start = 0;
++		alloc->end = 0;
+ 		return;
++	}
+ 
+-	intel_crtc->wm.active = pipe_wm;
++	ddb_size = SKL_DDB_SIZE;
+ 
+-	ilk_compute_wm_config(dev, &config);
++	ddb_size -= 4; /* 4 blocks for bypass path allocation */
+ 
+-	ilk_compute_wm_maximums(dev, 1, &config, INTEL_DDB_PART_1_2, &max);
+-	ilk_wm_merge(dev, &config, &max, &lp_wm_1_2);
++	nth_active_pipe = 0;
++	for_each_crtc(dev, crtc) {
++		if (!intel_crtc_active(crtc))
++			continue;
+ 
+-	/* 5/6 split only in single pipe config on IVB+ */
+-	if (INTEL_INFO(dev)->gen >= 7 &&
+-	    config.num_pipes_active == 1 && config.sprites_enabled) {
+-		ilk_compute_wm_maximums(dev, 1, &config, INTEL_DDB_PART_5_6, &max);
+-		ilk_wm_merge(dev, &config, &max, &lp_wm_5_6);
++		if (crtc == for_crtc)
++			break;
+ 
+-		best_lp_wm = ilk_find_best_result(dev, &lp_wm_1_2, &lp_wm_5_6);
+-	} else {
+-		best_lp_wm = &lp_wm_1_2;
++		nth_active_pipe++;
+ 	}
+ 
+-	partitioning = (best_lp_wm == &lp_wm_1_2) ?
+-		       INTEL_DDB_PART_1_2 : INTEL_DDB_PART_5_6;
++	pipe_size = ddb_size / config->num_pipes_active;
++	alloc->start = nth_active_pipe * ddb_size / config->num_pipes_active;
++	alloc->end = alloc->start + pipe_size;
++}
+ 
+-	ilk_compute_wm_results(dev, best_lp_wm, partitioning, &results);
++static unsigned int skl_cursor_allocation(const struct intel_wm_config *config)
++{
++	if (config->num_pipes_active == 1)
++		return 32;
+ 
+-	ilk_write_wm_values(dev_priv, &results);
++	return 8;
+ }
+ 
+-static void
+-ilk_update_sprite_wm(struct drm_plane *plane,
+-		     struct drm_crtc *crtc,
+-		     uint32_t sprite_width, uint32_t sprite_height,
+-		     int pixel_size, bool enabled, bool scaled)
++static void skl_ddb_entry_init_from_hw(struct skl_ddb_entry *entry, u32 reg)
+ {
+-	struct drm_device *dev = plane->dev;
+-	struct intel_plane *intel_plane = to_intel_plane(plane);
++	entry->start = reg & 0x3ff;
++	entry->end = (reg >> 16) & 0x3ff;
++	if (entry->end)
++		entry->end += 1;
++}
+ 
+-	intel_plane->wm.enabled = enabled;
+-	intel_plane->wm.scaled = scaled;
+-	intel_plane->wm.horiz_pixels = sprite_width;
+-	intel_plane->wm.vert_pixels = sprite_width;
+-	intel_plane->wm.bytes_per_pixel = pixel_size;
++void skl_ddb_get_hw_state(struct drm_i915_private *dev_priv,
++			  struct skl_ddb_allocation *ddb /* out */)
++{
++	struct drm_device *dev = dev_priv->dev;
++	enum pipe pipe;
++	int plane;
++	u32 val;
+ 
+-	/*
+-	 * IVB workaround: must disable low power watermarks for at least
+-	 * one frame before enabling scaling.  LP watermarks can be re-enabled
+-	 * when scaling is disabled.
+-	 *
+-	 * WaCxSRDisabledForSpriteScaling:ivb
+-	 */
+-	if (IS_IVYBRIDGE(dev) && scaled && ilk_disable_lp_wm(dev))
+-		intel_wait_for_vblank(dev, intel_plane->pipe);
++	for_each_pipe(dev_priv, pipe) {
++		for_each_plane(pipe, plane) {
++			val = I915_READ(PLANE_BUF_CFG(pipe, plane));
++			skl_ddb_entry_init_from_hw(&ddb->plane[pipe][plane],
++						   val);
++		}
+ 
+-	ilk_update_wm(crtc);
++		val = I915_READ(CUR_BUF_CFG(pipe));
++		skl_ddb_entry_init_from_hw(&ddb->cursor[pipe], val);
++	}
+ }
+ 
+-static void ilk_pipe_wm_get_hw_state(struct drm_crtc *crtc)
++static unsigned int
++skl_plane_relative_data_rate(const struct intel_plane_wm_parameters *p)
++{
++	return p->horiz_pixels * p->vert_pixels * p->bytes_per_pixel;
++}
++
++/*
++ * We don't overflow 32 bits. Worst case is 3 planes enabled, each fetching
++ * a 8192x4096@32bpp framebuffer:
++ *   3 * 4096 * 8192  * 4 < 2^32
++ */
++static unsigned int
++skl_get_total_relative_data_rate(struct intel_crtc *intel_crtc,
++				 const struct skl_pipe_wm_parameters *params)
++{
++	unsigned int total_data_rate = 0;
++	int plane;
++
++	for (plane = 0; plane < intel_num_planes(intel_crtc); plane++) {
++		const struct intel_plane_wm_parameters *p;
++
++		p = &params->plane[plane];
++		if (!p->enabled)
++			continue;
++
++		total_data_rate += skl_plane_relative_data_rate(p);
++	}
++
++	return total_data_rate;
++}
++
++static void
++skl_allocate_pipe_ddb(struct drm_crtc *crtc,
++		      const struct intel_wm_config *config,
++		      const struct skl_pipe_wm_parameters *params,
++		      struct skl_ddb_allocation *ddb /* out */)
+ {
+ 	struct drm_device *dev = crtc->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct ilk_wm_values *hw = &dev_priv->wm.hw;
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+-	struct intel_pipe_wm *active = &intel_crtc->wm.active;
+ 	enum pipe pipe = intel_crtc->pipe;
+-	static const unsigned int wm0_pipe_reg[] = {
+-		[PIPE_A] = WM0_PIPEA_ILK,
+-		[PIPE_B] = WM0_PIPEB_ILK,
+-		[PIPE_C] = WM0_PIPEC_IVB,
+-	};
++	struct skl_ddb_entry *alloc = &ddb->pipe[pipe];
++	uint16_t alloc_size, start, cursor_blocks;
++	unsigned int total_data_rate;
++	int plane;
+ 
+-	hw->wm_pipe[pipe] = I915_READ(wm0_pipe_reg[pipe]);
+-	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
+-		hw->wm_linetime[pipe] = I915_READ(PIPE_WM_LINETIME(pipe));
++	skl_ddb_get_pipe_allocation_limits(dev, crtc, config, params, alloc);
++	alloc_size = skl_ddb_entry_size(alloc);
++	if (alloc_size == 0) {
++		memset(ddb->plane[pipe], 0, sizeof(ddb->plane[pipe]));
++		memset(&ddb->cursor[pipe], 0, sizeof(ddb->cursor[pipe]));
++		return;
++	}
+ 
+-	active->pipe_enabled = intel_crtc_active(crtc);
++	cursor_blocks = skl_cursor_allocation(config);
++	ddb->cursor[pipe].start = alloc->end - cursor_blocks;
++	ddb->cursor[pipe].end = alloc->end;
+ 
+-	if (active->pipe_enabled) {
+-		u32 tmp = hw->wm_pipe[pipe];
++	alloc_size -= cursor_blocks;
++	alloc->end -= cursor_blocks;
+ 
+-		/*
+-		 * For active pipes LP0 watermark is marked as
+-		 * enabled, and LP1+ watermaks as disabled since
+-		 * we can't really reverse compute them in case
+-		 * multiple pipes are active.
+-		 */
+-		active->wm[0].enable = true;
+-		active->wm[0].pri_val = (tmp & WM0_PIPE_PLANE_MASK) >> WM0_PIPE_PLANE_SHIFT;
+-		active->wm[0].spr_val = (tmp & WM0_PIPE_SPRITE_MASK) >> WM0_PIPE_SPRITE_SHIFT;
+-		active->wm[0].cur_val = tmp & WM0_PIPE_CURSOR_MASK;
+-		active->linetime = hw->wm_linetime[pipe];
+-	} else {
+-		int level, max_level = ilk_wm_max_level(dev);
++	/*
++	 * Each active plane get a portion of the remaining space, in
++	 * proportion to the amount of data they need to fetch from memory.
++	 *
++	 * FIXME: we may not allocate every single block here.
++	 */
++	total_data_rate = skl_get_total_relative_data_rate(intel_crtc, params);
++
++	start = alloc->start;
++	for (plane = 0; plane < intel_num_planes(intel_crtc); plane++) {
++		const struct intel_plane_wm_parameters *p;
++		unsigned int data_rate;
++		uint16_t plane_blocks;
++
++		p = &params->plane[plane];
++		if (!p->enabled)
++			continue;
++
++		data_rate = skl_plane_relative_data_rate(p);
+ 
+ 		/*
+-		 * For inactive pipes, all watermark levels
+-		 * should be marked as enabled but zeroed,
+-		 * which is what we'd compute them to.
++		 * promote the expression to 64 bits to avoid overflowing, the
++		 * result is < available as data_rate / total_data_rate < 1
+ 		 */
+-		for (level = 0; level <= max_level; level++)
+-			active->wm[level].enable = true;
++		plane_blocks = div_u64((uint64_t)alloc_size * data_rate,
++				       total_data_rate);
++
++		ddb->plane[pipe][plane].start = start;
++		ddb->plane[pipe][plane].end = start + plane_blocks;
++
++		start += plane_blocks;
+ 	}
++
+ }
+ 
+-void ilk_wm_get_hw_state(struct drm_device *dev)
++static uint32_t skl_pipe_pixel_rate(const struct intel_crtc_config *config)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct ilk_wm_values *hw = &dev_priv->wm.hw;
+-	struct drm_crtc *crtc;
+-
+-	for_each_crtc(dev, crtc)
+-		ilk_pipe_wm_get_hw_state(crtc);
++	/* TODO: Take into account the scalers once we support them */
++	return config->adjusted_mode.crtc_clock;
++}
+ 
+-	hw->wm_lp[0] = I915_READ(WM1_LP_ILK);
+-	hw->wm_lp[1] = I915_READ(WM2_LP_ILK);
+-	hw->wm_lp[2] = I915_READ(WM3_LP_ILK);
++/*
++ * The max latency should be 257 (max the punit can code is 255 and we add 2us
++ * for the read latency) and bytes_per_pixel should always be <= 8, so that
++ * should allow pixel_rate up to ~2 GHz which seems sufficient since max
++ * 2xcdclk is 1350 MHz and the pixel rate should never exceed that.
++*/
++static uint32_t skl_wm_method1(uint32_t pixel_rate, uint8_t bytes_per_pixel,
++			       uint32_t latency)
++{
++	uint32_t wm_intermediate_val, ret;
+ 
+-	hw->wm_lp_spr[0] = I915_READ(WM1S_LP_ILK);
+-	if (INTEL_INFO(dev)->gen >= 7) {
+-		hw->wm_lp_spr[1] = I915_READ(WM2S_LP_IVB);
+-		hw->wm_lp_spr[2] = I915_READ(WM3S_LP_IVB);
+-	}
++	if (latency == 0)
++		return UINT_MAX;
+ 
+-	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
+-		hw->partitioning = (I915_READ(WM_MISC) & WM_MISC_DATA_PARTITION_5_6) ?
+-			INTEL_DDB_PART_5_6 : INTEL_DDB_PART_1_2;
+-	else if (IS_IVYBRIDGE(dev))
+-		hw->partitioning = (I915_READ(DISP_ARB_CTL2) & DISP_DATA_PARTITION_5_6) ?
+-			INTEL_DDB_PART_5_6 : INTEL_DDB_PART_1_2;
++	wm_intermediate_val = latency * pixel_rate * bytes_per_pixel;
++	ret = DIV_ROUND_UP(wm_intermediate_val, 1000);
+ 
+-	hw->enable_fbc_wm =
+-		!(I915_READ(DISP_ARB_CTL) & DISP_FBC_WM_DIS);
++	return ret;
+ }
+ 
+-/**
+- * intel_update_watermarks - update FIFO watermark values based on current modes
+- *
+- * Calculate watermark values for the various WM regs based on current mode
+- * and plane configuration.
+- *
+- * There are several cases to deal with here:
+- *   - normal (i.e. non-self-refresh)
+- *   - self-refresh (SR) mode
+- *   - lines are large relative to FIFO size (buffer can hold up to 2)
+- *   - lines are small relative to FIFO size (buffer can hold more than 2
+- *     lines), so need to account for TLB latency
+- *
+- *   The normal calculation is:
+- *     watermark = dotclock * bytes per pixel * latency
+- *   where latency is platform & configuration dependent (we assume pessimal
+- *   values here).
+- *
+- *   The SR calculation is:
+- *     watermark = (trunc(latency/line time)+1) * surface width *
+- *       bytes per pixel
+- *   where
+- *     line time = htotal / dotclock
+- *     surface width = hdisplay for normal plane and 64 for cursor
+- *   and latency is assumed to be high, as above.
+- *
+- * The final value programmed to the register should always be rounded up,
+- * and include an extra 2 entries to account for clock crossings.
+- *
+- * We don't use the sprite, so we can ignore that.  And on Crestline we have
+- * to set the non-SR watermarks to 8.
+- */
+-void intel_update_watermarks(struct drm_crtc *crtc)
++static uint32_t skl_wm_method2(uint32_t pixel_rate, uint32_t pipe_htotal,
++			       uint32_t horiz_pixels, uint8_t bytes_per_pixel,
++			       uint32_t latency)
+ {
+-	struct drm_i915_private *dev_priv = crtc->dev->dev_private;
++	uint32_t ret, plane_bytes_per_line, wm_intermediate_val;
+ 
+-	if (dev_priv->display.update_wm)
+-		dev_priv->display.update_wm(crtc);
++	if (latency == 0)
++		return UINT_MAX;
++
++	plane_bytes_per_line = horiz_pixels * bytes_per_pixel;
++	wm_intermediate_val = latency * pixel_rate;
++	ret = DIV_ROUND_UP(wm_intermediate_val, pipe_htotal * 1000) *
++				plane_bytes_per_line;
++
++	return ret;
+ }
+ 
+-void intel_update_sprite_watermarks(struct drm_plane *plane,
+-				    struct drm_crtc *crtc,
+-				    uint32_t sprite_width,
+-				    uint32_t sprite_height,
+-				    int pixel_size,
+-				    bool enabled, bool scaled)
++static bool skl_ddb_allocation_changed(const struct skl_ddb_allocation *new_ddb,
++				       const struct intel_crtc *intel_crtc)
+ {
+-	struct drm_i915_private *dev_priv = plane->dev->dev_private;
++	struct drm_device *dev = intel_crtc->base.dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	const struct skl_ddb_allocation *cur_ddb = &dev_priv->wm.skl_hw.ddb;
++	enum pipe pipe = intel_crtc->pipe;
+ 
+-	if (dev_priv->display.update_sprite_wm)
+-		dev_priv->display.update_sprite_wm(plane, crtc,
+-						   sprite_width, sprite_height,
+-						   pixel_size, enabled, scaled);
++	if (memcmp(new_ddb->plane[pipe], cur_ddb->plane[pipe],
++		   sizeof(new_ddb->plane[pipe])))
++		return true;
++
++	if (memcmp(&new_ddb->cursor[pipe], &cur_ddb->cursor[pipe],
++		    sizeof(new_ddb->cursor[pipe])))
++		return true;
++
++	return false;
+ }
+ 
+-static struct drm_i915_gem_object *
+-intel_alloc_context_page(struct drm_device *dev)
++static void skl_compute_wm_global_parameters(struct drm_device *dev,
++					     struct intel_wm_config *config)
+ {
+-	struct drm_i915_gem_object *ctx;
+-	int ret;
++	struct drm_crtc *crtc;
++	struct drm_plane *plane;
+ 
+-	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
++	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head)
++		config->num_pipes_active += intel_crtc_active(crtc);
+ 
+-	ctx = i915_gem_alloc_object(dev, 4096);
+-	if (!ctx) {
+-		DRM_DEBUG("failed to alloc power context, RC6 disabled\n");
+-		return NULL;
+-	}
++	/* FIXME: I don't think we need those two global parameters on SKL */
++	list_for_each_entry(plane, &dev->mode_config.plane_list, head) {
++		struct intel_plane *intel_plane = to_intel_plane(plane);
+ 
+-	ret = i915_gem_obj_ggtt_pin(ctx, 4096, 0);
+-	if (ret) {
+-		DRM_ERROR("failed to pin power context: %d\n", ret);
+-		goto err_unref;
++		config->sprites_enabled |= intel_plane->wm.enabled;
++		config->sprites_scaled |= intel_plane->wm.scaled;
+ 	}
++}
+ 
+-	ret = i915_gem_object_set_to_gtt_domain(ctx, 1);
+-	if (ret) {
+-		DRM_ERROR("failed to set-domain on power context: %d\n", ret);
+-		goto err_unpin;
++static void skl_compute_wm_pipe_parameters(struct drm_crtc *crtc,
++					   struct skl_pipe_wm_parameters *p)
++{
++	struct drm_device *dev = crtc->dev;
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	enum pipe pipe = intel_crtc->pipe;
++	struct drm_plane *plane;
++	int i = 1; /* Index for sprite planes start */
++
++	p->active = intel_crtc_active(crtc);
++	if (p->active) {
++		p->pipe_htotal = intel_crtc->config.adjusted_mode.crtc_htotal;
++		p->pixel_rate = skl_pipe_pixel_rate(&intel_crtc->config);
++
++		/*
++		 * For now, assume primary and cursor planes are always enabled.
++		 */
++		p->plane[0].enabled = true;
++		p->plane[0].bytes_per_pixel =
++			crtc->primary->fb->bits_per_pixel / 8;
++		p->plane[0].horiz_pixels = intel_crtc->config.pipe_src_w;
++		p->plane[0].vert_pixels = intel_crtc->config.pipe_src_h;
++
++		p->cursor.enabled = true;
++		p->cursor.bytes_per_pixel = 4;
++		p->cursor.horiz_pixels = intel_crtc->cursor_width ?
++					 intel_crtc->cursor_width : 64;
+ 	}
+ 
+-	return ctx;
++	list_for_each_entry(plane, &dev->mode_config.plane_list, head) {
++		struct intel_plane *intel_plane = to_intel_plane(plane);
+ 
+-err_unpin:
+-	i915_gem_object_ggtt_unpin(ctx);
+-err_unref:
+-	drm_gem_object_unreference(&ctx->base);
+-	return NULL;
++		if (intel_plane->pipe == pipe)
++			p->plane[i++] = intel_plane->wm;
++	}
+ }
+ 
+-/**
+- * Lock protecting IPS related data structures
+- */
+-DEFINE_SPINLOCK(mchdev_lock);
++static bool skl_compute_plane_wm(struct skl_pipe_wm_parameters *p,
++				 struct intel_plane_wm_parameters *p_params,
++				 uint16_t ddb_allocation,
++				 uint32_t mem_value,
++				 uint16_t *out_blocks, /* out */
++				 uint8_t *out_lines /* out */)
++{
++	uint32_t method1, method2, plane_bytes_per_line, res_blocks, res_lines;
++	uint32_t result_bytes;
+ 
+-/* Global for IPS driver to get at the current i915 device. Protected by
+- * mchdev_lock. */
+-static struct drm_i915_private *i915_mch_dev;
++	if (mem_value == 0 || !p->active || !p_params->enabled)
++		return false;
+ 
+-bool ironlake_set_drps(struct drm_device *dev, u8 val)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	u16 rgvswctl;
++	method1 = skl_wm_method1(p->pixel_rate,
++				 p_params->bytes_per_pixel,
++				 mem_value);
++	method2 = skl_wm_method2(p->pixel_rate,
++				 p->pipe_htotal,
++				 p_params->horiz_pixels,
++				 p_params->bytes_per_pixel,
++				 mem_value);
+ 
+-	assert_spin_locked(&mchdev_lock);
++	plane_bytes_per_line = p_params->horiz_pixels *
++					p_params->bytes_per_pixel;
+ 
+-	rgvswctl = I915_READ16(MEMSWCTL);
+-	if (rgvswctl & MEMCTL_CMD_STS) {
+-		DRM_DEBUG("gpu busy, RCS change rejected\n");
+-		return false; /* still busy with another command */
+-	}
++	/* For now xtile and linear */
++	if (((ddb_allocation * 512) / plane_bytes_per_line) >= 1)
++		result_bytes = min(method1, method2);
++	else
++		result_bytes = method1;
+ 
+-	rgvswctl = (MEMCTL_CMD_CHFREQ << MEMCTL_CMD_SHIFT) |
+-		(val << MEMCTL_FREQ_SHIFT) | MEMCTL_SFCAVM;
+-	I915_WRITE16(MEMSWCTL, rgvswctl);
+-	POSTING_READ16(MEMSWCTL);
++	res_blocks = DIV_ROUND_UP(result_bytes, 512) + 1;
++	res_lines = DIV_ROUND_UP(result_bytes, plane_bytes_per_line);
+ 
+-	rgvswctl |= MEMCTL_CMD_STS;
+-	I915_WRITE16(MEMSWCTL, rgvswctl);
++	if (res_blocks > ddb_allocation || res_lines > 31)
++		return false;
++
++	*out_blocks = res_blocks;
++	*out_lines = res_lines;
+ 
+ 	return true;
+ }
+ 
+-static void ironlake_enable_drps(struct drm_device *dev)
++static void skl_compute_wm_level(const struct drm_i915_private *dev_priv,
++				 struct skl_ddb_allocation *ddb,
++				 struct skl_pipe_wm_parameters *p,
++				 enum pipe pipe,
++				 int level,
++				 int num_planes,
++				 struct skl_wm_level *result)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	u32 rgvmodectl = I915_READ(MEMMODECTL);
+-	u8 fmax, fmin, fstart, vstart;
+-
+-	spin_lock_irq(&mchdev_lock);
++	uint16_t latency = dev_priv->wm.skl_latency[level];
++	uint16_t ddb_blocks;
++	int i;
+ 
+-	/* Enable temp reporting */
+-	I915_WRITE16(PMMISC, I915_READ(PMMISC) | MCPPCE_EN);
+-	I915_WRITE16(TSC1, I915_READ(TSC1) | TSE);
++	for (i = 0; i < num_planes; i++) {
++		ddb_blocks = skl_ddb_entry_size(&ddb->plane[pipe][i]);
+ 
+-	/* 100ms RC evaluation intervals */
+-	I915_WRITE(RCUPEI, 100000);
+-	I915_WRITE(RCDNEI, 100000);
++		result->plane_en[i] = skl_compute_plane_wm(p, &p->plane[i],
++						ddb_blocks,
++						latency,
++						&result->plane_res_b[i],
++						&result->plane_res_l[i]);
++	}
+ 
+-	/* Set max/min thresholds to 90ms and 80ms respectively */
+-	I915_WRITE(RCBMAXAVG, 90000);
+-	I915_WRITE(RCBMINAVG, 80000);
++	ddb_blocks = skl_ddb_entry_size(&ddb->cursor[pipe]);
++	result->cursor_en = skl_compute_plane_wm(p, &p->cursor, ddb_blocks,
++						 latency, &result->cursor_res_b,
++						 &result->cursor_res_l);
++}
+ 
+-	I915_WRITE(MEMIHYST, 1);
++static uint32_t
++skl_compute_linetime_wm(struct drm_crtc *crtc, struct skl_pipe_wm_parameters *p)
++{
++	if (!intel_crtc_active(crtc))
++		return 0;
+ 
+-	/* Set up min, max, and cur for interrupt handling */
+-	fmax = (rgvmodectl & MEMMODE_FMAX_MASK) >> MEMMODE_FMAX_SHIFT;
+-	fmin = (rgvmodectl & MEMMODE_FMIN_MASK);
+-	fstart = (rgvmodectl & MEMMODE_FSTART_MASK) >>
+-		MEMMODE_FSTART_SHIFT;
++	return DIV_ROUND_UP(8 * p->pipe_htotal * 1000, p->pixel_rate);
+ 
+-	vstart = (I915_READ(PXVFREQ_BASE + (fstart * 4)) & PXVFREQ_PX_MASK) >>
+-		PXVFREQ_PX_SHIFT;
++}
+ 
+-	dev_priv->ips.fmax = fmax; /* IPS callback will increase this */
+-	dev_priv->ips.fstart = fstart;
++static void skl_compute_transition_wm(struct drm_crtc *crtc,
++				      struct skl_pipe_wm_parameters *params,
++				      struct skl_wm_level *trans_wm /* out */)
++{
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	int i;
+ 
+-	dev_priv->ips.max_delay = fstart;
+-	dev_priv->ips.min_delay = fmin;
+-	dev_priv->ips.cur_delay = fstart;
++	if (!params->active)
++		return;
+ 
+-	DRM_DEBUG_DRIVER("fmax: %d, fmin: %d, fstart: %d\n",
+-			 fmax, fmin, fstart);
++	/* Until we know more, just disable transition WMs */
++	for (i = 0; i < intel_num_planes(intel_crtc); i++)
++		trans_wm->plane_en[i] = false;
++	trans_wm->cursor_en = false;
++}
+ 
+-	I915_WRITE(MEMINTREN, MEMINT_CX_SUPR_EN | MEMINT_EVAL_CHG_EN);
++static void skl_compute_pipe_wm(struct drm_crtc *crtc,
++				struct skl_ddb_allocation *ddb,
++				struct skl_pipe_wm_parameters *params,
++				struct skl_pipe_wm *pipe_wm)
++{
++	struct drm_device *dev = crtc->dev;
++	const struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	int level, max_level = ilk_wm_max_level(dev);
+ 
+-	/*
+-	 * Interrupts will be enabled in ironlake_irq_postinstall
+-	 */
++	for (level = 0; level <= max_level; level++) {
++		skl_compute_wm_level(dev_priv, ddb, params, intel_crtc->pipe,
++				     level, intel_num_planes(intel_crtc),
++				     &pipe_wm->wm[level]);
++	}
++	pipe_wm->linetime = skl_compute_linetime_wm(crtc, params);
+ 
+-	I915_WRITE(VIDSTART, vstart);
+-	POSTING_READ(VIDSTART);
++	skl_compute_transition_wm(crtc, params, &pipe_wm->trans_wm);
++}
+ 
+-	rgvmodectl |= MEMMODE_SWMODE_EN;
+-	I915_WRITE(MEMMODECTL, rgvmodectl);
++static void skl_compute_wm_results(struct drm_device *dev,
++				   struct skl_pipe_wm_parameters *p,
++				   struct skl_pipe_wm *p_wm,
++				   struct skl_wm_values *r,
++				   struct intel_crtc *intel_crtc)
++{
++	int level, max_level = ilk_wm_max_level(dev);
++	enum pipe pipe = intel_crtc->pipe;
++	uint32_t temp;
++	int i;
+ 
+-	if (wait_for_atomic((I915_READ(MEMSWCTL) & MEMCTL_CMD_STS) == 0, 10))
+-		DRM_ERROR("stuck trying to change perf mode\n");
+-	mdelay(1);
++	for (level = 0; level <= max_level; level++) {
++		for (i = 0; i < intel_num_planes(intel_crtc); i++) {
++			temp = 0;
+ 
+-	ironlake_set_drps(dev, fstart);
++			temp |= p_wm->wm[level].plane_res_l[i] <<
++					PLANE_WM_LINES_SHIFT;
++			temp |= p_wm->wm[level].plane_res_b[i];
++			if (p_wm->wm[level].plane_en[i])
++				temp |= PLANE_WM_EN;
+ 
+-	dev_priv->ips.last_count1 = I915_READ(0x112e4) + I915_READ(0x112e8) +
+-		I915_READ(0x112e0);
+-	dev_priv->ips.last_time1 = jiffies_to_msecs(jiffies);
+-	dev_priv->ips.last_count2 = I915_READ(0x112f4);
+-	dev_priv->ips.last_time2 = ktime_get_raw_ns();
++			r->plane[pipe][i][level] = temp;
++		}
+ 
+-	spin_unlock_irq(&mchdev_lock);
+-}
++		temp = 0;
+ 
+-static void ironlake_disable_drps(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	u16 rgvswctl;
++		temp |= p_wm->wm[level].cursor_res_l << PLANE_WM_LINES_SHIFT;
++		temp |= p_wm->wm[level].cursor_res_b;
+ 
+-	spin_lock_irq(&mchdev_lock);
++		if (p_wm->wm[level].cursor_en)
++			temp |= PLANE_WM_EN;
+ 
+-	rgvswctl = I915_READ16(MEMSWCTL);
++		r->cursor[pipe][level] = temp;
+ 
+-	/* Ack interrupts, disable EFC interrupt */
+-	I915_WRITE(MEMINTREN, I915_READ(MEMINTREN) & ~MEMINT_EVAL_CHG_EN);
+-	I915_WRITE(MEMINTRSTS, MEMINT_EVAL_CHG);
+-	I915_WRITE(DEIER, I915_READ(DEIER) & ~DE_PCU_EVENT);
+-	I915_WRITE(DEIIR, DE_PCU_EVENT);
+-	I915_WRITE(DEIMR, I915_READ(DEIMR) | DE_PCU_EVENT);
++	}
+ 
+-	/* Go back to the starting frequency */
+-	ironlake_set_drps(dev, dev_priv->ips.fstart);
+-	mdelay(1);
+-	rgvswctl |= MEMCTL_CMD_STS;
+-	I915_WRITE(MEMSWCTL, rgvswctl);
+-	mdelay(1);
++	/* transition WMs */
++	for (i = 0; i < intel_num_planes(intel_crtc); i++) {
++		temp = 0;
++		temp |= p_wm->trans_wm.plane_res_l[i] << PLANE_WM_LINES_SHIFT;
++		temp |= p_wm->trans_wm.plane_res_b[i];
++		if (p_wm->trans_wm.plane_en[i])
++			temp |= PLANE_WM_EN;
+ 
+-	spin_unlock_irq(&mchdev_lock);
+-}
++		r->plane_trans[pipe][i] = temp;
++	}
+ 
+-/* There's a funny hw issue where the hw returns all 0 when reading from
+- * GEN6_RP_INTERRUPT_LIMITS. Hence we always need to compute the desired value
+- * ourselves, instead of doing a rmw cycle (which might result in us clearing
+- * all limits and the gpu stuck at whatever frequency it is at atm).
+- */
+-static u32 gen6_rps_limits(struct drm_i915_private *dev_priv, u8 val)
+-{
+-	u32 limits;
++	temp = 0;
++	temp |= p_wm->trans_wm.cursor_res_l << PLANE_WM_LINES_SHIFT;
++	temp |= p_wm->trans_wm.cursor_res_b;
++	if (p_wm->trans_wm.cursor_en)
++		temp |= PLANE_WM_EN;
+ 
+-	/* Only set the down limit when we've reached the lowest level to avoid
+-	 * getting more interrupts, otherwise leave this clear. This prevents a
+-	 * race in the hw when coming out of rc6: There's a tiny window where
+-	 * the hw runs at the minimal clock before selecting the desired
+-	 * frequency, if the down threshold expires in that window we will not
+-	 * receive a down interrupt. */
+-	limits = dev_priv->rps.max_freq_softlimit << 24;
+-	if (val <= dev_priv->rps.min_freq_softlimit)
+-		limits |= dev_priv->rps.min_freq_softlimit << 16;
++	r->cursor_trans[pipe] = temp;
+ 
+-	return limits;
++	r->wm_linetime[pipe] = p_wm->linetime;
+ }
+ 
+-static void gen6_set_rps_thresholds(struct drm_i915_private *dev_priv, u8 val)
++static void skl_ddb_entry_write(struct drm_i915_private *dev_priv, uint32_t reg,
++				const struct skl_ddb_entry *entry)
+ {
+-	int new_power;
++	if (entry->end)
++		I915_WRITE(reg, (entry->end - 1) << 16 | entry->start);
++	else
++		I915_WRITE(reg, 0);
++}
+ 
+-	new_power = dev_priv->rps.power;
+-	switch (dev_priv->rps.power) {
+-	case LOW_POWER:
+-		if (val > dev_priv->rps.efficient_freq + 1 && val > dev_priv->rps.cur_freq)
+-			new_power = BETWEEN;
+-		break;
++static void skl_write_wm_values(struct drm_i915_private *dev_priv,
++				const struct skl_wm_values *new)
++{
++	struct drm_device *dev = dev_priv->dev;
++	struct intel_crtc *crtc;
+ 
+-	case BETWEEN:
+-		if (val <= dev_priv->rps.efficient_freq && val < dev_priv->rps.cur_freq)
+-			new_power = LOW_POWER;
+-		else if (val >= dev_priv->rps.rp0_freq && val > dev_priv->rps.cur_freq)
+-			new_power = HIGH_POWER;
+-		break;
++	list_for_each_entry(crtc, &dev->mode_config.crtc_list, base.head) {
++		int i, level, max_level = ilk_wm_max_level(dev);
++		enum pipe pipe = crtc->pipe;
+ 
+-	case HIGH_POWER:
+-		if (val < (dev_priv->rps.rp1_freq + dev_priv->rps.rp0_freq) >> 1 && val < dev_priv->rps.cur_freq)
+-			new_power = BETWEEN;
+-		break;
+-	}
+-	/* Max/min bins are special */
+-	if (val == dev_priv->rps.min_freq_softlimit)
+-		new_power = LOW_POWER;
+-	if (val == dev_priv->rps.max_freq_softlimit)
+-		new_power = HIGH_POWER;
+-	if (new_power == dev_priv->rps.power)
+-		return;
++		if (!new->dirty[pipe])
++			continue;
+ 
+-	/* Note the units here are not exactly 1us, but 1280ns. */
+-	switch (new_power) {
+-	case LOW_POWER:
+-		/* Upclock if more than 95% busy over 16ms */
+-		I915_WRITE(GEN6_RP_UP_EI, 12500);
+-		I915_WRITE(GEN6_RP_UP_THRESHOLD, 11800);
++		I915_WRITE(PIPE_WM_LINETIME(pipe), new->wm_linetime[pipe]);
+ 
+-		/* Downclock if less than 85% busy over 32ms */
+-		I915_WRITE(GEN6_RP_DOWN_EI, 25000);
+-		I915_WRITE(GEN6_RP_DOWN_THRESHOLD, 21250);
++		for (level = 0; level <= max_level; level++) {
++			for (i = 0; i < intel_num_planes(crtc); i++)
++				I915_WRITE(PLANE_WM(pipe, i, level),
++					   new->plane[pipe][i][level]);
++			I915_WRITE(CUR_WM(pipe, level),
++				   new->cursor[pipe][level]);
++		}
++		for (i = 0; i < intel_num_planes(crtc); i++)
++			I915_WRITE(PLANE_WM_TRANS(pipe, i),
++				   new->plane_trans[pipe][i]);
++		I915_WRITE(CUR_WM_TRANS(pipe), new->cursor_trans[pipe]);
++
++		for (i = 0; i < intel_num_planes(crtc); i++)
++			skl_ddb_entry_write(dev_priv,
++					    PLANE_BUF_CFG(pipe, i),
++					    &new->ddb.plane[pipe][i]);
+ 
+-		I915_WRITE(GEN6_RP_CONTROL,
+-			   GEN6_RP_MEDIA_TURBO |
+-			   GEN6_RP_MEDIA_HW_NORMAL_MODE |
+-			   GEN6_RP_MEDIA_IS_GFX |
+-			   GEN6_RP_ENABLE |
+-			   GEN6_RP_UP_BUSY_AVG |
+-			   GEN6_RP_DOWN_IDLE_AVG);
+-		break;
++		skl_ddb_entry_write(dev_priv, CUR_BUF_CFG(pipe),
++				    &new->ddb.cursor[pipe]);
++	}
++}
+ 
+-	case BETWEEN:
+-		/* Upclock if more than 90% busy over 13ms */
+-		I915_WRITE(GEN6_RP_UP_EI, 10250);
+-		I915_WRITE(GEN6_RP_UP_THRESHOLD, 9225);
++/*
++ * When setting up a new DDB allocation arrangement, we need to correctly
++ * sequence the times at which the new allocations for the pipes are taken into
++ * account or we'll have pipes fetching from space previously allocated to
++ * another pipe.
++ *
++ * Roughly the sequence looks like:
++ *  1. re-allocate the pipe(s) with the allocation being reduced and not
++ *     overlapping with a previous light-up pipe (another way to put it is:
++ *     pipes with their new allocation strickly included into their old ones).
++ *  2. re-allocate the other pipes that get their allocation reduced
++ *  3. allocate the pipes having their allocation increased
++ *
++ * Steps 1. and 2. are here to take care of the following case:
++ * - Initially DDB looks like this:
++ *     |   B    |   C    |
++ * - enable pipe A.
++ * - pipe B has a reduced DDB allocation that overlaps with the old pipe C
++ *   allocation
++ *     |  A  |  B  |  C  |
++ *
++ * We need to sequence the re-allocation: C, B, A (and not B, C, A).
++ */
+ 
+-		/* Downclock if less than 75% busy over 32ms */
+-		I915_WRITE(GEN6_RP_DOWN_EI, 25000);
+-		I915_WRITE(GEN6_RP_DOWN_THRESHOLD, 18750);
++static void
++skl_wm_flush_pipe(struct drm_i915_private *dev_priv, enum pipe pipe, int pass)
++{
++	struct drm_device *dev = dev_priv->dev;
++	int plane;
+ 
+-		I915_WRITE(GEN6_RP_CONTROL,
+-			   GEN6_RP_MEDIA_TURBO |
+-			   GEN6_RP_MEDIA_HW_NORMAL_MODE |
+-			   GEN6_RP_MEDIA_IS_GFX |
+-			   GEN6_RP_ENABLE |
+-			   GEN6_RP_UP_BUSY_AVG |
+-			   GEN6_RP_DOWN_IDLE_AVG);
+-		break;
++	DRM_DEBUG_KMS("flush pipe %c (pass %d)\n", pipe_name(pipe), pass);
+ 
+-	case HIGH_POWER:
+-		/* Upclock if more than 85% busy over 10ms */
+-		I915_WRITE(GEN6_RP_UP_EI, 8000);
+-		I915_WRITE(GEN6_RP_UP_THRESHOLD, 6800);
++	for_each_plane(pipe, plane) {
++		I915_WRITE(PLANE_SURF(pipe, plane),
++			   I915_READ(PLANE_SURF(pipe, plane)));
++	}
++	I915_WRITE(CURBASE(pipe), I915_READ(CURBASE(pipe)));
++}
+ 
+-		/* Downclock if less than 60% busy over 32ms */
+-		I915_WRITE(GEN6_RP_DOWN_EI, 25000);
+-		I915_WRITE(GEN6_RP_DOWN_THRESHOLD, 15000);
++static bool
++skl_ddb_allocation_included(const struct skl_ddb_allocation *old,
++			    const struct skl_ddb_allocation *new,
++			    enum pipe pipe)
++{
++	uint16_t old_size, new_size;
+ 
+-		I915_WRITE(GEN6_RP_CONTROL,
+-			   GEN6_RP_MEDIA_TURBO |
+-			   GEN6_RP_MEDIA_HW_NORMAL_MODE |
+-			   GEN6_RP_MEDIA_IS_GFX |
+-			   GEN6_RP_ENABLE |
+-			   GEN6_RP_UP_BUSY_AVG |
+-			   GEN6_RP_DOWN_IDLE_AVG);
+-		break;
+-	}
++	old_size = skl_ddb_entry_size(&old->pipe[pipe]);
++	new_size = skl_ddb_entry_size(&new->pipe[pipe]);
+ 
+-	dev_priv->rps.power = new_power;
+-	dev_priv->rps.last_adj = 0;
++	return old_size != new_size &&
++	       new->pipe[pipe].start >= old->pipe[pipe].start &&
++	       new->pipe[pipe].end <= old->pipe[pipe].end;
+ }
+ 
+-static u32 gen6_rps_pm_mask(struct drm_i915_private *dev_priv, u8 val)
++static void skl_flush_wm_values(struct drm_i915_private *dev_priv,
++				struct skl_wm_values *new_values)
+ {
+-	u32 mask = 0;
+-
+-	if (val > dev_priv->rps.min_freq_softlimit)
+-		mask |= GEN6_PM_RP_DOWN_THRESHOLD | GEN6_PM_RP_DOWN_TIMEOUT;
+-	if (val < dev_priv->rps.max_freq_softlimit)
+-		mask |= GEN6_PM_RP_UP_THRESHOLD;
++	struct drm_device *dev = dev_priv->dev;
++	struct skl_ddb_allocation *cur_ddb, *new_ddb;
++	bool reallocated[I915_MAX_PIPES] = {false, false, false};
++	struct intel_crtc *crtc;
++	enum pipe pipe;
+ 
+-	mask |= dev_priv->pm_rps_events & (GEN6_PM_RP_DOWN_EI_EXPIRED | GEN6_PM_RP_UP_EI_EXPIRED);
+-	mask &= dev_priv->pm_rps_events;
++	new_ddb = &new_values->ddb;
++	cur_ddb = &dev_priv->wm.skl_hw.ddb;
+ 
+-	/* IVB and SNB hard hangs on looping batchbuffer
+-	 * if GEN6_PM_UP_EI_EXPIRED is masked.
++	/*
++	 * First pass: flush the pipes with the new allocation contained into
++	 * the old space.
++	 *
++	 * We'll wait for the vblank on those pipes to ensure we can safely
++	 * re-allocate the freed space without this pipe fetching from it.
+ 	 */
+-	if (INTEL_INFO(dev_priv->dev)->gen <= 7 && !IS_HASWELL(dev_priv->dev))
+-		mask |= GEN6_PM_RP_UP_EI_EXPIRED;
++	for_each_intel_crtc(dev, crtc) {
++		if (!crtc->active)
++			continue;
+ 
+-	if (IS_GEN8(dev_priv->dev))
+-		mask |= GEN8_PMINTR_REDIRECT_TO_NON_DISP;
++		pipe = crtc->pipe;
+ 
+-	return ~mask;
+-}
++		if (!skl_ddb_allocation_included(cur_ddb, new_ddb, pipe))
++			continue;
+ 
+-/* gen6_set_rps is called to update the frequency request, but should also be
+- * called when the range (min_delay and max_delay) is modified so that we can
+- * update the GEN6_RP_INTERRUPT_LIMITS register accordingly. */
+-void gen6_set_rps(struct drm_device *dev, u8 val)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++		skl_wm_flush_pipe(dev_priv, pipe, 1);
++		intel_wait_for_vblank(dev, pipe);
+ 
+-	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
+-	WARN_ON(val > dev_priv->rps.max_freq_softlimit);
+-	WARN_ON(val < dev_priv->rps.min_freq_softlimit);
++		reallocated[pipe] = true;
++	}
+ 
+-	/* min/max delay may still have been modified so be sure to
+-	 * write the limits value.
++
++	/*
++	 * Second pass: flush the pipes that are having their allocation
++	 * reduced, but overlapping with a previous allocation.
++	 *
++	 * Here as well we need to wait for the vblank to make sure the freed
++	 * space is not used anymore.
+ 	 */
+-	if (val != dev_priv->rps.cur_freq) {
+-		gen6_set_rps_thresholds(dev_priv, val);
++	for_each_intel_crtc(dev, crtc) {
++		if (!crtc->active)
++			continue;
+ 
+-		if (IS_HASWELL(dev) || IS_BROADWELL(dev))
+-			I915_WRITE(GEN6_RPNSWREQ,
+-				   HSW_FREQUENCY(val));
+-		else
+-			I915_WRITE(GEN6_RPNSWREQ,
+-				   GEN6_FREQUENCY(val) |
+-				   GEN6_OFFSET(0) |
+-				   GEN6_AGGRESSIVE_TURBO);
++		pipe = crtc->pipe;
++
++		if (reallocated[pipe])
++			continue;
++
++		if (skl_ddb_entry_size(&new_ddb->pipe[pipe]) <
++		    skl_ddb_entry_size(&cur_ddb->pipe[pipe])) {
++			skl_wm_flush_pipe(dev_priv, pipe, 2);
++			intel_wait_for_vblank(dev, pipe);
++		}
++
++		reallocated[pipe] = true;
+ 	}
+ 
+-	/* Make sure we continue to get interrupts
+-	 * until we hit the minimum or maximum frequencies.
++	/*
++	 * Third pass: flush the pipes that got more space allocated.
++	 *
++	 * We don't need to actively wait for the update here, next vblank
++	 * will just get more DDB space with the correct WM values.
+ 	 */
+-	I915_WRITE(GEN6_RP_INTERRUPT_LIMITS, gen6_rps_limits(dev_priv, val));
+-	I915_WRITE(GEN6_PMINTRMSK, gen6_rps_pm_mask(dev_priv, val));
++	for_each_intel_crtc(dev, crtc) {
++		if (!crtc->active)
++			continue;
+ 
+-	POSTING_READ(GEN6_RPNSWREQ);
++		pipe = crtc->pipe;
+ 
+-	dev_priv->rps.cur_freq = val;
+-	trace_intel_gpu_freq_change(val * 50);
++		/*
++		 * At this point, only the pipes more space than before are
++		 * left to re-allocate.
++		 */
++		if (reallocated[pipe])
++			continue;
++
++		skl_wm_flush_pipe(dev_priv, pipe, 3);
++	}
+ }
+ 
+-/* vlv_set_rps_idle: Set the frequency to Rpn if Gfx clocks are down
+- *
+- * * If Gfx is Idle, then
+- * 1. Mask Turbo interrupts
+- * 2. Bring up Gfx clock
+- * 3. Change the freq to Rpn and wait till P-Unit updates freq
+- * 4. Clear the Force GFX CLK ON bit so that Gfx can down
+- * 5. Unmask Turbo interrupts
+-*/
+-static void vlv_set_rps_idle(struct drm_i915_private *dev_priv)
++static bool skl_update_pipe_wm(struct drm_crtc *crtc,
++			       struct skl_pipe_wm_parameters *params,
++			       struct intel_wm_config *config,
++			       struct skl_ddb_allocation *ddb, /* out */
++			       struct skl_pipe_wm *pipe_wm /* out */)
+ {
+-	struct drm_device *dev = dev_priv->dev;
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+ 
+-	/* Latest VLV doesn't need to force the gfx clock */
+-	if (dev->pdev->revision >= 0xd) {
+-		valleyview_set_rps(dev_priv->dev, dev_priv->rps.min_freq_softlimit);
+-		return;
+-	}
++	skl_compute_wm_pipe_parameters(crtc, params);
++	skl_allocate_pipe_ddb(crtc, config, params, ddb);
++	skl_compute_pipe_wm(crtc, ddb, params, pipe_wm);
++
++	if (!memcmp(&intel_crtc->wm.skl_active, pipe_wm, sizeof(*pipe_wm)))
++		return false;
++
++	intel_crtc->wm.skl_active = *pipe_wm;
++	return true;
++}
++
++static void skl_update_other_pipe_wm(struct drm_device *dev,
++				     struct drm_crtc *crtc,
++				     struct intel_wm_config *config,
++				     struct skl_wm_values *r)
++{
++	struct intel_crtc *intel_crtc;
++	struct intel_crtc *this_crtc = to_intel_crtc(crtc);
+ 
+ 	/*
+-	 * When we are idle.  Drop to min voltage state.
++	 * If the WM update hasn't changed the allocation for this_crtc (the
++	 * crtc we are currently computing the new WM values for), other
++	 * enabled crtcs will keep the same allocation and we don't need to
++	 * recompute anything for them.
+ 	 */
+-
+-	if (dev_priv->rps.cur_freq <= dev_priv->rps.min_freq_softlimit)
++	if (!skl_ddb_allocation_changed(&r->ddb, this_crtc))
+ 		return;
+ 
+-	/* Mask turbo interrupt so that they will not come in between */
+-	I915_WRITE(GEN6_PMINTRMSK, 0xffffffff);
+-
+-	vlv_force_gfx_clock(dev_priv, true);
++	/*
++	 * Otherwise, because of this_crtc being freshly enabled/disabled, the
++	 * other active pipes need new DDB allocation and WM values.
++	 */
++	list_for_each_entry(intel_crtc, &dev->mode_config.crtc_list,
++				base.head) {
++		struct skl_pipe_wm_parameters params = {};
++		struct skl_pipe_wm pipe_wm = {};
++		bool wm_changed;
+ 
+-	dev_priv->rps.cur_freq = dev_priv->rps.min_freq_softlimit;
++		if (this_crtc->pipe == intel_crtc->pipe)
++			continue;
+ 
+-	vlv_punit_write(dev_priv, PUNIT_REG_GPU_FREQ_REQ,
+-					dev_priv->rps.min_freq_softlimit);
++		if (!intel_crtc->active)
++			continue;
+ 
+-	if (wait_for(((vlv_punit_read(dev_priv, PUNIT_REG_GPU_FREQ_STS))
+-				& GENFREQSTATUS) == 0, 5))
+-		DRM_ERROR("timed out waiting for Punit\n");
++		wm_changed = skl_update_pipe_wm(&intel_crtc->base,
++						&params, config,
++						&r->ddb, &pipe_wm);
+ 
+-	vlv_force_gfx_clock(dev_priv, false);
++		/*
++		 * If we end up re-computing the other pipe WM values, it's
++		 * because it was really needed, so we expect the WM values to
++		 * be different.
++		 */
++		WARN_ON(!wm_changed);
+ 
+-	I915_WRITE(GEN6_PMINTRMSK,
+-		   gen6_rps_pm_mask(dev_priv, dev_priv->rps.cur_freq));
++		skl_compute_wm_results(dev, &params, &pipe_wm, r, intel_crtc);
++		r->dirty[intel_crtc->pipe] = true;
++	}
+ }
+ 
+-void gen6_rps_idle(struct drm_i915_private *dev_priv)
++static void skl_update_wm(struct drm_crtc *crtc)
+ {
+-	struct drm_device *dev = dev_priv->dev;
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	struct drm_device *dev = crtc->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct skl_pipe_wm_parameters params = {};
++	struct skl_wm_values *results = &dev_priv->wm.skl_results;
++	struct skl_pipe_wm pipe_wm = {};
++	struct intel_wm_config config = {};
+ 
+-	mutex_lock(&dev_priv->rps.hw_lock);
+-	if (dev_priv->rps.enabled) {
+-		if (IS_CHERRYVIEW(dev))
+-			valleyview_set_rps(dev_priv->dev, dev_priv->rps.min_freq_softlimit);
+-		else if (IS_VALLEYVIEW(dev))
+-			vlv_set_rps_idle(dev_priv);
+-		else
+-			gen6_set_rps(dev_priv->dev, dev_priv->rps.min_freq_softlimit);
+-		dev_priv->rps.last_adj = 0;
+-	}
+-	mutex_unlock(&dev_priv->rps.hw_lock);
+-}
++	memset(results, 0, sizeof(*results));
+ 
+-void gen6_rps_boost(struct drm_i915_private *dev_priv)
+-{
+-	struct drm_device *dev = dev_priv->dev;
++	skl_compute_wm_global_parameters(dev, &config);
+ 
+-	mutex_lock(&dev_priv->rps.hw_lock);
+-	if (dev_priv->rps.enabled) {
+-		if (IS_VALLEYVIEW(dev))
+-			valleyview_set_rps(dev_priv->dev, dev_priv->rps.max_freq_softlimit);
+-		else
+-			gen6_set_rps(dev_priv->dev, dev_priv->rps.max_freq_softlimit);
+-		dev_priv->rps.last_adj = 0;
+-	}
+-	mutex_unlock(&dev_priv->rps.hw_lock);
+-}
++	if (!skl_update_pipe_wm(crtc, &params, &config,
++				&results->ddb, &pipe_wm))
++		return;
+ 
+-void valleyview_set_rps(struct drm_device *dev, u8 val)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	skl_compute_wm_results(dev, &params, &pipe_wm, results, intel_crtc);
++	results->dirty[intel_crtc->pipe] = true;
+ 
+-	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
+-	WARN_ON(val > dev_priv->rps.max_freq_softlimit);
+-	WARN_ON(val < dev_priv->rps.min_freq_softlimit);
++	skl_update_other_pipe_wm(dev, crtc, &config, results);
++	skl_write_wm_values(dev_priv, results);
++	skl_flush_wm_values(dev_priv, results);
+ 
+-	DRM_DEBUG_DRIVER("GPU freq request from %d MHz (%u) to %d MHz (%u)\n",
+-			 vlv_gpu_freq(dev_priv, dev_priv->rps.cur_freq),
+-			 dev_priv->rps.cur_freq,
+-			 vlv_gpu_freq(dev_priv, val), val);
++	/* store the new configuration */
++	dev_priv->wm.skl_hw = *results;
++}
+ 
+-	if (val != dev_priv->rps.cur_freq)
+-		vlv_punit_write(dev_priv, PUNIT_REG_GPU_FREQ_REQ, val);
++static void
++skl_update_sprite_wm(struct drm_plane *plane, struct drm_crtc *crtc,
++		     uint32_t sprite_width, uint32_t sprite_height,
++		     int pixel_size, bool enabled, bool scaled)
++{
++	struct intel_plane *intel_plane = to_intel_plane(plane);
+ 
+-	I915_WRITE(GEN6_PMINTRMSK, gen6_rps_pm_mask(dev_priv, val));
++	intel_plane->wm.enabled = enabled;
++	intel_plane->wm.scaled = scaled;
++	intel_plane->wm.horiz_pixels = sprite_width;
++	intel_plane->wm.vert_pixels = sprite_height;
++	intel_plane->wm.bytes_per_pixel = pixel_size;
+ 
+-	dev_priv->rps.cur_freq = val;
+-	trace_intel_gpu_freq_change(vlv_gpu_freq(dev_priv, val));
++	skl_update_wm(crtc);
+ }
+ 
+-static void gen8_disable_rps_interrupts(struct drm_device *dev)
++static void ilk_update_wm(struct drm_crtc *crtc)
+ {
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	struct drm_device *dev = crtc->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct ilk_wm_maximums max;
++	struct ilk_pipe_wm_parameters params = {};
++	struct ilk_wm_values results = {};
++	enum intel_ddb_partitioning partitioning;
++	struct intel_pipe_wm pipe_wm = {};
++	struct intel_pipe_wm lp_wm_1_2 = {}, lp_wm_5_6 = {}, *best_lp_wm;
++	struct intel_wm_config config = {};
+ 
+-	I915_WRITE(GEN6_PMINTRMSK, ~GEN8_PMINTR_REDIRECT_TO_NON_DISP);
+-	I915_WRITE(GEN8_GT_IER(2), I915_READ(GEN8_GT_IER(2)) &
+-				   ~dev_priv->pm_rps_events);
+-	/* Complete PM interrupt masking here doesn't race with the rps work
+-	 * item again unmasking PM interrupts because that is using a different
+-	 * register (GEN8_GT_IMR(2)) to mask PM interrupts. The only risk is in
+-	 * leaving stale bits in GEN8_GT_IIR(2) and GEN8_GT_IMR(2) which
+-	 * gen8_enable_rps will clean up. */
+-
+-	spin_lock_irq(&dev_priv->irq_lock);
+-	dev_priv->rps.pm_iir = 0;
+-	spin_unlock_irq(&dev_priv->irq_lock);
++	ilk_compute_wm_parameters(crtc, &params);
+ 
+-	I915_WRITE(GEN8_GT_IIR(2), dev_priv->pm_rps_events);
+-}
++	intel_compute_pipe_wm(crtc, &params, &pipe_wm);
+ 
+-static void gen6_disable_rps_interrupts(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	if (!memcmp(&intel_crtc->wm.active, &pipe_wm, sizeof(pipe_wm)))
++		return;
+ 
+-	I915_WRITE(GEN6_PMINTRMSK, 0xffffffff);
+-	I915_WRITE(GEN6_PMIER, I915_READ(GEN6_PMIER) &
+-				~dev_priv->pm_rps_events);
+-	/* Complete PM interrupt masking here doesn't race with the rps work
+-	 * item again unmasking PM interrupts because that is using a different
+-	 * register (PMIMR) to mask PM interrupts. The only risk is in leaving
+-	 * stale bits in PMIIR and PMIMR which gen6_enable_rps will clean up. */
+-
+-	spin_lock_irq(&dev_priv->irq_lock);
+-	dev_priv->rps.pm_iir = 0;
+-	spin_unlock_irq(&dev_priv->irq_lock);
++	intel_crtc->wm.active = pipe_wm;
+ 
+-	I915_WRITE(GEN6_PMIIR, dev_priv->pm_rps_events);
+-}
++	ilk_compute_wm_config(dev, &config);
+ 
+-static void gen6_disable_rps(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	ilk_compute_wm_maximums(dev, 1, &config, INTEL_DDB_PART_1_2, &max);
++	ilk_wm_merge(dev, &config, &max, &lp_wm_1_2);
+ 
+-	I915_WRITE(GEN6_RC_CONTROL, 0);
+-	I915_WRITE(GEN6_RPNSWREQ, 1 << 31);
++	/* 5/6 split only in single pipe config on IVB+ */
++	if (INTEL_INFO(dev)->gen >= 7 &&
++	    config.num_pipes_active == 1 && config.sprites_enabled) {
++		ilk_compute_wm_maximums(dev, 1, &config, INTEL_DDB_PART_5_6, &max);
++		ilk_wm_merge(dev, &config, &max, &lp_wm_5_6);
+ 
+-	if (IS_BROADWELL(dev))
+-		gen8_disable_rps_interrupts(dev);
+-	else
+-		gen6_disable_rps_interrupts(dev);
+-}
++		best_lp_wm = ilk_find_best_result(dev, &lp_wm_1_2, &lp_wm_5_6);
++	} else {
++		best_lp_wm = &lp_wm_1_2;
++	}
+ 
+-static void cherryview_disable_rps(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	partitioning = (best_lp_wm == &lp_wm_1_2) ?
++		       INTEL_DDB_PART_1_2 : INTEL_DDB_PART_5_6;
+ 
+-	I915_WRITE(GEN6_RC_CONTROL, 0);
++	ilk_compute_wm_results(dev, best_lp_wm, partitioning, &results);
+ 
+-	gen8_disable_rps_interrupts(dev);
++	ilk_write_wm_values(dev_priv, &results);
+ }
+ 
+-static void valleyview_disable_rps(struct drm_device *dev)
++static void
++ilk_update_sprite_wm(struct drm_plane *plane,
++		     struct drm_crtc *crtc,
++		     uint32_t sprite_width, uint32_t sprite_height,
++		     int pixel_size, bool enabled, bool scaled)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_device *dev = plane->dev;
++	struct intel_plane *intel_plane = to_intel_plane(plane);
+ 
+-	I915_WRITE(GEN6_RC_CONTROL, 0);
++	intel_plane->wm.enabled = enabled;
++	intel_plane->wm.scaled = scaled;
++	intel_plane->wm.horiz_pixels = sprite_width;
++	intel_plane->wm.vert_pixels = sprite_width;
++	intel_plane->wm.bytes_per_pixel = pixel_size;
+ 
+-	gen6_disable_rps_interrupts(dev);
++	/*
++	 * IVB workaround: must disable low power watermarks for at least
++	 * one frame before enabling scaling.  LP watermarks can be re-enabled
++	 * when scaling is disabled.
++	 *
++	 * WaCxSRDisabledForSpriteScaling:ivb
++	 */
++	if (IS_IVYBRIDGE(dev) && scaled && ilk_disable_lp_wm(dev))
++		intel_wait_for_vblank(dev, intel_plane->pipe);
++
++	ilk_update_wm(crtc);
+ }
+ 
+-static void intel_print_rc6_info(struct drm_device *dev, u32 mode)
+-{
+-	if (IS_VALLEYVIEW(dev)) {
+-		if (mode & (GEN7_RC_CTL_TO_MODE | GEN6_RC_CTL_EI_MODE(1)))
+-			mode = GEN6_RC_CTL_RC6_ENABLE;
+-		else
+-			mode = 0;
++static void skl_pipe_wm_active_state(uint32_t val,
++				     struct skl_pipe_wm *active,
++				     bool is_transwm,
++				     bool is_cursor,
++				     int i,
++				     int level)
++{
++	bool is_enabled = (val & PLANE_WM_EN) != 0;
++
++	if (!is_transwm) {
++		if (!is_cursor) {
++			active->wm[level].plane_en[i] = is_enabled;
++			active->wm[level].plane_res_b[i] =
++					val & PLANE_WM_BLOCKS_MASK;
++			active->wm[level].plane_res_l[i] =
++					(val >> PLANE_WM_LINES_SHIFT) &
++						PLANE_WM_LINES_MASK;
++		} else {
++			active->wm[level].cursor_en = is_enabled;
++			active->wm[level].cursor_res_b =
++					val & PLANE_WM_BLOCKS_MASK;
++			active->wm[level].cursor_res_l =
++					(val >> PLANE_WM_LINES_SHIFT) &
++						PLANE_WM_LINES_MASK;
++		}
++	} else {
++		if (!is_cursor) {
++			active->trans_wm.plane_en[i] = is_enabled;
++			active->trans_wm.plane_res_b[i] =
++					val & PLANE_WM_BLOCKS_MASK;
++			active->trans_wm.plane_res_l[i] =
++					(val >> PLANE_WM_LINES_SHIFT) &
++						PLANE_WM_LINES_MASK;
++		} else {
++			active->trans_wm.cursor_en = is_enabled;
++			active->trans_wm.cursor_res_b =
++					val & PLANE_WM_BLOCKS_MASK;
++			active->trans_wm.cursor_res_l =
++					(val >> PLANE_WM_LINES_SHIFT) &
++						PLANE_WM_LINES_MASK;
++		}
+ 	}
+-	DRM_DEBUG_KMS("Enabling RC6 states: RC6 %s, RC6p %s, RC6pp %s\n",
+-		      (mode & GEN6_RC_CTL_RC6_ENABLE) ? "on" : "off",
+-		      (mode & GEN6_RC_CTL_RC6p_ENABLE) ? "on" : "off",
+-		      (mode & GEN6_RC_CTL_RC6pp_ENABLE) ? "on" : "off");
+ }
+ 
+-static int sanitize_rc6_option(const struct drm_device *dev, int enable_rc6)
++static void skl_pipe_wm_get_hw_state(struct drm_crtc *crtc)
+ {
+-	/* No RC6 before Ironlake */
+-	if (INTEL_INFO(dev)->gen < 5)
+-		return 0;
++	struct drm_device *dev = crtc->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct skl_wm_values *hw = &dev_priv->wm.skl_hw;
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	struct skl_pipe_wm *active = &intel_crtc->wm.skl_active;
++	enum pipe pipe = intel_crtc->pipe;
++	int level, i, max_level;
++	uint32_t temp;
+ 
+-	/* RC6 is only on Ironlake mobile not on desktop */
+-	if (INTEL_INFO(dev)->gen == 5 && !IS_IRONLAKE_M(dev))
+-		return 0;
++	max_level = ilk_wm_max_level(dev);
+ 
+-	/* Respect the kernel parameter if it is set */
+-	if (enable_rc6 >= 0) {
+-		int mask;
++	hw->wm_linetime[pipe] = I915_READ(PIPE_WM_LINETIME(pipe));
+ 
+-		if (INTEL_INFO(dev)->gen == 6 || IS_IVYBRIDGE(dev))
+-			mask = INTEL_RC6_ENABLE | INTEL_RC6p_ENABLE |
+-			       INTEL_RC6pp_ENABLE;
+-		else
+-			mask = INTEL_RC6_ENABLE;
++	for (level = 0; level <= max_level; level++) {
++		for (i = 0; i < intel_num_planes(intel_crtc); i++)
++			hw->plane[pipe][i][level] =
++					I915_READ(PLANE_WM(pipe, i, level));
++		hw->cursor[pipe][level] = I915_READ(CUR_WM(pipe, level));
++	}
+ 
+-		if ((enable_rc6 & mask) != enable_rc6)
+-			DRM_DEBUG_KMS("Adjusting RC6 mask to %d (requested %d, valid %d)\n",
+-				      enable_rc6 & mask, enable_rc6, mask);
++	for (i = 0; i < intel_num_planes(intel_crtc); i++)
++		hw->plane_trans[pipe][i] = I915_READ(PLANE_WM_TRANS(pipe, i));
++	hw->cursor_trans[pipe] = I915_READ(CUR_WM_TRANS(pipe));
+ 
+-		return enable_rc6 & mask;
+-	}
++	if (!intel_crtc_active(crtc))
++		return;
+ 
+-	/* Disable RC6 on Ironlake */
+-	if (INTEL_INFO(dev)->gen == 5)
+-		return 0;
++	hw->dirty[pipe] = true;
+ 
+-	if (IS_IVYBRIDGE(dev))
+-		return (INTEL_RC6_ENABLE | INTEL_RC6p_ENABLE);
++	active->linetime = hw->wm_linetime[pipe];
+ 
+-	return INTEL_RC6_ENABLE;
+-}
++	for (level = 0; level <= max_level; level++) {
++		for (i = 0; i < intel_num_planes(intel_crtc); i++) {
++			temp = hw->plane[pipe][i][level];
++			skl_pipe_wm_active_state(temp, active, false,
++						false, i, level);
++		}
++		temp = hw->cursor[pipe][level];
++		skl_pipe_wm_active_state(temp, active, false, true, i, level);
++	}
+ 
+-int intel_enable_rc6(const struct drm_device *dev)
+-{
+-	return i915.enable_rc6;
++	for (i = 0; i < intel_num_planes(intel_crtc); i++) {
++		temp = hw->plane_trans[pipe][i];
++		skl_pipe_wm_active_state(temp, active, true, false, i, 0);
++	}
++
++	temp = hw->cursor_trans[pipe];
++	skl_pipe_wm_active_state(temp, active, true, true, i, 0);
+ }
+ 
+-static void gen8_enable_rps_interrupts(struct drm_device *dev)
++void skl_wm_get_hw_state(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct skl_ddb_allocation *ddb = &dev_priv->wm.skl_hw.ddb;
++	struct drm_crtc *crtc;
+ 
+-	spin_lock_irq(&dev_priv->irq_lock);
+-	WARN_ON(dev_priv->rps.pm_iir);
+-	gen8_enable_pm_irq(dev_priv, dev_priv->pm_rps_events);
+-	I915_WRITE(GEN8_GT_IIR(2), dev_priv->pm_rps_events);
+-	spin_unlock_irq(&dev_priv->irq_lock);
++	skl_ddb_get_hw_state(dev_priv, ddb);
++	list_for_each_entry(crtc, &dev->mode_config.crtc_list, head)
++		skl_pipe_wm_get_hw_state(crtc);
+ }
+ 
+-static void gen6_enable_rps_interrupts(struct drm_device *dev)
++static void ilk_pipe_wm_get_hw_state(struct drm_crtc *crtc)
+ {
++	struct drm_device *dev = crtc->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct ilk_wm_values *hw = &dev_priv->wm.hw;
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	struct intel_pipe_wm *active = &intel_crtc->wm.active;
++	enum pipe pipe = intel_crtc->pipe;
++	static const unsigned int wm0_pipe_reg[] = {
++		[PIPE_A] = WM0_PIPEA_ILK,
++		[PIPE_B] = WM0_PIPEB_ILK,
++		[PIPE_C] = WM0_PIPEC_IVB,
++	};
+ 
+-	spin_lock_irq(&dev_priv->irq_lock);
+-	WARN_ON(dev_priv->rps.pm_iir);
+-	gen6_enable_pm_irq(dev_priv, dev_priv->pm_rps_events);
+-	I915_WRITE(GEN6_PMIIR, dev_priv->pm_rps_events);
+-	spin_unlock_irq(&dev_priv->irq_lock);
+-}
++	hw->wm_pipe[pipe] = I915_READ(wm0_pipe_reg[pipe]);
++	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
++		hw->wm_linetime[pipe] = I915_READ(PIPE_WM_LINETIME(pipe));
+ 
+-static void parse_rp_state_cap(struct drm_i915_private *dev_priv, u32 rp_state_cap)
+-{
+-	/* All of these values are in units of 50MHz */
+-	dev_priv->rps.cur_freq		= 0;
+-	/* static values from HW: RP0 < RPe < RP1 < RPn (min_freq) */
+-	dev_priv->rps.rp1_freq		= (rp_state_cap >>  8) & 0xff;
+-	dev_priv->rps.rp0_freq		= (rp_state_cap >>  0) & 0xff;
+-	dev_priv->rps.min_freq		= (rp_state_cap >> 16) & 0xff;
+-	/* XXX: only BYT has a special efficient freq */
+-	dev_priv->rps.efficient_freq	= dev_priv->rps.rp1_freq;
+-	/* hw_max = RP0 until we check for overclocking */
+-	dev_priv->rps.max_freq		= dev_priv->rps.rp0_freq;
++	active->pipe_enabled = intel_crtc_active(crtc);
+ 
+-	/* Preserve min/max settings in case of re-init */
+-	if (dev_priv->rps.max_freq_softlimit == 0)
+-		dev_priv->rps.max_freq_softlimit = dev_priv->rps.max_freq;
++	if (active->pipe_enabled) {
++		u32 tmp = hw->wm_pipe[pipe];
+ 
+-	if (dev_priv->rps.min_freq_softlimit == 0)
+-		dev_priv->rps.min_freq_softlimit = dev_priv->rps.min_freq;
++		/*
++		 * For active pipes LP0 watermark is marked as
++		 * enabled, and LP1+ watermaks as disabled since
++		 * we can't really reverse compute them in case
++		 * multiple pipes are active.
++		 */
++		active->wm[0].enable = true;
++		active->wm[0].pri_val = (tmp & WM0_PIPE_PLANE_MASK) >> WM0_PIPE_PLANE_SHIFT;
++		active->wm[0].spr_val = (tmp & WM0_PIPE_SPRITE_MASK) >> WM0_PIPE_SPRITE_SHIFT;
++		active->wm[0].cur_val = tmp & WM0_PIPE_CURSOR_MASK;
++		active->linetime = hw->wm_linetime[pipe];
++	} else {
++		int level, max_level = ilk_wm_max_level(dev);
++
++		/*
++		 * For inactive pipes, all watermark levels
++		 * should be marked as enabled but zeroed,
++		 * which is what we'd compute them to.
++		 */
++		for (level = 0; level <= max_level; level++)
++			active->wm[level].enable = true;
++	}
+ }
+ 
+-static void gen8_enable_rps(struct drm_device *dev)
++void ilk_wm_get_hw_state(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring;
+-	uint32_t rc6_mask = 0, rp_state_cap;
+-	int unused;
+-
+-	/* 1a: Software RC state - RC0 */
+-	I915_WRITE(GEN6_RC_STATE, 0);
++	struct ilk_wm_values *hw = &dev_priv->wm.hw;
++	struct drm_crtc *crtc;
+ 
+-	/* 1c & 1d: Get forcewake during program sequence. Although the driver
+-	 * hasn't enabled a state yet where we need forcewake, BIOS may have.*/
+-	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
++	for_each_crtc(dev, crtc)
++		ilk_pipe_wm_get_hw_state(crtc);
+ 
+-	/* 2a: Disable RC states. */
+-	I915_WRITE(GEN6_RC_CONTROL, 0);
++	hw->wm_lp[0] = I915_READ(WM1_LP_ILK);
++	hw->wm_lp[1] = I915_READ(WM2_LP_ILK);
++	hw->wm_lp[2] = I915_READ(WM3_LP_ILK);
+ 
+-	rp_state_cap = I915_READ(GEN6_RP_STATE_CAP);
+-	parse_rp_state_cap(dev_priv, rp_state_cap);
++	hw->wm_lp_spr[0] = I915_READ(WM1S_LP_ILK);
++	if (INTEL_INFO(dev)->gen >= 7) {
++		hw->wm_lp_spr[1] = I915_READ(WM2S_LP_IVB);
++		hw->wm_lp_spr[2] = I915_READ(WM3S_LP_IVB);
++	}
+ 
+-	/* 2b: Program RC6 thresholds.*/
+-	I915_WRITE(GEN6_RC6_WAKE_RATE_LIMIT, 40 << 16);
+-	I915_WRITE(GEN6_RC_EVALUATION_INTERVAL, 125000); /* 12500 * 1280ns */
+-	I915_WRITE(GEN6_RC_IDLE_HYSTERSIS, 25); /* 25 * 1280ns */
+-	for_each_ring(ring, dev_priv, unused)
+-		I915_WRITE(RING_MAX_IDLE(ring->mmio_base), 10);
+-	I915_WRITE(GEN6_RC_SLEEP, 0);
+-	if (IS_BROADWELL(dev))
+-		I915_WRITE(GEN6_RC6_THRESHOLD, 625); /* 800us/1.28 for TO */
+-	else
+-		I915_WRITE(GEN6_RC6_THRESHOLD, 50000); /* 50/125ms per EI */
++	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
++		hw->partitioning = (I915_READ(WM_MISC) & WM_MISC_DATA_PARTITION_5_6) ?
++			INTEL_DDB_PART_5_6 : INTEL_DDB_PART_1_2;
++	else if (IS_IVYBRIDGE(dev))
++		hw->partitioning = (I915_READ(DISP_ARB_CTL2) & DISP_DATA_PARTITION_5_6) ?
++			INTEL_DDB_PART_5_6 : INTEL_DDB_PART_1_2;
+ 
+-	/* 3: Enable RC6 */
+-	if (intel_enable_rc6(dev) & INTEL_RC6_ENABLE)
+-		rc6_mask = GEN6_RC_CTL_RC6_ENABLE;
+-	intel_print_rc6_info(dev, rc6_mask);
+-	if (IS_BROADWELL(dev))
+-		I915_WRITE(GEN6_RC_CONTROL, GEN6_RC_CTL_HW_ENABLE |
+-				GEN7_RC_CTL_TO_MODE |
+-				rc6_mask);
+-	else
+-		I915_WRITE(GEN6_RC_CONTROL, GEN6_RC_CTL_HW_ENABLE |
+-				GEN6_RC_CTL_EI_MODE(1) |
+-				rc6_mask);
++	hw->enable_fbc_wm =
++		!(I915_READ(DISP_ARB_CTL) & DISP_FBC_WM_DIS);
++}
+ 
+-	/* 4 Program defaults and thresholds for RPS*/
+-	I915_WRITE(GEN6_RPNSWREQ,
+-		   HSW_FREQUENCY(dev_priv->rps.rp1_freq));
+-	I915_WRITE(GEN6_RC_VIDEO_FREQ,
+-		   HSW_FREQUENCY(dev_priv->rps.rp1_freq));
+-	/* NB: Docs say 1s, and 1000000 - which aren't equivalent */
+-	I915_WRITE(GEN6_RP_DOWN_TIMEOUT, 100000000 / 128); /* 1 second timeout */
++/**
++ * intel_update_watermarks - update FIFO watermark values based on current modes
++ *
++ * Calculate watermark values for the various WM regs based on current mode
++ * and plane configuration.
++ *
++ * There are several cases to deal with here:
++ *   - normal (i.e. non-self-refresh)
++ *   - self-refresh (SR) mode
++ *   - lines are large relative to FIFO size (buffer can hold up to 2)
++ *   - lines are small relative to FIFO size (buffer can hold more than 2
++ *     lines), so need to account for TLB latency
++ *
++ *   The normal calculation is:
++ *     watermark = dotclock * bytes per pixel * latency
++ *   where latency is platform & configuration dependent (we assume pessimal
++ *   values here).
++ *
++ *   The SR calculation is:
++ *     watermark = (trunc(latency/line time)+1) * surface width *
++ *       bytes per pixel
++ *   where
++ *     line time = htotal / dotclock
++ *     surface width = hdisplay for normal plane and 64 for cursor
++ *   and latency is assumed to be high, as above.
++ *
++ * The final value programmed to the register should always be rounded up,
++ * and include an extra 2 entries to account for clock crossings.
++ *
++ * We don't use the sprite, so we can ignore that.  And on Crestline we have
++ * to set the non-SR watermarks to 8.
++ */
++void intel_update_watermarks(struct drm_crtc *crtc)
++{
++	struct drm_i915_private *dev_priv = crtc->dev->dev_private;
+ 
+-	/* Docs recommend 900MHz, and 300 MHz respectively */
+-	I915_WRITE(GEN6_RP_INTERRUPT_LIMITS,
+-		   dev_priv->rps.max_freq_softlimit << 24 |
+-		   dev_priv->rps.min_freq_softlimit << 16);
++	if (dev_priv->display.update_wm)
++		dev_priv->display.update_wm(crtc);
++}
+ 
+-	I915_WRITE(GEN6_RP_UP_THRESHOLD, 7600000 / 128); /* 76ms busyness per EI, 90% */
+-	I915_WRITE(GEN6_RP_DOWN_THRESHOLD, 31300000 / 128); /* 313ms busyness per EI, 70%*/
+-	I915_WRITE(GEN6_RP_UP_EI, 66000); /* 84.48ms, XXX: random? */
+-	I915_WRITE(GEN6_RP_DOWN_EI, 350000); /* 448ms, XXX: random? */
++void intel_update_sprite_watermarks(struct drm_plane *plane,
++				    struct drm_crtc *crtc,
++				    uint32_t sprite_width,
++				    uint32_t sprite_height,
++				    int pixel_size,
++				    bool enabled, bool scaled)
++{
++	struct drm_i915_private *dev_priv = plane->dev->dev_private;
+ 
+-	I915_WRITE(GEN6_RP_IDLE_HYSTERSIS, 10);
++	if (dev_priv->display.update_sprite_wm)
++		dev_priv->display.update_sprite_wm(plane, crtc,
++						   sprite_width, sprite_height,
++						   pixel_size, enabled, scaled);
++}
+ 
+-	/* 5: Enable RPS */
+-	I915_WRITE(GEN6_RP_CONTROL,
+-		   GEN6_RP_MEDIA_TURBO |
+-		   GEN6_RP_MEDIA_HW_NORMAL_MODE |
+-		   GEN6_RP_MEDIA_IS_GFX |
+-		   GEN6_RP_ENABLE |
+-		   GEN6_RP_UP_BUSY_AVG |
+-		   GEN6_RP_DOWN_IDLE_AVG);
++static struct drm_i915_gem_object *
++intel_alloc_context_page(struct drm_device *dev)
++{
++	struct drm_i915_gem_object *ctx;
++	int ret;
+ 
+-	/* 6: Ring frequency + overclocking (our driver does this later */
++	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
+ 
+-	gen6_set_rps(dev, (I915_READ(GEN6_GT_PERF_STATUS) & 0xff00) >> 8);
++	ctx = i915_gem_alloc_object(dev, 4096);
++	if (!ctx) {
++		DRM_DEBUG("failed to alloc power context, RC6 disabled\n");
++		return NULL;
++	}
+ 
+-	gen8_enable_rps_interrupts(dev);
++	ret = i915_gem_object_ggtt_pin(ctx, 4096, 0);
++	if (ret) {
++		DRM_ERROR("failed to pin power context: %d\n", ret);
++		goto err_unref;
++	}
+ 
+-	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
++	ret = i915_gem_object_set_to_gtt_domain(ctx, 1);
++	if (ret) {
++		DRM_ERROR("failed to set-domain on power context: %d\n", ret);
++		goto err_unpin;
++	}
++
++	return ctx;
++
++err_unpin:
++	i915_gem_object_ggtt_unpin(ctx);
++err_unref:
++	drm_gem_object_unreference(&ctx->base);
++	return NULL;
+ }
+ 
+-static void gen6_enable_rps(struct drm_device *dev)
++/**
++ * Lock protecting IPS related data structures
++ */
++DEFINE_SPINLOCK(mchdev_lock);
++
++/* Global for IPS driver to get at the current i915 device. Protected by
++ * mchdev_lock. */
++static struct drm_i915_private *i915_mch_dev;
++
++bool ironlake_set_drps(struct drm_device *dev, u8 val)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring;
+-	u32 rp_state_cap;
+-	u32 gt_perf_status;
+-	u32 rc6vids, pcu_mbox = 0, rc6_mask = 0;
+-	u32 gtfifodbg;
+-	int rc6_mode;
+-	int i, ret;
+-
+-	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
++	u16 rgvswctl;
+ 
+-	/* Here begins a magic sequence of register writes to enable
+-	 * auto-downclocking.
+-	 *
+-	 * Perhaps there might be some value in exposing these to
+-	 * userspace...
+-	 */
+-	I915_WRITE(GEN6_RC_STATE, 0);
++	assert_spin_locked(&mchdev_lock);
+ 
+-	/* Clear the DBG now so we don't confuse earlier errors */
+-	if ((gtfifodbg = I915_READ(GTFIFODBG))) {
+-		DRM_ERROR("GT fifo had a previous error %x\n", gtfifodbg);
+-		I915_WRITE(GTFIFODBG, gtfifodbg);
++	rgvswctl = I915_READ16(MEMSWCTL);
++	if (rgvswctl & MEMCTL_CMD_STS) {
++		DRM_DEBUG("gpu busy, RCS change rejected\n");
++		return false; /* still busy with another command */
+ 	}
+ 
+-	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
++	rgvswctl = (MEMCTL_CMD_CHFREQ << MEMCTL_CMD_SHIFT) |
++		(val << MEMCTL_FREQ_SHIFT) | MEMCTL_SFCAVM;
++	I915_WRITE16(MEMSWCTL, rgvswctl);
++	POSTING_READ16(MEMSWCTL);
+ 
+-	rp_state_cap = I915_READ(GEN6_RP_STATE_CAP);
+-	gt_perf_status = I915_READ(GEN6_GT_PERF_STATUS);
++	rgvswctl |= MEMCTL_CMD_STS;
++	I915_WRITE16(MEMSWCTL, rgvswctl);
+ 
+-	parse_rp_state_cap(dev_priv, rp_state_cap);
++	return true;
++}
+ 
+-	/* disable the counters and set deterministic thresholds */
+-	I915_WRITE(GEN6_RC_CONTROL, 0);
++static void ironlake_enable_drps(struct drm_device *dev)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	u32 rgvmodectl = I915_READ(MEMMODECTL);
++	u8 fmax, fmin, fstart, vstart;
+ 
+-	I915_WRITE(GEN6_RC1_WAKE_RATE_LIMIT, 1000 << 16);
+-	I915_WRITE(GEN6_RC6_WAKE_RATE_LIMIT, 40 << 16 | 30);
+-	I915_WRITE(GEN6_RC6pp_WAKE_RATE_LIMIT, 30);
+-	I915_WRITE(GEN6_RC_EVALUATION_INTERVAL, 125000);
+-	I915_WRITE(GEN6_RC_IDLE_HYSTERSIS, 25);
++	spin_lock_irq(&mchdev_lock);
+ 
+-	for_each_ring(ring, dev_priv, i)
+-		I915_WRITE(RING_MAX_IDLE(ring->mmio_base), 10);
++	/* Enable temp reporting */
++	I915_WRITE16(PMMISC, I915_READ(PMMISC) | MCPPCE_EN);
++	I915_WRITE16(TSC1, I915_READ(TSC1) | TSE);
+ 
+-	I915_WRITE(GEN6_RC_SLEEP, 0);
+-	I915_WRITE(GEN6_RC1e_THRESHOLD, 1000);
+-	if (IS_IVYBRIDGE(dev))
+-		I915_WRITE(GEN6_RC6_THRESHOLD, 125000);
+-	else
+-		I915_WRITE(GEN6_RC6_THRESHOLD, 50000);
+-	I915_WRITE(GEN6_RC6p_THRESHOLD, 150000);
+-	I915_WRITE(GEN6_RC6pp_THRESHOLD, 64000); /* unused */
++	/* 100ms RC evaluation intervals */
++	I915_WRITE(RCUPEI, 100000);
++	I915_WRITE(RCDNEI, 100000);
+ 
+-	/* Check if we are enabling RC6 */
+-	rc6_mode = intel_enable_rc6(dev_priv->dev);
+-	if (rc6_mode & INTEL_RC6_ENABLE)
+-		rc6_mask |= GEN6_RC_CTL_RC6_ENABLE;
++	/* Set max/min thresholds to 90ms and 80ms respectively */
++	I915_WRITE(RCBMAXAVG, 90000);
++	I915_WRITE(RCBMINAVG, 80000);
+ 
+-	/* We don't use those on Haswell */
+-	if (!IS_HASWELL(dev)) {
+-		if (rc6_mode & INTEL_RC6p_ENABLE)
+-			rc6_mask |= GEN6_RC_CTL_RC6p_ENABLE;
++	I915_WRITE(MEMIHYST, 1);
+ 
+-		if (rc6_mode & INTEL_RC6pp_ENABLE)
+-			rc6_mask |= GEN6_RC_CTL_RC6pp_ENABLE;
+-	}
++	/* Set up min, max, and cur for interrupt handling */
++	fmax = (rgvmodectl & MEMMODE_FMAX_MASK) >> MEMMODE_FMAX_SHIFT;
++	fmin = (rgvmodectl & MEMMODE_FMIN_MASK);
++	fstart = (rgvmodectl & MEMMODE_FSTART_MASK) >>
++		MEMMODE_FSTART_SHIFT;
+ 
+-	intel_print_rc6_info(dev, rc6_mask);
++	vstart = (I915_READ(PXVFREQ_BASE + (fstart * 4)) & PXVFREQ_PX_MASK) >>
++		PXVFREQ_PX_SHIFT;
+ 
+-	I915_WRITE(GEN6_RC_CONTROL,
+-		   rc6_mask |
+-		   GEN6_RC_CTL_EI_MODE(1) |
+-		   GEN6_RC_CTL_HW_ENABLE);
++	dev_priv->ips.fmax = fmax; /* IPS callback will increase this */
++	dev_priv->ips.fstart = fstart;
+ 
+-	/* Power down if completely idle for over 50ms */
+-	I915_WRITE(GEN6_RP_DOWN_TIMEOUT, 50000);
+-	I915_WRITE(GEN6_RP_IDLE_HYSTERSIS, 10);
++	dev_priv->ips.max_delay = fstart;
++	dev_priv->ips.min_delay = fmin;
++	dev_priv->ips.cur_delay = fstart;
+ 
+-	ret = sandybridge_pcode_write(dev_priv, GEN6_PCODE_WRITE_MIN_FREQ_TABLE, 0);
+-	if (ret)
+-		DRM_DEBUG_DRIVER("Failed to set the min frequency\n");
++	DRM_DEBUG_DRIVER("fmax: %d, fmin: %d, fstart: %d\n",
++			 fmax, fmin, fstart);
+ 
+-	ret = sandybridge_pcode_read(dev_priv, GEN6_READ_OC_PARAMS, &pcu_mbox);
+-	if (!ret && (pcu_mbox & (1<<31))) { /* OC supported */
+-		DRM_DEBUG_DRIVER("Overclocking supported. Max: %dMHz, Overclock max: %dMHz\n",
+-				 (dev_priv->rps.max_freq_softlimit & 0xff) * 50,
+-				 (pcu_mbox & 0xff) * 50);
+-		dev_priv->rps.max_freq = pcu_mbox & 0xff;
+-	}
++	I915_WRITE(MEMINTREN, MEMINT_CX_SUPR_EN | MEMINT_EVAL_CHG_EN);
+ 
+-	dev_priv->rps.power = HIGH_POWER; /* force a reset */
+-	gen6_set_rps(dev_priv->dev, dev_priv->rps.min_freq_softlimit);
++	/*
++	 * Interrupts will be enabled in ironlake_irq_postinstall
++	 */
+ 
+-	gen6_enable_rps_interrupts(dev);
++	I915_WRITE(VIDSTART, vstart);
++	POSTING_READ(VIDSTART);
+ 
+-	rc6vids = 0;
+-	ret = sandybridge_pcode_read(dev_priv, GEN6_PCODE_READ_RC6VIDS, &rc6vids);
+-	if (IS_GEN6(dev) && ret) {
+-		DRM_DEBUG_DRIVER("Couldn't check for BIOS workaround\n");
+-	} else if (IS_GEN6(dev) && (GEN6_DECODE_RC6_VID(rc6vids & 0xff) < 450)) {
+-		DRM_DEBUG_DRIVER("You should update your BIOS. Correcting minimum rc6 voltage (%dmV->%dmV)\n",
+-			  GEN6_DECODE_RC6_VID(rc6vids & 0xff), 450);
+-		rc6vids &= 0xffff00;
+-		rc6vids |= GEN6_ENCODE_RC6_VID(450);
+-		ret = sandybridge_pcode_write(dev_priv, GEN6_PCODE_WRITE_RC6VIDS, rc6vids);
+-		if (ret)
+-			DRM_ERROR("Couldn't fix incorrect rc6 voltage\n");
+-	}
++	rgvmodectl |= MEMMODE_SWMODE_EN;
++	I915_WRITE(MEMMODECTL, rgvmodectl);
+ 
+-	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
++	if (wait_for_atomic((I915_READ(MEMSWCTL) & MEMCTL_CMD_STS) == 0, 10))
++		DRM_ERROR("stuck trying to change perf mode\n");
++	mdelay(1);
++
++	ironlake_set_drps(dev, fstart);
++
++	dev_priv->ips.last_count1 = I915_READ(0x112e4) + I915_READ(0x112e8) +
++		I915_READ(0x112e0);
++	dev_priv->ips.last_time1 = jiffies_to_msecs(jiffies);
++	dev_priv->ips.last_count2 = I915_READ(0x112f4);
++	dev_priv->ips.last_time2 = ktime_get_raw_ns();
++
++	spin_unlock_irq(&mchdev_lock);
+ }
+ 
+-static void __gen6_update_ring_freq(struct drm_device *dev)
++static void ironlake_disable_drps(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	int min_freq = 15;
+-	unsigned int gpu_freq;
+-	unsigned int max_ia_freq, min_ring_freq;
+-	int scaling_factor = 180;
+-	struct cpufreq_policy *policy;
++	u16 rgvswctl;
+ 
+-	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
++	spin_lock_irq(&mchdev_lock);
+ 
+-	policy = cpufreq_cpu_get(0);
+-	if (policy) {
+-		max_ia_freq = policy->cpuinfo.max_freq;
+-		cpufreq_cpu_put(policy);
+-	} else {
+-		/*
+-		 * Default to measured freq if none found, PCU will ensure we
+-		 * don't go over
+-		 */
+-		max_ia_freq = tsc_khz;
+-	}
++	rgvswctl = I915_READ16(MEMSWCTL);
+ 
+-	/* Convert from kHz to MHz */
+-	max_ia_freq /= 1000;
++	/* Ack interrupts, disable EFC interrupt */
++	I915_WRITE(MEMINTREN, I915_READ(MEMINTREN) & ~MEMINT_EVAL_CHG_EN);
++	I915_WRITE(MEMINTRSTS, MEMINT_EVAL_CHG);
++	I915_WRITE(DEIER, I915_READ(DEIER) & ~DE_PCU_EVENT);
++	I915_WRITE(DEIIR, DE_PCU_EVENT);
++	I915_WRITE(DEIMR, I915_READ(DEIMR) | DE_PCU_EVENT);
+ 
+-	min_ring_freq = I915_READ(DCLK) & 0xf;
+-	/* convert DDR frequency from units of 266.6MHz to bandwidth */
+-	min_ring_freq = mult_frac(min_ring_freq, 8, 3);
++	/* Go back to the starting frequency */
++	ironlake_set_drps(dev, dev_priv->ips.fstart);
++	mdelay(1);
++	rgvswctl |= MEMCTL_CMD_STS;
++	I915_WRITE(MEMSWCTL, rgvswctl);
++	mdelay(1);
+ 
+-	/*
+-	 * For each potential GPU frequency, load a ring frequency we'd like
+-	 * to use for memory access.  We do this by specifying the IA frequency
+-	 * the PCU should use as a reference to determine the ring frequency.
+-	 */
+-	for (gpu_freq = dev_priv->rps.max_freq_softlimit; gpu_freq >= dev_priv->rps.min_freq_softlimit;
+-	     gpu_freq--) {
+-		int diff = dev_priv->rps.max_freq_softlimit - gpu_freq;
+-		unsigned int ia_freq = 0, ring_freq = 0;
++	spin_unlock_irq(&mchdev_lock);
++}
+ 
+-		if (INTEL_INFO(dev)->gen >= 8) {
+-			/* max(2 * GT, DDR). NB: GT is 50MHz units */
+-			ring_freq = max(min_ring_freq, gpu_freq);
+-		} else if (IS_HASWELL(dev)) {
+-			ring_freq = mult_frac(gpu_freq, 5, 4);
+-			ring_freq = max(min_ring_freq, ring_freq);
+-			/* leave ia_freq as the default, chosen by cpufreq */
+-		} else {
+-			/* On older processors, there is no separate ring
+-			 * clock domain, so in order to boost the bandwidth
+-			 * of the ring, we need to upclock the CPU (ia_freq).
+-			 *
+-			 * For GPU frequencies less than 750MHz,
+-			 * just use the lowest ring freq.
+-			 */
+-			if (gpu_freq < min_freq)
+-				ia_freq = 800;
+-			else
+-				ia_freq = max_ia_freq - ((diff * scaling_factor) / 2);
+-			ia_freq = DIV_ROUND_CLOSEST(ia_freq, 100);
+-		}
++/* There's a funny hw issue where the hw returns all 0 when reading from
++ * GEN6_RP_INTERRUPT_LIMITS. Hence we always need to compute the desired value
++ * ourselves, instead of doing a rmw cycle (which might result in us clearing
++ * all limits and the gpu stuck at whatever frequency it is at atm).
++ */
++static u32 gen6_rps_limits(struct drm_i915_private *dev_priv, u8 val)
++{
++	u32 limits;
+ 
+-		sandybridge_pcode_write(dev_priv,
+-					GEN6_PCODE_WRITE_MIN_FREQ_TABLE,
+-					ia_freq << GEN6_PCODE_FREQ_IA_RATIO_SHIFT |
+-					ring_freq << GEN6_PCODE_FREQ_RING_RATIO_SHIFT |
+-					gpu_freq);
+-	}
++	/* Only set the down limit when we've reached the lowest level to avoid
++	 * getting more interrupts, otherwise leave this clear. This prevents a
++	 * race in the hw when coming out of rc6: There's a tiny window where
++	 * the hw runs at the minimal clock before selecting the desired
++	 * frequency, if the down threshold expires in that window we will not
++	 * receive a down interrupt. */
++	limits = dev_priv->rps.max_freq_softlimit << 24;
++	if (val <= dev_priv->rps.min_freq_softlimit)
++		limits |= dev_priv->rps.min_freq_softlimit << 16;
++
++	return limits;
+ }
+ 
+-void gen6_update_ring_freq(struct drm_device *dev)
++static void gen6_set_rps_thresholds(struct drm_i915_private *dev_priv, u8 val)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	int new_power;
+ 
+-	if (INTEL_INFO(dev)->gen < 6 || IS_VALLEYVIEW(dev))
++	new_power = dev_priv->rps.power;
++	switch (dev_priv->rps.power) {
++	case LOW_POWER:
++		if (val > dev_priv->rps.efficient_freq + 1 && val > dev_priv->rps.cur_freq)
++			new_power = BETWEEN;
++		break;
++
++	case BETWEEN:
++		if (val <= dev_priv->rps.efficient_freq && val < dev_priv->rps.cur_freq)
++			new_power = LOW_POWER;
++		else if (val >= dev_priv->rps.rp0_freq && val > dev_priv->rps.cur_freq)
++			new_power = HIGH_POWER;
++		break;
++
++	case HIGH_POWER:
++		if (val < (dev_priv->rps.rp1_freq + dev_priv->rps.rp0_freq) >> 1 && val < dev_priv->rps.cur_freq)
++			new_power = BETWEEN;
++		break;
++	}
++	/* Max/min bins are special */
++	if (val <= dev_priv->rps.min_freq_softlimit)
++		new_power = LOW_POWER;
++	if (val >= dev_priv->rps.max_freq_softlimit)
++		new_power = HIGH_POWER;
++	if (new_power == dev_priv->rps.power)
+ 		return;
+ 
+-	mutex_lock(&dev_priv->rps.hw_lock);
+-	__gen6_update_ring_freq(dev);
+-	mutex_unlock(&dev_priv->rps.hw_lock);
+-}
++	/* Note the units here are not exactly 1us, but 1280ns. */
++	switch (new_power) {
++	case LOW_POWER:
++		/* Upclock if more than 95% busy over 16ms */
++		dev_priv->rps.up_threshold = 95;
++		I915_WRITE(GEN6_RP_UP_EI, 12500);
++		I915_WRITE(GEN6_RP_UP_THRESHOLD, 11800);
+ 
+-static int cherryview_rps_max_freq(struct drm_i915_private *dev_priv)
+-{
+-	u32 val, rp0;
++		/* Downclock if less than 85% busy over 32ms */
++		dev_priv->rps.down_threshold = 85;
++		I915_WRITE(GEN6_RP_DOWN_EI, 25000);
++		I915_WRITE(GEN6_RP_DOWN_THRESHOLD, 21250);
+ 
+-	val = vlv_punit_read(dev_priv, PUNIT_GPU_STATUS_REG);
+-	rp0 = (val >> PUNIT_GPU_STATUS_MAX_FREQ_SHIFT) & PUNIT_GPU_STATUS_MAX_FREQ_MASK;
++		I915_WRITE(GEN6_RP_CONTROL,
++			   GEN6_RP_MEDIA_TURBO |
++			   GEN6_RP_MEDIA_HW_NORMAL_MODE |
++			   GEN6_RP_MEDIA_IS_GFX |
++			   GEN6_RP_ENABLE |
++			   GEN6_RP_UP_BUSY_AVG |
++			   GEN6_RP_DOWN_IDLE_AVG);
++		break;
+ 
+-	return rp0;
+-}
++	case BETWEEN:
++		/* Upclock if more than 90% busy over 13ms */
++		dev_priv->rps.up_threshold = 90;
++		I915_WRITE(GEN6_RP_UP_EI, 10250);
++		I915_WRITE(GEN6_RP_UP_THRESHOLD, 9225);
+ 
+-static int cherryview_rps_rpe_freq(struct drm_i915_private *dev_priv)
+-{
+-	u32 val, rpe;
++		/* Downclock if less than 75% busy over 32ms */
++		dev_priv->rps.down_threshold = 75;
++		I915_WRITE(GEN6_RP_DOWN_EI, 25000);
++		I915_WRITE(GEN6_RP_DOWN_THRESHOLD, 18750);
+ 
+-	val = vlv_punit_read(dev_priv, PUNIT_GPU_DUTYCYCLE_REG);
+-	rpe = (val >> PUNIT_GPU_DUTYCYCLE_RPE_FREQ_SHIFT) & PUNIT_GPU_DUTYCYCLE_RPE_FREQ_MASK;
++		I915_WRITE(GEN6_RP_CONTROL,
++			   GEN6_RP_MEDIA_TURBO |
++			   GEN6_RP_MEDIA_HW_NORMAL_MODE |
++			   GEN6_RP_MEDIA_IS_GFX |
++			   GEN6_RP_ENABLE |
++			   GEN6_RP_UP_BUSY_AVG |
++			   GEN6_RP_DOWN_IDLE_AVG);
++		break;
+ 
+-	return rpe;
+-}
++	case HIGH_POWER:
++		/* Upclock if more than 85% busy over 10ms */
++		dev_priv->rps.up_threshold = 85;
++		I915_WRITE(GEN6_RP_UP_EI, 8000);
++		I915_WRITE(GEN6_RP_UP_THRESHOLD, 6800);
+ 
+-static int cherryview_rps_guar_freq(struct drm_i915_private *dev_priv)
+-{
+-	u32 val, rp1;
++		/* Downclock if less than 60% busy over 32ms */
++		dev_priv->rps.down_threshold = 60;
++		I915_WRITE(GEN6_RP_DOWN_EI, 25000);
++		I915_WRITE(GEN6_RP_DOWN_THRESHOLD, 15000);
+ 
+-	val = vlv_punit_read(dev_priv, PUNIT_REG_GPU_FREQ_STS);
+-	rp1 = (val >> PUNIT_GPU_STATUS_MAX_FREQ_SHIFT) & PUNIT_GPU_STATUS_MAX_FREQ_MASK;
++		I915_WRITE(GEN6_RP_CONTROL,
++			   GEN6_RP_MEDIA_TURBO |
++			   GEN6_RP_MEDIA_HW_NORMAL_MODE |
++			   GEN6_RP_MEDIA_IS_GFX |
++			   GEN6_RP_ENABLE |
++			   GEN6_RP_UP_BUSY_AVG |
++			   GEN6_RP_DOWN_IDLE_AVG);
++		break;
++	}
+ 
+-	return rp1;
++	dev_priv->rps.power = new_power;
++	dev_priv->rps.last_adj = 0;
+ }
+ 
+-static int cherryview_rps_min_freq(struct drm_i915_private *dev_priv)
++static u32 gen6_rps_pm_mask(struct drm_i915_private *dev_priv, u8 val)
+ {
+-	u32 val, rpn;
++	u32 mask = 0;
+ 
+-	val = vlv_punit_read(dev_priv, PUNIT_GPU_STATUS_REG);
+-	rpn = (val >> PUNIT_GPU_STATIS_GFX_MIN_FREQ_SHIFT) & PUNIT_GPU_STATUS_GFX_MIN_FREQ_MASK;
+-	return rpn;
+-}
++	if (val > dev_priv->rps.min_freq_softlimit)
++		mask |= GEN6_PM_RP_DOWN_EI_EXPIRED | GEN6_PM_RP_DOWN_THRESHOLD | GEN6_PM_RP_DOWN_TIMEOUT;
++	if (val < dev_priv->rps.max_freq_softlimit)
++		mask |= GEN6_PM_RP_UP_EI_EXPIRED | GEN6_PM_RP_UP_THRESHOLD;
+ 
+-static int valleyview_rps_guar_freq(struct drm_i915_private *dev_priv)
+-{
+-	u32 val, rp1;
++	mask &= dev_priv->rps.pm_events;
+ 
+-	val = vlv_nc_read(dev_priv, IOSF_NC_FB_GFX_FREQ_FUSE);
++	/* IVB and SNB hard hangs on looping batchbuffer
++	 * if GEN6_PM_UP_EI_EXPIRED is masked.
++	 */
++	if (INTEL_INFO(dev_priv->dev)->gen <= 7 && !IS_HASWELL(dev_priv->dev))
++		mask |= GEN6_PM_RP_UP_EI_EXPIRED;
+ 
+-	rp1 = (val & FB_GFX_FGUARANTEED_FREQ_FUSE_MASK) >> FB_GFX_FGUARANTEED_FREQ_FUSE_SHIFT;
++	if (IS_GEN8(dev_priv->dev))
++		mask |= GEN8_PMINTR_REDIRECT_TO_NON_DISP;
+ 
+-	return rp1;
++	return ~mask;
+ }
+ 
+-static int valleyview_rps_max_freq(struct drm_i915_private *dev_priv)
++/* gen6_set_rps is called to update the frequency request, but should also be
++ * called when the range (min_delay and max_delay) is modified so that we can
++ * update the GEN6_RP_INTERRUPT_LIMITS register accordingly. */
++void gen6_set_rps(struct drm_device *dev, u8 val)
+ {
+-	u32 val, rp0;
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	val = vlv_nc_read(dev_priv, IOSF_NC_FB_GFX_FREQ_FUSE);
++	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
++	WARN_ON(val > dev_priv->rps.max_freq);
++	WARN_ON(val < dev_priv->rps.min_freq);
+ 
+-	rp0 = (val & FB_GFX_MAX_FREQ_FUSE_MASK) >> FB_GFX_MAX_FREQ_FUSE_SHIFT;
+-	/* Clamp to max */
+-	rp0 = min_t(u32, rp0, 0xea);
++	/* min/max delay may still have been modified so be sure to
++	 * write the limits value.
++	 */
++	if (val != dev_priv->rps.cur_freq) {
++		gen6_set_rps_thresholds(dev_priv, val);
+ 
+-	return rp0;
++		if (IS_HASWELL(dev) || IS_BROADWELL(dev))
++			I915_WRITE(GEN6_RPNSWREQ,
++				   HSW_FREQUENCY(val));
++		else
++			I915_WRITE(GEN6_RPNSWREQ,
++				   GEN6_FREQUENCY(val) |
++				   GEN6_OFFSET(0) |
++				   GEN6_AGGRESSIVE_TURBO);
++	}
++
++	/* Make sure we continue to get interrupts
++	 * until we hit the minimum or maximum frequencies.
++	 */
++	I915_WRITE(GEN6_RP_INTERRUPT_LIMITS, gen6_rps_limits(dev_priv, val));
++	I915_WRITE(GEN6_PMINTRMSK, gen6_rps_pm_mask(dev_priv, val));
++
++	dev_priv->rps.cur_freq = val;
++	trace_intel_gpu_freq_change(val * 50);
+ }
+ 
+-static int valleyview_rps_rpe_freq(struct drm_i915_private *dev_priv)
++/* vlv_set_rps_idle: Set the frequency to Rpn if Gfx clocks are down
++ *
++ * * If Gfx is Idle, then
++ * 1. Mask Turbo interrupts
++ * 2. Bring up Gfx clock
++ * 3. Change the freq to Rpn and wait till P-Unit updates freq
++ * 4. Clear the Force GFX CLK ON bit so that Gfx can down
++ * 5. Unmask Turbo interrupts
++*/
++static void vlv_set_rps_idle(struct drm_i915_private *dev_priv)
+ {
+-	u32 val, rpe;
++	struct drm_device *dev = dev_priv->dev;
++	u32 val = dev_priv->rps.min_freq;
+ 
+-	val = vlv_nc_read(dev_priv, IOSF_NC_FB_GFX_FMAX_FUSE_LO);
+-	rpe = (val & FB_FMAX_VMIN_FREQ_LO_MASK) >> FB_FMAX_VMIN_FREQ_LO_SHIFT;
+-	val = vlv_nc_read(dev_priv, IOSF_NC_FB_GFX_FMAX_FUSE_HI);
+-	rpe |= (val & FB_FMAX_VMIN_FREQ_HI_MASK) << 5;
++	/* Latest VLV doesn't need to force the gfx clock */
++	if (dev->pdev->revision >= 0xd) {
++		valleyview_set_rps(dev_priv->dev, val);
++		return;
++	}
+ 
+-	return rpe;
++	/*
++	 * When we are idle.  Drop to min voltage state.
++	 */
++
++	if (dev_priv->rps.cur_freq <= val)
++		return;
++
++	/* Mask turbo interrupt so that they will not come in between */
++	I915_WRITE(GEN6_PMINTRMSK, 0xffffffff);
++
++	vlv_force_gfx_clock(dev_priv, true);
++
++	dev_priv->rps.cur_freq = val;
++
++	vlv_punit_write(dev_priv, PUNIT_REG_GPU_FREQ_REQ, val);
++
++	if (wait_for(((vlv_punit_read(dev_priv, PUNIT_REG_GPU_FREQ_STS))
++				& GENFREQSTATUS) == 0, 5))
++		DRM_ERROR("timed out waiting for Punit\n");
++
++	gen6_set_rps_thresholds(dev_priv, val);
++	vlv_force_gfx_clock(dev_priv, false);
++
++	I915_WRITE(GEN6_PMINTRMSK, gen6_rps_pm_mask(dev_priv, val));
+ }
+ 
+-static int valleyview_rps_min_freq(struct drm_i915_private *dev_priv)
++void gen6_rps_busy(struct drm_i915_private *dev_priv)
+ {
+-	return vlv_punit_read(dev_priv, PUNIT_REG_GPU_LFM) & 0xff;
++	mutex_lock(&dev_priv->rps.hw_lock);
++	if (dev_priv->rps.enabled) {
++		if (dev_priv->rps.pm_events & (GEN6_PM_RP_DOWN_EI_EXPIRED | GEN6_PM_RP_UP_EI_EXPIRED))
++			gen6_rps_reset_ei(dev_priv);
++		I915_WRITE(GEN6_PMINTRMSK,
++			   gen6_rps_pm_mask(dev_priv, dev_priv->rps.cur_freq));
++	}
++	mutex_unlock(&dev_priv->rps.hw_lock);
+ }
+ 
+-/* Check that the pctx buffer wasn't move under us. */
+-static void valleyview_check_pctx(struct drm_i915_private *dev_priv)
++void gen6_rps_idle(struct drm_i915_private *dev_priv)
+ {
+-	unsigned long pctx_addr = I915_READ(VLV_PCBR) & ~4095;
+-
+-	WARN_ON(pctx_addr != dev_priv->mm.stolen_base +
+-			     dev_priv->vlv_pctx->stolen->start);
+-}
++	struct drm_device *dev = dev_priv->dev;
+ 
++	mutex_lock(&dev_priv->rps.hw_lock);
++	if (dev_priv->rps.enabled) {
++		u32 val = dev_priv->rps.min_freq;
+ 
+-/* Check that the pcbr address is not empty. */
+-static void cherryview_check_pctx(struct drm_i915_private *dev_priv)
+-{
+-	unsigned long pctx_addr = I915_READ(VLV_PCBR) & ~4095;
++		if (IS_CHERRYVIEW(dev))
++			valleyview_set_rps(dev_priv->dev, val);
++		else if (IS_VALLEYVIEW(dev))
++			vlv_set_rps_idle(dev_priv);
++		else
++			gen6_set_rps(dev_priv->dev, val);
++		dev_priv->rps.last_adj = 0;
++		I915_WRITE(GEN6_PMINTRMSK, 0xffffffff);
++	}
+ 
+-	WARN_ON((pctx_addr >> VLV_PCBR_ADDR_SHIFT) == 0);
++	while (!list_empty(&dev_priv->rps.clients))
++		list_del_init(dev_priv->rps.clients.next);
++	mutex_unlock(&dev_priv->rps.hw_lock);
+ }
+ 
+-static void cherryview_setup_pctx(struct drm_device *dev)
++void gen6_rps_boost(struct drm_i915_private *dev_priv,
++		    struct drm_i915_file_private *file_priv)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	unsigned long pctx_paddr, paddr;
+-	struct i915_gtt *gtt = &dev_priv->gtt;
+-	u32 pcbr;
+-	int pctx_size = 32*1024;
+-
+-	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
++	struct drm_device *dev = dev_priv->dev;
++	u32 val;
+ 
+-	pcbr = I915_READ(VLV_PCBR);
+-	if ((pcbr >> VLV_PCBR_ADDR_SHIFT) == 0) {
+-		paddr = (dev_priv->mm.stolen_base +
+-			 (gtt->stolen_size - pctx_size));
++	mutex_lock(&dev_priv->rps.hw_lock);
++	val = dev_priv->rps.max_freq_softlimit;
++	if (dev_priv->rps.enabled &&
++	    dev_priv->mm.busy &&
++	    dev_priv->rps.cur_freq < val &&
++	    (file_priv == NULL || list_empty(&file_priv->rps_boost))) {
++		if (IS_VALLEYVIEW(dev))
++			valleyview_set_rps(dev_priv->dev, val);
++		else
++			gen6_set_rps(dev_priv->dev, val);
++		dev_priv->rps.last_adj = 0;
+ 
+-		pctx_paddr = (paddr & (~4095));
+-		I915_WRITE(VLV_PCBR, pctx_paddr);
++		if (file_priv != NULL) {
++			file_priv->rps_boosts++;
++			list_add(&file_priv->rps_boost, &dev_priv->rps.clients);
++		}
+ 	}
++	mutex_unlock(&dev_priv->rps.hw_lock);
+ }
+ 
+-static void valleyview_setup_pctx(struct drm_device *dev)
++void valleyview_set_rps(struct drm_device *dev, u8 val)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct drm_i915_gem_object *pctx;
+-	unsigned long pctx_paddr;
+-	u32 pcbr;
+-	int pctx_size = 24*1024;
+ 
+-	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
++	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
++	WARN_ON(val > dev_priv->rps.max_freq);
++	WARN_ON(val < dev_priv->rps.min_freq);
+ 
+-	pcbr = I915_READ(VLV_PCBR);
+-	if (pcbr) {
+-		/* BIOS set it up already, grab the pre-alloc'd space */
+-		int pcbr_offset;
++	if (WARN_ONCE(IS_CHERRYVIEW(dev) && (val & 1),
++		      "Odd GPU freq value\n"))
++		val &= ~1;
+ 
+-		pcbr_offset = (pcbr & (~4095)) - dev_priv->mm.stolen_base;
+-		pctx = i915_gem_object_create_stolen_for_preallocated(dev_priv->dev,
+-								      pcbr_offset,
+-								      I915_GTT_OFFSET_NONE,
+-								      pctx_size);
+-		goto out;
+-	}
++	if (val != dev_priv->rps.cur_freq) {
++		DRM_DEBUG_DRIVER("GPU freq request from %d MHz (%u) to %d MHz (%u)\n",
++				 vlv_gpu_freq(dev_priv, dev_priv->rps.cur_freq),
++				 dev_priv->rps.cur_freq,
++				 vlv_gpu_freq(dev_priv, val), val);
+ 
+-	/*
+-	 * From the Gunit register HAS:
+-	 * The Gfx driver is expected to program this register and ensure
+-	 * proper allocation within Gfx stolen memory.  For example, this
+-	 * register should be programmed such than the PCBR range does not
+-	 * overlap with other ranges, such as the frame buffer, protected
+-	 * memory, or any other relevant ranges.
+-	 */
+-	pctx = i915_gem_object_create_stolen(dev, pctx_size);
+-	if (!pctx) {
+-		DRM_DEBUG("not enough stolen space for PCTX, disabling\n");
+-		return;
++		vlv_punit_write(dev_priv, PUNIT_REG_GPU_FREQ_REQ, val);
++		gen6_set_rps_thresholds(dev_priv, val);
+ 	}
+ 
+-	pctx_paddr = dev_priv->mm.stolen_base + pctx->stolen->start;
+-	I915_WRITE(VLV_PCBR, pctx_paddr);
++	I915_WRITE(GEN6_PMINTRMSK, gen6_rps_pm_mask(dev_priv, val));
+ 
+-out:
+-	dev_priv->vlv_pctx = pctx;
++	dev_priv->rps.cur_freq = val;
++	trace_intel_gpu_freq_change(vlv_gpu_freq(dev_priv, val));
+ }
+ 
+-static void valleyview_cleanup_pctx(struct drm_device *dev)
++static void gen9_disable_rps(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	if (WARN_ON(!dev_priv->vlv_pctx))
+-		return;
+-
+-	drm_gem_object_unreference(&dev_priv->vlv_pctx->base);
+-	dev_priv->vlv_pctx = NULL;
++	I915_WRITE(GEN6_RC_CONTROL, 0);
+ }
+ 
+-static void valleyview_init_gt_powersave(struct drm_device *dev)
++static void gen6_disable_rps(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	valleyview_setup_pctx(dev);
+-
+-	mutex_lock(&dev_priv->rps.hw_lock);
+-
+-	dev_priv->rps.max_freq = valleyview_rps_max_freq(dev_priv);
+-	dev_priv->rps.rp0_freq = dev_priv->rps.max_freq;
+-	DRM_DEBUG_DRIVER("max GPU freq: %d MHz (%u)\n",
+-			 vlv_gpu_freq(dev_priv, dev_priv->rps.max_freq),
+-			 dev_priv->rps.max_freq);
+-
+-	dev_priv->rps.efficient_freq = valleyview_rps_rpe_freq(dev_priv);
+-	DRM_DEBUG_DRIVER("RPe GPU freq: %d MHz (%u)\n",
+-			 vlv_gpu_freq(dev_priv, dev_priv->rps.efficient_freq),
+-			 dev_priv->rps.efficient_freq);
+-
+-	dev_priv->rps.rp1_freq = valleyview_rps_guar_freq(dev_priv);
+-	DRM_DEBUG_DRIVER("RP1(Guar Freq) GPU freq: %d MHz (%u)\n",
+-			 vlv_gpu_freq(dev_priv, dev_priv->rps.rp1_freq),
+-			 dev_priv->rps.rp1_freq);
+-
+-	dev_priv->rps.min_freq = valleyview_rps_min_freq(dev_priv);
+-	DRM_DEBUG_DRIVER("min GPU freq: %d MHz (%u)\n",
+-			 vlv_gpu_freq(dev_priv, dev_priv->rps.min_freq),
+-			 dev_priv->rps.min_freq);
+-
+-	/* Preserve min/max settings in case of re-init */
+-	if (dev_priv->rps.max_freq_softlimit == 0)
+-		dev_priv->rps.max_freq_softlimit = dev_priv->rps.max_freq;
+-
+-	if (dev_priv->rps.min_freq_softlimit == 0)
+-		dev_priv->rps.min_freq_softlimit = dev_priv->rps.min_freq;
++	I915_WRITE(GEN6_RC_CONTROL, 0);
++	I915_WRITE(GEN6_RPNSWREQ, 1 << 31);
+ 
+-	mutex_unlock(&dev_priv->rps.hw_lock);
++	gen6_disable_rps_interrupts(dev);
+ }
+ 
+-static void cherryview_init_gt_powersave(struct drm_device *dev)
++static void cherryview_disable_rps(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	cherryview_setup_pctx(dev);
+-
+-	mutex_lock(&dev_priv->rps.hw_lock);
+-
+-	dev_priv->rps.max_freq = cherryview_rps_max_freq(dev_priv);
+-	dev_priv->rps.rp0_freq = dev_priv->rps.max_freq;
+-	DRM_DEBUG_DRIVER("max GPU freq: %d MHz (%u)\n",
+-			 vlv_gpu_freq(dev_priv, dev_priv->rps.max_freq),
+-			 dev_priv->rps.max_freq);
++	I915_WRITE(GEN6_RC_CONTROL, 0);
+ 
+-	dev_priv->rps.efficient_freq = cherryview_rps_rpe_freq(dev_priv);
+-	DRM_DEBUG_DRIVER("RPe GPU freq: %d MHz (%u)\n",
+-			 vlv_gpu_freq(dev_priv, dev_priv->rps.efficient_freq),
+-			 dev_priv->rps.efficient_freq);
++	gen6_disable_rps_interrupts(dev);
++}
+ 
+-	dev_priv->rps.rp1_freq = cherryview_rps_guar_freq(dev_priv);
+-	DRM_DEBUG_DRIVER("RP1(Guar) GPU freq: %d MHz (%u)\n",
+-			 vlv_gpu_freq(dev_priv, dev_priv->rps.rp1_freq),
+-			 dev_priv->rps.rp1_freq);
++static void valleyview_disable_rps(struct drm_device *dev)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	dev_priv->rps.min_freq = cherryview_rps_min_freq(dev_priv);
+-	DRM_DEBUG_DRIVER("min GPU freq: %d MHz (%u)\n",
+-			 vlv_gpu_freq(dev_priv, dev_priv->rps.min_freq),
+-			 dev_priv->rps.min_freq);
++	/* we're doing forcewake before Disabling RC6,
++	 * This what the BIOS expects when going into suspend */
++	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
+ 
+-	/* Preserve min/max settings in case of re-init */
+-	if (dev_priv->rps.max_freq_softlimit == 0)
+-		dev_priv->rps.max_freq_softlimit = dev_priv->rps.max_freq;
++	I915_WRITE(GEN6_RC_CONTROL, 0);
+ 
+-	if (dev_priv->rps.min_freq_softlimit == 0)
+-		dev_priv->rps.min_freq_softlimit = dev_priv->rps.min_freq;
++	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
+ 
+-	mutex_unlock(&dev_priv->rps.hw_lock);
++	gen6_disable_rps_interrupts(dev);
+ }
+ 
+-static void valleyview_cleanup_gt_powersave(struct drm_device *dev)
++static void intel_print_rc6_info(struct drm_device *dev, u32 mode)
+ {
+-	valleyview_cleanup_pctx(dev);
++	if (IS_VALLEYVIEW(dev)) {
++		if (mode & (GEN7_RC_CTL_TO_MODE | GEN6_RC_CTL_EI_MODE(1)))
++			mode = GEN6_RC_CTL_RC6_ENABLE;
++		else
++			mode = 0;
++	}
++	if (HAS_RC6p(dev))
++		DRM_DEBUG_KMS("Enabling RC6 states: RC6 %s RC6p %s RC6pp %s\n",
++			      (mode & GEN6_RC_CTL_RC6_ENABLE) ? "on" : "off",
++			      (mode & GEN6_RC_CTL_RC6p_ENABLE) ? "on" : "off",
++			      (mode & GEN6_RC_CTL_RC6pp_ENABLE) ? "on" : "off");
++
++	else
++		DRM_DEBUG_KMS("Enabling RC6 states: RC6 %s\n",
++			      (mode & GEN6_RC_CTL_RC6_ENABLE) ? "on" : "off");
+ }
+ 
+-static void cherryview_enable_rps(struct drm_device *dev)
++static int sanitize_rc6_option(const struct drm_device *dev, int enable_rc6)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring;
+-	u32 gtfifodbg, val, rc6_mode = 0, pcbr;
+-	int i;
++	/* No RC6 before Ironlake */
++	if (INTEL_INFO(dev)->gen < 5)
++		return 0;
+ 
+-	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
++	/* RC6 is only on Ironlake mobile not on desktop */
++	if (INTEL_INFO(dev)->gen == 5 && !IS_IRONLAKE_M(dev))
++		return 0;
+ 
+-	gtfifodbg = I915_READ(GTFIFODBG);
+-	if (gtfifodbg) {
+-		DRM_DEBUG_DRIVER("GT fifo had a previous error %x\n",
+-				 gtfifodbg);
+-		I915_WRITE(GTFIFODBG, gtfifodbg);
++	/* Respect the kernel parameter if it is set */
++	if (enable_rc6 >= 0) {
++		int mask;
++
++		if (HAS_RC6p(dev))
++			mask = INTEL_RC6_ENABLE | INTEL_RC6p_ENABLE |
++			       INTEL_RC6pp_ENABLE;
++		else
++			mask = INTEL_RC6_ENABLE;
++
++		if ((enable_rc6 & mask) != enable_rc6)
++			DRM_DEBUG_KMS("Adjusting RC6 mask to %d (requested %d, valid %d)\n",
++				      enable_rc6 & mask, enable_rc6, mask);
++
++		return enable_rc6 & mask;
+ 	}
+ 
+-	cherryview_check_pctx(dev_priv);
++#ifdef CONFIG_INTEL_IOMMU
++	/* Ironlake + RC6 + VT-d empirically blows up */
++	if (IS_GEN5(dev) && intel_iommu_gfx_mapped)
++		return 0;
++#endif
+ 
+-	/* 1a & 1b: Get forcewake during program sequence. Although the driver
++	if (IS_IVYBRIDGE(dev))
++		return (INTEL_RC6_ENABLE | INTEL_RC6p_ENABLE);
++
++	return INTEL_RC6_ENABLE;
++}
++
++int intel_enable_rc6(const struct drm_device *dev)
++{
++	return i915_module.enable_rc6;
++}
++
++static void parse_rp_state_cap(struct drm_i915_private *dev_priv, u32 rp_state_cap)
++{
++	/* All of these values are in units of 50MHz */
++	dev_priv->rps.cur_freq		= 0;
++	/* static values from HW: RP0 < RPe < RP1 < RPn (min_freq) */
++	dev_priv->rps.rp1_freq		= (rp_state_cap >>  8) & 0xff;
++	dev_priv->rps.rp0_freq		= (rp_state_cap >>  0) & 0xff;
++	dev_priv->rps.min_freq		= (rp_state_cap >> 16) & 0xff;
++	/* XXX: only BYT has a special efficient freq */
++	dev_priv->rps.efficient_freq	= dev_priv->rps.rp1_freq;
++	/* hw_max = RP0 until we check for overclocking */
++	dev_priv->rps.max_freq		= dev_priv->rps.rp0_freq;
++
++	/* Preserve min/max settings in case of re-init */
++	if (dev_priv->rps.max_freq_softlimit == 0)
++		dev_priv->rps.max_freq_softlimit = dev_priv->rps.max_freq;
++
++	if (dev_priv->rps.min_freq_softlimit == 0)
++		dev_priv->rps.min_freq_softlimit = dev_priv->rps.min_freq;
++}
++
++static void gen9_enable_rps(struct drm_device *dev)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_engine_cs *engine;
++	uint32_t rc6_mask = 0;
++	int unused;
++
++	/* 1a: Software RC state - RC0 */
++	I915_WRITE(GEN6_RC_STATE, 0);
++
++	/* 1b: Get forcewake during program sequence. Although the driver
+ 	 * hasn't enabled a state yet where we need forcewake, BIOS may have.*/
+ 	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
+ 
+-	/* 2a: Program RC6 thresholds.*/
+-	I915_WRITE(GEN6_RC6_WAKE_RATE_LIMIT, 40 << 16);
++	/* 2a: Disable RC states. */
++	I915_WRITE(GEN6_RC_CONTROL, 0);
++
++	/* 2b: Program RC6 thresholds.*/
++	I915_WRITE(GEN6_RC6_WAKE_RATE_LIMIT, 54 << 16);
+ 	I915_WRITE(GEN6_RC_EVALUATION_INTERVAL, 125000); /* 12500 * 1280ns */
+ 	I915_WRITE(GEN6_RC_IDLE_HYSTERSIS, 25); /* 25 * 1280ns */
+-
+-	for_each_ring(ring, dev_priv, i)
+-		I915_WRITE(RING_MAX_IDLE(ring->mmio_base), 10);
++	for_each_engine(engine, dev_priv, unused)
++		I915_WRITE(RING_MAX_IDLE(engine->mmio_base), 10);
+ 	I915_WRITE(GEN6_RC_SLEEP, 0);
++	I915_WRITE(GEN6_RC6_THRESHOLD, 37500); /* 37.5/125ms per EI */
+ 
+-	I915_WRITE(GEN6_RC6_THRESHOLD, 50000); /* 50/125ms per EI */
++	/* 3a: Enable RC6 */
++	if (intel_enable_rc6(dev) & INTEL_RC6_ENABLE)
++		rc6_mask = GEN6_RC_CTL_RC6_ENABLE;
++	DRM_INFO("RC6 %s\n", (rc6_mask & GEN6_RC_CTL_RC6_ENABLE) ?
++			"on" : "off");
++	I915_WRITE(GEN6_RC_CONTROL, GEN6_RC_CTL_HW_ENABLE |
++				   GEN6_RC_CTL_EI_MODE(1) |
++				   rc6_mask);
+ 
+-	/* allows RC6 residency counter to work */
+-	I915_WRITE(VLV_COUNTER_CONTROL,
+-		   _MASKED_BIT_ENABLE(VLV_COUNT_RANGE_HIGH |
+-				      VLV_MEDIA_RC6_COUNT_EN |
+-				      VLV_RENDER_RC6_COUNT_EN));
++	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
+ 
+-	/* For now we assume BIOS is allocating and populating the PCBR  */
+-	pcbr = I915_READ(VLV_PCBR);
++}
+ 
+-	DRM_DEBUG_DRIVER("PCBR offset : 0x%x\n", pcbr);
++static void gen8_enable_rps(struct drm_device *dev)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_engine_cs *engine;
++	uint32_t rc6_mask = 0, rp_state_cap;
++	int unused;
+ 
+-	/* 3: Enable RC6 */
+-	if ((intel_enable_rc6(dev) & INTEL_RC6_ENABLE) &&
+-						(pcbr >> VLV_PCBR_ADDR_SHIFT))
+-		rc6_mode = GEN6_RC_CTL_EI_MODE(1);
++	/* 1a: Software RC state - RC0 */
++	I915_WRITE(GEN6_RC_STATE, 0);
+ 
+-	I915_WRITE(GEN6_RC_CONTROL, rc6_mode);
++	/* 1c & 1d: Get forcewake during program sequence. Although the driver
++	 * hasn't enabled a state yet where we need forcewake, BIOS may have.*/
++	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
++
++	/* 2a: Disable RC states. */
++	I915_WRITE(GEN6_RC_CONTROL, 0);
++
++	rp_state_cap = I915_READ(GEN6_RP_STATE_CAP);
++	parse_rp_state_cap(dev_priv, rp_state_cap);
++
++	/* 2b: Program RC6 thresholds.*/
++	I915_WRITE(GEN6_RC6_WAKE_RATE_LIMIT, 40 << 16);
++	I915_WRITE(GEN6_RC_EVALUATION_INTERVAL, 125000); /* 12500 * 1280ns */
++	I915_WRITE(GEN6_RC_IDLE_HYSTERSIS, 25); /* 25 * 1280ns */
++	for_each_engine(engine, dev_priv, unused)
++		I915_WRITE(RING_MAX_IDLE(engine->mmio_base), 10);
++	I915_WRITE(GEN6_RC_SLEEP, 0);
++	if (IS_BROADWELL(dev))
++		I915_WRITE(GEN6_RC6_THRESHOLD, 625); /* 800us/1.28 for TO */
++	else
++		I915_WRITE(GEN6_RC6_THRESHOLD, 50000); /* 50/125ms per EI */
++
++	/* 3: Enable RC6 */
++	if (intel_enable_rc6(dev) & INTEL_RC6_ENABLE)
++		rc6_mask = GEN6_RC_CTL_RC6_ENABLE;
++	intel_print_rc6_info(dev, rc6_mask);
++	if (IS_BROADWELL(dev))
++		I915_WRITE(GEN6_RC_CONTROL, GEN6_RC_CTL_HW_ENABLE |
++				GEN7_RC_CTL_TO_MODE |
++				rc6_mask);
++	else
++		I915_WRITE(GEN6_RC_CONTROL, GEN6_RC_CTL_HW_ENABLE |
++				GEN6_RC_CTL_EI_MODE(1) |
++				rc6_mask);
+ 
+ 	/* 4 Program defaults and thresholds for RPS*/
+-	I915_WRITE(GEN6_RP_UP_THRESHOLD, 59400);
+-	I915_WRITE(GEN6_RP_DOWN_THRESHOLD, 245000);
+-	I915_WRITE(GEN6_RP_UP_EI, 66000);
+-	I915_WRITE(GEN6_RP_DOWN_EI, 350000);
++	I915_WRITE(GEN6_RPNSWREQ,
++		   HSW_FREQUENCY(dev_priv->rps.rp1_freq));
++	I915_WRITE(GEN6_RC_VIDEO_FREQ,
++		   HSW_FREQUENCY(dev_priv->rps.rp1_freq));
++	/* NB: Docs say 1s, and 1000000 - which aren't equivalent */
++	I915_WRITE(GEN6_RP_DOWN_TIMEOUT, 100000000 / 128); /* 1 second timeout */
+ 
+-	I915_WRITE(GEN6_RP_IDLE_HYSTERSIS, 10);
++	/* Docs recommend 900MHz, and 300 MHz respectively */
++	I915_WRITE(GEN6_RP_INTERRUPT_LIMITS,
++		   dev_priv->rps.max_freq_softlimit << 24 |
++		   dev_priv->rps.min_freq_softlimit << 16);
+ 
+-	/* WaDisablePwrmtrEvent:chv (pre-production hw) */
+-	I915_WRITE(0xA80C, I915_READ(0xA80C) & 0x00ffffff);
+-	I915_WRITE(0xA810, I915_READ(0xA810) & 0xffffff00);
++	I915_WRITE(GEN6_RP_UP_THRESHOLD, 7600000 / 128); /* 76ms busyness per EI, 90% */
++	I915_WRITE(GEN6_RP_DOWN_THRESHOLD, 31300000 / 128); /* 313ms busyness per EI, 70%*/
++	I915_WRITE(GEN6_RP_UP_EI, 66000); /* 84.48ms, XXX: random? */
++	I915_WRITE(GEN6_RP_DOWN_EI, 350000); /* 448ms, XXX: random? */
++
++	I915_WRITE(GEN6_RP_IDLE_HYSTERSIS, 10);
+ 
+ 	/* 5: Enable RPS */
+ 	I915_WRITE(GEN6_RP_CONTROL,
++		   GEN6_RP_MEDIA_TURBO |
+ 		   GEN6_RP_MEDIA_HW_NORMAL_MODE |
+-		   GEN6_RP_MEDIA_IS_GFX | /* WaSetMaskForGfxBusyness:chv (pre-production hw ?) */
++		   GEN6_RP_MEDIA_IS_GFX |
+ 		   GEN6_RP_ENABLE |
+ 		   GEN6_RP_UP_BUSY_AVG |
+ 		   GEN6_RP_DOWN_IDLE_AVG);
+ 
+-	val = vlv_punit_read(dev_priv, PUNIT_REG_GPU_FREQ_STS);
+-
+-	DRM_DEBUG_DRIVER("GPLL enabled? %s\n", val & 0x10 ? "yes" : "no");
+-	DRM_DEBUG_DRIVER("GPU status: 0x%08x\n", val);
+-
+-	dev_priv->rps.cur_freq = (val >> 8) & 0xff;
+-	DRM_DEBUG_DRIVER("current GPU freq: %d MHz (%u)\n",
+-			 vlv_gpu_freq(dev_priv, dev_priv->rps.cur_freq),
+-			 dev_priv->rps.cur_freq);
+-
+-	DRM_DEBUG_DRIVER("setting GPU freq to %d MHz (%u)\n",
+-			 vlv_gpu_freq(dev_priv, dev_priv->rps.efficient_freq),
+-			 dev_priv->rps.efficient_freq);
++	/* 6: Ring frequency + overclocking (our driver does this later */
+ 
+-	valleyview_set_rps(dev_priv->dev, dev_priv->rps.efficient_freq);
++	gen6_set_rps(dev, (I915_READ(GEN6_GT_PERF_STATUS) & 0xff00) >> 8);
+ 
+-	gen8_enable_rps_interrupts(dev);
++	gen6_enable_rps_interrupts(dev);
+ 
+ 	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
+ }
+ 
+-static void valleyview_enable_rps(struct drm_device *dev)
++static void gen6_enable_rps(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring;
+-	u32 gtfifodbg, val, rc6_mode = 0;
+-	int i;
++	struct intel_engine_cs *engine;
++	u32 rp_state_cap;
++	u32 rc6vids, pcu_mbox = 0, rc6_mask = 0;
++	u32 gtfifodbg;
++	int rc6_mode;
++	int i, ret;
+ 
+ 	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
+ 
+-	valleyview_check_pctx(dev_priv);
++	/* Here begins a magic sequence of register writes to enable
++	 * auto-downclocking.
++	 *
++	 * Perhaps there might be some value in exposing these to
++	 * userspace...
++	 */
++	I915_WRITE(GEN6_RC_STATE, 0);
+ 
++	/* Clear the DBG now so we don't confuse earlier errors */
+ 	if ((gtfifodbg = I915_READ(GTFIFODBG))) {
+-		DRM_DEBUG_DRIVER("GT fifo had a previous error %x\n",
+-				 gtfifodbg);
++		DRM_ERROR("GT fifo had a previous error %x\n", gtfifodbg);
+ 		I915_WRITE(GTFIFODBG, gtfifodbg);
+ 	}
+ 
+-	/* If VLV, Forcewake all wells, else re-direct to regular path */
+ 	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
+ 
+-	I915_WRITE(GEN6_RP_UP_THRESHOLD, 59400);
+-	I915_WRITE(GEN6_RP_DOWN_THRESHOLD, 245000);
+-	I915_WRITE(GEN6_RP_UP_EI, 66000);
+-	I915_WRITE(GEN6_RP_DOWN_EI, 350000);
++	rp_state_cap = I915_READ(GEN6_RP_STATE_CAP);
+ 
+-	I915_WRITE(GEN6_RP_IDLE_HYSTERSIS, 10);
+-	I915_WRITE(GEN6_RP_DOWN_TIMEOUT, 0xf4240);
++	parse_rp_state_cap(dev_priv, rp_state_cap);
+ 
+-	I915_WRITE(GEN6_RP_CONTROL,
+-		   GEN6_RP_MEDIA_TURBO |
+-		   GEN6_RP_MEDIA_HW_NORMAL_MODE |
+-		   GEN6_RP_MEDIA_IS_GFX |
+-		   GEN6_RP_ENABLE |
+-		   GEN6_RP_UP_BUSY_AVG |
+-		   GEN6_RP_DOWN_IDLE_CONT);
++	/* disable the counters and set deterministic thresholds */
++	I915_WRITE(GEN6_RC_CONTROL, 0);
+ 
+-	I915_WRITE(GEN6_RC6_WAKE_RATE_LIMIT, 0x00280000);
++	I915_WRITE(GEN6_RC1_WAKE_RATE_LIMIT, 1000 << 16);
++	I915_WRITE(GEN6_RC6_WAKE_RATE_LIMIT, 40 << 16 | 30);
++	I915_WRITE(GEN6_RC6pp_WAKE_RATE_LIMIT, 30);
+ 	I915_WRITE(GEN6_RC_EVALUATION_INTERVAL, 125000);
+ 	I915_WRITE(GEN6_RC_IDLE_HYSTERSIS, 25);
+ 
+-	for_each_ring(ring, dev_priv, i)
+-		I915_WRITE(RING_MAX_IDLE(ring->mmio_base), 10);
+-
+-	I915_WRITE(GEN6_RC6_THRESHOLD, 0x557);
++	for_each_engine(engine, dev_priv, i)
++		I915_WRITE(RING_MAX_IDLE(engine->mmio_base), 10);
+ 
+-	/* allows RC6 residency counter to work */
+-	I915_WRITE(VLV_COUNTER_CONTROL,
+-		   _MASKED_BIT_ENABLE(VLV_MEDIA_RC0_COUNT_EN |
+-				      VLV_RENDER_RC0_COUNT_EN |
+-				      VLV_MEDIA_RC6_COUNT_EN |
+-				      VLV_RENDER_RC6_COUNT_EN));
++	I915_WRITE(GEN6_RC_SLEEP, 0);
++	I915_WRITE(GEN6_RC1e_THRESHOLD, 1000);
++	if (IS_IVYBRIDGE(dev))
++		I915_WRITE(GEN6_RC6_THRESHOLD, 125000);
++	else
++		I915_WRITE(GEN6_RC6_THRESHOLD, 50000);
++	I915_WRITE(GEN6_RC6p_THRESHOLD, 150000);
++	I915_WRITE(GEN6_RC6pp_THRESHOLD, 64000); /* unused */
+ 
+-	if (intel_enable_rc6(dev) & INTEL_RC6_ENABLE)
+-		rc6_mode = GEN7_RC_CTL_TO_MODE | VLV_RC_CTL_CTX_RST_PARALLEL;
++	/* Check if we are enabling RC6 */
++	rc6_mode = intel_enable_rc6(dev_priv->dev);
++	if (rc6_mode & INTEL_RC6_ENABLE)
++		rc6_mask |= GEN6_RC_CTL_RC6_ENABLE;
+ 
+-	intel_print_rc6_info(dev, rc6_mode);
++	/* We don't use those on Haswell */
++	if (!IS_HASWELL(dev)) {
++		if (rc6_mode & INTEL_RC6p_ENABLE)
++			rc6_mask |= GEN6_RC_CTL_RC6p_ENABLE;
+ 
+-	I915_WRITE(GEN6_RC_CONTROL, rc6_mode);
++		if (rc6_mode & INTEL_RC6pp_ENABLE)
++			rc6_mask |= GEN6_RC_CTL_RC6pp_ENABLE;
++	}
+ 
+-	val = vlv_punit_read(dev_priv, PUNIT_REG_GPU_FREQ_STS);
++	intel_print_rc6_info(dev, rc6_mask);
+ 
+-	DRM_DEBUG_DRIVER("GPLL enabled? %s\n", val & 0x10 ? "yes" : "no");
+-	DRM_DEBUG_DRIVER("GPU status: 0x%08x\n", val);
++	I915_WRITE(GEN6_RC_CONTROL,
++		   rc6_mask |
++		   GEN6_RC_CTL_EI_MODE(1) |
++		   GEN6_RC_CTL_HW_ENABLE);
+ 
+-	dev_priv->rps.cur_freq = (val >> 8) & 0xff;
+-	DRM_DEBUG_DRIVER("current GPU freq: %d MHz (%u)\n",
+-			 vlv_gpu_freq(dev_priv, dev_priv->rps.cur_freq),
+-			 dev_priv->rps.cur_freq);
++	/* Power down if completely idle for over 50ms */
++	I915_WRITE(GEN6_RP_DOWN_TIMEOUT, 50000);
++	I915_WRITE(GEN6_RP_IDLE_HYSTERSIS, 10);
+ 
+-	DRM_DEBUG_DRIVER("setting GPU freq to %d MHz (%u)\n",
+-			 vlv_gpu_freq(dev_priv, dev_priv->rps.efficient_freq),
+-			 dev_priv->rps.efficient_freq);
++	ret = sandybridge_pcode_write(dev_priv, GEN6_PCODE_WRITE_MIN_FREQ_TABLE, 0);
++	if (ret)
++		DRM_DEBUG_DRIVER("Failed to set the min frequency\n");
+ 
+-	valleyview_set_rps(dev_priv->dev, dev_priv->rps.efficient_freq);
++	ret = sandybridge_pcode_read(dev_priv, GEN6_READ_OC_PARAMS, &pcu_mbox);
++	if (!ret && (pcu_mbox & (1<<31))) { /* OC supported */
++		DRM_DEBUG_DRIVER("Overclocking supported. Max: %dMHz, Overclock max: %dMHz\n",
++				 (dev_priv->rps.max_freq_softlimit & 0xff) * 50,
++				 (pcu_mbox & 0xff) * 50);
++		dev_priv->rps.max_freq = pcu_mbox & 0xff;
++	}
++
++	dev_priv->rps.power = HIGH_POWER; /* force a reset */
++	gen6_set_rps(dev_priv->dev, dev_priv->rps.min_freq);
+ 
+ 	gen6_enable_rps_interrupts(dev);
+ 
++	rc6vids = 0;
++	ret = sandybridge_pcode_read(dev_priv, GEN6_PCODE_READ_RC6VIDS, &rc6vids);
++	if (IS_GEN6(dev) && ret) {
++		DRM_DEBUG_DRIVER("Couldn't check for BIOS workaround\n");
++	} else if (IS_GEN6(dev) && (GEN6_DECODE_RC6_VID(rc6vids & 0xff) < 450)) {
++		DRM_DEBUG_DRIVER("You should update your BIOS. Correcting minimum rc6 voltage (%dmV->%dmV)\n",
++			  GEN6_DECODE_RC6_VID(rc6vids & 0xff), 450);
++		rc6vids &= 0xffff00;
++		rc6vids |= GEN6_ENCODE_RC6_VID(450);
++		ret = sandybridge_pcode_write(dev_priv, GEN6_PCODE_WRITE_RC6VIDS, rc6vids);
++		if (ret)
++			DRM_ERROR("Couldn't fix incorrect rc6 voltage\n");
++	}
++
+ 	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
+ }
+ 
+-void ironlake_teardown_rc6(struct drm_device *dev)
++static void __gen6_update_ring_freq(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
++	unsigned int max_ia_freq, min_ia_freq, min_ring_freq;
++	unsigned int gpu_freq;
++	struct cpufreq_policy *policy;
+ 
+-	if (dev_priv->ips.renderctx) {
+-		i915_gem_object_ggtt_unpin(dev_priv->ips.renderctx);
+-		drm_gem_object_unreference(&dev_priv->ips.renderctx->base);
+-		dev_priv->ips.renderctx = NULL;
+-	}
++	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
+ 
+-	if (dev_priv->ips.pwrctx) {
+-		i915_gem_object_ggtt_unpin(dev_priv->ips.pwrctx);
+-		drm_gem_object_unreference(&dev_priv->ips.pwrctx->base);
+-		dev_priv->ips.pwrctx = NULL;
++	min_ring_freq = I915_READ(DCLK) & 0xf;
++	/* convert DDR frequency from units of 266.6MHz to bandwidth */
++	min_ring_freq = mult_frac(min_ring_freq, 8, 3);
++
++	policy = cpufreq_cpu_get(0);
++	if (policy) {
++		/* Convert from kHz to MHz */
++		max_ia_freq = policy->cpuinfo.max_freq / 1000;
++		min_ia_freq = policy->cpuinfo.min_freq / 1000;
++		cpufreq_cpu_put(policy);
++	} else {
++		/*
++		 * Default to measured freq if none found, PCU will ensure we
++		 * don't go over
++		 */
++		max_ia_freq = tsc_khz / 1000;
++		min_ia_freq = min_ring_freq;
+ 	}
+-}
+ 
+-static void ironlake_disable_rc6(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	/*
++	 * For each potential GPU frequency, load a ring frequency we'd like
++	 * to use for memory access.  We do this by specifying the IA frequency
++	 * the PCU should use as a reference to determine the ring frequency.
++	 */
++	for (gpu_freq = dev_priv->rps.max_freq; gpu_freq >= dev_priv->rps.min_freq;
++	     gpu_freq--) {
++		unsigned int ia_freq = 0, ring_freq = 0;
+ 
+-	if (I915_READ(PWRCTXA)) {
+-		/* Wake the GPU, prevent RC6, then restore RSTDBYCTL */
+-		I915_WRITE(RSTDBYCTL, I915_READ(RSTDBYCTL) | RCX_SW_EXIT);
+-		wait_for(((I915_READ(RSTDBYCTL) & RSX_STATUS_MASK) == RSX_STATUS_ON),
+-			 50);
++		if (INTEL_INFO(dev)->gen >= 8) {
++			/* max(2 * GT, DDR). NB: GT is 50MHz units */
++			ring_freq = max(min_ring_freq, gpu_freq);
++		} else if (IS_HASWELL(dev)) {
++			ring_freq = mult_frac(gpu_freq, 5, 4);
++			ring_freq = max(min_ring_freq, ring_freq);
++			/* leave ia_freq as the default, chosen by cpufreq */
++		} else {
++			const int scaling_factor = 180;
++			int diff;
+ 
+-		I915_WRITE(PWRCTXA, 0);
+-		POSTING_READ(PWRCTXA);
++			/* On older processors, there is no separate ring
++			 * clock domain, so in order to boost the bandwidth
++			 * of the ring, we need to upclock the CPU (ia_freq).
++			 */
++			diff = dev_priv->rps.max_freq_softlimit - gpu_freq;
++			diff = max_ia_freq - diff * scaling_factor / 2;
++			ia_freq = max((int)min_ia_freq, diff);
++			ia_freq = DIV_ROUND_CLOSEST(ia_freq, 100);
++		}
+ 
+-		I915_WRITE(RSTDBYCTL, I915_READ(RSTDBYCTL) & ~RCX_SW_EXIT);
+-		POSTING_READ(RSTDBYCTL);
++		sandybridge_pcode_write(dev_priv,
++					GEN6_PCODE_WRITE_MIN_FREQ_TABLE,
++					ia_freq << GEN6_PCODE_FREQ_IA_RATIO_SHIFT |
++					ring_freq << GEN6_PCODE_FREQ_RING_RATIO_SHIFT |
++					gpu_freq);
+ 	}
+ }
+ 
+-static int ironlake_setup_rc6(struct drm_device *dev)
++void gen6_update_ring_freq(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	if (dev_priv->ips.renderctx == NULL)
+-		dev_priv->ips.renderctx = intel_alloc_context_page(dev);
+-	if (!dev_priv->ips.renderctx)
+-		return -ENOMEM;
+-
+-	if (dev_priv->ips.pwrctx == NULL)
+-		dev_priv->ips.pwrctx = intel_alloc_context_page(dev);
+-	if (!dev_priv->ips.pwrctx) {
+-		ironlake_teardown_rc6(dev);
+-		return -ENOMEM;
+-	}
++	if (INTEL_INFO(dev)->gen < 6 || IS_VALLEYVIEW(dev))
++		return;
+ 
+-	return 0;
++	mutex_lock(&dev_priv->rps.hw_lock);
++	__gen6_update_ring_freq(dev);
++	mutex_unlock(&dev_priv->rps.hw_lock);
+ }
+ 
+-static void ironlake_enable_rc6(struct drm_device *dev)
++static int cherryview_rps_max_freq(struct drm_i915_private *dev_priv)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring = &dev_priv->ring[RCS];
+-	bool was_interruptible;
+-	int ret;
++	u32 val, rp0;
+ 
+-	/* rc6 disabled by default due to repeated reports of hanging during
+-	 * boot and resume.
+-	 */
+-	if (!intel_enable_rc6(dev))
+-		return;
++	val = vlv_punit_read(dev_priv, PUNIT_GPU_STATUS_REG);
++	rp0 = (val >> PUNIT_GPU_STATUS_MAX_FREQ_SHIFT) & PUNIT_GPU_STATUS_MAX_FREQ_MASK;
+ 
+-	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
++	return rp0;
++}
+ 
+-	ret = ironlake_setup_rc6(dev);
+-	if (ret)
+-		return;
++static int cherryview_rps_rpe_freq(struct drm_i915_private *dev_priv)
++{
++	u32 val, rpe;
+ 
+-	was_interruptible = dev_priv->mm.interruptible;
+-	dev_priv->mm.interruptible = false;
++	val = vlv_punit_read(dev_priv, PUNIT_GPU_DUTYCYCLE_REG);
++	rpe = (val >> PUNIT_GPU_DUTYCYCLE_RPE_FREQ_SHIFT) & PUNIT_GPU_DUTYCYCLE_RPE_FREQ_MASK;
+ 
+-	/*
+-	 * GPU can automatically power down the render unit if given a page
+-	 * to save state.
+-	 */
+-	ret = intel_ring_begin(ring, 6);
+-	if (ret) {
+-		ironlake_teardown_rc6(dev);
+-		dev_priv->mm.interruptible = was_interruptible;
+-		return;
+-	}
++	return rpe;
++}
+ 
+-	intel_ring_emit(ring, MI_SUSPEND_FLUSH | MI_SUSPEND_FLUSH_EN);
+-	intel_ring_emit(ring, MI_SET_CONTEXT);
+-	intel_ring_emit(ring, i915_gem_obj_ggtt_offset(dev_priv->ips.renderctx) |
+-			MI_MM_SPACE_GTT |
+-			MI_SAVE_EXT_STATE_EN |
+-			MI_RESTORE_EXT_STATE_EN |
+-			MI_RESTORE_INHIBIT);
+-	intel_ring_emit(ring, MI_SUSPEND_FLUSH);
+-	intel_ring_emit(ring, MI_NOOP);
+-	intel_ring_emit(ring, MI_FLUSH);
+-	intel_ring_advance(ring);
++static int cherryview_rps_guar_freq(struct drm_i915_private *dev_priv)
++{
++	u32 val, rp1;
+ 
+-	/*
+-	 * Wait for the command parser to advance past MI_SET_CONTEXT. The HW
+-	 * does an implicit flush, combined with MI_FLUSH above, it should be
+-	 * safe to assume that renderctx is valid
+-	 */
+-	ret = intel_ring_idle(ring);
+-	dev_priv->mm.interruptible = was_interruptible;
+-	if (ret) {
+-		DRM_ERROR("failed to enable ironlake power savings\n");
+-		ironlake_teardown_rc6(dev);
+-		return;
+-	}
++	val = vlv_punit_read(dev_priv, PUNIT_REG_GPU_FREQ_STS);
++	rp1 = (val >> PUNIT_GPU_STATUS_MAX_FREQ_SHIFT) & PUNIT_GPU_STATUS_MAX_FREQ_MASK;
+ 
+-	I915_WRITE(PWRCTXA, i915_gem_obj_ggtt_offset(dev_priv->ips.pwrctx) | PWRCTX_EN);
+-	I915_WRITE(RSTDBYCTL, I915_READ(RSTDBYCTL) & ~RCX_SW_EXIT);
++	return rp1;
++}
+ 
+-	intel_print_rc6_info(dev, GEN6_RC_CTL_RC6_ENABLE);
++static int cherryview_rps_min_freq(struct drm_i915_private *dev_priv)
++{
++	u32 val, rpn;
++
++	val = vlv_punit_read(dev_priv, PUNIT_GPU_STATUS_REG);
++	rpn = (val >> PUNIT_GPU_STATIS_GFX_MIN_FREQ_SHIFT) & PUNIT_GPU_STATUS_GFX_MIN_FREQ_MASK;
++	return rpn;
+ }
+ 
+-static unsigned long intel_pxfreq(u32 vidfreq)
++static int valleyview_rps_guar_freq(struct drm_i915_private *dev_priv)
+ {
+-	unsigned long freq;
+-	int div = (vidfreq & 0x3f0000) >> 16;
+-	int post = (vidfreq & 0x3000) >> 12;
+-	int pre = (vidfreq & 0x7);
++	u32 val, rp1;
+ 
+-	if (!pre)
+-		return 0;
++	val = vlv_nc_read(dev_priv, IOSF_NC_FB_GFX_FREQ_FUSE);
+ 
+-	freq = ((div * 133333) / ((1<<post) * pre));
++	rp1 = (val & FB_GFX_FGUARANTEED_FREQ_FUSE_MASK) >> FB_GFX_FGUARANTEED_FREQ_FUSE_SHIFT;
+ 
+-	return freq;
++	return rp1;
+ }
+ 
+-static const struct cparams {
+-	u16 i;
+-	u16 t;
+-	u16 m;
+-	u16 c;
+-} cparams[] = {
+-	{ 1, 1333, 301, 28664 },
+-	{ 1, 1066, 294, 24460 },
+-	{ 1, 800, 294, 25192 },
+-	{ 0, 1333, 276, 27605 },
+-	{ 0, 1066, 276, 27605 },
+-	{ 0, 800, 231, 23784 },
+-};
+-
+-static unsigned long __i915_chipset_val(struct drm_i915_private *dev_priv)
++static int valleyview_rps_max_freq(struct drm_i915_private *dev_priv)
+ {
+-	u64 total_count, diff, ret;
+-	u32 count1, count2, count3, m = 0, c = 0;
+-	unsigned long now = jiffies_to_msecs(jiffies), diff1;
+-	int i;
++	u32 val, rp0;
+ 
+-	assert_spin_locked(&mchdev_lock);
++	val = vlv_nc_read(dev_priv, IOSF_NC_FB_GFX_FREQ_FUSE);
+ 
+-	diff1 = now - dev_priv->ips.last_time1;
++	rp0 = (val & FB_GFX_MAX_FREQ_FUSE_MASK) >> FB_GFX_MAX_FREQ_FUSE_SHIFT;
++	/* Clamp to max */
++	rp0 = min_t(u32, rp0, 0xea);
+ 
+-	/* Prevent division-by-zero if we are asking too fast.
+-	 * Also, we don't get interesting results if we are polling
+-	 * faster than once in 10ms, so just return the saved value
+-	 * in such cases.
+-	 */
+-	if (diff1 <= 10)
+-		return dev_priv->ips.chipset_power;
++	return rp0;
++}
+ 
+-	count1 = I915_READ(DMIEC);
+-	count2 = I915_READ(DDREC);
+-	count3 = I915_READ(CSIEC);
++static int valleyview_rps_rpe_freq(struct drm_i915_private *dev_priv)
++{
++	u32 val, rpe;
+ 
+-	total_count = count1 + count2 + count3;
++	val = vlv_nc_read(dev_priv, IOSF_NC_FB_GFX_FMAX_FUSE_LO);
++	rpe = (val & FB_FMAX_VMIN_FREQ_LO_MASK) >> FB_FMAX_VMIN_FREQ_LO_SHIFT;
++	val = vlv_nc_read(dev_priv, IOSF_NC_FB_GFX_FMAX_FUSE_HI);
++	rpe |= (val & FB_FMAX_VMIN_FREQ_HI_MASK) << 5;
+ 
+-	/* FIXME: handle per-counter overflow */
+-	if (total_count < dev_priv->ips.last_count1) {
+-		diff = ~0UL - dev_priv->ips.last_count1;
+-		diff += total_count;
+-	} else {
+-		diff = total_count - dev_priv->ips.last_count1;
+-	}
++	return rpe;
++}
+ 
+-	for (i = 0; i < ARRAY_SIZE(cparams); i++) {
+-		if (cparams[i].i == dev_priv->ips.c_m &&
+-		    cparams[i].t == dev_priv->ips.r_t) {
+-			m = cparams[i].m;
+-			c = cparams[i].c;
+-			break;
+-		}
+-	}
++static int valleyview_rps_min_freq(struct drm_i915_private *dev_priv)
++{
++	return vlv_punit_read(dev_priv, PUNIT_REG_GPU_LFM) & 0xff;
++}
+ 
+-	diff = div_u64(diff, diff1);
+-	ret = ((m * diff) + c);
+-	ret = div_u64(ret, 10);
+-
+-	dev_priv->ips.last_count1 = total_count;
+-	dev_priv->ips.last_time1 = now;
+-
+-	dev_priv->ips.chipset_power = ret;
+-
+-	return ret;
+-}
+-
+-unsigned long i915_chipset_val(struct drm_i915_private *dev_priv)
++/* Check that the pctx buffer wasn't move under us. */
++static void valleyview_check_pctx(struct drm_i915_private *dev_priv)
+ {
+-	struct drm_device *dev = dev_priv->dev;
+-	unsigned long val;
+-
+-	if (INTEL_INFO(dev)->gen != 5)
+-		return 0;
++	unsigned long pctx_addr = I915_READ(VLV_PCBR) & ~4095;
+ 
+-	spin_lock_irq(&mchdev_lock);
++	WARN_ON(pctx_addr != dev_priv->mm.stolen_base +
++			     dev_priv->vlv_pctx->stolen->start);
++}
+ 
+-	val = __i915_chipset_val(dev_priv);
+ 
+-	spin_unlock_irq(&mchdev_lock);
++/* Check that the pcbr address is not empty. */
++static void cherryview_check_pctx(struct drm_i915_private *dev_priv)
++{
++	unsigned long pctx_addr = I915_READ(VLV_PCBR) & ~4095;
+ 
+-	return val;
++	WARN_ON((pctx_addr >> VLV_PCBR_ADDR_SHIFT) == 0);
+ }
+ 
+-unsigned long i915_mch_val(struct drm_i915_private *dev_priv)
++static void cherryview_setup_pctx(struct drm_device *dev)
+ {
+-	unsigned long m, x, b;
+-	u32 tsfs;
+-
+-	tsfs = I915_READ(TSFS);
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	unsigned long pctx_paddr, paddr;
++	struct i915_gtt *gtt = &dev_priv->gtt;
++	u32 pcbr;
++	int pctx_size = 32*1024;
+ 
+-	m = ((tsfs & TSFS_SLOPE_MASK) >> TSFS_SLOPE_SHIFT);
+-	x = I915_READ8(TR1);
++	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
+ 
+-	b = tsfs & TSFS_INTR_MASK;
++	pcbr = I915_READ(VLV_PCBR);
++	if ((pcbr >> VLV_PCBR_ADDR_SHIFT) == 0) {
++		paddr = (dev_priv->mm.stolen_base +
++			 (gtt->stolen_size - pctx_size));
+ 
+-	return ((m * x) / 127) - b;
++		pctx_paddr = (paddr & (~4095));
++		I915_WRITE(VLV_PCBR, pctx_paddr);
++	}
+ }
+ 
+-static u16 pvid_to_extvid(struct drm_i915_private *dev_priv, u8 pxvid)
++static void valleyview_setup_pctx(struct drm_device *dev)
+ {
+-	struct drm_device *dev = dev_priv->dev;
+-	static const struct v_table {
+-		u16 vd; /* in .1 mil */
+-		u16 vm; /* in .1 mil */
+-	} v_table[] = {
+-		{ 0, 0, },
+-		{ 375, 0, },
+-		{ 500, 0, },
+-		{ 625, 0, },
+-		{ 750, 0, },
+-		{ 875, 0, },
+-		{ 1000, 0, },
+-		{ 1125, 0, },
+-		{ 4125, 3000, },
+-		{ 4125, 3000, },
+-		{ 4125, 3000, },
+-		{ 4125, 3000, },
+-		{ 4125, 3000, },
+-		{ 4125, 3000, },
+-		{ 4125, 3000, },
+-		{ 4125, 3000, },
+-		{ 4125, 3000, },
+-		{ 4125, 3000, },
+-		{ 4125, 3000, },
+-		{ 4125, 3000, },
+-		{ 4125, 3000, },
+-		{ 4125, 3000, },
+-		{ 4125, 3000, },
+-		{ 4125, 3000, },
+-		{ 4125, 3000, },
+-		{ 4125, 3000, },
+-		{ 4125, 3000, },
+-		{ 4125, 3000, },
+-		{ 4125, 3000, },
+-		{ 4125, 3000, },
+-		{ 4125, 3000, },
+-		{ 4125, 3000, },
+-		{ 4250, 3125, },
+-		{ 4375, 3250, },
+-		{ 4500, 3375, },
+-		{ 4625, 3500, },
+-		{ 4750, 3625, },
+-		{ 4875, 3750, },
+-		{ 5000, 3875, },
+-		{ 5125, 4000, },
+-		{ 5250, 4125, },
+-		{ 5375, 4250, },
+-		{ 5500, 4375, },
+-		{ 5625, 4500, },
+-		{ 5750, 4625, },
+-		{ 5875, 4750, },
+-		{ 6000, 4875, },
+-		{ 6125, 5000, },
+-		{ 6250, 5125, },
+-		{ 6375, 5250, },
+-		{ 6500, 5375, },
+-		{ 6625, 5500, },
+-		{ 6750, 5625, },
+-		{ 6875, 5750, },
+-		{ 7000, 5875, },
+-		{ 7125, 6000, },
+-		{ 7250, 6125, },
+-		{ 7375, 6250, },
+-		{ 7500, 6375, },
+-		{ 7625, 6500, },
+-		{ 7750, 6625, },
+-		{ 7875, 6750, },
+-		{ 8000, 6875, },
+-		{ 8125, 7000, },
+-		{ 8250, 7125, },
+-		{ 8375, 7250, },
+-		{ 8500, 7375, },
+-		{ 8625, 7500, },
+-		{ 8750, 7625, },
+-		{ 8875, 7750, },
+-		{ 9000, 7875, },
+-		{ 9125, 8000, },
+-		{ 9250, 8125, },
+-		{ 9375, 8250, },
+-		{ 9500, 8375, },
+-		{ 9625, 8500, },
+-		{ 9750, 8625, },
+-		{ 9875, 8750, },
+-		{ 10000, 8875, },
+-		{ 10125, 9000, },
+-		{ 10250, 9125, },
+-		{ 10375, 9250, },
+-		{ 10500, 9375, },
+-		{ 10625, 9500, },
+-		{ 10750, 9625, },
+-		{ 10875, 9750, },
+-		{ 11000, 9875, },
+-		{ 11125, 10000, },
+-		{ 11250, 10125, },
+-		{ 11375, 10250, },
+-		{ 11500, 10375, },
+-		{ 11625, 10500, },
+-		{ 11750, 10625, },
+-		{ 11875, 10750, },
+-		{ 12000, 10875, },
+-		{ 12125, 11000, },
+-		{ 12250, 11125, },
+-		{ 12375, 11250, },
+-		{ 12500, 11375, },
+-		{ 12625, 11500, },
+-		{ 12750, 11625, },
+-		{ 12875, 11750, },
+-		{ 13000, 11875, },
+-		{ 13125, 12000, },
+-		{ 13250, 12125, },
+-		{ 13375, 12250, },
+-		{ 13500, 12375, },
+-		{ 13625, 12500, },
+-		{ 13750, 12625, },
+-		{ 13875, 12750, },
+-		{ 14000, 12875, },
+-		{ 14125, 13000, },
+-		{ 14250, 13125, },
+-		{ 14375, 13250, },
+-		{ 14500, 13375, },
+-		{ 14625, 13500, },
+-		{ 14750, 13625, },
+-		{ 14875, 13750, },
+-		{ 15000, 13875, },
+-		{ 15125, 14000, },
+-		{ 15250, 14125, },
+-		{ 15375, 14250, },
+-		{ 15500, 14375, },
+-		{ 15625, 14500, },
+-		{ 15750, 14625, },
+-		{ 15875, 14750, },
+-		{ 16000, 14875, },
+-		{ 16125, 15000, },
+-	};
+-	if (INTEL_INFO(dev)->is_mobile)
+-		return v_table[pxvid].vm;
+-	else
+-		return v_table[pxvid].vd;
+-}
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_i915_gem_object *pctx;
++	unsigned long pctx_paddr;
++	u32 pcbr;
++	int pctx_size = 24*1024;
+ 
+-static void __i915_update_gfx_val(struct drm_i915_private *dev_priv)
+-{
+-	u64 now, diff, diffms;
+-	u32 count;
++	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
+ 
+-	assert_spin_locked(&mchdev_lock);
++	pcbr = I915_READ(VLV_PCBR);
++	if (pcbr) {
++		/* BIOS set it up already, grab the pre-alloc'd space */
++		int pcbr_offset;
+ 
+-	now = ktime_get_raw_ns();
+-	diffms = now - dev_priv->ips.last_time2;
+-	do_div(diffms, NSEC_PER_MSEC);
++		pcbr_offset = (pcbr & (~4095)) - dev_priv->mm.stolen_base;
++		pctx = i915_gem_object_create_stolen_for_preallocated(dev_priv->dev,
++								      pcbr_offset,
++								      I915_GTT_OFFSET_NONE,
++								      pctx_size);
++		goto out;
++	}
+ 
+-	/* Don't divide by 0 */
+-	if (!diffms)
++	/*
++	 * From the Gunit register HAS:
++	 * The Gfx driver is expected to program this register and ensure
++	 * proper allocation within Gfx stolen memory.  For example, this
++	 * register should be programmed such than the PCBR range does not
++	 * overlap with other ranges, such as the frame buffer, protected
++	 * memory, or any other relevant ranges.
++	 */
++	pctx = i915_gem_object_create_stolen(dev, pctx_size);
++	if (!pctx) {
++		DRM_DEBUG("not enough stolen space for PCTX, disabling\n");
+ 		return;
+-
+-	count = I915_READ(GFXEC);
+-
+-	if (count < dev_priv->ips.last_count2) {
+-		diff = ~0UL - dev_priv->ips.last_count2;
+-		diff += count;
+-	} else {
+-		diff = count - dev_priv->ips.last_count2;
+ 	}
+ 
+-	dev_priv->ips.last_count2 = count;
+-	dev_priv->ips.last_time2 = now;
++	pctx_paddr = dev_priv->mm.stolen_base + pctx->stolen->start;
++	I915_WRITE(VLV_PCBR, pctx_paddr);
+ 
+-	/* More magic constants... */
+-	diff = diff * 1181;
+-	diff = div_u64(diff, diffms * 10);
+-	dev_priv->ips.gfx_power = diff;
++out:
++	dev_priv->vlv_pctx = pctx;
+ }
+ 
+-void i915_update_gfx_val(struct drm_i915_private *dev_priv)
++static void valleyview_cleanup_pctx(struct drm_device *dev)
+ {
+-	struct drm_device *dev = dev_priv->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	if (INTEL_INFO(dev)->gen != 5)
++	if (WARN_ON(!dev_priv->vlv_pctx))
+ 		return;
+ 
+-	spin_lock_irq(&mchdev_lock);
+-
+-	__i915_update_gfx_val(dev_priv);
+-
+-	spin_unlock_irq(&mchdev_lock);
++	drm_gem_object_unreference(&dev_priv->vlv_pctx->base);
++	dev_priv->vlv_pctx = NULL;
+ }
+ 
+-static unsigned long __i915_gfx_val(struct drm_i915_private *dev_priv)
++static void valleyview_init_gt_powersave(struct drm_device *dev)
+ {
+-	unsigned long t, corr, state1, corr2, state2;
+-	u32 pxvid, ext_v;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	u32 val;
+ 
+-	assert_spin_locked(&mchdev_lock);
++	valleyview_setup_pctx(dev);
+ 
+-	pxvid = I915_READ(PXVFREQ_BASE + (dev_priv->rps.cur_freq * 4));
+-	pxvid = (pxvid >> 24) & 0x7f;
+-	ext_v = pvid_to_extvid(dev_priv, pxvid);
++	mutex_lock(&dev_priv->rps.hw_lock);
+ 
+-	state1 = ext_v;
++	val = vlv_punit_read(dev_priv, PUNIT_REG_GPU_FREQ_STS);
++	switch ((val >> 6) & 3) {
++	case 0:
++	case 1:
++		dev_priv->mem_freq = 800;
++		break;
++	case 2:
++		dev_priv->mem_freq = 1066;
++		break;
++	case 3:
++		dev_priv->mem_freq = 1333;
++		break;
++	}
++	DRM_DEBUG_DRIVER("DDR speed: %d MHz", dev_priv->mem_freq);
+ 
+-	t = i915_mch_val(dev_priv);
++	dev_priv->rps.max_freq = valleyview_rps_max_freq(dev_priv);
++	dev_priv->rps.rp0_freq = dev_priv->rps.max_freq;
++	DRM_DEBUG_DRIVER("max GPU freq: %d MHz (%u)\n",
++			 vlv_gpu_freq(dev_priv, dev_priv->rps.max_freq),
++			 dev_priv->rps.max_freq);
+ 
+-	/* Revel in the empirically derived constants */
++	dev_priv->rps.efficient_freq = valleyview_rps_rpe_freq(dev_priv);
++	DRM_DEBUG_DRIVER("RPe GPU freq: %d MHz (%u)\n",
++			 vlv_gpu_freq(dev_priv, dev_priv->rps.efficient_freq),
++			 dev_priv->rps.efficient_freq);
+ 
+-	/* Correction factor in 1/100000 units */
+-	if (t > 80)
+-		corr = ((t * 2349) + 135940);
+-	else if (t >= 50)
+-		corr = ((t * 964) + 29317);
+-	else /* < 50 */
+-		corr = ((t * 301) + 1004);
++	dev_priv->rps.rp1_freq = valleyview_rps_guar_freq(dev_priv);
++	DRM_DEBUG_DRIVER("RP1(Guar Freq) GPU freq: %d MHz (%u)\n",
++			 vlv_gpu_freq(dev_priv, dev_priv->rps.rp1_freq),
++			 dev_priv->rps.rp1_freq);
+ 
+-	corr = corr * ((150142 * state1) / 10000 - 78642);
+-	corr /= 100000;
+-	corr2 = (corr * dev_priv->ips.corr);
++	dev_priv->rps.min_freq = valleyview_rps_min_freq(dev_priv);
++	DRM_DEBUG_DRIVER("min GPU freq: %d MHz (%u)\n",
++			 vlv_gpu_freq(dev_priv, dev_priv->rps.min_freq),
++			 dev_priv->rps.min_freq);
+ 
+-	state2 = (corr2 * state1) / 10000;
+-	state2 /= 100; /* convert to mW */
++	/* Preserve min/max settings in case of re-init */
++	if (dev_priv->rps.max_freq_softlimit == 0)
++		dev_priv->rps.max_freq_softlimit = dev_priv->rps.max_freq;
+ 
+-	__i915_update_gfx_val(dev_priv);
++	if (dev_priv->rps.min_freq_softlimit == 0)
++		dev_priv->rps.min_freq_softlimit = dev_priv->rps.min_freq;
+ 
+-	return dev_priv->ips.gfx_power + state2;
++	mutex_unlock(&dev_priv->rps.hw_lock);
+ }
+ 
+-unsigned long i915_gfx_val(struct drm_i915_private *dev_priv)
++static void cherryview_init_gt_powersave(struct drm_device *dev)
+ {
+-	struct drm_device *dev = dev_priv->dev;
+-	unsigned long val;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	u32 val;
+ 
+-	if (INTEL_INFO(dev)->gen != 5)
+-		return 0;
++	cherryview_setup_pctx(dev);
+ 
+-	spin_lock_irq(&mchdev_lock);
++	mutex_lock(&dev_priv->rps.hw_lock);
+ 
+-	val = __i915_gfx_val(dev_priv);
++	mutex_lock(&dev_priv->dpio_lock);
++	val = vlv_cck_read(dev_priv, CCK_FUSE_REG);
++	mutex_unlock(&dev_priv->dpio_lock);
+ 
+-	spin_unlock_irq(&mchdev_lock);
++	switch ((val >> 2) & 0x7) {
++	case 0:
++	case 1:
++		dev_priv->rps.cz_freq = 200;
++		dev_priv->mem_freq = 1600;
++		break;
++	case 2:
++		dev_priv->rps.cz_freq = 267;
++		dev_priv->mem_freq = 1600;
++		break;
++	case 3:
++		dev_priv->rps.cz_freq = 333;
++		dev_priv->mem_freq = 2000;
++		break;
++	case 4:
++		dev_priv->rps.cz_freq = 320;
++		dev_priv->mem_freq = 1600;
++		break;
++	case 5:
++		dev_priv->rps.cz_freq = 400;
++		dev_priv->mem_freq = 1600;
++		break;
++	}
++	DRM_DEBUG_DRIVER("DDR speed: %d MHz", dev_priv->mem_freq);
+ 
+-	return val;
+-}
++	dev_priv->rps.max_freq = cherryview_rps_max_freq(dev_priv);
++	dev_priv->rps.rp0_freq = dev_priv->rps.max_freq;
++	DRM_DEBUG_DRIVER("max GPU freq: %d MHz (%u)\n",
++			 vlv_gpu_freq(dev_priv, dev_priv->rps.max_freq),
++			 dev_priv->rps.max_freq);
+ 
+-/**
+- * i915_read_mch_val - return value for IPS use
+- *
+- * Calculate and return a value for the IPS driver to use when deciding whether
+- * we have thermal and power headroom to increase CPU or GPU power budget.
+- */
+-unsigned long i915_read_mch_val(void)
+-{
+-	struct drm_i915_private *dev_priv;
+-	unsigned long chipset_val, graphics_val, ret = 0;
++	dev_priv->rps.efficient_freq = cherryview_rps_rpe_freq(dev_priv);
++	DRM_DEBUG_DRIVER("RPe GPU freq: %d MHz (%u)\n",
++			 vlv_gpu_freq(dev_priv, dev_priv->rps.efficient_freq),
++			 dev_priv->rps.efficient_freq);
+ 
+-	spin_lock_irq(&mchdev_lock);
+-	if (!i915_mch_dev)
+-		goto out_unlock;
+-	dev_priv = i915_mch_dev;
++	dev_priv->rps.rp1_freq = cherryview_rps_guar_freq(dev_priv);
++	DRM_DEBUG_DRIVER("RP1(Guar) GPU freq: %d MHz (%u)\n",
++			 vlv_gpu_freq(dev_priv, dev_priv->rps.rp1_freq),
++			 dev_priv->rps.rp1_freq);
+ 
+-	chipset_val = __i915_chipset_val(dev_priv);
+-	graphics_val = __i915_gfx_val(dev_priv);
++	dev_priv->rps.min_freq = cherryview_rps_min_freq(dev_priv);
++	DRM_DEBUG_DRIVER("min GPU freq: %d MHz (%u)\n",
++			 vlv_gpu_freq(dev_priv, dev_priv->rps.min_freq),
++			 dev_priv->rps.min_freq);
+ 
+-	ret = chipset_val + graphics_val;
++	WARN_ONCE((dev_priv->rps.max_freq |
++		   dev_priv->rps.efficient_freq |
++		   dev_priv->rps.rp1_freq |
++		   dev_priv->rps.min_freq) & 1,
++		  "Odd GPU freq values\n");
+ 
+-out_unlock:
+-	spin_unlock_irq(&mchdev_lock);
++	/* Preserve min/max settings in case of re-init */
++	if (dev_priv->rps.max_freq_softlimit == 0)
++		dev_priv->rps.max_freq_softlimit = dev_priv->rps.max_freq;
+ 
+-	return ret;
++	if (dev_priv->rps.min_freq_softlimit == 0)
++		dev_priv->rps.min_freq_softlimit = dev_priv->rps.min_freq;
++
++	mutex_unlock(&dev_priv->rps.hw_lock);
+ }
+-EXPORT_SYMBOL_GPL(i915_read_mch_val);
+ 
+-/**
+- * i915_gpu_raise - raise GPU frequency limit
+- *
+- * Raise the limit; IPS indicates we have thermal headroom.
+- */
+-bool i915_gpu_raise(void)
++static void valleyview_cleanup_gt_powersave(struct drm_device *dev)
+ {
+-	struct drm_i915_private *dev_priv;
+-	bool ret = true;
++	valleyview_cleanup_pctx(dev);
++}
+ 
+-	spin_lock_irq(&mchdev_lock);
+-	if (!i915_mch_dev) {
+-		ret = false;
+-		goto out_unlock;
++static void cherryview_enable_rps(struct drm_device *dev)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_engine_cs *engine;
++	u32 gtfifodbg, val, rc6_mode = 0, pcbr;
++	int i;
++
++	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
++
++	gtfifodbg = I915_READ(GTFIFODBG);
++	if (gtfifodbg) {
++		DRM_DEBUG_DRIVER("GT fifo had a previous error %x\n",
++				 gtfifodbg);
++		I915_WRITE(GTFIFODBG, gtfifodbg);
+ 	}
+-	dev_priv = i915_mch_dev;
+ 
+-	if (dev_priv->ips.max_delay > dev_priv->ips.fmax)
+-		dev_priv->ips.max_delay--;
++	cherryview_check_pctx(dev_priv);
+ 
+-out_unlock:
+-	spin_unlock_irq(&mchdev_lock);
++	/* 1a & 1b: Get forcewake during program sequence. Although the driver
++	 * hasn't enabled a state yet where we need forcewake, BIOS may have.*/
++	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
+ 
+-	return ret;
+-}
+-EXPORT_SYMBOL_GPL(i915_gpu_raise);
++	/* 2a: Program RC6 thresholds.*/
++	I915_WRITE(GEN6_RC6_WAKE_RATE_LIMIT, 40 << 16);
++	I915_WRITE(GEN6_RC_EVALUATION_INTERVAL, 125000); /* 12500 * 1280ns */
++	I915_WRITE(GEN6_RC_IDLE_HYSTERSIS, 25); /* 25 * 1280ns */
+ 
+-/**
+- * i915_gpu_lower - lower GPU frequency limit
+- *
+- * IPS indicates we're close to a thermal limit, so throttle back the GPU
+- * frequency maximum.
+- */
+-bool i915_gpu_lower(void)
+-{
+-	struct drm_i915_private *dev_priv;
+-	bool ret = true;
++	for_each_engine(engine, dev_priv, i)
++		I915_WRITE(RING_MAX_IDLE(engine->mmio_base), 10);
++	I915_WRITE(GEN6_RC_SLEEP, 0);
+ 
+-	spin_lock_irq(&mchdev_lock);
+-	if (!i915_mch_dev) {
+-		ret = false;
+-		goto out_unlock;
+-	}
+-	dev_priv = i915_mch_dev;
++	I915_WRITE(GEN6_RC6_THRESHOLD, 50000); /* 50/125ms per EI */
+ 
+-	if (dev_priv->ips.max_delay < dev_priv->ips.min_delay)
+-		dev_priv->ips.max_delay++;
++	/* allows RC6 residency counter to work */
++	I915_WRITE(VLV_COUNTER_CONTROL,
++		   _MASKED_BIT_ENABLE(VLV_COUNT_RANGE_HIGH |
++				      VLV_MEDIA_RC6_COUNT_EN |
++				      VLV_RENDER_RC6_COUNT_EN));
+ 
+-out_unlock:
+-	spin_unlock_irq(&mchdev_lock);
++	/* For now we assume BIOS is allocating and populating the PCBR  */
++	pcbr = I915_READ(VLV_PCBR);
+ 
+-	return ret;
+-}
+-EXPORT_SYMBOL_GPL(i915_gpu_lower);
++	DRM_DEBUG_DRIVER("PCBR offset : 0x%x\n", pcbr);
+ 
+-/**
+- * i915_gpu_busy - indicate GPU business to IPS
+- *
+- * Tell the IPS driver whether or not the GPU is busy.
+- */
+-bool i915_gpu_busy(void)
+-{
+-	struct drm_i915_private *dev_priv;
+-	struct intel_engine_cs *ring;
+-	bool ret = false;
+-	int i;
++	/* 3: Enable RC6 */
++	if ((intel_enable_rc6(dev) & INTEL_RC6_ENABLE) &&
++						(pcbr >> VLV_PCBR_ADDR_SHIFT))
++		rc6_mode = GEN6_RC_CTL_EI_MODE(1);
+ 
+-	spin_lock_irq(&mchdev_lock);
+-	if (!i915_mch_dev)
+-		goto out_unlock;
+-	dev_priv = i915_mch_dev;
++	I915_WRITE(GEN6_RC_CONTROL, rc6_mode);
+ 
+-	for_each_ring(ring, dev_priv, i)
+-		ret |= !list_empty(&ring->request_list);
++	/* 4 Program defaults and thresholds for RPS*/
++	I915_WRITE(GEN6_RP_UP_THRESHOLD, 59400);
++	I915_WRITE(GEN6_RP_DOWN_THRESHOLD, 245000);
++	I915_WRITE(GEN6_RP_UP_EI, 66000);
++	I915_WRITE(GEN6_RP_DOWN_EI, 350000);
+ 
+-out_unlock:
+-	spin_unlock_irq(&mchdev_lock);
++	I915_WRITE(GEN6_RP_IDLE_HYSTERSIS, 10);
+ 
+-	return ret;
+-}
+-EXPORT_SYMBOL_GPL(i915_gpu_busy);
++	/* WaDisablePwrmtrEvent:chv (pre-production hw) */
++	I915_WRITE(0xA80C, I915_READ(0xA80C) & 0x00ffffff);
++	I915_WRITE(0xA810, I915_READ(0xA810) & 0xffffff00);
+ 
+-/**
+- * i915_gpu_turbo_disable - disable graphics turbo
+- *
+- * Disable graphics turbo by resetting the max frequency and setting the
+- * current frequency to the default.
+- */
+-bool i915_gpu_turbo_disable(void)
+-{
+-	struct drm_i915_private *dev_priv;
+-	bool ret = true;
++	/* 5: Enable RPS */
++	I915_WRITE(GEN6_RP_CONTROL,
++		   GEN6_RP_MEDIA_HW_NORMAL_MODE |
++		   GEN6_RP_MEDIA_IS_GFX | /* WaSetMaskForGfxBusyness:chv (pre-production hw ?) */
++		   GEN6_RP_ENABLE |
++		   GEN6_RP_UP_BUSY_AVG |
++		   GEN6_RP_DOWN_IDLE_AVG);
+ 
+-	spin_lock_irq(&mchdev_lock);
+-	if (!i915_mch_dev) {
+-		ret = false;
+-		goto out_unlock;
+-	}
+-	dev_priv = i915_mch_dev;
++	val = vlv_punit_read(dev_priv, PUNIT_REG_GPU_FREQ_STS);
+ 
+-	dev_priv->ips.max_delay = dev_priv->ips.fstart;
++	DRM_DEBUG_DRIVER("GPLL enabled? %s\n", val & 0x10 ? "yes" : "no");
++	DRM_DEBUG_DRIVER("GPU status: 0x%08x\n", val);
+ 
+-	if (!ironlake_set_drps(dev_priv->dev, dev_priv->ips.fstart))
+-		ret = false;
++	dev_priv->rps.cur_freq = (val >> 8) & 0xff;
++	DRM_DEBUG_DRIVER("current GPU freq: %d MHz (%u)\n",
++			 vlv_gpu_freq(dev_priv, dev_priv->rps.cur_freq),
++			 dev_priv->rps.cur_freq);
+ 
+-out_unlock:
+-	spin_unlock_irq(&mchdev_lock);
++	DRM_DEBUG_DRIVER("setting GPU freq to %d MHz (%u)\n",
++			 vlv_gpu_freq(dev_priv, dev_priv->rps.efficient_freq),
++			 dev_priv->rps.efficient_freq);
+ 
+-	return ret;
++	valleyview_set_rps(dev_priv->dev, dev_priv->rps.efficient_freq);
++
++	gen6_enable_rps_interrupts(dev);
++
++	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
+ }
+-EXPORT_SYMBOL_GPL(i915_gpu_turbo_disable);
+ 
+-/**
+- * Tells the intel_ips driver that the i915 driver is now loaded, if
+- * IPS got loaded first.
+- *
+- * This awkward dance is so that neither module has to depend on the
+- * other in order for IPS to do the appropriate communication of
+- * GPU turbo limits to i915.
+- */
+-static void
+-ips_ping_for_i915_load(void)
++static void valleyview_enable_rps(struct drm_device *dev)
+ {
+-	void (*link)(void);
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_engine_cs *engine;
++	u32 gtfifodbg, val, rc6_mode = 0;
++	int i;
+ 
+-	link = symbol_get(ips_link_to_i915_driver);
+-	if (link) {
+-		link();
+-		symbol_put(ips_link_to_i915_driver);
++	WARN_ON(!mutex_is_locked(&dev_priv->rps.hw_lock));
++
++	valleyview_check_pctx(dev_priv);
++
++	if ((gtfifodbg = I915_READ(GTFIFODBG))) {
++		DRM_DEBUG_DRIVER("GT fifo had a previous error %x\n",
++				 gtfifodbg);
++		I915_WRITE(GTFIFODBG, gtfifodbg);
+ 	}
+-}
+ 
+-void intel_gpu_ips_init(struct drm_i915_private *dev_priv)
+-{
+-	/* We only register the i915 ips part with intel-ips once everything is
+-	 * set up, to avoid intel-ips sneaking in and reading bogus values. */
+-	spin_lock_irq(&mchdev_lock);
+-	i915_mch_dev = dev_priv;
+-	spin_unlock_irq(&mchdev_lock);
++	/* If VLV, Forcewake all wells, else re-direct to regular path */
++	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
+ 
+-	ips_ping_for_i915_load();
+-}
++	I915_WRITE(GEN6_RP_UP_THRESHOLD, 59400);
++	I915_WRITE(GEN6_RP_DOWN_THRESHOLD, 245000);
++	I915_WRITE(GEN6_RP_UP_EI, 66000);
++	I915_WRITE(GEN6_RP_DOWN_EI, 350000);
+ 
+-void intel_gpu_ips_teardown(void)
+-{
+-	spin_lock_irq(&mchdev_lock);
+-	i915_mch_dev = NULL;
+-	spin_unlock_irq(&mchdev_lock);
+-}
++	I915_WRITE(GEN6_RP_IDLE_HYSTERSIS, 10);
++	I915_WRITE(GEN6_RP_DOWN_TIMEOUT, 0xf4240);
+ 
+-static void intel_init_emon(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	u32 lcfuse;
+-	u8 pxw[16];
+-	int i;
++	I915_WRITE(GEN6_RP_CONTROL,
++		   GEN6_RP_MEDIA_TURBO |
++		   GEN6_RP_MEDIA_HW_NORMAL_MODE |
++		   GEN6_RP_MEDIA_IS_GFX |
++		   GEN6_RP_ENABLE |
++		   GEN6_RP_UP_BUSY_AVG |
++		   GEN6_RP_DOWN_IDLE_CONT);
+ 
+-	/* Disable to program */
+-	I915_WRITE(ECR, 0);
+-	POSTING_READ(ECR);
++	I915_WRITE(GEN6_RC6_WAKE_RATE_LIMIT, 0x00280000);
++	I915_WRITE(GEN6_RC_EVALUATION_INTERVAL, 125000);
++	I915_WRITE(GEN6_RC_IDLE_HYSTERSIS, 25);
+ 
+-	/* Program energy weights for various events */
+-	I915_WRITE(SDEW, 0x15040d00);
+-	I915_WRITE(CSIEW0, 0x007f0000);
+-	I915_WRITE(CSIEW1, 0x1e220004);
+-	I915_WRITE(CSIEW2, 0x04000004);
++	for_each_engine(engine, dev_priv, i)
++		I915_WRITE(RING_MAX_IDLE(engine->mmio_base), 10);
+ 
+-	for (i = 0; i < 5; i++)
+-		I915_WRITE(PEW + (i * 4), 0);
+-	for (i = 0; i < 3; i++)
+-		I915_WRITE(DEW + (i * 4), 0);
++	I915_WRITE(GEN6_RC6_THRESHOLD, 0x557);
+ 
+-	/* Program P-state weights to account for frequency power adjustment */
+-	for (i = 0; i < 16; i++) {
+-		u32 pxvidfreq = I915_READ(PXVFREQ_BASE + (i * 4));
+-		unsigned long freq = intel_pxfreq(pxvidfreq);
+-		unsigned long vid = (pxvidfreq & PXVFREQ_PX_MASK) >>
+-			PXVFREQ_PX_SHIFT;
+-		unsigned long val;
++	/* allows RC6 residency counter to work */
++	I915_WRITE(VLV_COUNTER_CONTROL,
++		   _MASKED_BIT_ENABLE(VLV_MEDIA_RC0_COUNT_EN |
++				      VLV_RENDER_RC0_COUNT_EN |
++				      VLV_MEDIA_RC6_COUNT_EN |
++				      VLV_RENDER_RC6_COUNT_EN));
+ 
+-		val = vid * vid;
+-		val *= (freq / 1000);
+-		val *= 255;
+-		val /= (127*127*900);
+-		if (val > 0xff)
+-			DRM_ERROR("bad pxval: %ld\n", val);
+-		pxw[i] = val;
+-	}
+-	/* Render standby states get 0 weight */
+-	pxw[14] = 0;
+-	pxw[15] = 0;
++	if (intel_enable_rc6(dev) & INTEL_RC6_ENABLE)
++		rc6_mode = GEN7_RC_CTL_TO_MODE | VLV_RC_CTL_CTX_RST_PARALLEL;
+ 
+-	for (i = 0; i < 4; i++) {
+-		u32 val = (pxw[i*4] << 24) | (pxw[(i*4)+1] << 16) |
+-			(pxw[(i*4)+2] << 8) | (pxw[(i*4)+3]);
+-		I915_WRITE(PXW + (i * 4), val);
+-	}
++	intel_print_rc6_info(dev, rc6_mode);
+ 
+-	/* Adjust magic regs to magic values (more experimental results) */
+-	I915_WRITE(OGW0, 0);
+-	I915_WRITE(OGW1, 0);
+-	I915_WRITE(EG0, 0x00007f00);
+-	I915_WRITE(EG1, 0x0000000e);
+-	I915_WRITE(EG2, 0x000e0000);
+-	I915_WRITE(EG3, 0x68000300);
+-	I915_WRITE(EG4, 0x42000000);
+-	I915_WRITE(EG5, 0x00140031);
+-	I915_WRITE(EG6, 0);
+-	I915_WRITE(EG7, 0);
++	I915_WRITE(GEN6_RC_CONTROL, rc6_mode);
+ 
+-	for (i = 0; i < 8; i++)
+-		I915_WRITE(PXWL + (i * 4), 0);
++	val = vlv_punit_read(dev_priv, PUNIT_REG_GPU_FREQ_STS);
+ 
+-	/* Enable PMON + select events */
+-	I915_WRITE(ECR, 0x80000019);
++	DRM_DEBUG_DRIVER("GPLL enabled? %s\n", val & 0x10 ? "yes" : "no");
++	DRM_DEBUG_DRIVER("GPU status: 0x%08x\n", val);
+ 
+-	lcfuse = I915_READ(LCFUSE02);
++	dev_priv->rps.cur_freq = (val >> 8) & 0xff;
++	DRM_DEBUG_DRIVER("current GPU freq: %d MHz (%u)\n",
++			 vlv_gpu_freq(dev_priv, dev_priv->rps.cur_freq),
++			 dev_priv->rps.cur_freq);
+ 
+-	dev_priv->ips.corr = (lcfuse & LCFUSE_HIV_MASK);
+-}
++	DRM_DEBUG_DRIVER("setting GPU freq to %d MHz (%u)\n",
++			 vlv_gpu_freq(dev_priv, dev_priv->rps.efficient_freq),
++			 dev_priv->rps.efficient_freq);
+ 
+-void intel_init_gt_powersave(struct drm_device *dev)
+-{
+-	i915.enable_rc6 = sanitize_rc6_option(dev, i915.enable_rc6);
++	valleyview_set_rps(dev_priv->dev, dev_priv->rps.efficient_freq);
+ 
+-	if (IS_CHERRYVIEW(dev))
+-		cherryview_init_gt_powersave(dev);
+-	else if (IS_VALLEYVIEW(dev))
+-		valleyview_init_gt_powersave(dev);
++	gen6_enable_rps_interrupts(dev);
++
++	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
+ }
+ 
+-void intel_cleanup_gt_powersave(struct drm_device *dev)
++void ironlake_teardown_rc6(struct drm_device *dev)
+ {
+-	if (IS_CHERRYVIEW(dev))
+-		return;
+-	else if (IS_VALLEYVIEW(dev))
+-		valleyview_cleanup_gt_powersave(dev);
++	struct drm_i915_private *dev_priv = dev->dev_private;
++
++	if (dev_priv->ips.pwrctx) {
++		i915_gem_object_ggtt_unpin(dev_priv->ips.pwrctx);
++		drm_gem_object_unreference(&dev_priv->ips.pwrctx->base);
++		dev_priv->ips.pwrctx = NULL;
++	}
+ }
+ 
+-/**
+- * intel_suspend_gt_powersave - suspend PM work and helper threads
+- * @dev: drm device
+- *
+- * We don't want to disable RC6 or other features here, we just want
+- * to make sure any work we've queued has finished and won't bother
+- * us while we're suspended.
+- */
+-void intel_suspend_gt_powersave(struct drm_device *dev)
++static void ironlake_disable_rc6(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	/* Interrupts should be disabled already to avoid re-arming. */
+-	WARN_ON(intel_irqs_enabled(dev_priv));
+-
+-	flush_delayed_work(&dev_priv->rps.delayed_resume_work);
++	if (I915_READ(PWRCTXA)) {
++		/* Wake the GPU, prevent RC6, then restore RSTDBYCTL */
++		I915_WRITE(RSTDBYCTL, I915_READ(RSTDBYCTL) | RCX_SW_EXIT);
++		wait_for(((I915_READ(RSTDBYCTL) & RSX_STATUS_MASK) == RSX_STATUS_ON),
++			 50);
+ 
+-	cancel_work_sync(&dev_priv->rps.work);
++		I915_WRITE(PWRCTXA, 0);
++		POSTING_READ(PWRCTXA);
+ 
+-	/* Force GPU to min freq during suspend */
+-	gen6_rps_idle(dev_priv);
++		I915_WRITE(RSTDBYCTL, I915_READ(RSTDBYCTL) & ~RCX_SW_EXIT);
++		POSTING_READ(RSTDBYCTL);
++	}
+ }
+ 
+-void intel_disable_gt_powersave(struct drm_device *dev)
++static int ironlake_setup_rc6(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	/* Interrupts should be disabled already to avoid re-arming. */
+-	WARN_ON(intel_irqs_enabled(dev_priv));
++	if (dev_priv->ips.pwrctx == NULL)
++		dev_priv->ips.pwrctx = intel_alloc_context_page(dev);
++	if (!dev_priv->ips.pwrctx) {
++		ironlake_teardown_rc6(dev);
++		return -ENOMEM;
++	}
+ 
+-	if (IS_IRONLAKE_M(dev)) {
+-		ironlake_disable_drps(dev);
+-		ironlake_disable_rc6(dev);
+-	} else if (INTEL_INFO(dev)->gen >= 6) {
+-		intel_suspend_gt_powersave(dev);
+-
+-		mutex_lock(&dev_priv->rps.hw_lock);
+-		if (IS_CHERRYVIEW(dev))
+-			cherryview_disable_rps(dev);
+-		else if (IS_VALLEYVIEW(dev))
+-			valleyview_disable_rps(dev);
+-		else
+-			gen6_disable_rps(dev);
+-		dev_priv->rps.enabled = false;
+-		mutex_unlock(&dev_priv->rps.hw_lock);
+-	}
++	return 0;
+ }
+ 
+-static void intel_gen6_powersave_work(struct work_struct *work)
++static void ironlake_enable_rc6(struct drm_device *dev)
+ {
+-	struct drm_i915_private *dev_priv =
+-		container_of(work, struct drm_i915_private,
+-			     rps.delayed_resume_work.work);
+-	struct drm_device *dev = dev_priv->dev;
+-
+-	mutex_lock(&dev_priv->rps.hw_lock);
+-
+-	if (IS_CHERRYVIEW(dev)) {
+-		cherryview_enable_rps(dev);
+-	} else if (IS_VALLEYVIEW(dev)) {
+-		valleyview_enable_rps(dev);
+-	} else if (IS_BROADWELL(dev)) {
+-		gen8_enable_rps(dev);
+-		__gen6_update_ring_freq(dev);
+-	} else {
+-		gen6_enable_rps(dev);
+-		__gen6_update_ring_freq(dev);
+-	}
+-	dev_priv->rps.enabled = true;
+-	mutex_unlock(&dev_priv->rps.hw_lock);
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	intel_runtime_pm_put(dev_priv);
+-}
++	/* rc6 disabled by default due to repeated reports of hanging during
++	 * boot and resume.
++	 */
++	if (!intel_enable_rc6(dev))
++		return;
+ 
+-void intel_enable_gt_powersave(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	WARN_ON(!mutex_is_locked(&dev->struct_mutex));
+ 
+-	if (IS_IRONLAKE_M(dev)) {
+-		mutex_lock(&dev->struct_mutex);
+-		ironlake_enable_drps(dev);
+-		ironlake_enable_rc6(dev);
+-		intel_init_emon(dev);
+-		mutex_unlock(&dev->struct_mutex);
+-	} else if (INTEL_INFO(dev)->gen >= 6) {
+-		/*
+-		 * PCU communication is slow and this doesn't need to be
+-		 * done at any specific time, so do this out of our fast path
+-		 * to make resume and init faster.
+-		 *
+-		 * We depend on the HW RC6 power context save/restore
+-		 * mechanism when entering D3 through runtime PM suspend. So
+-		 * disable RPM until RPS/RC6 is properly setup. We can only
+-		 * get here via the driver load/system resume/runtime resume
+-		 * paths, so the _noresume version is enough (and in case of
+-		 * runtime resume it's necessary).
+-		 */
+-		if (schedule_delayed_work(&dev_priv->rps.delayed_resume_work,
+-					   round_jiffies_up_relative(HZ)))
+-			intel_runtime_pm_get_noresume(dev_priv);
++	if (ironlake_setup_rc6(dev)) {
++		DRM_ERROR("failed to enable ironlake power savings\n");
++		return;
+ 	}
+-}
+ 
+-void intel_reset_gt_powersave(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	I915_WRITE(PWRCTXA, i915_gem_obj_ggtt_offset(dev_priv->ips.pwrctx) | PWRCTX_EN);
++	I915_WRITE(RSTDBYCTL, I915_READ(RSTDBYCTL) & ~RCX_SW_EXIT);
+ 
+-	dev_priv->rps.enabled = false;
+-	intel_enable_gt_powersave(dev);
++	intel_print_rc6_info(dev, GEN6_RC_CTL_RC6_ENABLE);
+ }
+ 
+-static void ibx_init_clock_gating(struct drm_device *dev)
++static unsigned long intel_pxfreq(u32 vidfreq)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	unsigned long freq;
++	int div = (vidfreq & 0x3f0000) >> 16;
++	int post = (vidfreq & 0x3000) >> 12;
++	int pre = (vidfreq & 0x7);
+ 
+-	/*
+-	 * On Ibex Peak and Cougar Point, we need to disable clock
+-	 * gating for the panel power sequencer or it will fail to
+-	 * start up when no ports are active.
+-	 */
+-	I915_WRITE(SOUTH_DSPCLK_GATE_D, PCH_DPLSUNIT_CLOCK_GATE_DISABLE);
+-}
++	if (!pre)
++		return 0;
+ 
+-static void g4x_disable_trickle_feed(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	int pipe;
++	freq = ((div * 133333) / ((1<<post) * pre));
+ 
+-	for_each_pipe(pipe) {
+-		I915_WRITE(DSPCNTR(pipe),
+-			   I915_READ(DSPCNTR(pipe)) |
+-			   DISPPLANE_TRICKLE_FEED_DISABLE);
+-		intel_flush_primary_plane(dev_priv, pipe);
+-	}
++	return freq;
+ }
+ 
+-static void ilk_init_lp_watermarks(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-
+-	I915_WRITE(WM3_LP_ILK, I915_READ(WM3_LP_ILK) & ~WM1_LP_SR_EN);
+-	I915_WRITE(WM2_LP_ILK, I915_READ(WM2_LP_ILK) & ~WM1_LP_SR_EN);
+-	I915_WRITE(WM1_LP_ILK, I915_READ(WM1_LP_ILK) & ~WM1_LP_SR_EN);
+-
+-	/*
+-	 * Don't touch WM1S_LP_EN here.
+-	 * Doing so could cause underruns.
+-	 */
+-}
++static const struct cparams {
++	u16 i;
++	u16 t;
++	u16 m;
++	u16 c;
++} cparams[] = {
++	{ 1, 1333, 301, 28664 },
++	{ 1, 1066, 294, 24460 },
++	{ 1, 800, 294, 25192 },
++	{ 0, 1333, 276, 27605 },
++	{ 0, 1066, 276, 27605 },
++	{ 0, 800, 231, 23784 },
++};
+ 
+-static void ironlake_init_clock_gating(struct drm_device *dev)
++static unsigned long __i915_chipset_val(struct drm_i915_private *dev_priv)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	uint32_t dspclk_gate = ILK_VRHUNIT_CLOCK_GATE_DISABLE;
++	u64 total_count, diff, ret;
++	u32 count1, count2, count3, m = 0, c = 0;
++	unsigned long now = jiffies_to_msecs(jiffies), diff1;
++	int i;
+ 
+-	/*
+-	 * Required for FBC
+-	 * WaFbcDisableDpfcClockGating:ilk
+-	 */
+-	dspclk_gate |= ILK_DPFCRUNIT_CLOCK_GATE_DISABLE |
+-		   ILK_DPFCUNIT_CLOCK_GATE_DISABLE |
+-		   ILK_DPFDUNIT_CLOCK_GATE_ENABLE;
++	assert_spin_locked(&mchdev_lock);
+ 
+-	I915_WRITE(PCH_3DCGDIS0,
+-		   MARIUNIT_CLOCK_GATE_DISABLE |
+-		   SVSMUNIT_CLOCK_GATE_DISABLE);
+-	I915_WRITE(PCH_3DCGDIS1,
+-		   VFMUNIT_CLOCK_GATE_DISABLE);
++	diff1 = now - dev_priv->ips.last_time1;
+ 
+-	/*
+-	 * According to the spec the following bits should be set in
+-	 * order to enable memory self-refresh
+-	 * The bit 22/21 of 0x42004
+-	 * The bit 5 of 0x42020
+-	 * The bit 15 of 0x45000
++	/* Prevent division-by-zero if we are asking too fast.
++	 * Also, we don't get interesting results if we are polling
++	 * faster than once in 10ms, so just return the saved value
++	 * in such cases.
+ 	 */
+-	I915_WRITE(ILK_DISPLAY_CHICKEN2,
+-		   (I915_READ(ILK_DISPLAY_CHICKEN2) |
+-		    ILK_DPARB_GATE | ILK_VSDPFD_FULL));
+-	dspclk_gate |= ILK_DPARBUNIT_CLOCK_GATE_ENABLE;
+-	I915_WRITE(DISP_ARB_CTL,
+-		   (I915_READ(DISP_ARB_CTL) |
+-		    DISP_FBC_WM_DIS));
++	if (diff1 <= 10)
++		return dev_priv->ips.chipset_power;
+ 
+-	ilk_init_lp_watermarks(dev);
++	count1 = I915_READ(DMIEC);
++	count2 = I915_READ(DDREC);
++	count3 = I915_READ(CSIEC);
+ 
+-	/*
+-	 * Based on the document from hardware guys the following bits
+-	 * should be set unconditionally in order to enable FBC.
+-	 * The bit 22 of 0x42000
+-	 * The bit 22 of 0x42004
+-	 * The bit 7,8,9 of 0x42020.
+-	 */
+-	if (IS_IRONLAKE_M(dev)) {
+-		/* WaFbcAsynchFlipDisableFbcQueue:ilk */
+-		I915_WRITE(ILK_DISPLAY_CHICKEN1,
+-			   I915_READ(ILK_DISPLAY_CHICKEN1) |
+-			   ILK_FBCQ_DIS);
+-		I915_WRITE(ILK_DISPLAY_CHICKEN2,
+-			   I915_READ(ILK_DISPLAY_CHICKEN2) |
+-			   ILK_DPARB_GATE);
+-	}
++	total_count = count1 + count2 + count3;
+ 
+-	I915_WRITE(ILK_DSPCLK_GATE_D, dspclk_gate);
++	/* FIXME: handle per-counter overflow */
++	if (total_count < dev_priv->ips.last_count1) {
++		diff = ~0UL - dev_priv->ips.last_count1;
++		diff += total_count;
++	} else {
++		diff = total_count - dev_priv->ips.last_count1;
++	}
+ 
+-	I915_WRITE(ILK_DISPLAY_CHICKEN2,
+-		   I915_READ(ILK_DISPLAY_CHICKEN2) |
+-		   ILK_ELPIN_409_SELECT);
+-	I915_WRITE(_3D_CHICKEN2,
+-		   _3D_CHICKEN2_WM_READ_PIPELINED << 16 |
+-		   _3D_CHICKEN2_WM_READ_PIPELINED);
++	for (i = 0; i < ARRAY_SIZE(cparams); i++) {
++		if (cparams[i].i == dev_priv->ips.c_m &&
++		    cparams[i].t == dev_priv->ips.r_t) {
++			m = cparams[i].m;
++			c = cparams[i].c;
++			break;
++		}
++	}
+ 
+-	/* WaDisableRenderCachePipelinedFlush:ilk */
+-	I915_WRITE(CACHE_MODE_0,
+-		   _MASKED_BIT_ENABLE(CM0_PIPELINED_RENDER_FLUSH_DISABLE));
++	diff = div_u64(diff, diff1);
++	ret = ((m * diff) + c);
++	ret = div_u64(ret, 10);
+ 
+-	/* WaDisable_RenderCache_OperationalFlush:ilk */
+-	I915_WRITE(CACHE_MODE_0, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
++	dev_priv->ips.last_count1 = total_count;
++	dev_priv->ips.last_time1 = now;
+ 
+-	g4x_disable_trickle_feed(dev);
++	dev_priv->ips.chipset_power = ret;
+ 
+-	ibx_init_clock_gating(dev);
++	return ret;
+ }
+ 
+-static void cpt_init_clock_gating(struct drm_device *dev)
++unsigned long i915_chipset_val(struct drm_i915_private *dev_priv)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	int pipe;
+-	uint32_t val;
+-
+-	/*
+-	 * On Ibex Peak and Cougar Point, we need to disable clock
+-	 * gating for the panel power sequencer or it will fail to
+-	 * start up when no ports are active.
+-	 */
+-	I915_WRITE(SOUTH_DSPCLK_GATE_D, PCH_DPLSUNIT_CLOCK_GATE_DISABLE |
+-		   PCH_DPLUNIT_CLOCK_GATE_DISABLE |
+-		   PCH_CPUNIT_CLOCK_GATE_DISABLE);
+-	I915_WRITE(SOUTH_CHICKEN2, I915_READ(SOUTH_CHICKEN2) |
+-		   DPLS_EDP_PPS_FIX_DIS);
+-	/* The below fixes the weird display corruption, a few pixels shifted
+-	 * downward, on (only) LVDS of some HP laptops with IVY.
+-	 */
+-	for_each_pipe(pipe) {
+-		val = I915_READ(TRANS_CHICKEN2(pipe));
+-		val |= TRANS_CHICKEN2_TIMING_OVERRIDE;
+-		val &= ~TRANS_CHICKEN2_FDI_POLARITY_REVERSED;
+-		if (dev_priv->vbt.fdi_rx_polarity_inverted)
+-			val |= TRANS_CHICKEN2_FDI_POLARITY_REVERSED;
+-		val &= ~TRANS_CHICKEN2_FRAME_START_DELAY_MASK;
+-		val &= ~TRANS_CHICKEN2_DISABLE_DEEP_COLOR_COUNTER;
+-		val &= ~TRANS_CHICKEN2_DISABLE_DEEP_COLOR_MODESWITCH;
+-		I915_WRITE(TRANS_CHICKEN2(pipe), val);
+-	}
+-	/* WADP0ClockGatingDisable */
+-	for_each_pipe(pipe) {
+-		I915_WRITE(TRANS_CHICKEN1(pipe),
+-			   TRANS_CHICKEN1_DP0UNIT_GC_DISABLE);
+-	}
+-}
++	struct drm_device *dev = dev_priv->dev;
++	unsigned long val;
+ 
+-static void gen6_check_mch_setup(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	uint32_t tmp;
++	if (INTEL_INFO(dev)->gen != 5)
++		return 0;
+ 
+-	tmp = I915_READ(MCH_SSKPD);
+-	if ((tmp & MCH_SSKPD_WM0_MASK) != MCH_SSKPD_WM0_VAL)
+-		DRM_DEBUG_KMS("Wrong MCH_SSKPD value: 0x%08x This can cause underruns.\n",
+-			      tmp);
+-}
++	spin_lock_irq(&mchdev_lock);
+ 
+-static void gen6_init_clock_gating(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	uint32_t dspclk_gate = ILK_VRHUNIT_CLOCK_GATE_DISABLE;
++	val = __i915_chipset_val(dev_priv);
+ 
+-	I915_WRITE(ILK_DSPCLK_GATE_D, dspclk_gate);
++	spin_unlock_irq(&mchdev_lock);
+ 
+-	I915_WRITE(ILK_DISPLAY_CHICKEN2,
+-		   I915_READ(ILK_DISPLAY_CHICKEN2) |
+-		   ILK_ELPIN_409_SELECT);
++	return val;
++}
+ 
+-	/* WaDisableHiZPlanesWhenMSAAEnabled:snb */
+-	I915_WRITE(_3D_CHICKEN,
+-		   _MASKED_BIT_ENABLE(_3D_CHICKEN_HIZ_PLANE_DISABLE_MSAA_4X_SNB));
++unsigned long i915_mch_val(struct drm_i915_private *dev_priv)
++{
++	unsigned long m, x, b;
++	u32 tsfs;
+ 
+-	/* WaSetupGtModeTdRowDispatch:snb */
+-	if (IS_SNB_GT1(dev))
+-		I915_WRITE(GEN6_GT_MODE,
+-			   _MASKED_BIT_ENABLE(GEN6_TD_FOUR_ROW_DISPATCH_DISABLE));
++	tsfs = I915_READ(TSFS);
+ 
+-	/* WaDisable_RenderCache_OperationalFlush:snb */
+-	I915_WRITE(CACHE_MODE_0, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
++	m = ((tsfs & TSFS_SLOPE_MASK) >> TSFS_SLOPE_SHIFT);
++	x = I915_READ8(TR1);
+ 
+-	/*
+-	 * BSpec recoomends 8x4 when MSAA is used,
+-	 * however in practice 16x4 seems fastest.
+-	 *
+-	 * Note that PS/WM thread counts depend on the WIZ hashing
+-	 * disable bit, which we don't touch here, but it's good
+-	 * to keep in mind (see 3DSTATE_PS and 3DSTATE_WM).
+-	 */
+-	I915_WRITE(GEN6_GT_MODE,
+-		   GEN6_WIZ_HASHING_MASK | GEN6_WIZ_HASHING_16x4);
++	b = tsfs & TSFS_INTR_MASK;
+ 
+-	ilk_init_lp_watermarks(dev);
++	return ((m * x) / 127) - b;
++}
+ 
+-	I915_WRITE(CACHE_MODE_0,
+-		   _MASKED_BIT_DISABLE(CM0_STC_EVICT_DISABLE_LRA_SNB));
++static u16 pvid_to_extvid(struct drm_i915_private *dev_priv, u8 pxvid)
++{
++	struct drm_device *dev = dev_priv->dev;
++	static const struct v_table {
++		u16 vd; /* in .1 mil */
++		u16 vm; /* in .1 mil */
++	} v_table[] = {
++		{ 0, 0, },
++		{ 375, 0, },
++		{ 500, 0, },
++		{ 625, 0, },
++		{ 750, 0, },
++		{ 875, 0, },
++		{ 1000, 0, },
++		{ 1125, 0, },
++		{ 4125, 3000, },
++		{ 4125, 3000, },
++		{ 4125, 3000, },
++		{ 4125, 3000, },
++		{ 4125, 3000, },
++		{ 4125, 3000, },
++		{ 4125, 3000, },
++		{ 4125, 3000, },
++		{ 4125, 3000, },
++		{ 4125, 3000, },
++		{ 4125, 3000, },
++		{ 4125, 3000, },
++		{ 4125, 3000, },
++		{ 4125, 3000, },
++		{ 4125, 3000, },
++		{ 4125, 3000, },
++		{ 4125, 3000, },
++		{ 4125, 3000, },
++		{ 4125, 3000, },
++		{ 4125, 3000, },
++		{ 4125, 3000, },
++		{ 4125, 3000, },
++		{ 4125, 3000, },
++		{ 4125, 3000, },
++		{ 4250, 3125, },
++		{ 4375, 3250, },
++		{ 4500, 3375, },
++		{ 4625, 3500, },
++		{ 4750, 3625, },
++		{ 4875, 3750, },
++		{ 5000, 3875, },
++		{ 5125, 4000, },
++		{ 5250, 4125, },
++		{ 5375, 4250, },
++		{ 5500, 4375, },
++		{ 5625, 4500, },
++		{ 5750, 4625, },
++		{ 5875, 4750, },
++		{ 6000, 4875, },
++		{ 6125, 5000, },
++		{ 6250, 5125, },
++		{ 6375, 5250, },
++		{ 6500, 5375, },
++		{ 6625, 5500, },
++		{ 6750, 5625, },
++		{ 6875, 5750, },
++		{ 7000, 5875, },
++		{ 7125, 6000, },
++		{ 7250, 6125, },
++		{ 7375, 6250, },
++		{ 7500, 6375, },
++		{ 7625, 6500, },
++		{ 7750, 6625, },
++		{ 7875, 6750, },
++		{ 8000, 6875, },
++		{ 8125, 7000, },
++		{ 8250, 7125, },
++		{ 8375, 7250, },
++		{ 8500, 7375, },
++		{ 8625, 7500, },
++		{ 8750, 7625, },
++		{ 8875, 7750, },
++		{ 9000, 7875, },
++		{ 9125, 8000, },
++		{ 9250, 8125, },
++		{ 9375, 8250, },
++		{ 9500, 8375, },
++		{ 9625, 8500, },
++		{ 9750, 8625, },
++		{ 9875, 8750, },
++		{ 10000, 8875, },
++		{ 10125, 9000, },
++		{ 10250, 9125, },
++		{ 10375, 9250, },
++		{ 10500, 9375, },
++		{ 10625, 9500, },
++		{ 10750, 9625, },
++		{ 10875, 9750, },
++		{ 11000, 9875, },
++		{ 11125, 10000, },
++		{ 11250, 10125, },
++		{ 11375, 10250, },
++		{ 11500, 10375, },
++		{ 11625, 10500, },
++		{ 11750, 10625, },
++		{ 11875, 10750, },
++		{ 12000, 10875, },
++		{ 12125, 11000, },
++		{ 12250, 11125, },
++		{ 12375, 11250, },
++		{ 12500, 11375, },
++		{ 12625, 11500, },
++		{ 12750, 11625, },
++		{ 12875, 11750, },
++		{ 13000, 11875, },
++		{ 13125, 12000, },
++		{ 13250, 12125, },
++		{ 13375, 12250, },
++		{ 13500, 12375, },
++		{ 13625, 12500, },
++		{ 13750, 12625, },
++		{ 13875, 12750, },
++		{ 14000, 12875, },
++		{ 14125, 13000, },
++		{ 14250, 13125, },
++		{ 14375, 13250, },
++		{ 14500, 13375, },
++		{ 14625, 13500, },
++		{ 14750, 13625, },
++		{ 14875, 13750, },
++		{ 15000, 13875, },
++		{ 15125, 14000, },
++		{ 15250, 14125, },
++		{ 15375, 14250, },
++		{ 15500, 14375, },
++		{ 15625, 14500, },
++		{ 15750, 14625, },
++		{ 15875, 14750, },
++		{ 16000, 14875, },
++		{ 16125, 15000, },
++	};
++	if (INTEL_INFO(dev)->is_mobile)
++		return v_table[pxvid].vm;
++	else
++		return v_table[pxvid].vd;
++}
+ 
+-	I915_WRITE(GEN6_UCGCTL1,
+-		   I915_READ(GEN6_UCGCTL1) |
+-		   GEN6_BLBUNIT_CLOCK_GATE_DISABLE |
+-		   GEN6_CSUNIT_CLOCK_GATE_DISABLE);
++static void __i915_update_gfx_val(struct drm_i915_private *dev_priv)
++{
++	u64 now, diff, diffms;
++	u32 count;
+ 
+-	/* According to the BSpec vol1g, bit 12 (RCPBUNIT) clock
+-	 * gating disable must be set.  Failure to set it results in
+-	 * flickering pixels due to Z write ordering failures after
+-	 * some amount of runtime in the Mesa "fire" demo, and Unigine
+-	 * Sanctuary and Tropics, and apparently anything else with
+-	 * alpha test or pixel discard.
+-	 *
+-	 * According to the spec, bit 11 (RCCUNIT) must also be set,
+-	 * but we didn't debug actual testcases to find it out.
+-	 *
+-	 * WaDisableRCCUnitClockGating:snb
+-	 * WaDisableRCPBUnitClockGating:snb
+-	 */
+-	I915_WRITE(GEN6_UCGCTL2,
+-		   GEN6_RCPBUNIT_CLOCK_GATE_DISABLE |
+-		   GEN6_RCCUNIT_CLOCK_GATE_DISABLE);
++	assert_spin_locked(&mchdev_lock);
+ 
+-	/* WaStripsFansDisableFastClipPerformanceFix:snb */
+-	I915_WRITE(_3D_CHICKEN3,
+-		   _MASKED_BIT_ENABLE(_3D_CHICKEN3_SF_DISABLE_FASTCLIP_CULL));
++	now = ktime_get_raw_ns();
++	diffms = now - dev_priv->ips.last_time2;
++	do_div(diffms, NSEC_PER_MSEC);
+ 
+-	/*
+-	 * Bspec says:
+-	 * "This bit must be set if 3DSTATE_CLIP clip mode is set to normal and
+-	 * 3DSTATE_SF number of SF output attributes is more than 16."
+-	 */
+-	I915_WRITE(_3D_CHICKEN3,
+-		   _MASKED_BIT_ENABLE(_3D_CHICKEN3_SF_DISABLE_PIPELINED_ATTR_FETCH));
++	/* Don't divide by 0 */
++	if (!diffms)
++		return;
+ 
+-	/*
+-	 * According to the spec the following bits should be
+-	 * set in order to enable memory self-refresh and fbc:
+-	 * The bit21 and bit22 of 0x42000
+-	 * The bit21 and bit22 of 0x42004
+-	 * The bit5 and bit7 of 0x42020
+-	 * The bit14 of 0x70180
+-	 * The bit14 of 0x71180
+-	 *
+-	 * WaFbcAsynchFlipDisableFbcQueue:snb
+-	 */
+-	I915_WRITE(ILK_DISPLAY_CHICKEN1,
+-		   I915_READ(ILK_DISPLAY_CHICKEN1) |
+-		   ILK_FBCQ_DIS | ILK_PABSTRETCH_DIS);
+-	I915_WRITE(ILK_DISPLAY_CHICKEN2,
+-		   I915_READ(ILK_DISPLAY_CHICKEN2) |
+-		   ILK_DPARB_GATE | ILK_VSDPFD_FULL);
+-	I915_WRITE(ILK_DSPCLK_GATE_D,
+-		   I915_READ(ILK_DSPCLK_GATE_D) |
+-		   ILK_DPARBUNIT_CLOCK_GATE_ENABLE  |
+-		   ILK_DPFDUNIT_CLOCK_GATE_ENABLE);
++	count = I915_READ(GFXEC);
+ 
+-	g4x_disable_trickle_feed(dev);
++	if (count < dev_priv->ips.last_count2) {
++		diff = ~0UL - dev_priv->ips.last_count2;
++		diff += count;
++	} else {
++		diff = count - dev_priv->ips.last_count2;
++	}
+ 
+-	cpt_init_clock_gating(dev);
++	dev_priv->ips.last_count2 = count;
++	dev_priv->ips.last_time2 = now;
+ 
+-	gen6_check_mch_setup(dev);
++	/* More magic constants... */
++	diff = diff * 1181;
++	diff = div_u64(diff, diffms * 10);
++	dev_priv->ips.gfx_power = diff;
+ }
+ 
+-static void gen7_setup_fixed_func_scheduler(struct drm_i915_private *dev_priv)
++void i915_update_gfx_val(struct drm_i915_private *dev_priv)
+ {
+-	uint32_t reg = I915_READ(GEN7_FF_THREAD_MODE);
+-
+-	/*
+-	 * WaVSThreadDispatchOverride:ivb,vlv
+-	 *
+-	 * This actually overrides the dispatch
+-	 * mode for all thread types.
+-	 */
+-	reg &= ~GEN7_FF_SCHED_MASK;
+-	reg |= GEN7_FF_TS_SCHED_HW;
+-	reg |= GEN7_FF_VS_SCHED_HW;
+-	reg |= GEN7_FF_DS_SCHED_HW;
++	struct drm_device *dev = dev_priv->dev;
+ 
+-	I915_WRITE(GEN7_FF_THREAD_MODE, reg);
+-}
++	if (INTEL_INFO(dev)->gen != 5)
++		return;
+ 
+-static void lpt_init_clock_gating(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	spin_lock_irq(&mchdev_lock);
+ 
+-	/*
+-	 * TODO: this bit should only be enabled when really needed, then
+-	 * disabled when not needed anymore in order to save power.
+-	 */
+-	if (dev_priv->pch_id == INTEL_PCH_LPT_LP_DEVICE_ID_TYPE)
+-		I915_WRITE(SOUTH_DSPCLK_GATE_D,
+-			   I915_READ(SOUTH_DSPCLK_GATE_D) |
+-			   PCH_LP_PARTITION_LEVEL_DISABLE);
++	__i915_update_gfx_val(dev_priv);
+ 
+-	/* WADPOClockGatingDisable:hsw */
+-	I915_WRITE(_TRANSA_CHICKEN1,
+-		   I915_READ(_TRANSA_CHICKEN1) |
+-		   TRANS_CHICKEN1_DP0UNIT_GC_DISABLE);
++	spin_unlock_irq(&mchdev_lock);
+ }
+ 
+-static void lpt_suspend_hw(struct drm_device *dev)
++static unsigned long __i915_gfx_val(struct drm_i915_private *dev_priv)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	unsigned long t, corr, state1, corr2, state2;
++	u32 pxvid, ext_v;
+ 
+-	if (dev_priv->pch_id == INTEL_PCH_LPT_LP_DEVICE_ID_TYPE) {
+-		uint32_t val = I915_READ(SOUTH_DSPCLK_GATE_D);
++	assert_spin_locked(&mchdev_lock);
+ 
+-		val &= ~PCH_LP_PARTITION_LEVEL_DISABLE;
+-		I915_WRITE(SOUTH_DSPCLK_GATE_D, val);
+-	}
+-}
++	pxvid = I915_READ(PXVFREQ_BASE + (dev_priv->rps.cur_freq * 4));
++	pxvid = (pxvid >> 24) & 0x7f;
++	ext_v = pvid_to_extvid(dev_priv, pxvid);
+ 
+-static void gen8_init_clock_gating(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	enum pipe pipe;
++	state1 = ext_v;
+ 
+-	I915_WRITE(WM3_LP_ILK, 0);
+-	I915_WRITE(WM2_LP_ILK, 0);
+-	I915_WRITE(WM1_LP_ILK, 0);
++	t = i915_mch_val(dev_priv);
+ 
+-	/* FIXME(BDW): Check all the w/a, some might only apply to
+-	 * pre-production hw. */
++	/* Revel in the empirically derived constants */
+ 
+-	/* WaDisablePartialInstShootdown:bdw */
+-	I915_WRITE(GEN8_ROW_CHICKEN,
+-		   _MASKED_BIT_ENABLE(PARTIAL_INSTRUCTION_SHOOTDOWN_DISABLE));
+-
+-	/* WaDisableThreadStallDopClockGating:bdw */
+-	/* FIXME: Unclear whether we really need this on production bdw. */
+-	I915_WRITE(GEN8_ROW_CHICKEN,
+-		   _MASKED_BIT_ENABLE(STALL_DOP_GATING_DISABLE));
++	/* Correction factor in 1/100000 units */
++	if (t > 80)
++		corr = ((t * 2349) + 135940);
++	else if (t >= 50)
++		corr = ((t * 964) + 29317);
++	else /* < 50 */
++		corr = ((t * 301) + 1004);
+ 
+-	/*
+-	 * This GEN8_CENTROID_PIXEL_OPT_DIS W/A is only needed for
+-	 * pre-production hardware
+-	 */
+-	I915_WRITE(HALF_SLICE_CHICKEN3,
+-		   _MASKED_BIT_ENABLE(GEN8_CENTROID_PIXEL_OPT_DIS));
+-	I915_WRITE(HALF_SLICE_CHICKEN3,
+-		   _MASKED_BIT_ENABLE(GEN8_SAMPLER_POWER_BYPASS_DIS));
+-	I915_WRITE(GAMTARBMODE, _MASKED_BIT_ENABLE(ARB_MODE_BWGTLB_DISABLE));
++	corr = corr * ((150142 * state1) / 10000 - 78642);
++	corr /= 100000;
++	corr2 = (corr * dev_priv->ips.corr);
+ 
+-	I915_WRITE(_3D_CHICKEN3,
+-		   _MASKED_BIT_ENABLE(_3D_CHICKEN_SDE_LIMIT_FIFO_POLY_DEPTH(2)));
++	state2 = (corr2 * state1) / 10000;
++	state2 /= 100; /* convert to mW */
+ 
+-	I915_WRITE(COMMON_SLICE_CHICKEN2,
+-		   _MASKED_BIT_ENABLE(GEN8_CSC2_SBE_VUE_CACHE_CONSERVATIVE));
++	__i915_update_gfx_val(dev_priv);
+ 
+-	I915_WRITE(GEN7_HALF_SLICE_CHICKEN1,
+-		   _MASKED_BIT_ENABLE(GEN7_SINGLE_SUBSCAN_DISPATCH_ENABLE));
++	return dev_priv->ips.gfx_power + state2;
++}
+ 
+-	/* WaDisableDopClockGating:bdw May not be needed for production */
+-	I915_WRITE(GEN7_ROW_CHICKEN2,
+-		   _MASKED_BIT_ENABLE(DOP_CLOCK_GATING_DISABLE));
++unsigned long i915_gfx_val(struct drm_i915_private *dev_priv)
++{
++	struct drm_device *dev = dev_priv->dev;
++	unsigned long val;
+ 
+-	/* WaSwitchSolVfFArbitrationPriority:bdw */
+-	I915_WRITE(GAM_ECOCHK, I915_READ(GAM_ECOCHK) | HSW_ECOCHK_ARB_PRIO_SOL);
++	if (INTEL_INFO(dev)->gen != 5)
++		return 0;
+ 
+-	/* WaPsrDPAMaskVBlankInSRD:bdw */
+-	I915_WRITE(CHICKEN_PAR1_1,
+-		   I915_READ(CHICKEN_PAR1_1) | DPA_MASK_VBLANK_SRD);
++	spin_lock_irq(&mchdev_lock);
+ 
+-	/* WaPsrDPRSUnmaskVBlankInSRD:bdw */
+-	for_each_pipe(pipe) {
+-		I915_WRITE(CHICKEN_PIPESL_1(pipe),
+-			   I915_READ(CHICKEN_PIPESL_1(pipe)) |
+-			   BDW_DPRS_MASK_VBLANK_SRD);
+-	}
++	val = __i915_gfx_val(dev_priv);
+ 
+-	/* Use Force Non-Coherent whenever executing a 3D context. This is a
+-	 * workaround for for a possible hang in the unlikely event a TLB
+-	 * invalidation occurs during a PSD flush.
+-	 */
+-	I915_WRITE(HDC_CHICKEN0,
+-		   I915_READ(HDC_CHICKEN0) |
+-		   _MASKED_BIT_ENABLE(HDC_FORCE_NON_COHERENT));
++	spin_unlock_irq(&mchdev_lock);
+ 
+-	/* WaVSRefCountFullforceMissDisable:bdw */
+-	/* WaDSRefCountFullforceMissDisable:bdw */
+-	I915_WRITE(GEN7_FF_THREAD_MODE,
+-		   I915_READ(GEN7_FF_THREAD_MODE) &
+-		   ~(GEN8_FF_DS_REF_CNT_FFME | GEN7_FF_VS_REF_CNT_FFME));
++	return val;
++}
+ 
+-	/*
+-	 * BSpec recommends 8x4 when MSAA is used,
+-	 * however in practice 16x4 seems fastest.
+-	 *
+-	 * Note that PS/WM thread counts depend on the WIZ hashing
+-	 * disable bit, which we don't touch here, but it's good
+-	 * to keep in mind (see 3DSTATE_PS and 3DSTATE_WM).
+-	 */
+-	I915_WRITE(GEN7_GT_MODE,
+-		   GEN6_WIZ_HASHING_MASK | GEN6_WIZ_HASHING_16x4);
++/**
++ * i915_read_mch_val - return value for IPS use
++ *
++ * Calculate and return a value for the IPS driver to use when deciding whether
++ * we have thermal and power headroom to increase CPU or GPU power budget.
++ */
++unsigned long i915_read_mch_val(void)
++{
++	struct drm_i915_private *dev_priv;
++	unsigned long chipset_val, graphics_val, ret = 0;
+ 
+-	I915_WRITE(GEN6_RC_SLEEP_PSMI_CONTROL,
+-		   _MASKED_BIT_ENABLE(GEN8_RC_SEMA_IDLE_MSG_DISABLE));
++	spin_lock_irq(&mchdev_lock);
++	if (!i915_mch_dev)
++		goto out_unlock;
++	dev_priv = i915_mch_dev;
+ 
+-	/* WaDisableSDEUnitClockGating:bdw */
+-	I915_WRITE(GEN8_UCGCTL6, I915_READ(GEN8_UCGCTL6) |
+-		   GEN8_SDEUNIT_CLOCK_GATE_DISABLE);
++	chipset_val = __i915_chipset_val(dev_priv);
++	graphics_val = __i915_gfx_val(dev_priv);
+ 
+-	/* Wa4x4STCOptimizationDisable:bdw */
+-	I915_WRITE(CACHE_MODE_1,
+-		   _MASKED_BIT_ENABLE(GEN8_4x4_STC_OPTIMIZATION_DISABLE));
+-}
++	ret = chipset_val + graphics_val;
+ 
+-static void haswell_init_clock_gating(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++out_unlock:
++	spin_unlock_irq(&mchdev_lock);
+ 
+-	ilk_init_lp_watermarks(dev);
++	return ret;
++}
++EXPORT_SYMBOL_GPL(i915_read_mch_val);
+ 
+-	/* L3 caching of data atomics doesn't work -- disable it. */
+-	I915_WRITE(HSW_SCRATCH1, HSW_SCRATCH1_L3_DATA_ATOMICS_DISABLE);
+-	I915_WRITE(HSW_ROW_CHICKEN3,
+-		   _MASKED_BIT_ENABLE(HSW_ROW_CHICKEN3_L3_GLOBAL_ATOMICS_DISABLE));
++/**
++ * i915_gpu_raise - raise GPU frequency limit
++ *
++ * Raise the limit; IPS indicates we have thermal headroom.
++ */
++bool i915_gpu_raise(void)
++{
++	struct drm_i915_private *dev_priv;
++	bool ret = true;
+ 
+-	/* This is required by WaCatErrorRejectionIssue:hsw */
+-	I915_WRITE(GEN7_SQ_CHICKEN_MBCUNIT_CONFIG,
+-			I915_READ(GEN7_SQ_CHICKEN_MBCUNIT_CONFIG) |
+-			GEN7_SQ_CHICKEN_MBCUNIT_SQINTMOB);
++	spin_lock_irq(&mchdev_lock);
++	if (!i915_mch_dev) {
++		ret = false;
++		goto out_unlock;
++	}
++	dev_priv = i915_mch_dev;
+ 
+-	/* WaVSRefCountFullforceMissDisable:hsw */
+-	I915_WRITE(GEN7_FF_THREAD_MODE,
+-		   I915_READ(GEN7_FF_THREAD_MODE) & ~GEN7_FF_VS_REF_CNT_FFME);
++	if (dev_priv->ips.max_delay > dev_priv->ips.fmax)
++		dev_priv->ips.max_delay--;
+ 
+-	/* WaDisable_RenderCache_OperationalFlush:hsw */
+-	I915_WRITE(CACHE_MODE_0_GEN7, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
++out_unlock:
++	spin_unlock_irq(&mchdev_lock);
+ 
+-	/* enable HiZ Raw Stall Optimization */
+-	I915_WRITE(CACHE_MODE_0_GEN7,
+-		   _MASKED_BIT_DISABLE(HIZ_RAW_STALL_OPT_DISABLE));
++	return ret;
++}
++EXPORT_SYMBOL_GPL(i915_gpu_raise);
+ 
+-	/* WaDisable4x2SubspanOptimization:hsw */
+-	I915_WRITE(CACHE_MODE_1,
+-		   _MASKED_BIT_ENABLE(PIXEL_SUBSPAN_COLLECT_OPT_DISABLE));
++/**
++ * i915_gpu_lower - lower GPU frequency limit
++ *
++ * IPS indicates we're close to a thermal limit, so throttle back the GPU
++ * frequency maximum.
++ */
++bool i915_gpu_lower(void)
++{
++	struct drm_i915_private *dev_priv;
++	bool ret = true;
+ 
+-	/*
+-	 * BSpec recommends 8x4 when MSAA is used,
+-	 * however in practice 16x4 seems fastest.
+-	 *
+-	 * Note that PS/WM thread counts depend on the WIZ hashing
+-	 * disable bit, which we don't touch here, but it's good
+-	 * to keep in mind (see 3DSTATE_PS and 3DSTATE_WM).
+-	 */
+-	I915_WRITE(GEN7_GT_MODE,
+-		   GEN6_WIZ_HASHING_MASK | GEN6_WIZ_HASHING_16x4);
++	spin_lock_irq(&mchdev_lock);
++	if (!i915_mch_dev) {
++		ret = false;
++		goto out_unlock;
++	}
++	dev_priv = i915_mch_dev;
+ 
+-	/* WaSwitchSolVfFArbitrationPriority:hsw */
+-	I915_WRITE(GAM_ECOCHK, I915_READ(GAM_ECOCHK) | HSW_ECOCHK_ARB_PRIO_SOL);
++	if (dev_priv->ips.max_delay < dev_priv->ips.min_delay)
++		dev_priv->ips.max_delay++;
+ 
+-	/* WaRsPkgCStateDisplayPMReq:hsw */
+-	I915_WRITE(CHICKEN_PAR1_1,
+-		   I915_READ(CHICKEN_PAR1_1) | FORCE_ARB_IDLE_PLANES);
++out_unlock:
++	spin_unlock_irq(&mchdev_lock);
+ 
+-	lpt_init_clock_gating(dev);
++	return ret;
+ }
++EXPORT_SYMBOL_GPL(i915_gpu_lower);
+ 
+-static void ivybridge_init_clock_gating(struct drm_device *dev)
++/**
++ * i915_gpu_busy - indicate GPU business to IPS
++ *
++ * Tell the IPS driver whether or not the GPU is busy.
++ */
++bool i915_gpu_busy(void)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	uint32_t snpcr;
+-
+-	ilk_init_lp_watermarks(dev);
+-
+-	I915_WRITE(ILK_DSPCLK_GATE_D, ILK_VRHUNIT_CLOCK_GATE_DISABLE);
++	struct drm_i915_private *dev_priv;
++	struct intel_engine_cs *engine;
++	bool ret = false;
++	int i;
+ 
+-	/* WaDisableEarlyCull:ivb */
+-	I915_WRITE(_3D_CHICKEN3,
+-		   _MASKED_BIT_ENABLE(_3D_CHICKEN_SF_DISABLE_OBJEND_CULL));
++	spin_lock_irq(&mchdev_lock);
++	if (!i915_mch_dev)
++		goto out_unlock;
++	dev_priv = i915_mch_dev;
+ 
+-	/* WaDisableBackToBackFlipFix:ivb */
+-	I915_WRITE(IVB_CHICKEN3,
+-		   CHICKEN3_DGMG_REQ_OUT_FIX_DISABLE |
+-		   CHICKEN3_DGMG_DONE_FIX_DISABLE);
++	for_each_engine(engine, dev_priv, i)
++		ret |= engine->last_request != NULL;
+ 
+-	/* WaDisablePSDDualDispatchEnable:ivb */
+-	if (IS_IVB_GT1(dev))
+-		I915_WRITE(GEN7_HALF_SLICE_CHICKEN1,
+-			   _MASKED_BIT_ENABLE(GEN7_PSD_SINGLE_PORT_DISPATCH_ENABLE));
++out_unlock:
++	spin_unlock_irq(&mchdev_lock);
+ 
+-	/* WaDisable_RenderCache_OperationalFlush:ivb */
+-	I915_WRITE(CACHE_MODE_0_GEN7, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
++	return ret;
++}
++EXPORT_SYMBOL_GPL(i915_gpu_busy);
+ 
+-	/* Apply the WaDisableRHWOOptimizationForRenderHang:ivb workaround. */
+-	I915_WRITE(GEN7_COMMON_SLICE_CHICKEN1,
+-		   GEN7_CSC1_RHWO_OPT_DISABLE_IN_RCC);
++/**
++ * i915_gpu_turbo_disable - disable graphics turbo
++ *
++ * Disable graphics turbo by resetting the max frequency and setting the
++ * current frequency to the default.
++ */
++bool i915_gpu_turbo_disable(void)
++{
++	struct drm_i915_private *dev_priv;
++	bool ret = true;
+ 
+-	/* WaApplyL3ControlAndL3ChickenMode:ivb */
+-	I915_WRITE(GEN7_L3CNTLREG1,
+-			GEN7_WA_FOR_GEN7_L3_CONTROL);
+-	I915_WRITE(GEN7_L3_CHICKEN_MODE_REGISTER,
+-		   GEN7_WA_L3_CHICKEN_MODE);
+-	if (IS_IVB_GT1(dev))
+-		I915_WRITE(GEN7_ROW_CHICKEN2,
+-			   _MASKED_BIT_ENABLE(DOP_CLOCK_GATING_DISABLE));
+-	else {
+-		/* must write both registers */
+-		I915_WRITE(GEN7_ROW_CHICKEN2,
+-			   _MASKED_BIT_ENABLE(DOP_CLOCK_GATING_DISABLE));
+-		I915_WRITE(GEN7_ROW_CHICKEN2_GT2,
+-			   _MASKED_BIT_ENABLE(DOP_CLOCK_GATING_DISABLE));
++	spin_lock_irq(&mchdev_lock);
++	if (!i915_mch_dev) {
++		ret = false;
++		goto out_unlock;
+ 	}
++	dev_priv = i915_mch_dev;
+ 
+-	/* WaForceL3Serialization:ivb */
+-	I915_WRITE(GEN7_L3SQCREG4, I915_READ(GEN7_L3SQCREG4) &
+-		   ~L3SQ_URB_READ_CAM_MATCH_DISABLE);
++	dev_priv->ips.max_delay = dev_priv->ips.fstart;
+ 
+-	/*
+-	 * According to the spec, bit 13 (RCZUNIT) must be set on IVB.
+-	 * This implements the WaDisableRCZUnitClockGating:ivb workaround.
+-	 */
+-	I915_WRITE(GEN6_UCGCTL2,
+-		   GEN6_RCZUNIT_CLOCK_GATE_DISABLE);
++	if (!ironlake_set_drps(dev_priv->dev, dev_priv->ips.fstart))
++		ret = false;
+ 
+-	/* This is required by WaCatErrorRejectionIssue:ivb */
+-	I915_WRITE(GEN7_SQ_CHICKEN_MBCUNIT_CONFIG,
+-			I915_READ(GEN7_SQ_CHICKEN_MBCUNIT_CONFIG) |
+-			GEN7_SQ_CHICKEN_MBCUNIT_SQINTMOB);
++out_unlock:
++	spin_unlock_irq(&mchdev_lock);
+ 
+-	g4x_disable_trickle_feed(dev);
++	return ret;
++}
++EXPORT_SYMBOL_GPL(i915_gpu_turbo_disable);
+ 
+-	gen7_setup_fixed_func_scheduler(dev_priv);
++/**
++ * Tells the intel_ips driver that the i915 driver is now loaded, if
++ * IPS got loaded first.
++ *
++ * This awkward dance is so that neither module has to depend on the
++ * other in order for IPS to do the appropriate communication of
++ * GPU turbo limits to i915.
++ */
++static void
++ips_ping_for_i915_load(void)
++{
++	void (*link)(void);
+ 
+-	if (0) { /* causes HiZ corruption on ivb:gt1 */
+-		/* enable HiZ Raw Stall Optimization */
+-		I915_WRITE(CACHE_MODE_0_GEN7,
+-			   _MASKED_BIT_DISABLE(HIZ_RAW_STALL_OPT_DISABLE));
++	link = symbol_get(ips_link_to_i915_driver);
++	if (link) {
++		link();
++		symbol_put(ips_link_to_i915_driver);
+ 	}
++}
+ 
+-	/* WaDisable4x2SubspanOptimization:ivb */
+-	I915_WRITE(CACHE_MODE_1,
+-		   _MASKED_BIT_ENABLE(PIXEL_SUBSPAN_COLLECT_OPT_DISABLE));
+-
+-	/*
+-	 * BSpec recommends 8x4 when MSAA is used,
+-	 * however in practice 16x4 seems fastest.
+-	 *
+-	 * Note that PS/WM thread counts depend on the WIZ hashing
+-	 * disable bit, which we don't touch here, but it's good
+-	 * to keep in mind (see 3DSTATE_PS and 3DSTATE_WM).
+-	 */
+-	I915_WRITE(GEN7_GT_MODE,
+-		   GEN6_WIZ_HASHING_MASK | GEN6_WIZ_HASHING_16x4);
+-
+-	snpcr = I915_READ(GEN6_MBCUNIT_SNPCR);
+-	snpcr &= ~GEN6_MBC_SNPCR_MASK;
+-	snpcr |= GEN6_MBC_SNPCR_MED;
+-	I915_WRITE(GEN6_MBCUNIT_SNPCR, snpcr);
++void intel_gpu_ips_init(struct drm_i915_private *dev_priv)
++{
++	/* We only register the i915 ips part with intel-ips once everything is
++	 * set up, to avoid intel-ips sneaking in and reading bogus values. */
++	spin_lock_irq(&mchdev_lock);
++	i915_mch_dev = dev_priv;
++	spin_unlock_irq(&mchdev_lock);
+ 
+-	if (!HAS_PCH_NOP(dev))
+-		cpt_init_clock_gating(dev);
++	ips_ping_for_i915_load();
++}
+ 
+-	gen6_check_mch_setup(dev);
++void intel_gpu_ips_teardown(void)
++{
++	spin_lock_irq(&mchdev_lock);
++	i915_mch_dev = NULL;
++	spin_unlock_irq(&mchdev_lock);
+ }
+ 
+-static void valleyview_init_clock_gating(struct drm_device *dev)
++static void intel_init_emon(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	u32 val;
++	u32 lcfuse;
++	u8 pxw[16];
++	int i;
+ 
+-	mutex_lock(&dev_priv->rps.hw_lock);
+-	val = vlv_punit_read(dev_priv, PUNIT_REG_GPU_FREQ_STS);
+-	mutex_unlock(&dev_priv->rps.hw_lock);
+-	switch ((val >> 6) & 3) {
+-	case 0:
+-	case 1:
+-		dev_priv->mem_freq = 800;
+-		break;
+-	case 2:
+-		dev_priv->mem_freq = 1066;
+-		break;
+-	case 3:
+-		dev_priv->mem_freq = 1333;
+-		break;
+-	}
+-	DRM_DEBUG_DRIVER("DDR speed: %d MHz", dev_priv->mem_freq);
++	/* Disable to program */
++	I915_WRITE(ECR, 0);
++	POSTING_READ(ECR);
++
++	/* Program energy weights for various events */
++	I915_WRITE(SDEW, 0x15040d00);
++	I915_WRITE(CSIEW0, 0x007f0000);
++	I915_WRITE(CSIEW1, 0x1e220004);
++	I915_WRITE(CSIEW2, 0x04000004);
++
++	for (i = 0; i < 5; i++)
++		I915_WRITE(PEW + (i * 4), 0);
++	for (i = 0; i < 3; i++)
++		I915_WRITE(DEW + (i * 4), 0);
++
++	/* Program P-state weights to account for frequency power adjustment */
++	for (i = 0; i < 16; i++) {
++		u32 pxvidfreq = I915_READ(PXVFREQ_BASE + (i * 4));
++		unsigned long freq = intel_pxfreq(pxvidfreq);
++		unsigned long vid = (pxvidfreq & PXVFREQ_PX_MASK) >>
++			PXVFREQ_PX_SHIFT;
++		unsigned long val;
+ 
+-	I915_WRITE(DSPCLK_GATE_D, VRHUNIT_CLOCK_GATE_DISABLE);
++		val = vid * vid;
++		val *= (freq / 1000);
++		val *= 255;
++		val /= (127*127*900);
++		if (val > 0xff)
++			DRM_ERROR("bad pxval: %ld\n", val);
++		pxw[i] = val;
++	}
++	/* Render standby states get 0 weight */
++	pxw[14] = 0;
++	pxw[15] = 0;
+ 
+-	/* WaDisableEarlyCull:vlv */
+-	I915_WRITE(_3D_CHICKEN3,
+-		   _MASKED_BIT_ENABLE(_3D_CHICKEN_SF_DISABLE_OBJEND_CULL));
++	for (i = 0; i < 4; i++) {
++		u32 val = (pxw[i*4] << 24) | (pxw[(i*4)+1] << 16) |
++			(pxw[(i*4)+2] << 8) | (pxw[(i*4)+3]);
++		I915_WRITE(PXW + (i * 4), val);
++	}
+ 
+-	/* WaDisableBackToBackFlipFix:vlv */
+-	I915_WRITE(IVB_CHICKEN3,
+-		   CHICKEN3_DGMG_REQ_OUT_FIX_DISABLE |
+-		   CHICKEN3_DGMG_DONE_FIX_DISABLE);
++	/* Adjust magic regs to magic values (more experimental results) */
++	I915_WRITE(OGW0, 0);
++	I915_WRITE(OGW1, 0);
++	I915_WRITE(EG0, 0x00007f00);
++	I915_WRITE(EG1, 0x0000000e);
++	I915_WRITE(EG2, 0x000e0000);
++	I915_WRITE(EG3, 0x68000300);
++	I915_WRITE(EG4, 0x42000000);
++	I915_WRITE(EG5, 0x00140031);
++	I915_WRITE(EG6, 0);
++	I915_WRITE(EG7, 0);
+ 
+-	/* WaPsdDispatchEnable:vlv */
+-	/* WaDisablePSDDualDispatchEnable:vlv */
+-	I915_WRITE(GEN7_HALF_SLICE_CHICKEN1,
+-		   _MASKED_BIT_ENABLE(GEN7_MAX_PS_THREAD_DEP |
+-				      GEN7_PSD_SINGLE_PORT_DISPATCH_ENABLE));
++	for (i = 0; i < 8; i++)
++		I915_WRITE(PXWL + (i * 4), 0);
+ 
+-	/* WaDisable_RenderCache_OperationalFlush:vlv */
+-	I915_WRITE(CACHE_MODE_0_GEN7, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
++	/* Enable PMON + select events */
++	I915_WRITE(ECR, 0x80000019);
+ 
+-	/* WaForceL3Serialization:vlv */
+-	I915_WRITE(GEN7_L3SQCREG4, I915_READ(GEN7_L3SQCREG4) &
+-		   ~L3SQ_URB_READ_CAM_MATCH_DISABLE);
++	lcfuse = I915_READ(LCFUSE02);
+ 
+-	/* WaDisableDopClockGating:vlv */
+-	I915_WRITE(GEN7_ROW_CHICKEN2,
+-		   _MASKED_BIT_ENABLE(DOP_CLOCK_GATING_DISABLE));
++	dev_priv->ips.corr = (lcfuse & LCFUSE_HIV_MASK);
++}
+ 
+-	/* This is required by WaCatErrorRejectionIssue:vlv */
+-	I915_WRITE(GEN7_SQ_CHICKEN_MBCUNIT_CONFIG,
+-		   I915_READ(GEN7_SQ_CHICKEN_MBCUNIT_CONFIG) |
+-		   GEN7_SQ_CHICKEN_MBCUNIT_SQINTMOB);
++void intel_init_gt_powersave(struct drm_device *dev)
++{
++	i915_module.enable_rc6 = sanitize_rc6_option(dev, i915_module.enable_rc6);
+ 
+-	gen7_setup_fixed_func_scheduler(dev_priv);
++	if (IS_CHERRYVIEW(dev))
++		cherryview_init_gt_powersave(dev);
++	else if (IS_VALLEYVIEW(dev))
++		valleyview_init_gt_powersave(dev);
++}
+ 
+-	/*
+-	 * According to the spec, bit 13 (RCZUNIT) must be set on IVB.
+-	 * This implements the WaDisableRCZUnitClockGating:vlv workaround.
+-	 */
+-	I915_WRITE(GEN6_UCGCTL2,
+-		   GEN6_RCZUNIT_CLOCK_GATE_DISABLE);
++void intel_cleanup_gt_powersave(struct drm_device *dev)
++{
++	if (IS_CHERRYVIEW(dev))
++		return;
++	else if (IS_VALLEYVIEW(dev))
++		valleyview_cleanup_gt_powersave(dev);
++}
+ 
+-	/* WaDisableL3Bank2xClockGate:vlv
+-	 * Disabling L3 clock gating- MMIO 940c[25] = 1
+-	 * Set bit 25, to disable L3_BANK_2x_CLK_GATING */
+-	I915_WRITE(GEN7_UCGCTL4,
+-		   I915_READ(GEN7_UCGCTL4) | GEN7_L3BANK2X_CLOCK_GATE_DISABLE);
++/**
++ * intel_suspend_gt_powersave - suspend PM work and helper threads
++ * @dev: drm device
++ *
++ * We don't want to disable RC6 or other features here, we just want
++ * to make sure any work we've queued has finished and won't bother
++ * us while we're suspended.
++ */
++void intel_suspend_gt_powersave(struct drm_device *dev)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	I915_WRITE(MI_ARB_VLV, MI_ARB_DISPLAY_TRICKLE_FEED_DISABLE);
++	/* Interrupts should be disabled already to avoid re-arming. */
++	WARN_ON(intel_irqs_enabled(dev_priv));
+ 
+-	/*
+-	 * BSpec says this must be set, even though
+-	 * WaDisable4x2SubspanOptimization isn't listed for VLV.
+-	 */
+-	I915_WRITE(CACHE_MODE_1,
+-		   _MASKED_BIT_ENABLE(PIXEL_SUBSPAN_COLLECT_OPT_DISABLE));
++	flush_delayed_work(&dev_priv->rps.delayed_resume_work);
+ 
+-	/*
+-	 * WaIncreaseL3CreditsForVLVB0:vlv
+-	 * This is the hardware default actually.
+-	 */
+-	I915_WRITE(GEN7_L3SQCREG1, VLV_B0_WA_L3SQCREG1_VALUE);
++	cancel_work_sync(&dev_priv->rps.work);
+ 
+-	/*
+-	 * WaDisableVLVClockGating_VBIIssue:vlv
+-	 * Disable clock gating on th GCFG unit to prevent a delay
+-	 * in the reporting of vblank events.
+-	 */
+-	I915_WRITE(VLV_GUNIT_CLOCK_GATE, GCFG_DIS);
++	/* Force GPU to min freq during suspend */
++	gen6_rps_idle(dev_priv);
+ }
+ 
+-static void cherryview_init_clock_gating(struct drm_device *dev)
++void intel_disable_gt_powersave(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	u32 val;
+-
+-	mutex_lock(&dev_priv->rps.hw_lock);
+-	val = vlv_punit_read(dev_priv, CCK_FUSE_REG);
+-	mutex_unlock(&dev_priv->rps.hw_lock);
+-	switch ((val >> 2) & 0x7) {
+-	case 0:
+-	case 1:
+-			dev_priv->rps.cz_freq = CHV_CZ_CLOCK_FREQ_MODE_200;
+-			dev_priv->mem_freq = 1600;
+-			break;
+-	case 2:
+-			dev_priv->rps.cz_freq = CHV_CZ_CLOCK_FREQ_MODE_267;
+-			dev_priv->mem_freq = 1600;
+-			break;
+-	case 3:
+-			dev_priv->rps.cz_freq = CHV_CZ_CLOCK_FREQ_MODE_333;
+-			dev_priv->mem_freq = 2000;
+-			break;
+-	case 4:
+-			dev_priv->rps.cz_freq = CHV_CZ_CLOCK_FREQ_MODE_320;
+-			dev_priv->mem_freq = 1600;
+-			break;
+-	case 5:
+-			dev_priv->rps.cz_freq = CHV_CZ_CLOCK_FREQ_MODE_400;
+-			dev_priv->mem_freq = 1600;
+-			break;
+-	}
+-	DRM_DEBUG_DRIVER("DDR speed: %d MHz", dev_priv->mem_freq);
+ 
+-	I915_WRITE(DSPCLK_GATE_D, VRHUNIT_CLOCK_GATE_DISABLE);
++	WARN_ON(dev_priv->mm.busy);
+ 
+-	I915_WRITE(MI_ARB_VLV, MI_ARB_DISPLAY_TRICKLE_FEED_DISABLE);
++	/* Interrupts should be disabled already to avoid re-arming. */
++	WARN_ON(intel_irqs_enabled(dev_priv));
+ 
+-	/* WaDisablePartialInstShootdown:chv */
+-	I915_WRITE(GEN8_ROW_CHICKEN,
+-		   _MASKED_BIT_ENABLE(PARTIAL_INSTRUCTION_SHOOTDOWN_DISABLE));
+-
+-	/* WaDisableThreadStallDopClockGating:chv */
+-	I915_WRITE(GEN8_ROW_CHICKEN,
+-		   _MASKED_BIT_ENABLE(STALL_DOP_GATING_DISABLE));
++	if (IS_IRONLAKE_M(dev)) {
++		ironlake_disable_drps(dev);
++		ironlake_disable_rc6(dev);
++	} else if (INTEL_INFO(dev)->gen >= 6) {
++		intel_suspend_gt_powersave(dev);
+ 
+-	/* WaVSRefCountFullforceMissDisable:chv */
+-	/* WaDSRefCountFullforceMissDisable:chv */
+-	I915_WRITE(GEN7_FF_THREAD_MODE,
+-		   I915_READ(GEN7_FF_THREAD_MODE) &
+-		   ~(GEN8_FF_DS_REF_CNT_FFME | GEN7_FF_VS_REF_CNT_FFME));
++		mutex_lock(&dev_priv->rps.hw_lock);
++		if (INTEL_INFO(dev)->gen >= 9)
++			gen9_disable_rps(dev);
++		else if (IS_CHERRYVIEW(dev))
++			cherryview_disable_rps(dev);
++		else if (IS_VALLEYVIEW(dev))
++			valleyview_disable_rps(dev);
++		else
++			gen6_disable_rps(dev);
++		dev_priv->rps.enabled = false;
++		mutex_unlock(&dev_priv->rps.hw_lock);
++	}
++}
+ 
+-	/* WaDisableSemaphoreAndSyncFlipWait:chv */
+-	I915_WRITE(GEN6_RC_SLEEP_PSMI_CONTROL,
+-		   _MASKED_BIT_ENABLE(GEN8_RC_SEMA_IDLE_MSG_DISABLE));
++static void intel_gen6_powersave_work(struct work_struct *work)
++{
++	struct drm_i915_private *dev_priv =
++		container_of(work, struct drm_i915_private,
++			     rps.delayed_resume_work.work);
++	struct drm_device *dev = dev_priv->dev;
+ 
+-	/* WaDisableCSUnitClockGating:chv */
+-	I915_WRITE(GEN6_UCGCTL1, I915_READ(GEN6_UCGCTL1) |
+-		   GEN6_CSUNIT_CLOCK_GATE_DISABLE);
++	mutex_lock(&dev_priv->rps.hw_lock);
+ 
+-	/* WaDisableSDEUnitClockGating:chv */
+-	I915_WRITE(GEN8_UCGCTL6, I915_READ(GEN8_UCGCTL6) |
+-		   GEN8_SDEUNIT_CLOCK_GATE_DISABLE);
++	if (IS_CHERRYVIEW(dev)) {
++		cherryview_enable_rps(dev);
++	} else if (IS_VALLEYVIEW(dev)) {
++		valleyview_enable_rps(dev);
++	} else if (INTEL_INFO(dev)->gen >= 9) {
++		gen9_enable_rps(dev);
++	} else if (IS_BROADWELL(dev)) {
++		gen8_enable_rps(dev);
++		__gen6_update_ring_freq(dev);
++	} else {
++		gen6_enable_rps(dev);
++		__gen6_update_ring_freq(dev);
++	}
++	dev_priv->rps.enabled = true;
++	mutex_unlock(&dev_priv->rps.hw_lock);
+ 
+-	/* WaDisableSamplerPowerBypass:chv (pre-production hw) */
+-	I915_WRITE(HALF_SLICE_CHICKEN3,
+-		   _MASKED_BIT_ENABLE(GEN8_SAMPLER_POWER_BYPASS_DIS));
+-
+-	/* WaDisableGunitClockGating:chv (pre-production hw) */
+-	I915_WRITE(VLV_GUNIT_CLOCK_GATE, I915_READ(VLV_GUNIT_CLOCK_GATE) |
+-		   GINT_DIS);
++	intel_runtime_pm_put(dev_priv);
++}
+ 
+-	/* WaDisableFfDopClockGating:chv (pre-production hw) */
+-	I915_WRITE(GEN6_RC_SLEEP_PSMI_CONTROL,
+-		   _MASKED_BIT_ENABLE(GEN8_FF_DOP_CLOCK_GATE_DISABLE));
++void intel_enable_gt_powersave(struct drm_device *dev)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	/* WaDisableDopClockGating:chv (pre-production hw) */
+-	I915_WRITE(GEN7_ROW_CHICKEN2,
+-		   _MASKED_BIT_ENABLE(DOP_CLOCK_GATING_DISABLE));
+-	I915_WRITE(GEN6_UCGCTL1, I915_READ(GEN6_UCGCTL1) |
+-		   GEN6_EU_TCUNIT_CLOCK_GATE_DISABLE);
++	if (IS_IRONLAKE_M(dev)) {
++		mutex_lock(&dev->struct_mutex);
++		ironlake_enable_drps(dev);
++		ironlake_enable_rc6(dev);
++		intel_init_emon(dev);
++		mutex_unlock(&dev->struct_mutex);
++	} else if (INTEL_INFO(dev)->gen >= 6) {
++		/*
++		 * PCU communication is slow and this doesn't need to be
++		 * done at any specific time, so do this out of our fast path
++		 * to make resume and init faster.
++		 *
++		 * We depend on the HW RC6 power context save/restore
++		 * mechanism when entering D3 through runtime PM suspend. So
++		 * disable RPM until RPS/RC6 is properly setup. We can only
++		 * get here via the driver load/system resume/runtime resume
++		 * paths, so the _noresume version is enough (and in case of
++		 * runtime resume it's necessary).
++		 */
++		if (schedule_delayed_work(&dev_priv->rps.delayed_resume_work,
++					   round_jiffies_up_relative(HZ)))
++			intel_runtime_pm_get_noresume(dev_priv);
++	}
+ }
+ 
+-static void g4x_init_clock_gating(struct drm_device *dev)
++void intel_reset_gt_powersave(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	uint32_t dspclk_gate;
+-
+-	I915_WRITE(RENCLK_GATE_D1, 0);
+-	I915_WRITE(RENCLK_GATE_D2, VF_UNIT_CLOCK_GATE_DISABLE |
+-		   GS_UNIT_CLOCK_GATE_DISABLE |
+-		   CL_UNIT_CLOCK_GATE_DISABLE);
+-	I915_WRITE(RAMCLK_GATE_D, 0);
+-	dspclk_gate = VRHUNIT_CLOCK_GATE_DISABLE |
+-		OVRUNIT_CLOCK_GATE_DISABLE |
+-		OVCUNIT_CLOCK_GATE_DISABLE;
+-	if (IS_GM45(dev))
+-		dspclk_gate |= DSSUNIT_CLOCK_GATE_DISABLE;
+-	I915_WRITE(DSPCLK_GATE_D, dspclk_gate);
+ 
+-	/* WaDisableRenderCachePipelinedFlush */
+-	I915_WRITE(CACHE_MODE_0,
+-		   _MASKED_BIT_ENABLE(CM0_PIPELINED_RENDER_FLUSH_DISABLE));
++	dev_priv->rps.enabled = false;
++	intel_enable_gt_powersave(dev);
++}
+ 
+-	/* WaDisable_RenderCache_OperationalFlush:g4x */
+-	I915_WRITE(CACHE_MODE_0, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
++static void ibx_init_clock_gating(struct drm_device *dev)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	g4x_disable_trickle_feed(dev);
++	/*
++	 * On Ibex Peak and Cougar Point, we need to disable clock
++	 * gating for the panel power sequencer or it will fail to
++	 * start up when no ports are active.
++	 */
++	I915_WRITE(SOUTH_DSPCLK_GATE_D, PCH_DPLSUNIT_CLOCK_GATE_DISABLE);
+ }
+ 
+-static void crestline_init_clock_gating(struct drm_device *dev)
++static void g4x_disable_trickle_feed(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
++	int pipe;
+ 
+-	I915_WRITE(RENCLK_GATE_D1, I965_RCC_CLOCK_GATE_DISABLE);
+-	I915_WRITE(RENCLK_GATE_D2, 0);
+-	I915_WRITE(DSPCLK_GATE_D, 0);
+-	I915_WRITE(RAMCLK_GATE_D, 0);
+-	I915_WRITE16(DEUC, 0);
+-	I915_WRITE(MI_ARB_STATE,
+-		   _MASKED_BIT_ENABLE(MI_ARB_DISPLAY_TRICKLE_FEED_DISABLE));
+-
+-	/* WaDisable_RenderCache_OperationalFlush:gen4 */
+-	I915_WRITE(CACHE_MODE_0, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
++	for_each_pipe(dev_priv, pipe) {
++		I915_WRITE(DSPCNTR(pipe),
++			   I915_READ(DSPCNTR(pipe)) |
++			   DISPPLANE_TRICKLE_FEED_DISABLE);
++		intel_flush_primary_plane(dev_priv, pipe);
++	}
+ }
+ 
+-static void broadwater_init_clock_gating(struct drm_device *dev)
++static void ilk_init_lp_watermarks(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	I915_WRITE(RENCLK_GATE_D1, I965_RCZ_CLOCK_GATE_DISABLE |
+-		   I965_RCC_CLOCK_GATE_DISABLE |
+-		   I965_RCPB_CLOCK_GATE_DISABLE |
+-		   I965_ISC_CLOCK_GATE_DISABLE |
+-		   I965_FBC_CLOCK_GATE_DISABLE);
+-	I915_WRITE(RENCLK_GATE_D2, 0);
+-	I915_WRITE(MI_ARB_STATE,
+-		   _MASKED_BIT_ENABLE(MI_ARB_DISPLAY_TRICKLE_FEED_DISABLE));
++	I915_WRITE(WM3_LP_ILK, I915_READ(WM3_LP_ILK) & ~WM1_LP_SR_EN);
++	I915_WRITE(WM2_LP_ILK, I915_READ(WM2_LP_ILK) & ~WM1_LP_SR_EN);
++	I915_WRITE(WM1_LP_ILK, I915_READ(WM1_LP_ILK) & ~WM1_LP_SR_EN);
+ 
+-	/* WaDisable_RenderCache_OperationalFlush:gen4 */
+-	I915_WRITE(CACHE_MODE_0, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
++	/*
++	 * Don't touch WM1S_LP_EN here.
++	 * Doing so could cause underruns.
++	 */
+ }
+ 
+-static void gen3_init_clock_gating(struct drm_device *dev)
++static void ironlake_init_clock_gating(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	u32 dstate = I915_READ(D_STATE);
++	uint32_t dspclk_gate = ILK_VRHUNIT_CLOCK_GATE_DISABLE;
+ 
+-	dstate |= DSTATE_PLL_D3_OFF | DSTATE_GFX_CLOCK_GATING |
+-		DSTATE_DOT_CLOCK_GATING;
+-	I915_WRITE(D_STATE, dstate);
++	/*
++	 * Required for FBC
++	 * WaFbcDisableDpfcClockGating:ilk
++	 */
++	dspclk_gate |= ILK_DPFCRUNIT_CLOCK_GATE_DISABLE |
++		   ILK_DPFCUNIT_CLOCK_GATE_DISABLE |
++		   ILK_DPFDUNIT_CLOCK_GATE_ENABLE;
+ 
+-	if (IS_PINEVIEW(dev))
+-		I915_WRITE(ECOSKPD, _MASKED_BIT_ENABLE(ECO_GATING_CX_ONLY));
++	I915_WRITE(PCH_3DCGDIS0,
++		   MARIUNIT_CLOCK_GATE_DISABLE |
++		   SVSMUNIT_CLOCK_GATE_DISABLE);
++	I915_WRITE(PCH_3DCGDIS1,
++		   VFMUNIT_CLOCK_GATE_DISABLE);
+ 
+-	/* IIR "flip pending" means done if this bit is set */
+-	I915_WRITE(ECOSKPD, _MASKED_BIT_DISABLE(ECO_FLIP_DONE));
++	/*
++	 * According to the spec the following bits should be set in
++	 * order to enable memory self-refresh
++	 * The bit 22/21 of 0x42004
++	 * The bit 5 of 0x42020
++	 * The bit 15 of 0x45000
++	 */
++	I915_WRITE(ILK_DISPLAY_CHICKEN2,
++		   (I915_READ(ILK_DISPLAY_CHICKEN2) |
++		    ILK_DPARB_GATE | ILK_VSDPFD_FULL));
++	dspclk_gate |= ILK_DPARBUNIT_CLOCK_GATE_ENABLE;
++	I915_WRITE(DISP_ARB_CTL,
++		   (I915_READ(DISP_ARB_CTL) |
++		    DISP_FBC_WM_DIS));
+ 
+-	/* interrupts should cause a wake up from C3 */
+-	I915_WRITE(INSTPM, _MASKED_BIT_ENABLE(INSTPM_AGPBUSY_INT_EN));
++	ilk_init_lp_watermarks(dev);
+ 
+-	/* On GEN3 we really need to make sure the ARB C3 LP bit is set */
+-	I915_WRITE(MI_ARB_STATE, _MASKED_BIT_ENABLE(MI_ARB_C3_LP_WRITE_ENABLE));
+-}
++	/*
++	 * Based on the document from hardware guys the following bits
++	 * should be set unconditionally in order to enable FBC.
++	 * The bit 22 of 0x42000
++	 * The bit 22 of 0x42004
++	 * The bit 7,8,9 of 0x42020.
++	 */
++	if (IS_IRONLAKE_M(dev)) {
++		/* WaFbcAsynchFlipDisableFbcQueue:ilk */
++		I915_WRITE(ILK_DISPLAY_CHICKEN1,
++			   I915_READ(ILK_DISPLAY_CHICKEN1) |
++			   ILK_FBCQ_DIS);
++		I915_WRITE(ILK_DISPLAY_CHICKEN2,
++			   I915_READ(ILK_DISPLAY_CHICKEN2) |
++			   ILK_DPARB_GATE);
++	}
+ 
+-static void i85x_init_clock_gating(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	I915_WRITE(ILK_DSPCLK_GATE_D, dspclk_gate);
+ 
+-	I915_WRITE(RENCLK_GATE_D1, SV_CLOCK_GATE_DISABLE);
++	I915_WRITE(ILK_DISPLAY_CHICKEN2,
++		   I915_READ(ILK_DISPLAY_CHICKEN2) |
++		   ILK_ELPIN_409_SELECT);
++	I915_WRITE(_3D_CHICKEN2,
++		   _3D_CHICKEN2_WM_READ_PIPELINED << 16 |
++		   _3D_CHICKEN2_WM_READ_PIPELINED);
+ 
+-	/* interrupts should cause a wake up from C3 */
+-	I915_WRITE(MI_STATE, _MASKED_BIT_ENABLE(MI_AGPBUSY_INT_EN) |
+-		   _MASKED_BIT_DISABLE(MI_AGPBUSY_830_MODE));
++	/* WaDisableRenderCachePipelinedFlush:ilk */
++	I915_WRITE(CACHE_MODE_0,
++		   _MASKED_BIT_ENABLE(CM0_PIPELINED_RENDER_FLUSH_DISABLE));
++
++	/* WaDisable_RenderCache_OperationalFlush:ilk */
++	I915_WRITE(CACHE_MODE_0, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
++
++	g4x_disable_trickle_feed(dev);
++
++	ibx_init_clock_gating(dev);
+ }
+ 
+-static void i830_init_clock_gating(struct drm_device *dev)
++static void cpt_init_clock_gating(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
++	int pipe;
++	uint32_t val;
+ 
+-	I915_WRITE(DSPCLK_GATE_D, OVRUNIT_CLOCK_GATE_DISABLE);
++	/*
++	 * On Ibex Peak and Cougar Point, we need to disable clock
++	 * gating for the panel power sequencer or it will fail to
++	 * start up when no ports are active.
++	 */
++	I915_WRITE(SOUTH_DSPCLK_GATE_D, PCH_DPLSUNIT_CLOCK_GATE_DISABLE |
++		   PCH_DPLUNIT_CLOCK_GATE_DISABLE |
++		   PCH_CPUNIT_CLOCK_GATE_DISABLE);
++	I915_WRITE(SOUTH_CHICKEN2, I915_READ(SOUTH_CHICKEN2) |
++		   DPLS_EDP_PPS_FIX_DIS);
++	/* The below fixes the weird display corruption, a few pixels shifted
++	 * downward, on (only) LVDS of some HP laptops with IVY.
++	 */
++	for_each_pipe(dev_priv, pipe) {
++		val = I915_READ(TRANS_CHICKEN2(pipe));
++		val |= TRANS_CHICKEN2_TIMING_OVERRIDE;
++		val &= ~TRANS_CHICKEN2_FDI_POLARITY_REVERSED;
++		if (dev_priv->vbt.fdi_rx_polarity_inverted)
++			val |= TRANS_CHICKEN2_FDI_POLARITY_REVERSED;
++		val &= ~TRANS_CHICKEN2_FRAME_START_DELAY_MASK;
++		val &= ~TRANS_CHICKEN2_DISABLE_DEEP_COLOR_COUNTER;
++		val &= ~TRANS_CHICKEN2_DISABLE_DEEP_COLOR_MODESWITCH;
++		I915_WRITE(TRANS_CHICKEN2(pipe), val);
++	}
++	/* WADP0ClockGatingDisable */
++	for_each_pipe(dev_priv, pipe) {
++		I915_WRITE(TRANS_CHICKEN1(pipe),
++			   TRANS_CHICKEN1_DP0UNIT_GC_DISABLE);
++	}
+ }
+ 
+-void intel_init_clock_gating(struct drm_device *dev)
++static void gen6_check_mch_setup(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
++	uint32_t tmp;
+ 
+-	dev_priv->display.init_clock_gating(dev);
++	tmp = I915_READ(MCH_SSKPD);
++	if ((tmp & MCH_SSKPD_WM0_MASK) != MCH_SSKPD_WM0_VAL)
++		DRM_DEBUG_KMS("Wrong MCH_SSKPD value: 0x%08x This can cause underruns.\n",
++			      tmp);
+ }
+ 
+-void intel_suspend_hw(struct drm_device *dev)
++static void gen6_init_clock_gating(struct drm_device *dev)
+ {
+-	if (HAS_PCH_LPT(dev))
+-		lpt_suspend_hw(dev);
+-}
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	uint32_t dspclk_gate = ILK_VRHUNIT_CLOCK_GATE_DISABLE;
++
++	I915_WRITE(ILK_DSPCLK_GATE_D, dspclk_gate);
+ 
+-#define for_each_power_well(i, power_well, domain_mask, power_domains)	\
+-	for (i = 0;							\
+-	     i < (power_domains)->power_well_count &&			\
+-		 ((power_well) = &(power_domains)->power_wells[i]);	\
+-	     i++)							\
+-		if ((power_well)->domains & (domain_mask))
+-
+-#define for_each_power_well_rev(i, power_well, domain_mask, power_domains) \
+-	for (i = (power_domains)->power_well_count - 1;			 \
+-	     i >= 0 && ((power_well) = &(power_domains)->power_wells[i]);\
+-	     i--)							 \
+-		if ((power_well)->domains & (domain_mask))
++	I915_WRITE(ILK_DISPLAY_CHICKEN2,
++		   I915_READ(ILK_DISPLAY_CHICKEN2) |
++		   ILK_ELPIN_409_SELECT);
+ 
+-/**
+- * We should only use the power well if we explicitly asked the hardware to
+- * enable it, so check if it's enabled and also check if we've requested it to
+- * be enabled.
+- */
+-static bool hsw_power_well_enabled(struct drm_i915_private *dev_priv,
+-				   struct i915_power_well *power_well)
+-{
+-	return I915_READ(HSW_PWR_WELL_DRIVER) ==
+-		     (HSW_PWR_WELL_ENABLE_REQUEST | HSW_PWR_WELL_STATE_ENABLED);
+-}
++	/* WaDisableHiZPlanesWhenMSAAEnabled:snb */
++	I915_WRITE(_3D_CHICKEN,
++		   _MASKED_BIT_ENABLE(_3D_CHICKEN_HIZ_PLANE_DISABLE_MSAA_4X_SNB));
+ 
+-bool intel_display_power_enabled_unlocked(struct drm_i915_private *dev_priv,
+-					  enum intel_display_power_domain domain)
+-{
+-	struct i915_power_domains *power_domains;
+-	struct i915_power_well *power_well;
+-	bool is_enabled;
+-	int i;
++	/* WaDisable_RenderCache_OperationalFlush:snb */
++	I915_WRITE(CACHE_MODE_0, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
+ 
+-	if (dev_priv->pm.suspended)
+-		return false;
++	I915_WRITE(GEN6_GT_MODE, _MASKED_BIT_DISABLE(0xffff));
+ 
+-	power_domains = &dev_priv->power_domains;
++	/*
++	 * BSpec recoomends 8x4 when MSAA is used,
++	 * however in practice 16x4 seems fastest.
++	 *
++	 * Note that PS/WM thread counts depend on the WIZ hashing
++	 * disable bit, which we don't touch here, but it's good
++	 * to keep in mind (see 3DSTATE_PS and 3DSTATE_WM).
++	 */
++	I915_WRITE(GEN6_GT_MODE,
++		   GEN6_WIZ_HASHING_MASK | GEN6_WIZ_HASHING_16x4);
+ 
+-	is_enabled = true;
++	/* WaSetupGtModeTdRowDispatch:snb */
++	if (IS_SNB_GT1(dev))
++		I915_WRITE(GEN6_GT_MODE,
++			   _MASKED_BIT_ENABLE(GEN6_TD_FOUR_ROW_DISPATCH_DISABLE));
+ 
+-	for_each_power_well_rev(i, power_well, BIT(domain), power_domains) {
+-		if (power_well->always_on)
+-			continue;
++	ilk_init_lp_watermarks(dev);
+ 
+-		if (!power_well->hw_enabled) {
+-			is_enabled = false;
+-			break;
+-		}
+-	}
++	I915_WRITE(CACHE_MODE_0,
++		   _MASKED_BIT_DISABLE(CM0_STC_EVICT_DISABLE_LRA_SNB));
+ 
+-	return is_enabled;
+-}
++	I915_WRITE(GEN6_UCGCTL1,
++		   I915_READ(GEN6_UCGCTL1) |
++		   GEN6_BLBUNIT_CLOCK_GATE_DISABLE |
++		   GEN6_CSUNIT_CLOCK_GATE_DISABLE);
+ 
+-bool intel_display_power_enabled(struct drm_i915_private *dev_priv,
+-				 enum intel_display_power_domain domain)
+-{
+-	struct i915_power_domains *power_domains;
+-	bool ret;
++	/* According to the BSpec vol1g, bit 12 (RCPBUNIT) clock
++	 * gating disable must be set.  Failure to set it results in
++	 * flickering pixels due to Z write ordering failures after
++	 * some amount of runtime in the Mesa "fire" demo, and Unigine
++	 * Sanctuary and Tropics, and apparently anything else with
++	 * alpha test or pixel discard.
++	 *
++	 * According to the spec, bit 11 (RCCUNIT) must also be set,
++	 * but we didn't debug actual testcases to find it out.
++	 *
++	 * WaDisableRCCUnitClockGating:snb
++	 * WaDisableRCPBUnitClockGating:snb
++	 */
++	I915_WRITE(GEN6_UCGCTL2,
++		   GEN6_RCPBUNIT_CLOCK_GATE_DISABLE |
++		   GEN6_RCCUNIT_CLOCK_GATE_DISABLE);
++
++	/* WaStripsFansDisableFastClipPerformanceFix:snb */
++	I915_WRITE(_3D_CHICKEN3,
++		   _MASKED_BIT_ENABLE(_3D_CHICKEN3_SF_DISABLE_FASTCLIP_CULL));
++
++	/*
++	 * Bspec says:
++	 * "This bit must be set if 3DSTATE_CLIP clip mode is set to normal and
++	 * 3DSTATE_SF number of SF output attributes is more than 16."
++	 */
++	I915_WRITE(_3D_CHICKEN3,
++		   _MASKED_BIT_ENABLE(_3D_CHICKEN3_SF_DISABLE_PIPELINED_ATTR_FETCH));
++
++	/*
++	 * According to the spec the following bits should be
++	 * set in order to enable memory self-refresh and fbc:
++	 * The bit21 and bit22 of 0x42000
++	 * The bit21 and bit22 of 0x42004
++	 * The bit5 and bit7 of 0x42020
++	 * The bit14 of 0x70180
++	 * The bit14 of 0x71180
++	 *
++	 * WaFbcAsynchFlipDisableFbcQueue:snb
++	 */
++	I915_WRITE(ILK_DISPLAY_CHICKEN1,
++		   I915_READ(ILK_DISPLAY_CHICKEN1) |
++		   ILK_FBCQ_DIS | ILK_PABSTRETCH_DIS);
++	I915_WRITE(ILK_DISPLAY_CHICKEN2,
++		   I915_READ(ILK_DISPLAY_CHICKEN2) |
++		   ILK_DPARB_GATE | ILK_VSDPFD_FULL);
++	I915_WRITE(ILK_DSPCLK_GATE_D,
++		   I915_READ(ILK_DSPCLK_GATE_D) |
++		   ILK_DPARBUNIT_CLOCK_GATE_ENABLE  |
++		   ILK_DPFDUNIT_CLOCK_GATE_ENABLE);
+ 
+-	power_domains = &dev_priv->power_domains;
++	g4x_disable_trickle_feed(dev);
+ 
+-	mutex_lock(&power_domains->lock);
+-	ret = intel_display_power_enabled_unlocked(dev_priv, domain);
+-	mutex_unlock(&power_domains->lock);
++	cpt_init_clock_gating(dev);
+ 
+-	return ret;
++	gen6_check_mch_setup(dev);
+ }
+ 
+-/*
+- * Starting with Haswell, we have a "Power Down Well" that can be turned off
+- * when not needed anymore. We have 4 registers that can request the power well
+- * to be enabled, and it will only be disabled if none of the registers is
+- * requesting it to be enabled.
+- */
+-static void hsw_power_well_post_enable(struct drm_i915_private *dev_priv)
++static void gen7_setup_fixed_func_scheduler(struct drm_i915_private *dev_priv)
+ {
+-	struct drm_device *dev = dev_priv->dev;
++	uint32_t reg = I915_READ(GEN7_FF_THREAD_MODE);
+ 
+ 	/*
+-	 * After we re-enable the power well, if we touch VGA register 0x3d5
+-	 * we'll get unclaimed register interrupts. This stops after we write
+-	 * anything to the VGA MSR register. The vgacon module uses this
+-	 * register all the time, so if we unbind our driver and, as a
+-	 * consequence, bind vgacon, we'll get stuck in an infinite loop at
+-	 * console_unlock(). So make here we touch the VGA MSR register, making
+-	 * sure vgacon can keep working normally without triggering interrupts
+-	 * and error messages.
+-	 */
+-	vga_get_uninterruptible(dev->pdev, VGA_RSRC_LEGACY_IO);
+-	outb(inb(VGA_MSR_READ), VGA_MSR_WRITE);
+-	vga_put(dev->pdev, VGA_RSRC_LEGACY_IO);
+-
+-	if (IS_BROADWELL(dev))
+-		gen8_irq_power_well_post_enable(dev_priv);
+-}
+-
+-static void hsw_set_power_well(struct drm_i915_private *dev_priv,
+-			       struct i915_power_well *power_well, bool enable)
+-{
+-	bool is_enabled, enable_requested;
+-	uint32_t tmp;
+-
+-	tmp = I915_READ(HSW_PWR_WELL_DRIVER);
+-	is_enabled = tmp & HSW_PWR_WELL_STATE_ENABLED;
+-	enable_requested = tmp & HSW_PWR_WELL_ENABLE_REQUEST;
+-
+-	if (enable) {
+-		if (!enable_requested)
+-			I915_WRITE(HSW_PWR_WELL_DRIVER,
+-				   HSW_PWR_WELL_ENABLE_REQUEST);
+-
+-		if (!is_enabled) {
+-			DRM_DEBUG_KMS("Enabling power well\n");
+-			if (wait_for((I915_READ(HSW_PWR_WELL_DRIVER) &
+-				      HSW_PWR_WELL_STATE_ENABLED), 20))
+-				DRM_ERROR("Timeout enabling power well\n");
+-		}
++	 * WaVSThreadDispatchOverride:ivb,vlv
++	 *
++	 * This actually overrides the dispatch
++	 * mode for all thread types.
++	 */
++	reg &= ~GEN7_FF_SCHED_MASK;
++	reg |= GEN7_FF_TS_SCHED_HW;
++	reg |= GEN7_FF_VS_SCHED_HW;
++	reg |= GEN7_FF_DS_SCHED_HW;
+ 
+-		hsw_power_well_post_enable(dev_priv);
+-	} else {
+-		if (enable_requested) {
+-			I915_WRITE(HSW_PWR_WELL_DRIVER, 0);
+-			POSTING_READ(HSW_PWR_WELL_DRIVER);
+-			DRM_DEBUG_KMS("Requesting to disable the power well\n");
+-		}
+-	}
++	I915_WRITE(GEN7_FF_THREAD_MODE, reg);
+ }
+ 
+-static void hsw_power_well_sync_hw(struct drm_i915_private *dev_priv,
+-				   struct i915_power_well *power_well)
++static void lpt_init_clock_gating(struct drm_device *dev)
+ {
+-	hsw_set_power_well(dev_priv, power_well, power_well->count > 0);
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+ 	/*
+-	 * We're taking over the BIOS, so clear any requests made by it since
+-	 * the driver is in charge now.
++	 * TODO: this bit should only be enabled when really needed, then
++	 * disabled when not needed anymore in order to save power.
+ 	 */
+-	if (I915_READ(HSW_PWR_WELL_BIOS) & HSW_PWR_WELL_ENABLE_REQUEST)
+-		I915_WRITE(HSW_PWR_WELL_BIOS, 0);
+-}
++	if (dev_priv->pch_id == INTEL_PCH_LPT_LP_DEVICE_ID_TYPE)
++		I915_WRITE(SOUTH_DSPCLK_GATE_D,
++			   I915_READ(SOUTH_DSPCLK_GATE_D) |
++			   PCH_LP_PARTITION_LEVEL_DISABLE);
+ 
+-static void hsw_power_well_enable(struct drm_i915_private *dev_priv,
+-				  struct i915_power_well *power_well)
+-{
+-	hsw_set_power_well(dev_priv, power_well, true);
++	/* WADPOClockGatingDisable:hsw */
++	I915_WRITE(_TRANSA_CHICKEN1,
++		   I915_READ(_TRANSA_CHICKEN1) |
++		   TRANS_CHICKEN1_DP0UNIT_GC_DISABLE);
+ }
+ 
+-static void hsw_power_well_disable(struct drm_i915_private *dev_priv,
+-				   struct i915_power_well *power_well)
++static void lpt_suspend_hw(struct drm_device *dev)
+ {
+-	hsw_set_power_well(dev_priv, power_well, false);
+-}
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-static void i9xx_always_on_power_well_noop(struct drm_i915_private *dev_priv,
+-					   struct i915_power_well *power_well)
+-{
+-}
++	if (dev_priv->pch_id == INTEL_PCH_LPT_LP_DEVICE_ID_TYPE) {
++		uint32_t val = I915_READ(SOUTH_DSPCLK_GATE_D);
+ 
+-static bool i9xx_always_on_power_well_enabled(struct drm_i915_private *dev_priv,
+-					     struct i915_power_well *power_well)
+-{
+-	return true;
++		val &= ~PCH_LP_PARTITION_LEVEL_DISABLE;
++		I915_WRITE(SOUTH_DSPCLK_GATE_D, val);
++	}
+ }
+ 
+-static void vlv_set_power_well(struct drm_i915_private *dev_priv,
+-			       struct i915_power_well *power_well, bool enable)
++static void broadwell_init_clock_gating(struct drm_device *dev)
+ {
+-	enum punit_power_well power_well_id = power_well->data;
+-	u32 mask;
+-	u32 state;
+-	u32 ctrl;
+-
+-	mask = PUNIT_PWRGT_MASK(power_well_id);
+-	state = enable ? PUNIT_PWRGT_PWR_ON(power_well_id) :
+-			 PUNIT_PWRGT_PWR_GATE(power_well_id);
+-
+-	mutex_lock(&dev_priv->rps.hw_lock);
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	enum pipe pipe;
+ 
+-#define COND \
+-	((vlv_punit_read(dev_priv, PUNIT_REG_PWRGT_STATUS) & mask) == state)
++	I915_WRITE(WM3_LP_ILK, 0);
++	I915_WRITE(WM2_LP_ILK, 0);
++	I915_WRITE(WM1_LP_ILK, 0);
+ 
+-	if (COND)
+-		goto out;
++	/* WaSwitchSolVfFArbitrationPriority:bdw */
++	I915_WRITE(GAM_ECOCHK, I915_READ(GAM_ECOCHK) | HSW_ECOCHK_ARB_PRIO_SOL);
+ 
+-	ctrl = vlv_punit_read(dev_priv, PUNIT_REG_PWRGT_CTRL);
+-	ctrl &= ~mask;
+-	ctrl |= state;
+-	vlv_punit_write(dev_priv, PUNIT_REG_PWRGT_CTRL, ctrl);
+-
+-	if (wait_for(COND, 100))
+-		DRM_ERROR("timout setting power well state %08x (%08x)\n",
+-			  state,
+-			  vlv_punit_read(dev_priv, PUNIT_REG_PWRGT_CTRL));
++	/* WaPsrDPAMaskVBlankInSRD:bdw */
++	I915_WRITE(CHICKEN_PAR1_1,
++		   I915_READ(CHICKEN_PAR1_1) | DPA_MASK_VBLANK_SRD);
+ 
+-#undef COND
++	/* WaPsrDPRSUnmaskVBlankInSRD:bdw */
++	for_each_pipe(dev_priv, pipe) {
++		I915_WRITE(CHICKEN_PIPESL_1(pipe),
++			   I915_READ(CHICKEN_PIPESL_1(pipe)) |
++			   BDW_DPRS_MASK_VBLANK_SRD);
++	}
+ 
+-out:
+-	mutex_unlock(&dev_priv->rps.hw_lock);
+-}
++	/* WaVSRefCountFullforceMissDisable:bdw */
++	/* WaDSRefCountFullforceMissDisable:bdw */
++	I915_WRITE(GEN7_FF_THREAD_MODE,
++		   I915_READ(GEN7_FF_THREAD_MODE) &
++		   ~(GEN8_FF_DS_REF_CNT_FFME | GEN7_FF_VS_REF_CNT_FFME));
+ 
+-static void vlv_power_well_sync_hw(struct drm_i915_private *dev_priv,
+-				   struct i915_power_well *power_well)
+-{
+-	vlv_set_power_well(dev_priv, power_well, power_well->count > 0);
+-}
++	I915_WRITE(GEN6_RC_SLEEP_PSMI_CONTROL,
++		   _MASKED_BIT_ENABLE(GEN8_RC_SEMA_IDLE_MSG_DISABLE));
+ 
+-static void vlv_power_well_enable(struct drm_i915_private *dev_priv,
+-				  struct i915_power_well *power_well)
+-{
+-	vlv_set_power_well(dev_priv, power_well, true);
+-}
++	/* WaDisableSDEUnitClockGating:bdw */
++	I915_WRITE(GEN8_UCGCTL6, I915_READ(GEN8_UCGCTL6) |
++		   GEN8_SDEUNIT_CLOCK_GATE_DISABLE);
+ 
+-static void vlv_power_well_disable(struct drm_i915_private *dev_priv,
+-				   struct i915_power_well *power_well)
+-{
+-	vlv_set_power_well(dev_priv, power_well, false);
++	lpt_init_clock_gating(dev);
+ }
+ 
+-static bool vlv_power_well_enabled(struct drm_i915_private *dev_priv,
+-				   struct i915_power_well *power_well)
++static void haswell_init_clock_gating(struct drm_device *dev)
+ {
+-	int power_well_id = power_well->data;
+-	bool enabled = false;
+-	u32 mask;
+-	u32 state;
+-	u32 ctrl;
+-
+-	mask = PUNIT_PWRGT_MASK(power_well_id);
+-	ctrl = PUNIT_PWRGT_PWR_ON(power_well_id);
+-
+-	mutex_lock(&dev_priv->rps.hw_lock);
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	state = vlv_punit_read(dev_priv, PUNIT_REG_PWRGT_STATUS) & mask;
+-	/*
+-	 * We only ever set the power-on and power-gate states, anything
+-	 * else is unexpected.
+-	 */
+-	WARN_ON(state != PUNIT_PWRGT_PWR_ON(power_well_id) &&
+-		state != PUNIT_PWRGT_PWR_GATE(power_well_id));
+-	if (state == ctrl)
+-		enabled = true;
++	ilk_init_lp_watermarks(dev);
+ 
+-	/*
+-	 * A transient state at this point would mean some unexpected party
+-	 * is poking at the power controls too.
+-	 */
+-	ctrl = vlv_punit_read(dev_priv, PUNIT_REG_PWRGT_CTRL) & mask;
+-	WARN_ON(ctrl != state);
++	/* L3 caching of data atomics doesn't work -- disable it. */
++	I915_WRITE(HSW_SCRATCH1, HSW_SCRATCH1_L3_DATA_ATOMICS_DISABLE);
++	I915_WRITE(HSW_ROW_CHICKEN3,
++		   _MASKED_BIT_ENABLE(HSW_ROW_CHICKEN3_L3_GLOBAL_ATOMICS_DISABLE));
+ 
+-	mutex_unlock(&dev_priv->rps.hw_lock);
++	/* This is required by WaCatErrorRejectionIssue:hsw */
++	I915_WRITE(GEN7_SQ_CHICKEN_MBCUNIT_CONFIG,
++			I915_READ(GEN7_SQ_CHICKEN_MBCUNIT_CONFIG) |
++			GEN7_SQ_CHICKEN_MBCUNIT_SQINTMOB);
+ 
+-	return enabled;
+-}
++	/* WaVSRefCountFullforceMissDisable:hsw */
++	I915_WRITE(GEN7_FF_THREAD_MODE,
++		   I915_READ(GEN7_FF_THREAD_MODE) & ~GEN7_FF_VS_REF_CNT_FFME);
+ 
+-static void vlv_display_power_well_enable(struct drm_i915_private *dev_priv,
+-					  struct i915_power_well *power_well)
+-{
+-	WARN_ON_ONCE(power_well->data != PUNIT_POWER_WELL_DISP2D);
++	/* WaDisable_RenderCache_OperationalFlush:hsw */
++	I915_WRITE(CACHE_MODE_0_GEN7, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
+ 
+-	vlv_set_power_well(dev_priv, power_well, true);
++	/* enable HiZ Raw Stall Optimization */
++	I915_WRITE(CACHE_MODE_0_GEN7,
++		   _MASKED_BIT_DISABLE(HIZ_RAW_STALL_OPT_DISABLE));
+ 
+-	spin_lock_irq(&dev_priv->irq_lock);
+-	valleyview_enable_display_irqs(dev_priv);
+-	spin_unlock_irq(&dev_priv->irq_lock);
++	/* WaDisable4x2SubspanOptimization:hsw */
++	I915_WRITE(CACHE_MODE_1,
++		   _MASKED_BIT_ENABLE(PIXEL_SUBSPAN_COLLECT_OPT_DISABLE));
+ 
+ 	/*
+-	 * During driver initialization/resume we can avoid restoring the
+-	 * part of the HW/SW state that will be inited anyway explicitly.
++	 * BSpec recommends 8x4 when MSAA is used,
++	 * however in practice 16x4 seems fastest.
++	 *
++	 * Note that PS/WM thread counts depend on the WIZ hashing
++	 * disable bit, which we don't touch here, but it's good
++	 * to keep in mind (see 3DSTATE_PS and 3DSTATE_WM).
+ 	 */
+-	if (dev_priv->power_domains.initializing)
+-		return;
+-
+-	intel_hpd_init(dev_priv->dev);
+-
+-	i915_redisable_vga_power_on(dev_priv->dev);
+-}
++	I915_WRITE(GEN7_GT_MODE,
++		   GEN6_WIZ_HASHING_MASK | GEN6_WIZ_HASHING_16x4);
+ 
+-static void vlv_display_power_well_disable(struct drm_i915_private *dev_priv,
+-					   struct i915_power_well *power_well)
+-{
+-	WARN_ON_ONCE(power_well->data != PUNIT_POWER_WELL_DISP2D);
++	/* WaSwitchSolVfFArbitrationPriority:hsw */
++	I915_WRITE(GAM_ECOCHK, I915_READ(GAM_ECOCHK) | HSW_ECOCHK_ARB_PRIO_SOL);
+ 
+-	spin_lock_irq(&dev_priv->irq_lock);
+-	valleyview_disable_display_irqs(dev_priv);
+-	spin_unlock_irq(&dev_priv->irq_lock);
++	/* WaRsPkgCStateDisplayPMReq:hsw */
++	I915_WRITE(CHICKEN_PAR1_1,
++		   I915_READ(CHICKEN_PAR1_1) | FORCE_ARB_IDLE_PLANES);
+ 
+-	vlv_set_power_well(dev_priv, power_well, false);
++	lpt_init_clock_gating(dev);
+ }
+ 
+-static void vlv_dpio_cmn_power_well_enable(struct drm_i915_private *dev_priv,
+-					   struct i915_power_well *power_well)
++static void ivybridge_init_clock_gating(struct drm_device *dev)
+ {
+-	WARN_ON_ONCE(power_well->data != PUNIT_POWER_WELL_DPIO_CMN_BC);
+-
+-	/*
+-	 * Enable the CRI clock source so we can get at the
+-	 * display and the reference clock for VGA
+-	 * hotplug / manual detection.
+-	 */
+-	I915_WRITE(DPLL(PIPE_B), I915_READ(DPLL(PIPE_B)) |
+-		   DPLL_REFA_CLK_ENABLE_VLV | DPLL_INTEGRATED_CRI_CLK_VLV);
+-	udelay(1); /* >10ns for cmnreset, >0ns for sidereset */
+-
+-	vlv_set_power_well(dev_priv, power_well, true);
+-
+-	/*
+-	 * From VLV2A0_DP_eDP_DPIO_driver_vbios_notes_10.docx -
+-	 *  6.	De-assert cmn_reset/side_reset. Same as VLV X0.
+-	 *   a.	GUnit 0x2110 bit[0] set to 1 (def 0)
+-	 *   b.	The other bits such as sfr settings / modesel may all
+-	 *	be set to 0.
+-	 *
+-	 * This should only be done on init and resume from S3 with
+-	 * both PLLs disabled, or we risk losing DPIO and PLL
+-	 * synchronization.
+-	 */
+-	I915_WRITE(DPIO_CTL, I915_READ(DPIO_CTL) | DPIO_CMNRST);
+-}
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	uint32_t snpcr;
+ 
+-static void vlv_dpio_cmn_power_well_disable(struct drm_i915_private *dev_priv,
+-					    struct i915_power_well *power_well)
+-{
+-	struct drm_device *dev = dev_priv->dev;
+-	enum pipe pipe;
++	ilk_init_lp_watermarks(dev);
+ 
+-	WARN_ON_ONCE(power_well->data != PUNIT_POWER_WELL_DPIO_CMN_BC);
++	I915_WRITE(ILK_DSPCLK_GATE_D, ILK_VRHUNIT_CLOCK_GATE_DISABLE);
+ 
+-	for_each_pipe(pipe)
+-		assert_pll_disabled(dev_priv, pipe);
++	/* WaDisableEarlyCull:ivb */
++	I915_WRITE(_3D_CHICKEN3,
++		   _MASKED_BIT_ENABLE(_3D_CHICKEN_SF_DISABLE_OBJEND_CULL));
+ 
+-	/* Assert common reset */
+-	I915_WRITE(DPIO_CTL, I915_READ(DPIO_CTL) & ~DPIO_CMNRST);
++	/* WaDisableBackToBackFlipFix:ivb */
++	I915_WRITE(IVB_CHICKEN3,
++		   CHICKEN3_DGMG_REQ_OUT_FIX_DISABLE |
++		   CHICKEN3_DGMG_DONE_FIX_DISABLE);
+ 
+-	vlv_set_power_well(dev_priv, power_well, false);
+-}
++	/* WaDisablePSDDualDispatchEnable:ivb */
++	if (IS_IVB_GT1(dev))
++		I915_WRITE(GEN7_HALF_SLICE_CHICKEN1,
++			   _MASKED_BIT_ENABLE(GEN7_PSD_SINGLE_PORT_DISPATCH_ENABLE));
+ 
+-static void check_power_well_state(struct drm_i915_private *dev_priv,
+-				   struct i915_power_well *power_well)
+-{
+-	bool enabled = power_well->ops->is_enabled(dev_priv, power_well);
++	/* WaDisable_RenderCache_OperationalFlush:ivb */
++	I915_WRITE(CACHE_MODE_0_GEN7, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
+ 
+-	if (power_well->always_on || !i915.disable_power_well) {
+-		if (!enabled)
+-			goto mismatch;
++	/* Apply the WaDisableRHWOOptimizationForRenderHang:ivb workaround. */
++	I915_WRITE(GEN7_COMMON_SLICE_CHICKEN1,
++		   GEN7_CSC1_RHWO_OPT_DISABLE_IN_RCC);
+ 
+-		return;
++	/* WaApplyL3ControlAndL3ChickenMode:ivb */
++	I915_WRITE(GEN7_L3CNTLREG1,
++			GEN7_WA_FOR_GEN7_L3_CONTROL);
++	I915_WRITE(GEN7_L3_CHICKEN_MODE_REGISTER,
++		   GEN7_WA_L3_CHICKEN_MODE);
++	if (IS_IVB_GT1(dev))
++		I915_WRITE(GEN7_ROW_CHICKEN2,
++			   _MASKED_BIT_ENABLE(DOP_CLOCK_GATING_DISABLE));
++	else {
++		/* must write both registers */
++		I915_WRITE(GEN7_ROW_CHICKEN2,
++			   _MASKED_BIT_ENABLE(DOP_CLOCK_GATING_DISABLE));
++		I915_WRITE(GEN7_ROW_CHICKEN2_GT2,
++			   _MASKED_BIT_ENABLE(DOP_CLOCK_GATING_DISABLE));
+ 	}
+ 
+-	if (enabled != (power_well->count > 0))
+-		goto mismatch;
++	/* WaForceL3Serialization:ivb */
++	I915_WRITE(GEN7_L3SQCREG4, I915_READ(GEN7_L3SQCREG4) &
++		   ~L3SQ_URB_READ_CAM_MATCH_DISABLE);
+ 
+-	return;
++	/*
++	 * According to the spec, bit 13 (RCZUNIT) must be set on IVB.
++	 * This implements the WaDisableRCZUnitClockGating:ivb workaround.
++	 */
++	I915_WRITE(GEN6_UCGCTL2,
++		   GEN6_RCZUNIT_CLOCK_GATE_DISABLE);
+ 
+-mismatch:
+-	WARN(1, "state mismatch for '%s' (always_on %d hw state %d use-count %d disable_power_well %d\n",
+-		  power_well->name, power_well->always_on, enabled,
+-		  power_well->count, i915.disable_power_well);
+-}
++	/* This is required by WaCatErrorRejectionIssue:ivb */
++	I915_WRITE(GEN7_SQ_CHICKEN_MBCUNIT_CONFIG,
++			I915_READ(GEN7_SQ_CHICKEN_MBCUNIT_CONFIG) |
++			GEN7_SQ_CHICKEN_MBCUNIT_SQINTMOB);
+ 
+-void intel_display_power_get(struct drm_i915_private *dev_priv,
+-			     enum intel_display_power_domain domain)
+-{
+-	struct i915_power_domains *power_domains;
+-	struct i915_power_well *power_well;
+-	int i;
++	g4x_disable_trickle_feed(dev);
+ 
+-	intel_runtime_pm_get(dev_priv);
++	gen7_setup_fixed_func_scheduler(dev_priv);
+ 
+-	power_domains = &dev_priv->power_domains;
++	if (0) { /* causes HiZ corruption on ivb:gt1 */
++		/* enable HiZ Raw Stall Optimization */
++		I915_WRITE(CACHE_MODE_0_GEN7,
++			   _MASKED_BIT_DISABLE(HIZ_RAW_STALL_OPT_DISABLE));
++	}
+ 
+-	mutex_lock(&power_domains->lock);
++	/* WaDisable4x2SubspanOptimization:ivb */
++	I915_WRITE(CACHE_MODE_1,
++		   _MASKED_BIT_ENABLE(PIXEL_SUBSPAN_COLLECT_OPT_DISABLE));
+ 
+-	for_each_power_well(i, power_well, BIT(domain), power_domains) {
+-		if (!power_well->count++) {
+-			DRM_DEBUG_KMS("enabling %s\n", power_well->name);
+-			power_well->ops->enable(dev_priv, power_well);
+-			power_well->hw_enabled = true;
+-		}
++	/*
++	 * BSpec recommends 8x4 when MSAA is used,
++	 * however in practice 16x4 seems fastest.
++	 *
++	 * Note that PS/WM thread counts depend on the WIZ hashing
++	 * disable bit, which we don't touch here, but it's good
++	 * to keep in mind (see 3DSTATE_PS and 3DSTATE_WM).
++	 */
++	I915_WRITE(GEN7_GT_MODE,
++		   GEN6_WIZ_HASHING_MASK | GEN6_WIZ_HASHING_16x4);
+ 
+-		check_power_well_state(dev_priv, power_well);
+-	}
++	snpcr = I915_READ(GEN6_MBCUNIT_SNPCR);
++	snpcr &= ~GEN6_MBC_SNPCR_MASK;
++	snpcr |= GEN6_MBC_SNPCR_MED;
++	I915_WRITE(GEN6_MBCUNIT_SNPCR, snpcr);
+ 
+-	power_domains->domain_use_count[domain]++;
++	if (!HAS_PCH_NOP(dev))
++		cpt_init_clock_gating(dev);
+ 
+-	mutex_unlock(&power_domains->lock);
++	gen6_check_mch_setup(dev);
+ }
+ 
+-void intel_display_power_put(struct drm_i915_private *dev_priv,
+-			     enum intel_display_power_domain domain)
++static void valleyview_init_clock_gating(struct drm_device *dev)
+ {
+-	struct i915_power_domains *power_domains;
+-	struct i915_power_well *power_well;
+-	int i;
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	power_domains = &dev_priv->power_domains;
++	I915_WRITE(DSPCLK_GATE_D, VRHUNIT_CLOCK_GATE_DISABLE);
+ 
+-	mutex_lock(&power_domains->lock);
++	/* WaDisableEarlyCull:vlv */
++	I915_WRITE(_3D_CHICKEN3,
++		   _MASKED_BIT_ENABLE(_3D_CHICKEN_SF_DISABLE_OBJEND_CULL));
+ 
+-	WARN_ON(!power_domains->domain_use_count[domain]);
+-	power_domains->domain_use_count[domain]--;
++	/* WaDisableBackToBackFlipFix:vlv */
++	I915_WRITE(IVB_CHICKEN3,
++		   CHICKEN3_DGMG_REQ_OUT_FIX_DISABLE |
++		   CHICKEN3_DGMG_DONE_FIX_DISABLE);
+ 
+-	for_each_power_well_rev(i, power_well, BIT(domain), power_domains) {
+-		WARN_ON(!power_well->count);
++	/* WaPsdDispatchEnable:vlv */
++	/* WaDisablePSDDualDispatchEnable:vlv */
++	I915_WRITE(GEN7_HALF_SLICE_CHICKEN1,
++		   _MASKED_BIT_ENABLE(GEN7_MAX_PS_THREAD_DEP |
++				      GEN7_PSD_SINGLE_PORT_DISPATCH_ENABLE));
+ 
+-		if (!--power_well->count && i915.disable_power_well) {
+-			DRM_DEBUG_KMS("disabling %s\n", power_well->name);
+-			power_well->hw_enabled = false;
+-			power_well->ops->disable(dev_priv, power_well);
+-		}
++	/* WaDisable_RenderCache_OperationalFlush:vlv */
++	I915_WRITE(CACHE_MODE_0_GEN7, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
+ 
+-		check_power_well_state(dev_priv, power_well);
+-	}
++	/* WaForceL3Serialization:vlv */
++	I915_WRITE(GEN7_L3SQCREG4, I915_READ(GEN7_L3SQCREG4) &
++		   ~L3SQ_URB_READ_CAM_MATCH_DISABLE);
+ 
+-	mutex_unlock(&power_domains->lock);
++	/* WaDisableDopClockGating:vlv */
++	I915_WRITE(GEN7_ROW_CHICKEN2,
++		   _MASKED_BIT_ENABLE(DOP_CLOCK_GATING_DISABLE));
+ 
+-	intel_runtime_pm_put(dev_priv);
+-}
++	/* This is required by WaCatErrorRejectionIssue:vlv */
++	I915_WRITE(GEN7_SQ_CHICKEN_MBCUNIT_CONFIG,
++		   I915_READ(GEN7_SQ_CHICKEN_MBCUNIT_CONFIG) |
++		   GEN7_SQ_CHICKEN_MBCUNIT_SQINTMOB);
+ 
+-static struct i915_power_domains *hsw_pwr;
++	gen7_setup_fixed_func_scheduler(dev_priv);
+ 
+-/* Display audio driver power well request */
+-int i915_request_power_well(void)
+-{
+-	struct drm_i915_private *dev_priv;
++	/*
++	 * According to the spec, bit 13 (RCZUNIT) must be set on IVB.
++	 * This implements the WaDisableRCZUnitClockGating:vlv workaround.
++	 */
++	I915_WRITE(GEN6_UCGCTL2,
++		   GEN6_RCZUNIT_CLOCK_GATE_DISABLE);
+ 
+-	if (!hsw_pwr)
+-		return -ENODEV;
++	/* WaDisableL3Bank2xClockGate:vlv
++	 * Disabling L3 clock gating- MMIO 940c[25] = 1
++	 * Set bit 25, to disable L3_BANK_2x_CLK_GATING */
++	I915_WRITE(GEN7_UCGCTL4,
++		   I915_READ(GEN7_UCGCTL4) | GEN7_L3BANK2X_CLOCK_GATE_DISABLE);
+ 
+-	dev_priv = container_of(hsw_pwr, struct drm_i915_private,
+-				power_domains);
+-	intel_display_power_get(dev_priv, POWER_DOMAIN_AUDIO);
+-	return 0;
+-}
+-EXPORT_SYMBOL_GPL(i915_request_power_well);
++	I915_WRITE(MI_ARB_VLV, MI_ARB_DISPLAY_TRICKLE_FEED_DISABLE);
+ 
+-/* Display audio driver power well release */
+-int i915_release_power_well(void)
+-{
+-	struct drm_i915_private *dev_priv;
++	/*
++	 * BSpec says this must be set, even though
++	 * WaDisable4x2SubspanOptimization isn't listed for VLV.
++	 */
++	I915_WRITE(CACHE_MODE_1,
++		   _MASKED_BIT_ENABLE(PIXEL_SUBSPAN_COLLECT_OPT_DISABLE));
+ 
+-	if (!hsw_pwr)
+-		return -ENODEV;
++	/*
++	 * WaIncreaseL3CreditsForVLVB0:vlv
++	 * This is the hardware default actually.
++	 */
++	I915_WRITE(GEN7_L3SQCREG1, VLV_B0_WA_L3SQCREG1_VALUE);
+ 
+-	dev_priv = container_of(hsw_pwr, struct drm_i915_private,
+-				power_domains);
+-	intel_display_power_put(dev_priv, POWER_DOMAIN_AUDIO);
+-	return 0;
++	/*
++	 * WaDisableVLVClockGating_VBIIssue:vlv
++	 * Disable clock gating on th GCFG unit to prevent a delay
++	 * in the reporting of vblank events.
++	 */
++	I915_WRITE(VLV_GUNIT_CLOCK_GATE, GCFG_DIS);
+ }
+-EXPORT_SYMBOL_GPL(i915_release_power_well);
+ 
+-/*
+- * Private interface for the audio driver to get CDCLK in kHz.
+- *
+- * Caller must request power well using i915_request_power_well() prior to
+- * making the call.
+- */
+-int i915_get_cdclk_freq(void)
++static void cherryview_init_clock_gating(struct drm_device *dev)
+ {
+-	struct drm_i915_private *dev_priv;
+-
+-	if (!hsw_pwr)
+-		return -ENODEV;
+-
+-	dev_priv = container_of(hsw_pwr, struct drm_i915_private,
+-				power_domains);
+-
+-	return intel_ddi_get_cdclk_freq(dev_priv);
+-}
+-EXPORT_SYMBOL_GPL(i915_get_cdclk_freq);
+-
+-
+-#define POWER_DOMAIN_MASK (BIT(POWER_DOMAIN_NUM) - 1)
+-
+-#define HSW_ALWAYS_ON_POWER_DOMAINS (			\
+-	BIT(POWER_DOMAIN_PIPE_A) |			\
+-	BIT(POWER_DOMAIN_TRANSCODER_EDP) |		\
+-	BIT(POWER_DOMAIN_PORT_DDI_A_2_LANES) |		\
+-	BIT(POWER_DOMAIN_PORT_DDI_A_4_LANES) |		\
+-	BIT(POWER_DOMAIN_PORT_DDI_B_2_LANES) |		\
+-	BIT(POWER_DOMAIN_PORT_DDI_B_4_LANES) |		\
+-	BIT(POWER_DOMAIN_PORT_DDI_C_2_LANES) |		\
+-	BIT(POWER_DOMAIN_PORT_DDI_C_4_LANES) |		\
+-	BIT(POWER_DOMAIN_PORT_DDI_D_2_LANES) |		\
+-	BIT(POWER_DOMAIN_PORT_DDI_D_4_LANES) |		\
+-	BIT(POWER_DOMAIN_PORT_CRT) |			\
+-	BIT(POWER_DOMAIN_PLLS) |			\
+-	BIT(POWER_DOMAIN_INIT))
+-#define HSW_DISPLAY_POWER_DOMAINS (				\
+-	(POWER_DOMAIN_MASK & ~HSW_ALWAYS_ON_POWER_DOMAINS) |	\
+-	BIT(POWER_DOMAIN_INIT))
+-
+-#define BDW_ALWAYS_ON_POWER_DOMAINS (			\
+-	HSW_ALWAYS_ON_POWER_DOMAINS |			\
+-	BIT(POWER_DOMAIN_PIPE_A_PANEL_FITTER))
+-#define BDW_DISPLAY_POWER_DOMAINS (				\
+-	(POWER_DOMAIN_MASK & ~BDW_ALWAYS_ON_POWER_DOMAINS) |	\
+-	BIT(POWER_DOMAIN_INIT))
+-
+-#define VLV_ALWAYS_ON_POWER_DOMAINS	BIT(POWER_DOMAIN_INIT)
+-#define VLV_DISPLAY_POWER_DOMAINS	POWER_DOMAIN_MASK
+-
+-#define VLV_DPIO_CMN_BC_POWER_DOMAINS (		\
+-	BIT(POWER_DOMAIN_PORT_DDI_B_2_LANES) |	\
+-	BIT(POWER_DOMAIN_PORT_DDI_B_4_LANES) |	\
+-	BIT(POWER_DOMAIN_PORT_DDI_C_2_LANES) |	\
+-	BIT(POWER_DOMAIN_PORT_DDI_C_4_LANES) |	\
+-	BIT(POWER_DOMAIN_PORT_CRT) |		\
+-	BIT(POWER_DOMAIN_INIT))
+-
+-#define VLV_DPIO_TX_B_LANES_01_POWER_DOMAINS (	\
+-	BIT(POWER_DOMAIN_PORT_DDI_B_2_LANES) |	\
+-	BIT(POWER_DOMAIN_PORT_DDI_B_4_LANES) |	\
+-	BIT(POWER_DOMAIN_INIT))
+-
+-#define VLV_DPIO_TX_B_LANES_23_POWER_DOMAINS (	\
+-	BIT(POWER_DOMAIN_PORT_DDI_B_4_LANES) |	\
+-	BIT(POWER_DOMAIN_INIT))
+-
+-#define VLV_DPIO_TX_C_LANES_01_POWER_DOMAINS (	\
+-	BIT(POWER_DOMAIN_PORT_DDI_C_2_LANES) |	\
+-	BIT(POWER_DOMAIN_PORT_DDI_C_4_LANES) |	\
+-	BIT(POWER_DOMAIN_INIT))
+-
+-#define VLV_DPIO_TX_C_LANES_23_POWER_DOMAINS (	\
+-	BIT(POWER_DOMAIN_PORT_DDI_C_4_LANES) |	\
+-	BIT(POWER_DOMAIN_INIT))
+-
+-static const struct i915_power_well_ops i9xx_always_on_power_well_ops = {
+-	.sync_hw = i9xx_always_on_power_well_noop,
+-	.enable = i9xx_always_on_power_well_noop,
+-	.disable = i9xx_always_on_power_well_noop,
+-	.is_enabled = i9xx_always_on_power_well_enabled,
+-};
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-static struct i915_power_well i9xx_always_on_power_well[] = {
+-	{
+-		.name = "always-on",
+-		.always_on = 1,
+-		.domains = POWER_DOMAIN_MASK,
+-		.ops = &i9xx_always_on_power_well_ops,
+-	},
+-};
++	I915_WRITE(DSPCLK_GATE_D, VRHUNIT_CLOCK_GATE_DISABLE);
+ 
+-static const struct i915_power_well_ops hsw_power_well_ops = {
+-	.sync_hw = hsw_power_well_sync_hw,
+-	.enable = hsw_power_well_enable,
+-	.disable = hsw_power_well_disable,
+-	.is_enabled = hsw_power_well_enabled,
+-};
++	I915_WRITE(MI_ARB_VLV, MI_ARB_DISPLAY_TRICKLE_FEED_DISABLE);
+ 
+-static struct i915_power_well hsw_power_wells[] = {
+-	{
+-		.name = "always-on",
+-		.always_on = 1,
+-		.domains = HSW_ALWAYS_ON_POWER_DOMAINS,
+-		.ops = &i9xx_always_on_power_well_ops,
+-	},
+-	{
+-		.name = "display",
+-		.domains = HSW_DISPLAY_POWER_DOMAINS,
+-		.ops = &hsw_power_well_ops,
+-	},
+-};
++	/* WaVSRefCountFullforceMissDisable:chv */
++	/* WaDSRefCountFullforceMissDisable:chv */
++	I915_WRITE(GEN7_FF_THREAD_MODE,
++		   I915_READ(GEN7_FF_THREAD_MODE) &
++		   ~(GEN8_FF_DS_REF_CNT_FFME | GEN7_FF_VS_REF_CNT_FFME));
+ 
+-static struct i915_power_well bdw_power_wells[] = {
+-	{
+-		.name = "always-on",
+-		.always_on = 1,
+-		.domains = BDW_ALWAYS_ON_POWER_DOMAINS,
+-		.ops = &i9xx_always_on_power_well_ops,
+-	},
+-	{
+-		.name = "display",
+-		.domains = BDW_DISPLAY_POWER_DOMAINS,
+-		.ops = &hsw_power_well_ops,
+-	},
+-};
++	/* WaDisableSemaphoreAndSyncFlipWait:chv */
++	I915_WRITE(GEN6_RC_SLEEP_PSMI_CONTROL,
++		   _MASKED_BIT_ENABLE(GEN8_RC_SEMA_IDLE_MSG_DISABLE));
+ 
+-static const struct i915_power_well_ops vlv_display_power_well_ops = {
+-	.sync_hw = vlv_power_well_sync_hw,
+-	.enable = vlv_display_power_well_enable,
+-	.disable = vlv_display_power_well_disable,
+-	.is_enabled = vlv_power_well_enabled,
+-};
++	/* WaDisableCSUnitClockGating:chv */
++	I915_WRITE(GEN6_UCGCTL1, I915_READ(GEN6_UCGCTL1) |
++		   GEN6_CSUNIT_CLOCK_GATE_DISABLE);
+ 
+-static const struct i915_power_well_ops vlv_dpio_cmn_power_well_ops = {
+-	.sync_hw = vlv_power_well_sync_hw,
+-	.enable = vlv_dpio_cmn_power_well_enable,
+-	.disable = vlv_dpio_cmn_power_well_disable,
+-	.is_enabled = vlv_power_well_enabled,
+-};
++	/* WaDisableSDEUnitClockGating:chv */
++	I915_WRITE(GEN8_UCGCTL6, I915_READ(GEN8_UCGCTL6) |
++		   GEN8_SDEUNIT_CLOCK_GATE_DISABLE);
++}
+ 
+-static const struct i915_power_well_ops vlv_dpio_power_well_ops = {
+-	.sync_hw = vlv_power_well_sync_hw,
+-	.enable = vlv_power_well_enable,
+-	.disable = vlv_power_well_disable,
+-	.is_enabled = vlv_power_well_enabled,
+-};
++static void g4x_init_clock_gating(struct drm_device *dev)
++{
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	uint32_t dspclk_gate;
+ 
+-static struct i915_power_well vlv_power_wells[] = {
+-	{
+-		.name = "always-on",
+-		.always_on = 1,
+-		.domains = VLV_ALWAYS_ON_POWER_DOMAINS,
+-		.ops = &i9xx_always_on_power_well_ops,
+-	},
+-	{
+-		.name = "display",
+-		.domains = VLV_DISPLAY_POWER_DOMAINS,
+-		.data = PUNIT_POWER_WELL_DISP2D,
+-		.ops = &vlv_display_power_well_ops,
+-	},
+-	{
+-		.name = "dpio-tx-b-01",
+-		.domains = VLV_DPIO_TX_B_LANES_01_POWER_DOMAINS |
+-			   VLV_DPIO_TX_B_LANES_23_POWER_DOMAINS |
+-			   VLV_DPIO_TX_C_LANES_01_POWER_DOMAINS |
+-			   VLV_DPIO_TX_C_LANES_23_POWER_DOMAINS,
+-		.ops = &vlv_dpio_power_well_ops,
+-		.data = PUNIT_POWER_WELL_DPIO_TX_B_LANES_01,
+-	},
+-	{
+-		.name = "dpio-tx-b-23",
+-		.domains = VLV_DPIO_TX_B_LANES_01_POWER_DOMAINS |
+-			   VLV_DPIO_TX_B_LANES_23_POWER_DOMAINS |
+-			   VLV_DPIO_TX_C_LANES_01_POWER_DOMAINS |
+-			   VLV_DPIO_TX_C_LANES_23_POWER_DOMAINS,
+-		.ops = &vlv_dpio_power_well_ops,
+-		.data = PUNIT_POWER_WELL_DPIO_TX_B_LANES_23,
+-	},
+-	{
+-		.name = "dpio-tx-c-01",
+-		.domains = VLV_DPIO_TX_B_LANES_01_POWER_DOMAINS |
+-			   VLV_DPIO_TX_B_LANES_23_POWER_DOMAINS |
+-			   VLV_DPIO_TX_C_LANES_01_POWER_DOMAINS |
+-			   VLV_DPIO_TX_C_LANES_23_POWER_DOMAINS,
+-		.ops = &vlv_dpio_power_well_ops,
+-		.data = PUNIT_POWER_WELL_DPIO_TX_C_LANES_01,
+-	},
+-	{
+-		.name = "dpio-tx-c-23",
+-		.domains = VLV_DPIO_TX_B_LANES_01_POWER_DOMAINS |
+-			   VLV_DPIO_TX_B_LANES_23_POWER_DOMAINS |
+-			   VLV_DPIO_TX_C_LANES_01_POWER_DOMAINS |
+-			   VLV_DPIO_TX_C_LANES_23_POWER_DOMAINS,
+-		.ops = &vlv_dpio_power_well_ops,
+-		.data = PUNIT_POWER_WELL_DPIO_TX_C_LANES_23,
+-	},
+-	{
+-		.name = "dpio-common",
+-		.domains = VLV_DPIO_CMN_BC_POWER_DOMAINS,
+-		.data = PUNIT_POWER_WELL_DPIO_CMN_BC,
+-		.ops = &vlv_dpio_cmn_power_well_ops,
+-	},
+-};
++	I915_WRITE(RENCLK_GATE_D1, 0);
++	I915_WRITE(RENCLK_GATE_D2, VF_UNIT_CLOCK_GATE_DISABLE |
++		   GS_UNIT_CLOCK_GATE_DISABLE |
++		   CL_UNIT_CLOCK_GATE_DISABLE);
++	I915_WRITE(RAMCLK_GATE_D, 0);
++	dspclk_gate = VRHUNIT_CLOCK_GATE_DISABLE |
++		OVRUNIT_CLOCK_GATE_DISABLE |
++		OVCUNIT_CLOCK_GATE_DISABLE;
++	if (IS_GM45(dev))
++		dspclk_gate |= DSSUNIT_CLOCK_GATE_DISABLE;
++	I915_WRITE(DSPCLK_GATE_D, dspclk_gate);
+ 
+-static struct i915_power_well *lookup_power_well(struct drm_i915_private *dev_priv,
+-						 enum punit_power_well power_well_id)
+-{
+-	struct i915_power_domains *power_domains = &dev_priv->power_domains;
+-	struct i915_power_well *power_well;
+-	int i;
++	/* WaDisableRenderCachePipelinedFlush */
++	I915_WRITE(CACHE_MODE_0,
++		   _MASKED_BIT_ENABLE(CM0_PIPELINED_RENDER_FLUSH_DISABLE));
+ 
+-	for_each_power_well(i, power_well, POWER_DOMAIN_MASK, power_domains) {
+-		if (power_well->data == power_well_id)
+-			return power_well;
+-	}
++	/* WaDisable_RenderCache_OperationalFlush:g4x */
++	I915_WRITE(CACHE_MODE_0, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
+ 
+-	return NULL;
++	g4x_disable_trickle_feed(dev);
+ }
+ 
+-#define set_power_wells(power_domains, __power_wells) ({		\
+-	(power_domains)->power_wells = (__power_wells);			\
+-	(power_domains)->power_well_count = ARRAY_SIZE(__power_wells);	\
+-})
+-
+-int intel_power_domains_init(struct drm_i915_private *dev_priv)
++static void crestline_init_clock_gating(struct drm_device *dev)
+ {
+-	struct i915_power_domains *power_domains = &dev_priv->power_domains;
+-
+-	mutex_init(&power_domains->lock);
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	/*
+-	 * The enabling order will be from lower to higher indexed wells,
+-	 * the disabling order is reversed.
+-	 */
+-	if (IS_HASWELL(dev_priv->dev)) {
+-		set_power_wells(power_domains, hsw_power_wells);
+-		hsw_pwr = power_domains;
+-	} else if (IS_BROADWELL(dev_priv->dev)) {
+-		set_power_wells(power_domains, bdw_power_wells);
+-		hsw_pwr = power_domains;
+-	} else if (IS_VALLEYVIEW(dev_priv->dev)) {
+-		set_power_wells(power_domains, vlv_power_wells);
+-	} else {
+-		set_power_wells(power_domains, i9xx_always_on_power_well);
+-	}
++	I915_WRITE(RENCLK_GATE_D1, I965_RCC_CLOCK_GATE_DISABLE);
++	I915_WRITE(RENCLK_GATE_D2, 0);
++	I915_WRITE(DSPCLK_GATE_D, 0);
++	I915_WRITE(RAMCLK_GATE_D, 0);
++	I915_WRITE16(DEUC, 0);
++	I915_WRITE(MI_ARB_STATE,
++		   _MASKED_BIT_ENABLE(MI_ARB_DISPLAY_TRICKLE_FEED_DISABLE));
+ 
+-	return 0;
++	/* WaDisable_RenderCache_OperationalFlush:gen4 */
++	I915_WRITE(CACHE_MODE_0, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
+ }
+ 
+-void intel_power_domains_remove(struct drm_i915_private *dev_priv)
++static void broadwater_init_clock_gating(struct drm_device *dev)
+ {
+-	hsw_pwr = NULL;
+-}
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-static void intel_power_domains_resume(struct drm_i915_private *dev_priv)
+-{
+-	struct i915_power_domains *power_domains = &dev_priv->power_domains;
+-	struct i915_power_well *power_well;
+-	int i;
++	I915_WRITE(RENCLK_GATE_D1, I965_RCZ_CLOCK_GATE_DISABLE |
++		   I965_RCC_CLOCK_GATE_DISABLE |
++		   I965_RCPB_CLOCK_GATE_DISABLE |
++		   I965_ISC_CLOCK_GATE_DISABLE |
++		   I965_FBC_CLOCK_GATE_DISABLE);
++	I915_WRITE(RENCLK_GATE_D2, 0);
++	I915_WRITE(MI_ARB_STATE,
++		   _MASKED_BIT_ENABLE(MI_ARB_DISPLAY_TRICKLE_FEED_DISABLE));
+ 
+-	mutex_lock(&power_domains->lock);
+-	for_each_power_well(i, power_well, POWER_DOMAIN_MASK, power_domains) {
+-		power_well->ops->sync_hw(dev_priv, power_well);
+-		power_well->hw_enabled = power_well->ops->is_enabled(dev_priv,
+-								     power_well);
+-	}
+-	mutex_unlock(&power_domains->lock);
++	/* WaDisable_RenderCache_OperationalFlush:gen4 */
++	I915_WRITE(CACHE_MODE_0, _MASKED_BIT_DISABLE(RC_OP_FLUSH_ENABLE));
+ }
+ 
+-static void vlv_cmnlane_wa(struct drm_i915_private *dev_priv)
++static void gen3_init_clock_gating(struct drm_device *dev)
+ {
+-	struct i915_power_well *cmn =
+-		lookup_power_well(dev_priv, PUNIT_POWER_WELL_DPIO_CMN_BC);
+-	struct i915_power_well *disp2d =
+-		lookup_power_well(dev_priv, PUNIT_POWER_WELL_DISP2D);
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	u32 dstate = I915_READ(D_STATE);
+ 
+-	/* nothing to do if common lane is already off */
+-	if (!cmn->ops->is_enabled(dev_priv, cmn))
+-		return;
++	dstate |= DSTATE_PLL_D3_OFF | DSTATE_GFX_CLOCK_GATING |
++		DSTATE_DOT_CLOCK_GATING;
++	I915_WRITE(D_STATE, dstate);
+ 
+-	/* If the display might be already active skip this */
+-	if (disp2d->ops->is_enabled(dev_priv, disp2d) &&
+-	    I915_READ(DPIO_CTL) & DPIO_CMNRST)
+-		return;
++	if (IS_PINEVIEW(dev))
++		I915_WRITE(ECOSKPD, _MASKED_BIT_ENABLE(ECO_GATING_CX_ONLY));
+ 
+-	DRM_DEBUG_KMS("toggling display PHY side reset\n");
++	/* IIR "flip pending" means done if this bit is set */
++	I915_WRITE(ECOSKPD, _MASKED_BIT_DISABLE(ECO_FLIP_DONE));
++
++	/* interrupts should cause a wake up from C3 */
++	I915_WRITE(INSTPM, _MASKED_BIT_ENABLE(INSTPM_AGPBUSY_INT_EN));
+ 
+-	/* cmnlane needs DPLL registers */
+-	disp2d->ops->enable(dev_priv, disp2d);
++	/* On GEN3 we really need to make sure the ARB C3 LP bit is set */
++	I915_WRITE(MI_ARB_STATE, _MASKED_BIT_ENABLE(MI_ARB_C3_LP_WRITE_ENABLE));
+ 
+-	/*
+-	 * From VLV2A0_DP_eDP_HDMI_DPIO_driver_vbios_notes_11.docx:
+-	 * Need to assert and de-assert PHY SB reset by gating the
+-	 * common lane power, then un-gating it.
+-	 * Simply ungating isn't enough to reset the PHY enough to get
+-	 * ports and lanes running.
+-	 */
+-	cmn->ops->disable(dev_priv, cmn);
++	I915_WRITE(MI_ARB_STATE,
++		   _MASKED_BIT_ENABLE(MI_ARB_DISPLAY_TRICKLE_FEED_DISABLE));
+ }
+ 
+-void intel_power_domains_init_hw(struct drm_i915_private *dev_priv)
++static void i85x_init_clock_gating(struct drm_device *dev)
+ {
+-	struct drm_device *dev = dev_priv->dev;
+-	struct i915_power_domains *power_domains = &dev_priv->power_domains;
+-
+-	power_domains->initializing = true;
+-
+-	if (IS_VALLEYVIEW(dev) && !IS_CHERRYVIEW(dev)) {
+-		mutex_lock(&power_domains->lock);
+-		vlv_cmnlane_wa(dev_priv);
+-		mutex_unlock(&power_domains->lock);
+-	}
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	/* For now, we need the power well to be always enabled. */
+-	intel_display_set_init_power(dev_priv, true);
+-	intel_power_domains_resume(dev_priv);
+-	power_domains->initializing = false;
+-}
++	I915_WRITE(RENCLK_GATE_D1, SV_CLOCK_GATE_DISABLE);
+ 
+-void intel_aux_display_runtime_get(struct drm_i915_private *dev_priv)
+-{
+-	intel_runtime_pm_get(dev_priv);
+-}
++	/* interrupts should cause a wake up from C3 */
++	I915_WRITE(MI_STATE, _MASKED_BIT_ENABLE(MI_AGPBUSY_INT_EN) |
++		   _MASKED_BIT_DISABLE(MI_AGPBUSY_830_MODE));
+ 
+-void intel_aux_display_runtime_put(struct drm_i915_private *dev_priv)
+-{
+-	intel_runtime_pm_put(dev_priv);
++	I915_WRITE(MEM_MODE,
++		   _MASKED_BIT_ENABLE(MEM_DISPLAY_TRICKLE_FEED_DISABLE));
+ }
+ 
+-void intel_runtime_pm_get(struct drm_i915_private *dev_priv)
++static void i830_init_clock_gating(struct drm_device *dev)
+ {
+-	struct drm_device *dev = dev_priv->dev;
+-	struct device *device = &dev->pdev->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	if (!HAS_RUNTIME_PM(dev))
+-		return;
++	I915_WRITE(DSPCLK_GATE_D, OVRUNIT_CLOCK_GATE_DISABLE);
+ 
+-	pm_runtime_get_sync(device);
+-	WARN(dev_priv->pm.suspended, "Device still suspended.\n");
++	I915_WRITE(MEM_MODE,
++		   _MASKED_BIT_ENABLE(MEM_DISPLAY_A_TRICKLE_FEED_DISABLE) |
++		   _MASKED_BIT_ENABLE(MEM_DISPLAY_B_TRICKLE_FEED_DISABLE));
+ }
+ 
+-void intel_runtime_pm_get_noresume(struct drm_i915_private *dev_priv)
++void intel_init_clock_gating(struct drm_device *dev)
+ {
+-	struct drm_device *dev = dev_priv->dev;
+-	struct device *device = &dev->pdev->dev;
+-
+-	if (!HAS_RUNTIME_PM(dev))
+-		return;
++	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	WARN(dev_priv->pm.suspended, "Getting nosync-ref while suspended.\n");
+-	pm_runtime_get_noresume(device);
++	dev_priv->display.init_clock_gating(dev);
+ }
+ 
+-void intel_runtime_pm_put(struct drm_i915_private *dev_priv)
++void intel_suspend_hw(struct drm_device *dev)
+ {
+-	struct drm_device *dev = dev_priv->dev;
+-	struct device *device = &dev->pdev->dev;
+-
+-	if (!HAS_RUNTIME_PM(dev))
+-		return;
+-
+-	pm_runtime_mark_last_busy(device);
+-	pm_runtime_put_autosuspend(device);
++	if (HAS_PCH_LPT(dev))
++		lpt_suspend_hw(dev);
+ }
+ 
+-void intel_init_runtime_pm(struct drm_i915_private *dev_priv)
++static void intel_init_fbc(struct drm_i915_private *dev_priv)
+ {
+-	struct drm_device *dev = dev_priv->dev;
+-	struct device *device = &dev->pdev->dev;
+-
+-	if (!HAS_RUNTIME_PM(dev))
+-		return;
+-
+-	pm_runtime_set_active(device);
+-
+-	/*
+-	 * RPM depends on RC6 to save restore the GT HW context, so make RC6 a
+-	 * requirement.
+-	 */
+-	if (!intel_enable_rc6(dev)) {
+-		DRM_INFO("RC6 disabled, disabling runtime PM support\n");
++	if (!HAS_FBC(dev_priv)) {
++		dev_priv->fbc.enabled = false;
+ 		return;
+ 	}
+ 
+-	pm_runtime_set_autosuspend_delay(device, 10000); /* 10s */
+-	pm_runtime_mark_last_busy(device);
+-	pm_runtime_use_autosuspend(device);
+-
+-	pm_runtime_put_autosuspend(device);
+-}
+-
+-void intel_fini_runtime_pm(struct drm_i915_private *dev_priv)
+-{
+-	struct drm_device *dev = dev_priv->dev;
+-	struct device *device = &dev->pdev->dev;
+-
+-	if (!HAS_RUNTIME_PM(dev))
+-		return;
++	if (INTEL_INFO(dev_priv)->gen >= 7) {
++		dev_priv->display.fbc_enabled = ironlake_fbc_enabled;
++		dev_priv->display.enable_fbc = gen7_enable_fbc;
++		dev_priv->display.disable_fbc = ironlake_disable_fbc;
++	} else if (INTEL_INFO(dev_priv)->gen >= 5) {
++		dev_priv->display.fbc_enabled = ironlake_fbc_enabled;
++		dev_priv->display.enable_fbc = ironlake_enable_fbc;
++		dev_priv->display.disable_fbc = ironlake_disable_fbc;
++	} else if (IS_GM45(dev_priv)) {
++		dev_priv->display.fbc_enabled = g4x_fbc_enabled;
++		dev_priv->display.enable_fbc = g4x_enable_fbc;
++		dev_priv->display.disable_fbc = g4x_disable_fbc;
++	} else {
++		dev_priv->display.fbc_enabled = i8xx_fbc_enabled;
++		dev_priv->display.enable_fbc = i8xx_enable_fbc;
++		dev_priv->display.disable_fbc = i8xx_disable_fbc;
+ 
+-	if (!intel_enable_rc6(dev))
+-		return;
++		/* This value was pulled out of someone's hat */
++		I915_WRITE(FBC_CONTROL, 500 << FBC_CTL_INTERVAL_SHIFT);
++	}
+ 
+-	/* Make sure we're not suspended first. */
+-	pm_runtime_get_sync(device);
+-	pm_runtime_disable(device);
++	dev_priv->fbc.enabled = dev_priv->display.fbc_enabled(dev_priv->dev);
+ }
+ 
+ /* Set up chip specific power management-related functions */
+@@ -6780,28 +7041,7 @@
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+-	if (HAS_FBC(dev)) {
+-		if (INTEL_INFO(dev)->gen >= 7) {
+-			dev_priv->display.fbc_enabled = ironlake_fbc_enabled;
+-			dev_priv->display.enable_fbc = gen7_enable_fbc;
+-			dev_priv->display.disable_fbc = ironlake_disable_fbc;
+-		} else if (INTEL_INFO(dev)->gen >= 5) {
+-			dev_priv->display.fbc_enabled = ironlake_fbc_enabled;
+-			dev_priv->display.enable_fbc = ironlake_enable_fbc;
+-			dev_priv->display.disable_fbc = ironlake_disable_fbc;
+-		} else if (IS_GM45(dev)) {
+-			dev_priv->display.fbc_enabled = g4x_fbc_enabled;
+-			dev_priv->display.enable_fbc = g4x_enable_fbc;
+-			dev_priv->display.disable_fbc = g4x_disable_fbc;
+-		} else {
+-			dev_priv->display.fbc_enabled = i8xx_fbc_enabled;
+-			dev_priv->display.enable_fbc = i8xx_enable_fbc;
+-			dev_priv->display.disable_fbc = i8xx_disable_fbc;
+-
+-			/* This value was pulled out of someone's hat */
+-			I915_WRITE(FBC_CONTROL, 500 << FBC_CTL_INTERVAL_SHIFT);
+-		}
+-	}
++	intel_init_fbc(dev_priv);
+ 
+ 	/* For cxsr */
+ 	if (IS_PINEVIEW(dev))
+@@ -6810,7 +7050,13 @@
+ 		i915_ironlake_get_mem_freq(dev);
+ 
+ 	/* For FIFO watermark updates */
+-	if (HAS_PCH_SPLIT(dev)) {
++	if (INTEL_INFO(dev)->gen >= 9) {
++		skl_setup_wm_latency(dev);
++
++		dev_priv->display.init_clock_gating = gen9_init_clock_gating;
++		dev_priv->display.update_wm = skl_update_wm;
++		dev_priv->display.update_sprite_wm = skl_update_sprite_wm;
++	} else if (HAS_PCH_SPLIT(dev)) {
+ 		ilk_setup_wm_latency(dev);
+ 
+ 		if ((IS_GEN5(dev) && dev_priv->wm.pri_latency[1] &&
+@@ -6833,13 +7079,15 @@
+ 		else if (IS_HASWELL(dev))
+ 			dev_priv->display.init_clock_gating = haswell_init_clock_gating;
+ 		else if (INTEL_INFO(dev)->gen == 8)
+-			dev_priv->display.init_clock_gating = gen8_init_clock_gating;
++			dev_priv->display.init_clock_gating = broadwell_init_clock_gating;
+ 	} else if (IS_CHERRYVIEW(dev)) {
+-		dev_priv->display.update_wm = valleyview_update_wm;
++		dev_priv->display.update_wm = cherryview_update_wm;
++		dev_priv->display.update_sprite_wm = valleyview_update_sprite_wm;
+ 		dev_priv->display.init_clock_gating =
+ 			cherryview_init_clock_gating;
+ 	} else if (IS_VALLEYVIEW(dev)) {
+ 		dev_priv->display.update_wm = valleyview_update_wm;
++		dev_priv->display.update_sprite_wm = valleyview_update_sprite_wm;
+ 		dev_priv->display.init_clock_gating =
+ 			valleyview_init_clock_gating;
+ 	} else if (IS_PINEVIEW(dev)) {
+@@ -6899,6 +7147,7 @@
+ 	}
+ 
+ 	I915_WRITE(GEN6_PCODE_DATA, *val);
++	I915_WRITE(GEN6_PCODE_DATA1, 0);
+ 	I915_WRITE(GEN6_PCODE_MAILBOX, GEN6_PCODE_READY | mbox);
+ 
+ 	if (wait_for((I915_READ(GEN6_PCODE_MAILBOX) & GEN6_PCODE_READY) == 0,
+@@ -7025,6 +7274,7 @@
+ 		return -1;
+ 	}
+ 
++	/* CHV needs even values */
+ 	opcode = (DIV_ROUND_CLOSEST((val * 2 * mul), dev_priv->rps.cz_freq) * 2);
+ 
+ 	return opcode;
+@@ -7054,6 +7304,40 @@
+ 	return ret;
+ }
+ 
++struct request_boost {
++	struct work_struct work;
++	struct i915_gem_request *rq;
++};
++
++static void __intel_rps_boost_work(struct work_struct *work)
++{
++	struct request_boost *boost = container_of(work, struct request_boost, work);
++
++	if (!i915_request_complete(boost->rq))
++		gen6_rps_boost(boost->rq->i915, NULL);
++
++	i915_request_put__unlocked(boost->rq);
++	kfree(boost);
++}
++
++void intel_queue_rps_boost_for_request(struct drm_device *dev,
++				       struct i915_gem_request *rq)
++{
++	struct request_boost *boost;
++
++	if (rq == NULL || INTEL_INFO(dev)->gen < 6)
++		return;
++
++	boost = kmalloc(sizeof(*boost), GFP_ATOMIC);
++	if (boost == NULL)
++		return;
++
++	INIT_WORK(&boost->work, __intel_rps_boost_work);
++	boost->rq = i915_request_get(rq);
++
++	queue_work(to_i915(dev)->wq, &boost->work);
++}
++
+ void intel_pm_setup(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+@@ -7062,7 +7346,7 @@
+ 
+ 	INIT_DELAYED_WORK(&dev_priv->rps.delayed_resume_work,
+ 			  intel_gen6_powersave_work);
++	INIT_LIST_HEAD(&dev_priv->rps.clients);
+ 
+ 	dev_priv->pm.suspended = false;
+-	dev_priv->pm._irqs_disabled = false;
+ }
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_renderstate_gen8.c b/drivers/gpu/drm/i915/intel_renderstate_gen8.c
+--- a/drivers/gpu/drm/i915/intel_renderstate_gen8.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_renderstate_gen8.c	2014-11-20 09:53:37.996762837 -0700
+@@ -1,70 +1,575 @@
+ #include "intel_renderstate.h"
+ 
+ static const u32 gen8_null_state_relocs[] = {
+-	0x00000048,
+-	0x00000050,
+-	0x00000060,
+-	0x000003ec,
++	0x00000798,
++	0x000007a4,
++	0x000007ac,
++	0x000007bc,
+ 	-1,
+ };
+ 
+ static const u32 gen8_null_state_batch[] = {
++	0x7a000004,
++	0x01000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
+ 	0x69040000,
+-	0x61020001,
++	0x78140000,
++	0x04000000,
++	0x7820000a,
++	0x00000000,
++	0x00000000,
++	0x80000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78130002,
++	0x00000000,
++	0x00000000,
++	0x02001808,
++	0x781f0002,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78510009,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78100007,
++	0x00000000,
++	0x00000000,
++	0x00010000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x781b0007,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000800,
++	0x00000000,
++	0x78110008,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x781e0003,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x781d0007,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78120002,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78500003,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x781c0002,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x780c0000,
++	0x00000000,
++	0x78520003,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78300000,
++	0x08010040,
++	0x78310000,
++	0x1e000000,
++	0x78320000,
++	0x1e000000,
++	0x78330000,
++	0x1e000000,
++	0x79190002,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x791a0002,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x791b0002,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x79120000,
++	0x00000000,
++	0x79130000,
++	0x00000000,
++	0x79140000,
++	0x00000000,
++	0x79150000,
++	0x00000000,
++	0x79160000,
++	0x00000000,
++	0x78150009,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78190009,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x781a0009,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78160009,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78170009,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78490001,
++	0x00000000,
++	0x00000000,
++	0x784a0000,
++	0x00000000,
++	0x784b0000,
++	0x00000004,
++	0x79170101,
++	0x00000000,
++	0x00000080,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+-	0x79120000,
+ 	0x00000000,
+-	0x79130000,
+ 	0x00000000,
+-	0x79140000,
+ 	0x00000000,
+-	0x79150000,
+ 	0x00000000,
+-	0x79160000,
+ 	0x00000000,
+-	0x6101000e,
+-	0x00000001,
+ 	0x00000000,
+-	0x00000001,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x79180006,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x79180006,
++	0x20000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x79180006,
++	0x40000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x79180006,
++	0x60000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x6101000e,
+ 	0x00000001,	 /* reloc */
+ 	0x00000000,
++	0x00000000,
+ 	0x00000001,	 /* reloc */
+ 	0x00000000,
++	0x00000001,	 /* reloc */
+ 	0x00000000,
++	0x00000001,
+ 	0x00000000,
+ 	0x00000001,	 /* reloc */
+ 	0x00000000,
+-	0xfffff001,
+ 	0x00001001,
+-	0xfffff001,
+ 	0x00001001,
+-	0x78230000,
+-	0x000006e0,
+-	0x78210000,
+-	0x00000700,
+-	0x78300000,
+-	0x08010040,
+-	0x78330000,
+-	0x08000000,
+-	0x78310000,
+-	0x08000000,
+-	0x78320000,
+-	0x08000000,
+-	0x78240000,
+-	0x00000641,
+-	0x780e0000,
+-	0x00000601,
++	0x00000001,
++	0x00001001,
++	0x61020001,
++	0x00000000,
++	0x00000000,
++	0x79000002,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78050006,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x79040002,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x79040002,
++	0x40000000,
++	0x00000000,
++	0x00000000,
++	0x79040002,
++	0x80000000,
++	0x00000000,
++	0x00000000,
++	0x79040002,
++	0xc0000000,
++	0x00000000,
++	0x00000000,
++	0x79080001,
++	0x00000000,
++	0x00000000,
++	0x790a0001,
++	0x00000000,
++	0x00000000,
++	0x78060003,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78070003,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78040001,
++	0x00000000,
++	0x00000000,
++	0x79110000,
++	0x00000000,
+ 	0x780d0000,
+ 	0x00000000,
+-	0x78180000,
+-	0x00000001,
+-	0x78520003,
++	0x79060000,
+ 	0x00000000,
++	0x7907001f,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+-	0x78190009,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+@@ -75,7 +580,6 @@
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+-	0x781b0007,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+@@ -84,26 +588,22 @@
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+-	0x78270000,
+ 	0x00000000,
+-	0x782c0000,
+ 	0x00000000,
+-	0x781c0002,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+-	0x78160009,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
++	0x7902000f,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+-	0x78110008,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+@@ -113,12 +613,10 @@
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+-	0x78290000,
+ 	0x00000000,
+-	0x782e0000,
+ 	0x00000000,
+-	0x781a0009,
+ 	0x00000000,
++	0x790c000f,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+@@ -128,7 +626,6 @@
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+-	0x781d0007,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+@@ -136,153 +633,153 @@
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
++	0x780a0003,
+ 	0x00000000,
+-	0x78280000,
+ 	0x00000000,
+-	0x782d0000,
+ 	0x00000000,
+-	0x78260000,
+ 	0x00000000,
+-	0x782b0000,
++	0x78080083,
++	0x00004000,
+ 	0x00000000,
+-	0x78150009,
+ 	0x00000000,
+ 	0x00000000,
++	0x04004000,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
++	0x08004000,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
++	0x0c004000,
+ 	0x00000000,
+ 	0x00000000,
+-	0x78100007,
+ 	0x00000000,
++	0x10004000,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
++	0x14004000,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
++	0x18004000,
+ 	0x00000000,
+-	0x781e0003,
+ 	0x00000000,
+ 	0x00000000,
++	0x1c004000,
+ 	0x00000000,
+ 	0x00000000,
+-	0x78120002,
+ 	0x00000000,
++	0x20004000,
+ 	0x00000000,
+ 	0x00000000,
+-	0x781f0002,
+-	0x30400820,
+ 	0x00000000,
++	0x24004000,
+ 	0x00000000,
+-	0x78510009,
+ 	0x00000000,
+ 	0x00000000,
++	0x28004000,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
++	0x2c004000,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
++	0x30004000,
+ 	0x00000000,
+ 	0x00000000,
+-	0x78500003,
+-	0x00210000,
+ 	0x00000000,
++	0x34004000,
+ 	0x00000000,
+ 	0x00000000,
+-	0x78130002,
+ 	0x00000000,
++	0x38004000,
+ 	0x00000000,
+ 	0x00000000,
+-	0x782a0000,
+-	0x00000480,
+-	0x782f0000,
+-	0x00000540,
+-	0x78140000,
+-	0x00000800,
+-	0x78170009,
+ 	0x00000000,
++	0x3c004000,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
++	0x40004000,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
++	0x44004000,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+-	0x7820000a,
+-	0x00000580,
++	0x48004000,
+ 	0x00000000,
+-	0x08080000,
+ 	0x00000000,
+ 	0x00000000,
+-	0x1f000002,
+-	0x00060000,
++	0x4c004000,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
++	0x50004000,
+ 	0x00000000,
+-	0x784d0000,
+-	0x40000000,
+-	0x784f0000,
+-	0x80000100,
+-	0x780f0000,
+-	0x00000740,
+-	0x78050006,
+ 	0x00000000,
+ 	0x00000000,
++	0x54004000,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
++	0x58004000,
+ 	0x00000000,
+ 	0x00000000,
+-	0x78070003,
+ 	0x00000000,
++	0x5c004000,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+-	0x78060003,
++	0x60004000,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
++	0x64004000,
+ 	0x00000000,
+-	0x78040001,
+ 	0x00000000,
+-	0x00000001,
+-	0x79000002,
+-	0xffffffff,
++	0x00000000,
++	0x68004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x6c004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x70004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x74004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78004000,
++	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+-	0x78080003,
+-	0x00006000,
+-	0x000005e0,	 /* reloc */
++	0x7c004000,
+ 	0x00000000,
+ 	0x00000000,
+-	0x78090005,
++	0x00000000,
++	0x80004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78090043,
+ 	0x02000000,
+ 	0x22220000,
+-	0x02f60000,
+-	0x11230000,
+-	0x02850004,
+-	0x11230000,
+-	0x784b0000,
+-	0x0000000f,
+-	0x78490001,
+ 	0x00000000,
+ 	0x00000000,
+-	0x7b000005,
+ 	0x00000000,
+-	0x00000003,
+ 	0x00000000,
+-	0x00000001,
+ 	0x00000000,
+ 	0x00000000,
+-	0x05000000,	 /* cmds end */
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+@@ -297,8 +794,6 @@
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+-	0x000004c0,	 /* state start */
+-	0x00000500,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+@@ -345,46 +840,65 @@
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
++	0x680b0001,
++	0x78260000,
++	0x00000000,
++	0x78270000,
++	0x00000000,
++	0x78280000,
++	0x00000000,
++	0x78290000,
++	0x00000000,
++	0x782a0000,
++	0x00000000,
++	0x780e0000,
++	0x00000dc1,
++	0x78240000,
++	0x00000e01,
++	0x784f0000,
++	0x80000100,
++	0x784d0000,
++	0x40000000,
++	0x782b0000,
++	0x00000000,
++	0x782c0000,
++	0x00000000,
++	0x782d0000,
++	0x00000000,
++	0x782e0000,
++	0x00000000,
++	0x782f0000,
++	0x00000000,
++	0x780f0000,
+ 	0x00000000,
++	0x78230000,
++	0x00000e60,
++	0x78210000,
++	0x00000e80,
++	0x7b000005,
++	0x00000004,
++	0x00000001,
+ 	0x00000000,
++	0x00000001,
+ 	0x00000000,
+-	0x00000092,
+ 	0x00000000,
++	0x05000000,	 /* cmds end */
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
++	0x00000000,	 /* state start */
+ 	0x00000000,
++	0x3f800000,
++	0x3f800000,
++	0x3f800000,
++	0x3f800000,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+-	0x0060005a,
+-	0x21403ae8,
+-	0x3a0000c0,
+-	0x008d0040,
+-	0x0060005a,
+-	0x21603ae8,
+-	0x3a0000c0,
+-	0x008d0080,
+-	0x0060005a,
+-	0x21803ae8,
+-	0x3a0000d0,
+-	0x008d0040,
+-	0x0060005a,
+-	0x21a03ae8,
+-	0x3a0000d0,
+-	0x008d0080,
+-	0x02800031,
+-	0x2e0022e8,
+-	0x0e000140,
+-	0x08840001,
+-	0x05800031,
+-	0x200022e0,
+-	0x0e000e00,
+-	0x90031000,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+@@ -410,38 +924,6 @@
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+-	0x06200000,
+-	0x00000002,
+-	0x06200000,
+-	0x00000002,
+-	0x06200000,
+-	0x00000002,
+-	0x06200000,
+-	0x00000002,
+-	0x06200000,
+-	0x00000002,
+-	0x06200000,
+-	0x00000002,
+-	0x06200000,
+-	0x00000002,
+-	0x06200000,
+-	0x00000002,
+-	0x06200000,
+-	0x00000002,
+-	0x06200000,
+-	0x00000002,
+-	0x06200000,
+-	0x00000002,
+-	0x06200000,
+-	0x00000002,
+-	0x06200000,
+-	0x00000002,
+-	0x06200000,
+-	0x00000002,
+-	0x06200000,
+-	0x00000002,
+-	0x06200000,
+-	0x00000002,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+@@ -449,8 +931,6 @@
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+-	0xf99a130c,
+-	0x799a130c,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+@@ -466,9 +946,7 @@
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+-	0x3f800000,
+ 	0x00000000,
+-	0x3f800000,
+ 	0x00000000,
+ 	0x00000000,
+ 	0x00000000,
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_renderstate_gen9.c b/drivers/gpu/drm/i915/intel_renderstate_gen9.c
+--- a/drivers/gpu/drm/i915/intel_renderstate_gen9.c	1969-12-31 17:00:00.000000000 -0700
++++ b/drivers/gpu/drm/i915/intel_renderstate_gen9.c	2014-11-20 09:53:37.996762837 -0700
+@@ -0,0 +1,974 @@
++#include "intel_renderstate.h"
++
++static const u32 gen9_null_state_relocs[] = {
++	0x000007a8,
++	0x000007b4,
++	0x000007bc,
++	0x000007cc,
++	-1,
++};
++
++static const u32 gen9_null_state_batch[] = {
++	0x7a000004,
++	0x01000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x69040300,
++	0x78140000,
++	0x04000000,
++	0x7820000a,
++	0x00000000,
++	0x00000000,
++	0x80000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78130002,
++	0x00000000,
++	0x00000000,
++	0x02001808,
++	0x781f0004,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78510009,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78100007,
++	0x00000000,
++	0x00000000,
++	0x00010000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x781b0007,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000800,
++	0x00000000,
++	0x78110008,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x781e0003,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x781d0009,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78120002,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78500003,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x781c0002,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x780c0000,
++	0x00000000,
++	0x78520003,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78300000,
++	0x08010040,
++	0x78310000,
++	0x1e000000,
++	0x78320000,
++	0x1e000000,
++	0x78330000,
++	0x1e000000,
++	0x79190002,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x791a0002,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x791b0002,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x79120000,
++	0x00000000,
++	0x79130000,
++	0x00000000,
++	0x79140000,
++	0x00000000,
++	0x79150000,
++	0x00000000,
++	0x79160000,
++	0x00000000,
++	0x78150009,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78190009,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x781a0009,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78160009,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78170009,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78490001,
++	0x00000000,
++	0x00000000,
++	0x784a0000,
++	0x00000000,
++	0x784b0000,
++	0x00000004,
++	0x79170101,
++	0x00000000,
++	0x00000080,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x79180006,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x79180006,
++	0x20000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x79180006,
++	0x40000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x79180006,
++	0x60000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x61010011,
++	0x00000001,	 /* reloc */
++	0x00000000,
++	0x00000000,
++	0x00000001,	 /* reloc */
++	0x00000000,
++	0x00000001,	 /* reloc */
++	0x00000000,
++	0x00000001,
++	0x00000000,
++	0x00000001,	 /* reloc */
++	0x00000000,
++	0x00001001,
++	0x00001001,
++	0x00000001,
++	0x00001001,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x61020001,
++	0x00000000,
++	0x00000000,
++	0x79000002,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78050006,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x79040002,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x79040002,
++	0x40000000,
++	0x00000000,
++	0x00000000,
++	0x79040002,
++	0x80000000,
++	0x00000000,
++	0x00000000,
++	0x79040002,
++	0xc0000000,
++	0x00000000,
++	0x00000000,
++	0x79080001,
++	0x00000000,
++	0x00000000,
++	0x790a0001,
++	0x00000000,
++	0x00000000,
++	0x78060003,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78070003,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78040001,
++	0x00000000,
++	0x00000000,
++	0x79110000,
++	0x00000000,
++	0x780d0000,
++	0x00000000,
++	0x79060000,
++	0x00000000,
++	0x7907001f,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x7902000f,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x790c000f,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x780a0003,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78080083,
++	0x00004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x04004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x08004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x0c004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x10004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x14004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x18004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x1c004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x20004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x24004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x28004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x2c004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x30004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x34004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x38004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x3c004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x40004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x44004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x48004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x4c004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x50004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x54004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x58004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x5c004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x60004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x64004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x68004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x6c004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x70004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x74004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x7c004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x80004000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78090043,
++	0x02000000,
++	0x22220000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x78550003,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x680b0001,
++	0x780e0000,
++	0x00000e01,
++	0x78240000,
++	0x00000e41,
++	0x784f0000,
++	0x80000100,
++	0x784d0000,
++	0x40000000,
++	0x782b0000,
++	0x00000000,
++	0x782c0000,
++	0x00000000,
++	0x782d0000,
++	0x00000000,
++	0x782e0000,
++	0x00000000,
++	0x782f0000,
++	0x00000000,
++	0x780f0000,
++	0x00000000,
++	0x78230000,
++	0x00000ea0,
++	0x78210000,
++	0x00000ec0,
++	0x78260000,
++	0x00000000,
++	0x78270000,
++	0x00000000,
++	0x78280000,
++	0x00000000,
++	0x78290000,
++	0x00000000,
++	0x782a0000,
++	0x00000000,
++	0x7b000005,
++	0x00000004,
++	0x00000001,
++	0x00000000,
++	0x00000001,
++	0x00000000,
++	0x00000000,
++	0x05000000,	 /* cmds end */
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,	 /* state start */
++	0x00000000,
++	0x3f800000,
++	0x3f800000,
++	0x3f800000,
++	0x3f800000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,
++	0x00000000,	 /* state end */
++};
++
++RO_RENDERSTATE(9);
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_renderstate.h b/drivers/gpu/drm/i915/intel_renderstate.h
+--- a/drivers/gpu/drm/i915/intel_renderstate.h	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_renderstate.h	2014-11-20 09:53:37.996762837 -0700
+@@ -35,6 +35,7 @@
+ extern const struct intel_renderstate_rodata gen6_null_state;
+ extern const struct intel_renderstate_rodata gen7_null_state;
+ extern const struct intel_renderstate_rodata gen8_null_state;
++extern const struct intel_renderstate_rodata gen9_null_state;
+ 
+ #define RO_RENDERSTATE(_g)						\
+ 	const struct intel_renderstate_rodata gen ## _g ## _null_state = { \
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_ringbuffer.c b/drivers/gpu/drm/i915/intel_ringbuffer.c
+--- a/drivers/gpu/drm/i915/intel_ringbuffer.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_ringbuffer.c	2014-11-20 09:53:37.996762837 -0700
+@@ -33,75 +33,39 @@
+ #include "i915_trace.h"
+ #include "intel_drv.h"
+ 
+-/* Early gen2 devices have a cacheline of just 32 bytes, using 64 is overkill,
+- * but keeps the logic simple. Indeed, the whole purpose of this macro is just
+- * to give some inclination as to some of the magic values used in the various
+- * workarounds!
+- */
+-#define CACHELINE_BYTES 64
+-
+-static inline int __ring_space(int head, int tail, int size)
+-{
+-	int space = head - (tail + I915_RING_FREE_SPACE);
+-	if (space < 0)
+-		space += size;
+-	return space;
+-}
+-
+-static inline int ring_space(struct intel_ringbuffer *ringbuf)
+-{
+-	return __ring_space(ringbuf->head & HEAD_ADDR, ringbuf->tail, ringbuf->size);
+-}
+-
+-static bool intel_ring_stopped(struct intel_engine_cs *ring)
+-{
+-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
+-	return dev_priv->gpu_error.stop_rings & intel_ring_flag(ring);
+-}
+-
+-void __intel_ring_advance(struct intel_engine_cs *ring)
+-{
+-	struct intel_ringbuffer *ringbuf = ring->buffer;
+-	ringbuf->tail &= ringbuf->size - 1;
+-	if (intel_ring_stopped(ring))
+-		return;
+-	ring->write_tail(ring, ringbuf->tail);
+-}
++/* Just userspace ABI convention to limit the wa batch bo to a resonable size */
++#define I830_BATCH_LIMIT (256*1024)
++#define I830_TLB_ENTRIES (2)
++#define I830_WA_SIZE max(I830_TLB_ENTRIES*4096, I830_BATCH_LIMIT)
+ 
+ static int
+-gen2_render_ring_flush(struct intel_engine_cs *ring,
+-		       u32	invalidate_domains,
+-		       u32	flush_domains)
++gen2_emit_flush(struct i915_gem_request *rq, u32 flags)
+ {
++	struct intel_ringbuffer *ring;
+ 	u32 cmd;
+-	int ret;
+ 
+ 	cmd = MI_FLUSH;
+-	if (((invalidate_domains|flush_domains) & I915_GEM_DOMAIN_RENDER) == 0)
++	if ((flags & (I915_FLUSH_CACHES | I915_INVALIDATE_CACHES)) == 0)
+ 		cmd |= MI_NO_WRITE_FLUSH;
+ 
+-	if (invalidate_domains & I915_GEM_DOMAIN_SAMPLER)
++	if (flags & I915_INVALIDATE_CACHES)
+ 		cmd |= MI_READ_FLUSH;
+ 
+-	ret = intel_ring_begin(ring, 2);
+-	if (ret)
+-		return ret;
++	ring = intel_ring_begin(rq, 1);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+ 	intel_ring_emit(ring, cmd);
+-	intel_ring_emit(ring, MI_NOOP);
+ 	intel_ring_advance(ring);
+ 
+ 	return 0;
+ }
+ 
+ static int
+-gen4_render_ring_flush(struct intel_engine_cs *ring,
+-		       u32	invalidate_domains,
+-		       u32	flush_domains)
++gen4_emit_flush(struct i915_gem_request *rq, u32 flags)
+ {
+-	struct drm_device *dev = ring->dev;
++	struct intel_ringbuffer *ring;
+ 	u32 cmd;
+-	int ret;
+ 
+ 	/*
+ 	 * read/write caches:
+@@ -131,22 +95,20 @@
+ 	 * are flushed at any MI_FLUSH.
+ 	 */
+ 
+-	cmd = MI_FLUSH | MI_NO_WRITE_FLUSH;
+-	if ((invalidate_domains|flush_domains) & I915_GEM_DOMAIN_RENDER)
+-		cmd &= ~MI_NO_WRITE_FLUSH;
+-	if (invalidate_domains & I915_GEM_DOMAIN_INSTRUCTION)
++	cmd = MI_FLUSH;
++	if ((flags & (I915_FLUSH_CACHES | I915_INVALIDATE_CACHES)) == 0)
++		cmd |= MI_NO_WRITE_FLUSH;
++	if (flags & I915_INVALIDATE_CACHES) {
+ 		cmd |= MI_EXE_FLUSH;
++		if (IS_G4X(rq->i915) || IS_GEN5(rq->i915))
++			cmd |= MI_INVALIDATE_ISP;
++	}
+ 
+-	if (invalidate_domains & I915_GEM_DOMAIN_COMMAND &&
+-	    (IS_G4X(dev) || IS_GEN5(dev)))
+-		cmd |= MI_INVALIDATE_ISP;
+-
+-	ret = intel_ring_begin(ring, 2);
+-	if (ret)
+-		return ret;
++	ring = intel_ring_begin(rq, 1);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+ 	intel_ring_emit(ring, cmd);
+-	intel_ring_emit(ring, MI_NOOP);
+ 	intel_ring_advance(ring);
+ 
+ 	return 0;
+@@ -190,100 +152,89 @@
+  * really our business.  That leaves only stall at scoreboard.
+  */
+ static int
+-intel_emit_post_sync_nonzero_flush(struct intel_engine_cs *ring)
++gen6_emit_post_sync_nonzero_flush(struct i915_gem_request *rq)
+ {
+-	u32 scratch_addr = ring->scratch.gtt_offset + 2 * CACHELINE_BYTES;
+-	int ret;
+-
++	const u32 scratch = rq->engine->scratch.gtt_offset + 2*CACHELINE_BYTES;
++	struct intel_ringbuffer *ring;
+ 
+-	ret = intel_ring_begin(ring, 6);
+-	if (ret)
+-		return ret;
++	ring = intel_ring_begin(rq, 8);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+-	intel_ring_emit(ring, GFX_OP_PIPE_CONTROL(5));
++	intel_ring_emit(ring, GFX_OP_PIPE_CONTROL(4));
+ 	intel_ring_emit(ring, PIPE_CONTROL_CS_STALL |
+ 			PIPE_CONTROL_STALL_AT_SCOREBOARD);
+-	intel_ring_emit(ring, scratch_addr | PIPE_CONTROL_GLOBAL_GTT); /* address */
+-	intel_ring_emit(ring, 0); /* low dword */
+-	intel_ring_emit(ring, 0); /* high dword */
+-	intel_ring_emit(ring, MI_NOOP);
+-	intel_ring_advance(ring);
+-
+-	ret = intel_ring_begin(ring, 6);
+-	if (ret)
+-		return ret;
++	intel_ring_emit(ring, scratch | PIPE_CONTROL_GLOBAL_GTT);
++	intel_ring_emit(ring, 0);
+ 
+-	intel_ring_emit(ring, GFX_OP_PIPE_CONTROL(5));
++	intel_ring_emit(ring, GFX_OP_PIPE_CONTROL(4));
+ 	intel_ring_emit(ring, PIPE_CONTROL_QW_WRITE);
+-	intel_ring_emit(ring, scratch_addr | PIPE_CONTROL_GLOBAL_GTT); /* address */
+-	intel_ring_emit(ring, 0);
++	intel_ring_emit(ring, scratch | PIPE_CONTROL_GLOBAL_GTT);
+ 	intel_ring_emit(ring, 0);
+-	intel_ring_emit(ring, MI_NOOP);
+-	intel_ring_advance(ring);
+ 
++	intel_ring_advance(ring);
+ 	return 0;
+ }
+ 
+ static int
+-gen6_render_ring_flush(struct intel_engine_cs *ring,
+-                         u32 invalidate_domains, u32 flush_domains)
++gen6_render_emit_flush(struct i915_gem_request *rq, u32 flags)
+ {
+-	u32 flags = 0;
+-	u32 scratch_addr = ring->scratch.gtt_offset + 2 * CACHELINE_BYTES;
++	const u32 scratch = rq->engine->scratch.gtt_offset + 2*CACHELINE_BYTES;
++	struct intel_ringbuffer *ring;
++	u32 cmd = 0;
+ 	int ret;
+ 
+-	/* Force SNB workarounds for PIPE_CONTROL flushes */
+-	ret = intel_emit_post_sync_nonzero_flush(ring);
+-	if (ret)
+-		return ret;
++	if (flags & I915_FLUSH_CACHES) {
++		/* Force SNB workarounds for PIPE_CONTROL flushes */
++		ret = gen6_emit_post_sync_nonzero_flush(rq);
++		if (ret)
++			return ret;
+ 
+-	/* Just flush everything.  Experiments have shown that reducing the
+-	 * number of bits based on the write domains has little performance
+-	 * impact.
+-	 */
+-	if (flush_domains) {
+-		flags |= PIPE_CONTROL_RENDER_TARGET_CACHE_FLUSH;
+-		flags |= PIPE_CONTROL_DEPTH_CACHE_FLUSH;
+-		/*
+-		 * Ensure that any following seqno writes only happen
+-		 * when the render cache is indeed flushed.
+-		 */
+-		flags |= PIPE_CONTROL_CS_STALL;
++		cmd |= PIPE_CONTROL_RENDER_TARGET_CACHE_FLUSH;
++		cmd |= PIPE_CONTROL_DEPTH_CACHE_FLUSH;
+ 	}
+-	if (invalidate_domains) {
+-		flags |= PIPE_CONTROL_TLB_INVALIDATE;
+-		flags |= PIPE_CONTROL_INSTRUCTION_CACHE_INVALIDATE;
+-		flags |= PIPE_CONTROL_TEXTURE_CACHE_INVALIDATE;
+-		flags |= PIPE_CONTROL_VF_CACHE_INVALIDATE;
+-		flags |= PIPE_CONTROL_CONST_CACHE_INVALIDATE;
+-		flags |= PIPE_CONTROL_STATE_CACHE_INVALIDATE;
++	if (flags & I915_INVALIDATE_CACHES) {
++		cmd |= PIPE_CONTROL_TLB_INVALIDATE;
++		cmd |= PIPE_CONTROL_INSTRUCTION_CACHE_INVALIDATE;
++		cmd |= PIPE_CONTROL_TEXTURE_CACHE_INVALIDATE;
++		cmd |= PIPE_CONTROL_VF_CACHE_INVALIDATE;
++		cmd |= PIPE_CONTROL_CONST_CACHE_INVALIDATE;
++		cmd |= PIPE_CONTROL_STATE_CACHE_INVALIDATE;
+ 		/*
+ 		 * TLB invalidate requires a post-sync write.
+ 		 */
+-		flags |= PIPE_CONTROL_QW_WRITE | PIPE_CONTROL_CS_STALL;
++		cmd |= PIPE_CONTROL_QW_WRITE | PIPE_CONTROL_CS_STALL;
+ 	}
++	if (flags & I915_COMMAND_BARRIER)
++		/*
++		 * Ensure that any following seqno writes only happen
++		 * when the render cache is indeed flushed.
++		 */
++		cmd |= PIPE_CONTROL_QW_WRITE | PIPE_CONTROL_CS_STALL;
+ 
+-	ret = intel_ring_begin(ring, 4);
+-	if (ret)
+-		return ret;
+-
+-	intel_ring_emit(ring, GFX_OP_PIPE_CONTROL(4));
+-	intel_ring_emit(ring, flags);
+-	intel_ring_emit(ring, scratch_addr | PIPE_CONTROL_GLOBAL_GTT);
+-	intel_ring_emit(ring, 0);
+-	intel_ring_advance(ring);
++	if (cmd) {
++		ring = intel_ring_begin(rq, 4);
++		if (IS_ERR(ring))
++			return PTR_ERR(ring);
++
++		intel_ring_emit(ring, GFX_OP_PIPE_CONTROL(4));
++		intel_ring_emit(ring, cmd);
++		intel_ring_emit(ring, scratch | PIPE_CONTROL_GLOBAL_GTT);
++		intel_ring_emit(ring, 0);
++		intel_ring_advance(ring);
++	}
+ 
+ 	return 0;
+ }
+ 
+ static int
+-gen7_render_ring_cs_stall_wa(struct intel_engine_cs *ring)
++gen7_render_ring_cs_stall_wa(struct i915_gem_request *rq)
+ {
+-	int ret;
++	struct intel_ringbuffer *ring;
+ 
+-	ret = intel_ring_begin(ring, 4);
+-	if (ret)
+-		return ret;
++	ring = intel_ring_begin(rq, 4);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+ 	intel_ring_emit(ring, GFX_OP_PIPE_CONTROL(4));
+ 	intel_ring_emit(ring, PIPE_CONTROL_CS_STALL |
+@@ -295,35 +246,32 @@
+ 	return 0;
+ }
+ 
+-static int gen7_ring_fbc_flush(struct intel_engine_cs *ring, u32 value)
++static int gen7_ring_fbc_flush(struct i915_gem_request *rq, u32 value)
+ {
+-	int ret;
++	struct intel_ringbuffer *ring;
+ 
+-	if (!ring->fbc_dirty)
+-		return 0;
++	ring = intel_ring_begin(rq, 6);
++	if (IS_ERR(ring))
++		return PTR_ERR(rq);
+ 
+-	ret = intel_ring_begin(ring, 6);
+-	if (ret)
+-		return ret;
+ 	/* WaFbcNukeOn3DBlt:ivb/hsw */
+ 	intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
+ 	intel_ring_emit(ring, MSG_FBC_REND_STATE);
+ 	intel_ring_emit(ring, value);
+ 	intel_ring_emit(ring, MI_STORE_REGISTER_MEM(1) | MI_SRM_LRM_GLOBAL_GTT);
+ 	intel_ring_emit(ring, MSG_FBC_REND_STATE);
+-	intel_ring_emit(ring, ring->scratch.gtt_offset + 256);
++	intel_ring_emit(ring, rq->engine->scratch.gtt_offset + 256);
+ 	intel_ring_advance(ring);
+ 
+-	ring->fbc_dirty = false;
+ 	return 0;
+ }
+ 
+ static int
+-gen7_render_ring_flush(struct intel_engine_cs *ring,
+-		       u32 invalidate_domains, u32 flush_domains)
++gen7_render_emit_flush(struct i915_gem_request *rq, u32 flags)
+ {
+-	u32 flags = 0;
+-	u32 scratch_addr = ring->scratch.gtt_offset + 2 * CACHELINE_BYTES;
++	const u32 scratch_addr = rq->engine->scratch.gtt_offset + 2 * CACHELINE_BYTES;
++	struct intel_ringbuffer *ring;
++	u32 cmd = 0;
+ 	int ret;
+ 
+ 	/*
+@@ -334,63 +282,72 @@
+ 	 * read-cache invalidate bits set) must have the CS_STALL bit set. We
+ 	 * don't try to be clever and just set it unconditionally.
+ 	 */
+-	flags |= PIPE_CONTROL_CS_STALL;
++	cmd |= PIPE_CONTROL_CS_STALL;
+ 
+ 	/* Just flush everything.  Experiments have shown that reducing the
+ 	 * number of bits based on the write domains has little performance
+ 	 * impact.
+ 	 */
+-	if (flush_domains) {
+-		flags |= PIPE_CONTROL_RENDER_TARGET_CACHE_FLUSH;
+-		flags |= PIPE_CONTROL_DEPTH_CACHE_FLUSH;
+-	}
+-	if (invalidate_domains) {
+-		flags |= PIPE_CONTROL_TLB_INVALIDATE;
+-		flags |= PIPE_CONTROL_INSTRUCTION_CACHE_INVALIDATE;
+-		flags |= PIPE_CONTROL_TEXTURE_CACHE_INVALIDATE;
+-		flags |= PIPE_CONTROL_VF_CACHE_INVALIDATE;
+-		flags |= PIPE_CONTROL_CONST_CACHE_INVALIDATE;
+-		flags |= PIPE_CONTROL_STATE_CACHE_INVALIDATE;
++	if (flags & I915_FLUSH_CACHES) {
++		cmd |= PIPE_CONTROL_RENDER_TARGET_CACHE_FLUSH;
++		cmd |= PIPE_CONTROL_DEPTH_CACHE_FLUSH;
++	}
++	if (flags & I915_INVALIDATE_CACHES) {
++		cmd |= PIPE_CONTROL_TLB_INVALIDATE;
++		cmd |= PIPE_CONTROL_INSTRUCTION_CACHE_INVALIDATE;
++		cmd |= PIPE_CONTROL_TEXTURE_CACHE_INVALIDATE;
++		cmd |= PIPE_CONTROL_VF_CACHE_INVALIDATE;
++		cmd |= PIPE_CONTROL_CONST_CACHE_INVALIDATE;
++		cmd |= PIPE_CONTROL_STATE_CACHE_INVALIDATE;
+ 		/*
+ 		 * TLB invalidate requires a post-sync write.
+ 		 */
+-		flags |= PIPE_CONTROL_QW_WRITE;
+-		flags |= PIPE_CONTROL_GLOBAL_GTT_IVB;
++		cmd |= PIPE_CONTROL_QW_WRITE | PIPE_CONTROL_GLOBAL_GTT_IVB;
+ 
+ 		/* Workaround: we must issue a pipe_control with CS-stall bit
+ 		 * set before a pipe_control command that has the state cache
+ 		 * invalidate bit set. */
+-		gen7_render_ring_cs_stall_wa(ring);
++		ret = gen7_render_ring_cs_stall_wa(rq);
++		if (ret)
++			return ret;
+ 	}
++	if ((flags & (I915_COMMAND_BARRIER | I915_FLUSH_CACHES)) == I915_COMMAND_BARRIER)
++		cmd |= PIPE_CONTROL_STALL_AT_SCOREBOARD;
+ 
+-	ret = intel_ring_begin(ring, 4);
+-	if (ret)
+-		return ret;
++	ring = intel_ring_begin(rq, 4);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+ 	intel_ring_emit(ring, GFX_OP_PIPE_CONTROL(4));
+-	intel_ring_emit(ring, flags);
++	intel_ring_emit(ring, cmd);
+ 	intel_ring_emit(ring, scratch_addr);
+ 	intel_ring_emit(ring, 0);
+ 	intel_ring_advance(ring);
+ 
+-	if (!invalidate_domains && flush_domains)
+-		return gen7_ring_fbc_flush(ring, FBC_REND_NUKE);
++	if (flags & I915_KICK_FBC) {
++		ret = gen7_ring_fbc_flush(rq, FBC_REND_NUKE);
++		if (ret)
++			return ret;
++	}
+ 
+ 	return 0;
+ }
+ 
+ static int
+-gen8_emit_pipe_control(struct intel_engine_cs *ring,
+-		       u32 flags, u32 scratch_addr)
++gen8_emit_pipe_control(struct i915_gem_request *rq,
++		       u32 cmd, u32 scratch_addr)
+ {
+-	int ret;
++	struct intel_ringbuffer *ring;
+ 
+-	ret = intel_ring_begin(ring, 6);
+-	if (ret)
+-		return ret;
++	if (cmd == 0)
++		return 0;
++
++	ring = intel_ring_begin(rq, 6);
++	if (IS_ERR(rq))
++		return PTR_ERR(rq);
+ 
+ 	intel_ring_emit(ring, GFX_OP_PIPE_CONTROL(6));
+-	intel_ring_emit(ring, flags);
++	intel_ring_emit(ring, cmd);
+ 	intel_ring_emit(ring, scratch_addr);
+ 	intel_ring_emit(ring, 0);
+ 	intel_ring_emit(ring, 0);
+@@ -401,31 +358,29 @@
+ }
+ 
+ static int
+-gen8_render_ring_flush(struct intel_engine_cs *ring,
+-		       u32 invalidate_domains, u32 flush_domains)
++gen8_render_emit_flush(struct i915_gem_request *rq,
++		       u32 flags)
+ {
+-	u32 flags = 0;
+-	u32 scratch_addr = ring->scratch.gtt_offset + 2 * CACHELINE_BYTES;
++	const u32 scratch_addr = rq->engine->scratch.gtt_offset + 2 * CACHELINE_BYTES;
++	u32 cmd = 0;
+ 	int ret;
+ 
+-	flags |= PIPE_CONTROL_CS_STALL;
+-
+-	if (flush_domains) {
+-		flags |= PIPE_CONTROL_RENDER_TARGET_CACHE_FLUSH;
+-		flags |= PIPE_CONTROL_DEPTH_CACHE_FLUSH;
+-	}
+-	if (invalidate_domains) {
+-		flags |= PIPE_CONTROL_TLB_INVALIDATE;
+-		flags |= PIPE_CONTROL_INSTRUCTION_CACHE_INVALIDATE;
+-		flags |= PIPE_CONTROL_TEXTURE_CACHE_INVALIDATE;
+-		flags |= PIPE_CONTROL_VF_CACHE_INVALIDATE;
+-		flags |= PIPE_CONTROL_CONST_CACHE_INVALIDATE;
+-		flags |= PIPE_CONTROL_STATE_CACHE_INVALIDATE;
+-		flags |= PIPE_CONTROL_QW_WRITE;
+-		flags |= PIPE_CONTROL_GLOBAL_GTT_IVB;
++	if (flags & I915_FLUSH_CACHES) {
++		cmd |= PIPE_CONTROL_RENDER_TARGET_CACHE_FLUSH;
++		cmd |= PIPE_CONTROL_DEPTH_CACHE_FLUSH;
++	}
++	if (flags & I915_INVALIDATE_CACHES) {
++		cmd |= PIPE_CONTROL_TLB_INVALIDATE;
++		cmd |= PIPE_CONTROL_INSTRUCTION_CACHE_INVALIDATE;
++		cmd |= PIPE_CONTROL_TEXTURE_CACHE_INVALIDATE;
++		cmd |= PIPE_CONTROL_VF_CACHE_INVALIDATE;
++		cmd |= PIPE_CONTROL_CONST_CACHE_INVALIDATE;
++		cmd |= PIPE_CONTROL_STATE_CACHE_INVALIDATE;
++		cmd |= PIPE_CONTROL_QW_WRITE;
++		cmd |= PIPE_CONTROL_GLOBAL_GTT_IVB;
+ 
+ 		/* WaCsStallBeforeStateCacheInvalidate:bdw,chv */
+-		ret = gen8_emit_pipe_control(ring,
++		ret = gen8_emit_pipe_control(rq,
+ 					     PIPE_CONTROL_CS_STALL |
+ 					     PIPE_CONTROL_STALL_AT_SCOREBOARD,
+ 					     0);
+@@ -433,200 +388,419 @@
+ 			return ret;
+ 	}
+ 
+-	return gen8_emit_pipe_control(ring, flags, scratch_addr);
++	if (flags & I915_COMMAND_BARRIER) {
++		cmd |= PIPE_CONTROL_CS_STALL;
++		cmd |= PIPE_CONTROL_QW_WRITE;
++		cmd |= PIPE_CONTROL_GLOBAL_GTT_IVB;
++	}
++
++	ret = gen8_emit_pipe_control(rq, cmd, scratch_addr);
++	if (ret)
++		return ret;
++
++	if (flags & I915_KICK_FBC) {
++		ret = gen7_ring_fbc_flush(rq, FBC_REND_NUKE);
++		if (ret)
++			return ret;
++	}
++
++	return 0;
+ }
+ 
+-static void ring_write_tail(struct intel_engine_cs *ring,
++static void ring_write_tail(struct intel_engine_cs *engine,
+ 			    u32 value)
+ {
+-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
+-	I915_WRITE_TAIL(ring, value);
++	struct drm_i915_private *dev_priv = engine->i915;
++	I915_WRITE_TAIL(engine, value);
+ }
+ 
+-u64 intel_ring_get_active_head(struct intel_engine_cs *ring)
++u64 intel_engine_get_active_head(struct intel_engine_cs *engine)
+ {
+-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
++	struct drm_i915_private *dev_priv = engine->i915;
+ 	u64 acthd;
+ 
+-	if (INTEL_INFO(ring->dev)->gen >= 8)
+-		acthd = I915_READ64_2x32(RING_ACTHD(ring->mmio_base),
+-					 RING_ACTHD_UDW(ring->mmio_base));
+-	else if (INTEL_INFO(ring->dev)->gen >= 4)
+-		acthd = I915_READ(RING_ACTHD(ring->mmio_base));
++	if (INTEL_INFO(dev_priv)->gen >= 8)
++		acthd = I915_READ64_2x32(RING_ACTHD(engine->mmio_base),
++					 RING_ACTHD_UDW(engine->mmio_base));
++	else if (INTEL_INFO(dev_priv)->gen >= 4)
++		acthd = I915_READ(RING_ACTHD(engine->mmio_base));
+ 	else
+ 		acthd = I915_READ(ACTHD);
+ 
+ 	return acthd;
+ }
+ 
+-static void ring_setup_phys_status_page(struct intel_engine_cs *ring)
+-{
+-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
+-	u32 addr;
+-
+-	addr = dev_priv->status_page_dmah->busaddr;
+-	if (INTEL_INFO(ring->dev)->gen >= 4)
+-		addr |= (dev_priv->status_page_dmah->busaddr >> 28) & 0xf0;
+-	I915_WRITE(HWS_PGA, addr);
+-}
+-
+-static bool stop_ring(struct intel_engine_cs *ring)
++static bool engine_stop(struct intel_engine_cs *engine)
+ {
+-	struct drm_i915_private *dev_priv = to_i915(ring->dev);
++	struct drm_i915_private *dev_priv = engine->i915;
+ 
+-	if (!IS_GEN2(ring->dev)) {
+-		I915_WRITE_MODE(ring, _MASKED_BIT_ENABLE(STOP_RING));
+-		if (wait_for_atomic((I915_READ_MODE(ring) & MODE_IDLE) != 0, 1000)) {
+-			DRM_ERROR("%s :timed out trying to stop ring\n", ring->name);
+-			return false;
++	if (!IS_GEN2(dev_priv)) {
++		I915_WRITE_MODE(engine, _MASKED_BIT_ENABLE(STOP_RING));
++		if (wait_for((I915_READ_MODE(engine) & MODE_IDLE) != 0, 1000)) {
++			DRM_ERROR("%s : timed out trying to stop ring\n", engine->name);
++			/* Sometimes we observe that the idle flag is not
++			 * set even though the ring is empty. So double
++			 * check before giving up.
++			 */
++			if (I915_READ_HEAD(engine) != I915_READ_TAIL(engine))
++				return false;
+ 		}
+ 	}
+ 
+-	I915_WRITE_CTL(ring, 0);
+-	I915_WRITE_HEAD(ring, 0);
+-	ring->write_tail(ring, 0);
++	I915_WRITE_CTL(engine, 0);
++	I915_WRITE_HEAD(engine, 0);
++	engine->write_tail(engine, 0);
+ 
+-	if (!IS_GEN2(ring->dev)) {
+-		(void)I915_READ_CTL(ring);
+-		I915_WRITE_MODE(ring, _MASKED_BIT_DISABLE(STOP_RING));
++	if (!IS_GEN2(dev_priv)) {
++		(void)I915_READ_CTL(engine);
++		I915_WRITE_MODE(engine, _MASKED_BIT_DISABLE(STOP_RING));
+ 	}
+ 
+-	return (I915_READ_HEAD(ring) & HEAD_ADDR) == 0;
++	return (I915_READ_HEAD(engine) & HEAD_ADDR) == 0;
+ }
+ 
+-static int init_ring_common(struct intel_engine_cs *ring)
++static int engine_suspend(struct intel_engine_cs *engine)
+ {
+-	struct drm_device *dev = ring->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_ringbuffer *ringbuf = ring->buffer;
+-	struct drm_i915_gem_object *obj = ringbuf->obj;
++	struct intel_ringbuffer *ring;
++
++	if (!engine_stop(engine))
++		return -EIO;
++
++	ring = engine->default_context->ring[engine->id].ring;
++	i915_gem_object_ggtt_unpin(ring->obj);
++	return 0;
++}
++
++static int enable_status_page(struct intel_engine_cs *engine)
++{
++	struct drm_i915_private *dev_priv = engine->i915;
++	u32 mmio, addr;
+ 	int ret = 0;
+ 
+-	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
++	if (!I915_NEED_GFX_HWS(dev_priv)) {
++		addr = dev_priv->status_page_dmah->busaddr;
++		if (INTEL_INFO(dev_priv)->gen >= 4)
++			addr |= (dev_priv->status_page_dmah->busaddr >> 28) & 0xf0;
++		mmio = HWS_PGA;
++	} else {
++		addr = engine->status_page.gfx_addr;
++		/* The ring status page addresses are no longer next to the rest of
++		 * the ring registers as of gen7.
++		 */
++		if (IS_GEN7(dev_priv)) {
++			switch (engine->id) {
++			default:
++			case RCS:
++				mmio = RENDER_HWS_PGA_GEN7;
++				break;
++			case BCS:
++				mmio = BLT_HWS_PGA_GEN7;
++				break;
++				/*
++				 * VCS2 actually doesn't exist on Gen7. Only shut up
++				 * gcc switch check warning
++				 */
++			case VCS2:
++			case VCS:
++				mmio = BSD_HWS_PGA_GEN7;
++				break;
++			case VECS:
++				mmio = VEBOX_HWS_PGA_GEN7;
++				break;
++			}
++		} else if (IS_GEN6(dev_priv)) {
++			mmio = RING_HWS_PGA_GEN6(engine->mmio_base);
++		} else {
++			/* XXX: gen8 returns to sanity */
++			mmio = RING_HWS_PGA(engine->mmio_base);
++		}
++	}
+ 
+-	if (!stop_ring(ring)) {
+-		/* G45 ring initialization often fails to reset head to zero */
+-		DRM_DEBUG_KMS("%s head not reset to zero "
+-			      "ctl %08x head %08x tail %08x start %08x\n",
+-			      ring->name,
+-			      I915_READ_CTL(ring),
+-			      I915_READ_HEAD(ring),
+-			      I915_READ_TAIL(ring),
+-			      I915_READ_START(ring));
++	if (IS_GEN5(dev_priv) || IS_G4X(dev_priv)) {
++		if (wait_for((I915_READ(mmio) & 1) == 0, 1000))
++			DRM_ERROR("%s: wait for Translation-in-Progress to complete for HWS timed out\n",
++				  engine->name);
++	}
+ 
+-		if (!stop_ring(ring)) {
+-			DRM_ERROR("failed to set %s head to zero "
+-				  "ctl %08x head %08x tail %08x start %08x\n",
+-				  ring->name,
+-				  I915_READ_CTL(ring),
+-				  I915_READ_HEAD(ring),
+-				  I915_READ_TAIL(ring),
+-				  I915_READ_START(ring));
++	I915_WRITE(mmio, addr);
++	POSTING_READ(mmio);
++
++	/*
++	 * Flush the TLB for this page
++	 *
++	 * FIXME: These two bits have disappeared on gen8, so a question
++	 * arises: do we still need this and if so how should we go about
++	 * invalidating the TLB?
++	 */
++	if (INTEL_INFO(dev_priv)->gen >= 6 && INTEL_INFO(dev_priv)->gen < 8) {
++		u32 reg = RING_INSTPM(engine->mmio_base);
++
++		/* ring should be idle before issuing a sync flush*/
++		WARN_ON((I915_READ_MODE(engine) & MODE_IDLE) == 0);
++
++		I915_WRITE(reg,
++			   _MASKED_BIT_ENABLE(INSTPM_TLB_INVALIDATE |
++					      INSTPM_SYNC_FLUSH));
++		if (wait_for((I915_READ(reg) & INSTPM_SYNC_FLUSH) == 0,
++			     1000)) {
++			DRM_ERROR("%s: wait for SyncFlush to complete for TLB invalidation timed out\n",
++				  engine->name);
+ 			ret = -EIO;
+-			goto out;
+ 		}
+ 	}
+ 
+-	if (I915_NEED_GFX_HWS(dev))
+-		intel_ring_setup_status_page(ring);
+-	else
+-		ring_setup_phys_status_page(ring);
++	return ret;
++}
++
++static int engine_resume(struct intel_engine_cs *engine)
++{
++	struct drm_i915_private *dev_priv = engine->i915;
++	struct intel_ringbuffer *ring;
++	int retry = 3, ret;
++
++	ring = engine->default_context->ring[engine->id].ring;
++	if (WARN_ON(ring == NULL))
++		return -ENODEV;
++
++	ret = i915_gem_object_ggtt_pin(ring->obj, 0, 0);
++	if (ret)
++		return ret;
++
++	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
++
++	ret = enable_status_page(engine);
+ 
++reset:
+ 	/* Enforce ordering by reading HEAD register back */
+-	I915_READ_HEAD(ring);
++	engine->write_tail(engine, ring->tail);
++	I915_WRITE_HEAD(engine, ring->head);
++	(void)I915_READ_HEAD(engine);
+ 
+ 	/* Initialize the ring. This must happen _after_ we've cleared the ring
+ 	 * registers with the above sequence (the readback of the HEAD registers
+ 	 * also enforces ordering), otherwise the hw might lose the new ring
+ 	 * register values. */
+-	I915_WRITE_START(ring, i915_gem_obj_ggtt_offset(obj));
+-	I915_WRITE_CTL(ring,
+-			((ringbuf->size - PAGE_SIZE) & RING_NR_PAGES)
+-			| RING_VALID);
+-
+-	/* If the head is still not zero, the ring is dead */
+-	if (wait_for((I915_READ_CTL(ring) & RING_VALID) != 0 &&
+-		     I915_READ_START(ring) == i915_gem_obj_ggtt_offset(obj) &&
+-		     (I915_READ_HEAD(ring) & HEAD_ADDR) == 0, 50)) {
+-		DRM_ERROR("%s initialization failed "
+-			  "ctl %08x (valid? %d) head %08x tail %08x start %08x [expected %08lx]\n",
+-			  ring->name,
+-			  I915_READ_CTL(ring), I915_READ_CTL(ring) & RING_VALID,
+-			  I915_READ_HEAD(ring), I915_READ_TAIL(ring),
+-			  I915_READ_START(ring), (unsigned long)i915_gem_obj_ggtt_offset(obj));
+-		ret = -EIO;
+-		goto out;
+-	}
++	I915_WRITE_START(engine, i915_gem_obj_ggtt_offset(ring->obj));
+ 
+-	if (!drm_core_check_feature(ring->dev, DRIVER_MODESET))
+-		i915_kernel_lost_context(ring->dev);
+-	else {
+-		ringbuf->head = I915_READ_HEAD(ring);
+-		ringbuf->tail = I915_READ_TAIL(ring) & TAIL_ADDR;
+-		ringbuf->space = ring_space(ringbuf);
+-		ringbuf->last_retired_head = -1;
++	/* WaClearRingBufHeadRegAtInit:ctg,elk */
++	if (I915_READ_HEAD(engine) != ring->head)
++		DRM_DEBUG("%s initialization failed [head=%08x], fudging\n",
++			  engine->name, I915_READ_HEAD(engine));
++	I915_WRITE_HEAD(engine, ring->head);
++	(void)I915_READ_HEAD(engine);
++
++	I915_WRITE_CTL(engine,
++		       ((ring->size - PAGE_SIZE) & RING_NR_PAGES)
++		       | RING_VALID);
++
++	if (wait_for((I915_READ_CTL(engine) & RING_VALID) != 0, 50)) {
++		if (retry-- && engine_stop(engine))
++			goto reset;
+ 	}
+ 
+-	memset(&ring->hangcheck, 0, sizeof(ring->hangcheck));
++	if ((I915_READ_CTL(engine) & RING_VALID) == 0 ||
++	    I915_READ_START(engine) != i915_gem_obj_ggtt_offset(ring->obj)) {
++		DRM_ERROR("%s initialization failed "
++			  "ctl %08x (valid? %d) head %08x [expected %08x], tail %08x [expected %08x], start %08x [expected %08lx]\n",
++			  engine->name,
++			  I915_READ_CTL(engine), I915_READ_CTL(engine) & RING_VALID,
++			  I915_READ_HEAD(engine), ring->head,
++			  I915_READ_TAIL(engine), ring->tail,
++			  I915_READ_START(engine), (unsigned long)i915_gem_obj_ggtt_offset(ring->obj));
++		ret = -EIO;
++	}
+ 
+-out:
+ 	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
+-
+ 	return ret;
+ }
+ 
++static int engine_add_request(struct i915_gem_request *rq)
++{
++	rq->engine->write_tail(rq->engine, rq->tail);
++	list_add_tail(&rq->engine_link, &rq->engine->requests);
++	return 0;
++}
++
++static bool engine_rq_is_complete(struct i915_gem_request *rq)
++{
++	return __i915_seqno_passed(intel_engine_get_seqno(rq->engine),
++				   rq->seqno);
++}
++
+ static int
+-init_pipe_control(struct intel_engine_cs *ring)
++init_broken_cs_tlb_wa(struct intel_engine_cs *engine)
+ {
++	struct drm_i915_gem_object *obj;
+ 	int ret;
+ 
+-	if (ring->scratch.obj)
+-		return 0;
++	obj = i915_gem_object_create_stolen(engine->i915->dev, I830_WA_SIZE);
++	if (obj == NULL)
++		obj = i915_gem_alloc_object(engine->i915->dev, I830_WA_SIZE);
++	if (obj == NULL) {
++		DRM_ERROR("Failed to allocate batch bo\n");
++		return -ENOMEM;
++	}
++
++	ret = i915_gem_object_ggtt_pin(obj, 0, 0);
++	if (ret != 0) {
++		drm_gem_object_unreference(&obj->base);
++		DRM_ERROR("Failed to ping batch bo\n");
++		return ret;
++	}
++
++	engine->scratch.obj = obj;
++	engine->scratch.gtt_offset = i915_gem_obj_ggtt_offset(obj);
++	return 0;
++}
++
++static int
++init_pipe_control(struct intel_engine_cs *engine)
++{
++	int ret;
+ 
+-	ring->scratch.obj = i915_gem_alloc_object(ring->dev, 4096);
+-	if (ring->scratch.obj == NULL) {
+-		DRM_ERROR("Failed to allocate seqno page\n");
++	engine->scratch.obj = i915_gem_alloc_object(engine->i915->dev, 4096);
++	if (engine->scratch.obj == NULL) {
+ 		ret = -ENOMEM;
+ 		goto err;
+ 	}
+ 
+-	ret = i915_gem_object_set_cache_level(ring->scratch.obj, I915_CACHE_LLC);
++	ret = i915_gem_object_set_cache_level(engine->scratch.obj,
++					      I915_CACHE_LLC);
+ 	if (ret)
+ 		goto err_unref;
+ 
+-	ret = i915_gem_obj_ggtt_pin(ring->scratch.obj, 4096, 0);
++	ret = i915_gem_object_ggtt_pin(engine->scratch.obj, 4096, 0);
+ 	if (ret)
+ 		goto err_unref;
+ 
+-	ring->scratch.gtt_offset = i915_gem_obj_ggtt_offset(ring->scratch.obj);
+-	ring->scratch.cpu_page = kmap(sg_page(ring->scratch.obj->pages->sgl));
+-	if (ring->scratch.cpu_page == NULL) {
+-		ret = -ENOMEM;
+-		goto err_unpin;
+-	}
+-
++	engine->scratch.gtt_offset =
++		i915_gem_obj_ggtt_offset(engine->scratch.obj);
+ 	DRM_DEBUG_DRIVER("%s pipe control offset: 0x%08x\n",
+-			 ring->name, ring->scratch.gtt_offset);
++			 engine->name, engine->scratch.gtt_offset);
+ 	return 0;
+ 
+-err_unpin:
+-	i915_gem_object_ggtt_unpin(ring->scratch.obj);
+ err_unref:
+-	drm_gem_object_unreference(&ring->scratch.obj->base);
++	drm_gem_object_unreference(&engine->scratch.obj->base);
++	engine->scratch.obj = NULL;
+ err:
++	DRM_ERROR("Failed to allocate seqno page [%d]\n", ret);
+ 	return ret;
+ }
+ 
+-static int init_render_ring(struct intel_engine_cs *ring)
++static int
++emit_lri(struct i915_gem_request *rq,
++	 int num_registers,
++	 ...)
++{
++	struct intel_ringbuffer *ring;
++	va_list ap;
++
++	BUG_ON(num_registers > 60);
++
++	ring = intel_ring_begin(rq, 2*num_registers + 1);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
++
++	intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(num_registers));
++	va_start(ap, num_registers);
++	while (num_registers--) {
++		intel_ring_emit(ring, va_arg(ap, u32));
++		intel_ring_emit(ring, va_arg(ap, u32));
++	}
++	va_end(ap);
++	intel_ring_advance(ring);
++
++	return 0;
++}
++
++static int bdw_render_init_context(struct i915_gem_request *rq)
+ {
+-	struct drm_device *dev = ring->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	int ret = init_ring_common(ring);
++	int ret;
++
++	ret = emit_lri(rq, 6,
++
++	/* FIXME: Unclear whether we really need this on production bdw. */
++	GEN8_ROW_CHICKEN,
++	/* WaDisablePartialInstShootdown:bdw */
++	_MASKED_BIT_ENABLE(PARTIAL_INSTRUCTION_SHOOTDOWN_DISABLE) |
++	/* WaDisableThreadStallDopClockGating:bdw (pre-production) */
++	_MASKED_BIT_ENABLE(STALL_DOP_GATING_DISABLE),
++
++	GEN7_ROW_CHICKEN2,
++	/* WaDisableDopClockGating:bdw */
++	_MASKED_BIT_ENABLE(DOP_CLOCK_GATING_DISABLE),
++
++	HALF_SLICE_CHICKEN3,
++	_MASKED_BIT_ENABLE(GEN8_SAMPLER_POWER_BYPASS_DIS),
++
++	/* Use Force Non-Coherent whenever executing a 3D context. This is a
++	 * workaround for for a possible hang in the unlikely event a TLB
++	 * invalidation occurs during a PSD flush.
++	 */
++	HDC_CHICKEN0,
++	_MASKED_BIT_ENABLE(HDC_FORCE_NON_COHERENT) |
++	/* WaDisableFenceDestinationToSLM:bdw (GT3 pre-production) */
++	_MASKED_BIT_ENABLE(IS_BDW_GT3(rq->i915) ? HDC_FENCE_DEST_SLM_DISABLE : 0),
++
++	CACHE_MODE_1,
++	/* Wa4x4STCOptimizationDisable:bdw */
++	_MASKED_BIT_ENABLE(GEN8_4x4_STC_OPTIMIZATION_DISABLE),
++
++	/*
++	 * BSpec recommends 8x4 when MSAA is used,
++	 * however in practice 16x4 seems fastest.
++	 *
++	 * Note that PS/WM thread counts depend on the WIZ hashing
++	 * disable bit, which we don't touch here, but it's good
++	 * to keep in mind (see 3DSTATE_PS and 3DSTATE_WM).
++	 */
++	GEN7_GT_MODE,
++	GEN6_WIZ_HASHING_MASK | GEN6_WIZ_HASHING_16x4);
++	if (ret)
++		return ret;
++
++	return i915_gem_render_state_init(rq);
++}
++
++static int chv_render_init_context(struct i915_gem_request *rq)
++{
++	int ret;
++
++	ret = emit_lri(rq, 2,
++
++	GEN8_ROW_CHICKEN,
++	/* WaDisablePartialInstShootdown:chv */
++	_MASKED_BIT_ENABLE(PARTIAL_INSTRUCTION_SHOOTDOWN_DISABLE) |
++	/* WaDisableThreadStallDopClockGating:chv */
++	_MASKED_BIT_ENABLE(STALL_DOP_GATING_DISABLE),
++
++	/* Use Force Non-Coherent whenever executing a 3D context. This is a
++	 * workaround for a possible hang in the unlikely event a TLB
++	 * invalidation occurs during a PSD flush.
++	 */
++	HDC_CHICKEN0,
++	/* WaForceEnableNonCoherent:chv */
++	HDC_FORCE_NON_COHERENT |
++	/* WaHdcDisableFetchWhenMasked:chv */
++	HDC_DONOT_FETCH_MEM_WHEN_MASKED);
++
++	if (ret)
++		return ret;
++
++	return i915_gem_render_state_init(rq);
++}
++
++static int render_resume(struct intel_engine_cs *engine)
++{
++	struct drm_i915_private *dev_priv = engine->i915;
++	int ret;
++
++	ret = engine_resume(engine);
+ 	if (ret)
+ 		return ret;
+ 
+ 	/* WaTimedSingleVertexDispatch:cl,bw,ctg,elk,ilk,snb */
+-	if (INTEL_INFO(dev)->gen >= 4 && INTEL_INFO(dev)->gen < 7)
++	if (INTEL_INFO(dev_priv)->gen >= 4 && INTEL_INFO(dev_priv)->gen < 7)
+ 		I915_WRITE(MI_MODE, _MASKED_BIT_ENABLE(VS_TIMER_DISPATCH));
+ 
+ 	/* We need to disable the AsyncFlip performance optimisations in order
+@@ -635,28 +809,22 @@
+ 	 *
+ 	 * WaDisableAsyncFlipPerfMode:snb,ivb,hsw,vlv,bdw,chv
+ 	 */
+-	if (INTEL_INFO(dev)->gen >= 6)
++	if (INTEL_INFO(dev_priv)->gen >= 6 && INTEL_INFO(dev_priv)->gen < 9)
+ 		I915_WRITE(MI_MODE, _MASKED_BIT_ENABLE(ASYNC_FLIP_PERF_DISABLE));
+ 
+ 	/* Required for the hardware to program scanline values for waiting */
+ 	/* WaEnableFlushTlbInvalidationMode:snb */
+-	if (INTEL_INFO(dev)->gen == 6)
++	if (INTEL_INFO(dev_priv)->gen == 6)
+ 		I915_WRITE(GFX_MODE,
+ 			   _MASKED_BIT_ENABLE(GFX_TLB_INVALIDATE_EXPLICIT));
+ 
+ 	/* WaBCSVCSTlbInvalidationMode:ivb,vlv,hsw */
+-	if (IS_GEN7(dev))
++	if (IS_GEN7(dev_priv))
+ 		I915_WRITE(GFX_MODE_GEN7,
+ 			   _MASKED_BIT_ENABLE(GFX_TLB_INVALIDATE_EXPLICIT) |
+ 			   _MASKED_BIT_ENABLE(GFX_REPLAY_MODE));
+ 
+-	if (INTEL_INFO(dev)->gen >= 5) {
+-		ret = init_pipe_control(ring);
+-		if (ret)
+-			return ret;
+-	}
+-
+-	if (IS_GEN6(dev)) {
++	if (IS_GEN6(dev_priv)) {
+ 		/* From the Sandybridge PRM, volume 1 part 3, page 24:
+ 		 * "If this bit is set, STCunit will have LRA as replacement
+ 		 *  policy. [...] This bit must be reset.  LRA replacement
+@@ -666,19 +834,50 @@
+ 			   _MASKED_BIT_DISABLE(CM0_STC_EVICT_DISABLE_LRA_SNB));
+ 	}
+ 
+-	if (INTEL_INFO(dev)->gen >= 6)
++	if (INTEL_INFO(dev_priv)->gen >= 6)
+ 		I915_WRITE(INSTPM, _MASKED_BIT_ENABLE(INSTPM_FORCE_ORDERING));
+ 
+-	if (HAS_L3_DPF(dev))
+-		I915_WRITE_IMR(ring, ~GT_PARITY_ERROR(dev));
++	return 0;
++}
+ 
+-	return ret;
++static void cleanup_status_page(struct intel_engine_cs *engine)
++{
++	struct drm_i915_gem_object *obj;
++
++	obj = engine->status_page.obj;
++	if (obj == NULL)
++		return;
++
++	kunmap(sg_page(obj->pages->sgl));
++	i915_gem_object_ggtt_unpin(obj);
++	drm_gem_object_unreference(&obj->base);
++	engine->status_page.obj = NULL;
++}
++
++static void engine_cleanup(struct intel_engine_cs *engine)
++{
++	cleanup_status_page(engine);
++	i915_cmd_parser_fini_engine(engine);
++}
++
++static bool engine_is_idle(struct intel_engine_cs *engine)
++{
++	struct drm_i915_private *dev_priv = engine->i915;
++	return I915_READ_MODE(engine) & MODE_IDLE;
++}
++
++static bool i8xx_is_idle(struct intel_engine_cs *engine)
++{
++	struct drm_i915_private *dev_priv = engine->i915;
++	return (I915_READ_HEAD(engine) == I915_READ(RING_ACTHD(engine->mmio_base)) &&
++		I915_READ_HEAD(engine) == I915_READ_TAIL(engine));
+ }
+ 
+-static void render_ring_cleanup(struct intel_engine_cs *ring)
++static void render_cleanup(struct intel_engine_cs *engine)
+ {
+-	struct drm_device *dev = ring->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_i915_private *dev_priv = engine->i915;
++
++	engine_cleanup(engine);
+ 
+ 	if (dev_priv->semaphore_obj) {
+ 		i915_gem_object_ggtt_unpin(dev_priv->semaphore_obj);
+@@ -686,163 +885,86 @@
+ 		dev_priv->semaphore_obj = NULL;
+ 	}
+ 
+-	if (ring->scratch.obj == NULL)
+-		return;
+-
+-	if (INTEL_INFO(dev)->gen >= 5) {
+-		kunmap(sg_page(ring->scratch.obj->pages->sgl));
+-		i915_gem_object_ggtt_unpin(ring->scratch.obj);
++	if (engine->scratch.obj) {
++		i915_gem_object_ggtt_unpin(engine->scratch.obj);
++		drm_gem_object_unreference(&engine->scratch.obj->base);
++		engine->scratch.obj = NULL;
+ 	}
+-
+-	drm_gem_object_unreference(&ring->scratch.obj->base);
+-	ring->scratch.obj = NULL;
+ }
+ 
+-static int gen8_rcs_signal(struct intel_engine_cs *signaller,
+-			   unsigned int num_dwords)
++static int
++gen8_rcs_emit_signal(struct i915_gem_request *rq, int id)
+ {
+-#define MBOX_UPDATE_DWORDS 8
+-	struct drm_device *dev = signaller->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *waiter;
+-	int i, ret, num_rings;
+-
+-	num_rings = hweight32(INTEL_INFO(dev)->ring_mask);
+-	num_dwords += (num_rings-1) * MBOX_UPDATE_DWORDS;
+-#undef MBOX_UPDATE_DWORDS
++	u64 offset = GEN8_SEMAPHORE_OFFSET(rq->i915, rq->engine->id, id);
++	struct intel_ringbuffer *ring;
+ 
+-	ret = intel_ring_begin(signaller, num_dwords);
+-	if (ret)
+-		return ret;
++	ring = intel_ring_begin(rq, 8);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+-	for_each_ring(waiter, dev_priv, i) {
+-		u64 gtt_offset = signaller->semaphore.signal_ggtt[i];
+-		if (gtt_offset == MI_SEMAPHORE_SYNC_INVALID)
+-			continue;
+-
+-		intel_ring_emit(signaller, GFX_OP_PIPE_CONTROL(6));
+-		intel_ring_emit(signaller, PIPE_CONTROL_GLOBAL_GTT_IVB |
+-					   PIPE_CONTROL_QW_WRITE |
+-					   PIPE_CONTROL_FLUSH_ENABLE);
+-		intel_ring_emit(signaller, lower_32_bits(gtt_offset));
+-		intel_ring_emit(signaller, upper_32_bits(gtt_offset));
+-		intel_ring_emit(signaller, signaller->outstanding_lazy_seqno);
+-		intel_ring_emit(signaller, 0);
+-		intel_ring_emit(signaller, MI_SEMAPHORE_SIGNAL |
+-					   MI_SEMAPHORE_TARGET(waiter->id));
+-		intel_ring_emit(signaller, 0);
+-	}
+-
+-	return 0;
+-}
+-
+-static int gen8_xcs_signal(struct intel_engine_cs *signaller,
+-			   unsigned int num_dwords)
+-{
+-#define MBOX_UPDATE_DWORDS 6
+-	struct drm_device *dev = signaller->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *waiter;
+-	int i, ret, num_rings;
+-
+-	num_rings = hweight32(INTEL_INFO(dev)->ring_mask);
+-	num_dwords += (num_rings-1) * MBOX_UPDATE_DWORDS;
+-#undef MBOX_UPDATE_DWORDS
+-
+-	ret = intel_ring_begin(signaller, num_dwords);
+-	if (ret)
+-		return ret;
+-
+-	for_each_ring(waiter, dev_priv, i) {
+-		u64 gtt_offset = signaller->semaphore.signal_ggtt[i];
+-		if (gtt_offset == MI_SEMAPHORE_SYNC_INVALID)
+-			continue;
+-
+-		intel_ring_emit(signaller, (MI_FLUSH_DW + 1) |
+-					   MI_FLUSH_DW_OP_STOREDW);
+-		intel_ring_emit(signaller, lower_32_bits(gtt_offset) |
+-					   MI_FLUSH_DW_USE_GTT);
+-		intel_ring_emit(signaller, upper_32_bits(gtt_offset));
+-		intel_ring_emit(signaller, signaller->outstanding_lazy_seqno);
+-		intel_ring_emit(signaller, MI_SEMAPHORE_SIGNAL |
+-					   MI_SEMAPHORE_TARGET(waiter->id));
+-		intel_ring_emit(signaller, 0);
+-	}
++	intel_ring_emit(ring, GFX_OP_PIPE_CONTROL(6));
++	intel_ring_emit(ring,
++			PIPE_CONTROL_GLOBAL_GTT_IVB |
++			PIPE_CONTROL_QW_WRITE |
++			PIPE_CONTROL_FLUSH_ENABLE);
++	intel_ring_emit(ring, lower_32_bits(offset));
++	intel_ring_emit(ring, upper_32_bits(offset));
++	intel_ring_emit(ring, rq->seqno);
++	intel_ring_emit(ring, 0);
++	intel_ring_emit(ring,
++			MI_SEMAPHORE_SIGNAL |
++			MI_SEMAPHORE_TARGET(id));
++	intel_ring_emit(ring, 0);
++	intel_ring_advance(ring);
+ 
+ 	return 0;
+ }
+ 
+-static int gen6_signal(struct intel_engine_cs *signaller,
+-		       unsigned int num_dwords)
++static int
++gen8_xcs_emit_signal(struct i915_gem_request *rq, int id)
+ {
+-	struct drm_device *dev = signaller->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *useless;
+-	int i, ret, num_rings;
+-
+-#define MBOX_UPDATE_DWORDS 3
+-	num_rings = hweight32(INTEL_INFO(dev)->ring_mask);
+-	num_dwords += round_up((num_rings-1) * MBOX_UPDATE_DWORDS, 2);
+-#undef MBOX_UPDATE_DWORDS
+-
+-	ret = intel_ring_begin(signaller, num_dwords);
+-	if (ret)
+-		return ret;
++	u64 offset = GEN8_SEMAPHORE_OFFSET(rq->i915, rq->engine->id, id);
++	struct intel_ringbuffer *ring;
+ 
+-	for_each_ring(useless, dev_priv, i) {
+-		u32 mbox_reg = signaller->semaphore.mbox.signal[i];
+-		if (mbox_reg != GEN6_NOSYNC) {
+-			intel_ring_emit(signaller, MI_LOAD_REGISTER_IMM(1));
+-			intel_ring_emit(signaller, mbox_reg);
+-			intel_ring_emit(signaller, signaller->outstanding_lazy_seqno);
+-		}
+-	}
++	ring = intel_ring_begin(rq, 6);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+-	/* If num_dwords was rounded, make sure the tail pointer is correct */
+-	if (num_rings % 2 == 0)
+-		intel_ring_emit(signaller, MI_NOOP);
++	intel_ring_emit(ring,
++			MI_FLUSH_DW |
++			MI_FLUSH_DW_OP_STOREDW |
++			(4 - 2));
++	intel_ring_emit(ring,
++			lower_32_bits(offset) |
++			MI_FLUSH_DW_USE_GTT);
++	intel_ring_emit(ring, upper_32_bits(offset));
++	intel_ring_emit(ring, rq->seqno);
++	intel_ring_emit(ring,
++			MI_SEMAPHORE_SIGNAL |
++			MI_SEMAPHORE_TARGET(id));
++	intel_ring_emit(ring, 0);
++	intel_ring_advance(ring);
+ 
+ 	return 0;
+ }
+ 
+-/**
+- * gen6_add_request - Update the semaphore mailbox registers
+- * 
+- * @ring - ring that is adding a request
+- * @seqno - return seqno stuck into the ring
+- *
+- * Update the mailbox registers in the *other* rings with the current seqno.
+- * This acts like a signal in the canonical semaphore.
+- */
+ static int
+-gen6_add_request(struct intel_engine_cs *ring)
++gen6_emit_signal(struct i915_gem_request *rq, int id)
+ {
+-	int ret;
+-
+-	if (ring->semaphore.signal)
+-		ret = ring->semaphore.signal(ring, 4);
+-	else
+-		ret = intel_ring_begin(ring, 4);
++	struct intel_ringbuffer *ring;
+ 
+-	if (ret)
+-		return ret;
++	ring = intel_ring_begin(rq, 3);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+-	intel_ring_emit(ring, MI_STORE_DWORD_INDEX);
+-	intel_ring_emit(ring, I915_GEM_HWS_INDEX << MI_STORE_DWORD_INDEX_SHIFT);
+-	intel_ring_emit(ring, ring->outstanding_lazy_seqno);
+-	intel_ring_emit(ring, MI_USER_INTERRUPT);
+-	__intel_ring_advance(ring);
++	intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
++	intel_ring_emit(ring, rq->engine->semaphore.mbox.signal[id]);
++	intel_ring_emit(ring, rq->seqno);
++	intel_ring_advance(ring);
+ 
+ 	return 0;
+ }
+ 
+-static inline bool i915_gem_has_seqno_wrapped(struct drm_device *dev,
+-					      u32 seqno)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	return dev_priv->last_seqno < seqno;
+-}
+-
+ /**
+  * intel_ring_sync - sync the waiter to the signaller on seqno
+  *
+@@ -852,504 +974,292 @@
+  */
+ 
+ static int
+-gen8_ring_sync(struct intel_engine_cs *waiter,
+-	       struct intel_engine_cs *signaller,
+-	       u32 seqno)
++gen8_emit_wait(struct i915_gem_request *waiter,
++	       struct i915_gem_request *signaller)
+ {
+-	struct drm_i915_private *dev_priv = waiter->dev->dev_private;
+-	int ret;
++	u64 offset = GEN8_SEMAPHORE_OFFSET(waiter->i915, signaller->engine->id, waiter->engine->id);
++	struct intel_ringbuffer *ring;
+ 
+-	ret = intel_ring_begin(waiter, 4);
+-	if (ret)
+-		return ret;
++	ring = intel_ring_begin(waiter, 4);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+-	intel_ring_emit(waiter, MI_SEMAPHORE_WAIT |
+-				MI_SEMAPHORE_GLOBAL_GTT |
+-				MI_SEMAPHORE_POLL |
+-				MI_SEMAPHORE_SAD_GTE_SDD);
+-	intel_ring_emit(waiter, seqno);
+-	intel_ring_emit(waiter,
+-			lower_32_bits(GEN8_WAIT_OFFSET(waiter, signaller->id)));
+-	intel_ring_emit(waiter,
+-			upper_32_bits(GEN8_WAIT_OFFSET(waiter, signaller->id)));
+-	intel_ring_advance(waiter);
++	intel_ring_emit(ring,
++			MI_SEMAPHORE_WAIT |
++			MI_SEMAPHORE_GLOBAL_GTT |
++			MI_SEMAPHORE_POLL |
++			MI_SEMAPHORE_SAD_GTE_SDD);
++	intel_ring_emit(ring, signaller->breadcrumb[waiter->engine->id]);
++	intel_ring_emit(ring, lower_32_bits(offset));
++	intel_ring_emit(ring, upper_32_bits(offset));
++	intel_ring_advance(ring);
+ 	return 0;
+ }
+ 
+ static int
+-gen6_ring_sync(struct intel_engine_cs *waiter,
+-	       struct intel_engine_cs *signaller,
+-	       u32 seqno)
++gen6_emit_wait(struct i915_gem_request *waiter,
++	       struct i915_gem_request *signaller)
+ {
+ 	u32 dw1 = MI_SEMAPHORE_MBOX |
+ 		  MI_SEMAPHORE_COMPARE |
+ 		  MI_SEMAPHORE_REGISTER;
+-	u32 wait_mbox = signaller->semaphore.mbox.wait[waiter->id];
+-	int ret;
+-
+-	/* Throughout all of the GEM code, seqno passed implies our current
+-	 * seqno is >= the last seqno executed. However for hardware the
+-	 * comparison is strictly greater than.
+-	 */
+-	seqno -= 1;
++	u32 wait_mbox = signaller->engine->semaphore.mbox.wait[waiter->engine->id];
++	struct intel_ringbuffer *ring;
+ 
+ 	WARN_ON(wait_mbox == MI_SEMAPHORE_SYNC_INVALID);
+ 
+-	ret = intel_ring_begin(waiter, 4);
+-	if (ret)
+-		return ret;
+-
+-	/* If seqno wrap happened, omit the wait with no-ops */
+-	if (likely(!i915_gem_has_seqno_wrapped(waiter->dev, seqno))) {
+-		intel_ring_emit(waiter, dw1 | wait_mbox);
+-		intel_ring_emit(waiter, seqno);
+-		intel_ring_emit(waiter, 0);
+-		intel_ring_emit(waiter, MI_NOOP);
+-	} else {
+-		intel_ring_emit(waiter, MI_NOOP);
+-		intel_ring_emit(waiter, MI_NOOP);
+-		intel_ring_emit(waiter, MI_NOOP);
+-		intel_ring_emit(waiter, MI_NOOP);
+-	}
+-	intel_ring_advance(waiter);
+-
+-	return 0;
+-}
+-
+-#define PIPE_CONTROL_FLUSH(ring__, addr__)					\
+-do {									\
+-	intel_ring_emit(ring__, GFX_OP_PIPE_CONTROL(4) | PIPE_CONTROL_QW_WRITE |		\
+-		 PIPE_CONTROL_DEPTH_STALL);				\
+-	intel_ring_emit(ring__, (addr__) | PIPE_CONTROL_GLOBAL_GTT);			\
+-	intel_ring_emit(ring__, 0);							\
+-	intel_ring_emit(ring__, 0);							\
+-} while (0)
+-
+-static int
+-pc_render_add_request(struct intel_engine_cs *ring)
+-{
+-	u32 scratch_addr = ring->scratch.gtt_offset + 2 * CACHELINE_BYTES;
+-	int ret;
++	ring = intel_ring_begin(waiter, 3);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+-	/* For Ironlake, MI_USER_INTERRUPT was deprecated and apparently
+-	 * incoherent with writes to memory, i.e. completely fubar,
+-	 * so we need to use PIPE_NOTIFY instead.
+-	 *
+-	 * However, we also need to workaround the qword write
+-	 * incoherence by flushing the 6 PIPE_NOTIFY buffers out to
+-	 * memory before requesting an interrupt.
++	intel_ring_emit(ring, dw1 | wait_mbox);
++	/* Throughout all of the GEM code, seqno passed implies our current
++	 * seqno is >= the last seqno executed. However for hardware the
++	 * comparison is strictly greater than.
+ 	 */
+-	ret = intel_ring_begin(ring, 32);
+-	if (ret)
+-		return ret;
+-
+-	intel_ring_emit(ring, GFX_OP_PIPE_CONTROL(4) | PIPE_CONTROL_QW_WRITE |
+-			PIPE_CONTROL_WRITE_FLUSH |
+-			PIPE_CONTROL_TEXTURE_CACHE_INVALIDATE);
+-	intel_ring_emit(ring, ring->scratch.gtt_offset | PIPE_CONTROL_GLOBAL_GTT);
+-	intel_ring_emit(ring, ring->outstanding_lazy_seqno);
++	intel_ring_emit(ring, signaller->breadcrumb[waiter->engine->id] - 1);
+ 	intel_ring_emit(ring, 0);
+-	PIPE_CONTROL_FLUSH(ring, scratch_addr);
+-	scratch_addr += 2 * CACHELINE_BYTES; /* write to separate cachelines */
+-	PIPE_CONTROL_FLUSH(ring, scratch_addr);
+-	scratch_addr += 2 * CACHELINE_BYTES;
+-	PIPE_CONTROL_FLUSH(ring, scratch_addr);
+-	scratch_addr += 2 * CACHELINE_BYTES;
+-	PIPE_CONTROL_FLUSH(ring, scratch_addr);
+-	scratch_addr += 2 * CACHELINE_BYTES;
+-	PIPE_CONTROL_FLUSH(ring, scratch_addr);
+-	scratch_addr += 2 * CACHELINE_BYTES;
+-	PIPE_CONTROL_FLUSH(ring, scratch_addr);
+-
+-	intel_ring_emit(ring, GFX_OP_PIPE_CONTROL(4) | PIPE_CONTROL_QW_WRITE |
+-			PIPE_CONTROL_WRITE_FLUSH |
+-			PIPE_CONTROL_TEXTURE_CACHE_INVALIDATE |
+-			PIPE_CONTROL_NOTIFY);
+-	intel_ring_emit(ring, ring->scratch.gtt_offset | PIPE_CONTROL_GLOBAL_GTT);
+-	intel_ring_emit(ring, ring->outstanding_lazy_seqno);
+-	intel_ring_emit(ring, 0);
+-	__intel_ring_advance(ring);
++	intel_ring_advance(ring);
+ 
+ 	return 0;
+ }
+ 
+-static u32
+-gen6_ring_get_seqno(struct intel_engine_cs *ring, bool lazy_coherency)
+-{
+-	/* Workaround to force correct ordering between irq and seqno writes on
+-	 * ivb (and maybe also on snb) by reading from a CS register (like
+-	 * ACTHD) before reading the status page. */
+-	if (!lazy_coherency) {
+-		struct drm_i915_private *dev_priv = ring->dev->dev_private;
+-		POSTING_READ(RING_ACTHD(ring->mmio_base));
+-	}
+-
+-	return intel_read_status_page(ring, I915_GEM_HWS_INDEX);
+-}
+-
+-static u32
+-ring_get_seqno(struct intel_engine_cs *ring, bool lazy_coherency)
+-{
+-	return intel_read_status_page(ring, I915_GEM_HWS_INDEX);
+-}
+-
+ static void
+-ring_set_seqno(struct intel_engine_cs *ring, u32 seqno)
++gen5_irq_get(struct intel_engine_cs *engine)
+ {
+-	intel_write_status_page(ring, I915_GEM_HWS_INDEX, seqno);
+-}
+-
+-static u32
+-pc_render_get_seqno(struct intel_engine_cs *ring, bool lazy_coherency)
+-{
+-	return ring->scratch.cpu_page[0];
+-}
+-
+-static void
+-pc_render_set_seqno(struct intel_engine_cs *ring, u32 seqno)
+-{
+-	ring->scratch.cpu_page[0] = seqno;
+-}
+-
+-static bool
+-gen5_ring_get_irq(struct intel_engine_cs *ring)
+-{
+-	struct drm_device *dev = ring->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_i915_private *i915 = engine->i915;
+ 	unsigned long flags;
+ 
+-	if (!dev->irq_enabled)
+-		return false;
+-
+-	spin_lock_irqsave(&dev_priv->irq_lock, flags);
+-	if (ring->irq_refcount++ == 0)
+-		gen5_enable_gt_irq(dev_priv, ring->irq_enable_mask);
+-	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
+-
+-	return true;
++	spin_lock_irqsave(&i915->irq_lock, flags);
++	if (engine->irq_refcount++ == 0)
++		gen5_enable_gt_irq(i915, engine->irq_enable_mask);
++	spin_unlock_irqrestore(&i915->irq_lock, flags);
+ }
+ 
+ static void
+-gen5_ring_put_irq(struct intel_engine_cs *ring)
++gen5_irq_put(struct intel_engine_cs *engine)
+ {
+-	struct drm_device *dev = ring->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_i915_private *i915 = engine->i915;
+ 	unsigned long flags;
+ 
+-	spin_lock_irqsave(&dev_priv->irq_lock, flags);
+-	if (--ring->irq_refcount == 0)
+-		gen5_disable_gt_irq(dev_priv, ring->irq_enable_mask);
+-	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
++	spin_lock_irqsave(&i915->irq_lock, flags);
++	if (--engine->irq_refcount == 0)
++		gen5_disable_gt_irq(i915, engine->irq_enable_mask);
++	spin_unlock_irqrestore(&i915->irq_lock, flags);
+ }
+ 
+-static bool
+-i9xx_ring_get_irq(struct intel_engine_cs *ring)
++static void
++i9xx_irq_get(struct intel_engine_cs *engine)
+ {
+-	struct drm_device *dev = ring->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_i915_private *dev_priv = engine->i915;
+ 	unsigned long flags;
+ 
+-	if (!dev->irq_enabled)
+-		return false;
+-
+ 	spin_lock_irqsave(&dev_priv->irq_lock, flags);
+-	if (ring->irq_refcount++ == 0) {
+-		dev_priv->irq_mask &= ~ring->irq_enable_mask;
++	if (engine->irq_refcount++ == 0) {
++		dev_priv->irq_mask &= ~engine->irq_enable_mask;
+ 		I915_WRITE(IMR, dev_priv->irq_mask);
+ 		POSTING_READ(IMR);
+ 	}
+ 	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
+-
+-	return true;
+ }
+ 
+ static void
+-i9xx_ring_put_irq(struct intel_engine_cs *ring)
++i9xx_irq_put(struct intel_engine_cs *engine)
+ {
+-	struct drm_device *dev = ring->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_i915_private *dev_priv = engine->i915;
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&dev_priv->irq_lock, flags);
+-	if (--ring->irq_refcount == 0) {
+-		dev_priv->irq_mask |= ring->irq_enable_mask;
++	if (--engine->irq_refcount == 0) {
++		dev_priv->irq_mask |= engine->irq_enable_mask;
+ 		I915_WRITE(IMR, dev_priv->irq_mask);
+ 		POSTING_READ(IMR);
+ 	}
+ 	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
+ }
+ 
+-static bool
+-i8xx_ring_get_irq(struct intel_engine_cs *ring)
++static void
++i8xx_irq_get(struct intel_engine_cs *engine)
+ {
+-	struct drm_device *dev = ring->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_i915_private *dev_priv = engine->i915;
+ 	unsigned long flags;
+ 
+-	if (!dev->irq_enabled)
+-		return false;
+-
+ 	spin_lock_irqsave(&dev_priv->irq_lock, flags);
+-	if (ring->irq_refcount++ == 0) {
+-		dev_priv->irq_mask &= ~ring->irq_enable_mask;
++	if (engine->irq_refcount++ == 0) {
++		dev_priv->irq_mask &= ~engine->irq_enable_mask;
+ 		I915_WRITE16(IMR, dev_priv->irq_mask);
+ 		POSTING_READ16(IMR);
+ 	}
+ 	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
+-
+-	return true;
+ }
+ 
+ static void
+-i8xx_ring_put_irq(struct intel_engine_cs *ring)
++i8xx_irq_put(struct intel_engine_cs *engine)
+ {
+-	struct drm_device *dev = ring->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_i915_private *dev_priv = engine->i915;
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&dev_priv->irq_lock, flags);
+-	if (--ring->irq_refcount == 0) {
+-		dev_priv->irq_mask |= ring->irq_enable_mask;
++	if (--engine->irq_refcount == 0) {
++		dev_priv->irq_mask |= engine->irq_enable_mask;
+ 		I915_WRITE16(IMR, dev_priv->irq_mask);
+ 		POSTING_READ16(IMR);
+ 	}
+ 	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
+ }
+ 
+-void intel_ring_setup_status_page(struct intel_engine_cs *ring)
+-{
+-	struct drm_device *dev = ring->dev;
+-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
+-	u32 mmio = 0;
+-
+-	/* The ring status page addresses are no longer next to the rest of
+-	 * the ring registers as of gen7.
+-	 */
+-	if (IS_GEN7(dev)) {
+-		switch (ring->id) {
+-		case RCS:
+-			mmio = RENDER_HWS_PGA_GEN7;
+-			break;
+-		case BCS:
+-			mmio = BLT_HWS_PGA_GEN7;
+-			break;
+-		/*
+-		 * VCS2 actually doesn't exist on Gen7. Only shut up
+-		 * gcc switch check warning
+-		 */
+-		case VCS2:
+-		case VCS:
+-			mmio = BSD_HWS_PGA_GEN7;
+-			break;
+-		case VECS:
+-			mmio = VEBOX_HWS_PGA_GEN7;
+-			break;
+-		}
+-	} else if (IS_GEN6(ring->dev)) {
+-		mmio = RING_HWS_PGA_GEN6(ring->mmio_base);
+-	} else {
+-		/* XXX: gen8 returns to sanity */
+-		mmio = RING_HWS_PGA(ring->mmio_base);
+-	}
+-
+-	I915_WRITE(mmio, (u32)ring->status_page.gfx_addr);
+-	POSTING_READ(mmio);
+-
+-	/*
+-	 * Flush the TLB for this page
+-	 *
+-	 * FIXME: These two bits have disappeared on gen8, so a question
+-	 * arises: do we still need this and if so how should we go about
+-	 * invalidating the TLB?
+-	 */
+-	if (INTEL_INFO(dev)->gen >= 6 && INTEL_INFO(dev)->gen < 8) {
+-		u32 reg = RING_INSTPM(ring->mmio_base);
+-
+-		/* ring should be idle before issuing a sync flush*/
+-		WARN_ON((I915_READ_MODE(ring) & MODE_IDLE) == 0);
+-
+-		I915_WRITE(reg,
+-			   _MASKED_BIT_ENABLE(INSTPM_TLB_INVALIDATE |
+-					      INSTPM_SYNC_FLUSH));
+-		if (wait_for((I915_READ(reg) & INSTPM_SYNC_FLUSH) == 0,
+-			     1000))
+-			DRM_ERROR("%s: wait for SyncFlush to complete for TLB invalidation timed out\n",
+-				  ring->name);
+-	}
+-}
+-
+ static int
+-bsd_ring_flush(struct intel_engine_cs *ring,
+-	       u32     invalidate_domains,
+-	       u32     flush_domains)
++bsd_emit_flush(struct i915_gem_request *rq,
++	       u32 flags)
+ {
+-	int ret;
++	struct intel_ringbuffer *ring;
+ 
+-	ret = intel_ring_begin(ring, 2);
+-	if (ret)
+-		return ret;
++	ring = intel_ring_begin(rq, 1);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+ 	intel_ring_emit(ring, MI_FLUSH);
+-	intel_ring_emit(ring, MI_NOOP);
+ 	intel_ring_advance(ring);
+ 	return 0;
+ }
+ 
+ static int
+-i9xx_add_request(struct intel_engine_cs *ring)
++emit_breadcrumb(struct i915_gem_request *rq)
+ {
+-	int ret;
++	struct intel_ringbuffer *ring;
+ 
+-	ret = intel_ring_begin(ring, 4);
+-	if (ret)
+-		return ret;
++	ring = intel_ring_begin(rq, 4);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+ 	intel_ring_emit(ring, MI_STORE_DWORD_INDEX);
+ 	intel_ring_emit(ring, I915_GEM_HWS_INDEX << MI_STORE_DWORD_INDEX_SHIFT);
+-	intel_ring_emit(ring, ring->outstanding_lazy_seqno);
++	intel_ring_emit(ring, rq->seqno);
+ 	intel_ring_emit(ring, MI_USER_INTERRUPT);
+-	__intel_ring_advance(ring);
++	intel_ring_advance(ring);
+ 
+ 	return 0;
+ }
+ 
+-static bool
+-gen6_ring_get_irq(struct intel_engine_cs *ring)
++static void
++gen6_irq_get(struct intel_engine_cs *engine)
+ {
+-	struct drm_device *dev = ring->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_i915_private *dev_priv = engine->i915;
+ 	unsigned long flags;
+ 
+-	if (!dev->irq_enabled)
+-	       return false;
+-
+ 	spin_lock_irqsave(&dev_priv->irq_lock, flags);
+-	if (ring->irq_refcount++ == 0) {
+-		if (HAS_L3_DPF(dev) && ring->id == RCS)
+-			I915_WRITE_IMR(ring,
+-				       ~(ring->irq_enable_mask |
+-					 GT_PARITY_ERROR(dev)));
+-		else
+-			I915_WRITE_IMR(ring, ~ring->irq_enable_mask);
+-		gen5_enable_gt_irq(dev_priv, ring->irq_enable_mask);
++	if (engine->irq_refcount++ == 0) {
++		I915_WRITE_IMR(engine,
++			       ~(engine->irq_enable_mask |
++				 engine->irq_keep_mask));
++		gen5_enable_gt_irq(dev_priv, engine->irq_enable_mask);
+ 	}
+ 	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
+ 
+-	return true;
++	/* Keep the device awake to save expensive CPU cycles when
++	 * reading the registers.
++	 */
++	gen6_gt_force_wake_get(dev_priv, engine->power_domains);
++}
++
++static void
++gen6_irq_barrier(struct intel_engine_cs *engine)
++{
++	/* w/a for lax serialisation of GPU writes with IRQs */
++	struct drm_i915_private *dev_priv = engine->i915;
++	(void)I915_READ(RING_ACTHD(engine->mmio_base));
+ }
+ 
+ static void
+-gen6_ring_put_irq(struct intel_engine_cs *ring)
++gen6_irq_put(struct intel_engine_cs *engine)
+ {
+-	struct drm_device *dev = ring->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_i915_private *dev_priv = engine->i915;
+ 	unsigned long flags;
+ 
++	gen6_gt_force_wake_put(dev_priv, engine->power_domains);
++
+ 	spin_lock_irqsave(&dev_priv->irq_lock, flags);
+-	if (--ring->irq_refcount == 0) {
+-		if (HAS_L3_DPF(dev) && ring->id == RCS)
+-			I915_WRITE_IMR(ring, ~GT_PARITY_ERROR(dev));
+-		else
+-			I915_WRITE_IMR(ring, ~0);
+-		gen5_disable_gt_irq(dev_priv, ring->irq_enable_mask);
++	if (--engine->irq_refcount == 0) {
++		I915_WRITE_IMR(engine, ~engine->irq_keep_mask);
++		gen5_disable_gt_irq(dev_priv, engine->irq_enable_mask);
+ 	}
+ 	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
+ }
+ 
+-static bool
+-hsw_vebox_get_irq(struct intel_engine_cs *ring)
++static void
++hsw_vebox_irq_get(struct intel_engine_cs *engine)
+ {
+-	struct drm_device *dev = ring->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_i915_private *dev_priv = engine->i915;
+ 	unsigned long flags;
+ 
+-	if (!dev->irq_enabled)
+-		return false;
+-
+ 	spin_lock_irqsave(&dev_priv->irq_lock, flags);
+-	if (ring->irq_refcount++ == 0) {
+-		I915_WRITE_IMR(ring, ~ring->irq_enable_mask);
+-		gen6_enable_pm_irq(dev_priv, ring->irq_enable_mask);
++	if (engine->irq_refcount++ == 0) {
++		I915_WRITE_IMR(engine,
++			       ~(engine->irq_enable_mask |
++				 engine->irq_keep_mask));
++		gen6_enable_pm_irq(dev_priv, engine->irq_enable_mask);
+ 	}
+ 	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
+ 
+-	return true;
++	gen6_gt_force_wake_get(dev_priv, engine->power_domains);
+ }
+ 
+ static void
+-hsw_vebox_put_irq(struct intel_engine_cs *ring)
++hsw_vebox_irq_put(struct intel_engine_cs *engine)
+ {
+-	struct drm_device *dev = ring->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_i915_private *dev_priv = engine->i915;
+ 	unsigned long flags;
+ 
+-	if (!dev->irq_enabled)
+-		return;
++	gen6_gt_force_wake_put(dev_priv, engine->power_domains);
+ 
+ 	spin_lock_irqsave(&dev_priv->irq_lock, flags);
+-	if (--ring->irq_refcount == 0) {
+-		I915_WRITE_IMR(ring, ~0);
+-		gen6_disable_pm_irq(dev_priv, ring->irq_enable_mask);
++	if (--engine->irq_refcount == 0) {
++		I915_WRITE_IMR(engine, ~engine->irq_keep_mask);
++		gen6_disable_pm_irq(dev_priv, engine->irq_enable_mask);
+ 	}
+ 	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
+ }
+ 
+-static bool
+-gen8_ring_get_irq(struct intel_engine_cs *ring)
++static void
++gen8_irq_get(struct intel_engine_cs *engine)
+ {
+-	struct drm_device *dev = ring->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_i915_private *dev_priv = engine->i915;
+ 	unsigned long flags;
+ 
+-	if (!dev->irq_enabled)
+-		return false;
+-
+ 	spin_lock_irqsave(&dev_priv->irq_lock, flags);
+-	if (ring->irq_refcount++ == 0) {
+-		if (HAS_L3_DPF(dev) && ring->id == RCS) {
+-			I915_WRITE_IMR(ring,
+-				       ~(ring->irq_enable_mask |
+-					 GT_RENDER_L3_PARITY_ERROR_INTERRUPT));
+-		} else {
+-			I915_WRITE_IMR(ring, ~ring->irq_enable_mask);
+-		}
+-		POSTING_READ(RING_IMR(ring->mmio_base));
++	if (engine->irq_refcount++ == 0) {
++		I915_WRITE_IMR(engine,
++			       ~(engine->irq_enable_mask |
++				 engine->irq_keep_mask));
++		POSTING_READ(RING_IMR(engine->mmio_base));
+ 	}
+ 	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
+-
+-	return true;
+ }
+ 
+ static void
+-gen8_ring_put_irq(struct intel_engine_cs *ring)
++gen8_irq_put(struct intel_engine_cs *engine)
+ {
+-	struct drm_device *dev = ring->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct drm_i915_private *dev_priv = engine->i915;
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&dev_priv->irq_lock, flags);
+-	if (--ring->irq_refcount == 0) {
+-		if (HAS_L3_DPF(dev) && ring->id == RCS) {
+-			I915_WRITE_IMR(ring,
+-				       ~GT_RENDER_L3_PARITY_ERROR_INTERRUPT);
+-		} else {
+-			I915_WRITE_IMR(ring, ~0);
+-		}
+-		POSTING_READ(RING_IMR(ring->mmio_base));
++	if (--engine->irq_refcount == 0) {
++		I915_WRITE_IMR(engine, ~engine->irq_keep_mask);
++		POSTING_READ(RING_IMR(engine->mmio_base));
+ 	}
+ 	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
+ }
+ 
+ static int
+-i965_dispatch_execbuffer(struct intel_engine_cs *ring,
+-			 u64 offset, u32 length,
+-			 unsigned flags)
+-{
+-	int ret;
+-
+-	ret = intel_ring_begin(ring, 2);
+-	if (ret)
+-		return ret;
++i965_emit_batchbuffer(struct i915_gem_request *rq,
++		      u64 offset, u32 length,
++		      unsigned flags)
++{
++	struct intel_ringbuffer *ring;
++
++	ring = intel_ring_begin(rq, 2);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+ 	intel_ring_emit(ring,
+ 			MI_BATCH_BUFFER_START |
+@@ -1361,38 +1271,35 @@
+ 	return 0;
+ }
+ 
+-/* Just userspace ABI convention to limit the wa batch bo to a resonable size */
+-#define I830_BATCH_LIMIT (256*1024)
+-#define I830_TLB_ENTRIES (2)
+-#define I830_WA_SIZE max(I830_TLB_ENTRIES*4096, I830_BATCH_LIMIT)
+ static int
+-i830_dispatch_execbuffer(struct intel_engine_cs *ring,
+-				u64 offset, u32 len,
+-				unsigned flags)
+-{
+-	u32 cs_offset = ring->scratch.gtt_offset;
+-	int ret;
+-
+-	ret = intel_ring_begin(ring, 6);
+-	if (ret)
+-		return ret;
+-
+-	/* Evict the invalid PTE TLBs */
+-	intel_ring_emit(ring, COLOR_BLT_CMD | BLT_WRITE_RGBA);
+-	intel_ring_emit(ring, BLT_DEPTH_32 | BLT_ROP_COLOR_COPY | 4096);
+-	intel_ring_emit(ring, I830_TLB_ENTRIES << 16 | 4); /* load each page */
+-	intel_ring_emit(ring, cs_offset);
+-	intel_ring_emit(ring, 0xdeadbeef);
+-	intel_ring_emit(ring, MI_NOOP);
+-	intel_ring_advance(ring);
++i830_emit_batchbuffer(struct i915_gem_request *rq,
++		      u64 offset, u32 len,
++		      unsigned flags)
++{
++	u32 cs_offset = rq->engine->scratch.gtt_offset;
++	struct intel_ringbuffer *ring;
++
++	if (rq->batch->vm->dirty) {
++		ring = intel_ring_begin(rq, 5);
++		if (IS_ERR(ring))
++			return PTR_ERR(ring);
++
++		/* Evict the invalid PTE TLBs */
++		intel_ring_emit(ring, COLOR_BLT_CMD | BLT_WRITE_RGBA);
++		intel_ring_emit(ring, BLT_DEPTH_32 | BLT_ROP_COLOR_COPY | 4096);
++		intel_ring_emit(ring, I830_TLB_ENTRIES << 16 | 4);
++		intel_ring_emit(ring, cs_offset);
++		intel_ring_emit(ring, 0xdeadbeef);
++		intel_ring_advance(ring);
++	}
+ 
+ 	if ((flags & I915_DISPATCH_PINNED) == 0) {
+ 		if (len > I830_BATCH_LIMIT)
+ 			return -ENOSPC;
+ 
+-		ret = intel_ring_begin(ring, 6 + 2);
+-		if (ret)
+-			return ret;
++		ring = intel_ring_begin(rq, 6 + 1);
++		if (IS_ERR(ring))
++			return PTR_ERR(ring);
+ 
+ 		/* Blit the batch (which has now all relocs applied) to the
+ 		 * stable batch scratch bo area (so that the CS never
+@@ -1406,36 +1313,34 @@
+ 		intel_ring_emit(ring, offset);
+ 
+ 		intel_ring_emit(ring, MI_FLUSH);
+-		intel_ring_emit(ring, MI_NOOP);
+ 		intel_ring_advance(ring);
+ 
+ 		/* ... and execute it. */
+ 		offset = cs_offset;
+ 	}
+ 
+-	ret = intel_ring_begin(ring, 4);
+-	if (ret)
+-		return ret;
++	ring = intel_ring_begin(rq, 3);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+ 	intel_ring_emit(ring, MI_BATCH_BUFFER);
+ 	intel_ring_emit(ring, offset | (flags & I915_DISPATCH_SECURE ? 0 : MI_BATCH_NON_SECURE));
+ 	intel_ring_emit(ring, offset + len - 8);
+-	intel_ring_emit(ring, MI_NOOP);
+ 	intel_ring_advance(ring);
+ 
+ 	return 0;
+ }
+ 
+ static int
+-i915_dispatch_execbuffer(struct intel_engine_cs *ring,
+-			 u64 offset, u32 len,
+-			 unsigned flags)
+-{
+-	int ret;
+-
+-	ret = intel_ring_begin(ring, 2);
+-	if (ret)
+-		return ret;
++i915_emit_batchbuffer(struct i915_gem_request *rq,
++		      u64 offset, u32 len,
++		      unsigned flags)
++{
++	struct intel_ringbuffer *ring;
++
++	ring = intel_ring_begin(rq, 2);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+ 	intel_ring_emit(ring, MI_BATCH_BUFFER_START | MI_BATCH_GTT);
+ 	intel_ring_emit(ring, offset | (flags & I915_DISPATCH_SECURE ? 0 : MI_BATCH_NON_SECURE));
+@@ -1444,485 +1349,434 @@
+ 	return 0;
+ }
+ 
+-static void cleanup_status_page(struct intel_engine_cs *ring)
+-{
+-	struct drm_i915_gem_object *obj;
+-
+-	obj = ring->status_page.obj;
+-	if (obj == NULL)
+-		return;
+-
+-	kunmap(sg_page(obj->pages->sgl));
+-	i915_gem_object_ggtt_unpin(obj);
+-	drm_gem_object_unreference(&obj->base);
+-	ring->status_page.obj = NULL;
+-}
+-
+-static int init_status_page(struct intel_engine_cs *ring)
++static int setup_status_page(struct intel_engine_cs *engine)
+ {
+ 	struct drm_i915_gem_object *obj;
++	unsigned flags;
++	int ret;
+ 
+-	if ((obj = ring->status_page.obj) == NULL) {
+-		unsigned flags;
+-		int ret;
+-
+-		obj = i915_gem_alloc_object(ring->dev, 4096);
+-		if (obj == NULL) {
+-			DRM_ERROR("Failed to allocate status page\n");
+-			return -ENOMEM;
+-		}
++	obj = i915_gem_alloc_object(engine->i915->dev, 4096);
++	if (obj == NULL) {
++		DRM_ERROR("Failed to allocate status page\n");
++		return -ENOMEM;
++	}
+ 
+-		ret = i915_gem_object_set_cache_level(obj, I915_CACHE_LLC);
+-		if (ret)
+-			goto err_unref;
++	ret = i915_gem_object_set_cache_level(obj, I915_CACHE_LLC);
++	if (ret)
++		goto err_unref;
+ 
+-		flags = 0;
+-		if (!HAS_LLC(ring->dev))
+-			/* On g33, we cannot place HWS above 256MiB, so
+-			 * restrict its pinning to the low mappable arena.
+-			 * Though this restriction is not documented for
+-			 * gen4, gen5, or byt, they also behave similarly
+-			 * and hang if the HWS is placed at the top of the
+-			 * GTT. To generalise, it appears that all !llc
+-			 * platforms have issues with us placing the HWS
+-			 * above the mappable region (even though we never
+-			 * actualy map it).
+-			 */
+-			flags |= PIN_MAPPABLE;
+-		ret = i915_gem_obj_ggtt_pin(obj, 4096, flags);
+-		if (ret) {
++	flags = 0;
++	if (!HAS_LLC(engine->i915))
++		/* On g33, we cannot place HWS above 256MiB, so
++		 * restrict its pinning to the low mappable arena.
++		 * Though this restriction is not documented for
++		 * gen4, gen5, or byt, they also behave similarly
++		 * and hang if the HWS is placed at the top of the
++		 * GTT. To generalise, it appears that all !llc
++		 * platforms have issues with us placing the HWS
++		 * above the mappable region (even though we never
++		 * actualy map it).
++		 */
++		flags |= PIN_MAPPABLE;
++	ret = i915_gem_object_ggtt_pin(obj, 4096, flags);
++	if (ret) {
+ err_unref:
+-			drm_gem_object_unreference(&obj->base);
+-			return ret;
+-		}
+-
+-		ring->status_page.obj = obj;
++		drm_gem_object_unreference(&obj->base);
++		return ret;
+ 	}
+ 
+-	ring->status_page.gfx_addr = i915_gem_obj_ggtt_offset(obj);
+-	ring->status_page.page_addr = kmap(sg_page(obj->pages->sgl));
+-	memset(ring->status_page.page_addr, 0, PAGE_SIZE);
++	engine->status_page.obj = obj;
+ 
+-	DRM_DEBUG_DRIVER("%s hws offset: 0x%08x\n",
+-			ring->name, ring->status_page.gfx_addr);
++	engine->status_page.gfx_addr = i915_gem_obj_ggtt_offset(obj);
++	engine->status_page.page_addr = kmap(sg_page(obj->pages->sgl));
++	memset(engine->status_page.page_addr, 0, PAGE_SIZE);
+ 
++	DRM_DEBUG_DRIVER("%s hws offset: 0x%08x\n",
++			engine->name, engine->status_page.gfx_addr);
+ 	return 0;
+ }
+ 
+-static int init_phys_status_page(struct intel_engine_cs *ring)
++static int setup_phys_status_page(struct intel_engine_cs *engine)
+ {
+-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
++	struct drm_i915_private *i915 = engine->i915;
+ 
+-	if (!dev_priv->status_page_dmah) {
+-		dev_priv->status_page_dmah =
+-			drm_pci_alloc(ring->dev, PAGE_SIZE, PAGE_SIZE);
+-		if (!dev_priv->status_page_dmah)
+-			return -ENOMEM;
+-	}
++	i915->status_page_dmah =
++		drm_pci_alloc(i915->dev, PAGE_SIZE, PAGE_SIZE);
++	if (!i915->status_page_dmah)
++		return -ENOMEM;
+ 
+-	ring->status_page.page_addr = dev_priv->status_page_dmah->vaddr;
+-	memset(ring->status_page.page_addr, 0, PAGE_SIZE);
++	engine->status_page.page_addr = i915->status_page_dmah->vaddr;
++	memset(engine->status_page.page_addr, 0, PAGE_SIZE);
+ 
+ 	return 0;
+ }
+ 
+-static void intel_destroy_ringbuffer_obj(struct intel_ringbuffer *ringbuf)
++void intel_ring_free(struct intel_ringbuffer *ring)
+ {
+-	if (!ringbuf->obj)
+-		return;
++	if (ring->obj) {
++		if (ring->iomap) {
++			i915_gem_object_ggtt_unpin(ring->obj);
++			iounmap(ring->virtual_start);
++		} else
++			vunmap(ring->virtual_start);
++		i915_gem_object_unpin_pages(ring->obj);
++		drm_gem_object_unreference(&ring->obj->base);
++	}
+ 
+-	iounmap(ringbuf->virtual_start);
+-	i915_gem_object_ggtt_unpin(ringbuf->obj);
+-	drm_gem_object_unreference(&ringbuf->obj->base);
+-	ringbuf->obj = NULL;
++	list_del(&ring->engine_link);
++	kfree(ring);
+ }
+ 
+-static int intel_alloc_ringbuffer_obj(struct drm_device *dev,
+-				      struct intel_ringbuffer *ringbuf)
++static void *obj_vmap(struct drm_i915_gem_object *obj)
+ {
+-	struct drm_i915_private *dev_priv = to_i915(dev);
+-	struct drm_i915_gem_object *obj;
+-	int ret;
++	struct page **pages;
++	struct sg_page_iter sg_iter;
++	void *vaddr;
++	int i;
+ 
+-	if (ringbuf->obj)
+-		return 0;
++	if (!HAS_LLC(obj->base.dev))
++		return NULL;
+ 
+-	obj = NULL;
+-	if (!HAS_LLC(dev))
+-		obj = i915_gem_object_create_stolen(dev, ringbuf->size);
+-	if (obj == NULL)
+-		obj = i915_gem_alloc_object(dev, ringbuf->size);
+-	if (obj == NULL)
+-		return -ENOMEM;
+-
+-	/* mark ring buffers as read-only from GPU side by default */
+-	obj->gt_ro = 1;
+-
+-	ret = i915_gem_obj_ggtt_pin(obj, PAGE_SIZE, PIN_MAPPABLE);
+-	if (ret)
+-		goto err_unref;
++	if (obj->base.filp == NULL)
++		return NULL;
+ 
+-	ret = i915_gem_object_set_to_gtt_domain(obj, true);
+-	if (ret)
+-		goto err_unpin;
++	pages = drm_malloc_ab(obj->base.size >> PAGE_SHIFT, sizeof(*pages));
++	if (pages == NULL)
++		return NULL;
+ 
+-	ringbuf->virtual_start =
+-		ioremap_wc(dev_priv->gtt.mappable_base + i915_gem_obj_ggtt_offset(obj),
+-				ringbuf->size);
+-	if (ringbuf->virtual_start == NULL) {
+-		ret = -EINVAL;
+-		goto err_unpin;
+-	}
++	i = 0;
++	for_each_sg_page(obj->pages->sgl, &sg_iter, obj->pages->nents, 0)
++		pages[i++] = sg_page_iter_page(&sg_iter);
+ 
+-	ringbuf->obj = obj;
+-	return 0;
++	vaddr = vmap(pages, i, 0, PAGE_KERNEL);
++	drm_free_large(pages);
+ 
+-err_unpin:
+-	i915_gem_object_ggtt_unpin(obj);
+-err_unref:
+-	drm_gem_object_unreference(&obj->base);
+-	return ret;
++	return vaddr;
+ }
+ 
+-static int intel_init_ring_buffer(struct drm_device *dev,
+-				  struct intel_engine_cs *ring)
++struct intel_ringbuffer *
++intel_engine_alloc_ring(struct intel_engine_cs *engine,
++			struct intel_context *ctx,
++			int size)
+ {
+-	struct intel_ringbuffer *ringbuf = ring->buffer;
++	struct drm_i915_private *i915 = engine->i915;
++	struct intel_ringbuffer *ring;
++	struct drm_i915_gem_object *obj;
+ 	int ret;
+ 
+-	if (ringbuf == NULL) {
+-		ringbuf = kzalloc(sizeof(*ringbuf), GFP_KERNEL);
+-		if (!ringbuf)
+-			return -ENOMEM;
+-		ring->buffer = ringbuf;
+-	}
++	DRM_DEBUG("creating ringbuffer for %s, size %d\n", engine->name, size);
+ 
+-	ring->dev = dev;
+-	INIT_LIST_HEAD(&ring->active_list);
+-	INIT_LIST_HEAD(&ring->request_list);
+-	ringbuf->size = 32 * PAGE_SIZE;
+-	memset(ring->semaphore.sync_seqno, 0, sizeof(ring->semaphore.sync_seqno));
++	if (WARN_ON(!is_power_of_2(size)))
++		return ERR_PTR(-EINVAL);
+ 
+-	init_waitqueue_head(&ring->irq_queue);
++	ring = kzalloc(sizeof(*ring), GFP_KERNEL);
++	if (ring == NULL)
++		return ERR_PTR(-ENOMEM);
+ 
+-	if (I915_NEED_GFX_HWS(dev)) {
+-		ret = init_status_page(ring);
+-		if (ret)
+-			goto error;
+-	} else {
+-		BUG_ON(ring->id != RCS);
+-		ret = init_phys_status_page(ring);
+-		if (ret)
+-			goto error;
+-	}
++	ring->engine = engine;
++	ring->ctx = ctx;
++
++	obj = NULL;
++	if (!HAS_LLC(i915))
++		obj = i915_gem_object_create_stolen(i915->dev, size);
++	if (obj == NULL)
++		obj = i915_gem_alloc_object(i915->dev, size);
++	if (obj == NULL)
++		return ERR_PTR(-ENOMEM);
++
++	/* mark ring buffers as read-only from GPU side by default */
++	obj->gt_ro = 1;
+ 
+-	ret = intel_alloc_ringbuffer_obj(dev, ringbuf);
++	ret = i915_gem_object_get_pages(obj);
+ 	if (ret) {
+-		DRM_ERROR("Failed to allocate ringbuffer %s: %d\n", ring->name, ret);
+-		goto error;
++		DRM_ERROR("failed to get ringbuffer pages\n");
++		goto err_unref;
++	}
++
++	i915_gem_object_pin_pages(obj);
++
++	ring->virtual_start = obj_vmap(obj);
++	if (ring->virtual_start == NULL) {
++		ret = i915_gem_object_ggtt_pin(obj, PAGE_SIZE, PIN_MAPPABLE);
++		if (ret) {
++			DRM_ERROR("failed pin ringbuffer into GGTT\n");
++			goto err_unref;
++		}
++
++		ret = i915_gem_object_set_to_gtt_domain(obj, true);
++		if (ret) {
++			DRM_ERROR("failed mark ringbuffer for GTT writes\n");
++			goto err_unpin;
++		}
++
++		ring->virtual_start =
++			ioremap_wc(i915->gtt.mappable_base + i915_gem_obj_ggtt_offset(obj),
++					size);
++		if (ring->virtual_start == NULL) {
++			DRM_ERROR("failed to map ringbuffer through GTT\n");
++			ret = -EINVAL;
++			goto err_unpin;
++		}
++		ring->iomap = true;
+ 	}
+ 
++	ring->obj = obj;
++	ring->size = size;
++
+ 	/* Workaround an erratum on the i830 which causes a hang if
+ 	 * the TAIL pointer points to within the last 2 cachelines
+ 	 * of the buffer.
+ 	 */
+-	ringbuf->effective_size = ringbuf->size;
+-	if (IS_I830(dev) || IS_845G(dev))
+-		ringbuf->effective_size -= 2 * CACHELINE_BYTES;
++	ring->effective_size = size;
++	if (IS_I830(i915) || IS_845G(i915))
++		ring->effective_size -= 2 * CACHELINE_BYTES;
++
++	ring->space = intel_ring_space(ring);
++	ring->retired_head = -1;
++
++	INIT_LIST_HEAD(&ring->requests);
++	INIT_LIST_HEAD(&ring->breadcrumbs);
++	list_add_tail(&ring->engine_link, &engine->rings);
+ 
+-	ret = i915_cmd_parser_init_ring(ring);
+-	if (ret)
+-		goto error;
+-
+-	ret = ring->init(ring);
+-	if (ret)
+-		goto error;
++	return ring;
+ 
+-	return 0;
+-
+-error:
+-	kfree(ringbuf);
+-	ring->buffer = NULL;
+-	return ret;
++err_unpin:
++	i915_gem_object_ggtt_unpin(obj);
++err_unref:
++	i915_gem_object_unpin_pages(obj);
++	drm_gem_object_unreference(&obj->base);
++	return ERR_PTR(ret);
+ }
+ 
+-void intel_cleanup_ring_buffer(struct intel_engine_cs *ring)
++static void
++nop_irq_barrier(struct intel_engine_cs *engine)
+ {
+-	struct drm_i915_private *dev_priv = to_i915(ring->dev);
+-	struct intel_ringbuffer *ringbuf = ring->buffer;
+-
+-	if (!intel_ring_initialized(ring))
+-		return;
+-
+-	intel_stop_ring_buffer(ring);
+-	WARN_ON(!IS_GEN2(ring->dev) && (I915_READ_MODE(ring) & MODE_IDLE) == 0);
+-
+-	intel_destroy_ringbuffer_obj(ringbuf);
+-	ring->preallocated_lazy_request = NULL;
+-	ring->outstanding_lazy_seqno = 0;
+-
+-	if (ring->cleanup)
+-		ring->cleanup(ring);
+-
+-	cleanup_status_page(ring);
++}
+ 
+-	i915_cmd_parser_fini_ring(ring);
++static size_t get_context_alignment(struct drm_i915_private *i915)
++{
++	/* This is a HW constraint. The value below is the largest known
++	 * requirement I've seen in a spec to date, and that was a
++	 * workaround for a non-shipping part. It should be safe to
++	 * decrease this, but it's more future proof as is.
++	 */
++	if (INTEL_INFO(i915)->gen == 6)
++		return 64 << 10;
+ 
+-	kfree(ringbuf);
+-	ring->buffer = NULL;
++	return 0;
+ }
+ 
+-static int intel_ring_wait_request(struct intel_engine_cs *ring, int n)
++static struct intel_ringbuffer *
++engine_get_ring(struct intel_engine_cs *engine)
+ {
+-	struct intel_ringbuffer *ringbuf = ring->buffer;
+-	struct drm_i915_gem_request *request;
+-	u32 seqno = 0;
++	struct drm_i915_private *dev_priv = engine->i915;
++	struct intel_ringbuffer *ring;
+ 	int ret;
+ 
+-	if (ringbuf->last_retired_head != -1) {
+-		ringbuf->head = ringbuf->last_retired_head;
+-		ringbuf->last_retired_head = -1;
++	ring = engine->default_context->ring[engine->id].ring;
++	if (ring)
++		return ring;
+ 
+-		ringbuf->space = ring_space(ringbuf);
+-		if (ringbuf->space >= n)
+-			return 0;
++	if (I915_NEED_GFX_HWS(dev_priv)) {
++		ret = setup_status_page(engine);
++	} else {
++		BUG_ON(engine->id != RCS);
++		ret = setup_phys_status_page(engine);
+ 	}
++	if (ret)
++		return ERR_PTR(ret);
+ 
+-	list_for_each_entry(request, &ring->request_list, list) {
+-		if (__ring_space(request->tail, ringbuf->tail, ringbuf->size) >= n) {
+-			seqno = request->seqno;
+-			break;
+-		}
++	ring = intel_engine_alloc_ring(engine, NULL, 32 * PAGE_SIZE);
++	if (IS_ERR(ring)) {
++		DRM_ERROR("Failed to allocate ringbuffer for %s: %ld\n", engine->name, PTR_ERR(ring));
++		return ring;
+ 	}
+ 
+-	if (seqno == 0)
+-		return -ENOSPC;
+-
+-	ret = i915_wait_seqno(ring, seqno);
+-	if (ret)
+-		return ret;
+-
+-	i915_gem_retire_requests_ring(ring);
+-	ringbuf->head = ringbuf->last_retired_head;
+-	ringbuf->last_retired_head = -1;
++	gen6_gt_force_wake_get(dev_priv, FORCEWAKE_ALL);
++	if (!engine_stop(engine)) {
++		/* G45 ring initialization often fails to reset head to zero */
++		DRM_DEBUG_KMS("%s head not reset to zero "
++			      "ctl %08x head %08x tail %08x start %08x\n",
++			      engine->name,
++			      I915_READ_CTL(engine),
++			      I915_READ_HEAD(engine),
++			      I915_READ_TAIL(engine),
++			      I915_READ_START(engine));
++		if (!engine_stop(engine)) {
++			DRM_ERROR("failed to set %s head to zero "
++				  "ctl %08x head %08x tail %08x start %08x\n",
++				  engine->name,
++				  I915_READ_CTL(engine),
++				  I915_READ_HEAD(engine),
++				  I915_READ_TAIL(engine),
++				  I915_READ_START(engine));
++			ret = -EIO;
++		}
++	}
++	gen6_gt_force_wake_put(dev_priv, FORCEWAKE_ALL);
++	if (ret) {
++		intel_ring_free(ring);
++		return ERR_PTR(ret);
++	}
+ 
+-	ringbuf->space = ring_space(ringbuf);
+-	return 0;
++	return engine->default_context->ring[engine->id].ring = ring;
+ }
+ 
+-static int ring_wait_for_space(struct intel_engine_cs *ring, int n)
++static struct intel_ringbuffer *
++engine_pin_context(struct intel_engine_cs *engine,
++		   struct intel_context *ctx)
+ {
+-	struct drm_device *dev = ring->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_ringbuffer *ringbuf = ring->buffer;
+-	unsigned long end;
++	struct intel_ringbuffer *ring;
+ 	int ret;
+ 
+-	ret = intel_ring_wait_request(ring, n);
+-	if (ret != -ENOSPC)
+-		return ret;
+-
+-	/* force the tail write in case we have been skipping them */
+-	__intel_ring_advance(ring);
+-
+-	/* With GEM the hangcheck timer should kick us out of the loop,
+-	 * leaving it early runs the risk of corrupting GEM state (due
+-	 * to running on almost untested codepaths). But on resume
+-	 * timers don't work yet, so prevent a complete hang in that
+-	 * case by choosing an insanely large timeout. */
+-	end = jiffies + 60 * HZ;
+-
+-	trace_i915_ring_wait_begin(ring);
+-	do {
+-		ringbuf->head = I915_READ_HEAD(ring);
+-		ringbuf->space = ring_space(ringbuf);
+-		if (ringbuf->space >= n) {
+-			ret = 0;
+-			break;
+-		}
+-
+-		if (!drm_core_check_feature(dev, DRIVER_MODESET) &&
+-		    dev->primary->master) {
+-			struct drm_i915_master_private *master_priv = dev->primary->master->driver_priv;
+-			if (master_priv->sarea_priv)
+-				master_priv->sarea_priv->perf_boxes |= I915_BOX_WAIT;
+-		}
+-
+-		msleep(1);
+-
+-		if (dev_priv->mm.interruptible && signal_pending(current)) {
+-			ret = -ERESTARTSYS;
+-			break;
+-		}
++	if (ctx->ring[engine->id].state) {
++		struct drm_i915_gem_object *obj = ctx->ring[engine->id].state;
+ 
+-		ret = i915_gem_check_wedge(&dev_priv->gpu_error,
+-					   dev_priv->mm.interruptible);
++		ret = i915_gem_object_ggtt_pin(obj,
++					       get_context_alignment(engine->i915),
++					       0);
+ 		if (ret)
+-			break;
++			goto err;
+ 
+-		if (time_after(jiffies, end)) {
+-			ret = -EBUSY;
+-			break;
+-		}
+-	} while (1);
+-	trace_i915_ring_wait_end(ring);
+-	return ret;
+-}
+-
+-static int intel_wrap_ring_buffer(struct intel_engine_cs *ring)
+-{
+-	uint32_t __iomem *virt;
+-	struct intel_ringbuffer *ringbuf = ring->buffer;
+-	int rem = ringbuf->size - ringbuf->tail;
++		/*
++		 * Clear this page out of any CPU caches for coherent swap-in/out. Note
++		 * that thanks to write = false in this call and us not setting any gpu
++		 * write domains when putting a context object onto the active list
++		 * (when switching away from it), this won't block.
++		 *
++		 * XXX: We need a real interface to do this instead of trickery.
++		 */
++		ret = i915_gem_object_set_to_gtt_domain(obj, false);
++		if (ret)
++			goto err_ctx;
++	}
+ 
+-	if (ringbuf->space < rem) {
+-		int ret = ring_wait_for_space(ring, rem);
++	if (ctx->ppgtt) {
++		ret = i915_gem_object_ggtt_pin(ctx->ppgtt->state,
++					       ctx->ppgtt->alignment,
++					       0);
+ 		if (ret)
+-			return ret;
++			goto err_ctx;
+ 	}
+ 
+-	virt = ringbuf->virtual_start + ringbuf->tail;
+-	rem /= 4;
+-	while (rem--)
+-		iowrite32(MI_NOOP, virt++);
++	ring = engine_get_ring(engine);
++	if (IS_ERR(ring)) {
++		ret = PTR_ERR(ring);
++		goto err_mm;
++	}
+ 
+-	ringbuf->tail = 0;
+-	ringbuf->space = ring_space(ringbuf);
++	return ring;
+ 
+-	return 0;
++err_mm:
++	if (ctx->ppgtt)
++		i915_gem_object_ggtt_unpin(ctx->ppgtt->state);
++err_ctx:
++	if (ctx->ring[engine->id].state)
++		i915_gem_object_ggtt_unpin(ctx->ring[engine->id].state);
++err:
++	return ERR_PTR(ret);
+ }
+ 
+-int intel_ring_idle(struct intel_engine_cs *ring)
++static struct drm_i915_gem_object *
++rq_add_ggtt(struct i915_gem_request *rq,
++	    struct drm_i915_gem_object *obj)
+ {
+-	u32 seqno;
+-	int ret;
+-
+-	/* We need to add any requests required to flush the objects and ring */
+-	if (ring->outstanding_lazy_seqno) {
+-		ret = i915_add_request(ring, NULL);
+-		if (ret)
+-			return ret;
+-	}
+-
+-	/* Wait upon the last request to be completed */
+-	if (list_empty(&ring->request_list))
+-		return 0;
+-
+-	seqno = list_entry(ring->request_list.prev,
+-			   struct drm_i915_gem_request,
+-			   list)->seqno;
++	obj->base.pending_read_domains = I915_GEM_DOMAIN_INSTRUCTION;
++	/* obj is kept alive until the next request by its active ref */
++	drm_gem_object_reference(&obj->base);
++	i915_request_add_vma(rq, i915_gem_obj_get_ggtt(obj), 0);
+ 
+-	return i915_wait_seqno(ring, seqno);
++	return obj;
+ }
+ 
+-static int
+-intel_ring_alloc_seqno(struct intel_engine_cs *ring)
++static void engine_add_context(struct i915_gem_request *rq,
++			       struct intel_context *ctx)
+ {
+-	if (ring->outstanding_lazy_seqno)
+-		return 0;
+-
+-	if (ring->preallocated_lazy_request == NULL) {
+-		struct drm_i915_gem_request *request;
++	if (ctx->ring[rq->engine->id].state)
++		/* As long as MI_SET_CONTEXT is serializing, ie. it flushes the
++		 * whole damn pipeline, we don't need to explicitly mark the
++		 * object dirty. The only exception is that the context must be
++		 * correct in case the object gets swapped out. Ideally we'd be
++		 * able to defer doing this until we know the object would be
++		 * swapped, but there is no way to do that yet.
++		 */
++		rq_add_ggtt(rq, ctx->ring[rq->engine->id].state)->dirty = 1;
+ 
+-		request = kmalloc(sizeof(*request), GFP_KERNEL);
+-		if (request == NULL)
+-			return -ENOMEM;
++	if (ctx->ppgtt)
++		rq_add_ggtt(rq, ctx->ppgtt->state);
++}
+ 
+-		ring->preallocated_lazy_request = request;
+-	}
++static void
++engine_unpin_context(struct intel_engine_cs *engine,
++		     struct intel_context *ctx)
++{
++	if (ctx->ppgtt)
++		i915_gem_object_ggtt_unpin(ctx->ppgtt->state);
++	if (ctx->ring[engine->id].state)
++		i915_gem_object_ggtt_unpin(ctx->ring[engine->id].state);
++}
+ 
+-	return i915_gem_get_seqno(ring->dev, &ring->outstanding_lazy_seqno);
++static void engine_free_context(struct intel_engine_cs *engine,
++				struct intel_context *ctx)
++{
++	if (ctx->ring[engine->id].state)
++		drm_gem_object_unreference(&ctx->ring[engine->id].state->base);
++	if (ctx == engine->default_context)
++		intel_ring_free(ctx->ring[engine->id].ring);
+ }
+ 
+-static int __intel_ring_prepare(struct intel_engine_cs *ring,
+-				int bytes)
++static int intel_engine_init(struct intel_engine_cs *engine,
++			     struct drm_i915_private *i915)
+ {
+-	struct intel_ringbuffer *ringbuf = ring->buffer;
+ 	int ret;
+ 
+-	if (unlikely(ringbuf->tail + bytes > ringbuf->effective_size)) {
+-		ret = intel_wrap_ring_buffer(ring);
+-		if (unlikely(ret))
+-			return ret;
+-	}
++	engine->i915 = i915;
+ 
+-	if (unlikely(ringbuf->space < bytes)) {
+-		ret = ring_wait_for_space(ring, bytes);
+-		if (unlikely(ret))
+-			return ret;
+-	}
++	INIT_LIST_HEAD(&engine->rings);
++	INIT_LIST_HEAD(&engine->vma_list);
++	INIT_LIST_HEAD(&engine->read_list);
++	INIT_LIST_HEAD(&engine->write_list);
++	INIT_LIST_HEAD(&engine->requests);
++	INIT_LIST_HEAD(&engine->pending);
++	INIT_LIST_HEAD(&engine->submitted);
+ 
+-	return 0;
+-}
++	spin_lock_init(&engine->lock);
++	spin_lock_init(&engine->irqlock);
+ 
+-int intel_ring_begin(struct intel_engine_cs *ring,
+-		     int num_dwords)
+-{
+-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
+-	int ret;
++	engine->suspend = engine_suspend;
++	engine->resume = engine_resume;
++	engine->cleanup = engine_cleanup;
++	engine->is_idle = engine_is_idle;
+ 
+-	ret = i915_gem_check_wedge(&dev_priv->gpu_error,
+-				   dev_priv->mm.interruptible);
+-	if (ret)
+-		return ret;
++	engine->irq_barrier = nop_irq_barrier;
++	engine->emit_breadcrumb = emit_breadcrumb;
+ 
+-	ret = __intel_ring_prepare(ring, num_dwords * sizeof(uint32_t));
+-	if (ret)
+-		return ret;
++	engine->power_domains = FORCEWAKE_ALL;
+ 
+-	/* Preallocate the olr before touching the ring */
+-	ret = intel_ring_alloc_seqno(ring);
+-	if (ret)
+-		return ret;
++	engine->semaphore.wait = NULL;
+ 
+-	ring->buffer->space -= num_dwords * sizeof(uint32_t);
+-	return 0;
+-}
++	engine->pin_context = engine_pin_context;
++	engine->add_context = engine_add_context;
++	engine->unpin_context = engine_unpin_context;
++	engine->free_context = engine_free_context;
+ 
+-/* Align the ring tail to a cacheline boundary */
+-int intel_ring_cacheline_align(struct intel_engine_cs *ring)
+-{
+-	int num_dwords = (ring->buffer->tail & (CACHELINE_BYTES - 1)) / sizeof(uint32_t);
+-	int ret;
++	engine->add_request = engine_add_request;
++	engine->write_tail = ring_write_tail;
++	engine->is_complete = engine_rq_is_complete;
+ 
+-	if (num_dwords == 0)
+-		return 0;
++	init_waitqueue_head(&engine->irq_queue);
+ 
+-	num_dwords = CACHELINE_BYTES / sizeof(uint32_t) - num_dwords;
+-	ret = intel_ring_begin(ring, num_dwords);
++	ret = i915_cmd_parser_init_engine(engine);
+ 	if (ret)
+ 		return ret;
+ 
+-	while (num_dwords--)
+-		intel_ring_emit(ring, MI_NOOP);
+-
+-	intel_ring_advance(ring);
+-
+ 	return 0;
+ }
+ 
+-void intel_ring_init_seqno(struct intel_engine_cs *ring, u32 seqno)
+-{
+-	struct drm_device *dev = ring->dev;
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-
+-	BUG_ON(ring->outstanding_lazy_seqno);
+-
+-	if (INTEL_INFO(dev)->gen == 6 || INTEL_INFO(dev)->gen == 7) {
+-		I915_WRITE(RING_SYNC_0(ring->mmio_base), 0);
+-		I915_WRITE(RING_SYNC_1(ring->mmio_base), 0);
+-		if (HAS_VEBOX(dev))
+-			I915_WRITE(RING_SYNC_2(ring->mmio_base), 0);
+-	}
+-
+-	ring->set_seqno(ring, seqno);
+-	ring->hangcheck.seqno = seqno;
+-}
+-
+-static void gen6_bsd_ring_write_tail(struct intel_engine_cs *ring,
++static void gen6_bsd_ring_write_tail(struct intel_engine_cs *engine,
+ 				     u32 value)
+ {
+-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
++	struct drm_i915_private *dev_priv = engine->i915;
+ 
+        /* Every tail move must follow the sequence below */
+ 
+@@ -1942,8 +1796,8 @@
+ 		DRM_ERROR("timed out waiting for the BSD ring to wake up\n");
+ 
+ 	/* Now that the ring is fully powered up, update the tail */
+-	I915_WRITE_TAIL(ring, value);
+-	POSTING_READ(RING_TAIL(ring->mmio_base));
++	I915_WRITE_TAIL(engine, value);
++	POSTING_READ(RING_TAIL(engine->mmio_base));
+ 
+ 	/* Let the ring send IDLE messages to the GT again,
+ 	 * and so let it sleep to conserve power when idle.
+@@ -1952,79 +1806,77 @@
+ 		   _MASKED_BIT_DISABLE(GEN6_BSD_SLEEP_MSG_DISABLE));
+ }
+ 
+-static int gen6_bsd_ring_flush(struct intel_engine_cs *ring,
+-			       u32 invalidate, u32 flush)
++static int gen6_bsd_emit_flush(struct i915_gem_request *rq,
++			       u32 flags)
+ {
++	struct intel_ringbuffer *ring;
+ 	uint32_t cmd;
+-	int ret;
+-
+-	ret = intel_ring_begin(ring, 4);
+-	if (ret)
+-		return ret;
+ 
+-	cmd = MI_FLUSH_DW;
+-	if (INTEL_INFO(ring->dev)->gen >= 8)
++	cmd = 3;
++	if (INTEL_INFO(rq->i915)->gen >= 8)
+ 		cmd += 1;
++
++	ring = intel_ring_begin(rq, cmd);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
++
+ 	/*
+ 	 * Bspec vol 1c.5 - video engine command streamer:
+ 	 * "If ENABLED, all TLBs will be invalidated once the flush
+ 	 * operation is complete. This bit is only valid when the
+ 	 * Post-Sync Operation field is a value of 1h or 3h."
+ 	 */
+-	if (invalidate & I915_GEM_GPU_DOMAINS)
+-		cmd |= MI_INVALIDATE_TLB | MI_INVALIDATE_BSD |
+-			MI_FLUSH_DW_STORE_INDEX | MI_FLUSH_DW_OP_STOREDW;
++	cmd = MI_FLUSH_DW | (cmd - 2);
++	if (flags & I915_INVALIDATE_CACHES)
++		cmd |= (MI_INVALIDATE_TLB |
++			MI_INVALIDATE_BSD |
++			MI_FLUSH_DW_STORE_INDEX |
++			MI_FLUSH_DW_OP_STOREDW);
+ 	intel_ring_emit(ring, cmd);
+ 	intel_ring_emit(ring, I915_GEM_HWS_SCRATCH_ADDR | MI_FLUSH_DW_USE_GTT);
+-	if (INTEL_INFO(ring->dev)->gen >= 8) {
++	if (INTEL_INFO(rq->i915)->gen >= 8)
+ 		intel_ring_emit(ring, 0); /* upper addr */
+-		intel_ring_emit(ring, 0); /* value */
+-	} else  {
+-		intel_ring_emit(ring, 0);
+-		intel_ring_emit(ring, MI_NOOP);
+-	}
++	intel_ring_emit(ring, 0); /* value */
+ 	intel_ring_advance(ring);
+ 	return 0;
+ }
+ 
+ static int
+-gen8_ring_dispatch_execbuffer(struct intel_engine_cs *ring,
+-			      u64 offset, u32 len,
+-			      unsigned flags)
+-{
+-	struct drm_i915_private *dev_priv = ring->dev->dev_private;
+-	bool ppgtt = dev_priv->mm.aliasing_ppgtt != NULL &&
+-		!(flags & I915_DISPATCH_SECURE);
+-	int ret;
+-
+-	ret = intel_ring_begin(ring, 4);
+-	if (ret)
+-		return ret;
++gen8_emit_batchbuffer(struct i915_gem_request *rq,
++		      u64 offset, u32 len,
++		      unsigned flags)
++{
++	bool ppgtt = USES_PPGTT(ring->dev) && !(flags & I915_DISPATCH_SECURE);
++	struct intel_ringbuffer *ring;
++
++	ring = intel_ring_begin(rq, 3);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+ 	/* FIXME(BDW): Address space and security selectors. */
+ 	intel_ring_emit(ring, MI_BATCH_BUFFER_START_GEN8 | (ppgtt<<8));
+ 	intel_ring_emit(ring, lower_32_bits(offset));
+ 	intel_ring_emit(ring, upper_32_bits(offset));
+-	intel_ring_emit(ring, MI_NOOP);
+ 	intel_ring_advance(ring);
+ 
+ 	return 0;
+ }
+ 
+ static int
+-hsw_ring_dispatch_execbuffer(struct intel_engine_cs *ring,
+-			      u64 offset, u32 len,
+-			      unsigned flags)
+-{
+-	int ret;
+-
+-	ret = intel_ring_begin(ring, 2);
+-	if (ret)
+-		return ret;
++hsw_emit_batchbuffer(struct i915_gem_request *rq,
++		     u64 offset, u32 len,
++		     unsigned flags)
++{
++	struct intel_ringbuffer *ring;
++
++	ring = intel_ring_begin(rq, 2);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+ 	intel_ring_emit(ring,
+-			MI_BATCH_BUFFER_START | MI_BATCH_PPGTT_HSW |
+-			(flags & I915_DISPATCH_SECURE ? 0 : MI_BATCH_NON_SECURE_HSW));
++			MI_BATCH_BUFFER_START |
++			(flags & I915_DISPATCH_SECURE ?
++			 0 : MI_BATCH_PPGTT_HSW | MI_BATCH_NON_SECURE_HSW));
+ 	/* bit0-7 is the length on GEN6+ */
+ 	intel_ring_emit(ring, offset);
+ 	intel_ring_advance(ring);
+@@ -2033,15 +1885,15 @@
+ }
+ 
+ static int
+-gen6_ring_dispatch_execbuffer(struct intel_engine_cs *ring,
+-			      u64 offset, u32 len,
+-			      unsigned flags)
+-{
+-	int ret;
+-
+-	ret = intel_ring_begin(ring, 2);
+-	if (ret)
+-		return ret;
++gen6_emit_batchbuffer(struct i915_gem_request *rq,
++		      u64 offset, u32 len,
++		      unsigned flags)
++{
++	struct intel_ringbuffer *ring;
++
++	ring = intel_ring_begin(rq, 2);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
+ 
+ 	intel_ring_emit(ring,
+ 			MI_BATCH_BUFFER_START |
+@@ -2055,100 +1907,141 @@
+ 
+ /* Blitter support (SandyBridge+) */
+ 
+-static int gen6_ring_flush(struct intel_engine_cs *ring,
+-			   u32 invalidate, u32 flush)
++static int gen6_blt_emit_flush(struct i915_gem_request *rq,
++			       u32 flags)
+ {
+-	struct drm_device *dev = ring->dev;
++	struct intel_ringbuffer *ring;
+ 	uint32_t cmd;
+-	int ret;
+-
+-	ret = intel_ring_begin(ring, 4);
+-	if (ret)
+-		return ret;
+ 
+-	cmd = MI_FLUSH_DW;
+-	if (INTEL_INFO(ring->dev)->gen >= 8)
++	cmd = 3;
++	if (INTEL_INFO(rq->i915)->gen >= 8)
+ 		cmd += 1;
++
++	ring = intel_ring_begin(rq, cmd);
++	if (IS_ERR(ring))
++		return PTR_ERR(ring);
++
+ 	/*
+ 	 * Bspec vol 1c.3 - blitter engine command streamer:
+ 	 * "If ENABLED, all TLBs will be invalidated once the flush
+ 	 * operation is complete. This bit is only valid when the
+ 	 * Post-Sync Operation field is a value of 1h or 3h."
+ 	 */
+-	if (invalidate & I915_GEM_DOMAIN_RENDER)
+-		cmd |= MI_INVALIDATE_TLB | MI_FLUSH_DW_STORE_INDEX |
+-			MI_FLUSH_DW_OP_STOREDW;
++	cmd = MI_FLUSH_DW | (cmd - 2);
++	if (flags & I915_INVALIDATE_CACHES)
++		cmd |= (MI_INVALIDATE_TLB |
++			MI_FLUSH_DW_STORE_INDEX |
++			MI_FLUSH_DW_OP_STOREDW);
+ 	intel_ring_emit(ring, cmd);
+ 	intel_ring_emit(ring, I915_GEM_HWS_SCRATCH_ADDR | MI_FLUSH_DW_USE_GTT);
+-	if (INTEL_INFO(ring->dev)->gen >= 8) {
++	if (INTEL_INFO(rq->i915)->gen >= 8)
+ 		intel_ring_emit(ring, 0); /* upper addr */
+-		intel_ring_emit(ring, 0); /* value */
+-	} else  {
+-		intel_ring_emit(ring, 0);
+-		intel_ring_emit(ring, MI_NOOP);
+-	}
++	intel_ring_emit(ring, 0); /* value */
+ 	intel_ring_advance(ring);
+ 
+-	if (IS_GEN7(dev) && !invalidate && flush)
+-		return gen7_ring_fbc_flush(ring, FBC_REND_CACHE_CLEAN);
++	if (flags & I915_KICK_FBC) {
++		if (IS_GEN7(rq->i915))
++			return gen7_ring_fbc_flush(rq, FBC_REND_CACHE_CLEAN);
++		if (IS_BROADWELL(rq->i915))
++			rq->i915->fbc.need_sw_cache_clean = true; /* XXX */
++	}
+ 
+ 	return 0;
+ }
+ 
+-int intel_init_render_ring_buffer(struct drm_device *dev)
++static void gen8_engine_init_semaphore(struct intel_engine_cs *engine)
++{
++	if (engine->i915->semaphore_obj == NULL)
++		return;
++
++	engine->semaphore.wait = gen8_emit_wait;
++	engine->semaphore.signal =
++		engine->id == RCS ? gen8_rcs_emit_signal : gen8_xcs_emit_signal;
++}
++
++static bool semaphores_enabled(struct drm_i915_private *dev_priv)
++{
++	if (INTEL_INFO(dev_priv)->gen < 6)
++		return false;
++
++	if (i915_module.semaphores >= 0)
++		return i915_module.semaphores;
++
++	/* Until we get further testing... */
++	if (IS_GEN8(dev_priv))
++		return false;
++
++#ifdef CONFIG_INTEL_IOMMU
++	/* Enable semaphores on SNB when IO remapping is off */
++	if (INTEL_INFO(dev_priv)->gen == 6 && intel_iommu_gfx_mapped)
++		return false;
++#endif
++
++	return true;
++}
++
++int intel_init_render_engine(struct drm_i915_private *dev_priv)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring = &dev_priv->ring[RCS];
++	struct intel_engine_cs *engine = &dev_priv->engine[RCS];
+ 	struct drm_i915_gem_object *obj;
+ 	int ret;
+ 
+-	ring->name = "render ring";
+-	ring->id = RCS;
+-	ring->mmio_base = RENDER_RING_BASE;
+-
+-	if (INTEL_INFO(dev)->gen >= 8) {
+-		if (i915_semaphore_is_enabled(dev)) {
+-			obj = i915_gem_alloc_object(dev, 4096);
++	ret = intel_engine_init(engine, dev_priv);
++	if (ret)
++		return ret;
++
++	engine->name = "render ring";
++	engine->id = RCS;
++	engine->mmio_base = RENDER_RING_BASE;
++	if (IS_VALLEYVIEW(dev_priv))
++		engine->power_domains = FORCEWAKE_RENDER;
++
++	engine->init_context = i915_gem_render_state_init;
++
++	if (HAS_L3_DPF(dev_priv)) {
++		if (INTEL_INFO(dev_priv)->gen >= 8)
++			engine->irq_keep_mask |= GT_RENDER_L3_PARITY_ERROR_INTERRUPT;
++		else
++			engine->irq_keep_mask |= GT_PARITY_ERROR(dev_priv);
++	}
++
++	if (INTEL_INFO(dev_priv)->gen >= 8) {
++		if (semaphores_enabled(dev_priv)) {
++			obj = i915_gem_alloc_object(dev_priv->dev, 4096);
+ 			if (obj == NULL) {
+ 				DRM_ERROR("Failed to allocate semaphore bo. Disabling semaphores\n");
+-				i915.semaphores = 0;
++				i915_module.semaphores = 0;
+ 			} else {
+ 				i915_gem_object_set_cache_level(obj, I915_CACHE_LLC);
+-				ret = i915_gem_obj_ggtt_pin(obj, 0, PIN_NONBLOCK);
++				ret = i915_gem_object_ggtt_pin(obj, 0, PIN_NONBLOCK);
+ 				if (ret != 0) {
+ 					drm_gem_object_unreference(&obj->base);
+ 					DRM_ERROR("Failed to pin semaphore bo. Disabling semaphores\n");
+-					i915.semaphores = 0;
++					i915_module.semaphores = 0;
+ 				} else
+ 					dev_priv->semaphore_obj = obj;
+ 			}
+ 		}
+-		ring->add_request = gen6_add_request;
+-		ring->flush = gen8_render_ring_flush;
+-		ring->irq_get = gen8_ring_get_irq;
+-		ring->irq_put = gen8_ring_put_irq;
+-		ring->irq_enable_mask = GT_RENDER_USER_INTERRUPT;
+-		ring->get_seqno = gen6_ring_get_seqno;
+-		ring->set_seqno = ring_set_seqno;
+-		if (i915_semaphore_is_enabled(dev)) {
+-			WARN_ON(!dev_priv->semaphore_obj);
+-			ring->semaphore.sync_to = gen8_ring_sync;
+-			ring->semaphore.signal = gen8_rcs_signal;
+-			GEN8_RING_SEMAPHORE_INIT;
+-		}
+-	} else if (INTEL_INFO(dev)->gen >= 6) {
+-		ring->add_request = gen6_add_request;
+-		ring->flush = gen7_render_ring_flush;
+-		if (INTEL_INFO(dev)->gen == 6)
+-			ring->flush = gen6_render_ring_flush;
+-		ring->irq_get = gen6_ring_get_irq;
+-		ring->irq_put = gen6_ring_put_irq;
+-		ring->irq_enable_mask = GT_RENDER_USER_INTERRUPT;
+-		ring->get_seqno = gen6_ring_get_seqno;
+-		ring->set_seqno = ring_set_seqno;
+-		if (i915_semaphore_is_enabled(dev)) {
+-			ring->semaphore.sync_to = gen6_ring_sync;
+-			ring->semaphore.signal = gen6_signal;
++		if (IS_CHERRYVIEW(dev_priv))
++			engine->init_context = chv_render_init_context;
++		else
++			engine->init_context = bdw_render_init_context;
++		engine->emit_flush = gen8_render_emit_flush;
++		engine->irq_get = gen8_irq_get;
++		engine->irq_put = gen8_irq_put;
++		engine->irq_enable_mask = GT_RENDER_USER_INTERRUPT;
++		gen8_engine_init_semaphore(engine);
++	} else if (INTEL_INFO(dev_priv)->gen >= 6) {
++		engine->emit_flush = gen7_render_emit_flush;
++		if (INTEL_INFO(dev_priv)->gen == 6)
++			engine->emit_flush = gen6_render_emit_flush;
++		engine->irq_get = gen6_irq_get;
++		engine->irq_barrier = gen6_irq_barrier;
++		engine->irq_put = gen6_irq_put;
++		engine->irq_enable_mask = GT_RENDER_USER_INTERRUPT;
++		if (semaphores_enabled(dev_priv)) {
++			engine->semaphore.wait = gen6_emit_wait;
++			engine->semaphore.signal = gen6_emit_signal;
+ 			/*
+ 			 * The current semaphore is only applied on pre-gen8
+ 			 * platform.  And there is no VCS2 ring on the pre-gen8
+@@ -2156,312 +2049,204 @@
+ 			 * initialized as INVALID.  Gen8 will initialize the
+ 			 * sema between VCS2 and RCS later.
+ 			 */
+-			ring->semaphore.mbox.wait[RCS] = MI_SEMAPHORE_SYNC_INVALID;
+-			ring->semaphore.mbox.wait[VCS] = MI_SEMAPHORE_SYNC_RV;
+-			ring->semaphore.mbox.wait[BCS] = MI_SEMAPHORE_SYNC_RB;
+-			ring->semaphore.mbox.wait[VECS] = MI_SEMAPHORE_SYNC_RVE;
+-			ring->semaphore.mbox.wait[VCS2] = MI_SEMAPHORE_SYNC_INVALID;
+-			ring->semaphore.mbox.signal[RCS] = GEN6_NOSYNC;
+-			ring->semaphore.mbox.signal[VCS] = GEN6_VRSYNC;
+-			ring->semaphore.mbox.signal[BCS] = GEN6_BRSYNC;
+-			ring->semaphore.mbox.signal[VECS] = GEN6_VERSYNC;
+-			ring->semaphore.mbox.signal[VCS2] = GEN6_NOSYNC;
+-		}
+-	} else if (IS_GEN5(dev)) {
+-		ring->add_request = pc_render_add_request;
+-		ring->flush = gen4_render_ring_flush;
+-		ring->get_seqno = pc_render_get_seqno;
+-		ring->set_seqno = pc_render_set_seqno;
+-		ring->irq_get = gen5_ring_get_irq;
+-		ring->irq_put = gen5_ring_put_irq;
+-		ring->irq_enable_mask = GT_RENDER_USER_INTERRUPT |
+-					GT_RENDER_PIPECTL_NOTIFY_INTERRUPT;
++			engine->semaphore.mbox.wait[RCS] = MI_SEMAPHORE_SYNC_INVALID;
++			engine->semaphore.mbox.wait[VCS] = MI_SEMAPHORE_SYNC_RV;
++			engine->semaphore.mbox.wait[BCS] = MI_SEMAPHORE_SYNC_RB;
++			engine->semaphore.mbox.wait[VECS] = MI_SEMAPHORE_SYNC_RVE;
++			engine->semaphore.mbox.wait[VCS2] = MI_SEMAPHORE_SYNC_INVALID;
++			engine->semaphore.mbox.signal[RCS] = GEN6_NOSYNC;
++			engine->semaphore.mbox.signal[VCS] = GEN6_VRSYNC;
++			engine->semaphore.mbox.signal[BCS] = GEN6_BRSYNC;
++			engine->semaphore.mbox.signal[VECS] = GEN6_VERSYNC;
++			engine->semaphore.mbox.signal[VCS2] = GEN6_NOSYNC;
++		}
++	} else if (IS_GEN5(dev_priv)) {
++		engine->emit_flush = gen4_emit_flush;
++		engine->irq_get = gen5_irq_get;
++		engine->irq_put = gen5_irq_put;
++		engine->irq_enable_mask =
++			GT_RENDER_USER_INTERRUPT |
++			GT_RENDER_PIPECTL_NOTIFY_INTERRUPT;
+ 	} else {
+-		ring->add_request = i9xx_add_request;
+-		if (INTEL_INFO(dev)->gen < 4)
+-			ring->flush = gen2_render_ring_flush;
++		if (INTEL_INFO(dev_priv)->gen < 4)
++			engine->emit_flush = gen2_emit_flush;
+ 		else
+-			ring->flush = gen4_render_ring_flush;
+-		ring->get_seqno = ring_get_seqno;
+-		ring->set_seqno = ring_set_seqno;
+-		if (IS_GEN2(dev)) {
+-			ring->irq_get = i8xx_ring_get_irq;
+-			ring->irq_put = i8xx_ring_put_irq;
++			engine->emit_flush = gen4_emit_flush;
++		if (IS_GEN2(dev_priv)) {
++			engine->irq_get = i8xx_irq_get;
++			engine->irq_put = i8xx_irq_put;
++			engine->is_idle = i8xx_is_idle;
+ 		} else {
+-			ring->irq_get = i9xx_ring_get_irq;
+-			ring->irq_put = i9xx_ring_put_irq;
++			engine->irq_get = i9xx_irq_get;
++			engine->irq_put = i9xx_irq_put;
+ 		}
+-		ring->irq_enable_mask = I915_USER_INTERRUPT;
++		engine->irq_enable_mask = I915_USER_INTERRUPT;
+ 	}
+-	ring->write_tail = ring_write_tail;
+ 
+-	if (IS_HASWELL(dev))
+-		ring->dispatch_execbuffer = hsw_ring_dispatch_execbuffer;
+-	else if (IS_GEN8(dev))
+-		ring->dispatch_execbuffer = gen8_ring_dispatch_execbuffer;
+-	else if (INTEL_INFO(dev)->gen >= 6)
+-		ring->dispatch_execbuffer = gen6_ring_dispatch_execbuffer;
+-	else if (INTEL_INFO(dev)->gen >= 4)
+-		ring->dispatch_execbuffer = i965_dispatch_execbuffer;
+-	else if (IS_I830(dev) || IS_845G(dev))
+-		ring->dispatch_execbuffer = i830_dispatch_execbuffer;
++	if (IS_GEN8(dev_priv))
++		engine->emit_batchbuffer = gen8_emit_batchbuffer;
++	else if (IS_HASWELL(dev_priv))
++		engine->emit_batchbuffer = hsw_emit_batchbuffer;
++	else if (INTEL_INFO(dev_priv)->gen >= 6)
++		engine->emit_batchbuffer = gen6_emit_batchbuffer;
++	else if (INTEL_INFO(dev_priv)->gen >= 4)
++		engine->emit_batchbuffer = i965_emit_batchbuffer;
++	else if (IS_I830(dev_priv) || IS_845G(dev_priv))
++		engine->emit_batchbuffer = i830_emit_batchbuffer;
+ 	else
+-		ring->dispatch_execbuffer = i915_dispatch_execbuffer;
+-	ring->init = init_render_ring;
+-	ring->cleanup = render_ring_cleanup;
+-
+-	/* Workaround batchbuffer to combat CS tlb bug. */
+-	if (HAS_BROKEN_CS_TLB(dev)) {
+-		obj = i915_gem_alloc_object(dev, I830_WA_SIZE);
+-		if (obj == NULL) {
+-			DRM_ERROR("Failed to allocate batch bo\n");
+-			return -ENOMEM;
+-		}
++		engine->emit_batchbuffer = i915_emit_batchbuffer;
+ 
+-		ret = i915_gem_obj_ggtt_pin(obj, 0, 0);
+-		if (ret != 0) {
+-			drm_gem_object_unreference(&obj->base);
+-			DRM_ERROR("Failed to ping batch bo\n");
+-			return ret;
+-		}
++	engine->resume = render_resume;
++	engine->cleanup = render_cleanup;
+ 
+-		ring->scratch.obj = obj;
+-		ring->scratch.gtt_offset = i915_gem_obj_ggtt_offset(obj);
+-	}
++	if (HAS_BROKEN_CS_TLB(dev_priv))
++		/* Workaround batchbuffer to combat CS tlb bug. */
++		ret = init_broken_cs_tlb_wa(engine);
++	else if (INTEL_INFO(dev_priv)->gen >= 6)
++		ret = init_pipe_control(engine);
++	if (ret)
++		return ret;
+ 
+-	return intel_init_ring_buffer(dev, ring);
++	return intel_engine_enable_execlists(engine);
+ }
+ 
+-int intel_render_ring_init_dri(struct drm_device *dev, u64 start, u32 size)
++int intel_init_bsd_engine(struct drm_i915_private *dev_priv)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring = &dev_priv->ring[RCS];
+-	struct intel_ringbuffer *ringbuf = ring->buffer;
++	struct intel_engine_cs *engine = &dev_priv->engine[VCS];
+ 	int ret;
+ 
+-	if (ringbuf == NULL) {
+-		ringbuf = kzalloc(sizeof(*ringbuf), GFP_KERNEL);
+-		if (!ringbuf)
+-			return -ENOMEM;
+-		ring->buffer = ringbuf;
+-	}
+-
+-	ring->name = "render ring";
+-	ring->id = RCS;
+-	ring->mmio_base = RENDER_RING_BASE;
+-
+-	if (INTEL_INFO(dev)->gen >= 6) {
+-		/* non-kms not supported on gen6+ */
+-		ret = -ENODEV;
+-		goto err_ringbuf;
+-	}
+-
+-	/* Note: gem is not supported on gen5/ilk without kms (the corresponding
+-	 * gem_init ioctl returns with -ENODEV). Hence we do not need to set up
+-	 * the special gen5 functions. */
+-	ring->add_request = i9xx_add_request;
+-	if (INTEL_INFO(dev)->gen < 4)
+-		ring->flush = gen2_render_ring_flush;
+-	else
+-		ring->flush = gen4_render_ring_flush;
+-	ring->get_seqno = ring_get_seqno;
+-	ring->set_seqno = ring_set_seqno;
+-	if (IS_GEN2(dev)) {
+-		ring->irq_get = i8xx_ring_get_irq;
+-		ring->irq_put = i8xx_ring_put_irq;
+-	} else {
+-		ring->irq_get = i9xx_ring_get_irq;
+-		ring->irq_put = i9xx_ring_put_irq;
+-	}
+-	ring->irq_enable_mask = I915_USER_INTERRUPT;
+-	ring->write_tail = ring_write_tail;
+-	if (INTEL_INFO(dev)->gen >= 4)
+-		ring->dispatch_execbuffer = i965_dispatch_execbuffer;
+-	else if (IS_I830(dev) || IS_845G(dev))
+-		ring->dispatch_execbuffer = i830_dispatch_execbuffer;
+-	else
+-		ring->dispatch_execbuffer = i915_dispatch_execbuffer;
+-	ring->init = init_render_ring;
+-	ring->cleanup = render_ring_cleanup;
+-
+-	ring->dev = dev;
+-	INIT_LIST_HEAD(&ring->active_list);
+-	INIT_LIST_HEAD(&ring->request_list);
+-
+-	ringbuf->size = size;
+-	ringbuf->effective_size = ringbuf->size;
+-	if (IS_I830(ring->dev) || IS_845G(ring->dev))
+-		ringbuf->effective_size -= 2 * CACHELINE_BYTES;
+-
+-	ringbuf->virtual_start = ioremap_wc(start, size);
+-	if (ringbuf->virtual_start == NULL) {
+-		DRM_ERROR("can not ioremap virtual address for"
+-			  " ring buffer\n");
+-		ret = -ENOMEM;
+-		goto err_ringbuf;
+-	}
+-
+-	if (!I915_NEED_GFX_HWS(dev)) {
+-		ret = init_phys_status_page(ring);
+-		if (ret)
+-			goto err_vstart;
+-	}
+-
+-	return 0;
+-
+-err_vstart:
+-	iounmap(ringbuf->virtual_start);
+-err_ringbuf:
+-	kfree(ringbuf);
+-	ring->buffer = NULL;
+-	return ret;
+-}
++	ret = intel_engine_init(engine, dev_priv);
++	if (ret)
++		return ret;
+ 
+-int intel_init_bsd_ring_buffer(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring = &dev_priv->ring[VCS];
++	engine->name = "bsd ring";
++	engine->id = VCS;
++	if (IS_VALLEYVIEW(dev_priv))
++		engine->power_domains = FORCEWAKE_MEDIA;
+ 
+-	ring->name = "bsd ring";
+-	ring->id = VCS;
++	if (INTEL_INFO(dev_priv)->gen >= 6) {
++		engine->mmio_base = GEN6_BSD_RING_BASE;
+ 
+-	ring->write_tail = ring_write_tail;
+-	if (INTEL_INFO(dev)->gen >= 6) {
+-		ring->mmio_base = GEN6_BSD_RING_BASE;
+ 		/* gen6 bsd needs a special wa for tail updates */
+-		if (IS_GEN6(dev))
+-			ring->write_tail = gen6_bsd_ring_write_tail;
+-		ring->flush = gen6_bsd_ring_flush;
+-		ring->add_request = gen6_add_request;
+-		ring->get_seqno = gen6_ring_get_seqno;
+-		ring->set_seqno = ring_set_seqno;
+-		if (INTEL_INFO(dev)->gen >= 8) {
+-			ring->irq_enable_mask =
++		if (IS_GEN6(dev_priv))
++			engine->write_tail = gen6_bsd_ring_write_tail;
++		engine->emit_flush = gen6_bsd_emit_flush;
++		if (INTEL_INFO(dev_priv)->gen >= 8) {
++			engine->irq_enable_mask =
+ 				GT_RENDER_USER_INTERRUPT << GEN8_VCS1_IRQ_SHIFT;
+-			ring->irq_get = gen8_ring_get_irq;
+-			ring->irq_put = gen8_ring_put_irq;
+-			ring->dispatch_execbuffer =
+-				gen8_ring_dispatch_execbuffer;
+-			if (i915_semaphore_is_enabled(dev)) {
+-				ring->semaphore.sync_to = gen8_ring_sync;
+-				ring->semaphore.signal = gen8_xcs_signal;
+-				GEN8_RING_SEMAPHORE_INIT;
+-			}
++			engine->irq_get = gen8_irq_get;
++			engine->irq_put = gen8_irq_put;
++			engine->emit_batchbuffer = gen8_emit_batchbuffer;
++			gen8_engine_init_semaphore(engine);
+ 		} else {
+-			ring->irq_enable_mask = GT_BSD_USER_INTERRUPT;
+-			ring->irq_get = gen6_ring_get_irq;
+-			ring->irq_put = gen6_ring_put_irq;
+-			ring->dispatch_execbuffer =
+-				gen6_ring_dispatch_execbuffer;
+-			if (i915_semaphore_is_enabled(dev)) {
+-				ring->semaphore.sync_to = gen6_ring_sync;
+-				ring->semaphore.signal = gen6_signal;
+-				ring->semaphore.mbox.wait[RCS] = MI_SEMAPHORE_SYNC_VR;
+-				ring->semaphore.mbox.wait[VCS] = MI_SEMAPHORE_SYNC_INVALID;
+-				ring->semaphore.mbox.wait[BCS] = MI_SEMAPHORE_SYNC_VB;
+-				ring->semaphore.mbox.wait[VECS] = MI_SEMAPHORE_SYNC_VVE;
+-				ring->semaphore.mbox.wait[VCS2] = MI_SEMAPHORE_SYNC_INVALID;
+-				ring->semaphore.mbox.signal[RCS] = GEN6_RVSYNC;
+-				ring->semaphore.mbox.signal[VCS] = GEN6_NOSYNC;
+-				ring->semaphore.mbox.signal[BCS] = GEN6_BVSYNC;
+-				ring->semaphore.mbox.signal[VECS] = GEN6_VEVSYNC;
+-				ring->semaphore.mbox.signal[VCS2] = GEN6_NOSYNC;
++			engine->irq_enable_mask = GT_BSD_USER_INTERRUPT;
++			engine->irq_get = gen6_irq_get;
++			engine->irq_barrier = gen6_irq_barrier;
++			engine->irq_put = gen6_irq_put;
++			engine->emit_batchbuffer = gen6_emit_batchbuffer;
++			if (semaphores_enabled(dev_priv)) {
++				engine->semaphore.wait = gen6_emit_wait;
++				engine->semaphore.signal = gen6_emit_signal;
++				engine->semaphore.mbox.wait[RCS] = MI_SEMAPHORE_SYNC_VR;
++				engine->semaphore.mbox.wait[VCS] = MI_SEMAPHORE_SYNC_INVALID;
++				engine->semaphore.mbox.wait[BCS] = MI_SEMAPHORE_SYNC_VB;
++				engine->semaphore.mbox.wait[VECS] = MI_SEMAPHORE_SYNC_VVE;
++				engine->semaphore.mbox.wait[VCS2] = MI_SEMAPHORE_SYNC_INVALID;
++				engine->semaphore.mbox.signal[RCS] = GEN6_RVSYNC;
++				engine->semaphore.mbox.signal[VCS] = GEN6_NOSYNC;
++				engine->semaphore.mbox.signal[BCS] = GEN6_BVSYNC;
++				engine->semaphore.mbox.signal[VECS] = GEN6_VEVSYNC;
++				engine->semaphore.mbox.signal[VCS2] = GEN6_NOSYNC;
+ 			}
+ 		}
+ 	} else {
+-		ring->mmio_base = BSD_RING_BASE;
+-		ring->flush = bsd_ring_flush;
+-		ring->add_request = i9xx_add_request;
+-		ring->get_seqno = ring_get_seqno;
+-		ring->set_seqno = ring_set_seqno;
+-		if (IS_GEN5(dev)) {
+-			ring->irq_enable_mask = ILK_BSD_USER_INTERRUPT;
+-			ring->irq_get = gen5_ring_get_irq;
+-			ring->irq_put = gen5_ring_put_irq;
++		engine->mmio_base = BSD_RING_BASE;
++
++		engine->emit_flush = bsd_emit_flush;
++		if (IS_GEN5(dev_priv)) {
++			engine->irq_enable_mask = ILK_BSD_USER_INTERRUPT;
++			engine->irq_get = gen5_irq_get;
++			engine->irq_put = gen5_irq_put;
+ 		} else {
+-			ring->irq_enable_mask = I915_BSD_USER_INTERRUPT;
+-			ring->irq_get = i9xx_ring_get_irq;
+-			ring->irq_put = i9xx_ring_put_irq;
++			engine->irq_enable_mask = I915_BSD_USER_INTERRUPT;
++			engine->irq_get = i9xx_irq_get;
++			engine->irq_put = i9xx_irq_put;
+ 		}
+-		ring->dispatch_execbuffer = i965_dispatch_execbuffer;
++		engine->emit_batchbuffer = i965_emit_batchbuffer;
+ 	}
+-	ring->init = init_ring_common;
+ 
+-	return intel_init_ring_buffer(dev, ring);
++	return intel_engine_enable_execlists(engine);
+ }
+ 
+ /**
+  * Initialize the second BSD ring for Broadwell GT3.
+  * It is noted that this only exists on Broadwell GT3.
+  */
+-int intel_init_bsd2_ring_buffer(struct drm_device *dev)
++int intel_init_bsd2_engine(struct drm_i915_private *dev_priv)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring = &dev_priv->ring[VCS2];
++	struct intel_engine_cs *engine = &dev_priv->engine[VCS2];
++	int ret;
+ 
+-	if ((INTEL_INFO(dev)->gen != 8)) {
++	if ((INTEL_INFO(dev_priv)->gen != 8)) {
+ 		DRM_ERROR("No dual-BSD ring on non-BDW machine\n");
+ 		return -EINVAL;
+ 	}
+ 
+-	ring->name = "bsd2 ring";
+-	ring->id = VCS2;
++	ret = intel_engine_init(engine, dev_priv);
++	if (ret)
++		return ret;
++
++	engine->name = "bsd2 ring";
++	engine->id = VCS2;
++	engine->mmio_base = GEN8_BSD2_RING_BASE;
++	if (IS_VALLEYVIEW(dev_priv))
++		engine->power_domains = FORCEWAKE_MEDIA;
+ 
+-	ring->write_tail = ring_write_tail;
+-	ring->mmio_base = GEN8_BSD2_RING_BASE;
+-	ring->flush = gen6_bsd_ring_flush;
+-	ring->add_request = gen6_add_request;
+-	ring->get_seqno = gen6_ring_get_seqno;
+-	ring->set_seqno = ring_set_seqno;
+-	ring->irq_enable_mask =
++	engine->emit_flush = gen6_bsd_emit_flush;
++	engine->emit_batchbuffer = gen8_emit_batchbuffer;
++	engine->irq_enable_mask =
+ 			GT_RENDER_USER_INTERRUPT << GEN8_VCS2_IRQ_SHIFT;
+-	ring->irq_get = gen8_ring_get_irq;
+-	ring->irq_put = gen8_ring_put_irq;
+-	ring->dispatch_execbuffer =
+-			gen8_ring_dispatch_execbuffer;
+-	if (i915_semaphore_is_enabled(dev)) {
+-		ring->semaphore.sync_to = gen8_ring_sync;
+-		ring->semaphore.signal = gen8_xcs_signal;
+-		GEN8_RING_SEMAPHORE_INIT;
+-	}
+-	ring->init = init_ring_common;
+-
+-	return intel_init_ring_buffer(dev, ring);
+-}
+-
+-int intel_init_blt_ring_buffer(struct drm_device *dev)
+-{
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring = &dev_priv->ring[BCS];
+-
+-	ring->name = "blitter ring";
+-	ring->id = BCS;
+-
+-	ring->mmio_base = BLT_RING_BASE;
+-	ring->write_tail = ring_write_tail;
+-	ring->flush = gen6_ring_flush;
+-	ring->add_request = gen6_add_request;
+-	ring->get_seqno = gen6_ring_get_seqno;
+-	ring->set_seqno = ring_set_seqno;
+-	if (INTEL_INFO(dev)->gen >= 8) {
+-		ring->irq_enable_mask =
++	engine->irq_get = gen8_irq_get;
++	engine->irq_put = gen8_irq_put;
++	gen8_engine_init_semaphore(engine);
++
++	return intel_engine_enable_execlists(engine);
++}
++
++int intel_init_blt_engine(struct drm_i915_private *dev_priv)
++{
++	struct intel_engine_cs *engine = &dev_priv->engine[BCS];
++	int ret;
++
++	ret = intel_engine_init(engine, dev_priv);
++	if (ret)
++		return ret;
++
++	engine->name = "blitter ring";
++	engine->id = BCS;
++	engine->mmio_base = BLT_RING_BASE;
++	if (IS_VALLEYVIEW(dev_priv))
++		engine->power_domains = FORCEWAKE_MEDIA;
++	else if (IS_GEN9(dev_priv))
++		engine->power_domains = FORCEWAKE_BLITTER;
++
++	engine->emit_flush = gen6_blt_emit_flush;
++	if (INTEL_INFO(dev_priv)->gen >= 8) {
++		engine->irq_enable_mask =
+ 			GT_RENDER_USER_INTERRUPT << GEN8_BCS_IRQ_SHIFT;
+-		ring->irq_get = gen8_ring_get_irq;
+-		ring->irq_put = gen8_ring_put_irq;
+-		ring->dispatch_execbuffer = gen8_ring_dispatch_execbuffer;
+-		if (i915_semaphore_is_enabled(dev)) {
+-			ring->semaphore.sync_to = gen8_ring_sync;
+-			ring->semaphore.signal = gen8_xcs_signal;
+-			GEN8_RING_SEMAPHORE_INIT;
+-		}
++		engine->irq_get = gen8_irq_get;
++		engine->irq_put = gen8_irq_put;
++		engine->emit_batchbuffer = gen8_emit_batchbuffer;
++		gen8_engine_init_semaphore(engine);
+ 	} else {
+-		ring->irq_enable_mask = GT_BLT_USER_INTERRUPT;
+-		ring->irq_get = gen6_ring_get_irq;
+-		ring->irq_put = gen6_ring_put_irq;
+-		ring->dispatch_execbuffer = gen6_ring_dispatch_execbuffer;
+-		if (i915_semaphore_is_enabled(dev)) {
+-			ring->semaphore.signal = gen6_signal;
+-			ring->semaphore.sync_to = gen6_ring_sync;
++		engine->irq_enable_mask = GT_BLT_USER_INTERRUPT;
++		engine->irq_get = gen6_irq_get;
++		engine->irq_barrier = gen6_irq_barrier;
++		engine->irq_put = gen6_irq_put;
++		engine->emit_batchbuffer = gen6_emit_batchbuffer;
++		if (semaphores_enabled(dev_priv)) {
++			engine->semaphore.signal = gen6_emit_signal;
++			engine->semaphore.wait = gen6_emit_wait;
+ 			/*
+ 			 * The current semaphore is only applied on pre-gen8
+ 			 * platform.  And there is no VCS2 ring on the pre-gen8
+@@ -2469,124 +2254,439 @@
+ 			 * initialized as INVALID.  Gen8 will initialize the
+ 			 * sema between BCS and VCS2 later.
+ 			 */
+-			ring->semaphore.mbox.wait[RCS] = MI_SEMAPHORE_SYNC_BR;
+-			ring->semaphore.mbox.wait[VCS] = MI_SEMAPHORE_SYNC_BV;
+-			ring->semaphore.mbox.wait[BCS] = MI_SEMAPHORE_SYNC_INVALID;
+-			ring->semaphore.mbox.wait[VECS] = MI_SEMAPHORE_SYNC_BVE;
+-			ring->semaphore.mbox.wait[VCS2] = MI_SEMAPHORE_SYNC_INVALID;
+-			ring->semaphore.mbox.signal[RCS] = GEN6_RBSYNC;
+-			ring->semaphore.mbox.signal[VCS] = GEN6_VBSYNC;
+-			ring->semaphore.mbox.signal[BCS] = GEN6_NOSYNC;
+-			ring->semaphore.mbox.signal[VECS] = GEN6_VEBSYNC;
+-			ring->semaphore.mbox.signal[VCS2] = GEN6_NOSYNC;
++			engine->semaphore.mbox.wait[RCS] = MI_SEMAPHORE_SYNC_BR;
++			engine->semaphore.mbox.wait[VCS] = MI_SEMAPHORE_SYNC_BV;
++			engine->semaphore.mbox.wait[BCS] = MI_SEMAPHORE_SYNC_INVALID;
++			engine->semaphore.mbox.wait[VECS] = MI_SEMAPHORE_SYNC_BVE;
++			engine->semaphore.mbox.wait[VCS2] = MI_SEMAPHORE_SYNC_INVALID;
++			engine->semaphore.mbox.signal[RCS] = GEN6_RBSYNC;
++			engine->semaphore.mbox.signal[VCS] = GEN6_VBSYNC;
++			engine->semaphore.mbox.signal[BCS] = GEN6_NOSYNC;
++			engine->semaphore.mbox.signal[VECS] = GEN6_VEBSYNC;
++			engine->semaphore.mbox.signal[VCS2] = GEN6_NOSYNC;
+ 		}
+ 	}
+-	ring->init = init_ring_common;
+ 
+-	return intel_init_ring_buffer(dev, ring);
++	return intel_engine_enable_execlists(engine);
+ }
+ 
+-int intel_init_vebox_ring_buffer(struct drm_device *dev)
++int intel_init_vebox_engine(struct drm_i915_private *dev_priv)
+ {
+-	struct drm_i915_private *dev_priv = dev->dev_private;
+-	struct intel_engine_cs *ring = &dev_priv->ring[VECS];
++	struct intel_engine_cs *engine = &dev_priv->engine[VECS];
++	int ret;
++
++	ret = intel_engine_init(engine, dev_priv);
++	if (ret)
++		return ret;
+ 
+-	ring->name = "video enhancement ring";
+-	ring->id = VECS;
++	engine->name = "video enhancement ring";
++	engine->id = VECS;
++	engine->mmio_base = VEBOX_RING_BASE;
++	if (IS_VALLEYVIEW(dev_priv))
++		engine->power_domains = FORCEWAKE_MEDIA;
+ 
+-	ring->mmio_base = VEBOX_RING_BASE;
+-	ring->write_tail = ring_write_tail;
+-	ring->flush = gen6_ring_flush;
+-	ring->add_request = gen6_add_request;
+-	ring->get_seqno = gen6_ring_get_seqno;
+-	ring->set_seqno = ring_set_seqno;
++	engine->emit_flush = gen6_blt_emit_flush;
+ 
+-	if (INTEL_INFO(dev)->gen >= 8) {
+-		ring->irq_enable_mask =
++	if (INTEL_INFO(dev_priv)->gen >= 8) {
++		engine->irq_enable_mask =
+ 			GT_RENDER_USER_INTERRUPT << GEN8_VECS_IRQ_SHIFT;
+-		ring->irq_get = gen8_ring_get_irq;
+-		ring->irq_put = gen8_ring_put_irq;
+-		ring->dispatch_execbuffer = gen8_ring_dispatch_execbuffer;
+-		if (i915_semaphore_is_enabled(dev)) {
+-			ring->semaphore.sync_to = gen8_ring_sync;
+-			ring->semaphore.signal = gen8_xcs_signal;
+-			GEN8_RING_SEMAPHORE_INIT;
+-		}
++		engine->irq_get = gen8_irq_get;
++		engine->irq_put = gen8_irq_put;
++		engine->emit_batchbuffer = gen8_emit_batchbuffer;
++		gen8_engine_init_semaphore(engine);
+ 	} else {
+-		ring->irq_enable_mask = PM_VEBOX_USER_INTERRUPT;
+-		ring->irq_get = hsw_vebox_get_irq;
+-		ring->irq_put = hsw_vebox_put_irq;
+-		ring->dispatch_execbuffer = gen6_ring_dispatch_execbuffer;
+-		if (i915_semaphore_is_enabled(dev)) {
+-			ring->semaphore.sync_to = gen6_ring_sync;
+-			ring->semaphore.signal = gen6_signal;
+-			ring->semaphore.mbox.wait[RCS] = MI_SEMAPHORE_SYNC_VER;
+-			ring->semaphore.mbox.wait[VCS] = MI_SEMAPHORE_SYNC_VEV;
+-			ring->semaphore.mbox.wait[BCS] = MI_SEMAPHORE_SYNC_VEB;
+-			ring->semaphore.mbox.wait[VECS] = MI_SEMAPHORE_SYNC_INVALID;
+-			ring->semaphore.mbox.wait[VCS2] = MI_SEMAPHORE_SYNC_INVALID;
+-			ring->semaphore.mbox.signal[RCS] = GEN6_RVESYNC;
+-			ring->semaphore.mbox.signal[VCS] = GEN6_VVESYNC;
+-			ring->semaphore.mbox.signal[BCS] = GEN6_BVESYNC;
+-			ring->semaphore.mbox.signal[VECS] = GEN6_NOSYNC;
+-			ring->semaphore.mbox.signal[VCS2] = GEN6_NOSYNC;
++		engine->irq_enable_mask = PM_VEBOX_USER_INTERRUPT;
++		engine->irq_get = hsw_vebox_irq_get;
++		engine->irq_barrier = gen6_irq_barrier;
++		engine->irq_put = hsw_vebox_irq_put;
++		engine->emit_batchbuffer = gen6_emit_batchbuffer;
++		if (semaphores_enabled(dev_priv)) {
++			engine->semaphore.wait = gen6_emit_wait;
++			engine->semaphore.signal = gen6_emit_signal;
++			engine->semaphore.mbox.wait[RCS] = MI_SEMAPHORE_SYNC_VER;
++			engine->semaphore.mbox.wait[VCS] = MI_SEMAPHORE_SYNC_VEV;
++			engine->semaphore.mbox.wait[BCS] = MI_SEMAPHORE_SYNC_VEB;
++			engine->semaphore.mbox.wait[VECS] = MI_SEMAPHORE_SYNC_INVALID;
++			engine->semaphore.mbox.wait[VCS2] = MI_SEMAPHORE_SYNC_INVALID;
++			engine->semaphore.mbox.signal[RCS] = GEN6_RVESYNC;
++			engine->semaphore.mbox.signal[VCS] = GEN6_VVESYNC;
++			engine->semaphore.mbox.signal[BCS] = GEN6_BVESYNC;
++			engine->semaphore.mbox.signal[VECS] = GEN6_NOSYNC;
++			engine->semaphore.mbox.signal[VCS2] = GEN6_NOSYNC;
+ 		}
+ 	}
+-	ring->init = init_ring_common;
+ 
+-	return intel_init_ring_buffer(dev, ring);
++	return intel_engine_enable_execlists(engine);
+ }
+ 
+ int
+-intel_ring_flush_all_caches(struct intel_engine_cs *ring)
++intel_engine_flush(struct intel_engine_cs *engine,
++		   struct intel_context *ctx)
+ {
++	struct i915_gem_request *rq;
+ 	int ret;
+ 
+-	if (!ring->gpu_caches_dirty)
++	rq = i915_request_create(ctx, engine);
++	if (IS_ERR(rq))
++		return PTR_ERR(rq);
++
++	ret = i915_request_emit_breadcrumb(rq);
++	if (ret == 0)
++		ret = i915_request_commit(rq);
++	i915_request_put(rq);
++
++	return ret;
++}
++
++int intel_engine_sync(struct intel_engine_cs *engine)
++{
++	/* Wait upon the last request to be completed */
++	if (engine->last_request == NULL)
+ 		return 0;
+ 
+-	ret = ring->flush(ring, 0, I915_GEM_GPU_DOMAINS);
+-	if (ret)
+-		return ret;
++	return i915_request_wait(engine->last_request);
++}
++
++struct i915_gem_request *
++intel_engine_seqno_to_request(struct intel_engine_cs *engine,
++			      u32 seqno)
++{
++	struct i915_gem_request *rq;
++
++	list_for_each_entry(rq, &engine->requests, engine_link) {
++		if (rq->seqno == seqno)
++			return rq;
++
++		if (__i915_seqno_passed(rq->seqno, seqno))
++			break;
++	}
++
++	return NULL;
++}
++
++void intel_engine_cleanup(struct intel_engine_cs *engine)
++{
++	WARN_ON(engine->last_request);
++
++	if (engine->cleanup)
++		engine->cleanup(engine);
++}
++
++static void intel_engine_clear_rings(struct intel_engine_cs *engine)
++{
++	struct intel_ringbuffer *ring;
++
++	list_for_each_entry(ring, &engine->rings, engine_link) {
++		if (ring->retired_head != -1) {
++			ring->head = ring->retired_head;
++			ring->retired_head = -1;
+ 
+-	trace_i915_gem_ring_flush(ring, 0, I915_GEM_GPU_DOMAINS);
++			ring->space = intel_ring_space(ring);
++		}
++
++		ring->pending_flush = 0;
++	}
++
++	if (engine->last_context) {
++		engine->unpin_context(engine, engine->last_context);
++		i915_gem_context_unreference(engine->last_context);
++		engine->last_context = NULL;
++	}
++}
++
++int intel_engine_suspend(struct intel_engine_cs *engine)
++{
++	struct drm_i915_private *dev_priv = engine->i915;
++	int ret = 0;
++
++	if (WARN_ON(!intel_engine_initialized(engine)))
++		return 0;
++
++	I915_WRITE_IMR(engine, ~0);
++
++	if (engine->suspend)
++		ret = engine->suspend(engine);
++
++	intel_engine_clear_rings(engine);
++
++	return ret;
++}
++
++int intel_engine_resume(struct intel_engine_cs *engine)
++{
++	struct drm_i915_private *dev_priv = engine->i915;
++	int ret = 0;
++
++	if (WARN_ON(!intel_engine_initialized(engine)))
++		return 0;
++
++	if (engine->resume)
++		ret = engine->resume(engine);
++
++	I915_WRITE_IMR(engine, ~engine->irq_keep_mask);
++	return ret;
++}
++
++int intel_engine_retire(struct intel_engine_cs *engine,
++			u32 seqno)
++{
++	int count;
++
++	if (engine->retire)
++		engine->retire(engine, seqno);
++
++	count = 0;
++	while (!list_empty(&engine->requests)) {
++		struct i915_gem_request *rq;
++
++		rq = list_first_entry(&engine->requests,
++				      struct i915_gem_request,
++				      engine_link);
++
++		if (!__i915_seqno_passed(seqno, rq->seqno))
++			break;
++
++		i915_request_retire(rq);
++		count++;
++	}
++
++	if (unlikely(engine->trace_irq_seqno &&
++		     __i915_seqno_passed(seqno, engine->trace_irq_seqno))) {
++		engine->irq_put(engine);
++		engine->trace_irq_seqno = 0;
++	}
++
++	return count;
++}
++
++static struct i915_gem_request *
++find_active_batch(struct list_head *list)
++{
++	struct i915_gem_request *rq, *last = NULL;
++
++	list_for_each_entry(rq, list, engine_link) {
++		if (rq->batch == NULL)
++			continue;
++
++		if (!__i915_request_complete__wa(rq))
++			return rq;
++
++		last = rq;
++	}
++
++	return last;
++}
++
++static bool context_is_banned(const struct intel_context *ctx,
++			      unsigned long now)
++{
++	const struct i915_ctx_hang_stats *hs = &ctx->hang_stats;
++
++	if (hs->banned)
++		return true;
++
++	if (hs->ban_period_seconds == 0)
++		return false;
++
++	if (now - hs->guilty_ts <= hs->ban_period_seconds) {
++		if (!i915_gem_context_is_default(ctx)) {
++			DRM_DEBUG("context hanging too fast, banning!\n");
++			return true;
++		} else if (i915_stop_ring_allow_ban(ctx->i915)) {
++			if (i915_stop_ring_allow_warn(ctx->i915))
++				DRM_ERROR("gpu hanging too fast, banning!\n");
++			return true;
++		}
++	}
++
++	return false;
++}
++
++static void
++intel_engine_hangstats(struct intel_engine_cs *engine)
++{
++	struct i915_ctx_hang_stats *hs;
++	struct i915_gem_request *rq;
++
++	rq = find_active_batch(&engine->requests);
++	if (rq == NULL)
++		return;
++
++	hs = &rq->ctx->hang_stats;
++	if (engine->hangcheck.score >= HANGCHECK_SCORE_RING_HUNG) {
++		unsigned long now = get_seconds();
++		hs->banned = context_is_banned(rq->ctx, now);
++		hs->guilty_ts = now;
++		hs->batch_active++;
++	} else
++		hs->batch_pending++;
++
++	list_for_each_entry_continue(rq, &engine->requests, engine_link) {
++		if (rq->batch == NULL)
++			continue;
++
++		if (__i915_request_complete__wa(rq))
++			continue;
++
++		rq->ctx->hang_stats.batch_pending++;
++	}
++}
++
++void intel_engine_reset(struct intel_engine_cs *engine)
++{
++	if (WARN_ON(!intel_engine_initialized(engine)))
++		return;
++
++	if (engine->reset)
++		engine->reset(engine);
++
++	memset(&engine->hangcheck, 0, sizeof(engine->hangcheck));
++	intel_engine_hangstats(engine);
++
++	intel_engine_retire(engine, engine->i915->next_seqno);
++	intel_engine_clear_rings(engine);
++}
++
++static int ring_wait(struct intel_ringbuffer *ring, int n)
++{
++	int ret;
++
++	trace_intel_ringbuffer_wait(ring, n);
++
++	do {
++		struct i915_gem_request *rq;
++
++		i915_gem_retire_requests__engine(ring->engine);
++		if (ring->retired_head != -1) {
++			ring->head = ring->retired_head;
++			ring->retired_head = -1;
++
++			ring->space = intel_ring_space(ring);
++			if (ring->space >= n)
++				return 0;
++		}
++
++		list_for_each_entry(rq, &ring->breadcrumbs, breadcrumb_link)
++			if (__intel_ring_space(rq->tail, ring->tail,
++					       ring->size, I915_RING_RSVD) >= n)
++				break;
++
++		if (WARN_ON(&rq->breadcrumb_link == &ring->breadcrumbs))
++			return -EDEADLK;
++
++		ret = i915_request_wait(rq);
++	} while (ret == 0);
++
++	return ret;
++}
++
++static int ring_wrap(struct intel_ringbuffer *ring, int bytes)
++{
++	uint32_t __iomem *virt;
++	int rem;
++
++	rem = ring->size - ring->tail;
++	if (unlikely(ring->space < rem)) {
++		rem = ring_wait(ring, rem);
++		if (rem)
++			return rem;
++	}
++
++	trace_intel_ringbuffer_wrap(ring, rem);
++
++	virt = ring->virtual_start + ring->tail;
++	rem = ring->size - ring->tail;
++
++	ring->space -= rem;
++	ring->tail = 0;
++
++	rem /= 4;
++	while (rem--)
++		iowrite32(MI_NOOP, virt++);
+ 
+-	ring->gpu_caches_dirty = false;
+ 	return 0;
+ }
+ 
+-int
+-intel_ring_invalidate_all_caches(struct intel_engine_cs *ring)
++static int __intel_ring_prepare(struct intel_ringbuffer *ring,
++				int bytes)
+ {
+-	uint32_t flush_domains;
+ 	int ret;
+ 
+-	flush_domains = 0;
+-	if (ring->gpu_caches_dirty)
+-		flush_domains = I915_GEM_GPU_DOMAINS;
++	trace_intel_ringbuffer_begin(ring, bytes);
+ 
+-	ret = ring->flush(ring, I915_GEM_GPU_DOMAINS, flush_domains);
+-	if (ret)
+-		return ret;
++	if (unlikely(ring->tail + bytes > ring->effective_size)) {
++		ret = ring_wrap(ring, bytes);
++		if (unlikely(ret))
++			return ret;
++	}
+ 
+-	trace_i915_gem_ring_flush(ring, I915_GEM_GPU_DOMAINS, flush_domains);
++	if (unlikely(ring->space < bytes)) {
++		ret = ring_wait(ring, bytes);
++		if (unlikely(ret))
++			return ret;
++	}
+ 
+-	ring->gpu_caches_dirty = false;
+ 	return 0;
+ }
+ 
+-void
+-intel_stop_ring_buffer(struct intel_engine_cs *ring)
++struct intel_ringbuffer *
++intel_ring_begin(struct i915_gem_request *rq,
++		 int num_dwords)
+ {
++	struct intel_ringbuffer *ring = rq->ring;
+ 	int ret;
+ 
+-	if (!intel_ring_initialized(ring))
+-		return;
++	/* TAIL updates must be aligned to a qword, so make sure we
++	 * reserve space for any implicit padding required for this
++	 * command.
++	 */
++	ret = __intel_ring_prepare(ring,
++				   ALIGN(num_dwords, 2) * sizeof(uint32_t));
++	if (ret)
++		return ERR_PTR(ret);
++
++	ring->space -= num_dwords * sizeof(uint32_t);
++
++	return ring;
++}
++
++/* Align the ring tail to a cacheline boundary */
++int intel_ring_cacheline_align(struct i915_gem_request *rq)
++{
++	struct intel_ringbuffer *ring;
++	int tail, num_dwords;
++
++	do {
++		tail = rq->ring->tail;
++		num_dwords = (tail & (CACHELINE_BYTES - 1)) / sizeof(uint32_t);
++		if (num_dwords == 0)
++			return 0;
++
++		num_dwords = CACHELINE_BYTES / sizeof(uint32_t) - num_dwords;
++		ring = intel_ring_begin(rq, num_dwords);
++		if (IS_ERR(ring))
++			return PTR_ERR(ring);
++	} while (tail != rq->ring->tail);
++
++	while (num_dwords--)
++		intel_ring_emit(ring, MI_NOOP);
++
++	intel_ring_advance(ring);
++
++	return 0;
++}
++
++struct i915_gem_request *
++intel_engine_find_active_batch(struct intel_engine_cs *engine)
++{
++	struct i915_gem_request *rq;
++	unsigned long flags;
+ 
+-	ret = intel_ring_idle(ring);
+-	if (ret && !i915_reset_in_progress(&to_i915(ring->dev)->gpu_error))
+-		DRM_ERROR("failed to quiesce %s whilst cleaning up: %d\n",
+-			  ring->name, ret);
++	spin_lock_irqsave(&engine->irqlock, flags);
++	rq = find_active_batch(&engine->submitted);
++	spin_unlock_irqrestore(&engine->irqlock, flags);
++	if (rq)
++		return rq;
+ 
+-	stop_ring(ring);
++	return find_active_batch(&engine->requests);
+ }
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_ringbuffer.h b/drivers/gpu/drm/i915/intel_ringbuffer.h
+--- a/drivers/gpu/drm/i915/intel_ringbuffer.h	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_ringbuffer.h	2014-11-20 09:53:37.996762837 -0700
+@@ -5,6 +5,13 @@
+ 
+ #define I915_CMD_HASH_ORDER 9
+ 
++/* Early gen2 devices have a cacheline of just 32 bytes, using 64 is overkill,
++ * but keeps the logic simple. Indeed, the whole purpose of this macro is just
++ * to give some inclination as to some of the magic values used in the various
++ * workarounds!
++ */
++#define CACHELINE_BYTES 64
++
+ /*
+  * Gen2 BSpec "1. Programming Environment" / 1.4.4.6 "Ring Buffer Use"
+  * Gen3 BSpec "vol1c Memory Interface Functions" / 2.3.4.5 "Ring Buffer Use"
+@@ -13,61 +20,48 @@
+  * "If the Ring Buffer Head Pointer and the Tail Pointer are on the same
+  * cacheline, the Head Pointer must not be greater than the Tail
+  * Pointer."
++ *
++ * To also accommodate errata on 830/845 which makes the last pair of
++ * cachelines in the ringbuffer unavailable, reduce the available space
++ * further.
+  */
+-#define I915_RING_FREE_SPACE 64
++#define I915_RING_RSVD (2*CACHELINE_BYTES)
+ 
+-struct  intel_hw_status_page {
++struct intel_hw_status_page {
+ 	u32		*page_addr;
+ 	unsigned int	gfx_addr;
+ 	struct		drm_i915_gem_object *obj;
+ };
+ 
+-#define I915_READ_TAIL(ring) I915_READ(RING_TAIL((ring)->mmio_base))
+-#define I915_WRITE_TAIL(ring, val) I915_WRITE(RING_TAIL((ring)->mmio_base), val)
++#define I915_READ_TAIL(engine) I915_READ(RING_TAIL((engine)->mmio_base))
++#define I915_WRITE_TAIL(engine, val) I915_WRITE(RING_TAIL((engine)->mmio_base), val)
+ 
+-#define I915_READ_START(ring) I915_READ(RING_START((ring)->mmio_base))
+-#define I915_WRITE_START(ring, val) I915_WRITE(RING_START((ring)->mmio_base), val)
++#define I915_READ_START(engine) I915_READ(RING_START((engine)->mmio_base))
++#define I915_WRITE_START(engine, val) I915_WRITE(RING_START((engine)->mmio_base), val)
+ 
+-#define I915_READ_HEAD(ring)  I915_READ(RING_HEAD((ring)->mmio_base))
+-#define I915_WRITE_HEAD(ring, val) I915_WRITE(RING_HEAD((ring)->mmio_base), val)
++#define I915_READ_HEAD(engine)  I915_READ(RING_HEAD((engine)->mmio_base))
++#define I915_WRITE_HEAD(engine, val) I915_WRITE(RING_HEAD((engine)->mmio_base), val)
+ 
+-#define I915_READ_CTL(ring) I915_READ(RING_CTL((ring)->mmio_base))
+-#define I915_WRITE_CTL(ring, val) I915_WRITE(RING_CTL((ring)->mmio_base), val)
++#define I915_READ_CTL(engine) I915_READ(RING_CTL((engine)->mmio_base))
++#define I915_WRITE_CTL(engine, val) I915_WRITE(RING_CTL((engine)->mmio_base), val)
+ 
+-#define I915_READ_IMR(ring) I915_READ(RING_IMR((ring)->mmio_base))
+-#define I915_WRITE_IMR(ring, val) I915_WRITE(RING_IMR((ring)->mmio_base), val)
++#define I915_READ_IMR(engine) I915_READ(RING_IMR((engine)->mmio_base))
++#define I915_WRITE_IMR(engine, val) I915_WRITE(RING_IMR((engine)->mmio_base), val)
+ 
+-#define I915_READ_MODE(ring) I915_READ(RING_MI_MODE((ring)->mmio_base))
+-#define I915_WRITE_MODE(ring, val) I915_WRITE(RING_MI_MODE((ring)->mmio_base), val)
++#define I915_READ_MODE(engine) I915_READ(RING_MI_MODE((engine)->mmio_base))
++#define I915_WRITE_MODE(engine, val) I915_WRITE(RING_MI_MODE((engine)->mmio_base), val)
+ 
+ /* seqno size is actually only a uint32, but since we plan to use MI_FLUSH_DW to
+  * do the writes, and that must have qw aligned offsets, simply pretend it's 8b.
+  */
+ #define i915_semaphore_seqno_size sizeof(uint64_t)
+-#define GEN8_SIGNAL_OFFSET(__ring, to)			     \
+-	(i915_gem_obj_ggtt_offset(dev_priv->semaphore_obj) + \
+-	((__ring)->id * I915_NUM_RINGS * i915_semaphore_seqno_size) +	\
+-	(i915_semaphore_seqno_size * (to)))
+-
+-#define GEN8_WAIT_OFFSET(__ring, from)			     \
+-	(i915_gem_obj_ggtt_offset(dev_priv->semaphore_obj) + \
+-	((from) * I915_NUM_RINGS * i915_semaphore_seqno_size) + \
+-	(i915_semaphore_seqno_size * (__ring)->id))
+-
+-#define GEN8_RING_SEMAPHORE_INIT do { \
+-	if (!dev_priv->semaphore_obj) { \
+-		break; \
+-	} \
+-	ring->semaphore.signal_ggtt[RCS] = GEN8_SIGNAL_OFFSET(ring, RCS); \
+-	ring->semaphore.signal_ggtt[VCS] = GEN8_SIGNAL_OFFSET(ring, VCS); \
+-	ring->semaphore.signal_ggtt[BCS] = GEN8_SIGNAL_OFFSET(ring, BCS); \
+-	ring->semaphore.signal_ggtt[VECS] = GEN8_SIGNAL_OFFSET(ring, VECS); \
+-	ring->semaphore.signal_ggtt[VCS2] = GEN8_SIGNAL_OFFSET(ring, VCS2); \
+-	ring->semaphore.signal_ggtt[ring->id] = MI_SEMAPHORE_SYNC_INVALID; \
+-	} while(0)
++#define GEN8_SEMAPHORE_OFFSET(__dp, __from, __to)			     \
++	(i915_gem_obj_ggtt_offset((__dp)->semaphore_obj) + \
++	 ((__from) * I915_NUM_ENGINES + (__to)) * i915_semaphore_seqno_size)
+ 
+-enum intel_ring_hangcheck_action {
++enum intel_engine_hangcheck_action {
+ 	HANGCHECK_IDLE = 0,
++	HANGCHECK_IDLE_WAITERS,
+ 	HANGCHECK_WAIT,
+ 	HANGCHECK_ACTIVE,
+ 	HANGCHECK_ACTIVE_LOOP,
+@@ -77,38 +71,61 @@
+ 
+ #define HANGCHECK_SCORE_RING_HUNG 31
+ 
+-struct intel_ring_hangcheck {
++struct intel_engine_hangcheck {
+ 	u64 acthd;
+ 	u64 max_acthd;
+ 	u32 seqno;
++	u32 interrupts;
+ 	int score;
+-	enum intel_ring_hangcheck_action action;
++	enum intel_engine_hangcheck_action action;
+ 	int deadlock;
+ };
+ 
++struct i915_gem_request;
++struct intel_context;
++struct intel_engine_cs;
++
+ struct intel_ringbuffer {
++	struct intel_engine_cs *engine;
++	struct intel_context *ctx;
++	struct list_head engine_link;
++
+ 	struct drm_i915_gem_object *obj;
+ 	void __iomem *virtual_start;
++	uint32_t ggtt_offset;
++
++	/**
++	 * List of breadcrumbs associated with GPU requests currently
++	 * outstanding.
++	 */
++	struct list_head requests;
++	struct list_head breadcrumbs;
+ 
+-	u32 head;
+-	u32 tail;
++	int head;
++	int tail;
+ 	int space;
++
+ 	int size;
+ 	int effective_size;
+ 
+ 	/** We track the position of the requests in the ring buffer, and
+-	 * when each is retired we increment last_retired_head as the GPU
++	 * when each is retired we increment retired_head as the GPU
+ 	 * must have finished processing the request and so we know we
+ 	 * can advance the ringbuffer up to that position.
+ 	 *
+-	 * last_retired_head is set to -1 after the value is consumed so
++	 * retired_head is set to -1 after the value is consumed so
+ 	 * we can detect new retirements.
+ 	 */
+-	u32 last_retired_head;
++	int retired_head;
++	int breadcrumb_tail;
++
++	unsigned pending_flush:4;
++	unsigned iomap:1;
+ };
+ 
+-struct  intel_engine_cs {
+-	const char	*name;
++struct intel_engine_cs {
++	struct drm_i915_private *i915;
++	const char *name;
+ 	enum intel_ring_id {
+ 		RCS = 0x0,
+ 		VCS,
+@@ -116,44 +133,77 @@
+ 		VECS,
+ 		VCS2
+ 	} id;
+-#define I915_NUM_RINGS 5
++#define I915_NUM_ENGINES 5
++#define I915_NUM_ENGINE_BITS 4
+ #define LAST_USER_RING (VECS + 1)
+-	u32		mmio_base;
+-	struct		drm_device *dev;
+-	struct intel_ringbuffer *buffer;
++	u32 mmio_base;
++	u32 power_domains;
++
++	/* protects requests against hangcheck */
++	spinlock_t lock;
++	/* protects exlists: pending + submitted */
++	spinlock_t irqlock;
++
++	atomic_t interrupts;
++	u32 breadcrumb[I915_NUM_ENGINES];
++	u16 tag, next_tag;
++
++	struct list_head rings;
++	struct list_head requests;
++	struct list_head pending, submitted;
++	struct i915_gem_request *last_request;
++	struct intel_context *last_context;
+ 
+ 	struct intel_hw_status_page status_page;
+ 
+-	unsigned irq_refcount; /* protected by dev_priv->irq_lock */
+-	u32		irq_enable_mask;	/* bitmask to enable ring interrupt */
++	unsigned irq_refcount; /* protected by i915->irq_lock */
++	u32		irq_enable_mask; /* bitmask to enable ring interrupt */
++	u32             irq_keep_mask; /* never mask these interrupts */
+ 	u32		trace_irq_seqno;
+-	bool __must_check (*irq_get)(struct intel_engine_cs *ring);
+-	void		(*irq_put)(struct intel_engine_cs *ring);
++	void		(*irq_get)(struct intel_engine_cs *engine);
++	void		(*irq_barrier)(struct intel_engine_cs *engine);
++	void		(*irq_put)(struct intel_engine_cs *engine);
++
++	void		(*retire)(struct intel_engine_cs *engine,
++				  u32 seqno);
++	void		(*reset)(struct intel_engine_cs *engine);
++	int		(*suspend)(struct intel_engine_cs *engine);
++	int		(*resume)(struct intel_engine_cs *engine);
++	void		(*cleanup)(struct intel_engine_cs *engine);
++
++	int		(*init_context)(struct i915_gem_request *rq);
++
++	int __must_check (*emit_flush)(struct i915_gem_request *rq,
++				       u32 domains);
++#define I915_COMMAND_BARRIER 0x1
++#define I915_FLUSH_CACHES 0x2
++#define I915_INVALIDATE_CACHES 0x4
++#define I915_KICK_FBC 0x8
++	int __must_check (*emit_batchbuffer)(struct i915_gem_request *rq,
++					     u64 offset, u32 length,
++					     unsigned flags);
++#define I915_DISPATCH_SECURE 0x1
++#define I915_DISPATCH_PINNED 0x2
++	int __must_check (*emit_breadcrumb)(struct i915_gem_request *rq);
+ 
+-	int		(*init)(struct intel_engine_cs *ring);
++	struct intel_ringbuffer *  __must_check
++		(*pin_context)(struct intel_engine_cs *engine,
++			       struct intel_context *ctx);
++	void (*add_context)(struct i915_gem_request *rq,
++			    struct intel_context *ctx);
++	void (*unpin_context)(struct intel_engine_cs *engine,
++			      struct intel_context *ctx);
++	void (*free_context)(struct intel_engine_cs *engine,
++			     struct intel_context *ctx);
+ 
+-	void		(*write_tail)(struct intel_engine_cs *ring,
++
++	int __must_check (*add_request)(struct i915_gem_request *rq);
++	void		(*write_tail)(struct intel_engine_cs *engine,
+ 				      u32 value);
+-	int __must_check (*flush)(struct intel_engine_cs *ring,
+-				  u32	invalidate_domains,
+-				  u32	flush_domains);
+-	int		(*add_request)(struct intel_engine_cs *ring);
+-	/* Some chipsets are not quite as coherent as advertised and need
+-	 * an expensive kick to force a true read of the up-to-date seqno.
+-	 * However, the up-to-date seqno is not always required and the last
+-	 * seen value is good enough. Note that the seqno will always be
+-	 * monotonic, even if not coherent.
+-	 */
+-	u32		(*get_seqno)(struct intel_engine_cs *ring,
+-				     bool lazy_coherency);
+-	void		(*set_seqno)(struct intel_engine_cs *ring,
+-				     u32 seqno);
+-	int		(*dispatch_execbuffer)(struct intel_engine_cs *ring,
+-					       u64 offset, u32 length,
+-					       unsigned flags);
+-#define I915_DISPATCH_SECURE 0x1
+-#define I915_DISPATCH_PINNED 0x2
+-	void		(*cleanup)(struct intel_engine_cs *ring);
++
++	bool (*is_complete)(struct i915_gem_request *rq);
++	bool (*is_idle)(struct intel_engine_cs *engine);
++
+ 
+ 	/* GEN8 signal/wait table - never trust comments!
+ 	 *	  signal to	signal to    signal to   signal to      signal to
+@@ -193,27 +243,25 @@
+ 	 *  ie. transpose of f(x, y)
+ 	 */
+ 	struct {
+-		u32	sync_seqno[I915_NUM_RINGS-1];
++		struct {
++			/* our mbox written by others */
++			u32		wait[I915_NUM_ENGINES];
++			/* mboxes this ring signals to */
++			u32		signal[I915_NUM_ENGINES];
++		} mbox;
++
++		int	(*wait)(struct i915_gem_request *waiter,
++				struct i915_gem_request *signaller);
++		int	(*signal)(struct i915_gem_request *rq, int id);
+ 
+-		union {
+-			struct {
+-				/* our mbox written by others */
+-				u32		wait[I915_NUM_RINGS];
+-				/* mboxes this ring signals to */
+-				u32		signal[I915_NUM_RINGS];
+-			} mbox;
+-			u64		signal_ggtt[I915_NUM_RINGS];
+-		};
+-
+-		/* AKA wait() */
+-		int	(*sync_to)(struct intel_engine_cs *ring,
+-				   struct intel_engine_cs *to,
+-				   u32 seqno);
+-		int	(*signal)(struct intel_engine_cs *signaller,
+-				  /* num_dwords needed by caller */
+-				  unsigned int num_dwords);
++		u32 sync[I915_NUM_ENGINES];
+ 	} semaphore;
+ 
++	/* Execlists */
++	bool execlists_enabled;
++	u32 execlists_submitted;
++	u8 next_context_status_buffer;
++
+ 	/**
+ 	 * List of objects currently involved in rendering from the
+ 	 * ringbuffer.
+@@ -224,33 +272,19 @@
+ 	 *
+ 	 * A reference is held on the buffer while on this list.
+ 	 */
+-	struct list_head active_list;
++	struct list_head vma_list, read_list, write_list, fence_list;
+ 
+-	/**
+-	 * List of breadcrumbs associated with GPU requests currently
+-	 * outstanding.
+-	 */
+-	struct list_head request_list;
+-
+-	/**
+-	 * Do we have some not yet emitted requests outstanding?
+-	 */
+-	struct drm_i915_gem_request *preallocated_lazy_request;
+-	u32 outstanding_lazy_seqno;
+-	bool gpu_caches_dirty;
+-	bool fbc_dirty;
++	u64 pmu_sample[3];
+ 
+ 	wait_queue_head_t irq_queue;
+ 
+ 	struct intel_context *default_context;
+-	struct intel_context *last_context;
+ 
+-	struct intel_ring_hangcheck hangcheck;
++	struct intel_engine_hangcheck hangcheck;
+ 
+ 	struct {
+ 		struct drm_i915_gem_object *obj;
+ 		u32 gtt_offset;
+-		volatile u32 *cpu_page;
+ 	} scratch;
+ 
+ 	bool needs_cmd_parser;
+@@ -288,52 +322,31 @@
+ };
+ 
+ static inline bool
+-intel_ring_initialized(struct intel_engine_cs *ring)
++intel_engine_initialized(struct intel_engine_cs *engine)
+ {
+-	return ring->buffer && ring->buffer->obj;
++	return engine->default_context;
+ }
+ 
+ static inline unsigned
+-intel_ring_flag(struct intel_engine_cs *ring)
+-{
+-	return 1 << ring->id;
+-}
+-
+-static inline u32
+-intel_ring_sync_index(struct intel_engine_cs *ring,
+-		      struct intel_engine_cs *other)
++intel_engine_flag(struct intel_engine_cs *engine)
+ {
+-	int idx;
+-
+-	/*
+-	 * rcs -> 0 = vcs, 1 = bcs, 2 = vecs, 3 = vcs2;
+-	 * vcs -> 0 = bcs, 1 = vecs, 2 = vcs2, 3 = rcs;
+-	 * bcs -> 0 = vecs, 1 = vcs2. 2 = rcs, 3 = vcs;
+-	 * vecs -> 0 = vcs2, 1 = rcs, 2 = vcs, 3 = bcs;
+-	 * vcs2 -> 0 = rcs, 1 = vcs, 2 = bcs, 3 = vecs;
+-	 */
+-
+-	idx = (other - ring) - 1;
+-	if (idx < 0)
+-		idx += I915_NUM_RINGS;
+-
+-	return idx;
++	return 1 << engine->id;
+ }
+ 
+ static inline u32
+-intel_read_status_page(struct intel_engine_cs *ring,
++intel_read_status_page(struct intel_engine_cs *engine,
+ 		       int reg)
+ {
+ 	/* Ensure that the compiler doesn't optimize away the load. */
+ 	barrier();
+-	return ring->status_page.page_addr[reg];
++	return engine->status_page.page_addr[reg];
+ }
+ 
+ static inline void
+-intel_write_status_page(struct intel_engine_cs *ring,
++intel_write_status_page(struct intel_engine_cs *engine,
+ 			int reg, u32 value)
+ {
+-	ring->status_page.page_addr[reg] = value;
++	engine->status_page.page_addr[reg] = value;
+ }
+ 
+ /**
+@@ -355,57 +368,79 @@
+ #define I915_GEM_HWS_SCRATCH_INDEX	0x30
+ #define I915_GEM_HWS_SCRATCH_ADDR (I915_GEM_HWS_SCRATCH_INDEX << MI_STORE_DWORD_INDEX_SHIFT)
+ 
+-void intel_stop_ring_buffer(struct intel_engine_cs *ring);
+-void intel_cleanup_ring_buffer(struct intel_engine_cs *ring);
++static inline u32
++intel_engine_get_seqno(struct intel_engine_cs *engine)
++{
++	return intel_read_status_page(engine, I915_GEM_HWS_INDEX);
++}
+ 
+-int __must_check intel_ring_begin(struct intel_engine_cs *ring, int n);
+-int __must_check intel_ring_cacheline_align(struct intel_engine_cs *ring);
+-static inline void intel_ring_emit(struct intel_engine_cs *ring,
++struct intel_ringbuffer *
++intel_engine_alloc_ring(struct intel_engine_cs *engine,
++			struct intel_context *ctx,
++			int size);
++void intel_ring_free(struct intel_ringbuffer *ring);
++
++struct intel_ringbuffer *__must_check
++intel_ring_begin(struct i915_gem_request *rq, int n);
++int __must_check intel_ring_cacheline_align(struct i915_gem_request *rq);
++static inline void intel_ring_emit(struct intel_ringbuffer *ring,
+ 				   u32 data)
+ {
+-	struct intel_ringbuffer *ringbuf = ring->buffer;
+-	iowrite32(data, ringbuf->virtual_start + ringbuf->tail);
+-	ringbuf->tail += 4;
++	iowrite32(data, ring->virtual_start + ring->tail);
++	ring->tail += 4;
+ }
+-static inline void intel_ring_advance(struct intel_engine_cs *ring)
++static inline void intel_ring_advance(struct intel_ringbuffer *ring)
+ {
+-	struct intel_ringbuffer *ringbuf = ring->buffer;
+-	ringbuf->tail &= ringbuf->size - 1;
++	ring->tail &= ring->size - 1;
+ }
+-void __intel_ring_advance(struct intel_engine_cs *ring);
+-
+-int __must_check intel_ring_idle(struct intel_engine_cs *ring);
+-void intel_ring_init_seqno(struct intel_engine_cs *ring, u32 seqno);
+-int intel_ring_flush_all_caches(struct intel_engine_cs *ring);
+-int intel_ring_invalidate_all_caches(struct intel_engine_cs *ring);
+-
+-int intel_init_render_ring_buffer(struct drm_device *dev);
+-int intel_init_bsd_ring_buffer(struct drm_device *dev);
+-int intel_init_bsd2_ring_buffer(struct drm_device *dev);
+-int intel_init_blt_ring_buffer(struct drm_device *dev);
+-int intel_init_vebox_ring_buffer(struct drm_device *dev);
+ 
+-u64 intel_ring_get_active_head(struct intel_engine_cs *ring);
+-void intel_ring_setup_status_page(struct intel_engine_cs *ring);
+-
+-static inline u32 intel_ring_get_tail(struct intel_ringbuffer *ringbuf)
++static inline int __intel_ring_space(int head, int tail, int size, int rsvd)
+ {
+-	return ringbuf->tail;
++	int space = head - (tail + 8);
++	if (space < 0)
++		space += size;
++	return space - rsvd;
+ }
+ 
+-static inline u32 intel_ring_get_seqno(struct intel_engine_cs *ring)
++static inline int intel_ring_space(struct intel_ringbuffer *ring)
+ {
+-	BUG_ON(ring->outstanding_lazy_seqno == 0);
+-	return ring->outstanding_lazy_seqno;
++	return __intel_ring_space(ring->head, ring->tail,
++				  ring->size, I915_RING_RSVD);
+ }
+ 
+-static inline void i915_trace_irq_get(struct intel_engine_cs *ring, u32 seqno)
++
++struct i915_gem_request *
++intel_engine_find_active_batch(struct intel_engine_cs *engine);
++
++struct i915_gem_request *
++intel_engine_seqno_to_request(struct intel_engine_cs *engine,
++			      u32 seqno);
++
++int intel_init_render_engine(struct drm_i915_private *i915);
++int intel_init_bsd_engine(struct drm_i915_private *i915);
++int intel_init_bsd2_engine(struct drm_i915_private *i915);
++int intel_init_blt_engine(struct drm_i915_private *i915);
++int intel_init_vebox_engine(struct drm_i915_private *i915);
++
++int __must_check intel_engine_sync(struct intel_engine_cs *engine);
++int __must_check intel_engine_flush(struct intel_engine_cs *engine,
++				    struct intel_context *ctx);
++
++int intel_engine_retire(struct intel_engine_cs *engine, u32 seqno);
++void intel_engine_reset(struct intel_engine_cs *engine);
++int intel_engine_suspend(struct intel_engine_cs *engine);
++int intel_engine_resume(struct intel_engine_cs *engine);
++void intel_engine_cleanup(struct intel_engine_cs *engine);
++
++
++u64 intel_engine_get_active_head(struct intel_engine_cs *engine);
++
++static inline void i915_trace_irq_get(struct intel_engine_cs *engine, u32 seqno)
+ {
+-	if (ring->trace_irq_seqno == 0 && ring->irq_get(ring))
+-		ring->trace_irq_seqno = seqno;
+-}
++	if (engine->trace_irq_seqno == 0)
++		engine->irq_get(engine);
+ 
+-/* DRI warts */
+-int intel_render_ring_init_dri(struct drm_device *dev, u64 start, u32 size);
++	engine->trace_irq_seqno = seqno;
++}
+ 
+ #endif /* _INTEL_RINGBUFFER_H_ */
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_runtime_pm.c b/drivers/gpu/drm/i915/intel_runtime_pm.c
+--- a/drivers/gpu/drm/i915/intel_runtime_pm.c	1969-12-31 17:00:00.000000000 -0700
++++ b/drivers/gpu/drm/i915/intel_runtime_pm.c	2014-11-20 09:53:37.996762837 -0700
+@@ -0,0 +1,1406 @@
++/*
++ * Copyright  2012-2014 Intel Corporation
++ *
++ * Permission is hereby granted, free of charge, to any person obtaining a
++ * copy of this software and associated documentation files (the "Software"),
++ * to deal in the Software without restriction, including without limitation
++ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
++ * and/or sell copies of the Software, and to permit persons to whom the
++ * Software is furnished to do so, subject to the following conditions:
++ *
++ * The above copyright notice and this permission notice (including the next
++ * paragraph) shall be included in all copies or substantial portions of the
++ * Software.
++ *
++ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
++ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
++ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
++ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
++ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
++ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
++ * IN THE SOFTWARE.
++ *
++ * Authors:
++ *    Eugeni Dodonov <eugeni.dodonov@intel.com>
++ *    Daniel Vetter <daniel.vetter@ffwll.ch>
++ *
++ */
++
++#include <linux/pm_runtime.h>
++#include <linux/vgaarb.h>
++
++#include "i915_drv.h"
++#include "intel_drv.h"
++#include <drm/i915_powerwell.h>
++
++/**
++ * DOC: runtime pm
++ *
++ * The i915 driver supports dynamic enabling and disabling of entire hardware
++ * blocks at runtime. This is especially important on the display side where
++ * software is supposed to control many power gates manually on recent hardware,
++ * since on the GT side a lot of the power management is done by the hardware.
++ * But even there some manual control at the device level is required.
++ *
++ * Since i915 supports a diverse set of platforms with a unified codebase and
++ * hardware engineers just love to shuffle functionality around between power
++ * domains there's a sizeable amount of indirection required. This file provides
++ * generic functions to the driver for grabbing and releasing references for
++ * abstract power domains. It then maps those to the actual power wells
++ * present for a given platform.
++ */
++
++static struct i915_power_domains *hsw_pwr;
++
++#define for_each_power_well(i, power_well, domain_mask, power_domains)	\
++	for (i = 0;							\
++	     i < (power_domains)->power_well_count &&			\
++		 ((power_well) = &(power_domains)->power_wells[i]);	\
++	     i++)							\
++		if ((power_well)->domains & (domain_mask))
++
++#define for_each_power_well_rev(i, power_well, domain_mask, power_domains) \
++	for (i = (power_domains)->power_well_count - 1;			 \
++	     i >= 0 && ((power_well) = &(power_domains)->power_wells[i]);\
++	     i--)							 \
++		if ((power_well)->domains & (domain_mask))
++
++/*
++ * We should only use the power well if we explicitly asked the hardware to
++ * enable it, so check if it's enabled and also check if we've requested it to
++ * be enabled.
++ */
++static bool hsw_power_well_enabled(struct drm_i915_private *dev_priv,
++				   struct i915_power_well *power_well)
++{
++	return I915_READ(HSW_PWR_WELL_DRIVER) ==
++		     (HSW_PWR_WELL_ENABLE_REQUEST | HSW_PWR_WELL_STATE_ENABLED);
++}
++
++/**
++ * __intel_display_power_is_enabled - unlocked check for a power domain
++ * @dev_priv: i915 device instance
++ * @domain: power domain to check
++ *
++ * This is the unlocked version of intel_display_power_is_enabled() and should
++ * only be used from error capture and recovery code where deadlocks are
++ * possible.
++ *
++ * Returns:
++ * True when the power domain is enabled, false otherwise.
++ */
++bool __intel_display_power_is_enabled(struct drm_i915_private *dev_priv,
++				      enum intel_display_power_domain domain)
++{
++	struct i915_power_domains *power_domains;
++	struct i915_power_well *power_well;
++	bool is_enabled;
++	int i;
++
++	if (dev_priv->pm.suspended)
++		return false;
++
++	power_domains = &dev_priv->power_domains;
++
++	is_enabled = true;
++
++	for_each_power_well_rev(i, power_well, BIT(domain), power_domains) {
++		if (power_well->always_on)
++			continue;
++
++		if (!power_well->hw_enabled) {
++			is_enabled = false;
++			break;
++		}
++	}
++
++	return is_enabled;
++}
++
++/**
++ * intel_display_power_is_enabled - unlocked check for a power domain
++ * @dev_priv: i915 device instance
++ * @domain: power domain to check
++ *
++ * This function can be used to check the hw power domain state. It is mostly
++ * used in hardware state readout functions. Everywhere else code should rely
++ * upon explicit power domain reference counting to ensure that the hardware
++ * block is powered up before accessing it.
++ *
++ * Callers must hold the relevant modesetting locks to ensure that concurrent
++ * threads can't disable the power well while the caller tries to read a few
++ * registers.
++ *
++ * Returns:
++ * True when the power domain is enabled, false otherwise.
++ */
++bool intel_display_power_is_enabled(struct drm_i915_private *dev_priv,
++				    enum intel_display_power_domain domain)
++{
++	struct i915_power_domains *power_domains;
++	bool ret;
++
++	power_domains = &dev_priv->power_domains;
++
++	mutex_lock(&power_domains->lock);
++	ret = __intel_display_power_is_enabled(dev_priv, domain);
++	mutex_unlock(&power_domains->lock);
++
++	return ret;
++}
++
++/**
++ * intel_display_set_init_power - set the initial power domain state
++ * @dev_priv: i915 device instance
++ * @enable: whether to enable or disable the initial power domain state
++ *
++ * For simplicity our driver load/unload and system suspend/resume code assumes
++ * that all power domains are always enabled. This functions controls the state
++ * of this little hack. While the initial power domain state is enabled runtime
++ * pm is effectively disabled.
++ */
++void intel_display_set_init_power(struct drm_i915_private *dev_priv,
++				  bool enable)
++{
++	if (dev_priv->power_domains.init_power_on == enable)
++		return;
++
++	if (enable)
++		intel_display_power_get(dev_priv, POWER_DOMAIN_INIT);
++	else
++		intel_display_power_put(dev_priv, POWER_DOMAIN_INIT);
++
++	dev_priv->power_domains.init_power_on = enable;
++}
++
++/*
++ * Starting with Haswell, we have a "Power Down Well" that can be turned off
++ * when not needed anymore. We have 4 registers that can request the power well
++ * to be enabled, and it will only be disabled if none of the registers is
++ * requesting it to be enabled.
++ */
++static void hsw_power_well_post_enable(struct drm_i915_private *dev_priv)
++{
++	struct drm_device *dev = dev_priv->dev;
++
++	/*
++	 * After we re-enable the power well, if we touch VGA register 0x3d5
++	 * we'll get unclaimed register interrupts. This stops after we write
++	 * anything to the VGA MSR register. The vgacon module uses this
++	 * register all the time, so if we unbind our driver and, as a
++	 * consequence, bind vgacon, we'll get stuck in an infinite loop at
++	 * console_unlock(). So make here we touch the VGA MSR register, making
++	 * sure vgacon can keep working normally without triggering interrupts
++	 * and error messages.
++	 */
++	vga_get_uninterruptible(dev->pdev, VGA_RSRC_LEGACY_IO);
++	outb(inb(VGA_MSR_READ), VGA_MSR_WRITE);
++	vga_put(dev->pdev, VGA_RSRC_LEGACY_IO);
++
++	if (IS_BROADWELL(dev) || (INTEL_INFO(dev)->gen >= 9))
++		gen8_irq_power_well_post_enable(dev_priv);
++}
++
++static void hsw_set_power_well(struct drm_i915_private *dev_priv,
++			       struct i915_power_well *power_well, bool enable)
++{
++	bool is_enabled, enable_requested;
++	uint32_t tmp;
++
++	tmp = I915_READ(HSW_PWR_WELL_DRIVER);
++	is_enabled = tmp & HSW_PWR_WELL_STATE_ENABLED;
++	enable_requested = tmp & HSW_PWR_WELL_ENABLE_REQUEST;
++
++	if (enable) {
++		if (!enable_requested)
++			I915_WRITE(HSW_PWR_WELL_DRIVER,
++				   HSW_PWR_WELL_ENABLE_REQUEST);
++
++		if (!is_enabled) {
++			DRM_DEBUG_KMS("Enabling power well\n");
++			if (wait_for((I915_READ(HSW_PWR_WELL_DRIVER) &
++				      HSW_PWR_WELL_STATE_ENABLED), 20))
++				DRM_ERROR("Timeout enabling power well\n");
++			hsw_power_well_post_enable(dev_priv);
++		}
++
++	} else {
++		if (enable_requested) {
++			I915_WRITE(HSW_PWR_WELL_DRIVER, 0);
++			POSTING_READ(HSW_PWR_WELL_DRIVER);
++			DRM_DEBUG_KMS("Requesting to disable the power well\n");
++		}
++	}
++}
++
++static void hsw_power_well_sync_hw(struct drm_i915_private *dev_priv,
++				   struct i915_power_well *power_well)
++{
++	hsw_set_power_well(dev_priv, power_well, power_well->count > 0);
++
++	/*
++	 * We're taking over the BIOS, so clear any requests made by it since
++	 * the driver is in charge now.
++	 */
++	if (I915_READ(HSW_PWR_WELL_BIOS) & HSW_PWR_WELL_ENABLE_REQUEST)
++		I915_WRITE(HSW_PWR_WELL_BIOS, 0);
++}
++
++static void hsw_power_well_enable(struct drm_i915_private *dev_priv,
++				  struct i915_power_well *power_well)
++{
++	hsw_set_power_well(dev_priv, power_well, true);
++}
++
++static void hsw_power_well_disable(struct drm_i915_private *dev_priv,
++				   struct i915_power_well *power_well)
++{
++	hsw_set_power_well(dev_priv, power_well, false);
++}
++
++static void i9xx_always_on_power_well_noop(struct drm_i915_private *dev_priv,
++					   struct i915_power_well *power_well)
++{
++}
++
++static bool i9xx_always_on_power_well_enabled(struct drm_i915_private *dev_priv,
++					     struct i915_power_well *power_well)
++{
++	return true;
++}
++
++static void vlv_set_power_well(struct drm_i915_private *dev_priv,
++			       struct i915_power_well *power_well, bool enable)
++{
++	enum punit_power_well power_well_id = power_well->data;
++	u32 mask;
++	u32 state;
++	u32 ctrl;
++
++	mask = PUNIT_PWRGT_MASK(power_well_id);
++	state = enable ? PUNIT_PWRGT_PWR_ON(power_well_id) :
++			 PUNIT_PWRGT_PWR_GATE(power_well_id);
++
++	mutex_lock(&dev_priv->rps.hw_lock);
++
++#define COND \
++	((vlv_punit_read(dev_priv, PUNIT_REG_PWRGT_STATUS) & mask) == state)
++
++	if (COND)
++		goto out;
++
++	ctrl = vlv_punit_read(dev_priv, PUNIT_REG_PWRGT_CTRL);
++	ctrl &= ~mask;
++	ctrl |= state;
++	vlv_punit_write(dev_priv, PUNIT_REG_PWRGT_CTRL, ctrl);
++
++	if (wait_for(COND, 100))
++		DRM_ERROR("timout setting power well state %08x (%08x)\n",
++			  state,
++			  vlv_punit_read(dev_priv, PUNIT_REG_PWRGT_CTRL));
++
++#undef COND
++
++out:
++	mutex_unlock(&dev_priv->rps.hw_lock);
++}
++
++static void vlv_power_well_sync_hw(struct drm_i915_private *dev_priv,
++				   struct i915_power_well *power_well)
++{
++	vlv_set_power_well(dev_priv, power_well, power_well->count > 0);
++}
++
++static void vlv_power_well_enable(struct drm_i915_private *dev_priv,
++				  struct i915_power_well *power_well)
++{
++	vlv_set_power_well(dev_priv, power_well, true);
++}
++
++static void vlv_power_well_disable(struct drm_i915_private *dev_priv,
++				   struct i915_power_well *power_well)
++{
++	vlv_set_power_well(dev_priv, power_well, false);
++}
++
++static bool vlv_power_well_enabled(struct drm_i915_private *dev_priv,
++				   struct i915_power_well *power_well)
++{
++	int power_well_id = power_well->data;
++	bool enabled = false;
++	u32 mask;
++	u32 state;
++	u32 ctrl;
++
++	mask = PUNIT_PWRGT_MASK(power_well_id);
++	ctrl = PUNIT_PWRGT_PWR_ON(power_well_id);
++
++	mutex_lock(&dev_priv->rps.hw_lock);
++
++	state = vlv_punit_read(dev_priv, PUNIT_REG_PWRGT_STATUS) & mask;
++	/*
++	 * We only ever set the power-on and power-gate states, anything
++	 * else is unexpected.
++	 */
++	WARN_ON(state != PUNIT_PWRGT_PWR_ON(power_well_id) &&
++		state != PUNIT_PWRGT_PWR_GATE(power_well_id));
++	if (state == ctrl)
++		enabled = true;
++
++	/*
++	 * A transient state at this point would mean some unexpected party
++	 * is poking at the power controls too.
++	 */
++	ctrl = vlv_punit_read(dev_priv, PUNIT_REG_PWRGT_CTRL) & mask;
++	WARN_ON(ctrl != state);
++
++	mutex_unlock(&dev_priv->rps.hw_lock);
++
++	return enabled;
++}
++
++static void vlv_display_power_well_enable(struct drm_i915_private *dev_priv,
++					  struct i915_power_well *power_well)
++{
++	WARN_ON_ONCE(power_well->data != PUNIT_POWER_WELL_DISP2D);
++
++	vlv_set_power_well(dev_priv, power_well, true);
++
++	spin_lock_irq(&dev_priv->irq_lock);
++	valleyview_enable_display_irqs(dev_priv);
++	spin_unlock_irq(&dev_priv->irq_lock);
++
++	/*
++	 * During driver initialization/resume we can avoid restoring the
++	 * part of the HW/SW state that will be inited anyway explicitly.
++	 */
++	if (dev_priv->power_domains.initializing)
++		return;
++
++	intel_hpd_init(dev_priv);
++
++	i915_redisable_vga_power_on(dev_priv->dev);
++}
++
++static void vlv_display_power_well_disable(struct drm_i915_private *dev_priv,
++					   struct i915_power_well *power_well)
++{
++	WARN_ON_ONCE(power_well->data != PUNIT_POWER_WELL_DISP2D);
++
++	spin_lock_irq(&dev_priv->irq_lock);
++	valleyview_disable_display_irqs(dev_priv);
++	spin_unlock_irq(&dev_priv->irq_lock);
++
++	vlv_set_power_well(dev_priv, power_well, false);
++
++	vlv_power_sequencer_reset(dev_priv);
++}
++
++static void vlv_dpio_cmn_power_well_enable(struct drm_i915_private *dev_priv,
++					   struct i915_power_well *power_well)
++{
++	WARN_ON_ONCE(power_well->data != PUNIT_POWER_WELL_DPIO_CMN_BC);
++
++	/*
++	 * Enable the CRI clock source so we can get at the
++	 * display and the reference clock for VGA
++	 * hotplug / manual detection.
++	 */
++	I915_WRITE(DPLL(PIPE_B), I915_READ(DPLL(PIPE_B)) |
++		   DPLL_REFA_CLK_ENABLE_VLV | DPLL_INTEGRATED_CRI_CLK_VLV);
++	udelay(1); /* >10ns for cmnreset, >0ns for sidereset */
++
++	vlv_set_power_well(dev_priv, power_well, true);
++
++	/*
++	 * From VLV2A0_DP_eDP_DPIO_driver_vbios_notes_10.docx -
++	 *  6.	De-assert cmn_reset/side_reset. Same as VLV X0.
++	 *   a.	GUnit 0x2110 bit[0] set to 1 (def 0)
++	 *   b.	The other bits such as sfr settings / modesel may all
++	 *	be set to 0.
++	 *
++	 * This should only be done on init and resume from S3 with
++	 * both PLLs disabled, or we risk losing DPIO and PLL
++	 * synchronization.
++	 */
++	I915_WRITE(DPIO_CTL, I915_READ(DPIO_CTL) | DPIO_CMNRST);
++}
++
++static void vlv_dpio_cmn_power_well_disable(struct drm_i915_private *dev_priv,
++					    struct i915_power_well *power_well)
++{
++	enum pipe pipe;
++
++	WARN_ON_ONCE(power_well->data != PUNIT_POWER_WELL_DPIO_CMN_BC);
++
++	for_each_pipe(dev_priv, pipe)
++		assert_pll_disabled(dev_priv, pipe);
++
++	/* Assert common reset */
++	I915_WRITE(DPIO_CTL, I915_READ(DPIO_CTL) & ~DPIO_CMNRST);
++
++	vlv_set_power_well(dev_priv, power_well, false);
++}
++
++static void chv_dpio_cmn_power_well_enable(struct drm_i915_private *dev_priv,
++					   struct i915_power_well *power_well)
++{
++	enum dpio_phy phy;
++
++	WARN_ON_ONCE(power_well->data != PUNIT_POWER_WELL_DPIO_CMN_BC &&
++		     power_well->data != PUNIT_POWER_WELL_DPIO_CMN_D);
++
++	/*
++	 * Enable the CRI clock source so we can get at the
++	 * display and the reference clock for VGA
++	 * hotplug / manual detection.
++	 */
++	if (power_well->data == PUNIT_POWER_WELL_DPIO_CMN_BC) {
++		phy = DPIO_PHY0;
++		I915_WRITE(DPLL(PIPE_B), I915_READ(DPLL(PIPE_B)) |
++			   DPLL_REFA_CLK_ENABLE_VLV);
++		I915_WRITE(DPLL(PIPE_B), I915_READ(DPLL(PIPE_B)) |
++			   DPLL_REFA_CLK_ENABLE_VLV | DPLL_INTEGRATED_CRI_CLK_VLV);
++	} else {
++		phy = DPIO_PHY1;
++		I915_WRITE(DPLL(PIPE_C), I915_READ(DPLL(PIPE_C)) |
++			   DPLL_REFA_CLK_ENABLE_VLV | DPLL_INTEGRATED_CRI_CLK_VLV);
++	}
++	udelay(1); /* >10ns for cmnreset, >0ns for sidereset */
++	vlv_set_power_well(dev_priv, power_well, true);
++
++	/* Poll for phypwrgood signal */
++	if (wait_for(I915_READ(DISPLAY_PHY_STATUS) & PHY_POWERGOOD(phy), 1))
++		DRM_ERROR("Display PHY %d is not power up\n", phy);
++
++	I915_WRITE(DISPLAY_PHY_CONTROL, I915_READ(DISPLAY_PHY_CONTROL) |
++		   PHY_COM_LANE_RESET_DEASSERT(phy));
++}
++
++static void chv_dpio_cmn_power_well_disable(struct drm_i915_private *dev_priv,
++					    struct i915_power_well *power_well)
++{
++	enum dpio_phy phy;
++
++	WARN_ON_ONCE(power_well->data != PUNIT_POWER_WELL_DPIO_CMN_BC &&
++		     power_well->data != PUNIT_POWER_WELL_DPIO_CMN_D);
++
++	if (power_well->data == PUNIT_POWER_WELL_DPIO_CMN_BC) {
++		phy = DPIO_PHY0;
++		assert_pll_disabled(dev_priv, PIPE_A);
++		assert_pll_disabled(dev_priv, PIPE_B);
++	} else {
++		phy = DPIO_PHY1;
++		assert_pll_disabled(dev_priv, PIPE_C);
++	}
++
++	I915_WRITE(DISPLAY_PHY_CONTROL, I915_READ(DISPLAY_PHY_CONTROL) &
++		   ~PHY_COM_LANE_RESET_DEASSERT(phy));
++
++	vlv_set_power_well(dev_priv, power_well, false);
++}
++
++static bool chv_pipe_power_well_enabled(struct drm_i915_private *dev_priv,
++					struct i915_power_well *power_well)
++{
++	enum pipe pipe = power_well->data;
++	bool enabled;
++	u32 state, ctrl;
++
++	mutex_lock(&dev_priv->rps.hw_lock);
++
++	state = vlv_punit_read(dev_priv, PUNIT_REG_DSPFREQ) & DP_SSS_MASK(pipe);
++	/*
++	 * We only ever set the power-on and power-gate states, anything
++	 * else is unexpected.
++	 */
++	WARN_ON(state != DP_SSS_PWR_ON(pipe) && state != DP_SSS_PWR_GATE(pipe));
++	enabled = state == DP_SSS_PWR_ON(pipe);
++
++	/*
++	 * A transient state at this point would mean some unexpected party
++	 * is poking at the power controls too.
++	 */
++	ctrl = vlv_punit_read(dev_priv, PUNIT_REG_DSPFREQ) & DP_SSC_MASK(pipe);
++	WARN_ON(ctrl << 16 != state);
++
++	mutex_unlock(&dev_priv->rps.hw_lock);
++
++	return enabled;
++}
++
++static void chv_set_pipe_power_well(struct drm_i915_private *dev_priv,
++				    struct i915_power_well *power_well,
++				    bool enable)
++{
++	enum pipe pipe = power_well->data;
++	u32 state;
++	u32 ctrl;
++
++	state = enable ? DP_SSS_PWR_ON(pipe) : DP_SSS_PWR_GATE(pipe);
++
++	mutex_lock(&dev_priv->rps.hw_lock);
++
++#define COND \
++	((vlv_punit_read(dev_priv, PUNIT_REG_DSPFREQ) & DP_SSS_MASK(pipe)) == state)
++
++	if (COND)
++		goto out;
++
++	ctrl = vlv_punit_read(dev_priv, PUNIT_REG_DSPFREQ);
++	ctrl &= ~DP_SSC_MASK(pipe);
++	ctrl |= enable ? DP_SSC_PWR_ON(pipe) : DP_SSC_PWR_GATE(pipe);
++	vlv_punit_write(dev_priv, PUNIT_REG_DSPFREQ, ctrl);
++
++	if (wait_for(COND, 100))
++		DRM_ERROR("timout setting power well state %08x (%08x)\n",
++			  state,
++			  vlv_punit_read(dev_priv, PUNIT_REG_DSPFREQ));
++
++#undef COND
++
++out:
++	mutex_unlock(&dev_priv->rps.hw_lock);
++}
++
++static void chv_pipe_power_well_sync_hw(struct drm_i915_private *dev_priv,
++					struct i915_power_well *power_well)
++{
++	chv_set_pipe_power_well(dev_priv, power_well, power_well->count > 0);
++}
++
++static void chv_pipe_power_well_enable(struct drm_i915_private *dev_priv,
++				       struct i915_power_well *power_well)
++{
++	WARN_ON_ONCE(power_well->data != PIPE_A &&
++		     power_well->data != PIPE_B &&
++		     power_well->data != PIPE_C);
++
++	chv_set_pipe_power_well(dev_priv, power_well, true);
++
++	if (power_well->data == PIPE_A) {
++		spin_lock_irq(&dev_priv->irq_lock);
++		valleyview_enable_display_irqs(dev_priv);
++		spin_unlock_irq(&dev_priv->irq_lock);
++
++		/*
++		 * During driver initialization/resume we can avoid restoring the
++		 * part of the HW/SW state that will be inited anyway explicitly.
++		 */
++		if (dev_priv->power_domains.initializing)
++			return;
++
++		intel_hpd_init(dev_priv);
++
++		i915_redisable_vga_power_on(dev_priv->dev);
++	}
++}
++
++static void chv_pipe_power_well_disable(struct drm_i915_private *dev_priv,
++					struct i915_power_well *power_well)
++{
++	WARN_ON_ONCE(power_well->data != PIPE_A &&
++		     power_well->data != PIPE_B &&
++		     power_well->data != PIPE_C);
++
++	if (power_well->data == PIPE_A) {
++		spin_lock_irq(&dev_priv->irq_lock);
++		valleyview_disable_display_irqs(dev_priv);
++		spin_unlock_irq(&dev_priv->irq_lock);
++	}
++
++	chv_set_pipe_power_well(dev_priv, power_well, false);
++
++	if (power_well->data == PIPE_A)
++		vlv_power_sequencer_reset(dev_priv);
++}
++
++static void check_power_well_state(struct drm_i915_private *dev_priv,
++				   struct i915_power_well *power_well)
++{
++	bool enabled = power_well->ops->is_enabled(dev_priv, power_well);
++
++	if (power_well->always_on || !i915_module.disable_power_well) {
++		if (!enabled)
++			goto mismatch;
++
++		return;
++	}
++
++	if (enabled != (power_well->count > 0))
++		goto mismatch;
++
++	return;
++
++mismatch:
++	WARN(1, "state mismatch for '%s' (always_on %d hw state %d use-count %d disable_power_well %d\n",
++		  power_well->name, power_well->always_on, enabled,
++		  power_well->count, i915_module.disable_power_well);
++}
++
++/**
++ * intel_display_power_get - grab a power domain reference
++ * @dev_priv: i915 device instance
++ * @domain: power domain to reference
++ *
++ * This function grabs a power domain reference for @domain and ensures that the
++ * power domain and all its parents are powered up. Therefore users should only
++ * grab a reference to the innermost power domain they need.
++ *
++ * Any power domain reference obtained by this function must have a symmetric
++ * call to intel_display_power_put() to release the reference again.
++ */
++void intel_display_power_get(struct drm_i915_private *dev_priv,
++			     enum intel_display_power_domain domain)
++{
++	struct i915_power_domains *power_domains;
++	struct i915_power_well *power_well;
++	int i;
++
++	intel_runtime_pm_get(dev_priv);
++
++	power_domains = &dev_priv->power_domains;
++
++	mutex_lock(&power_domains->lock);
++
++	for_each_power_well(i, power_well, BIT(domain), power_domains) {
++		if (!power_well->count++) {
++			DRM_DEBUG_KMS("enabling %s\n", power_well->name);
++			power_well->ops->enable(dev_priv, power_well);
++			power_well->hw_enabled = true;
++		}
++
++		check_power_well_state(dev_priv, power_well);
++	}
++
++	power_domains->domain_use_count[domain]++;
++
++	mutex_unlock(&power_domains->lock);
++}
++
++/**
++ * intel_display_power_put - release a power domain reference
++ * @dev_priv: i915 device instance
++ * @domain: power domain to reference
++ *
++ * This function drops the power domain reference obtained by
++ * intel_display_power_get() and might power down the corresponding hardware
++ * block right away if this is the last reference.
++ */
++void intel_display_power_put(struct drm_i915_private *dev_priv,
++			     enum intel_display_power_domain domain)
++{
++	struct i915_power_domains *power_domains;
++	struct i915_power_well *power_well;
++	int i;
++
++	power_domains = &dev_priv->power_domains;
++
++	mutex_lock(&power_domains->lock);
++
++	WARN_ON(!power_domains->domain_use_count[domain]);
++	power_domains->domain_use_count[domain]--;
++
++	for_each_power_well_rev(i, power_well, BIT(domain), power_domains) {
++		WARN_ON(!power_well->count);
++
++		if (!--power_well->count && i915_module.disable_power_well) {
++			DRM_DEBUG_KMS("disabling %s\n", power_well->name);
++			power_well->hw_enabled = false;
++			power_well->ops->disable(dev_priv, power_well);
++		}
++
++		check_power_well_state(dev_priv, power_well);
++	}
++
++	mutex_unlock(&power_domains->lock);
++
++	intel_runtime_pm_put(dev_priv);
++}
++
++#define POWER_DOMAIN_MASK (BIT(POWER_DOMAIN_NUM) - 1)
++
++#define HSW_ALWAYS_ON_POWER_DOMAINS (			\
++	BIT(POWER_DOMAIN_PIPE_A) |			\
++	BIT(POWER_DOMAIN_TRANSCODER_EDP) |		\
++	BIT(POWER_DOMAIN_PORT_DDI_A_2_LANES) |		\
++	BIT(POWER_DOMAIN_PORT_DDI_A_4_LANES) |		\
++	BIT(POWER_DOMAIN_PORT_DDI_B_2_LANES) |		\
++	BIT(POWER_DOMAIN_PORT_DDI_B_4_LANES) |		\
++	BIT(POWER_DOMAIN_PORT_DDI_C_2_LANES) |		\
++	BIT(POWER_DOMAIN_PORT_DDI_C_4_LANES) |		\
++	BIT(POWER_DOMAIN_PORT_DDI_D_2_LANES) |		\
++	BIT(POWER_DOMAIN_PORT_DDI_D_4_LANES) |		\
++	BIT(POWER_DOMAIN_PORT_CRT) |			\
++	BIT(POWER_DOMAIN_PLLS) |			\
++	BIT(POWER_DOMAIN_INIT))
++#define HSW_DISPLAY_POWER_DOMAINS (				\
++	(POWER_DOMAIN_MASK & ~HSW_ALWAYS_ON_POWER_DOMAINS) |	\
++	BIT(POWER_DOMAIN_INIT))
++
++#define BDW_ALWAYS_ON_POWER_DOMAINS (			\
++	HSW_ALWAYS_ON_POWER_DOMAINS |			\
++	BIT(POWER_DOMAIN_PIPE_A_PANEL_FITTER))
++#define BDW_DISPLAY_POWER_DOMAINS (				\
++	(POWER_DOMAIN_MASK & ~BDW_ALWAYS_ON_POWER_DOMAINS) |	\
++	BIT(POWER_DOMAIN_INIT))
++
++#define VLV_ALWAYS_ON_POWER_DOMAINS	BIT(POWER_DOMAIN_INIT)
++#define VLV_DISPLAY_POWER_DOMAINS	POWER_DOMAIN_MASK
++
++#define VLV_DPIO_CMN_BC_POWER_DOMAINS (		\
++	BIT(POWER_DOMAIN_PORT_DDI_B_2_LANES) |	\
++	BIT(POWER_DOMAIN_PORT_DDI_B_4_LANES) |	\
++	BIT(POWER_DOMAIN_PORT_DDI_C_2_LANES) |	\
++	BIT(POWER_DOMAIN_PORT_DDI_C_4_LANES) |	\
++	BIT(POWER_DOMAIN_PORT_CRT) |		\
++	BIT(POWER_DOMAIN_INIT))
++
++#define VLV_DPIO_TX_B_LANES_01_POWER_DOMAINS (	\
++	BIT(POWER_DOMAIN_PORT_DDI_B_2_LANES) |	\
++	BIT(POWER_DOMAIN_PORT_DDI_B_4_LANES) |	\
++	BIT(POWER_DOMAIN_INIT))
++
++#define VLV_DPIO_TX_B_LANES_23_POWER_DOMAINS (	\
++	BIT(POWER_DOMAIN_PORT_DDI_B_4_LANES) |	\
++	BIT(POWER_DOMAIN_INIT))
++
++#define VLV_DPIO_TX_C_LANES_01_POWER_DOMAINS (	\
++	BIT(POWER_DOMAIN_PORT_DDI_C_2_LANES) |	\
++	BIT(POWER_DOMAIN_PORT_DDI_C_4_LANES) |	\
++	BIT(POWER_DOMAIN_INIT))
++
++#define VLV_DPIO_TX_C_LANES_23_POWER_DOMAINS (	\
++	BIT(POWER_DOMAIN_PORT_DDI_C_4_LANES) |	\
++	BIT(POWER_DOMAIN_INIT))
++
++#define CHV_PIPE_A_POWER_DOMAINS (	\
++	BIT(POWER_DOMAIN_PIPE_A) |	\
++	BIT(POWER_DOMAIN_INIT))
++
++#define CHV_PIPE_B_POWER_DOMAINS (	\
++	BIT(POWER_DOMAIN_PIPE_B) |	\
++	BIT(POWER_DOMAIN_INIT))
++
++#define CHV_PIPE_C_POWER_DOMAINS (	\
++	BIT(POWER_DOMAIN_PIPE_C) |	\
++	BIT(POWER_DOMAIN_INIT))
++
++#define CHV_DPIO_CMN_BC_POWER_DOMAINS (		\
++	BIT(POWER_DOMAIN_PORT_DDI_B_2_LANES) |	\
++	BIT(POWER_DOMAIN_PORT_DDI_B_4_LANES) |	\
++	BIT(POWER_DOMAIN_PORT_DDI_C_2_LANES) |	\
++	BIT(POWER_DOMAIN_PORT_DDI_C_4_LANES) |	\
++	BIT(POWER_DOMAIN_INIT))
++
++#define CHV_DPIO_CMN_D_POWER_DOMAINS (		\
++	BIT(POWER_DOMAIN_PORT_DDI_D_2_LANES) |	\
++	BIT(POWER_DOMAIN_PORT_DDI_D_4_LANES) |	\
++	BIT(POWER_DOMAIN_INIT))
++
++#define CHV_DPIO_TX_D_LANES_01_POWER_DOMAINS (	\
++	BIT(POWER_DOMAIN_PORT_DDI_D_2_LANES) |	\
++	BIT(POWER_DOMAIN_PORT_DDI_D_4_LANES) |	\
++	BIT(POWER_DOMAIN_INIT))
++
++#define CHV_DPIO_TX_D_LANES_23_POWER_DOMAINS (	\
++	BIT(POWER_DOMAIN_PORT_DDI_D_4_LANES) |	\
++	BIT(POWER_DOMAIN_INIT))
++
++static const struct i915_power_well_ops i9xx_always_on_power_well_ops = {
++	.sync_hw = i9xx_always_on_power_well_noop,
++	.enable = i9xx_always_on_power_well_noop,
++	.disable = i9xx_always_on_power_well_noop,
++	.is_enabled = i9xx_always_on_power_well_enabled,
++};
++
++static const struct i915_power_well_ops chv_pipe_power_well_ops = {
++	.sync_hw = chv_pipe_power_well_sync_hw,
++	.enable = chv_pipe_power_well_enable,
++	.disable = chv_pipe_power_well_disable,
++	.is_enabled = chv_pipe_power_well_enabled,
++};
++
++static const struct i915_power_well_ops chv_dpio_cmn_power_well_ops = {
++	.sync_hw = vlv_power_well_sync_hw,
++	.enable = chv_dpio_cmn_power_well_enable,
++	.disable = chv_dpio_cmn_power_well_disable,
++	.is_enabled = vlv_power_well_enabled,
++};
++
++static struct i915_power_well i9xx_always_on_power_well[] = {
++	{
++		.name = "always-on",
++		.always_on = 1,
++		.domains = POWER_DOMAIN_MASK,
++		.ops = &i9xx_always_on_power_well_ops,
++	},
++};
++
++static const struct i915_power_well_ops hsw_power_well_ops = {
++	.sync_hw = hsw_power_well_sync_hw,
++	.enable = hsw_power_well_enable,
++	.disable = hsw_power_well_disable,
++	.is_enabled = hsw_power_well_enabled,
++};
++
++static struct i915_power_well hsw_power_wells[] = {
++	{
++		.name = "always-on",
++		.always_on = 1,
++		.domains = HSW_ALWAYS_ON_POWER_DOMAINS,
++		.ops = &i9xx_always_on_power_well_ops,
++	},
++	{
++		.name = "display",
++		.domains = HSW_DISPLAY_POWER_DOMAINS,
++		.ops = &hsw_power_well_ops,
++	},
++};
++
++static struct i915_power_well bdw_power_wells[] = {
++	{
++		.name = "always-on",
++		.always_on = 1,
++		.domains = BDW_ALWAYS_ON_POWER_DOMAINS,
++		.ops = &i9xx_always_on_power_well_ops,
++	},
++	{
++		.name = "display",
++		.domains = BDW_DISPLAY_POWER_DOMAINS,
++		.ops = &hsw_power_well_ops,
++	},
++};
++
++static const struct i915_power_well_ops vlv_display_power_well_ops = {
++	.sync_hw = vlv_power_well_sync_hw,
++	.enable = vlv_display_power_well_enable,
++	.disable = vlv_display_power_well_disable,
++	.is_enabled = vlv_power_well_enabled,
++};
++
++static const struct i915_power_well_ops vlv_dpio_cmn_power_well_ops = {
++	.sync_hw = vlv_power_well_sync_hw,
++	.enable = vlv_dpio_cmn_power_well_enable,
++	.disable = vlv_dpio_cmn_power_well_disable,
++	.is_enabled = vlv_power_well_enabled,
++};
++
++static const struct i915_power_well_ops vlv_dpio_power_well_ops = {
++	.sync_hw = vlv_power_well_sync_hw,
++	.enable = vlv_power_well_enable,
++	.disable = vlv_power_well_disable,
++	.is_enabled = vlv_power_well_enabled,
++};
++
++static struct i915_power_well vlv_power_wells[] = {
++	{
++		.name = "always-on",
++		.always_on = 1,
++		.domains = VLV_ALWAYS_ON_POWER_DOMAINS,
++		.ops = &i9xx_always_on_power_well_ops,
++	},
++	{
++		.name = "display",
++		.domains = VLV_DISPLAY_POWER_DOMAINS,
++		.data = PUNIT_POWER_WELL_DISP2D,
++		.ops = &vlv_display_power_well_ops,
++	},
++	{
++		.name = "dpio-tx-b-01",
++		.domains = VLV_DPIO_TX_B_LANES_01_POWER_DOMAINS |
++			   VLV_DPIO_TX_B_LANES_23_POWER_DOMAINS |
++			   VLV_DPIO_TX_C_LANES_01_POWER_DOMAINS |
++			   VLV_DPIO_TX_C_LANES_23_POWER_DOMAINS,
++		.ops = &vlv_dpio_power_well_ops,
++		.data = PUNIT_POWER_WELL_DPIO_TX_B_LANES_01,
++	},
++	{
++		.name = "dpio-tx-b-23",
++		.domains = VLV_DPIO_TX_B_LANES_01_POWER_DOMAINS |
++			   VLV_DPIO_TX_B_LANES_23_POWER_DOMAINS |
++			   VLV_DPIO_TX_C_LANES_01_POWER_DOMAINS |
++			   VLV_DPIO_TX_C_LANES_23_POWER_DOMAINS,
++		.ops = &vlv_dpio_power_well_ops,
++		.data = PUNIT_POWER_WELL_DPIO_TX_B_LANES_23,
++	},
++	{
++		.name = "dpio-tx-c-01",
++		.domains = VLV_DPIO_TX_B_LANES_01_POWER_DOMAINS |
++			   VLV_DPIO_TX_B_LANES_23_POWER_DOMAINS |
++			   VLV_DPIO_TX_C_LANES_01_POWER_DOMAINS |
++			   VLV_DPIO_TX_C_LANES_23_POWER_DOMAINS,
++		.ops = &vlv_dpio_power_well_ops,
++		.data = PUNIT_POWER_WELL_DPIO_TX_C_LANES_01,
++	},
++	{
++		.name = "dpio-tx-c-23",
++		.domains = VLV_DPIO_TX_B_LANES_01_POWER_DOMAINS |
++			   VLV_DPIO_TX_B_LANES_23_POWER_DOMAINS |
++			   VLV_DPIO_TX_C_LANES_01_POWER_DOMAINS |
++			   VLV_DPIO_TX_C_LANES_23_POWER_DOMAINS,
++		.ops = &vlv_dpio_power_well_ops,
++		.data = PUNIT_POWER_WELL_DPIO_TX_C_LANES_23,
++	},
++	{
++		.name = "dpio-common",
++		.domains = VLV_DPIO_CMN_BC_POWER_DOMAINS,
++		.data = PUNIT_POWER_WELL_DPIO_CMN_BC,
++		.ops = &vlv_dpio_cmn_power_well_ops,
++	},
++};
++
++static struct i915_power_well chv_power_wells[] = {
++	{
++		.name = "always-on",
++		.always_on = 1,
++		.domains = VLV_ALWAYS_ON_POWER_DOMAINS,
++		.ops = &i9xx_always_on_power_well_ops,
++	},
++#if 0
++	{
++		.name = "display",
++		.domains = VLV_DISPLAY_POWER_DOMAINS,
++		.data = PUNIT_POWER_WELL_DISP2D,
++		.ops = &vlv_display_power_well_ops,
++	},
++#endif
++	{
++		.name = "pipe-a",
++		/*
++		 * FIXME: pipe A power well seems to be the new disp2d well.
++		 * At least all registers seem to be housed there. Figure
++		 * out if this a a temporary situation in pre-production
++		 * hardware or a permanent state of affairs.
++		 */
++		.domains = CHV_PIPE_A_POWER_DOMAINS | VLV_DISPLAY_POWER_DOMAINS,
++		.data = PIPE_A,
++		.ops = &chv_pipe_power_well_ops,
++	},
++#if 0
++	{
++		.name = "pipe-b",
++		.domains = CHV_PIPE_B_POWER_DOMAINS,
++		.data = PIPE_B,
++		.ops = &chv_pipe_power_well_ops,
++	},
++	{
++		.name = "pipe-c",
++		.domains = CHV_PIPE_C_POWER_DOMAINS,
++		.data = PIPE_C,
++		.ops = &chv_pipe_power_well_ops,
++	},
++#endif
++	{
++		.name = "dpio-common-bc",
++		/*
++		 * XXX: cmnreset for one PHY seems to disturb the other.
++		 * As a workaround keep both powered on at the same
++		 * time for now.
++		 */
++		.domains = CHV_DPIO_CMN_BC_POWER_DOMAINS | CHV_DPIO_CMN_D_POWER_DOMAINS,
++		.data = PUNIT_POWER_WELL_DPIO_CMN_BC,
++		.ops = &chv_dpio_cmn_power_well_ops,
++	},
++	{
++		.name = "dpio-common-d",
++		/*
++		 * XXX: cmnreset for one PHY seems to disturb the other.
++		 * As a workaround keep both powered on at the same
++		 * time for now.
++		 */
++		.domains = CHV_DPIO_CMN_BC_POWER_DOMAINS | CHV_DPIO_CMN_D_POWER_DOMAINS,
++		.data = PUNIT_POWER_WELL_DPIO_CMN_D,
++		.ops = &chv_dpio_cmn_power_well_ops,
++	},
++#if 0
++	{
++		.name = "dpio-tx-b-01",
++		.domains = VLV_DPIO_TX_B_LANES_01_POWER_DOMAINS |
++			   VLV_DPIO_TX_B_LANES_23_POWER_DOMAINS,
++		.ops = &vlv_dpio_power_well_ops,
++		.data = PUNIT_POWER_WELL_DPIO_TX_B_LANES_01,
++	},
++	{
++		.name = "dpio-tx-b-23",
++		.domains = VLV_DPIO_TX_B_LANES_01_POWER_DOMAINS |
++			   VLV_DPIO_TX_B_LANES_23_POWER_DOMAINS,
++		.ops = &vlv_dpio_power_well_ops,
++		.data = PUNIT_POWER_WELL_DPIO_TX_B_LANES_23,
++	},
++	{
++		.name = "dpio-tx-c-01",
++		.domains = VLV_DPIO_TX_C_LANES_01_POWER_DOMAINS |
++			   VLV_DPIO_TX_C_LANES_23_POWER_DOMAINS,
++		.ops = &vlv_dpio_power_well_ops,
++		.data = PUNIT_POWER_WELL_DPIO_TX_C_LANES_01,
++	},
++	{
++		.name = "dpio-tx-c-23",
++		.domains = VLV_DPIO_TX_C_LANES_01_POWER_DOMAINS |
++			   VLV_DPIO_TX_C_LANES_23_POWER_DOMAINS,
++		.ops = &vlv_dpio_power_well_ops,
++		.data = PUNIT_POWER_WELL_DPIO_TX_C_LANES_23,
++	},
++	{
++		.name = "dpio-tx-d-01",
++		.domains = CHV_DPIO_TX_D_LANES_01_POWER_DOMAINS |
++			   CHV_DPIO_TX_D_LANES_23_POWER_DOMAINS,
++		.ops = &vlv_dpio_power_well_ops,
++		.data = PUNIT_POWER_WELL_DPIO_TX_D_LANES_01,
++	},
++	{
++		.name = "dpio-tx-d-23",
++		.domains = CHV_DPIO_TX_D_LANES_01_POWER_DOMAINS |
++			   CHV_DPIO_TX_D_LANES_23_POWER_DOMAINS,
++		.ops = &vlv_dpio_power_well_ops,
++		.data = PUNIT_POWER_WELL_DPIO_TX_D_LANES_23,
++	},
++#endif
++};
++
++static struct i915_power_well *lookup_power_well(struct drm_i915_private *dev_priv,
++						 enum punit_power_well power_well_id)
++{
++	struct i915_power_domains *power_domains = &dev_priv->power_domains;
++	struct i915_power_well *power_well;
++	int i;
++
++	for_each_power_well(i, power_well, POWER_DOMAIN_MASK, power_domains) {
++		if (power_well->data == power_well_id)
++			return power_well;
++	}
++
++	return NULL;
++}
++
++#define set_power_wells(power_domains, __power_wells) ({		\
++	(power_domains)->power_wells = (__power_wells);			\
++	(power_domains)->power_well_count = ARRAY_SIZE(__power_wells);	\
++})
++
++/**
++ * intel_power_domains_init - initializes the power domain structures
++ * @dev_priv: i915 device instance
++ *
++ * Initializes the power domain structures for @dev_priv depending upon the
++ * supported platform.
++ */
++int intel_power_domains_init(struct drm_i915_private *dev_priv)
++{
++	struct i915_power_domains *power_domains = &dev_priv->power_domains;
++
++	mutex_init(&power_domains->lock);
++
++	/*
++	 * The enabling order will be from lower to higher indexed wells,
++	 * the disabling order is reversed.
++	 */
++	if (IS_HASWELL(dev_priv->dev)) {
++		set_power_wells(power_domains, hsw_power_wells);
++		hsw_pwr = power_domains;
++	} else if (IS_BROADWELL(dev_priv->dev)) {
++		set_power_wells(power_domains, bdw_power_wells);
++		hsw_pwr = power_domains;
++	} else if (IS_CHERRYVIEW(dev_priv->dev)) {
++		set_power_wells(power_domains, chv_power_wells);
++	} else if (IS_VALLEYVIEW(dev_priv->dev)) {
++		set_power_wells(power_domains, vlv_power_wells);
++	} else {
++		set_power_wells(power_domains, i9xx_always_on_power_well);
++	}
++
++	return 0;
++}
++
++static void intel_runtime_pm_disable(struct drm_i915_private *dev_priv)
++{
++	struct drm_device *dev = dev_priv->dev;
++	struct device *device = &dev->pdev->dev;
++
++	if (!HAS_RUNTIME_PM(dev))
++		return;
++
++	if (!intel_enable_rc6(dev))
++		return;
++
++	/* Make sure we're not suspended first. */
++	pm_runtime_get_sync(device);
++	pm_runtime_disable(device);
++}
++
++/**
++ * intel_power_domains_fini - finalizes the power domain structures
++ * @dev_priv: i915 device instance
++ *
++ * Finalizes the power domain structures for @dev_priv depending upon the
++ * supported platform. This function also disables runtime pm and ensures that
++ * the device stays powered up so that the driver can be reloaded.
++ */
++void intel_power_domains_fini(struct drm_i915_private *dev_priv)
++{
++	intel_runtime_pm_disable(dev_priv);
++
++	/* The i915.ko module is still not prepared to be loaded when
++	 * the power well is not enabled, so just enable it in case
++	 * we're going to unload/reload. */
++	intel_display_set_init_power(dev_priv, true);
++
++	hsw_pwr = NULL;
++}
++
++static void intel_power_domains_resume(struct drm_i915_private *dev_priv)
++{
++	struct i915_power_domains *power_domains = &dev_priv->power_domains;
++	struct i915_power_well *power_well;
++	int i;
++
++	mutex_lock(&power_domains->lock);
++	for_each_power_well(i, power_well, POWER_DOMAIN_MASK, power_domains) {
++		power_well->ops->sync_hw(dev_priv, power_well);
++		power_well->hw_enabled = power_well->ops->is_enabled(dev_priv,
++								     power_well);
++	}
++	mutex_unlock(&power_domains->lock);
++}
++
++static void vlv_cmnlane_wa(struct drm_i915_private *dev_priv)
++{
++	struct i915_power_well *cmn =
++		lookup_power_well(dev_priv, PUNIT_POWER_WELL_DPIO_CMN_BC);
++	struct i915_power_well *disp2d =
++		lookup_power_well(dev_priv, PUNIT_POWER_WELL_DISP2D);
++
++	/* If the display might be already active skip this */
++	if (cmn->ops->is_enabled(dev_priv, cmn) &&
++	    disp2d->ops->is_enabled(dev_priv, disp2d) &&
++	    I915_READ(DPIO_CTL) & DPIO_CMNRST)
++		return;
++
++	DRM_DEBUG_KMS("toggling display PHY side reset\n");
++
++	/* cmnlane needs DPLL registers */
++	disp2d->ops->enable(dev_priv, disp2d);
++
++	/*
++	 * From VLV2A0_DP_eDP_HDMI_DPIO_driver_vbios_notes_11.docx:
++	 * Need to assert and de-assert PHY SB reset by gating the
++	 * common lane power, then un-gating it.
++	 * Simply ungating isn't enough to reset the PHY enough to get
++	 * ports and lanes running.
++	 */
++	cmn->ops->disable(dev_priv, cmn);
++}
++
++/**
++ * intel_power_domains_init_hw - initialize hardware power domain state
++ * @dev_priv: i915 device instance
++ *
++ * This function initializes the hardware power domain state and enables all
++ * power domains using intel_display_set_init_power().
++ */
++void intel_power_domains_init_hw(struct drm_i915_private *dev_priv)
++{
++	struct drm_device *dev = dev_priv->dev;
++	struct i915_power_domains *power_domains = &dev_priv->power_domains;
++
++	power_domains->initializing = true;
++
++	if (IS_VALLEYVIEW(dev) && !IS_CHERRYVIEW(dev)) {
++		mutex_lock(&power_domains->lock);
++		vlv_cmnlane_wa(dev_priv);
++		mutex_unlock(&power_domains->lock);
++	}
++
++	/* For now, we need the power well to be always enabled. */
++	intel_display_set_init_power(dev_priv, true);
++	intel_power_domains_resume(dev_priv);
++	power_domains->initializing = false;
++}
++
++/**
++ * intel_aux_display_runtime_get - grab an auxilliary power domain reference
++ * @dev_priv: i915 device instance
++ *
++ * This function grabs a power domain reference for the auxiliary power domain
++ * (for access to the GMBUS and DP AUX blocks) and ensures that it and all its
++ * parents are powered up. Therefore users should only grab a reference to the
++ * innermost power domain they need.
++ *
++ * Any power domain reference obtained by this function must have a symmetric
++ * call to intel_aux_display_runtime_put() to release the reference again.
++ */
++void intel_aux_display_runtime_get(struct drm_i915_private *dev_priv)
++{
++	intel_runtime_pm_get(dev_priv);
++}
++
++/**
++ * intel_aux_display_runtime_put - release an auxilliary power domain reference
++ * @dev_priv: i915 device instance
++ *
++ * This function drops the auxilliary power domain reference obtained by
++ * intel_aux_display_runtime_get() and might power down the corresponding
++ * hardware block right away if this is the last reference.
++ */
++void intel_aux_display_runtime_put(struct drm_i915_private *dev_priv)
++{
++	intel_runtime_pm_put(dev_priv);
++}
++
++/**
++ * intel_runtime_pm_get - grab a runtime pm reference
++ * @dev_priv: i915 device instance
++ *
++ * This function grabs a device-level runtime pm reference (mostly used for GEM
++ * code to ensure the GTT or GT is on) and ensures that it is powered up.
++ *
++ * Any runtime pm reference obtained by this function must have a symmetric
++ * call to intel_runtime_pm_put() to release the reference again.
++ */
++void intel_runtime_pm_get(struct drm_i915_private *dev_priv)
++{
++	struct drm_device *dev = dev_priv->dev;
++	struct device *device = &dev->pdev->dev;
++
++	if (!HAS_RUNTIME_PM(dev))
++		return;
++
++	pm_runtime_get_sync(device);
++	WARN(dev_priv->pm.suspended, "Device still suspended.\n");
++}
++
++/**
++ * intel_runtime_pm_get_noresume - grab a runtime pm reference
++ * @dev_priv: i915 device instance
++ *
++ * This function grabs a device-level runtime pm reference (mostly used for GEM
++ * code to ensure the GTT or GT is on).
++ *
++ * It will _not_ power up the device but instead only check that it's powered
++ * on.  Therefore it is only valid to call this functions from contexts where
++ * the device is known to be powered up and where trying to power it up would
++ * result in hilarity and deadlocks. That pretty much means only the system
++ * suspend/resume code where this is used to grab runtime pm references for
++ * delayed setup down in work items.
++ *
++ * Any runtime pm reference obtained by this function must have a symmetric
++ * call to intel_runtime_pm_put() to release the reference again.
++ */
++void intel_runtime_pm_get_noresume(struct drm_i915_private *dev_priv)
++{
++	struct drm_device *dev = dev_priv->dev;
++	struct device *device = &dev->pdev->dev;
++
++	if (!HAS_RUNTIME_PM(dev))
++		return;
++
++	WARN(dev_priv->pm.suspended, "Getting nosync-ref while suspended.\n");
++	pm_runtime_get_noresume(device);
++}
++
++/**
++ * intel_runtime_pm_put - release a runtime pm reference
++ * @dev_priv: i915 device instance
++ *
++ * This function drops the device-level runtime pm reference obtained by
++ * intel_runtime_pm_get() and might power down the corresponding
++ * hardware block right away if this is the last reference.
++ */
++void intel_runtime_pm_put(struct drm_i915_private *dev_priv)
++{
++	struct drm_device *dev = dev_priv->dev;
++	struct device *device = &dev->pdev->dev;
++
++	if (!HAS_RUNTIME_PM(dev))
++		return;
++
++	pm_runtime_mark_last_busy(device);
++	pm_runtime_put_autosuspend(device);
++}
++
++/**
++ * intel_runtime_pm_enable - enable runtime pm
++ * @dev_priv: i915 device instance
++ *
++ * This function enables runtime pm at the end of the driver load sequence.
++ *
++ * Note that this function does currently not enable runtime pm for the
++ * subordinate display power domains. That is only done on the first modeset
++ * using intel_display_set_init_power().
++ */
++void intel_runtime_pm_enable(struct drm_i915_private *dev_priv)
++{
++	struct drm_device *dev = dev_priv->dev;
++	struct device *device = &dev->pdev->dev;
++
++	if (!HAS_RUNTIME_PM(dev))
++		return;
++
++	pm_runtime_set_active(device);
++
++	/*
++	 * RPM depends on RC6 to save restore the GT HW context, so make RC6 a
++	 * requirement.
++	 */
++	if (!intel_enable_rc6(dev)) {
++		DRM_INFO("RC6 disabled, disabling runtime PM support\n");
++		return;
++	}
++
++	pm_runtime_set_autosuspend_delay(device, 10000); /* 10s */
++	pm_runtime_mark_last_busy(device);
++	pm_runtime_use_autosuspend(device);
++
++	pm_runtime_put_autosuspend(device);
++}
++
++/* Display audio driver power well request */
++int i915_request_power_well(void)
++{
++	struct drm_i915_private *dev_priv;
++
++	if (!hsw_pwr)
++		return -ENODEV;
++
++	dev_priv = container_of(hsw_pwr, struct drm_i915_private,
++				power_domains);
++	intel_display_power_get(dev_priv, POWER_DOMAIN_AUDIO);
++	return 0;
++}
++EXPORT_SYMBOL_GPL(i915_request_power_well);
++
++/* Display audio driver power well release */
++int i915_release_power_well(void)
++{
++	struct drm_i915_private *dev_priv;
++
++	if (!hsw_pwr)
++		return -ENODEV;
++
++	dev_priv = container_of(hsw_pwr, struct drm_i915_private,
++				power_domains);
++	intel_display_power_put(dev_priv, POWER_DOMAIN_AUDIO);
++	return 0;
++}
++EXPORT_SYMBOL_GPL(i915_release_power_well);
++
++/*
++ * Private interface for the audio driver to get CDCLK in kHz.
++ *
++ * Caller must request power well using i915_request_power_well() prior to
++ * making the call.
++ */
++int i915_get_cdclk_freq(void)
++{
++	struct drm_i915_private *dev_priv;
++
++	if (!hsw_pwr)
++		return -ENODEV;
++
++	dev_priv = container_of(hsw_pwr, struct drm_i915_private,
++				power_domains);
++
++	return intel_ddi_get_cdclk_freq(dev_priv);
++}
++EXPORT_SYMBOL_GPL(i915_get_cdclk_freq);
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_sdvo.c b/drivers/gpu/drm/i915/intel_sdvo.c
+--- a/drivers/gpu/drm/i915/intel_sdvo.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_sdvo.c	2014-11-20 09:53:37.996762837 -0700
+@@ -52,6 +52,8 @@
+ #define IS_DIGITAL(c) (c->output_flag & (SDVO_TMDS_MASK | SDVO_LVDS_MASK))
+ 
+ 
++static void intel_sdvo_get_lvds_modes(struct drm_connector *connector);
++
+ static const char *tv_format_names[] = {
+ 	"NTSC_M"   , "NTSC_J"  , "NTSC_443",
+ 	"PAL_B"    , "PAL_D"   , "PAL_G"   ,
+@@ -773,9 +775,9 @@
+ 	args.height = height;
+ 	args.interlace = 0;
+ 
+-	if (intel_sdvo->is_lvds &&
+-	   (intel_sdvo->sdvo_lvds_fixed_mode->hdisplay != width ||
+-	    intel_sdvo->sdvo_lvds_fixed_mode->vdisplay != height))
++	if (intel_sdvo->sdvo_lvds_fixed_mode &&
++	    (intel_sdvo->sdvo_lvds_fixed_mode->hdisplay != width ||
++	     intel_sdvo->sdvo_lvds_fixed_mode->vdisplay != height))
+ 		args.scaled = 1;
+ 
+ 	return intel_sdvo_set_value(intel_sdvo,
+@@ -1211,7 +1213,7 @@
+ 		return;
+ 
+ 	/* lvds has a special fixed output timing. */
+-	if (intel_sdvo->is_lvds)
++	if (intel_sdvo->sdvo_lvds_fixed_mode)
+ 		intel_sdvo_get_dtd_from_mode(&output_dtd,
+ 					     intel_sdvo->sdvo_lvds_fixed_mode);
+ 	else
+@@ -1564,7 +1566,7 @@
+ 	if (intel_sdvo->pixel_clock_max < mode->clock)
+ 		return MODE_CLOCK_HIGH;
+ 
+-	if (intel_sdvo->is_lvds) {
++	if (intel_sdvo->sdvo_lvds_fixed_mode) {
+ 		if (mode->hdisplay > intel_sdvo->sdvo_lvds_fixed_mode->hdisplay)
+ 			return MODE_PANEL;
+ 
+@@ -1724,6 +1726,17 @@
+ 	return status;
+ }
+ 
++static enum drm_connector_status
++intel_sdvo_lvds_detect(struct drm_connector *connector)
++{
++	struct intel_sdvo *intel_sdvo = intel_attached_sdvo(connector);
++
++	intel_sdvo_get_lvds_modes(connector);
++	return (intel_sdvo->sdvo_lvds_fixed_mode ?
++		connector_status_connected :
++		connector_status_disconnected);
++}
++
+ static bool
+ intel_sdvo_connector_matches_edid(struct intel_sdvo_connector *sdvo,
+ 				  struct edid *edid)
+@@ -1756,11 +1769,13 @@
+ 		      response & 0xff, response >> 8,
+ 		      intel_sdvo_connector->output_flag);
+ 
+-	if (response == 0)
+-		return connector_status_disconnected;
+-
+ 	intel_sdvo->attached_output = response;
+ 
++	/* Discard the fixed mode, LVDS will reattach during detect */
++	if (intel_sdvo->sdvo_lvds_fixed_mode != NULL)
++		drm_mode_destroy(connector->dev,
++				 intel_sdvo->sdvo_lvds_fixed_mode);
++
+ 	intel_sdvo->has_hdmi_monitor = false;
+ 	intel_sdvo->has_hdmi_audio = false;
+ 	intel_sdvo->rgb_quant_range_selectable = false;
+@@ -1769,6 +1784,8 @@
+ 		ret = connector_status_disconnected;
+ 	else if (IS_TMDS(intel_sdvo_connector))
+ 		ret = intel_sdvo_tmds_sink_detect(connector);
++	else if (IS_LVDS(intel_sdvo_connector))
++		ret = intel_sdvo_lvds_detect(connector);
+ 	else {
+ 		struct edid *edid;
+ 
+@@ -1796,7 +1813,7 @@
+ 		if (response & SDVO_TV_MASK)
+ 			intel_sdvo->is_tv = true;
+ 		if (response & SDVO_LVDS_MASK)
+-			intel_sdvo->is_lvds = intel_sdvo->sdvo_lvds_fixed_mode != NULL;
++			intel_sdvo->is_lvds = true;
+ 	}
+ 
+ 	return ret;
+@@ -1966,13 +1983,13 @@
+ 	 */
+ 	intel_ddc_get_modes(connector, &intel_sdvo->ddc);
+ 
+-	list_for_each_entry(newmode, &connector->probed_modes, head) {
+-		if (newmode->type & DRM_MODE_TYPE_PREFERRED) {
+-			intel_sdvo->sdvo_lvds_fixed_mode =
+-				drm_mode_duplicate(connector->dev, newmode);
+-
+-			intel_sdvo->is_lvds = true;
+-			break;
++	if (intel_sdvo->sdvo_lvds_fixed_mode == NULL) {
++		list_for_each_entry(newmode, &connector->probed_modes, head) {
++			if (newmode->type & DRM_MODE_TYPE_PREFERRED) {
++				intel_sdvo->sdvo_lvds_fixed_mode =
++					drm_mode_duplicate(connector->dev, newmode);
++				break;
++			}
+ 		}
+ 	}
+ }
+@@ -1991,57 +2008,10 @@
+ 	return !list_empty(&connector->probed_modes);
+ }
+ 
+-static void
+-intel_sdvo_destroy_enhance_property(struct drm_connector *connector)
+-{
+-	struct intel_sdvo_connector *intel_sdvo_connector = to_intel_sdvo_connector(connector);
+-	struct drm_device *dev = connector->dev;
+-
+-	if (intel_sdvo_connector->left)
+-		drm_property_destroy(dev, intel_sdvo_connector->left);
+-	if (intel_sdvo_connector->right)
+-		drm_property_destroy(dev, intel_sdvo_connector->right);
+-	if (intel_sdvo_connector->top)
+-		drm_property_destroy(dev, intel_sdvo_connector->top);
+-	if (intel_sdvo_connector->bottom)
+-		drm_property_destroy(dev, intel_sdvo_connector->bottom);
+-	if (intel_sdvo_connector->hpos)
+-		drm_property_destroy(dev, intel_sdvo_connector->hpos);
+-	if (intel_sdvo_connector->vpos)
+-		drm_property_destroy(dev, intel_sdvo_connector->vpos);
+-	if (intel_sdvo_connector->saturation)
+-		drm_property_destroy(dev, intel_sdvo_connector->saturation);
+-	if (intel_sdvo_connector->contrast)
+-		drm_property_destroy(dev, intel_sdvo_connector->contrast);
+-	if (intel_sdvo_connector->hue)
+-		drm_property_destroy(dev, intel_sdvo_connector->hue);
+-	if (intel_sdvo_connector->sharpness)
+-		drm_property_destroy(dev, intel_sdvo_connector->sharpness);
+-	if (intel_sdvo_connector->flicker_filter)
+-		drm_property_destroy(dev, intel_sdvo_connector->flicker_filter);
+-	if (intel_sdvo_connector->flicker_filter_2d)
+-		drm_property_destroy(dev, intel_sdvo_connector->flicker_filter_2d);
+-	if (intel_sdvo_connector->flicker_filter_adaptive)
+-		drm_property_destroy(dev, intel_sdvo_connector->flicker_filter_adaptive);
+-	if (intel_sdvo_connector->tv_luma_filter)
+-		drm_property_destroy(dev, intel_sdvo_connector->tv_luma_filter);
+-	if (intel_sdvo_connector->tv_chroma_filter)
+-		drm_property_destroy(dev, intel_sdvo_connector->tv_chroma_filter);
+-	if (intel_sdvo_connector->dot_crawl)
+-		drm_property_destroy(dev, intel_sdvo_connector->dot_crawl);
+-	if (intel_sdvo_connector->brightness)
+-		drm_property_destroy(dev, intel_sdvo_connector->brightness);
+-}
+-
+ static void intel_sdvo_destroy(struct drm_connector *connector)
+ {
+ 	struct intel_sdvo_connector *intel_sdvo_connector = to_intel_sdvo_connector(connector);
+ 
+-	if (intel_sdvo_connector->tv_format)
+-		drm_property_destroy(connector->dev,
+-				     intel_sdvo_connector->tv_format);
+-
+-	intel_sdvo_destroy_enhance_property(connector);
+ 	drm_connector_cleanup(connector);
+ 	kfree(intel_sdvo_connector);
+ }
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_sprite.c b/drivers/gpu/drm/i915/intel_sprite.c
+--- a/drivers/gpu/drm/i915/intel_sprite.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_sprite.c	2014-11-20 09:53:38.000762837 -0700
+@@ -37,6 +37,20 @@
+ #include <drm/i915_drm.h>
+ #include "i915_drv.h"
+ 
++static bool
++format_is_yuv(uint32_t format)
++{
++	switch (format) {
++	case DRM_FORMAT_YUYV:
++	case DRM_FORMAT_UYVY:
++	case DRM_FORMAT_VYUY:
++	case DRM_FORMAT_YVYU:
++		return true;
++	default:
++		return false;
++	}
++}
++
+ static int usecs_to_scanlines(const struct drm_display_mode *mode, int usecs)
+ {
+ 	/* paranoia */
+@@ -46,17 +60,32 @@
+ 	return DIV_ROUND_UP(usecs * mode->crtc_clock, 1000 * mode->crtc_htotal);
+ }
+ 
+-static bool intel_pipe_update_start(struct intel_crtc *crtc, uint32_t *start_vbl_count)
++/**
++ * intel_pipe_update_start() - start update of a set of display registers
++ * @crtc: the crtc of which the registers are going to be updated
++ * @start_vbl_count: vblank counter return pointer used for error checking
++ *
++ * Mark the start of an update to pipe registers that should be updated
++ * atomically regarding vblank. If the next vblank will happens within
++ * the next 100 us, this function waits until the vblank passes.
++ *
++ * After a successful call to this function, interrupts will be disabled
++ * until a subsequent call to intel_pipe_update_end(). That is done to
++ * avoid random delays. The value written to @start_vbl_count should be
++ * supplied to intel_pipe_update_end() for error checking.
++ *
++ * Return: true if the call was successful
++ */
++bool intel_pipe_update_start(struct intel_crtc *crtc, uint32_t *start_vbl_count)
+ {
+ 	struct drm_device *dev = crtc->base.dev;
+ 	const struct drm_display_mode *mode = &crtc->config.adjusted_mode;
+ 	enum pipe pipe = crtc->pipe;
+ 	long timeout = msecs_to_jiffies_timeout(1);
+ 	int scanline, min, max, vblank_start;
++	wait_queue_head_t *wq = drm_crtc_vblank_waitqueue(&crtc->base);
+ 	DEFINE_WAIT(wait);
+ 
+-	WARN_ON(!drm_modeset_is_locked(&crtc->base.mutex));
+-
+ 	vblank_start = mode->crtc_vblank_start;
+ 	if (mode->flags & DRM_MODE_FLAG_INTERLACE)
+ 		vblank_start = DIV_ROUND_UP(vblank_start, 2);
+@@ -81,7 +110,7 @@
+ 		 * other CPUs can see the task state update by the time we
+ 		 * read the scanline.
+ 		 */
+-		prepare_to_wait(&crtc->vbl_wait, &wait, TASK_UNINTERRUPTIBLE);
++		prepare_to_wait(wq, &wait, TASK_UNINTERRUPTIBLE);
+ 
+ 		scanline = intel_get_crtc_scanline(crtc);
+ 		if (scanline < min || scanline > max)
+@@ -100,7 +129,7 @@
+ 		local_irq_disable();
+ 	}
+ 
+-	finish_wait(&crtc->vbl_wait, &wait);
++	finish_wait(wq, &wait);
+ 
+ 	drm_vblank_put(dev, pipe);
+ 
+@@ -111,7 +140,16 @@
+ 	return true;
+ }
+ 
+-static void intel_pipe_update_end(struct intel_crtc *crtc, u32 start_vbl_count)
++/**
++ * intel_pipe_update_end() - end update of a set of display registers
++ * @crtc: the crtc of which the registers were updated
++ * @start_vbl_count: start vblank counter (used for error checking)
++ *
++ * Mark the end of an update started with intel_pipe_update_start(). This
++ * re-enables interrupts and verifies the update was actually completed
++ * before a vblank using the value of @start_vbl_count.
++ */
++void intel_pipe_update_end(struct intel_crtc *crtc, u32 start_vbl_count)
+ {
+ 	struct drm_device *dev = crtc->base.dev;
+ 	enum pipe pipe = crtc->pipe;
+@@ -138,6 +176,226 @@
+ }
+ 
+ static void
++skl_update_plane(struct drm_plane *drm_plane, struct drm_crtc *crtc,
++		 struct drm_framebuffer *fb,
++		 struct drm_i915_gem_object *obj, int crtc_x, int crtc_y,
++		 unsigned int crtc_w, unsigned int crtc_h,
++		 uint32_t x, uint32_t y,
++		 uint32_t src_w, uint32_t src_h)
++{
++	struct drm_device *dev = drm_plane->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_plane *intel_plane = to_intel_plane(drm_plane);
++	const int pipe = intel_plane->pipe;
++	const int plane = intel_plane->plane + 1;
++	u32 plane_ctl, stride;
++	int pixel_size = drm_format_plane_cpp(fb->pixel_format, 0);
++
++	plane_ctl = I915_READ(PLANE_CTL(pipe, plane));
++
++	/* Mask out pixel format bits in case we change it */
++	plane_ctl &= ~PLANE_CTL_FORMAT_MASK;
++	plane_ctl &= ~PLANE_CTL_ORDER_RGBX;
++	plane_ctl &= ~PLANE_CTL_YUV422_ORDER_MASK;
++	plane_ctl &= ~PLANE_CTL_TILED_MASK;
++	plane_ctl &= ~PLANE_CTL_ALPHA_MASK;
++	plane_ctl &= ~PLANE_CTL_ROTATE_MASK;
++
++	/* Trickle feed has to be enabled */
++	plane_ctl &= ~PLANE_CTL_TRICKLE_FEED_DISABLE;
++
++	switch (fb->pixel_format) {
++	case DRM_FORMAT_RGB565:
++		plane_ctl |= PLANE_CTL_FORMAT_RGB_565;
++		break;
++	case DRM_FORMAT_XBGR8888:
++		plane_ctl |= PLANE_CTL_FORMAT_XRGB_8888 | PLANE_CTL_ORDER_RGBX;
++		break;
++	case DRM_FORMAT_XRGB8888:
++		plane_ctl |= PLANE_CTL_FORMAT_XRGB_8888;
++		break;
++	/*
++	 * XXX: For ARBG/ABGR formats we default to expecting scanout buffers
++	 * to be already pre-multiplied. We need to add a knob (or a different
++	 * DRM_FORMAT) for user-space to configure that.
++	 */
++	case DRM_FORMAT_ABGR8888:
++		plane_ctl |= PLANE_CTL_FORMAT_XRGB_8888 |
++			     PLANE_CTL_ORDER_RGBX |
++			     PLANE_CTL_ALPHA_SW_PREMULTIPLY;
++		break;
++	case DRM_FORMAT_ARGB8888:
++		plane_ctl |= PLANE_CTL_FORMAT_XRGB_8888 |
++			     PLANE_CTL_ALPHA_SW_PREMULTIPLY;
++		break;
++	case DRM_FORMAT_YUYV:
++		plane_ctl |= PLANE_CTL_FORMAT_YUV422 | PLANE_CTL_YUV422_YUYV;
++		break;
++	case DRM_FORMAT_YVYU:
++		plane_ctl |= PLANE_CTL_FORMAT_YUV422 | PLANE_CTL_YUV422_YVYU;
++		break;
++	case DRM_FORMAT_UYVY:
++		plane_ctl |= PLANE_CTL_FORMAT_YUV422 | PLANE_CTL_YUV422_UYVY;
++		break;
++	case DRM_FORMAT_VYUY:
++		plane_ctl |= PLANE_CTL_FORMAT_YUV422 | PLANE_CTL_YUV422_VYUY;
++		break;
++	default:
++		BUG();
++	}
++
++	switch (obj->tiling_mode) {
++	case I915_TILING_NONE:
++		stride = fb->pitches[0] >> 6;
++		break;
++	case I915_TILING_X:
++		plane_ctl |= PLANE_CTL_TILED_X;
++		stride = fb->pitches[0] >> 9;
++		break;
++	default:
++		BUG();
++	}
++	if (intel_plane->rotation == BIT(DRM_ROTATE_180))
++		plane_ctl |= PLANE_CTL_ROTATE_180;
++
++	plane_ctl |= PLANE_CTL_ENABLE;
++	plane_ctl |= PLANE_CTL_PIPE_CSC_ENABLE;
++
++	intel_update_sprite_watermarks(drm_plane, crtc, src_w, src_h,
++				       pixel_size, true,
++				       src_w != crtc_w || src_h != crtc_h);
++
++	/* Sizes are 0 based */
++	src_w--;
++	src_h--;
++	crtc_w--;
++	crtc_h--;
++
++	I915_WRITE(PLANE_OFFSET(pipe, plane), (y << 16) | x);
++	I915_WRITE(PLANE_STRIDE(pipe, plane), stride);
++	I915_WRITE(PLANE_POS(pipe, plane), (crtc_y << 16) | crtc_x);
++	I915_WRITE(PLANE_SIZE(pipe, plane), (crtc_h << 16) | crtc_w);
++	I915_WRITE(PLANE_CTL(pipe, plane), plane_ctl);
++	I915_WRITE(PLANE_SURF(pipe, plane), i915_gem_obj_ggtt_offset(obj));
++	POSTING_READ(PLANE_SURF(pipe, plane));
++}
++
++static void
++skl_disable_plane(struct drm_plane *drm_plane, struct drm_crtc *crtc)
++{
++	struct drm_device *dev = drm_plane->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_plane *intel_plane = to_intel_plane(drm_plane);
++	const int pipe = intel_plane->pipe;
++	const int plane = intel_plane->plane + 1;
++
++	I915_WRITE(PLANE_CTL(pipe, plane),
++		   I915_READ(PLANE_CTL(pipe, plane)) & ~PLANE_CTL_ENABLE);
++
++	/* Activate double buffered register update */
++	I915_WRITE(PLANE_CTL(pipe, plane), 0);
++	POSTING_READ(PLANE_CTL(pipe, plane));
++
++	intel_update_sprite_watermarks(drm_plane, crtc, 0, 0, 0, false, false);
++}
++
++static int
++skl_update_colorkey(struct drm_plane *drm_plane,
++		    struct drm_intel_sprite_colorkey *key)
++{
++	struct drm_device *dev = drm_plane->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_plane *intel_plane = to_intel_plane(drm_plane);
++	const int pipe = intel_plane->pipe;
++	const int plane = intel_plane->plane;
++	u32 plane_ctl;
++
++	I915_WRITE(PLANE_KEYVAL(pipe, plane), key->min_value);
++	I915_WRITE(PLANE_KEYMAX(pipe, plane), key->max_value);
++	I915_WRITE(PLANE_KEYMSK(pipe, plane), key->channel_mask);
++
++	plane_ctl = I915_READ(PLANE_CTL(pipe, plane));
++	plane_ctl &= ~PLANE_CTL_KEY_ENABLE_MASK;
++	if (key->flags & I915_SET_COLORKEY_DESTINATION)
++		plane_ctl |= PLANE_CTL_KEY_ENABLE_DESTINATION;
++	else if (key->flags & I915_SET_COLORKEY_SOURCE)
++		plane_ctl |= PLANE_CTL_KEY_ENABLE_SOURCE;
++	I915_WRITE(PLANE_CTL(pipe, plane), plane_ctl);
++
++	POSTING_READ(PLANE_CTL(pipe, plane));
++
++	return 0;
++}
++
++static void
++skl_get_colorkey(struct drm_plane *drm_plane,
++		 struct drm_intel_sprite_colorkey *key)
++{
++	struct drm_device *dev = drm_plane->dev;
++	struct drm_i915_private *dev_priv = dev->dev_private;
++	struct intel_plane *intel_plane = to_intel_plane(drm_plane);
++	const int pipe = intel_plane->pipe;
++	const int plane = intel_plane->plane;
++	u32 plane_ctl;
++
++	key->min_value = I915_READ(PLANE_KEYVAL(pipe, plane));
++	key->max_value = I915_READ(PLANE_KEYMAX(pipe, plane));
++	key->channel_mask = I915_READ(PLANE_KEYMSK(pipe, plane));
++
++	plane_ctl = I915_READ(PLANE_CTL(pipe, plane));
++
++	switch (plane_ctl & PLANE_CTL_KEY_ENABLE_MASK) {
++	case PLANE_CTL_KEY_ENABLE_DESTINATION:
++		key->flags = I915_SET_COLORKEY_DESTINATION;
++		break;
++	case PLANE_CTL_KEY_ENABLE_SOURCE:
++		key->flags = I915_SET_COLORKEY_SOURCE;
++		break;
++	default:
++		key->flags = I915_SET_COLORKEY_NONE;
++	}
++}
++
++static void
++chv_update_csc(struct intel_plane *intel_plane, uint32_t format)
++{
++	struct drm_i915_private *dev_priv = intel_plane->base.dev->dev_private;
++	int plane = intel_plane->plane;
++
++	/* Seems RGB data bypasses the CSC always */
++	if (!format_is_yuv(format))
++		return;
++
++	/*
++	 * BT.601 limited range YCbCr -> full range RGB
++	 *
++	 * |r|   | 6537 4769     0|   |cr  |
++	 * |g| = |-3330 4769 -1605| x |y-64|
++	 * |b|   |    0 4769  8263|   |cb  |
++	 *
++	 * Cb and Cr apparently come in as signed already, so no
++	 * need for any offset. For Y we need to remove the offset.
++	 */
++	I915_WRITE(SPCSCYGOFF(plane), SPCSC_OOFF(0) | SPCSC_IOFF(-64));
++	I915_WRITE(SPCSCCBOFF(plane), SPCSC_OOFF(0) | SPCSC_IOFF(0));
++	I915_WRITE(SPCSCCROFF(plane), SPCSC_OOFF(0) | SPCSC_IOFF(0));
++
++	I915_WRITE(SPCSCC01(plane), SPCSC_C1(4769) | SPCSC_C0(6537));
++	I915_WRITE(SPCSCC23(plane), SPCSC_C1(-3330) | SPCSC_C0(0));
++	I915_WRITE(SPCSCC45(plane), SPCSC_C1(-1605) | SPCSC_C0(4769));
++	I915_WRITE(SPCSCC67(plane), SPCSC_C1(4769) | SPCSC_C0(0));
++	I915_WRITE(SPCSCC8(plane), SPCSC_C0(8263));
++
++	I915_WRITE(SPCSCYGICLAMP(plane), SPCSC_IMAX(940) | SPCSC_IMIN(64));
++	I915_WRITE(SPCSCCBICLAMP(plane), SPCSC_IMAX(448) | SPCSC_IMIN(-448));
++	I915_WRITE(SPCSCCRICLAMP(plane), SPCSC_IMAX(448) | SPCSC_IMIN(-448));
++
++	I915_WRITE(SPCSCYGOCLAMP(plane), SPCSC_OMAX(1023) | SPCSC_OMIN(0));
++	I915_WRITE(SPCSCCBOCLAMP(plane), SPCSC_OMAX(1023) | SPCSC_OMIN(0));
++	I915_WRITE(SPCSCCROCLAMP(plane), SPCSC_OMAX(1023) | SPCSC_OMIN(0));
++}
++
++static void
+ vlv_update_plane(struct drm_plane *dplane, struct drm_crtc *crtc,
+ 		 struct drm_framebuffer *fb,
+ 		 struct drm_i915_gem_object *obj, int crtc_x, int crtc_y,
+@@ -163,6 +421,7 @@
+ 	sprctl &= ~SP_PIXFORMAT_MASK;
+ 	sprctl &= ~SP_YUV_BYTE_ORDER_MASK;
+ 	sprctl &= ~SP_TILED;
++	sprctl &= ~SP_ROTATE_180;
+ 
+ 	switch (fb->pixel_format) {
+ 	case DRM_FORMAT_YUYV:
+@@ -235,10 +494,21 @@
+ 							fb->pitches[0]);
+ 	linear_offset -= sprsurf_offset;
+ 
++	if (intel_plane->rotation == BIT(DRM_ROTATE_180)) {
++		sprctl |= SP_ROTATE_180;
++
++		x += src_w;
++		y += src_h;
++		linear_offset += src_h * fb->pitches[0] + src_w * pixel_size;
++	}
++
+ 	atomic_update = intel_pipe_update_start(intel_crtc, &start_vbl_count);
+ 
+ 	intel_update_primary_plane(intel_crtc);
+ 
++	if (IS_CHERRYVIEW(dev) && pipe == PIPE_B)
++		chv_update_csc(intel_plane, fb->pixel_format);
++
+ 	I915_WRITE(SPSTRIDE(pipe, plane), fb->pitches[0]);
+ 	I915_WRITE(SPPOS(pipe, plane), (crtc_y << 16) | crtc_x);
+ 
+@@ -247,6 +517,8 @@
+ 	else
+ 		I915_WRITE(SPLINOFF(pipe, plane), linear_offset);
+ 
++	I915_WRITE(SPCONSTALPHA(pipe, plane), 0);
++
+ 	I915_WRITE(SPSIZE(pipe, plane), (crtc_h << 16) | crtc_w);
+ 	I915_WRITE(SPCNTR(pipe, plane), sprctl);
+ 	I915_WRITE(SPSURF(pipe, plane), i915_gem_obj_ggtt_offset(obj) +
+@@ -364,6 +636,7 @@
+ 	sprctl &= ~SPRITE_RGB_ORDER_RGBX;
+ 	sprctl &= ~SPRITE_YUV_BYTE_ORDER_MASK;
+ 	sprctl &= ~SPRITE_TILED;
++	sprctl &= ~SPRITE_ROTATE_180;
+ 
+ 	switch (fb->pixel_format) {
+ 	case DRM_FORMAT_XBGR8888:
+@@ -426,6 +699,18 @@
+ 					       pixel_size, fb->pitches[0]);
+ 	linear_offset -= sprsurf_offset;
+ 
++	if (intel_plane->rotation == BIT(DRM_ROTATE_180)) {
++		sprctl |= SPRITE_ROTATE_180;
++
++		/* HSW and BDW does this automagically in hardware */
++		if (!IS_HASWELL(dev) && !IS_BROADWELL(dev)) {
++			x += src_w;
++			y += src_h;
++			linear_offset += src_h * fb->pitches[0] +
++				src_w * pixel_size;
++		}
++	}
++
+ 	atomic_update = intel_pipe_update_start(intel_crtc, &start_vbl_count);
+ 
+ 	intel_update_primary_plane(intel_crtc);
+@@ -571,6 +856,7 @@
+ 	dvscntr &= ~DVS_RGB_ORDER_XBGR;
+ 	dvscntr &= ~DVS_YUV_BYTE_ORDER_MASK;
+ 	dvscntr &= ~DVS_TILED;
++	dvscntr &= ~DVS_ROTATE_180;
+ 
+ 	switch (fb->pixel_format) {
+ 	case DRM_FORMAT_XBGR8888:
+@@ -628,6 +914,14 @@
+ 					       pixel_size, fb->pitches[0]);
+ 	linear_offset -= dvssurf_offset;
+ 
++	if (intel_plane->rotation == BIT(DRM_ROTATE_180)) {
++		dvscntr |= DVS_ROTATE_180;
++
++		x += src_w;
++		y += src_h;
++		linear_offset += src_h * fb->pitches[0] + src_w * pixel_size;
++	}
++
+ 	atomic_update = intel_pipe_update_start(intel_crtc, &start_vbl_count);
+ 
+ 	intel_update_primary_plane(intel_crtc);
+@@ -789,20 +1083,6 @@
+ 		key->flags = I915_SET_COLORKEY_NONE;
+ }
+ 
+-static bool
+-format_is_yuv(uint32_t format)
+-{
+-	switch (format) {
+-	case DRM_FORMAT_YUYV:
+-	case DRM_FORMAT_UYVY:
+-	case DRM_FORMAT_VYUY:
+-	case DRM_FORMAT_YVYU:
+-		return true;
+-	default:
+-		return false;
+-	}
+-}
+-
+ static bool colorkey_enabled(struct intel_plane *intel_plane)
+ {
+ 	struct drm_intel_sprite_colorkey key;
+@@ -813,57 +1093,23 @@
+ }
+ 
+ static int
+-intel_update_plane(struct drm_plane *plane, struct drm_crtc *crtc,
+-		   struct drm_framebuffer *fb, int crtc_x, int crtc_y,
+-		   unsigned int crtc_w, unsigned int crtc_h,
+-		   uint32_t src_x, uint32_t src_y,
+-		   uint32_t src_w, uint32_t src_h)
++intel_check_sprite_plane(struct drm_plane *plane,
++			 struct intel_plane_state *state)
+ {
+-	struct drm_device *dev = plane->dev;
+-	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	struct intel_crtc *intel_crtc = to_intel_crtc(state->crtc);
+ 	struct intel_plane *intel_plane = to_intel_plane(plane);
+-	enum pipe pipe = intel_crtc->pipe;
+-	struct intel_framebuffer *intel_fb = to_intel_framebuffer(fb);
+-	struct drm_i915_gem_object *obj = intel_fb->obj;
+-	struct drm_i915_gem_object *old_obj = intel_plane->obj;
+-	int ret;
+-	bool primary_enabled;
+-	bool visible;
++	struct drm_framebuffer *fb = state->fb;
++	struct drm_i915_gem_object *obj = intel_fb_obj(fb);
++	int crtc_x, crtc_y;
++	unsigned int crtc_w, crtc_h;
++	uint32_t src_x, src_y, src_w, src_h;
++	struct drm_rect *src = &state->src;
++	struct drm_rect *dst = &state->dst;
++	struct drm_rect *orig_src = &state->orig_src;
++	const struct drm_rect *clip = &state->clip;
+ 	int hscale, vscale;
+ 	int max_scale, min_scale;
+ 	int pixel_size = drm_format_plane_cpp(fb->pixel_format, 0);
+-	struct drm_rect src = {
+-		/* sample coordinates in 16.16 fixed point */
+-		.x1 = src_x,
+-		.x2 = src_x + src_w,
+-		.y1 = src_y,
+-		.y2 = src_y + src_h,
+-	};
+-	struct drm_rect dst = {
+-		/* integer pixels */
+-		.x1 = crtc_x,
+-		.x2 = crtc_x + crtc_w,
+-		.y1 = crtc_y,
+-		.y2 = crtc_y + crtc_h,
+-	};
+-	const struct drm_rect clip = {
+-		.x2 = intel_crtc->active ? intel_crtc->config.pipe_src_w : 0,
+-		.y2 = intel_crtc->active ? intel_crtc->config.pipe_src_h : 0,
+-	};
+-	const struct {
+-		int crtc_x, crtc_y;
+-		unsigned int crtc_w, crtc_h;
+-		uint32_t src_x, src_y, src_w, src_h;
+-	} orig = {
+-		.crtc_x = crtc_x,
+-		.crtc_y = crtc_y,
+-		.crtc_w = crtc_w,
+-		.crtc_h = crtc_h,
+-		.src_x = src_x,
+-		.src_y = src_y,
+-		.src_w = src_w,
+-		.src_h = src_h,
+-	};
+ 
+ 	/* Don't modify another pipe's plane */
+ 	if (intel_plane->pipe != intel_crtc->pipe) {
+@@ -895,49 +1141,55 @@
+ 	max_scale = intel_plane->max_downscale << 16;
+ 	min_scale = intel_plane->can_scale ? 1 : (1 << 16);
+ 
+-	hscale = drm_rect_calc_hscale_relaxed(&src, &dst, min_scale, max_scale);
++	drm_rect_rotate(src, fb->width << 16, fb->height << 16,
++			intel_plane->rotation);
++
++	hscale = drm_rect_calc_hscale_relaxed(src, dst, min_scale, max_scale);
+ 	BUG_ON(hscale < 0);
+ 
+-	vscale = drm_rect_calc_vscale_relaxed(&src, &dst, min_scale, max_scale);
++	vscale = drm_rect_calc_vscale_relaxed(src, dst, min_scale, max_scale);
+ 	BUG_ON(vscale < 0);
+ 
+-	visible = drm_rect_clip_scaled(&src, &dst, &clip, hscale, vscale);
++	state->visible =  drm_rect_clip_scaled(src, dst, clip, hscale, vscale);
+ 
+-	crtc_x = dst.x1;
+-	crtc_y = dst.y1;
+-	crtc_w = drm_rect_width(&dst);
+-	crtc_h = drm_rect_height(&dst);
++	crtc_x = dst->x1;
++	crtc_y = dst->y1;
++	crtc_w = drm_rect_width(dst);
++	crtc_h = drm_rect_height(dst);
+ 
+-	if (visible) {
++	if (state->visible) {
+ 		/* check again in case clipping clamped the results */
+-		hscale = drm_rect_calc_hscale(&src, &dst, min_scale, max_scale);
++		hscale = drm_rect_calc_hscale(src, dst, min_scale, max_scale);
+ 		if (hscale < 0) {
+ 			DRM_DEBUG_KMS("Horizontal scaling factor out of limits\n");
+-			drm_rect_debug_print(&src, true);
+-			drm_rect_debug_print(&dst, false);
++			drm_rect_debug_print(src, true);
++			drm_rect_debug_print(dst, false);
+ 
+ 			return hscale;
+ 		}
+ 
+-		vscale = drm_rect_calc_vscale(&src, &dst, min_scale, max_scale);
++		vscale = drm_rect_calc_vscale(src, dst, min_scale, max_scale);
+ 		if (vscale < 0) {
+ 			DRM_DEBUG_KMS("Vertical scaling factor out of limits\n");
+-			drm_rect_debug_print(&src, true);
+-			drm_rect_debug_print(&dst, false);
++			drm_rect_debug_print(src, true);
++			drm_rect_debug_print(dst, false);
+ 
+ 			return vscale;
+ 		}
+ 
+ 		/* Make the source viewport size an exact multiple of the scaling factors. */
+-		drm_rect_adjust_size(&src,
+-				     drm_rect_width(&dst) * hscale - drm_rect_width(&src),
+-				     drm_rect_height(&dst) * vscale - drm_rect_height(&src));
++		drm_rect_adjust_size(src,
++				     drm_rect_width(dst) * hscale - drm_rect_width(src),
++				     drm_rect_height(dst) * vscale - drm_rect_height(src));
++
++		drm_rect_rotate_inv(src, fb->width << 16, fb->height << 16,
++				    intel_plane->rotation);
+ 
+ 		/* sanity check to make sure the src viewport wasn't enlarged */
+-		WARN_ON(src.x1 < (int) src_x ||
+-			src.y1 < (int) src_y ||
+-			src.x2 > (int) (src_x + src_w) ||
+-			src.y2 > (int) (src_y + src_h));
++		WARN_ON(src->x1 < (int) orig_src->x1 ||
++			src->y1 < (int) orig_src->y1 ||
++			src->x2 > (int) orig_src->x2 ||
++			src->y2 > (int) orig_src->y2);
+ 
+ 		/*
+ 		 * Hardware doesn't handle subpixel coordinates.
+@@ -945,10 +1197,10 @@
+ 		 * increase the source viewport size, because that could
+ 		 * push the downscaling factor out of bounds.
+ 		 */
+-		src_x = src.x1 >> 16;
+-		src_w = drm_rect_width(&src) >> 16;
+-		src_y = src.y1 >> 16;
+-		src_h = drm_rect_height(&src) >> 16;
++		src_x = src->x1 >> 16;
++		src_w = drm_rect_width(src) >> 16;
++		src_y = src->y1 >> 16;
++		src_h = drm_rect_height(src) >> 16;
+ 
+ 		if (format_is_yuv(fb->pixel_format)) {
+ 			src_x &= ~1;
+@@ -962,12 +1214,12 @@
+ 				crtc_w &= ~1;
+ 
+ 			if (crtc_w == 0)
+-				visible = false;
++				state->visible = false;
+ 		}
+ 	}
+ 
+ 	/* Check size restrictions when scaling */
+-	if (visible && (src_w != crtc_w || src_h != crtc_h)) {
++	if (state->visible && (src_w != crtc_w || src_h != crtc_h)) {
+ 		unsigned int width_bytes;
+ 
+ 		WARN_ON(!intel_plane->can_scale);
+@@ -975,12 +1227,13 @@
+ 		/* FIXME interlacing min height is 6 */
+ 
+ 		if (crtc_w < 3 || crtc_h < 3)
+-			visible = false;
++			state->visible = false;
+ 
+ 		if (src_w < 3 || src_h < 3)
+-			visible = false;
++			state->visible = false;
+ 
+-		width_bytes = ((src_x * pixel_size) & 63) + src_w * pixel_size;
++		width_bytes = ((src_x * pixel_size) & 63) +
++					src_w * pixel_size;
+ 
+ 		if (src_w > 2048 || src_h > 2048 ||
+ 		    width_bytes > 4096 || fb->pitches[0] > 4096) {
+@@ -989,42 +1242,90 @@
+ 		}
+ 	}
+ 
+-	dst.x1 = crtc_x;
+-	dst.x2 = crtc_x + crtc_w;
+-	dst.y1 = crtc_y;
+-	dst.y2 = crtc_y + crtc_h;
++	if (state->visible) {
++		src->x1 = src_x;
++		src->x2 = src_x + src_w;
++		src->y1 = src_y;
++		src->y2 = src_y + src_h;
++	}
++
++	dst->x1 = crtc_x;
++	dst->x2 = crtc_x + crtc_w;
++	dst->y1 = crtc_y;
++	dst->y2 = crtc_y + crtc_h;
+ 
+-	/*
+-	 * If the sprite is completely covering the primary plane,
+-	 * we can disable the primary and save power.
+-	 */
+-	primary_enabled = !drm_rect_equals(&dst, &clip) || colorkey_enabled(intel_plane);
+-	WARN_ON(!primary_enabled && !visible && intel_crtc->active);
++	return 0;
++}
+ 
+-	mutex_lock(&dev->struct_mutex);
++static int
++intel_prepare_sprite_plane(struct drm_plane *plane,
++			   struct intel_plane_state *state)
++{
++	struct drm_device *dev = plane->dev;
++	struct drm_crtc *crtc = state->crtc;
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	struct intel_plane *intel_plane = to_intel_plane(plane);
++	enum pipe pipe = intel_crtc->pipe;
++	struct drm_framebuffer *fb = state->fb;
++	struct drm_i915_gem_object *obj = intel_fb_obj(fb);
++	struct drm_i915_gem_object *old_obj = intel_plane->obj;
++	int ret;
+ 
+-	/* Note that this will apply the VT-d workaround for scanouts,
+-	 * which is more restrictive than required for sprites. (The
+-	 * primary plane requires 256KiB alignment with 64 PTE padding,
+-	 * the sprite planes only require 128KiB alignment and 32 PTE padding.
+-	 */
+-	ret = intel_pin_and_fence_fb_obj(dev, obj, NULL);
++	if (old_obj != obj) {
++		mutex_lock(&dev->struct_mutex);
+ 
+-	i915_gem_track_fb(old_obj, obj,
+-			  INTEL_FRONTBUFFER_SPRITE(pipe));
+-	mutex_unlock(&dev->struct_mutex);
++		/* Note that this will apply the VT-d workaround for scanouts,
++		 * which is more restrictive than required for sprites. (The
++		 * primary plane requires 256KiB alignment with 64 PTE padding,
++		 * the sprite planes only require 128KiB alignment and 32 PTE
++		 * padding.
++		 */
++		ret = intel_pin_and_fence_fb_obj(plane, fb, NULL);
++		if (ret == 0)
++			i915_gem_track_fb(old_obj, obj,
++					  INTEL_FRONTBUFFER_SPRITE(pipe));
++		mutex_unlock(&dev->struct_mutex);
++		if (ret)
++			return ret;
++	}
+ 
+-	if (ret)
+-		return ret;
++	return 0;
++}
+ 
+-	intel_plane->crtc_x = orig.crtc_x;
+-	intel_plane->crtc_y = orig.crtc_y;
+-	intel_plane->crtc_w = orig.crtc_w;
+-	intel_plane->crtc_h = orig.crtc_h;
+-	intel_plane->src_x = orig.src_x;
+-	intel_plane->src_y = orig.src_y;
+-	intel_plane->src_w = orig.src_w;
+-	intel_plane->src_h = orig.src_h;
++static void
++intel_commit_sprite_plane(struct drm_plane *plane,
++			  struct intel_plane_state *state)
++{
++	struct drm_device *dev = plane->dev;
++	struct drm_crtc *crtc = state->crtc;
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	struct intel_plane *intel_plane = to_intel_plane(plane);
++	enum pipe pipe = intel_crtc->pipe;
++	struct drm_framebuffer *fb = state->fb;
++	struct drm_i915_gem_object *obj = intel_fb_obj(fb);
++	struct drm_i915_gem_object *old_obj = intel_plane->obj;
++	int crtc_x, crtc_y;
++	unsigned int crtc_w, crtc_h;
++	uint32_t src_x, src_y, src_w, src_h;
++	struct drm_rect *dst = &state->dst;
++	const struct drm_rect *clip = &state->clip;
++	bool primary_enabled;
++
++	/*
++	 * If the sprite is completely covering the primary plane,
++	 * we can disable the primary and save power.
++	 */
++	primary_enabled = !drm_rect_equals(dst, clip) || colorkey_enabled(intel_plane);
++	WARN_ON(!primary_enabled && !state->visible && intel_crtc->active);
++
++	intel_plane->crtc_x = state->orig_dst.x1;
++	intel_plane->crtc_y = state->orig_dst.y1;
++	intel_plane->crtc_w = drm_rect_width(&state->orig_dst);
++	intel_plane->crtc_h = drm_rect_height(&state->orig_dst);
++	intel_plane->src_x = state->orig_src.x1;
++	intel_plane->src_y = state->orig_src.y1;
++	intel_plane->src_w = drm_rect_width(&state->orig_src);
++	intel_plane->src_h = drm_rect_height(&state->orig_src);
+ 	intel_plane->obj = obj;
+ 
+ 	if (intel_crtc->active) {
+@@ -1038,12 +1339,22 @@
+ 		if (primary_was_enabled && !primary_enabled)
+ 			intel_pre_disable_primary(crtc);
+ 
+-		if (visible)
++		if (state->visible) {
++			crtc_x = state->dst.x1;
++			crtc_y = state->dst.y1;
++			crtc_w = drm_rect_width(&state->dst);
++			crtc_h = drm_rect_height(&state->dst);
++			src_x = state->src.x1;
++			src_y = state->src.y1;
++			src_w = drm_rect_width(&state->src);
++			src_h = drm_rect_height(&state->src);
+ 			intel_plane->update_plane(plane, crtc, fb, obj,
+ 						  crtc_x, crtc_y, crtc_w, crtc_h,
+ 						  src_x, src_y, src_w, src_h);
+-		else
++		} else {
+ 			intel_plane->disable_plane(plane, crtc);
++		}
++
+ 
+ 		intel_frontbuffer_flip(dev, INTEL_FRONTBUFFER_SPRITE(pipe));
+ 
+@@ -1052,21 +1363,65 @@
+ 	}
+ 
+ 	/* Unpin old obj after new one is active to avoid ugliness */
+-	if (old_obj) {
++	if (old_obj && old_obj != obj) {
++
+ 		/*
+ 		 * It's fairly common to simply update the position of
+ 		 * an existing object.  In that case, we don't need to
+ 		 * wait for vblank to avoid ugliness, we only need to
+ 		 * do the pin & ref bookkeeping.
+ 		 */
+-		if (old_obj != obj && intel_crtc->active)
++		if (intel_crtc->active)
+ 			intel_wait_for_vblank(dev, intel_crtc->pipe);
+ 
+ 		mutex_lock(&dev->struct_mutex);
+ 		intel_unpin_fb_obj(old_obj);
+ 		mutex_unlock(&dev->struct_mutex);
+ 	}
++}
++
++static int
++intel_update_plane(struct drm_plane *plane, struct drm_crtc *crtc,
++		   struct drm_framebuffer *fb, int crtc_x, int crtc_y,
++		   unsigned int crtc_w, unsigned int crtc_h,
++		   uint32_t src_x, uint32_t src_y,
++		   uint32_t src_w, uint32_t src_h)
++{
++	struct intel_plane_state state;
++	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
++	int ret;
++
++	state.crtc = crtc;
++	state.fb = fb;
++
++	/* sample coordinates in 16.16 fixed point */
++	state.src.x1 = src_x;
++	state.src.x2 = src_x + src_w;
++	state.src.y1 = src_y;
++	state.src.y2 = src_y + src_h;
++
++	/* integer pixels */
++	state.dst.x1 = crtc_x;
++	state.dst.x2 = crtc_x + crtc_w;
++	state.dst.y1 = crtc_y;
++	state.dst.y2 = crtc_y + crtc_h;
++
++	state.clip.x1 = 0;
++	state.clip.y1 = 0;
++	state.clip.x2 = intel_crtc->active ? intel_crtc->config.pipe_src_w : 0;
++	state.clip.y2 = intel_crtc->active ? intel_crtc->config.pipe_src_h : 0;
++	state.orig_src = state.src;
++	state.orig_dst = state.dst;
++
++	ret = intel_check_sprite_plane(plane, &state);
++	if (ret)
++		return ret;
++
++	ret = intel_prepare_sprite_plane(plane, &state);
++	if (ret)
++		return ret;
+ 
++	intel_commit_sprite_plane(plane, &state);
+ 	return 0;
+ }
+ 
+@@ -1180,18 +1535,45 @@
+ 	return ret;
+ }
+ 
+-void intel_plane_restore(struct drm_plane *plane)
++int intel_plane_set_property(struct drm_plane *plane,
++			     struct drm_property *prop,
++			     uint64_t val)
++{
++	struct drm_device *dev = plane->dev;
++	struct intel_plane *intel_plane = to_intel_plane(plane);
++	uint64_t old_val;
++	int ret = -ENOENT;
++
++	if (prop == dev->mode_config.rotation_property) {
++		/* exactly one rotation angle please */
++		if (hweight32(val & 0xf) != 1)
++			return -EINVAL;
++
++		if (intel_plane->rotation == val)
++			return 0;
++
++		old_val = intel_plane->rotation;
++		intel_plane->rotation = val;
++		ret = intel_plane_restore(plane);
++		if (ret)
++			intel_plane->rotation = old_val;
++	}
++
++	return ret;
++}
++
++int intel_plane_restore(struct drm_plane *plane)
+ {
+ 	struct intel_plane *intel_plane = to_intel_plane(plane);
+ 
+ 	if (!plane->crtc || !plane->fb)
+-		return;
++		return 0;
+ 
+-	intel_update_plane(plane, plane->crtc, plane->fb,
+-			   intel_plane->crtc_x, intel_plane->crtc_y,
+-			   intel_plane->crtc_w, intel_plane->crtc_h,
+-			   intel_plane->src_x, intel_plane->src_y,
+-			   intel_plane->src_w, intel_plane->src_h);
++	return plane->funcs->update_plane(plane, plane->crtc, plane->fb,
++				  intel_plane->crtc_x, intel_plane->crtc_y,
++				  intel_plane->crtc_w, intel_plane->crtc_h,
++				  intel_plane->src_x, intel_plane->src_y,
++				  intel_plane->src_w, intel_plane->src_h);
+ }
+ 
+ void intel_plane_disable(struct drm_plane *plane)
+@@ -1206,6 +1588,7 @@
+ 	.update_plane = intel_update_plane,
+ 	.disable_plane = intel_disable_plane,
+ 	.destroy = intel_destroy_plane,
++	.set_property = intel_plane_set_property,
+ };
+ 
+ static uint32_t ilk_plane_formats[] = {
+@@ -1239,6 +1622,18 @@
+ 	DRM_FORMAT_VYUY,
+ };
+ 
++static uint32_t skl_plane_formats[] = {
++	DRM_FORMAT_RGB565,
++	DRM_FORMAT_ABGR8888,
++	DRM_FORMAT_ARGB8888,
++	DRM_FORMAT_XBGR8888,
++	DRM_FORMAT_XRGB8888,
++	DRM_FORMAT_YUYV,
++	DRM_FORMAT_YVYU,
++	DRM_FORMAT_UYVY,
++	DRM_FORMAT_VYUY,
++};
++
+ int
+ intel_plane_init(struct drm_device *dev, enum pipe pipe, int plane)
+ {
+@@ -1302,7 +1697,21 @@
+ 			num_plane_formats = ARRAY_SIZE(snb_plane_formats);
+ 		}
+ 		break;
++	case 9:
++		/*
++		 * FIXME: Skylake planes can be scaled (with some restrictions),
++		 * but this is for another time.
++		 */
++		intel_plane->can_scale = false;
++		intel_plane->max_downscale = 1;
++		intel_plane->update_plane = skl_update_plane;
++		intel_plane->disable_plane = skl_disable_plane;
++		intel_plane->update_colorkey = skl_update_colorkey;
++		intel_plane->get_colorkey = skl_get_colorkey;
+ 
++		plane_formats = skl_plane_formats;
++		num_plane_formats = ARRAY_SIZE(skl_plane_formats);
++		break;
+ 	default:
+ 		kfree(intel_plane);
+ 		return -ENODEV;
+@@ -1310,13 +1719,28 @@
+ 
+ 	intel_plane->pipe = pipe;
+ 	intel_plane->plane = plane;
++	intel_plane->rotation = BIT(DRM_ROTATE_0);
+ 	possible_crtcs = (1 << pipe);
+-	ret = drm_plane_init(dev, &intel_plane->base, possible_crtcs,
+-			     &intel_plane_funcs,
+-			     plane_formats, num_plane_formats,
+-			     false);
+-	if (ret)
++	ret = drm_universal_plane_init(dev, &intel_plane->base, possible_crtcs,
++				       &intel_plane_funcs,
++				       plane_formats, num_plane_formats,
++				       DRM_PLANE_TYPE_OVERLAY);
++	if (ret) {
+ 		kfree(intel_plane);
++		goto out;
++	}
++
++	if (!dev->mode_config.rotation_property)
++		dev->mode_config.rotation_property =
++			drm_mode_create_rotation_property(dev,
++							  BIT(DRM_ROTATE_0) |
++							  BIT(DRM_ROTATE_180));
++
++	if (dev->mode_config.rotation_property)
++		drm_object_attach_property(&intel_plane->base.base,
++					   dev->mode_config.rotation_property,
++					   intel_plane->rotation);
+ 
++ out:
+ 	return ret;
+ }
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_tv.c b/drivers/gpu/drm/i915/intel_tv.c
+--- a/drivers/gpu/drm/i915/intel_tv.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_tv.c	2014-11-20 09:53:38.000762837 -0700
+@@ -1173,7 +1173,7 @@
+  * \return true if TV is connected.
+  * \return false if TV is disconnected.
+  */
+-static int
++static enum drm_connector_status
+ intel_tv_detect_type(struct intel_tv *intel_tv,
+ 		      struct drm_connector *connector)
+ {
+@@ -1182,18 +1182,17 @@
+ 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+ 	struct drm_device *dev = encoder->dev;
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+-	unsigned long irqflags;
++	enum drm_connector_status status;
+ 	u32 tv_ctl, save_tv_ctl;
+ 	u32 tv_dac, save_tv_dac;
+-	int type;
+ 
+ 	/* Disable TV interrupts around load detect or we'll recurse */
+ 	if (connector->polled & DRM_CONNECTOR_POLL_HPD) {
+-		spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
++		spin_lock_irq(&dev_priv->irq_lock);
+ 		i915_disable_pipestat(dev_priv, 0,
+ 				      PIPE_HOTPLUG_INTERRUPT_STATUS |
+ 				      PIPE_HOTPLUG_TV_INTERRUPT_STATUS);
+-		spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
++		spin_unlock_irq(&dev_priv->irq_lock);
+ 	}
+ 
+ 	save_tv_dac = tv_dac = I915_READ(TV_DAC);
+@@ -1233,7 +1232,6 @@
+ 	intel_wait_for_vblank(intel_tv->base.base.dev,
+ 			      to_intel_crtc(intel_tv->base.base.crtc)->pipe);
+ 
+-	type = -1;
+ 	tv_dac = I915_READ(TV_DAC);
+ 	DRM_DEBUG_KMS("TV detected: %x, %x\n", tv_ctl, tv_dac);
+ 	/*
+@@ -1242,18 +1240,19 @@
+ 	 *  1 0 X svideo
+ 	 *  0 0 0 Component
+ 	 */
++	status = connector_status_connected;
+ 	if ((tv_dac & TVDAC_SENSE_MASK) == (TVDAC_B_SENSE | TVDAC_C_SENSE)) {
+ 		DRM_DEBUG_KMS("Detected Composite TV connection\n");
+-		type = DRM_MODE_CONNECTOR_Composite;
++		intel_tv->type = DRM_MODE_CONNECTOR_Composite;
+ 	} else if ((tv_dac & (TVDAC_A_SENSE|TVDAC_B_SENSE)) == TVDAC_A_SENSE) {
+ 		DRM_DEBUG_KMS("Detected S-Video TV connection\n");
+-		type = DRM_MODE_CONNECTOR_SVIDEO;
++		intel_tv->type = DRM_MODE_CONNECTOR_SVIDEO;
+ 	} else if ((tv_dac & TVDAC_SENSE_MASK) == 0) {
+ 		DRM_DEBUG_KMS("Detected Component TV connection\n");
+-		type = DRM_MODE_CONNECTOR_Component;
++		intel_tv->type = DRM_MODE_CONNECTOR_Component;
+ 	} else {
+ 		DRM_DEBUG_KMS("Unrecognised TV connection\n");
+-		type = -1;
++		status = connector_status_disconnected;
+ 	}
+ 
+ 	I915_WRITE(TV_DAC, save_tv_dac & ~TVDAC_STATE_CHG_EN);
+@@ -1266,14 +1265,14 @@
+ 
+ 	/* Restore interrupt config */
+ 	if (connector->polled & DRM_CONNECTOR_POLL_HPD) {
+-		spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
++		spin_lock_irq(&dev_priv->irq_lock);
+ 		i915_enable_pipestat(dev_priv, 0,
+ 				     PIPE_HOTPLUG_INTERRUPT_STATUS |
+ 				     PIPE_HOTPLUG_TV_INTERRUPT_STATUS);
+-		spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
++		spin_unlock_irq(&dev_priv->irq_lock);
+ 	}
+ 
+-	return type;
++	return status;
+ }
+ 
+ /*
+@@ -1313,43 +1312,33 @@
+ static enum drm_connector_status
+ intel_tv_detect(struct drm_connector *connector, bool force)
+ {
+-	struct drm_display_mode mode;
+ 	struct intel_tv *intel_tv = intel_attached_tv(connector);
++	struct drm_display_mode mode = reported_modes[0];
++	struct intel_load_detect_pipe tmp;
++	struct drm_modeset_acquire_ctx ctx;
+ 	enum drm_connector_status status;
+-	int type;
+ 
+ 	DRM_DEBUG_KMS("[CONNECTOR:%d:%s] force=%d\n",
+ 		      connector->base.id, connector->name,
+ 		      force);
++	if (!force)
++		return connector->status;
+ 
+-	mode = reported_modes[0];
+-
+-	if (force) {
+-		struct intel_load_detect_pipe tmp;
+-		struct drm_modeset_acquire_ctx ctx;
+-
+-		drm_modeset_acquire_init(&ctx, 0);
+-
+-		if (intel_get_load_detect_pipe(connector, &mode, &tmp, &ctx)) {
+-			type = intel_tv_detect_type(intel_tv, connector);
+-			intel_release_load_detect_pipe(connector, &tmp);
+-			status = type < 0 ?
+-				connector_status_disconnected :
+-				connector_status_connected;
+-		} else
+-			status = connector_status_unknown;
++	drm_modeset_acquire_init(&ctx, 0);
+ 
+-		drm_modeset_drop_locks(&ctx);
+-		drm_modeset_acquire_fini(&ctx);
++	if (intel_get_load_detect_pipe(connector, &mode, &tmp, &ctx)) {
++		status = intel_tv_detect_type(intel_tv, connector);
++		intel_release_load_detect_pipe(connector, &tmp);
+ 	} else
+-		return connector->status;
++		status = connector_status_unknown;
++
++	drm_modeset_drop_locks(&ctx);
++	drm_modeset_acquire_fini(&ctx);
+ 
+ 	if (status != connector_status_connected)
+ 		return status;
+ 
+-	intel_tv->type = type;
+ 	intel_tv_find_better_format(connector);
+-
+ 	return connector_status_connected;
+ }
+ 
+diff -urN -x arch a/drivers/gpu/drm/i915/intel_uncore.c b/drivers/gpu/drm/i915/intel_uncore.c
+--- a/drivers/gpu/drm/i915/intel_uncore.c	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/intel_uncore.c	2014-11-20 09:53:38.000762837 -0700
+@@ -24,6 +24,8 @@
+ #include "i915_drv.h"
+ #include "intel_drv.h"
+ 
++#include <linux/pm_runtime.h>
++
+ #define FORCEWAKE_ACK_TIMEOUT_MS 2
+ 
+ #define __raw_i915_read8(dev_priv__, reg__) readb((dev_priv__)->regs + (reg__))
+@@ -71,7 +73,7 @@
+ }
+ 
+ static void __gen6_gt_force_wake_get(struct drm_i915_private *dev_priv,
+-							int fw_engine)
++				     int fw_engine)
+ {
+ 	if (wait_for_atomic((__raw_i915_read32(dev_priv, FORCEWAKE_ACK) & 1) == 0,
+ 			    FORCEWAKE_ACK_TIMEOUT_MS))
+@@ -97,11 +99,11 @@
+ }
+ 
+ static void __gen7_gt_force_wake_mt_get(struct drm_i915_private *dev_priv,
+-							int fw_engine)
++					int fw_engine)
+ {
+ 	u32 forcewake_ack;
+ 
+-	if (IS_HASWELL(dev_priv->dev) || IS_GEN8(dev_priv->dev))
++	if (IS_HASWELL(dev_priv->dev) || IS_BROADWELL(dev_priv->dev))
+ 		forcewake_ack = FORCEWAKE_ACK_HSW;
+ 	else
+ 		forcewake_ack = FORCEWAKE_MT_ACK;
+@@ -120,8 +122,7 @@
+ 		DRM_ERROR("Timed out waiting for forcewake to ack request.\n");
+ 
+ 	/* WaRsForcewakeWaitTC0:ivb,hsw */
+-	if (INTEL_INFO(dev_priv->dev)->gen < 8)
+-		__gen6_gt_wait_for_thread_c0(dev_priv);
++	__gen6_gt_wait_for_thread_c0(dev_priv);
+ }
+ 
+ static void gen6_gt_check_fifodbg(struct drm_i915_private *dev_priv)
+@@ -134,7 +135,7 @@
+ }
+ 
+ static void __gen6_gt_force_wake_put(struct drm_i915_private *dev_priv,
+-							int fw_engine)
++				     int fw_engine)
+ {
+ 	__raw_i915_write32(dev_priv, FORCEWAKE, 0);
+ 	/* something from same cacheline, but !FORCEWAKE */
+@@ -143,7 +144,7 @@
+ }
+ 
+ static void __gen7_gt_force_wake_mt_put(struct drm_i915_private *dev_priv,
+-							int fw_engine)
++					int fw_engine)
+ {
+ 	__raw_i915_write32(dev_priv, FORCEWAKE_MT,
+ 			   _MASKED_BIT_DISABLE(FORCEWAKE_KERNEL));
+@@ -192,7 +193,7 @@
+ }
+ 
+ static void __vlv_force_wake_get(struct drm_i915_private *dev_priv,
+-						int fw_engine)
++				 int fw_engine)
+ {
+ 	/* Check for Render Engine */
+ 	if (FORCEWAKE_RENDER & fw_engine) {
+@@ -236,9 +237,8 @@
+ }
+ 
+ static void __vlv_force_wake_put(struct drm_i915_private *dev_priv,
+-					int fw_engine)
++				 int fw_engine)
+ {
+-
+ 	/* Check for Render Engine */
+ 	if (FORCEWAKE_RENDER & fw_engine)
+ 		__raw_i915_write32(dev_priv, FORCEWAKE_VLV,
+@@ -256,73 +256,124 @@
+ 		gen6_gt_check_fifodbg(dev_priv);
+ }
+ 
+-static void vlv_force_wake_get(struct drm_i915_private *dev_priv, int fw_engine)
++static void __gen9_gt_force_wake_mt_reset(struct drm_i915_private *dev_priv)
+ {
+-	unsigned long irqflags;
+-
+-	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
+-
+-	if (fw_engine & FORCEWAKE_RENDER &&
+-	    dev_priv->uncore.fw_rendercount++ != 0)
+-		fw_engine &= ~FORCEWAKE_RENDER;
+-	if (fw_engine & FORCEWAKE_MEDIA &&
+-	    dev_priv->uncore.fw_mediacount++ != 0)
+-		fw_engine &= ~FORCEWAKE_MEDIA;
++	__raw_i915_write32(dev_priv, FORCEWAKE_RENDER_GEN9,
++			_MASKED_BIT_DISABLE(0xffff));
+ 
+-	if (fw_engine)
+-		dev_priv->uncore.funcs.force_wake_get(dev_priv, fw_engine);
++	__raw_i915_write32(dev_priv, FORCEWAKE_MEDIA_GEN9,
++			_MASKED_BIT_DISABLE(0xffff));
+ 
+-	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
++	__raw_i915_write32(dev_priv, FORCEWAKE_BLITTER_GEN9,
++			_MASKED_BIT_DISABLE(0xffff));
+ }
+ 
+-static void vlv_force_wake_put(struct drm_i915_private *dev_priv, int fw_engine)
++static void
++__gen9_force_wake_get(struct drm_i915_private *dev_priv, int fw_engine)
+ {
+-	unsigned long irqflags;
++	/* Check for Render Engine */
++	if (FORCEWAKE_RENDER & fw_engine) {
++		if (wait_for_atomic((__raw_i915_read32(dev_priv,
++						FORCEWAKE_ACK_RENDER_GEN9) &
++						FORCEWAKE_KERNEL) == 0,
++					FORCEWAKE_ACK_TIMEOUT_MS))
++			DRM_ERROR("Timed out: Render forcewake old ack to clear.\n");
+ 
+-	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
++		__raw_i915_write32(dev_priv, FORCEWAKE_RENDER_GEN9,
++				   _MASKED_BIT_ENABLE(FORCEWAKE_KERNEL));
+ 
+-	if (fw_engine & FORCEWAKE_RENDER) {
+-		WARN_ON(!dev_priv->uncore.fw_rendercount);
+-		if (--dev_priv->uncore.fw_rendercount != 0)
+-			fw_engine &= ~FORCEWAKE_RENDER;
++		if (wait_for_atomic((__raw_i915_read32(dev_priv,
++						FORCEWAKE_ACK_RENDER_GEN9) &
++						FORCEWAKE_KERNEL),
++					FORCEWAKE_ACK_TIMEOUT_MS))
++			DRM_ERROR("Timed out: waiting for Render to ack.\n");
+ 	}
+ 
+-	if (fw_engine & FORCEWAKE_MEDIA) {
+-		WARN_ON(!dev_priv->uncore.fw_mediacount);
+-		if (--dev_priv->uncore.fw_mediacount != 0)
+-			fw_engine &= ~FORCEWAKE_MEDIA;
++	/* Check for Media Engine */
++	if (FORCEWAKE_MEDIA & fw_engine) {
++		if (wait_for_atomic((__raw_i915_read32(dev_priv,
++						FORCEWAKE_ACK_MEDIA_GEN9) &
++						FORCEWAKE_KERNEL) == 0,
++					FORCEWAKE_ACK_TIMEOUT_MS))
++			DRM_ERROR("Timed out: Media forcewake old ack to clear.\n");
++
++		__raw_i915_write32(dev_priv, FORCEWAKE_MEDIA_GEN9,
++				   _MASKED_BIT_ENABLE(FORCEWAKE_KERNEL));
++
++		if (wait_for_atomic((__raw_i915_read32(dev_priv,
++						FORCEWAKE_ACK_MEDIA_GEN9) &
++						FORCEWAKE_KERNEL),
++					FORCEWAKE_ACK_TIMEOUT_MS))
++			DRM_ERROR("Timed out: waiting for Media to ack.\n");
+ 	}
+ 
+-	if (fw_engine)
+-		dev_priv->uncore.funcs.force_wake_put(dev_priv, fw_engine);
++	/* Check for Blitter Engine */
++	if (FORCEWAKE_BLITTER & fw_engine) {
++		if (wait_for_atomic((__raw_i915_read32(dev_priv,
++						FORCEWAKE_ACK_BLITTER_GEN9) &
++						FORCEWAKE_KERNEL) == 0,
++					FORCEWAKE_ACK_TIMEOUT_MS))
++			DRM_ERROR("Timed out: Blitter forcewake old ack to clear.\n");
+ 
+-	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
++		__raw_i915_write32(dev_priv, FORCEWAKE_BLITTER_GEN9,
++				   _MASKED_BIT_ENABLE(FORCEWAKE_KERNEL));
++
++		if (wait_for_atomic((__raw_i915_read32(dev_priv,
++						FORCEWAKE_ACK_BLITTER_GEN9) &
++						FORCEWAKE_KERNEL),
++					FORCEWAKE_ACK_TIMEOUT_MS))
++			DRM_ERROR("Timed out: waiting for Blitter to ack.\n");
++	}
++}
++
++static void
++__gen9_force_wake_put(struct drm_i915_private *dev_priv, int fw_engine)
++{
++	/* Check for Render Engine */
++	if (FORCEWAKE_RENDER & fw_engine)
++		__raw_i915_write32(dev_priv, FORCEWAKE_RENDER_GEN9,
++				_MASKED_BIT_DISABLE(FORCEWAKE_KERNEL));
++
++	/* Check for Media Engine */
++	if (FORCEWAKE_MEDIA & fw_engine)
++		__raw_i915_write32(dev_priv, FORCEWAKE_MEDIA_GEN9,
++				_MASKED_BIT_DISABLE(FORCEWAKE_KERNEL));
++
++	/* Check for Blitter Engine */
++	if (FORCEWAKE_BLITTER & fw_engine)
++		__raw_i915_write32(dev_priv, FORCEWAKE_BLITTER_GEN9,
++				_MASKED_BIT_DISABLE(FORCEWAKE_KERNEL));
+ }
+ 
+ static void gen6_force_wake_timer(unsigned long arg)
+ {
+-	struct drm_i915_private *dev_priv = (void *)arg;
++	struct intel_uncore_forcewake_domain *domain = (void *)arg;
+ 	unsigned long irqflags;
+ 
+-	assert_device_not_suspended(dev_priv);
+-
+-	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
+-	WARN_ON(!dev_priv->uncore.forcewake_count);
++	assert_device_not_suspended(domain->i915);
+ 
+-	if (--dev_priv->uncore.forcewake_count == 0)
+-		dev_priv->uncore.funcs.force_wake_put(dev_priv, FORCEWAKE_ALL);
+-	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
++	spin_lock_irqsave(&domain->i915->uncore.lock, irqflags);
++	WARN_ON(!domain->wake_count);
+ 
+-	intel_runtime_pm_put(dev_priv);
++	if (--domain->wake_count == 0)
++		domain->i915->uncore.funcs.force_wake_put(domain->i915,
++							  1 << domain->id);
++	spin_unlock_irqrestore(&domain->i915->uncore.lock, irqflags);
+ }
+ 
+ void intel_uncore_forcewake_reset(struct drm_device *dev, bool restore)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 	unsigned long irqflags;
++	int i;
+ 
+-	if (del_timer_sync(&dev_priv->uncore.force_wake_timer))
+-		gen6_force_wake_timer((unsigned long)dev_priv);
++	for (i = 0; i < FW_DOMAIN_COUNT; i++ ) {
++		if ((dev_priv->uncore.fw_domains & (1 << i)) == 0)
++			continue;
++
++		if (del_timer_sync(&dev_priv->uncore.fw_domain[i].timer))
++			gen6_force_wake_timer((unsigned long)&dev_priv->uncore.fw_domain[i]);
++	}
+ 
+ 	/* Hold uncore.lock across reset to prevent any register access
+ 	 * with forcewake not set correctly
+@@ -334,23 +385,20 @@
+ 	else if (IS_GEN6(dev) || IS_GEN7(dev))
+ 		__gen6_gt_force_wake_reset(dev_priv);
+ 
+-	if (IS_IVYBRIDGE(dev) || IS_HASWELL(dev) || IS_GEN8(dev))
++	if (IS_IVYBRIDGE(dev) || IS_HASWELL(dev) || IS_BROADWELL(dev))
+ 		__gen7_gt_force_wake_mt_reset(dev_priv);
+ 
++	if (IS_GEN9(dev))
++		__gen9_gt_force_wake_mt_reset(dev_priv);
++
+ 	if (restore) { /* If reset with a user forcewake, try to restore */
+ 		unsigned fw = 0;
++		int i;
+ 
+-		if (IS_VALLEYVIEW(dev)) {
+-			if (dev_priv->uncore.fw_rendercount)
+-				fw |= FORCEWAKE_RENDER;
+-
+-			if (dev_priv->uncore.fw_mediacount)
+-				fw |= FORCEWAKE_MEDIA;
+-		} else {
+-			if (dev_priv->uncore.forcewake_count)
+-				fw = FORCEWAKE_ALL;
++		for (i = 0; i < FW_DOMAIN_COUNT; i++) {
++			if (dev_priv->uncore.fw_domain[i].wake_count)
++				fw |= 1 << i;
+ 		}
+-
+ 		if (fw)
+ 			dev_priv->uncore.funcs.force_wake_get(dev_priv, fw);
+ 
+@@ -363,7 +411,8 @@
+ 	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
+ }
+ 
+-void intel_uncore_early_sanitize(struct drm_device *dev, bool restore_forcewake)
++static void __intel_uncore_early_sanitize(struct drm_device *dev,
++					  bool restore_forcewake)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
+ 
+@@ -389,6 +438,12 @@
+ 	intel_uncore_forcewake_reset(dev, restore_forcewake);
+ }
+ 
++void intel_uncore_early_sanitize(struct drm_device *dev, bool restore_forcewake)
++{
++	__intel_uncore_early_sanitize(dev, restore_forcewake);
++	i915_check_and_clear_faults(dev);
++}
++
+ void intel_uncore_sanitize(struct drm_device *dev)
+ {
+ 	/* BIOS often leaves RC6 enabled, but disable it for hw init */
+@@ -401,65 +456,76 @@
+  * be called at the beginning of the sequence followed by a call to
+  * gen6_gt_force_wake_put() at the end of the sequence.
+  */
+-void gen6_gt_force_wake_get(struct drm_i915_private *dev_priv, int fw_engine)
++void gen6_gt_force_wake_get(struct drm_i915_private *dev_priv,
++			    unsigned fw_domains)
+ {
+ 	unsigned long irqflags;
++	int i;
+ 
+ 	if (!dev_priv->uncore.funcs.force_wake_get)
+ 		return;
+ 
+-	intel_runtime_pm_get(dev_priv);
++	WARN_ON(!pm_runtime_active(&dev_priv->dev->pdev->dev));
+ 
+-	/* Redirect to VLV specific routine */
+-	if (IS_VALLEYVIEW(dev_priv->dev))
+-		return vlv_force_wake_get(dev_priv, fw_engine);
++	fw_domains &= dev_priv->uncore.fw_domains;
+ 
+ 	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
+-	if (dev_priv->uncore.forcewake_count++ == 0)
+-		dev_priv->uncore.funcs.force_wake_get(dev_priv, FORCEWAKE_ALL);
++
++	for (i = 0; i < FW_DOMAIN_COUNT; i++) {
++		if ((fw_domains & (1 << i)) == 0)
++			continue;
++
++		if (dev_priv->uncore.fw_domain[i].wake_count++)
++			fw_domains &= ~(1 << i);
++	}
++
++	if (fw_domains)
++		dev_priv->uncore.funcs.force_wake_get(dev_priv, fw_domains);
++
+ 	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
+ }
+ 
+ /*
+  * see gen6_gt_force_wake_get()
+  */
+-void gen6_gt_force_wake_put(struct drm_i915_private *dev_priv, int fw_engine)
++void gen6_gt_force_wake_put(struct drm_i915_private *dev_priv,
++			    unsigned fw_domains)
+ {
+ 	unsigned long irqflags;
+-	bool delayed = false;
++	int i;
+ 
+ 	if (!dev_priv->uncore.funcs.force_wake_put)
+ 		return;
+ 
+-	/* Redirect to VLV specific routine */
+-	if (IS_VALLEYVIEW(dev_priv->dev)) {
+-		vlv_force_wake_put(dev_priv, fw_engine);
+-		goto out;
+-	}
+-
++	fw_domains &= dev_priv->uncore.fw_domains;
+ 
+ 	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
+-	WARN_ON(!dev_priv->uncore.forcewake_count);
+ 
+-	if (--dev_priv->uncore.forcewake_count == 0) {
+-		dev_priv->uncore.forcewake_count++;
+-		delayed = true;
+-		mod_timer_pinned(&dev_priv->uncore.force_wake_timer,
++	for (i = 0; i < FW_DOMAIN_COUNT; i++) {
++		if ((fw_domains & (1 << i)) == 0)
++			continue;
++
++		WARN_ON(!dev_priv->uncore.fw_domain[i].wake_count);
++		if (--dev_priv->uncore.fw_domain[i].wake_count)
++			continue;
++
++		dev_priv->uncore.fw_domain[i].wake_count++;
++		mod_timer_pinned(&dev_priv->uncore.fw_domain[i].timer,
+ 				 jiffies + 1);
+ 	}
+-	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
+ 
+-out:
+-	if (!delayed)
+-		intel_runtime_pm_put(dev_priv);
++	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
+ }
+ 
+ void assert_force_wake_inactive(struct drm_i915_private *dev_priv)
+ {
++	int i;
++
+ 	if (!dev_priv->uncore.funcs.force_wake_get)
+ 		return;
+ 
+-	WARN_ON(dev_priv->uncore.forcewake_count > 0);
++	for (i = 0; i < FW_DOMAIN_COUNT; i++)
++		WARN_ON(dev_priv->uncore.fw_domain[i].wake_count > 0);
+ }
+ 
+ /* We give fast paths for the really cool registers */
+@@ -520,7 +586,7 @@
+ 	const char *op = read ? "reading" : "writing to";
+ 	const char *when = before ? "before" : "after";
+ 
+-	if (!i915.mmio_debug)
++	if (!i915_module.mmio_debug)
+ 		return;
+ 
+ 	if (__raw_i915_read32(dev_priv, FPGA_DBG) & FPGA_DBG_RM_NOCLAIM) {
+@@ -533,105 +599,128 @@
+ static void
+ hsw_unclaimed_reg_detect(struct drm_i915_private *dev_priv)
+ {
+-	if (i915.mmio_debug)
++	if (!i915_module.mmio_debug)
+ 		return;
+ 
+ 	if (__raw_i915_read32(dev_priv, FPGA_DBG) & FPGA_DBG_RM_NOCLAIM) {
+-		DRM_ERROR("Unclaimed register detected. Please use the i915.mmio_debug=1 to debug this problem.");
++		DRM_ERROR_ONCE("Unclaimed register detected. Please use the i915.mmio_debug=1 to debug this problem.");
+ 		__raw_i915_write32(dev_priv, FPGA_DBG, FPGA_DBG_RM_NOCLAIM);
+ 	}
+ }
+ 
+-#define REG_READ_HEADER(x) \
+-	unsigned long irqflags; \
++#define GEN2_READ_HEADER(x) \
+ 	u##x val = 0; \
+-	assert_device_not_suspended(dev_priv); \
+-	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags)
++	assert_device_not_suspended(dev_priv);
+ 
+-#define REG_READ_FOOTER \
+-	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags); \
++#define GEN2_READ_FOOTER \
+ 	trace_i915_reg_rw(false, reg, val, sizeof(val), trace); \
+ 	return val
+ 
+-#define __gen4_read(x) \
++#define __gen2_read(x) \
+ static u##x \
+-gen4_read##x(struct drm_i915_private *dev_priv, off_t reg, bool trace) { \
+-	REG_READ_HEADER(x); \
++gen2_read##x(struct drm_i915_private *dev_priv, off_t reg, bool trace) { \
++	GEN2_READ_HEADER(x); \
+ 	val = __raw_i915_read##x(dev_priv, reg); \
+-	REG_READ_FOOTER; \
++	GEN2_READ_FOOTER; \
+ }
+ 
+ #define __gen5_read(x) \
+ static u##x \
+ gen5_read##x(struct drm_i915_private *dev_priv, off_t reg, bool trace) { \
+-	REG_READ_HEADER(x); \
++	GEN2_READ_HEADER(x); \
+ 	ilk_dummy_write(dev_priv); \
+ 	val = __raw_i915_read##x(dev_priv, reg); \
+-	REG_READ_FOOTER; \
++	GEN2_READ_FOOTER; \
++}
++
++__gen5_read(8)
++__gen5_read(16)
++__gen5_read(32)
++__gen5_read(64)
++__gen2_read(8)
++__gen2_read(16)
++__gen2_read(32)
++__gen2_read(64)
++
++#undef __gen5_read
++#undef __gen2_read
++
++#undef GEN2_READ_FOOTER
++#undef GEN2_READ_HEADER
++
++#define GEN6_READ_HEADER(x) \
++	unsigned long irqflags; \
++	u##x val = 0; \
++	assert_device_not_suspended(dev_priv); \
++	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags)
++
++#define GEN6_READ_FOOTER \
++	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags); \
++	trace_i915_reg_rw(false, reg, val, sizeof(val), trace); \
++	return val
++
++static inline void __force_wake_get(struct drm_i915_private *dev_priv,
++				    unsigned fw_domains)
++{
++	int i;
++
++	/* Ideally GCC would be constant-fold and eliminate this loop */
++
++	for (i = 0; i < FW_DOMAIN_COUNT; i++) {
++		if ((fw_domains & (1 << i)) == 0)
++			continue;
++
++		if (dev_priv->uncore.fw_domain[i].wake_count) {
++			fw_domains &= ~(1 << i);
++			continue;
++		}
++
++		dev_priv->uncore.fw_domain[i].wake_count++;
++		mod_timer_pinned(&dev_priv->uncore.fw_domain[i].timer,
++				 jiffies + 1);
++	}
++
++	if (fw_domains)
++		dev_priv->uncore.funcs.force_wake_get(dev_priv, fw_domains);
+ }
+ 
+ #define __gen6_read(x) \
+ static u##x \
+ gen6_read##x(struct drm_i915_private *dev_priv, off_t reg, bool trace) { \
+-	REG_READ_HEADER(x); \
++	GEN6_READ_HEADER(x); \
+ 	hsw_unclaimed_reg_debug(dev_priv, reg, true, true); \
+-	if (dev_priv->uncore.forcewake_count == 0 && \
+-	    NEEDS_FORCE_WAKE((dev_priv), (reg))) { \
+-		dev_priv->uncore.funcs.force_wake_get(dev_priv, \
+-						      FORCEWAKE_ALL); \
+-		val = __raw_i915_read##x(dev_priv, reg); \
+-		dev_priv->uncore.funcs.force_wake_put(dev_priv, \
+-						      FORCEWAKE_ALL); \
+-	} else { \
+-		val = __raw_i915_read##x(dev_priv, reg); \
+-	} \
++	if (NEEDS_FORCE_WAKE((dev_priv), (reg))) \
++		__force_wake_get(dev_priv, FORCEWAKE_RENDER); \
++	val = __raw_i915_read##x(dev_priv, reg); \
+ 	hsw_unclaimed_reg_debug(dev_priv, reg, true, false); \
+-	REG_READ_FOOTER; \
++	GEN6_READ_FOOTER; \
+ }
+ 
+ #define __vlv_read(x) \
+ static u##x \
+ vlv_read##x(struct drm_i915_private *dev_priv, off_t reg, bool trace) { \
+-	unsigned fwengine = 0; \
+-	REG_READ_HEADER(x); \
+-	if (FORCEWAKE_VLV_RENDER_RANGE_OFFSET(reg)) { \
+-		if (dev_priv->uncore.fw_rendercount == 0) \
+-			fwengine = FORCEWAKE_RENDER; \
+-	} else if (FORCEWAKE_VLV_MEDIA_RANGE_OFFSET(reg)) { \
+-		if (dev_priv->uncore.fw_mediacount == 0) \
+-			fwengine = FORCEWAKE_MEDIA; \
+-	}  \
+-	if (fwengine) \
+-		dev_priv->uncore.funcs.force_wake_get(dev_priv, fwengine); \
++	GEN6_READ_HEADER(x); \
++	if (FORCEWAKE_VLV_RENDER_RANGE_OFFSET(reg)) \
++		__force_wake_get(dev_priv, FORCEWAKE_RENDER); \
++	else if (FORCEWAKE_VLV_MEDIA_RANGE_OFFSET(reg)) \
++		__force_wake_get(dev_priv, FORCEWAKE_MEDIA); \
+ 	val = __raw_i915_read##x(dev_priv, reg); \
+-	if (fwengine) \
+-		dev_priv->uncore.funcs.force_wake_put(dev_priv, fwengine); \
+-	REG_READ_FOOTER; \
++	GEN6_READ_FOOTER; \
+ }
+ 
+ #define __chv_read(x) \
+ static u##x \
+ chv_read##x(struct drm_i915_private *dev_priv, off_t reg, bool trace) { \
+-	unsigned fwengine = 0; \
+-	REG_READ_HEADER(x); \
+-	if (FORCEWAKE_CHV_RENDER_RANGE_OFFSET(reg)) { \
+-		if (dev_priv->uncore.fw_rendercount == 0) \
+-			fwengine = FORCEWAKE_RENDER; \
+-	} else if (FORCEWAKE_CHV_MEDIA_RANGE_OFFSET(reg)) { \
+-		if (dev_priv->uncore.fw_mediacount == 0) \
+-			fwengine = FORCEWAKE_MEDIA; \
+-	} else if (FORCEWAKE_CHV_COMMON_RANGE_OFFSET(reg)) { \
+-		if (dev_priv->uncore.fw_rendercount == 0) \
+-			fwengine |= FORCEWAKE_RENDER; \
+-		if (dev_priv->uncore.fw_mediacount == 0) \
+-			fwengine |= FORCEWAKE_MEDIA; \
+-	} \
+-	if (fwengine) \
+-		dev_priv->uncore.funcs.force_wake_get(dev_priv, fwengine); \
++	GEN6_READ_HEADER(x); \
++	if (FORCEWAKE_CHV_RENDER_RANGE_OFFSET(reg)) \
++		__force_wake_get(dev_priv, FORCEWAKE_RENDER); \
++	else if (FORCEWAKE_CHV_MEDIA_RANGE_OFFSET(reg)) \
++		__force_wake_get(dev_priv, FORCEWAKE_MEDIA); \
++	else if (FORCEWAKE_CHV_COMMON_RANGE_OFFSET(reg)) \
++		__force_wake_get(dev_priv, \
++				 FORCEWAKE_RENDER | FORCEWAKE_MEDIA); \
+ 	val = __raw_i915_read##x(dev_priv, reg); \
+-	if (fwengine) \
+-		dev_priv->uncore.funcs.force_wake_put(dev_priv, fwengine); \
+-	REG_READ_FOOTER; \
++	GEN6_READ_FOOTER; \
+ }
+ 
+ __chv_read(8)
+@@ -646,54 +735,65 @@
+ __gen6_read(16)
+ __gen6_read(32)
+ __gen6_read(64)
+-__gen5_read(8)
+-__gen5_read(16)
+-__gen5_read(32)
+-__gen5_read(64)
+-__gen4_read(8)
+-__gen4_read(16)
+-__gen4_read(32)
+-__gen4_read(64)
+ 
+ #undef __chv_read
+ #undef __vlv_read
+ #undef __gen6_read
+-#undef __gen5_read
+-#undef __gen4_read
+-#undef REG_READ_FOOTER
+-#undef REG_READ_HEADER
++#undef GEN6_READ_FOOTER
++#undef GEN6_READ_HEADER
+ 
+-#define REG_WRITE_HEADER \
+-	unsigned long irqflags; \
++#define GEN2_WRITE_HEADER \
+ 	trace_i915_reg_rw(true, reg, val, sizeof(val), trace); \
+ 	assert_device_not_suspended(dev_priv); \
+-	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags)
+ 
+-#define REG_WRITE_FOOTER \
+-	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags)
++#define GEN2_WRITE_FOOTER
+ 
+-#define __gen4_write(x) \
++#define __gen2_write(x) \
+ static void \
+-gen4_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
+-	REG_WRITE_HEADER; \
++gen2_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
++	GEN2_WRITE_HEADER; \
+ 	__raw_i915_write##x(dev_priv, reg, val); \
+-	REG_WRITE_FOOTER; \
++	GEN2_WRITE_FOOTER; \
+ }
+ 
+ #define __gen5_write(x) \
+ static void \
+ gen5_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
+-	REG_WRITE_HEADER; \
++	GEN2_WRITE_HEADER; \
+ 	ilk_dummy_write(dev_priv); \
+ 	__raw_i915_write##x(dev_priv, reg, val); \
+-	REG_WRITE_FOOTER; \
++	GEN2_WRITE_FOOTER; \
+ }
+ 
++__gen5_write(8)
++__gen5_write(16)
++__gen5_write(32)
++__gen5_write(64)
++__gen2_write(8)
++__gen2_write(16)
++__gen2_write(32)
++__gen2_write(64)
++
++#undef __gen5_write
++#undef __gen2_write
++
++#undef GEN2_WRITE_FOOTER
++#undef GEN2_WRITE_HEADER
++
++#define GEN6_WRITE_HEADER \
++	unsigned long irqflags; \
++	trace_i915_reg_rw(true, reg, val, sizeof(val), trace); \
++	assert_device_not_suspended(dev_priv); \
++	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags)
++
++#define GEN6_WRITE_FOOTER \
++	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags)
++
+ #define __gen6_write(x) \
+ static void \
+ gen6_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
+ 	u32 __fifo_ret = 0; \
+-	REG_WRITE_HEADER; \
++	GEN6_WRITE_HEADER; \
+ 	if (NEEDS_FORCE_WAKE((dev_priv), (reg))) { \
+ 		__fifo_ret = __gen6_gt_wait_for_fifo(dev_priv); \
+ 	} \
+@@ -701,14 +801,14 @@
+ 	if (unlikely(__fifo_ret)) { \
+ 		gen6_gt_check_fifodbg(dev_priv); \
+ 	} \
+-	REG_WRITE_FOOTER; \
++	GEN6_WRITE_FOOTER; \
+ }
+ 
+ #define __hsw_write(x) \
+ static void \
+ hsw_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
+ 	u32 __fifo_ret = 0; \
+-	REG_WRITE_HEADER; \
++	GEN6_WRITE_HEADER; \
+ 	if (NEEDS_FORCE_WAKE((dev_priv), (reg))) { \
+ 		__fifo_ret = __gen6_gt_wait_for_fifo(dev_priv); \
+ 	} \
+@@ -719,7 +819,7 @@
+ 	} \
+ 	hsw_unclaimed_reg_debug(dev_priv, reg, false, false); \
+ 	hsw_unclaimed_reg_detect(dev_priv); \
+-	REG_WRITE_FOOTER; \
++	GEN6_WRITE_FOOTER; \
+ }
+ 
+ static const u32 gen8_shadowed_regs[] = {
+@@ -746,50 +846,31 @@
+ #define __gen8_write(x) \
+ static void \
+ gen8_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
+-	REG_WRITE_HEADER; \
++	GEN6_WRITE_HEADER; \
+ 	hsw_unclaimed_reg_debug(dev_priv, reg, false, true); \
+-	if (reg < 0x40000 && !is_gen8_shadowed(dev_priv, reg)) { \
+-		if (dev_priv->uncore.forcewake_count == 0) \
+-			dev_priv->uncore.funcs.force_wake_get(dev_priv,	\
+-							      FORCEWAKE_ALL); \
+-		__raw_i915_write##x(dev_priv, reg, val); \
+-		if (dev_priv->uncore.forcewake_count == 0) \
+-			dev_priv->uncore.funcs.force_wake_put(dev_priv, \
+-							      FORCEWAKE_ALL); \
+-	} else { \
+-		__raw_i915_write##x(dev_priv, reg, val); \
+-	} \
++	if (reg < 0x40000 && !is_gen8_shadowed(dev_priv, reg)) \
++		__force_wake_get(dev_priv, FORCEWAKE_RENDER); \
++	__raw_i915_write##x(dev_priv, reg, val); \
+ 	hsw_unclaimed_reg_debug(dev_priv, reg, false, false); \
+ 	hsw_unclaimed_reg_detect(dev_priv); \
+-	REG_WRITE_FOOTER; \
++	GEN6_WRITE_FOOTER; \
+ }
+ 
+ #define __chv_write(x) \
+ static void \
+ chv_write##x(struct drm_i915_private *dev_priv, off_t reg, u##x val, bool trace) { \
+-	unsigned fwengine = 0; \
+ 	bool shadowed = is_gen8_shadowed(dev_priv, reg); \
+-	REG_WRITE_HEADER; \
++	GEN6_WRITE_HEADER; \
+ 	if (!shadowed) { \
+-		if (FORCEWAKE_CHV_RENDER_RANGE_OFFSET(reg)) { \
+-			if (dev_priv->uncore.fw_rendercount == 0) \
+-				fwengine = FORCEWAKE_RENDER; \
+-		} else if (FORCEWAKE_CHV_MEDIA_RANGE_OFFSET(reg)) { \
+-			if (dev_priv->uncore.fw_mediacount == 0) \
+-				fwengine = FORCEWAKE_MEDIA; \
+-		} else if (FORCEWAKE_CHV_COMMON_RANGE_OFFSET(reg)) { \
+-			if (dev_priv->uncore.fw_rendercount == 0) \
+-				fwengine |= FORCEWAKE_RENDER; \
+-			if (dev_priv->uncore.fw_mediacount == 0) \
+-				fwengine |= FORCEWAKE_MEDIA; \
+-		} \
++		if (FORCEWAKE_CHV_RENDER_RANGE_OFFSET(reg)) \
++			__force_wake_get(dev_priv, FORCEWAKE_RENDER); \
++		else if (FORCEWAKE_CHV_MEDIA_RANGE_OFFSET(reg)) \
++			__force_wake_get(dev_priv, FORCEWAKE_MEDIA); \
++		else if (FORCEWAKE_CHV_COMMON_RANGE_OFFSET(reg)) \
++			__force_wake_get(dev_priv, FORCEWAKE_RENDER | FORCEWAKE_MEDIA); \
+ 	} \
+-	if (fwengine) \
+-		dev_priv->uncore.funcs.force_wake_get(dev_priv, fwengine); \
+ 	__raw_i915_write##x(dev_priv, reg, val); \
+-	if (fwengine) \
+-		dev_priv->uncore.funcs.force_wake_put(dev_priv, fwengine); \
+-	REG_WRITE_FOOTER; \
++	GEN6_WRITE_FOOTER; \
+ }
+ 
+ __chv_write(8)
+@@ -808,39 +889,49 @@
+ __gen6_write(16)
+ __gen6_write(32)
+ __gen6_write(64)
+-__gen5_write(8)
+-__gen5_write(16)
+-__gen5_write(32)
+-__gen5_write(64)
+-__gen4_write(8)
+-__gen4_write(16)
+-__gen4_write(32)
+-__gen4_write(64)
+ 
+ #undef __chv_write
+ #undef __gen8_write
+ #undef __hsw_write
+ #undef __gen6_write
+-#undef __gen5_write
+-#undef __gen4_write
+-#undef REG_WRITE_FOOTER
+-#undef REG_WRITE_HEADER
++#undef GEN6_WRITE_FOOTER
++#undef GEN6_WRITE_HEADER
++
++#define ASSIGN_WRITE_MMIO_VFUNCS(x) \
++do { \
++	dev_priv->uncore.funcs.mmio_writeb = x##_write8; \
++	dev_priv->uncore.funcs.mmio_writew = x##_write16; \
++	dev_priv->uncore.funcs.mmio_writel = x##_write32; \
++	dev_priv->uncore.funcs.mmio_writeq = x##_write64; \
++} while (0)
++
++#define ASSIGN_READ_MMIO_VFUNCS(x) \
++do { \
++	dev_priv->uncore.funcs.mmio_readb = x##_read8; \
++	dev_priv->uncore.funcs.mmio_readw = x##_read16; \
++	dev_priv->uncore.funcs.mmio_readl = x##_read32; \
++	dev_priv->uncore.funcs.mmio_readq = x##_read64; \
++} while (0)
+ 
+ void intel_uncore_init(struct drm_device *dev)
+ {
+ 	struct drm_i915_private *dev_priv = dev->dev_private;
++	int i;
+ 
+-	setup_timer(&dev_priv->uncore.force_wake_timer,
+-		    gen6_force_wake_timer, (unsigned long)dev_priv);
+-
+-	intel_uncore_early_sanitize(dev, false);
++	__intel_uncore_early_sanitize(dev, false);
+ 
+-	if (IS_VALLEYVIEW(dev)) {
++	if (IS_GEN9(dev)) {
++		dev_priv->uncore.funcs.force_wake_get = __gen9_force_wake_get;
++		dev_priv->uncore.funcs.force_wake_put = __gen9_force_wake_put;
++		dev_priv->uncore.fw_domains = FORCEWAKE_RENDER | FORCEWAKE_MEDIA | FORCEWAKE_BLITTER;
++	} else if (IS_VALLEYVIEW(dev)) {
+ 		dev_priv->uncore.funcs.force_wake_get = __vlv_force_wake_get;
+ 		dev_priv->uncore.funcs.force_wake_put = __vlv_force_wake_put;
+-	} else if (IS_HASWELL(dev) || IS_GEN8(dev)) {
++		dev_priv->uncore.fw_domains = FORCEWAKE_RENDER | FORCEWAKE_MEDIA;
++	} else if (IS_HASWELL(dev) || IS_BROADWELL(dev)) {
+ 		dev_priv->uncore.funcs.force_wake_get = __gen7_gt_force_wake_mt_get;
+ 		dev_priv->uncore.funcs.force_wake_put = __gen7_gt_force_wake_mt_put;
++		dev_priv->uncore.fw_domains = FORCEWAKE_RENDER;
+ 	} else if (IS_IVYBRIDGE(dev)) {
+ 		u32 ecobus;
+ 
+@@ -872,86 +963,65 @@
+ 			dev_priv->uncore.funcs.force_wake_put =
+ 				__gen6_gt_force_wake_put;
+ 		}
++		dev_priv->uncore.fw_domains = FORCEWAKE_RENDER;
+ 	} else if (IS_GEN6(dev)) {
+ 		dev_priv->uncore.funcs.force_wake_get =
+ 			__gen6_gt_force_wake_get;
+ 		dev_priv->uncore.funcs.force_wake_put =
+ 			__gen6_gt_force_wake_put;
++		dev_priv->uncore.fw_domains = FORCEWAKE_RENDER;
++	}
++
++	for (i = 0; i < FW_DOMAIN_COUNT; i++) {
++		dev_priv->uncore.fw_domain[i].i915 = dev_priv;
++		dev_priv->uncore.fw_domain[i].id = i;
++
++		setup_timer(&dev_priv->uncore.fw_domain[i].timer,
++			    gen6_force_wake_timer,
++			    (unsigned long)&dev_priv->uncore.fw_domain[i]);
+ 	}
+ 
+ 	switch (INTEL_INFO(dev)->gen) {
+ 	default:
+ 		if (IS_CHERRYVIEW(dev)) {
+-			dev_priv->uncore.funcs.mmio_writeb  = chv_write8;
+-			dev_priv->uncore.funcs.mmio_writew  = chv_write16;
+-			dev_priv->uncore.funcs.mmio_writel  = chv_write32;
+-			dev_priv->uncore.funcs.mmio_writeq  = chv_write64;
+-			dev_priv->uncore.funcs.mmio_readb  = chv_read8;
+-			dev_priv->uncore.funcs.mmio_readw  = chv_read16;
+-			dev_priv->uncore.funcs.mmio_readl  = chv_read32;
+-			dev_priv->uncore.funcs.mmio_readq  = chv_read64;
++			ASSIGN_WRITE_MMIO_VFUNCS(chv);
++			ASSIGN_READ_MMIO_VFUNCS(chv);
+ 
+ 		} else {
+-			dev_priv->uncore.funcs.mmio_writeb  = gen8_write8;
+-			dev_priv->uncore.funcs.mmio_writew  = gen8_write16;
+-			dev_priv->uncore.funcs.mmio_writel  = gen8_write32;
+-			dev_priv->uncore.funcs.mmio_writeq  = gen8_write64;
+-			dev_priv->uncore.funcs.mmio_readb  = gen6_read8;
+-			dev_priv->uncore.funcs.mmio_readw  = gen6_read16;
+-			dev_priv->uncore.funcs.mmio_readl  = gen6_read32;
+-			dev_priv->uncore.funcs.mmio_readq  = gen6_read64;
++			ASSIGN_WRITE_MMIO_VFUNCS(gen8);
++			ASSIGN_READ_MMIO_VFUNCS(gen6);
+ 		}
+ 		break;
+ 	case 7:
+ 	case 6:
+ 		if (IS_HASWELL(dev)) {
+-			dev_priv->uncore.funcs.mmio_writeb  = hsw_write8;
+-			dev_priv->uncore.funcs.mmio_writew  = hsw_write16;
+-			dev_priv->uncore.funcs.mmio_writel  = hsw_write32;
+-			dev_priv->uncore.funcs.mmio_writeq  = hsw_write64;
++			ASSIGN_WRITE_MMIO_VFUNCS(hsw);
+ 		} else {
+-			dev_priv->uncore.funcs.mmio_writeb  = gen6_write8;
+-			dev_priv->uncore.funcs.mmio_writew  = gen6_write16;
+-			dev_priv->uncore.funcs.mmio_writel  = gen6_write32;
+-			dev_priv->uncore.funcs.mmio_writeq  = gen6_write64;
++			ASSIGN_WRITE_MMIO_VFUNCS(gen6);
+ 		}
+ 
+ 		if (IS_VALLEYVIEW(dev)) {
+-			dev_priv->uncore.funcs.mmio_readb  = vlv_read8;
+-			dev_priv->uncore.funcs.mmio_readw  = vlv_read16;
+-			dev_priv->uncore.funcs.mmio_readl  = vlv_read32;
+-			dev_priv->uncore.funcs.mmio_readq  = vlv_read64;
++			ASSIGN_READ_MMIO_VFUNCS(vlv);
+ 		} else {
+-			dev_priv->uncore.funcs.mmio_readb  = gen6_read8;
+-			dev_priv->uncore.funcs.mmio_readw  = gen6_read16;
+-			dev_priv->uncore.funcs.mmio_readl  = gen6_read32;
+-			dev_priv->uncore.funcs.mmio_readq  = gen6_read64;
++			ASSIGN_READ_MMIO_VFUNCS(gen6);
+ 		}
+ 		break;
+ 	case 5:
+-		dev_priv->uncore.funcs.mmio_writeb  = gen5_write8;
+-		dev_priv->uncore.funcs.mmio_writew  = gen5_write16;
+-		dev_priv->uncore.funcs.mmio_writel  = gen5_write32;
+-		dev_priv->uncore.funcs.mmio_writeq  = gen5_write64;
+-		dev_priv->uncore.funcs.mmio_readb  = gen5_read8;
+-		dev_priv->uncore.funcs.mmio_readw  = gen5_read16;
+-		dev_priv->uncore.funcs.mmio_readl  = gen5_read32;
+-		dev_priv->uncore.funcs.mmio_readq  = gen5_read64;
++		ASSIGN_WRITE_MMIO_VFUNCS(gen5);
++		ASSIGN_READ_MMIO_VFUNCS(gen5);
+ 		break;
+ 	case 4:
+ 	case 3:
+ 	case 2:
+-		dev_priv->uncore.funcs.mmio_writeb  = gen4_write8;
+-		dev_priv->uncore.funcs.mmio_writew  = gen4_write16;
+-		dev_priv->uncore.funcs.mmio_writel  = gen4_write32;
+-		dev_priv->uncore.funcs.mmio_writeq  = gen4_write64;
+-		dev_priv->uncore.funcs.mmio_readb  = gen4_read8;
+-		dev_priv->uncore.funcs.mmio_readw  = gen4_read16;
+-		dev_priv->uncore.funcs.mmio_readl  = gen4_read32;
+-		dev_priv->uncore.funcs.mmio_readq  = gen4_read64;
++		ASSIGN_WRITE_MMIO_VFUNCS(gen2);
++		ASSIGN_READ_MMIO_VFUNCS(gen2);
+ 		break;
+ 	}
++
++	i915_check_and_clear_faults(dev);
+ }
++#undef ASSIGN_WRITE_MMIO_VFUNCS
++#undef ASSIGN_READ_MMIO_VFUNCS
+ 
+ void intel_uncore_fini(struct drm_device *dev)
+ {
+@@ -968,7 +1038,7 @@
+ 	/* supported gens, 0x10 for 4, 0x30 for 4 and 5, etc. */
+ 	uint32_t gen_bitmask;
+ } whitelist[] = {
+-	{ RING_TIMESTAMP(RENDER_RING_BASE), 8, GEN_RANGE(4, 8) },
++	{ RING_TIMESTAMP(RENDER_RING_BASE), 8, GEN_RANGE(4, 9) },
+ };
+ 
+ int i915_reg_read_ioctl(struct drm_device *dev,
+@@ -1026,9 +1096,6 @@
+ 	if (args->flags || args->pad)
+ 		return -EINVAL;
+ 
+-	if (args->ctx_id == DEFAULT_CONTEXT_HANDLE && !capable(CAP_SYS_ADMIN))
+-		return -EPERM;
+-
+ 	ret = mutex_lock_interruptible(&dev->struct_mutex);
+ 	if (ret)
+ 		return ret;
+diff -urN -x arch a/drivers/gpu/drm/i915/Makefile b/drivers/gpu/drm/i915/Makefile
+--- a/drivers/gpu/drm/i915/Makefile	2014-10-30 10:43:25.000000000 -0600
++++ b/drivers/gpu/drm/i915/Makefile	2014-11-21 07:55:49.538444939 -0700
+@@ -11,37 +11,45 @@
+ 	  i915_params.o \
+           i915_suspend.o \
+ 	  i915_sysfs.o \
+-	  intel_pm.o
++	  intel_pm.o \
++	  intel_runtime_pm.o
++
+ i915-$(CONFIG_COMPAT)   += i915_ioc32.o
+ i915-$(CONFIG_DEBUG_FS) += i915_debugfs.o
++i915-$(CONFIG_PERF_EVENTS) += i915_perf.o
+ 
+ # GEM code
+ i915-y += i915_cmd_parser.o \
++	  i915_gem.o \
+ 	  i915_gem_context.o \
+ 	  i915_gem_render_state.o \
+-	  i915_gem_debug.o \
+ 	  i915_gem_dmabuf.o \
+ 	  i915_gem_evict.o \
+ 	  i915_gem_execbuffer.o \
+ 	  i915_gem_gtt.o \
+-	  i915_gem.o \
++	  i915_gem_request.o \
+ 	  i915_gem_stolen.o \
+ 	  i915_gem_tiling.o \
+ 	  i915_gem_userptr.o \
+ 	  i915_gpu_error.o \
+ 	  i915_irq.o \
+ 	  i915_trace_points.o \
++	  intel_lrc.o \
+ 	  intel_ringbuffer.o \
+ 	  intel_uncore.o
+ 
+ # autogenerated null render state
+ i915-y += intel_renderstate_gen6.o \
+ 	  intel_renderstate_gen7.o \
+-	  intel_renderstate_gen8.o
++	  intel_renderstate_gen8.o \
++	  intel_renderstate_gen9.o
+ 
+ # modesetting core code
+-i915-y += intel_bios.o \
++i915-y += intel_audio.o \
++	  intel_bios.o \
+ 	  intel_display.o \
++	  intel_fifo_underrun.o \
++	  intel_frontbuffer.o \
+ 	  intel_modes.o \
+ 	  intel_overlay.o \
+ 	  intel_sideband.o \
+diff -urN -x arch a/include/drm/drm_crtc.h b/include/drm/drm_crtc.h
+--- a/include/drm/drm_crtc.h	2014-10-30 10:43:25.000000000 -0600
++++ b/include/drm/drm_crtc.h	2014-11-21 15:03:09.422416557 -0700
+@@ -567,6 +567,39 @@
+ };
+ 
+ /**
++ * struct drm_plane_state - mutable plane state
++ * @crtc: currently bound CRTC, NULL if disabled
++ * @fb: currently bound framebuffer
++ * @fence: optional fence to wait for before scanning out @fb
++ * @crtc_x: left position of visible portion of plane on crtc
++ * @crtc_y: upper position of visible portion of plane on crtc
++ * @crtc_w: width of visible portion of plane on crtc
++ * @crtc_h: height of visible portion of plane on crtc
++ * @src_x: left position of visible portion of plane within
++ *	plane (in 16.16)
++ * @src_y: upper position of visible portion of plane within
++ *	plane (in 16.16)
++ * @src_w: width of visible portion of plane (in 16.16)
++ * @src_h: height of visible portion of plane (in 16.16)
++ * @state: backpointer to global drm_atomic_state
++ */
++struct drm_plane_state {
++	struct drm_crtc *crtc;
++	struct drm_framebuffer *fb;
++	struct fence *fence;
++
++	/* Signed dest location allows it to be partially off screen */
++	int32_t crtc_x, crtc_y;
++	uint32_t crtc_w, crtc_h;
++
++	/* Source values are 16.16 fixed point */
++	uint32_t src_x, src_y;
++	uint32_t src_h, src_w;
++
++	struct drm_atomic_state *state;
++};
++
++/**
+  * drm_plane_funcs - driver plane control functions
+  * @update_plane: update the plane configuration
+  * @disable_plane: shut down the plane
+@@ -611,6 +644,8 @@
+ 	struct drm_device *dev;
+ 	struct list_head head;
+ 
++	struct drm_modeset_lock mutex;
++
+ 	struct drm_mode_object base;
+ 
+ 	uint32_t possible_crtcs;
+@@ -620,11 +655,17 @@
+ 	struct drm_crtc *crtc;
+ 	struct drm_framebuffer *fb;
+ 
++	struct drm_framebuffer *old_fb;
++
+ 	const struct drm_plane_funcs *funcs;
+ 
+ 	struct drm_object_properties properties;
+ 
+ 	enum drm_plane_type type;
++
++	void *helper_private;
++
++	struct drm_plane_state *state;
+ };
+ 
+ /**
+@@ -772,14 +813,7 @@
+ 	struct idr crtc_idr; /* use this idr for all IDs, fb, crtc, connector, modes - just makes life easier */
+ 	/* this is limited to one for now */
+ 
+-
+-	/**
+-	 * fb_lock - mutex to protect fb state
+-	 *
+-	 * Besides the global fb list his also protects the fbs list in the
+-	 * file_priv
+-	 */
+-	struct mutex fb_lock;
++	struct mutex fb_lock; /* proctects global and per-file fb lists */
+ 	int num_fb;
+ 	struct list_head fb_list;
+ 
+@@ -821,6 +855,7 @@
+ 	struct drm_property *dpms_property;
+ 	struct drm_property *path_property;
+ 	struct drm_property *plane_type_property;
++	struct drm_property *rotation_property;
+ 
+ 	/* DVI-I properties */
+ 	struct drm_property *dvi_i_subconnector_property;
+@@ -846,6 +881,10 @@
+ 	struct drm_property *aspect_ratio_property;
+ 	struct drm_property *dirty_info_property;
+ 
++	/* properties for virtual machine layout */
++	struct drm_property *suggested_x_property;
++	struct drm_property *suggested_y_property;
++
+ 	/* dumb ioctl parameters */
+ 	uint32_t preferred_depth, prefer_shadow;
+ 
+@@ -971,9 +1010,9 @@
+ extern void drm_mode_config_cleanup(struct drm_device *dev);
+ 
+ extern int drm_mode_connector_set_path_property(struct drm_connector *connector,
+-						char *path);
++						const char *path);
+ extern int drm_mode_connector_update_edid_property(struct drm_connector *connector,
+-						struct edid *edid);
++						   const struct edid *edid);
+ 
+ static inline bool drm_property_type_is(struct drm_property *property,
+ 		uint32_t type)
+diff -urN -x arch a/include/drm/drm_dp_helper.h b/include/drm/drm_dp_helper.h
+--- a/include/drm/drm_dp_helper.h	2014-10-30 10:43:25.000000000 -0600
++++ b/include/drm/drm_dp_helper.h	2014-11-21 13:50:07.445852635 -0700
+@@ -194,12 +194,20 @@
+ # define DP_TRAIN_VOLTAGE_SWING_600	    (1 << 0)
+ # define DP_TRAIN_VOLTAGE_SWING_800	    (2 << 0)
+ # define DP_TRAIN_VOLTAGE_SWING_1200	    (3 << 0)
++# define DP_TRAIN_VOLTAGE_SWING_LEVEL_0 (0 << 0)
++# define DP_TRAIN_VOLTAGE_SWING_LEVEL_1 (1 << 0)
++# define DP_TRAIN_VOLTAGE_SWING_LEVEL_2 (2 << 0)
++# define DP_TRAIN_VOLTAGE_SWING_LEVEL_3 (3 << 0)
+ 
+ # define DP_TRAIN_PRE_EMPHASIS_MASK	    (3 << 3)
+ # define DP_TRAIN_PRE_EMPHASIS_0	    (0 << 3)
+ # define DP_TRAIN_PRE_EMPHASIS_3_5	    (1 << 3)
+ # define DP_TRAIN_PRE_EMPHASIS_6	    (2 << 3)
+ # define DP_TRAIN_PRE_EMPHASIS_9_5	    (3 << 3)
++# define DP_TRAIN_PRE_EMPH_LEVEL_0	    (0 << 3)
++# define DP_TRAIN_PRE_EMPH_LEVEL_1	    (1 << 3)
++# define DP_TRAIN_PRE_EMPH_LEVEL_2	    (2 << 3)
++# define DP_TRAIN_PRE_EMPH_LEVEL_3	    (3 << 3)
+ 
+ # define DP_TRAIN_PRE_EMPHASIS_SHIFT	    3
+ # define DP_TRAIN_MAX_PRE_EMPHASIS_REACHED  (1 << 5)
+@@ -304,6 +312,7 @@
+ 
+ #define DP_TEST_SINK_MISC		    0x246
+ #define DP_TEST_CRC_SUPPORTED		    (1 << 5)
++#define DP_TEST_COUNT_MASK		    0x7
+ 
+ #define DP_TEST_RESPONSE		    0x260
+ # define DP_TEST_ACK			    (1 << 0)
+diff -urN -x arch a/include/drm/drm_dp_mst_helper.h b/include/drm/drm_dp_mst_helper.h
+--- a/include/drm/drm_dp_mst_helper.h	2014-10-30 10:43:25.000000000 -0600
++++ b/include/drm/drm_dp_mst_helper.h	2014-11-21 14:07:33.553320749 -0700
+@@ -371,7 +371,7 @@
+ struct drm_dp_mst_topology_mgr;
+ struct drm_dp_mst_topology_cbs {
+ 	/* create a connector for a port */
+-	struct drm_connector *(*add_connector)(struct drm_dp_mst_topology_mgr *mgr, struct drm_dp_mst_port *port, char *path);
++	struct drm_connector *(*add_connector)(struct drm_dp_mst_topology_mgr *mgr, struct drm_dp_mst_port *port, const char *path);
+ 	void (*destroy_connector)(struct drm_dp_mst_topology_mgr *mgr,
+ 				  struct drm_connector *connector);
+ 	void (*hotplug)(struct drm_dp_mst_topology_mgr *mgr);
+diff -urN -x arch a/include/drm/drm_edid.h b/include/drm/drm_edid.h
+--- a/include/drm/drm_edid.h	2014-10-30 10:43:25.000000000 -0600
++++ b/include/drm/drm_edid.h	2014-11-21 13:26:57.248359345 -0700
+@@ -207,6 +207,61 @@
+ #define DRM_EDID_HDMI_DC_30               (1 << 4)
+ #define DRM_EDID_HDMI_DC_Y444             (1 << 3)
+ 
++/* ELD Header Block */
++#define DRM_ELD_HEADER_BLOCK_SIZE	4
++
++#define DRM_ELD_VER			0
++# define DRM_ELD_VER_SHIFT		3
++# define DRM_ELD_VER_MASK		(0x1f << 3)
++
++#define DRM_ELD_BASELINE_ELD_LEN	2	/* in dwords! */
++
++/* ELD Baseline Block for ELD_Ver == 2 */
++#define DRM_ELD_CEA_EDID_VER_MNL	4
++# define DRM_ELD_CEA_EDID_VER_SHIFT	5
++# define DRM_ELD_CEA_EDID_VER_MASK	(7 << 5)
++# define DRM_ELD_CEA_EDID_VER_NONE	(0 << 5)
++# define DRM_ELD_CEA_EDID_VER_CEA861	(1 << 5)
++# define DRM_ELD_CEA_EDID_VER_CEA861A	(2 << 5)
++# define DRM_ELD_CEA_EDID_VER_CEA861BCD	(3 << 5)
++# define DRM_ELD_MNL_SHIFT		0
++# define DRM_ELD_MNL_MASK		(0x1f << 0)
++
++#define DRM_ELD_SAD_COUNT_CONN_TYPE	5
++# define DRM_ELD_SAD_COUNT_SHIFT	4
++# define DRM_ELD_SAD_COUNT_MASK		(0xf << 4)
++# define DRM_ELD_CONN_TYPE_SHIFT	2
++# define DRM_ELD_CONN_TYPE_MASK		(3 << 2)
++# define DRM_ELD_CONN_TYPE_HDMI		(0 << 2)
++# define DRM_ELD_CONN_TYPE_DP		(1 << 2)
++# define DRM_ELD_SUPPORTS_AI		(1 << 1)
++# define DRM_ELD_SUPPORTS_HDCP		(1 << 0)
++
++#define DRM_ELD_AUD_SYNCH_DELAY		6	/* in units of 2 ms */
++# define DRM_ELD_AUD_SYNCH_DELAY_MAX	0xfa	/* 500 ms */
++
++#define DRM_ELD_SPEAKER			7
++# define DRM_ELD_SPEAKER_RLRC		(1 << 6)
++# define DRM_ELD_SPEAKER_FLRC		(1 << 5)
++# define DRM_ELD_SPEAKER_RC		(1 << 4)
++# define DRM_ELD_SPEAKER_RLR		(1 << 3)
++# define DRM_ELD_SPEAKER_FC		(1 << 2)
++# define DRM_ELD_SPEAKER_LFE		(1 << 1)
++# define DRM_ELD_SPEAKER_FLR		(1 << 0)
++
++#define DRM_ELD_PORT_ID			8	/* offsets 8..15 inclusive */
++# define DRM_ELD_PORT_ID_LEN		8
++
++#define DRM_ELD_MANUFACTURER_NAME0	16
++#define DRM_ELD_MANUFACTURER_NAME1	17
++
++#define DRM_ELD_PRODUCT_CODE0		18
++#define DRM_ELD_PRODUCT_CODE1		19
++
++#define DRM_ELD_MONITOR_NAME_STRING	20	/* offsets 20..(20+mnl-1) inclusive */
++
++#define DRM_ELD_CEA_SAD(mnl, sad)	(20 + (mnl) + 3 * (sad))
++
+ struct edid {
+ 	u8 header[8];
+ 	/* Vendor & product info */
+@@ -279,4 +334,51 @@
+ drm_hdmi_vendor_infoframe_from_display_mode(struct hdmi_vendor_infoframe *frame,
+ 					    const struct drm_display_mode *mode);
+ 
++/**
++ * drm_eld_mnl - Get ELD monitor name length in bytes.
++ * @eld: pointer to an eld memory structure with mnl set
++ */
++static inline int drm_eld_mnl(const uint8_t *eld)
++{
++	return (eld[DRM_ELD_CEA_EDID_VER_MNL] & DRM_ELD_MNL_MASK) >> DRM_ELD_MNL_SHIFT;
++}
++
++/**
++ * drm_eld_sad_count - Get ELD SAD count.
++ * @eld: pointer to an eld memory structure with sad_count set
++ */
++static inline int drm_eld_sad_count(const uint8_t *eld)
++{
++	return (eld[DRM_ELD_SAD_COUNT_CONN_TYPE] & DRM_ELD_SAD_COUNT_MASK) >>
++		DRM_ELD_SAD_COUNT_SHIFT;
++}
++
++/**
++ * drm_eld_calc_baseline_block_size - Calculate baseline block size in bytes
++ * @eld: pointer to an eld memory structure with mnl and sad_count set
++ *
++ * This is a helper for determining the payload size of the baseline block, in
++ * bytes, for e.g. setting the Baseline_ELD_Len field in the ELD header block.
++ */
++static inline int drm_eld_calc_baseline_block_size(const uint8_t *eld)
++{
++	return DRM_ELD_MONITOR_NAME_STRING - DRM_ELD_HEADER_BLOCK_SIZE +
++		drm_eld_mnl(eld) + drm_eld_sad_count(eld) * 3;
++}
++
++/**
++ * drm_eld_size - Get ELD size in bytes
++ * @eld: pointer to a complete eld memory structure
++ *
++ * The returned value does not include the vendor block. It's vendor specific,
++ * and comprises of the remaining bytes in the ELD memory buffer after
++ * drm_eld_size() bytes of header and baseline block.
++ *
++ * The returned value is guaranteed to be a multiple of 4.
++ */
++static inline int drm_eld_size(const uint8_t *eld)
++{
++	return DRM_ELD_HEADER_BLOCK_SIZE + eld[DRM_ELD_BASELINE_ELD_LEN] * 4;
++}
++
+ #endif /* __DRM_EDID_H__ */
+diff -urN -x arch a/include/drm/drm_legacy.h b/include/drm/drm_legacy.h
+--- a/include/drm/drm_legacy.h	1969-12-31 17:00:00.000000000 -0700
++++ b/include/drm/drm_legacy.h	2014-10-30 10:43:25.000000000 -0600
+@@ -0,0 +1,51 @@
++#ifndef __DRM_LEGACY_H__
++#define __DRM_LEGACY_H__
++
++/*
++ * Copyright (c) 2014 David Herrmann <dh.herrmann@gmail.com>
++ *
++ * Permission is hereby granted, free of charge, to any person obtaining a
++ * copy of this software and associated documentation files (the "Software"),
++ * to deal in the Software without restriction, including without limitation
++ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
++ * and/or sell copies of the Software, and to permit persons to whom the
++ * Software is furnished to do so, subject to the following conditions:
++ *
++ * The above copyright notice and this permission notice shall be included in
++ * all copies or substantial portions of the Software.
++ *
++ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
++ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
++ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
++ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
++ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
++ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
++ * OTHER DEALINGS IN THE SOFTWARE.
++ */
++
++struct drm_device;
++struct drm_file;
++
++/*
++ * Generic DRM Contexts
++ */
++
++#define DRM_KERNEL_CONTEXT		0
++#define DRM_RESERVED_CONTEXTS		1
++
++int drm_legacy_ctxbitmap_init(struct drm_device *dev);
++void drm_legacy_ctxbitmap_cleanup(struct drm_device *dev);
++void drm_legacy_ctxbitmap_free(struct drm_device *dev, int ctx_handle);
++void drm_legacy_ctxbitmap_flush(struct drm_device *dev, struct drm_file *file);
++
++int drm_legacy_resctx(struct drm_device *d, void *v, struct drm_file *f);
++int drm_legacy_addctx(struct drm_device *d, void *v, struct drm_file *f);
++int drm_legacy_getctx(struct drm_device *d, void *v, struct drm_file *f);
++int drm_legacy_switchctx(struct drm_device *d, void *v, struct drm_file *f);
++int drm_legacy_newctx(struct drm_device *d, void *v, struct drm_file *f);
++int drm_legacy_rmctx(struct drm_device *d, void *v, struct drm_file *f);
++
++int drm_legacy_setsareactx(struct drm_device *d, void *v, struct drm_file *f);
++int drm_legacy_getsareactx(struct drm_device *d, void *v, struct drm_file *f);
++
++#endif /* __DRM_LEGACY_H__ */
+diff -urN -x arch a/include/drm/drmP.h b/include/drm/drmP.h
+--- a/include/drm/drmP.h	2014-10-30 10:43:25.000000000 -0600
++++ b/include/drm/drmP.h	2014-11-21 14:47:13.615860285 -0700
+@@ -170,6 +170,9 @@
+ #define DRM_ERROR(fmt, ...)				\
+ 	drm_err(__func__, fmt, ##__VA_ARGS__)
+ 
++#define DRM_ERROR_ONCE(fmt, ...)				\
++	printk_once(KERN_ERR "[" DRM_NAME "] " fmt, ##__VA_ARGS__)
++
+ /**
+  * Rate limited error output.  Like DRM_ERROR() but won't flood the log.
+  *
+@@ -681,6 +684,7 @@
+ 
+ /* get_scanout_position() return flags */
+ #define DRM_SCANOUTPOS_VALID        (1 << 0)
++#define DRM_SCANOUTPOS_IN_VBLANK    (1 << 1)
+ #define DRM_SCANOUTPOS_INVBL        (1 << 1)
+ #define DRM_SCANOUTPOS_ACCURATE     (1 << 2)
+ 
+@@ -706,6 +710,7 @@
+ 	int (*dma_ioctl) (struct drm_device *dev, void *data, struct drm_file *file_priv);
+ 	int (*dma_quiescent) (struct drm_device *);
+ 	int (*context_dtor) (struct drm_device *dev, int context);
++	int (*set_busid)(struct drm_device *dev, struct drm_master *master);
+ 
+ 	/**
+ 	 * get_vblank_counter - get raw hardware vblank counter
+@@ -1079,6 +1084,16 @@
+ 	 */
+ 	bool vblank_disable_allowed;
+ 
++	/*
++	 * If true, vblank interrupt will be disabled immediately when the
++	 * refcount drops to zero, as opposed to via the vblank disable
++	 * timer.
++	 * This can be set to true it the hardware has a working vblank
++	 * counter and the driver uses drm_vblank_on() and drm_vblank_off()
++	 * appropriately.
++	 */
++	bool vblank_disable_immediate;
++
+ 	/* array of size num_crtcs */
+ 	struct drm_vblank_crtc *vblank;
+ 
+@@ -1294,6 +1309,7 @@
+ extern void drm_vblank_put(struct drm_device *dev, int crtc);
+ extern int drm_crtc_vblank_get(struct drm_crtc *crtc);
+ extern void drm_crtc_vblank_put(struct drm_crtc *crtc);
++extern void drm_wait_one_vblank(struct drm_device *dev, int crtc);
+ extern void drm_vblank_off(struct drm_device *dev, int crtc);
+ extern void drm_vblank_on(struct drm_device *dev, int crtc);
+ extern void drm_crtc_vblank_off(struct drm_crtc *crtc);
+@@ -1311,6 +1327,17 @@
+ extern void drm_calc_timestamping_constants(struct drm_crtc *crtc,
+ 					    const struct drm_display_mode *mode);
+ 
++/**
++ * drm_crtc_vblank_waitqueue - get vblank waitqueue for the CRTC
++ * @crtc: which CRTC's vblank waitqueue to retrieve
++ *
++ * This function returns a pointer to the vblank waitqueue for the CRTC.
++ * Drivers can use this to implement vblank waits using wait_event() & co.
++ */
++static inline wait_queue_head_t *drm_crtc_vblank_waitqueue(struct drm_crtc *crtc)
++{
++	return &crtc->dev->vblank[drm_crtc_index(crtc)].queue;
++}
+ 
+ /* Modesetting support */
+ extern void drm_vblank_pre_modeset(struct drm_device *dev, int crtc);
+@@ -1584,9 +1611,25 @@
+ 
+ extern int drm_pci_init(struct drm_driver *driver, struct pci_driver *pdriver);
+ extern void drm_pci_exit(struct drm_driver *driver, struct pci_driver *pdriver);
++#ifdef CONFIG_PCI
+ extern int drm_get_pci_dev(struct pci_dev *pdev,
+ 			   const struct pci_device_id *ent,
+ 			   struct drm_driver *driver);
++extern int drm_pci_set_busid(struct drm_device *dev, struct drm_master *master);
++#else
++static inline int drm_get_pci_dev(struct pci_dev *pdev,
++				  const struct pci_device_id *ent,
++				  struct drm_driver *driver)
++{
++	return -ENOSYS;
++}
++
++static inline int drm_pci_set_busid(struct drm_device *dev,
++				    struct drm_master *master)
++{
++	return -ENOSYS;
++}
++#endif
+ 
+ #define DRM_PCIE_SPEED_25 1
+ #define DRM_PCIE_SPEED_50 2
+diff -urN -x arch a/include/drm/i915_pciids.h b/include/drm/i915_pciids.h
+--- a/include/drm/i915_pciids.h	2014-10-30 10:43:25.000000000 -0600
++++ b/include/drm/i915_pciids.h	2014-11-21 08:11:16.275657378 -0700
+@@ -259,4 +259,21 @@
+ 	INTEL_VGA_DEVICE(0x22b2, info), \
+ 	INTEL_VGA_DEVICE(0x22b3, info)
+ 
++#define INTEL_SKL_IDS(info) \
++	INTEL_VGA_DEVICE(0x1916, info), /* ULT GT2 */ \
++	INTEL_VGA_DEVICE(0x1906, info), /* ULT GT1 */ \
++	INTEL_VGA_DEVICE(0x1926, info), /* ULT GT3 */ \
++	INTEL_VGA_DEVICE(0x1921, info), /* ULT GT2F */ \
++	INTEL_VGA_DEVICE(0x190E, info), /* ULX GT1 */ \
++	INTEL_VGA_DEVICE(0x191E, info), /* ULX GT2 */ \
++	INTEL_VGA_DEVICE(0x1912, info), /* DT  GT2 */ \
++	INTEL_VGA_DEVICE(0x1902, info), /* DT  GT1 */ \
++	INTEL_VGA_DEVICE(0x191B, info), /* Halo GT2 */ \
++	INTEL_VGA_DEVICE(0x192B, info), /* Halo GT3 */ \
++	INTEL_VGA_DEVICE(0x190B, info), /* Halo GT1 */ \
++	INTEL_VGA_DEVICE(0x191A, info), /* SRV GT2 */ \
++	INTEL_VGA_DEVICE(0x192A, info), /* SRV GT3 */ \
++	INTEL_VGA_DEVICE(0x190A, info), /* SRV GT1 */ \
++	INTEL_VGA_DEVICE(0x191D, info)  /* WKS GT2 */
++
+ #endif /* _I915_PCIIDS_H */
+diff -urN -x arch a/include/linux/io-mapping.h b/include/linux/io-mapping.h
+--- a/include/linux/io-mapping.h	2014-10-30 10:43:25.000000000 -0600
++++ b/include/linux/io-mapping.h	2014-11-21 09:21:30.032204139 -0700
+@@ -31,16 +31,17 @@
+  * See Documentation/io-mapping.txt
+  */
+ 
+-#ifdef CONFIG_HAVE_ATOMIC_IOMAP
+-
+-#include <asm/iomap.h>
+-
+ struct io_mapping {
+ 	resource_size_t base;
+ 	unsigned long size;
+ 	pgprot_t prot;
++	void __iomem *iomem;
+ };
+ 
++#ifdef CONFIG_HAVE_ATOMIC_IOMAP
++
++#include <asm/iomap.h>
++
+ /*
+  * For small address space machines, mapping large objects
+  * into the kernel virtual space isn't practical. Where
+diff -urN -x arch a/include/linux/mm.h b/include/linux/mm.h
+--- a/include/linux/mm.h	2014-10-30 10:43:25.000000000 -0600
++++ b/include/linux/mm.h	2014-11-21 21:54:11.525087701 -0700
+@@ -1973,6 +1973,10 @@
+ struct vm_area_struct *find_extend_vma(struct mm_struct *, unsigned long addr);
+ int remap_pfn_range(struct vm_area_struct *, unsigned long addr,
+ 			unsigned long pfn, unsigned long size, pgprot_t);
++struct io_mapping;
++int remap_io_mapping(struct vm_area_struct *,
++                     unsigned long addr, unsigned long pfn, unsigned long size,
++                     struct io_mapping *iomap);
+ int vm_insert_page(struct vm_area_struct *, unsigned long addr, struct page *);
+ int vm_insert_pfn(struct vm_area_struct *vma, unsigned long addr,
+ 			unsigned long pfn);
+diff -urN -x arch a/include/linux/mutex.h b/include/linux/mutex.h
+--- a/include/linux/mutex.h	2014-10-30 10:43:25.000000000 -0600
++++ b/include/linux/mutex.h	2014-11-21 12:04:02.582118598 -0700
+@@ -143,10 +143,15 @@
+ 					unsigned int subclass);
+ extern int __must_check mutex_lock_killable_nested(struct mutex *lock,
+ 					unsigned int subclass);
++extern int __must_check mutex_lock_wrapper_nested(struct mutex *lock,
++						  unsigned int subclass,
++						  long state,
++						  unsigned long ip);
+ 
+ #define mutex_lock(lock) mutex_lock_nested(lock, 0)
+ #define mutex_lock_interruptible(lock) mutex_lock_interruptible_nested(lock, 0)
+ #define mutex_lock_killable(lock) mutex_lock_killable_nested(lock, 0)
++#define mutex_lock_wrapper(lock, state, ip) mutex_lock_wrapper_nested(lock, 0, state, ip)
+ 
+ #define mutex_lock_nest_lock(lock, nest_lock)				\
+ do {									\
+@@ -158,10 +163,14 @@
+ extern void mutex_lock(struct mutex *lock);
+ extern int __must_check mutex_lock_interruptible(struct mutex *lock);
+ extern int __must_check mutex_lock_killable(struct mutex *lock);
++extern int __must_check mutex_lock_wrapper(struct mutex *lock,
++					   long state,
++					   unsigned long ip);
+ 
+ # define mutex_lock_nested(lock, subclass) mutex_lock(lock)
+ # define mutex_lock_interruptible_nested(lock, subclass) mutex_lock_interruptible(lock)
+ # define mutex_lock_killable_nested(lock, subclass) mutex_lock_killable(lock)
++# define mutex_lock_wrapper_nested(lock, subclass, state, ip) mutex_lock_wrapper(lock, state, ip)
+ # define mutex_lock_nest_lock(lock, nest_lock) mutex_lock(lock)
+ #endif
+ 
+diff -urN -x arch a/include/uapi/drm/i915_drm.h b/include/uapi/drm/i915_drm.h
+--- a/include/uapi/drm/i915_drm.h	2014-10-30 10:43:25.000000000 -0600
++++ b/include/uapi/drm/i915_drm.h	2014-11-21 13:58:52.955778363 -0700
+@@ -58,6 +58,54 @@
+ #define I915_ERROR_UEVENT		"ERROR"
+ #define I915_RESET_UEVENT		"RESET"
+ 
++/**
++ * DOC: perf_events exposed by i915 through /sys/bus/event_sources/drivers/i915
++ *
++ */
++#define I915_SAMPLE_BUSY	0
++#define I915_SAMPLE_WAIT	1
++#define I915_SAMPLE_SEMA	2
++
++#define I915_SAMPLE_RCS		0
++#define I915_SAMPLE_VCS		1
++#define I915_SAMPLE_BCS		2
++#define I915_SAMPLE_VECS	3
++
++#define __I915_PERF_COUNT(ring, id) ((ring) << 4 | (id))
++
++#define I915_PERF_COUNT_RCS_BUSY __I915_PERF_COUNT(I915_SAMPLE_RCS, I915_SAMPLE_BUSY)
++#define I915_PERF_COUNT_RCS_WAIT __I915_PERF_COUNT(I915_SAMPLE_RCS, I915_SAMPLE_WAIT)
++#define I915_PERF_COUNT_RCS_SEMA __I915_PERF_COUNT(I915_SAMPLE_RCS, I915_SAMPLE_SEMA)
++
++#define I915_PERF_COUNT_VCS_BUSY __I915_PERF_COUNT(I915_SAMPLE_VCS, I915_SAMPLE_BUSY)
++#define I915_PERF_COUNT_VCS_WAIT __I915_PERF_COUNT(I915_SAMPLE_VCS, I915_SAMPLE_WAIT)
++#define I915_PERF_COUNT_VCS_SEMA __I915_PERF_COUNT(I915_SAMPLE_VCS, I915_SAMPLE_SEMA)
++
++#define I915_PERF_COUNT_BCS_BUSY __I915_PERF_COUNT(I915_SAMPLE_BCS, I915_SAMPLE_BUSY)
++#define I915_PERF_COUNT_BCS_WAIT __I915_PERF_COUNT(I915_SAMPLE_BCS, I915_SAMPLE_WAIT)
++#define I915_PERF_COUNT_BCS_SEMA __I915_PERF_COUNT(I915_SAMPLE_BCS, I915_SAMPLE_SEMA)
++
++#define I915_PERF_COUNT_VECS_BUSY __I915_PERF_COUNT(I915_SAMPLE_VECS, I915_SAMPLE_BUSY)
++#define I915_PERF_COUNT_VECS_WAIT __I915_PERF_COUNT(I915_SAMPLE_VECS, I915_SAMPLE_WAIT)
++#define I915_PERF_COUNT_VECS_SEMA __I915_PERF_COUNT(I915_SAMPLE_VECS, I915_SAMPLE_SEMA)
++
++#define I915_PERF_ACTUAL_FREQUENCY 32
++#define I915_PERF_REQUESTED_FREQUENCY 33
++#define I915_PERF_ENERGY 34
++#define I915_PERF_INTERRUPTS 35
++
++#define I915_PERF_RC6_RESIDENCY		40
++#define I915_PERF_RC6p_RESIDENCY	41
++#define I915_PERF_RC6pp_RESIDENCY	42
++
++#define I915_PERF_STATISTIC_0	48
++#define I915_PERF_STATISTIC(n)	(I915_PERF_STATISTIC_0+(n))
++#define I915_PERF_STATISTIC_8	I915_PERF_STATISTIC(8)
++
++#define I915_PERF_INSTDONE_0	64
++#define I915_PERF_INSTDONE(n)	(I915_PERF_INSTDONE_0+(n))
++#define I915_PERF_INSTDONE_63	I915_PERF_INSTDONE(64)
++
+ /* Each region is a minimum of 16k, and there are at most 255 of them.
+  */
+ #define I915_NR_TEX_REGIONS 255	/* table size 2k - maximum due to use
+@@ -224,6 +272,9 @@
+ #define DRM_I915_REG_READ		0x31
+ #define DRM_I915_GET_RESET_STATS	0x32
+ #define DRM_I915_GEM_USERPTR		0x33
++#define DRM_I915_GEM_CONTEXT_GETPARAM	0x34
++#define DRM_I915_GEM_CONTEXT_SETPARAM	0x35
++#define DRM_I915_GEM_CONTEXT_DUMP	0x36
+ 
+ #define DRM_IOCTL_I915_INIT		DRM_IOW( DRM_COMMAND_BASE + DRM_I915_INIT, drm_i915_init_t)
+ #define DRM_IOCTL_I915_FLUSH		DRM_IO ( DRM_COMMAND_BASE + DRM_I915_FLUSH)
+@@ -275,6 +326,9 @@
+ #define DRM_IOCTL_I915_REG_READ			DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_REG_READ, struct drm_i915_reg_read)
+ #define DRM_IOCTL_I915_GET_RESET_STATS		DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_GET_RESET_STATS, struct drm_i915_reset_stats)
+ #define DRM_IOCTL_I915_GEM_USERPTR			DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_GEM_USERPTR, struct drm_i915_gem_userptr)
++#define DRM_IOCTL_I915_GEM_CONTEXT_GETPARAM	DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_GEM_CONTEXT_GETPARAM, struct drm_i915_gem_context_param)
++#define DRM_IOCTL_I915_GEM_CONTEXT_SETPARAM	DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_GEM_CONTEXT_SETPARAM, struct drm_i915_gem_context_param)
++#define DRM_IOCTL_I915_GEM_CONTEXT_DUMP	DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_GEM_CONTEXT_DUMP, struct drm_i915_gem_context_dump)
+ 
+ /* Allow drivers to submit batchbuffers directly to hardware, relying
+  * on the security mechanisms provided by hardware.
+@@ -340,6 +394,8 @@
+ #define I915_PARAM_HAS_EXEC_HANDLE_LUT   26
+ #define I915_PARAM_HAS_WT     	 	 27
+ #define I915_PARAM_CMD_PARSER_VERSION	 28
++#define I915_PARAM_HAS_COHERENT_PHYS_GTT 29
++#define I915_PARAM_MMAP_VERSION 30
+ 
+ typedef struct drm_i915_getparam {
+ 	int param;
+@@ -487,6 +543,9 @@
+ 	 * This is a fixed-size type for 32/64 compatibility.
+ 	 */
+ 	__u64 addr_ptr;
++
++	__u64 flags;
++#define I915_MMAP_WC 0x1
+ };
+ 
+ struct drm_i915_gem_mmap_gtt {
+@@ -662,7 +721,8 @@
+ #define EXEC_OBJECT_NEEDS_FENCE (1<<0)
+ #define EXEC_OBJECT_NEEDS_GTT	(1<<1)
+ #define EXEC_OBJECT_WRITE	(1<<2)
+-#define __EXEC_OBJECT_UNKNOWN_FLAGS -(EXEC_OBJECT_WRITE<<1)
++#define EXEC_OBJECT_PINNED	(1<<4)
++#define __EXEC_OBJECT_UNKNOWN_FLAGS -(EXEC_OBJECT_PINNED<<1)
+ 	__u64 flags;
+ 
+ 	__u64 rsvd1;
+@@ -876,6 +936,12 @@
+ 	 * mmap mapping.
+ 	 */
+ 	__u32 swizzle_mode;
++
++	/**
++	 * Returned address bit 6 swizzling required for CPU access through
++	 * mmap mapping whilst bound.
++	 */
++	__u32 phys_swizzle_mode;
+ };
+ 
+ struct drm_i915_gem_get_aperture {
+@@ -887,6 +953,31 @@
+ 	 * bytes
+ 	 */
+ 	__u64 aper_available_size;
++
++	/**
++	 * Total size of the mappable region of the aperture, in bytes
++	 */
++	__u64 map_total_size;
++
++	/**
++	 * Available space in the mappable region of the aperture, in bytes
++	 */
++	__u64 map_available_size;
++
++	/**
++	 * Single largest available region inside the mappable region, in bytes.
++	 */
++	__u64 map_largest_size;
++
++	/**
++	 * Culmulative space available for fences, in bytes
++	 */
++	__u64 fence_available_size;
++
++	/**
++	 * Single largest fenceable region, in bytes.
++	 */
++	__u64 fence_largest_size;
+ };
+ 
+ struct drm_i915_get_pipe_from_crtc_id {
+@@ -900,6 +991,8 @@
+ #define I915_MADV_WILLNEED 0
+ #define I915_MADV_DONTNEED 1
+ #define __I915_MADV_PURGED 2 /* internal state */
++#define I915_MADV_POPULATE   3
++#define I915_MADV_INVALIDATE 4
+ 
+ struct drm_i915_gem_madvise {
+ 	/** Handle of the buffer to change the backing store advice */
+@@ -966,6 +1059,7 @@
+ /* flags */
+ #define I915_OVERLAY_UPDATE_ATTRS	(1<<0)
+ #define I915_OVERLAY_UPDATE_GAMMA	(1<<1)
++#define I915_OVERLAY_DISABLE_DEST_COLORKEY	(1<<2)
+ struct drm_intel_overlay_attrs {
+ 	__u32 flags;
+ 	__u32 color_key;
+@@ -1066,4 +1160,18 @@
+ 	__u32 handle;
+ };
+ 
++struct drm_i915_gem_context_param {
++	__u32 ctx_id;
++	__u32 size;
++	__u64 param;
++#define I915_CONTEXT_PARAM_BAN_PERIOD 0x1
++	__u64 value;
++};
++
++struct drm_i915_gem_context_dump {
++	__u32 ctx_id;
++	__u32 size;
++	__u64 ptr;
++};
++
+ #endif /* _UAPI_I915_DRM_H_ */
+diff -urN -x arch a/kernel/events/core.c b/kernel/events/core.c
+--- a/kernel/events/core.c	2014-10-30 10:43:25.000000000 -0600
++++ b/kernel/events/core.c	2014-11-21 19:20:05.154408727 -0700
+@@ -5509,6 +5509,7 @@
+ {
+ 	return __perf_event_overflow(event, 1, data, regs);
+ }
++EXPORT_SYMBOL_GPL(perf_event_overflow);
+ 
+ /*
+  * Generic software event infrastructure
+diff -urN -x arch a/kernel/locking/mutex.c b/kernel/locking/mutex.c
+--- a/kernel/locking/mutex.c	2014-10-30 10:43:25.000000000 -0600
++++ b/kernel/locking/mutex.c	2014-11-21 11:58:47.660958893 -0700
+@@ -616,6 +616,17 @@
+ 
+ EXPORT_SYMBOL_GPL(mutex_lock_interruptible_nested);
+ 
++int __sched
++mutex_lock_wrapper_nested(struct mutex *lock, unsigned int subclass,
++			  long state, unsigned long ip)
++{
++	might_sleep();
++	return __mutex_lock_common(lock, state,
++				   subclass, NULL, ip, NULL, 0);
++}
++
++EXPORT_SYMBOL_GPL(mutex_lock_wrapper_nested);
++
+ static inline int
+ ww_mutex_deadlock_injection(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)
+ {
+@@ -730,6 +741,9 @@
+ static noinline int __sched
+ __mutex_lock_interruptible_slowpath(struct mutex *lock);
+ 
++static noinline int __sched
++__mutex_lock_wrapper_slowpath(struct mutex *lock, long state, unsigned long ip);
++
+ /**
+  * mutex_lock_interruptible - acquire the mutex, interruptible
+  * @lock: the mutex to be acquired
+@@ -756,6 +770,21 @@
+ 
+ EXPORT_SYMBOL(mutex_lock_interruptible);
+ 
++int __sched mutex_lock_wrapper(struct mutex *lock, long state, unsigned long ip)
++{
++	int ret;
++
++	might_sleep();
++	ret =  __mutex_fastpath_lock_retval(&lock->count);
++	if (likely(!ret)) {
++		mutex_set_owner(lock);
++		return 0;
++	} else
++		return __mutex_lock_wrapper_slowpath(lock, state, ip);
++}
++
++EXPORT_SYMBOL(mutex_lock_wrapper);
++
+ int __sched mutex_lock_killable(struct mutex *lock)
+ {
+ 	int ret;
+@@ -794,6 +823,13 @@
+ }
+ 
+ static noinline int __sched
++__mutex_lock_wrapper_slowpath(struct mutex *lock, long state, unsigned long ip)
++{
++	return __mutex_lock_common(lock, state, 0,
++				   NULL, ip, NULL, 0);
++}
++
++static noinline int __sched
+ __ww_mutex_lock_slowpath(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)
+ {
+ 	return __mutex_lock_common(&lock->base, TASK_UNINTERRUPTIBLE, 0,
+diff -urN -x arch a/mm/memory.c b/mm/memory.c
+--- a/mm/memory.c	2014-10-30 10:43:25.000000000 -0600
++++ b/mm/memory.c	2014-11-21 21:54:11.525087701 -0700
+@@ -61,6 +61,7 @@
+ #include <linux/string.h>
+ #include <linux/dma-debug.h>
+ #include <linux/debugfs.h>
++#include <linux/io-mapping.h>
+ 
+ #include <asm/io.h>
+ #include <asm/pgalloc.h>
+@@ -1650,71 +1651,81 @@
+ }
+ EXPORT_SYMBOL(vm_insert_mixed);
+ 
++struct remap_pfn {
++	struct mm_struct *mm;
++	unsigned long addr;
++	unsigned long pfn;
++	pgprot_t prot;
++};
++
+ /*
+  * maps a range of physical memory into the requested pages. the old
+  * mappings are removed. any references to nonexistent pages results
+  * in null mappings (currently treated as "copy-on-access")
+  */
+-static int remap_pte_range(struct mm_struct *mm, pmd_t *pmd,
+-			unsigned long addr, unsigned long end,
+-			unsigned long pfn, pgprot_t prot)
++static inline int remap_pfn(struct remap_pfn *r, pte_t *pte)
++{
++	if (!pte_none(*pte))
++		return -EBUSY;
++
++	set_pte_at(r->mm, r->addr, pte,
++		   pte_mkspecial(pfn_pte(r->pfn, r->prot)));
++	r->pfn++;
++	r->addr += PAGE_SIZE;
++	return 0;
++}
++
++static int remap_pte_range(struct remap_pfn *r, pmd_t *pmd, unsigned long end)
+ {
+ 	pte_t *pte;
+ 	spinlock_t *ptl;
++	int err;
+ 
+-	pte = pte_alloc_map_lock(mm, pmd, addr, &ptl);
++	pte = pte_alloc_map_lock(r->mm, pmd, r->addr, &ptl);
+ 	if (!pte)
+ 		return -ENOMEM;
++
+ 	arch_enter_lazy_mmu_mode();
+ 	do {
+-		BUG_ON(!pte_none(*pte));
+-		set_pte_at(mm, addr, pte, pte_mkspecial(pfn_pte(pfn, prot)));
+-		pfn++;
+-	} while (pte++, addr += PAGE_SIZE, addr != end);
++		err = remap_pfn(r, pte++);
++	} while (err == 0 && r->addr < end);
+ 	arch_leave_lazy_mmu_mode();
++
+ 	pte_unmap_unlock(pte - 1, ptl);
+-	return 0;
++	return err;
+ }
+ 
+-static inline int remap_pmd_range(struct mm_struct *mm, pud_t *pud,
+-			unsigned long addr, unsigned long end,
+-			unsigned long pfn, pgprot_t prot)
++static inline int remap_pmd_range(struct remap_pfn *r, pud_t *pud, unsigned long end)
+ {
+ 	pmd_t *pmd;
+-	unsigned long next;
++	int err;
+ 
+-	pfn -= addr >> PAGE_SHIFT;
+-	pmd = pmd_alloc(mm, pud, addr);
++	pmd = pmd_alloc(r->mm, pud, r->addr);
+ 	if (!pmd)
+ 		return -ENOMEM;
+ 	VM_BUG_ON(pmd_trans_huge(*pmd));
++
+ 	do {
+-		next = pmd_addr_end(addr, end);
+-		if (remap_pte_range(mm, pmd, addr, next,
+-				pfn + (addr >> PAGE_SHIFT), prot))
+-			return -ENOMEM;
+-	} while (pmd++, addr = next, addr != end);
+-	return 0;
++		err = remap_pte_range(r, pmd++, pmd_addr_end(r->addr, end));
++	} while (err == 0 && r->addr < end);
++
++	return err;
+ }
+ 
+-static inline int remap_pud_range(struct mm_struct *mm, pgd_t *pgd,
+-			unsigned long addr, unsigned long end,
+-			unsigned long pfn, pgprot_t prot)
++static inline int remap_pud_range(struct remap_pfn *r, pgd_t *pgd, unsigned long end)
+ {
+ 	pud_t *pud;
+-	unsigned long next;
++	int err;
+ 
+-	pfn -= addr >> PAGE_SHIFT;
+-	pud = pud_alloc(mm, pgd, addr);
++	pud = pud_alloc(r->mm, pgd, r->addr);
+ 	if (!pud)
+ 		return -ENOMEM;
++
+ 	do {
+-		next = pud_addr_end(addr, end);
+-		if (remap_pmd_range(mm, pud, addr, next,
+-				pfn + (addr >> PAGE_SHIFT), prot))
+-			return -ENOMEM;
+-	} while (pud++, addr = next, addr != end);
+-	return 0;
++		err = remap_pmd_range(r, pud++, pud_addr_end(r->addr, end));
++	} while (err == 0 && r->addr < end);
++
++	return err;
+ }
+ 
+ /**
+@@ -1730,10 +1741,9 @@
+ int remap_pfn_range(struct vm_area_struct *vma, unsigned long addr,
+ 		    unsigned long pfn, unsigned long size, pgprot_t prot)
+ {
+-	pgd_t *pgd;
+-	unsigned long next;
+ 	unsigned long end = addr + PAGE_ALIGN(size);
+-	struct mm_struct *mm = vma->vm_mm;
++	struct remap_pfn r;
++	pgd_t *pgd;
+ 	int err;
+ 
+ 	/*
+@@ -1767,25 +1777,73 @@
+ 	vma->vm_flags |= VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP;
+ 
+ 	BUG_ON(addr >= end);
+-	pfn -= addr >> PAGE_SHIFT;
+-	pgd = pgd_offset(mm, addr);
+ 	flush_cache_range(vma, addr, end);
++
++	r.mm = vma->vm_mm;
++	r.addr = addr;
++	r.pfn = pfn;
++	r.prot = prot;
++
++	pgd = pgd_offset(r.mm, addr);
+ 	do {
+-		next = pgd_addr_end(addr, end);
+-		err = remap_pud_range(mm, pgd, addr, next,
+-				pfn + (addr >> PAGE_SHIFT), prot);
+-		if (err)
+-			break;
+-	} while (pgd++, addr = next, addr != end);
++		err = remap_pud_range(&r, pgd++, pgd_addr_end(r.addr, end));
++	} while (err == 0 && r.addr < end);
+ 
+-	if (err)
++	if (err) {
+ 		untrack_pfn(vma, pfn, PAGE_ALIGN(size));
++		BUG_ON(err == -EBUSY);
++	}
+ 
+ 	return err;
+ }
+ EXPORT_SYMBOL(remap_pfn_range);
+ 
+ /**
++ * remap_io_mapping - remap an IO mapping to userspace
++ * @vma: user vma to map to
++ * @addr: target user address to start at
++ * @pfn: physical address of kernel memory
++ * @size: size of map area
++ * @iomap: the source io_mapping
++ *
++ *  Note: this is only safe if the mm semaphore is held when called.
++ */
++int remap_io_mapping(struct vm_area_struct *vma,
++		     unsigned long addr, unsigned long pfn, unsigned long size,
++		     struct io_mapping *iomap)
++{
++	unsigned long end = addr + PAGE_ALIGN(size);
++	struct remap_pfn r;
++	pgd_t *pgd;
++	int err;
++
++	if (WARN_ON(addr >= end))
++		return -EINVAL;
++
++#define MUST_SET (VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP)
++	BUG_ON(is_cow_mapping(vma->vm_flags));
++	BUG_ON((vma->vm_flags & MUST_SET) != MUST_SET);
++#undef MUST_SET
++
++	r.mm = vma->vm_mm;
++	r.addr = addr;
++	r.pfn = pfn;
++	r.prot = __pgprot((pgprot_val(iomap->prot) & _PAGE_CACHE_MASK) |
++			  (pgprot_val(vma->vm_page_prot) & ~_PAGE_CACHE_MASK));
++
++	pgd = pgd_offset(r.mm, addr);
++	do {
++		err = remap_pud_range(&r, pgd++, pgd_addr_end(r.addr, end));
++	} while (err == 0 && r.addr < end);
++
++	if (err)
++		zap_page_range_single(vma, addr, r.addr - addr, NULL);
++
++	return err;
++}
++EXPORT_SYMBOL(remap_io_mapping);
++
++/**
+  * vm_iomap_memory - remap memory to userspace
+  * @vma: user vma to map to
+  * @start: start of area
diff -urN a/include/drm/drm_crtc.h b/include/drm/drm_crtc.h
--- a/include/drm/drm_crtc.h	2014-11-21 10:24:10.000000000 -0700
+++ b/include/drm/drm_crtc.h	2014-11-22 09:55:56.160828513 -0700
@@ -567,6 +567,39 @@
 };
 
 /**
+ * struct drm_plane_state - mutable plane state
+ * @crtc: currently bound CRTC, NULL if disabled
+ * @fb: currently bound framebuffer
+ * @fence: optional fence to wait for before scanning out @fb
+ * @crtc_x: left position of visible portion of plane on crtc
+ * @crtc_y: upper position of visible portion of plane on crtc
+ * @crtc_w: width of visible portion of plane on crtc
+ * @crtc_h: height of visible portion of plane on crtc
+ * @src_x: left position of visible portion of plane within
+ *	plane (in 16.16)
+ * @src_y: upper position of visible portion of plane within
+ *	plane (in 16.16)
+ * @src_w: width of visible portion of plane (in 16.16)
+ * @src_h: height of visible portion of plane (in 16.16)
+ * @state: backpointer to global drm_atomic_state
+ */
+struct drm_plane_state {
+	struct drm_crtc *crtc;
+	struct drm_framebuffer *fb;
+	struct fence *fence;
+
+	/* Signed dest location allows it to be partially off screen */
+	int32_t crtc_x, crtc_y;
+	uint32_t crtc_w, crtc_h;
+
+	/* Source values are 16.16 fixed point */
+	uint32_t src_x, src_y;
+	uint32_t src_h, src_w;
+
+	struct drm_atomic_state *state;
+};
+
+/**
  * drm_plane_funcs - driver plane control functions
  * @update_plane: update the plane configuration
  * @disable_plane: shut down the plane
@@ -611,6 +644,8 @@
 	struct drm_device *dev;
 	struct list_head head;
 
+	struct drm_modeset_lock mutex;
+
 	struct drm_mode_object base;
 
 	uint32_t possible_crtcs;
@@ -620,11 +655,17 @@
 	struct drm_crtc *crtc;
 	struct drm_framebuffer *fb;
 
+	struct drm_framebuffer *old_fb;
+
 	const struct drm_plane_funcs *funcs;
 
 	struct drm_object_properties properties;
 
 	enum drm_plane_type type;
+
+	void *helper_private;
+
+	struct drm_plane_state *state;
 };
 
 /**
@@ -772,14 +813,7 @@
 	struct idr crtc_idr; /* use this idr for all IDs, fb, crtc, connector, modes - just makes life easier */
 	/* this is limited to one for now */
 
-
-	/**
-	 * fb_lock - mutex to protect fb state
-	 *
-	 * Besides the global fb list his also protects the fbs list in the
-	 * file_priv
-	 */
-	struct mutex fb_lock;
+	struct mutex fb_lock; /* proctects global and per-file fb lists */
 	int num_fb;
 	struct list_head fb_list;
 
@@ -821,6 +855,7 @@
 	struct drm_property *dpms_property;
 	struct drm_property *path_property;
 	struct drm_property *plane_type_property;
+	struct drm_property *rotation_property;
 
 	/* DVI-I properties */
 	struct drm_property *dvi_i_subconnector_property;
@@ -846,6 +881,10 @@
 	struct drm_property *aspect_ratio_property;
 	struct drm_property *dirty_info_property;
 
+	/* properties for virtual machine layout */
+	struct drm_property *suggested_x_property;
+	struct drm_property *suggested_y_property;
+
 	/* dumb ioctl parameters */
 	uint32_t preferred_depth, prefer_shadow;
 
@@ -971,9 +1010,9 @@
 extern void drm_mode_config_cleanup(struct drm_device *dev);
 
 extern int drm_mode_connector_set_path_property(struct drm_connector *connector,
-						char *path);
+						const char *path);
 extern int drm_mode_connector_update_edid_property(struct drm_connector *connector,
-						struct edid *edid);
+						   const struct edid *edid);
 
 static inline bool drm_property_type_is(struct drm_property *property,
 		uint32_t type)
diff -urN a/include/drm/drm_dp_helper.h b/include/drm/drm_dp_helper.h
--- a/include/drm/drm_dp_helper.h	2014-11-21 10:24:10.000000000 -0700
+++ b/include/drm/drm_dp_helper.h	2014-11-22 09:55:56.164830513 -0700
@@ -194,12 +194,20 @@
 # define DP_TRAIN_VOLTAGE_SWING_600	    (1 << 0)
 # define DP_TRAIN_VOLTAGE_SWING_800	    (2 << 0)
 # define DP_TRAIN_VOLTAGE_SWING_1200	    (3 << 0)
+# define DP_TRAIN_VOLTAGE_SWING_LEVEL_0 (0 << 0)
+# define DP_TRAIN_VOLTAGE_SWING_LEVEL_1 (1 << 0)
+# define DP_TRAIN_VOLTAGE_SWING_LEVEL_2 (2 << 0)
+# define DP_TRAIN_VOLTAGE_SWING_LEVEL_3 (3 << 0)
 
 # define DP_TRAIN_PRE_EMPHASIS_MASK	    (3 << 3)
 # define DP_TRAIN_PRE_EMPHASIS_0	    (0 << 3)
 # define DP_TRAIN_PRE_EMPHASIS_3_5	    (1 << 3)
 # define DP_TRAIN_PRE_EMPHASIS_6	    (2 << 3)
 # define DP_TRAIN_PRE_EMPHASIS_9_5	    (3 << 3)
+# define DP_TRAIN_PRE_EMPH_LEVEL_0	    (0 << 3)
+# define DP_TRAIN_PRE_EMPH_LEVEL_1	    (1 << 3)
+# define DP_TRAIN_PRE_EMPH_LEVEL_2	    (2 << 3)
+# define DP_TRAIN_PRE_EMPH_LEVEL_3	    (3 << 3)
 
 # define DP_TRAIN_PRE_EMPHASIS_SHIFT	    3
 # define DP_TRAIN_MAX_PRE_EMPHASIS_REACHED  (1 << 5)
@@ -304,6 +312,7 @@
 
 #define DP_TEST_SINK_MISC		    0x246
 #define DP_TEST_CRC_SUPPORTED		    (1 << 5)
+#define DP_TEST_COUNT_MASK		    0x7
 
 #define DP_TEST_RESPONSE		    0x260
 # define DP_TEST_ACK			    (1 << 0)
diff -urN a/include/drm/drm_dp_mst_helper.h b/include/drm/drm_dp_mst_helper.h
--- a/include/drm/drm_dp_mst_helper.h	2014-11-21 10:24:10.000000000 -0700
+++ b/include/drm/drm_dp_mst_helper.h	2014-11-22 09:55:56.164830513 -0700
@@ -371,7 +371,7 @@
 struct drm_dp_mst_topology_mgr;
 struct drm_dp_mst_topology_cbs {
 	/* create a connector for a port */
-	struct drm_connector *(*add_connector)(struct drm_dp_mst_topology_mgr *mgr, struct drm_dp_mst_port *port, char *path);
+	struct drm_connector *(*add_connector)(struct drm_dp_mst_topology_mgr *mgr, struct drm_dp_mst_port *port, const char *path);
 	void (*destroy_connector)(struct drm_dp_mst_topology_mgr *mgr,
 				  struct drm_connector *connector);
 	void (*hotplug)(struct drm_dp_mst_topology_mgr *mgr);
diff -urN a/include/drm/drm_edid.h b/include/drm/drm_edid.h
--- a/include/drm/drm_edid.h	2014-11-21 10:24:10.000000000 -0700
+++ b/include/drm/drm_edid.h	2014-11-22 09:55:56.164830513 -0700
@@ -207,6 +207,61 @@
 #define DRM_EDID_HDMI_DC_30               (1 << 4)
 #define DRM_EDID_HDMI_DC_Y444             (1 << 3)
 
+/* ELD Header Block */
+#define DRM_ELD_HEADER_BLOCK_SIZE	4
+
+#define DRM_ELD_VER			0
+# define DRM_ELD_VER_SHIFT		3
+# define DRM_ELD_VER_MASK		(0x1f << 3)
+
+#define DRM_ELD_BASELINE_ELD_LEN	2	/* in dwords! */
+
+/* ELD Baseline Block for ELD_Ver == 2 */
+#define DRM_ELD_CEA_EDID_VER_MNL	4
+# define DRM_ELD_CEA_EDID_VER_SHIFT	5
+# define DRM_ELD_CEA_EDID_VER_MASK	(7 << 5)
+# define DRM_ELD_CEA_EDID_VER_NONE	(0 << 5)
+# define DRM_ELD_CEA_EDID_VER_CEA861	(1 << 5)
+# define DRM_ELD_CEA_EDID_VER_CEA861A	(2 << 5)
+# define DRM_ELD_CEA_EDID_VER_CEA861BCD	(3 << 5)
+# define DRM_ELD_MNL_SHIFT		0
+# define DRM_ELD_MNL_MASK		(0x1f << 0)
+
+#define DRM_ELD_SAD_COUNT_CONN_TYPE	5
+# define DRM_ELD_SAD_COUNT_SHIFT	4
+# define DRM_ELD_SAD_COUNT_MASK		(0xf << 4)
+# define DRM_ELD_CONN_TYPE_SHIFT	2
+# define DRM_ELD_CONN_TYPE_MASK		(3 << 2)
+# define DRM_ELD_CONN_TYPE_HDMI		(0 << 2)
+# define DRM_ELD_CONN_TYPE_DP		(1 << 2)
+# define DRM_ELD_SUPPORTS_AI		(1 << 1)
+# define DRM_ELD_SUPPORTS_HDCP		(1 << 0)
+
+#define DRM_ELD_AUD_SYNCH_DELAY		6	/* in units of 2 ms */
+# define DRM_ELD_AUD_SYNCH_DELAY_MAX	0xfa	/* 500 ms */
+
+#define DRM_ELD_SPEAKER			7
+# define DRM_ELD_SPEAKER_RLRC		(1 << 6)
+# define DRM_ELD_SPEAKER_FLRC		(1 << 5)
+# define DRM_ELD_SPEAKER_RC		(1 << 4)
+# define DRM_ELD_SPEAKER_RLR		(1 << 3)
+# define DRM_ELD_SPEAKER_FC		(1 << 2)
+# define DRM_ELD_SPEAKER_LFE		(1 << 1)
+# define DRM_ELD_SPEAKER_FLR		(1 << 0)
+
+#define DRM_ELD_PORT_ID			8	/* offsets 8..15 inclusive */
+# define DRM_ELD_PORT_ID_LEN		8
+
+#define DRM_ELD_MANUFACTURER_NAME0	16
+#define DRM_ELD_MANUFACTURER_NAME1	17
+
+#define DRM_ELD_PRODUCT_CODE0		18
+#define DRM_ELD_PRODUCT_CODE1		19
+
+#define DRM_ELD_MONITOR_NAME_STRING	20	/* offsets 20..(20+mnl-1) inclusive */
+
+#define DRM_ELD_CEA_SAD(mnl, sad)	(20 + (mnl) + 3 * (sad))
+
 struct edid {
 	u8 header[8];
 	/* Vendor & product info */
@@ -279,4 +334,51 @@
 drm_hdmi_vendor_infoframe_from_display_mode(struct hdmi_vendor_infoframe *frame,
 					    const struct drm_display_mode *mode);
 
+/**
+ * drm_eld_mnl - Get ELD monitor name length in bytes.
+ * @eld: pointer to an eld memory structure with mnl set
+ */
+static inline int drm_eld_mnl(const uint8_t *eld)
+{
+	return (eld[DRM_ELD_CEA_EDID_VER_MNL] & DRM_ELD_MNL_MASK) >> DRM_ELD_MNL_SHIFT;
+}
+
+/**
+ * drm_eld_sad_count - Get ELD SAD count.
+ * @eld: pointer to an eld memory structure with sad_count set
+ */
+static inline int drm_eld_sad_count(const uint8_t *eld)
+{
+	return (eld[DRM_ELD_SAD_COUNT_CONN_TYPE] & DRM_ELD_SAD_COUNT_MASK) >>
+		DRM_ELD_SAD_COUNT_SHIFT;
+}
+
+/**
+ * drm_eld_calc_baseline_block_size - Calculate baseline block size in bytes
+ * @eld: pointer to an eld memory structure with mnl and sad_count set
+ *
+ * This is a helper for determining the payload size of the baseline block, in
+ * bytes, for e.g. setting the Baseline_ELD_Len field in the ELD header block.
+ */
+static inline int drm_eld_calc_baseline_block_size(const uint8_t *eld)
+{
+	return DRM_ELD_MONITOR_NAME_STRING - DRM_ELD_HEADER_BLOCK_SIZE +
+		drm_eld_mnl(eld) + drm_eld_sad_count(eld) * 3;
+}
+
+/**
+ * drm_eld_size - Get ELD size in bytes
+ * @eld: pointer to a complete eld memory structure
+ *
+ * The returned value does not include the vendor block. It's vendor specific,
+ * and comprises of the remaining bytes in the ELD memory buffer after
+ * drm_eld_size() bytes of header and baseline block.
+ *
+ * The returned value is guaranteed to be a multiple of 4.
+ */
+static inline int drm_eld_size(const uint8_t *eld)
+{
+	return DRM_ELD_HEADER_BLOCK_SIZE + eld[DRM_ELD_BASELINE_ELD_LEN] * 4;
+}
+
 #endif /* __DRM_EDID_H__ */
diff -urN a/include/drm/drm_legacy.h b/include/drm/drm_legacy.h
--- a/include/drm/drm_legacy.h	1969-12-31 17:00:00.000000000 -0700
+++ b/include/drm/drm_legacy.h	2014-11-22 09:55:56.164830513 -0700
@@ -0,0 +1,51 @@
+#ifndef __DRM_LEGACY_H__
+#define __DRM_LEGACY_H__
+
+/*
+ * Copyright (c) 2014 David Herrmann <dh.herrmann@gmail.com>
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+struct drm_device;
+struct drm_file;
+
+/*
+ * Generic DRM Contexts
+ */
+
+#define DRM_KERNEL_CONTEXT		0
+#define DRM_RESERVED_CONTEXTS		1
+
+int drm_legacy_ctxbitmap_init(struct drm_device *dev);
+void drm_legacy_ctxbitmap_cleanup(struct drm_device *dev);
+void drm_legacy_ctxbitmap_free(struct drm_device *dev, int ctx_handle);
+void drm_legacy_ctxbitmap_flush(struct drm_device *dev, struct drm_file *file);
+
+int drm_legacy_resctx(struct drm_device *d, void *v, struct drm_file *f);
+int drm_legacy_addctx(struct drm_device *d, void *v, struct drm_file *f);
+int drm_legacy_getctx(struct drm_device *d, void *v, struct drm_file *f);
+int drm_legacy_switchctx(struct drm_device *d, void *v, struct drm_file *f);
+int drm_legacy_newctx(struct drm_device *d, void *v, struct drm_file *f);
+int drm_legacy_rmctx(struct drm_device *d, void *v, struct drm_file *f);
+
+int drm_legacy_setsareactx(struct drm_device *d, void *v, struct drm_file *f);
+int drm_legacy_getsareactx(struct drm_device *d, void *v, struct drm_file *f);
+
+#endif /* __DRM_LEGACY_H__ */
diff -urN a/include/drm/drmP.h b/include/drm/drmP.h
--- a/include/drm/drmP.h	2014-11-21 10:24:10.000000000 -0700
+++ b/include/drm/drmP.h	2014-11-22 09:55:56.164830513 -0700
@@ -170,6 +170,9 @@
 #define DRM_ERROR(fmt, ...)				\
 	drm_err(__func__, fmt, ##__VA_ARGS__)
 
+#define DRM_ERROR_ONCE(fmt, ...)				\
+	printk_once(KERN_ERR "[" DRM_NAME "] " fmt, ##__VA_ARGS__)
+
 /**
  * Rate limited error output.  Like DRM_ERROR() but won't flood the log.
  *
@@ -681,6 +684,7 @@
 
 /* get_scanout_position() return flags */
 #define DRM_SCANOUTPOS_VALID        (1 << 0)
+#define DRM_SCANOUTPOS_IN_VBLANK    (1 << 1)
 #define DRM_SCANOUTPOS_INVBL        (1 << 1)
 #define DRM_SCANOUTPOS_ACCURATE     (1 << 2)
 
@@ -706,6 +710,7 @@
 	int (*dma_ioctl) (struct drm_device *dev, void *data, struct drm_file *file_priv);
 	int (*dma_quiescent) (struct drm_device *);
 	int (*context_dtor) (struct drm_device *dev, int context);
+	int (*set_busid)(struct drm_device *dev, struct drm_master *master);
 
 	/**
 	 * get_vblank_counter - get raw hardware vblank counter
@@ -1079,6 +1084,16 @@
 	 */
 	bool vblank_disable_allowed;
 
+	/*
+	 * If true, vblank interrupt will be disabled immediately when the
+	 * refcount drops to zero, as opposed to via the vblank disable
+	 * timer.
+	 * This can be set to true it the hardware has a working vblank
+	 * counter and the driver uses drm_vblank_on() and drm_vblank_off()
+	 * appropriately.
+	 */
+	bool vblank_disable_immediate;
+
 	/* array of size num_crtcs */
 	struct drm_vblank_crtc *vblank;
 
@@ -1294,6 +1309,7 @@
 extern void drm_vblank_put(struct drm_device *dev, int crtc);
 extern int drm_crtc_vblank_get(struct drm_crtc *crtc);
 extern void drm_crtc_vblank_put(struct drm_crtc *crtc);
+extern void drm_wait_one_vblank(struct drm_device *dev, int crtc);
 extern void drm_vblank_off(struct drm_device *dev, int crtc);
 extern void drm_vblank_on(struct drm_device *dev, int crtc);
 extern void drm_crtc_vblank_off(struct drm_crtc *crtc);
@@ -1311,6 +1327,17 @@
 extern void drm_calc_timestamping_constants(struct drm_crtc *crtc,
 					    const struct drm_display_mode *mode);
 
+/**
+ * drm_crtc_vblank_waitqueue - get vblank waitqueue for the CRTC
+ * @crtc: which CRTC's vblank waitqueue to retrieve
+ *
+ * This function returns a pointer to the vblank waitqueue for the CRTC.
+ * Drivers can use this to implement vblank waits using wait_event() & co.
+ */
+static inline wait_queue_head_t *drm_crtc_vblank_waitqueue(struct drm_crtc *crtc)
+{
+	return &crtc->dev->vblank[drm_crtc_index(crtc)].queue;
+}
 
 /* Modesetting support */
 extern void drm_vblank_pre_modeset(struct drm_device *dev, int crtc);
@@ -1584,9 +1611,25 @@
 
 extern int drm_pci_init(struct drm_driver *driver, struct pci_driver *pdriver);
 extern void drm_pci_exit(struct drm_driver *driver, struct pci_driver *pdriver);
+#ifdef CONFIG_PCI
 extern int drm_get_pci_dev(struct pci_dev *pdev,
 			   const struct pci_device_id *ent,
 			   struct drm_driver *driver);
+extern int drm_pci_set_busid(struct drm_device *dev, struct drm_master *master);
+#else
+static inline int drm_get_pci_dev(struct pci_dev *pdev,
+				  const struct pci_device_id *ent,
+				  struct drm_driver *driver)
+{
+	return -ENOSYS;
+}
+
+static inline int drm_pci_set_busid(struct drm_device *dev,
+				    struct drm_master *master)
+{
+	return -ENOSYS;
+}
+#endif
 
 #define DRM_PCIE_SPEED_25 1
 #define DRM_PCIE_SPEED_50 2
diff -urN a/include/drm/i915_pciids.h b/include/drm/i915_pciids.h
--- a/include/drm/i915_pciids.h	2014-11-21 10:24:10.000000000 -0700
+++ b/include/drm/i915_pciids.h	2014-11-22 09:55:56.164830513 -0700
@@ -259,4 +259,21 @@
 	INTEL_VGA_DEVICE(0x22b2, info), \
 	INTEL_VGA_DEVICE(0x22b3, info)
 
+#define INTEL_SKL_IDS(info) \
+	INTEL_VGA_DEVICE(0x1916, info), /* ULT GT2 */ \
+	INTEL_VGA_DEVICE(0x1906, info), /* ULT GT1 */ \
+	INTEL_VGA_DEVICE(0x1926, info), /* ULT GT3 */ \
+	INTEL_VGA_DEVICE(0x1921, info), /* ULT GT2F */ \
+	INTEL_VGA_DEVICE(0x190E, info), /* ULX GT1 */ \
+	INTEL_VGA_DEVICE(0x191E, info), /* ULX GT2 */ \
+	INTEL_VGA_DEVICE(0x1912, info), /* DT  GT2 */ \
+	INTEL_VGA_DEVICE(0x1902, info), /* DT  GT1 */ \
+	INTEL_VGA_DEVICE(0x191B, info), /* Halo GT2 */ \
+	INTEL_VGA_DEVICE(0x192B, info), /* Halo GT3 */ \
+	INTEL_VGA_DEVICE(0x190B, info), /* Halo GT1 */ \
+	INTEL_VGA_DEVICE(0x191A, info), /* SRV GT2 */ \
+	INTEL_VGA_DEVICE(0x192A, info), /* SRV GT3 */ \
+	INTEL_VGA_DEVICE(0x190A, info), /* SRV GT1 */ \
+	INTEL_VGA_DEVICE(0x191D, info)  /* WKS GT2 */
+
 #endif /* _I915_PCIIDS_H */
diff -urN a/include/linux/io-mapping.h b/include/linux/io-mapping.h
--- a/include/linux/io-mapping.h	2014-11-21 10:24:10.000000000 -0700
+++ b/include/linux/io-mapping.h	2014-11-22 09:55:56.164830513 -0700
@@ -31,16 +31,17 @@
  * See Documentation/io-mapping.txt
  */
 
-#ifdef CONFIG_HAVE_ATOMIC_IOMAP
-
-#include <asm/iomap.h>
-
 struct io_mapping {
 	resource_size_t base;
 	unsigned long size;
 	pgprot_t prot;
+	void __iomem *iomem;
 };
 
+#ifdef CONFIG_HAVE_ATOMIC_IOMAP
+
+#include <asm/iomap.h>
+
 /*
  * For small address space machines, mapping large objects
  * into the kernel virtual space isn't practical. Where
diff -urN a/include/linux/mm.h b/include/linux/mm.h
--- a/include/linux/mm.h	2014-11-21 10:24:10.000000000 -0700
+++ b/include/linux/mm.h	2014-11-22 09:55:56.164830513 -0700
@@ -1973,6 +1973,10 @@
 struct vm_area_struct *find_extend_vma(struct mm_struct *, unsigned long addr);
 int remap_pfn_range(struct vm_area_struct *, unsigned long addr,
 			unsigned long pfn, unsigned long size, pgprot_t);
+struct io_mapping;
+int remap_io_mapping(struct vm_area_struct *,
+                     unsigned long addr, unsigned long pfn, unsigned long size,
+                     struct io_mapping *iomap);
 int vm_insert_page(struct vm_area_struct *, unsigned long addr, struct page *);
 int vm_insert_pfn(struct vm_area_struct *vma, unsigned long addr,
 			unsigned long pfn);
diff -urN a/include/linux/mutex.h b/include/linux/mutex.h
--- a/include/linux/mutex.h	2014-11-21 10:24:10.000000000 -0700
+++ b/include/linux/mutex.h	2014-11-22 09:55:56.168832512 -0700
@@ -143,10 +143,15 @@
 					unsigned int subclass);
 extern int __must_check mutex_lock_killable_nested(struct mutex *lock,
 					unsigned int subclass);
+extern int __must_check mutex_lock_wrapper_nested(struct mutex *lock,
+						  unsigned int subclass,
+						  long state,
+						  unsigned long ip);
 
 #define mutex_lock(lock) mutex_lock_nested(lock, 0)
 #define mutex_lock_interruptible(lock) mutex_lock_interruptible_nested(lock, 0)
 #define mutex_lock_killable(lock) mutex_lock_killable_nested(lock, 0)
+#define mutex_lock_wrapper(lock, state, ip) mutex_lock_wrapper_nested(lock, 0, state, ip)
 
 #define mutex_lock_nest_lock(lock, nest_lock)				\
 do {									\
@@ -158,10 +163,14 @@
 extern void mutex_lock(struct mutex *lock);
 extern int __must_check mutex_lock_interruptible(struct mutex *lock);
 extern int __must_check mutex_lock_killable(struct mutex *lock);
+extern int __must_check mutex_lock_wrapper(struct mutex *lock,
+					   long state,
+					   unsigned long ip);
 
 # define mutex_lock_nested(lock, subclass) mutex_lock(lock)
 # define mutex_lock_interruptible_nested(lock, subclass) mutex_lock_interruptible(lock)
 # define mutex_lock_killable_nested(lock, subclass) mutex_lock_killable(lock)
+# define mutex_lock_wrapper_nested(lock, subclass, state, ip) mutex_lock_wrapper(lock, state, ip)
 # define mutex_lock_nest_lock(lock, nest_lock) mutex_lock(lock)
 #endif
 
diff -urN a/include/uapi/drm/i915_drm.h b/include/uapi/drm/i915_drm.h
--- a/include/uapi/drm/i915_drm.h	2014-11-21 10:24:10.000000000 -0700
+++ b/include/uapi/drm/i915_drm.h	2014-11-22 09:55:56.168832512 -0700
@@ -58,6 +58,54 @@
 #define I915_ERROR_UEVENT		"ERROR"
 #define I915_RESET_UEVENT		"RESET"
 
+/**
+ * DOC: perf_events exposed by i915 through /sys/bus/event_sources/drivers/i915
+ *
+ */
+#define I915_SAMPLE_BUSY	0
+#define I915_SAMPLE_WAIT	1
+#define I915_SAMPLE_SEMA	2
+
+#define I915_SAMPLE_RCS		0
+#define I915_SAMPLE_VCS		1
+#define I915_SAMPLE_BCS		2
+#define I915_SAMPLE_VECS	3
+
+#define __I915_PERF_COUNT(ring, id) ((ring) << 4 | (id))
+
+#define I915_PERF_COUNT_RCS_BUSY __I915_PERF_COUNT(I915_SAMPLE_RCS, I915_SAMPLE_BUSY)
+#define I915_PERF_COUNT_RCS_WAIT __I915_PERF_COUNT(I915_SAMPLE_RCS, I915_SAMPLE_WAIT)
+#define I915_PERF_COUNT_RCS_SEMA __I915_PERF_COUNT(I915_SAMPLE_RCS, I915_SAMPLE_SEMA)
+
+#define I915_PERF_COUNT_VCS_BUSY __I915_PERF_COUNT(I915_SAMPLE_VCS, I915_SAMPLE_BUSY)
+#define I915_PERF_COUNT_VCS_WAIT __I915_PERF_COUNT(I915_SAMPLE_VCS, I915_SAMPLE_WAIT)
+#define I915_PERF_COUNT_VCS_SEMA __I915_PERF_COUNT(I915_SAMPLE_VCS, I915_SAMPLE_SEMA)
+
+#define I915_PERF_COUNT_BCS_BUSY __I915_PERF_COUNT(I915_SAMPLE_BCS, I915_SAMPLE_BUSY)
+#define I915_PERF_COUNT_BCS_WAIT __I915_PERF_COUNT(I915_SAMPLE_BCS, I915_SAMPLE_WAIT)
+#define I915_PERF_COUNT_BCS_SEMA __I915_PERF_COUNT(I915_SAMPLE_BCS, I915_SAMPLE_SEMA)
+
+#define I915_PERF_COUNT_VECS_BUSY __I915_PERF_COUNT(I915_SAMPLE_VECS, I915_SAMPLE_BUSY)
+#define I915_PERF_COUNT_VECS_WAIT __I915_PERF_COUNT(I915_SAMPLE_VECS, I915_SAMPLE_WAIT)
+#define I915_PERF_COUNT_VECS_SEMA __I915_PERF_COUNT(I915_SAMPLE_VECS, I915_SAMPLE_SEMA)
+
+#define I915_PERF_ACTUAL_FREQUENCY 32
+#define I915_PERF_REQUESTED_FREQUENCY 33
+#define I915_PERF_ENERGY 34
+#define I915_PERF_INTERRUPTS 35
+
+#define I915_PERF_RC6_RESIDENCY		40
+#define I915_PERF_RC6p_RESIDENCY	41
+#define I915_PERF_RC6pp_RESIDENCY	42
+
+#define I915_PERF_STATISTIC_0	48
+#define I915_PERF_STATISTIC(n)	(I915_PERF_STATISTIC_0+(n))
+#define I915_PERF_STATISTIC_8	I915_PERF_STATISTIC(8)
+
+#define I915_PERF_INSTDONE_0	64
+#define I915_PERF_INSTDONE(n)	(I915_PERF_INSTDONE_0+(n))
+#define I915_PERF_INSTDONE_63	I915_PERF_INSTDONE(64)
+
 /* Each region is a minimum of 16k, and there are at most 255 of them.
  */
 #define I915_NR_TEX_REGIONS 255	/* table size 2k - maximum due to use
@@ -224,6 +272,9 @@
 #define DRM_I915_REG_READ		0x31
 #define DRM_I915_GET_RESET_STATS	0x32
 #define DRM_I915_GEM_USERPTR		0x33
+#define DRM_I915_GEM_CONTEXT_GETPARAM	0x34
+#define DRM_I915_GEM_CONTEXT_SETPARAM	0x35
+#define DRM_I915_GEM_CONTEXT_DUMP	0x36
 
 #define DRM_IOCTL_I915_INIT		DRM_IOW( DRM_COMMAND_BASE + DRM_I915_INIT, drm_i915_init_t)
 #define DRM_IOCTL_I915_FLUSH		DRM_IO ( DRM_COMMAND_BASE + DRM_I915_FLUSH)
@@ -275,6 +326,9 @@
 #define DRM_IOCTL_I915_REG_READ			DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_REG_READ, struct drm_i915_reg_read)
 #define DRM_IOCTL_I915_GET_RESET_STATS		DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_GET_RESET_STATS, struct drm_i915_reset_stats)
 #define DRM_IOCTL_I915_GEM_USERPTR			DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_GEM_USERPTR, struct drm_i915_gem_userptr)
+#define DRM_IOCTL_I915_GEM_CONTEXT_GETPARAM	DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_GEM_CONTEXT_GETPARAM, struct drm_i915_gem_context_param)
+#define DRM_IOCTL_I915_GEM_CONTEXT_SETPARAM	DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_GEM_CONTEXT_SETPARAM, struct drm_i915_gem_context_param)
+#define DRM_IOCTL_I915_GEM_CONTEXT_DUMP	DRM_IOWR (DRM_COMMAND_BASE + DRM_I915_GEM_CONTEXT_DUMP, struct drm_i915_gem_context_dump)
 
 /* Allow drivers to submit batchbuffers directly to hardware, relying
  * on the security mechanisms provided by hardware.
@@ -340,6 +394,8 @@
 #define I915_PARAM_HAS_EXEC_HANDLE_LUT   26
 #define I915_PARAM_HAS_WT     	 	 27
 #define I915_PARAM_CMD_PARSER_VERSION	 28
+#define I915_PARAM_HAS_COHERENT_PHYS_GTT 29
+#define I915_PARAM_MMAP_VERSION 30
 
 typedef struct drm_i915_getparam {
 	int param;
@@ -487,6 +543,9 @@
 	 * This is a fixed-size type for 32/64 compatibility.
 	 */
 	__u64 addr_ptr;
+
+	__u64 flags;
+#define I915_MMAP_WC 0x1
 };
 
 struct drm_i915_gem_mmap_gtt {
@@ -662,7 +721,8 @@
 #define EXEC_OBJECT_NEEDS_FENCE (1<<0)
 #define EXEC_OBJECT_NEEDS_GTT	(1<<1)
 #define EXEC_OBJECT_WRITE	(1<<2)
-#define __EXEC_OBJECT_UNKNOWN_FLAGS -(EXEC_OBJECT_WRITE<<1)
+#define EXEC_OBJECT_PINNED	(1<<4)
+#define __EXEC_OBJECT_UNKNOWN_FLAGS -(EXEC_OBJECT_PINNED<<1)
 	__u64 flags;
 
 	__u64 rsvd1;
@@ -876,6 +936,12 @@
 	 * mmap mapping.
 	 */
 	__u32 swizzle_mode;
+
+	/**
+	 * Returned address bit 6 swizzling required for CPU access through
+	 * mmap mapping whilst bound.
+	 */
+	__u32 phys_swizzle_mode;
 };
 
 struct drm_i915_gem_get_aperture {
@@ -887,6 +953,31 @@
 	 * bytes
 	 */
 	__u64 aper_available_size;
+
+	/**
+	 * Total size of the mappable region of the aperture, in bytes
+	 */
+	__u64 map_total_size;
+
+	/**
+	 * Available space in the mappable region of the aperture, in bytes
+	 */
+	__u64 map_available_size;
+
+	/**
+	 * Single largest available region inside the mappable region, in bytes.
+	 */
+	__u64 map_largest_size;
+
+	/**
+	 * Culmulative space available for fences, in bytes
+	 */
+	__u64 fence_available_size;
+
+	/**
+	 * Single largest fenceable region, in bytes.
+	 */
+	__u64 fence_largest_size;
 };
 
 struct drm_i915_get_pipe_from_crtc_id {
@@ -900,6 +991,8 @@
 #define I915_MADV_WILLNEED 0
 #define I915_MADV_DONTNEED 1
 #define __I915_MADV_PURGED 2 /* internal state */
+#define I915_MADV_POPULATE   3
+#define I915_MADV_INVALIDATE 4
 
 struct drm_i915_gem_madvise {
 	/** Handle of the buffer to change the backing store advice */
@@ -966,6 +1059,7 @@
 /* flags */
 #define I915_OVERLAY_UPDATE_ATTRS	(1<<0)
 #define I915_OVERLAY_UPDATE_GAMMA	(1<<1)
+#define I915_OVERLAY_DISABLE_DEST_COLORKEY	(1<<2)
 struct drm_intel_overlay_attrs {
 	__u32 flags;
 	__u32 color_key;
@@ -1066,4 +1160,18 @@
 	__u32 handle;
 };
 
+struct drm_i915_gem_context_param {
+	__u32 ctx_id;
+	__u32 size;
+	__u64 param;
+#define I915_CONTEXT_PARAM_BAN_PERIOD 0x1
+	__u64 value;
+};
+
+struct drm_i915_gem_context_dump {
+	__u32 ctx_id;
+	__u32 size;
+	__u64 ptr;
+};
+
 #endif /* _UAPI_I915_DRM_H_ */
diff -urN a/kernel/events/core.c b/kernel/events/core.c
--- a/kernel/events/core.c	2014-11-21 10:24:10.000000000 -0700
+++ b/kernel/events/core.c	2014-11-22 09:55:56.168832512 -0700
@@ -5529,6 +5529,7 @@
 {
 	return __perf_event_overflow(event, 1, data, regs);
 }
+EXPORT_SYMBOL_GPL(perf_event_overflow);
 
 /*
  * Generic software event infrastructure
diff -urN a/kernel/locking/mutex.c b/kernel/locking/mutex.c
--- a/kernel/locking/mutex.c	2014-11-21 10:24:10.000000000 -0700
+++ b/kernel/locking/mutex.c	2014-11-22 09:55:56.168832512 -0700
@@ -616,6 +616,17 @@
 
 EXPORT_SYMBOL_GPL(mutex_lock_interruptible_nested);
 
+int __sched
+mutex_lock_wrapper_nested(struct mutex *lock, unsigned int subclass,
+			  long state, unsigned long ip)
+{
+	might_sleep();
+	return __mutex_lock_common(lock, state,
+				   subclass, NULL, ip, NULL, 0);
+}
+
+EXPORT_SYMBOL_GPL(mutex_lock_wrapper_nested);
+
 static inline int
 ww_mutex_deadlock_injection(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)
 {
@@ -730,6 +741,9 @@
 static noinline int __sched
 __mutex_lock_interruptible_slowpath(struct mutex *lock);
 
+static noinline int __sched
+__mutex_lock_wrapper_slowpath(struct mutex *lock, long state, unsigned long ip);
+
 /**
  * mutex_lock_interruptible - acquire the mutex, interruptible
  * @lock: the mutex to be acquired
@@ -756,6 +770,21 @@
 
 EXPORT_SYMBOL(mutex_lock_interruptible);
 
+int __sched mutex_lock_wrapper(struct mutex *lock, long state, unsigned long ip)
+{
+	int ret;
+
+	might_sleep();
+	ret =  __mutex_fastpath_lock_retval(&lock->count);
+	if (likely(!ret)) {
+		mutex_set_owner(lock);
+		return 0;
+	} else
+		return __mutex_lock_wrapper_slowpath(lock, state, ip);
+}
+
+EXPORT_SYMBOL(mutex_lock_wrapper);
+
 int __sched mutex_lock_killable(struct mutex *lock)
 {
 	int ret;
@@ -794,6 +823,13 @@
 }
 
 static noinline int __sched
+__mutex_lock_wrapper_slowpath(struct mutex *lock, long state, unsigned long ip)
+{
+	return __mutex_lock_common(lock, state, 0,
+				   NULL, ip, NULL, 0);
+}
+
+static noinline int __sched
 __ww_mutex_lock_slowpath(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)
 {
 	return __mutex_lock_common(&lock->base, TASK_UNINTERRUPTIBLE, 0,
diff -urN a/mm/memory.c b/mm/memory.c
--- a/mm/memory.c	2014-11-21 10:24:10.000000000 -0700
+++ b/mm/memory.c	2014-11-22 09:55:56.168832512 -0700
@@ -61,6 +61,7 @@
 #include <linux/string.h>
 #include <linux/dma-debug.h>
 #include <linux/debugfs.h>
+#include <linux/io-mapping.h>
 
 #include <asm/io.h>
 #include <asm/pgalloc.h>
@@ -1651,71 +1652,81 @@
 }
 EXPORT_SYMBOL(vm_insert_mixed);
 
+struct remap_pfn {
+	struct mm_struct *mm;
+	unsigned long addr;
+	unsigned long pfn;
+	pgprot_t prot;
+};
+
 /*
  * maps a range of physical memory into the requested pages. the old
  * mappings are removed. any references to nonexistent pages results
  * in null mappings (currently treated as "copy-on-access")
  */
-static int remap_pte_range(struct mm_struct *mm, pmd_t *pmd,
-			unsigned long addr, unsigned long end,
-			unsigned long pfn, pgprot_t prot)
+static inline int remap_pfn(struct remap_pfn *r, pte_t *pte)
+{
+	if (!pte_none(*pte))
+		return -EBUSY;
+
+	set_pte_at(r->mm, r->addr, pte,
+		   pte_mkspecial(pfn_pte(r->pfn, r->prot)));
+	r->pfn++;
+	r->addr += PAGE_SIZE;
+	return 0;
+}
+
+static int remap_pte_range(struct remap_pfn *r, pmd_t *pmd, unsigned long end)
 {
 	pte_t *pte;
 	spinlock_t *ptl;
+	int err;
 
-	pte = pte_alloc_map_lock(mm, pmd, addr, &ptl);
+	pte = pte_alloc_map_lock(r->mm, pmd, r->addr, &ptl);
 	if (!pte)
 		return -ENOMEM;
+
 	arch_enter_lazy_mmu_mode();
 	do {
-		BUG_ON(!pte_none(*pte));
-		set_pte_at(mm, addr, pte, pte_mkspecial(pfn_pte(pfn, prot)));
-		pfn++;
-	} while (pte++, addr += PAGE_SIZE, addr != end);
+		err = remap_pfn(r, pte++);
+	} while (err == 0 && r->addr < end);
 	arch_leave_lazy_mmu_mode();
+
 	pte_unmap_unlock(pte - 1, ptl);
-	return 0;
+	return err;
 }
 
-static inline int remap_pmd_range(struct mm_struct *mm, pud_t *pud,
-			unsigned long addr, unsigned long end,
-			unsigned long pfn, pgprot_t prot)
+static inline int remap_pmd_range(struct remap_pfn *r, pud_t *pud, unsigned long end)
 {
 	pmd_t *pmd;
-	unsigned long next;
+	int err;
 
-	pfn -= addr >> PAGE_SHIFT;
-	pmd = pmd_alloc(mm, pud, addr);
+	pmd = pmd_alloc(r->mm, pud, r->addr);
 	if (!pmd)
 		return -ENOMEM;
 	VM_BUG_ON(pmd_trans_huge(*pmd));
+
 	do {
-		next = pmd_addr_end(addr, end);
-		if (remap_pte_range(mm, pmd, addr, next,
-				pfn + (addr >> PAGE_SHIFT), prot))
-			return -ENOMEM;
-	} while (pmd++, addr = next, addr != end);
-	return 0;
+		err = remap_pte_range(r, pmd++, pmd_addr_end(r->addr, end));
+	} while (err == 0 && r->addr < end);
+
+	return err;
 }
 
-static inline int remap_pud_range(struct mm_struct *mm, pgd_t *pgd,
-			unsigned long addr, unsigned long end,
-			unsigned long pfn, pgprot_t prot)
+static inline int remap_pud_range(struct remap_pfn *r, pgd_t *pgd, unsigned long end)
 {
 	pud_t *pud;
-	unsigned long next;
+	int err;
 
-	pfn -= addr >> PAGE_SHIFT;
-	pud = pud_alloc(mm, pgd, addr);
+	pud = pud_alloc(r->mm, pgd, r->addr);
 	if (!pud)
 		return -ENOMEM;
+
 	do {
-		next = pud_addr_end(addr, end);
-		if (remap_pmd_range(mm, pud, addr, next,
-				pfn + (addr >> PAGE_SHIFT), prot))
-			return -ENOMEM;
-	} while (pud++, addr = next, addr != end);
-	return 0;
+		err = remap_pmd_range(r, pud++, pud_addr_end(r->addr, end));
+	} while (err == 0 && r->addr < end);
+
+	return err;
 }
 
 /**
@@ -1731,10 +1742,9 @@
 int remap_pfn_range(struct vm_area_struct *vma, unsigned long addr,
 		    unsigned long pfn, unsigned long size, pgprot_t prot)
 {
-	pgd_t *pgd;
-	unsigned long next;
 	unsigned long end = addr + PAGE_ALIGN(size);
-	struct mm_struct *mm = vma->vm_mm;
+	struct remap_pfn r;
+	pgd_t *pgd;
 	int err;
 
 	/*
@@ -1768,25 +1778,73 @@
 	vma->vm_flags |= VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP;
 
 	BUG_ON(addr >= end);
-	pfn -= addr >> PAGE_SHIFT;
-	pgd = pgd_offset(mm, addr);
 	flush_cache_range(vma, addr, end);
+
+	r.mm = vma->vm_mm;
+	r.addr = addr;
+	r.pfn = pfn;
+	r.prot = prot;
+
+	pgd = pgd_offset(r.mm, addr);
 	do {
-		next = pgd_addr_end(addr, end);
-		err = remap_pud_range(mm, pgd, addr, next,
-				pfn + (addr >> PAGE_SHIFT), prot);
-		if (err)
-			break;
-	} while (pgd++, addr = next, addr != end);
+		err = remap_pud_range(&r, pgd++, pgd_addr_end(r.addr, end));
+	} while (err == 0 && r.addr < end);
 
-	if (err)
+	if (err) {
 		untrack_pfn(vma, pfn, PAGE_ALIGN(size));
+		BUG_ON(err == -EBUSY);
+	}
 
 	return err;
 }
 EXPORT_SYMBOL(remap_pfn_range);
 
 /**
+ * remap_io_mapping - remap an IO mapping to userspace
+ * @vma: user vma to map to
+ * @addr: target user address to start at
+ * @pfn: physical address of kernel memory
+ * @size: size of map area
+ * @iomap: the source io_mapping
+ *
+ *  Note: this is only safe if the mm semaphore is held when called.
+ */
+int remap_io_mapping(struct vm_area_struct *vma,
+		     unsigned long addr, unsigned long pfn, unsigned long size,
+		     struct io_mapping *iomap)
+{
+	unsigned long end = addr + PAGE_ALIGN(size);
+	struct remap_pfn r;
+	pgd_t *pgd;
+	int err;
+
+	if (WARN_ON(addr >= end))
+		return -EINVAL;
+
+#define MUST_SET (VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP)
+	BUG_ON(is_cow_mapping(vma->vm_flags));
+	BUG_ON((vma->vm_flags & MUST_SET) != MUST_SET);
+#undef MUST_SET
+
+	r.mm = vma->vm_mm;
+	r.addr = addr;
+	r.pfn = pfn;
+	r.prot = __pgprot((pgprot_val(iomap->prot) & _PAGE_CACHE_MASK) |
+			  (pgprot_val(vma->vm_page_prot) & ~_PAGE_CACHE_MASK));
+
+	pgd = pgd_offset(r.mm, addr);
+	do {
+		err = remap_pud_range(&r, pgd++, pgd_addr_end(r.addr, end));
+	} while (err == 0 && r.addr < end);
+
+	if (err)
+		zap_page_range_single(vma, addr, r.addr - addr, NULL);
+
+	return err;
+}
+EXPORT_SYMBOL(remap_io_mapping);
+
+/**
  * vm_iomap_memory - remap memory to userspace
  * @vma: user vma to map to
  * @start: start of area
